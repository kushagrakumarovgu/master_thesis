review,reasoning,sentiment_score,politeness_score
"This paper presents a new contribution for a largely understudied problem of label shift (also called target shift), a situation occurring when the class proportions vary between the training and test sets. The proposed contribution builds upon a recent work on the subject by Lipton et al., 2018 and addresses several of its weaknesses. The paper also gives several improved generalisation bounds w.r.t. that of Lipton et al. that are further used as guidelines to tune the regularisation parameter based on the size of source and target samples. Finally, the empirical results show that the proposed algorithm outperforms that of Lipton et al. especially in cases where the shift in proportions becomes quite important. 

*Pros: 
   - A work in an area with very view contributions and a certain lack of theoretical results
    -Theoretical results that are actually used in the algorithmic implementation and that allow to define the regularisation parameter based on the size of the available samples
    -Improved empirical results


*Cons: 
    -An incomplete state-of-the-art section that does not cite several important contributions on the subject;
    -Lack of baselines due to the incomplete state-of-the-art section;
    -Lack of clear comparison with Lipton et al. both in terms of the proposed method and the obtained theoretical guarantees.  


*Detailed comments:
This paper is rather interesting and well-written.

I have several major concerns regarding this paper. They can be summarised as follows:

    There is an important part of literature review on target shift that is missing in this paper. Even though, the paper mentioned the work of Chang, 2005 and Zhang, 2013, it completely ignores several other highly relevant methods such as [1,2]. These works also propose algorithms that allow to estimate class proportions that vary between training and test data. This estimation can then be used for cost-sensitive learning to correct the target shift. The paper should mention this work and add the corresponding methods to the baselines for comparison. 

    Several statements that justify the contribution of this paper are unsupported. For instance, the paper states that the estimator obtained with the inverse of the confusion matrix can be arbitrary bad when the sample size and/or the singular values are small. However, this exact dependence can be found in Lemma 1 for the proposed contribution also! This is repeated in the beginning of Section 2.2 to justify the regularised version of the estimator but once again no evidence was provided to support the claim. The obtained bound for the regularised algorithm also has these two terms and thus it is not clear why the regularised algorithm is supposed to work better. 

    The paper may want to clearly state the differences between the proposed algorithm and that of Lipton et al. and also between the obtained error bounds. The paper states that it achieves a k*log(k) improvement over Lipton et al. bounds but as fair as I can see this improvement is achieved only when h_0 is an ideal estimator. Furthermore, Lipton et al.’s bounds are linear in k while the proposed bounds replace this term with log(k/delta) so that when \delta is small, ie the bound holds with high probability, the bound becomes much worse. I would suggest to add a brief discussion on the relationship between the two to better highlight the original contribution of the paper. 

    The proofs are quite badly written with many lacking results used to move from one inequality to another. For instance, Lemma 2 is proved using the theorem 1.4[Matrix Bernstein] and dilation technique from Tropp but it is not clear which results the authors are using in particular; Theorem 1.4 is related to the largest eigenvalue of the sum of matrices while the authors obtain an inequality for the norm of the sum without any further comment on how this transition was made. Also, I do not see why delta is smaller than 1/2 in Lemma 2. 


*Minor comments:

   - p.1: expected have -> expected to have
   - p.4: we are instead only gave access -> given access to 
   - I do not understand Figure 1. Should it be n_q*n_p on the y axis ?
   - The inequality for n_q next to Figure 1 is derived from the bound (6). Why it is independent of k?
   - Why the authors choose to the black box predictor h0 to be a two-layer fully connected neural? Is there any particular reason to use this classification model?

[1] Class Proportion Estimation with Application to Multiclass Anomaly Rejection, AISTATS14
[2] Mixture Proportion Estimation via Kernel Embeddings of Distributions, ICML16","The sentiment of the review is generally positive, as indicated by the initial paragraph that praises the paper for addressing an understudied problem and improving upon previous work. However, the review also contains several major and minor concerns, which slightly temper the overall positive sentiment. Therefore, the sentiment score is 40. The language used in the review is polite and constructive, offering specific recommendations and detailed comments without being rude or dismissive. Therefore, the politeness score is 80.",40,80
"The paper considers deep nets with piecewise linear activation functions, which are known to give rise to piecewise linear input-output mappings, and proposes loss functions which discourage datapoints in the input space from lying near the boundary between linear regions. These loss functions are well-motivated theoretically, and have the intended effect of increasing the distance to the nearest boundary and reducing the number of distinct linear regions. 

My only concern is that while their method appears to effectively increase the l_1 and l_2 margin (as they have defined it), the utility of doing so is not clearly demonstrated. If improving the quality or validity of local linearization for explaining predictions is one of the main motivations for this work, showing that the proposed method does so would strengthen the overall message. However, I do feel that “establishing robust derivatives over larger regions” is an important problem in its own right. 

With the exception of some minor typos, the exposition is clear and the theoretical claims all appear correct. The authors may have missed some relevant recent work [1], but their contributions are complementary. It is not immediately clear that the parallel computation of gradients proposed in section 4.1 is any faster than standard backpropagation, as this has to be carried out separately for each linear region. A basic complexity analysis or running time comparison would help clarify this. I think I am missing the point of the gradient visualizations in figure 4, panels b-e and g-j. 


[1] Elsayed, Gamaleldin F., et al. ""Large Margin Deep Networks for Classification."" arXiv preprint arXiv:1803.05598 (To appear in NIPS 2018).","The sentiment of the review is generally positive, as the reviewer acknowledges the theoretical motivation and effectiveness of the proposed loss functions. However, there is a concern about the practical utility of the method, which slightly tempers the overall positive sentiment. The politeness of the language is high, as the reviewer provides constructive feedback and suggestions in a respectful and professional manner, even when pointing out concerns and areas for improvement.",70,90
"In this paper, the authors associated with the generalization gap of robust adversarial training with the distance between the test point and the manifold of training data. A so-called 'blind-spot attack' is proposed to show the weakness of robust adversarial training.  Although the paper contains interesting ideas and empirical results, I have several concerns about the current version. 

a) In the paper, the authors mentioned that ""This simple metric is non-parametric and we found that the results are not sensitive to the selection of k"". Can authors provide more details, e.g., empirical results, about it? What is its rationale?

b) In the paper, ""We find that these blind-spots are prevalent and can be easily found without resorting to complex
generative models like in Song et al. (2018). For the MNIST dataset which Madry et al. (2018) demonstrate the strongest defense results so far, we propose a simple transformation to find the blind-spots in this model."" Can authors provide empirical comparison between blind-spot attacks and the work by Song et al. (2018), e.g., attack success rate & distortion? 

c) The linear transformation x^\prime = \alpha x + \beta yields a blind-spot attack which can defeat robust adversarial training. However, given the linear transformation, one can further modify the inner maximization (adv. example generation) in robust training framework so that the $\ell_infty$ attack satisfies  max_{\alpha, \beta} f(\alpha x + \beta) subject to \| \alpha x + \beta \|\leq \epsilon. In this case, robust training framework can defend blind-spot attacks, right? I agree with the authors that the generalization error is due to the mismatch between training data and test data distribution, however, I am not convinced that blind-spot attacks are effective enough to robust training. 

d) ""Because we scale the image by a factor of \alpha, we also set a stricter criterion of success, ..., perturbation must be less
than \alpha \epsilon to be counted as a successful attack."" I did not get the point. Even if you have a scaling factor in x^\prime = \alpha x + \beta, the universal perturbation rule should still be | x - x^\prime  |_\infty \leq \epsilon. The metric the authors used would result in a higher attack success rate, right? 
","The sentiment of the review is mixed but leans towards the positive side. The reviewer acknowledges that the paper contains interesting ideas and empirical results, which is a positive remark. However, the reviewer also expresses several concerns and requests for additional details and clarifications, which introduces a critical tone. Therefore, the sentiment score is moderately positive. The politeness of the language is high; the reviewer uses polite phrases such as 'Can authors provide more details' and 'I agree with the authors,' which indicates a respectful and constructive approach. The reviewer avoids any harsh or rude language, making the politeness score high.",30,80
"The paper considers a number of streaming learning settings with various forms of dataset shift/drift of interest for continual learning research, and proposes a novel regularization-based objective enabled by a replay memory managed using the well known reservoir sampling algorithm.

Pros:
The new objective is not too surprising, but figuring out how to effectively implement this objective in a streaming setting is the strong point of this paper. 

Task labels are not used, yet performance seems superior to competing methods, many of which use task labels.

Results are good on popular benchmarks, I find the baselines convincing in the supervised case.

Cons:
Despite somewhat frequent usage, I would like to respectfully point out that Permuted MNIST experiments are not very indicative for a majority of desiderata of interest in continual learning, and i.m.h.o. should be used only as a prototyping tool. To pick one issue, such results can be misleading since the benchmark allows for “trivial” solutions which effectively freeze the upper part of the network and only change first (few) layer(s) which “undo” the permutation. This is an artificial type of dataset shift, and is not realistic for the type of continual learning issues which appear even in single task deep reinforcement learning, where policies or value functions represented by the model need to change substantially across learning.

I was pleased to see the RL experiments, which I find more convincing because dataset drifts/shifts are more interesting. Also, such applications of continual learning solutions are attempting to solve a ‘real problem’, or at least something which researchers in that field struggle with. That said, I do have a few suggestions. At first glance, it’s not clear whether anything is learned in the last 3 versions of Catcher, also what the y axis actually means. What is good performance for each game is very specific to your actual settings so I have no reference to compare the scores with. The sequence of games is progressively harder, so it makes sense that scores are lower, but it’s not clear whether your approach impedes learning of new tasks, i.e. what is the price to pay for not forgetting?

This is particularly important for the points you’re trying to make because a large number of competing approaches either saturate the available capacity and memory with the first few tasks, or they faithfully model the recent ones. Any improvement there is worth a lot of attention, given proper comparisons. Even if this approach does not strike the ‘optimal’ balance, it is still worth knowing how much training would be required to reach full single-task performance on each game variant, and what kind of forgetting that induces. 
","The sentiment of the review is generally positive, as the reviewer acknowledges the strengths of the paper, such as the novel objective and good performance on benchmarks. However, there are also some critical points, particularly regarding the use of Permuted MNIST experiments and the clarity of the RL experiments. Therefore, the sentiment score is not fully positive but leans towards the positive side. The language used is polite and respectful, even when pointing out the cons and making suggestions for improvement. The reviewer uses phrases like 'I would like to respectfully point out' and 'I was pleased to see,' which indicate a high level of politeness.",60,90
"This paper puts forward a new global+local memory pointer network to tackle task-oriented dialogue problem.

The idea of introducing global memory is novel and experimental results show its effectiveness to encode external knowledge in most cases.

Here're some comments:
1. In global memory pointer, the users employ non-normalized probability (non-softmax). What is the difference in performance if one uses softmax?

2. In (11), there's no linear weights. Will higher weights in global/local help?

3. As pointed out in ablation study, it's weird that in task5 global memory pointer does not help.

4. The main competitor of this algorithm is mem2seq. While mem2seq includes DSTC2 and In-car Assistant, and especially in-car assistant provides the first example dialogue, why does the paper not include expeirments on these two datasets?","The sentiment of the review is generally positive, as it acknowledges the novelty and effectiveness of the proposed method. However, it also raises several critical questions and points out some issues, which slightly temper the overall positive sentiment. Therefore, the sentiment score is 50. The language used in the review is neutral to slightly polite. The reviewer uses phrases like 'Here're some comments' and asks questions in a straightforward manner without being rude or overly critical. Therefore, the politeness score is 20.",50,20
"This paper reinvestigate several recent works on network pruning and find that the common belief about the necessity to train a large network before pruning may not hold. The authors find that training the pruned model from scratch can achieve similar, if not better, performance given enough time of training. Based on these observations, the author conclude that training a larger model followed by pruning is not necessary for obtaining an efficient model with similar performance. In other words, the pruned architecture is more important than the weights inherited from the large model. It reminds researchers to perform stronger baselines before showing complex pruning methods. 

The paper is well organized and written. It re-evaluate the recent progresses made on this topic. Instead of comparing approaches by simply using the numbers from previous paper, the authors perform extensive experiments to verify whether training the pruned network from scratch would work. The results are very interesting, it suggests the researchers to tune the baseline “hardly” and stick to simple approach. However, here are some places that I have concerns with:

1. The two “common beliefs” actually state one thing, that is the weights of a pre-trained larger model can potentially help optimization for a smaller model. 

2. I don’t quite agree with that “training” is the first step of a pruning pipeline as illustrated in Figure 1.  Actually the motivation or the common assumption for pruning is that there are already existing trained models (training is already finished) with good performance. If a trained model does not even exist, then one can certainly train various thin/smaller model from scratch as before, this is still a trial and error process. 

3. “The value of pruning”. The goal of pruning is to explore a “thin” or “shallower” version of it with similar accuracy while avoiding the exhaustive architecture search with heavy training processes. Thus the first value of pruning is to explore efficient architecture while avoiding heavy training. Therefore, it should be fast and efficient, ideally with no retraining or little fine-tuning. When the pruning method is too complex to implement or requires much more time than training from scratch, it could be an overkill and adds little value, especially when the performance is not better enough. Therefore, it is more informative if the authors would report the time/complexities for pruning/fine-tuning .

4. The second value of pruning lies at understand the redundancy of the model and providing insights for more efficient architecture designs. 

5. Comparing to random initialization, pruning simply provide an initialization point inherited from the larger network. The essential question the author asked is whether a subset of pre-trained weights can outperform random initialization. This seems to be a common belief in transfer learning, knowledge distillation and the studies on initialization. The authors conclude that the accuracy of an architecture is determined by the architecture itself, but not the initialization. If this is true, training from scratch should have similar (but not better) result as fine-tuning a pruned model.  As the inherited weights can also be viewed as a “random” initialization. Both methods should reach equivalent good solution if they are trained with enough number of epochs. Can this be verified with experiments?

6. The experiments might not be enough to reject the common belief. The experiments only spoke that the pruned architectures can still be easily trained and encounter no difficulties during the optimization. One conjecture is that the pruned models in the previous work still have enough capacity for keeping good accuracy. What if the models are significantly pruned (say more than 70% of channels got pruned), is training from scratch still working well? It would add much value if the author can identify when training from scratch fails to match the performance obtained by pruning and fine-tuning.

7. In Section 4.1, “scratch-trained models achieve at least the same level of accuracy as fine-tuned models”. First, the ResNet-34-pruned A/B for this comparison does not have significant FLOPs reduction (10% and 24% FLOPs reduction). Fine-tuning still has advantage as it only takes ¼ of training time compare to scratch-E. Second, it is interesting that fine-tuning has generally smaller variance than stratch-E (except VGG-19). Would this imply that fine-tuning a pruned model produce more stable result? It would be more complete if there is variance analysis for the imagenet result. 

8. What is the training/fine-tuning hyperparameters used in section 4.1?  Note that in the experiment of Li et al, 2017, scratch-E takes 164 epochs to train from scratch, while fine-tuning takes only 40 epochs. Like suggested above, if we fine-tune it with more epochs, would it achieve equivalent performance? Also, what is the hyperparameter used in scratch-E? Note that the original paper use batch size 128. If the authors adopts a smaller batch-size for scratch-E, then it has in more iterations and could certainly result in better performance according to recent belief that small batch-size generates better.

9. The conclusion of section 5 is not quite clear or novel. Using uniform pruning ratio for pruning is expected to perform worse than automatic pruning methods as it does not consider the importance difference of each layer and. This comes back to my point 3 & 4 about the value of pruning, that is the value of pruning lies at the analysis of the redundancy of the network. There are a number of works worked on analyzing the importance of different layers of filters. So I think the “hypothesis” of “the value of automatic pruning methods actually lies in the resulting architecture rather than the inherited weight” is kind of straightforward. Also, why not use FLOPs as x-axis in Figure 3?


Minor: It might be more accurate to use “L1-norm based Filter Pruning (Li et al., 2017)” as literally “channels” usually refers to feature maps, which are by-products of the model but not the model itself.

I  will revise my score if authors can address above concerns.


--------- review after rebuttal----------
#1#2 It would be great if the authors can make it clear that training is not the always the first step and the value of pruning in introduction rather than mentioning in conclusion. Saving training time is still an important factor when training from scratch is expensive. 

#5 “fine-tuning with enough epochs”. 
I understand that the authors are mainly questioning about whether training from scratch is necessarily bad than pruning and fine-tuning. The author do find that “training from scratch is better when the number of epochs is large enough”. But we see that fine-tuning ResNet-56 A/B with 20 epochs does outperform (or is equivalent to) scratch training for the first 160 epochs, which validates “fine-tuning is faster to converge”.  However, training 320 epochs (16x more comparing to 20 epochs fine-tuning and 2x comparing with normal training from scratch) is not quite coherent with the setting of “scratch B”, as ResNet-56 B just reduce 27% FLOPs. 

The other part of the question is still unclear, i.e., the author claimed that the accuracy of an architecture is determined by the architecture itself, but not the initialization, then both fine-tuning and scratch training should reach equivalent solution if they are well trained enough, regardless of the initialization or pruning method. The learning rate for scratch training is already well known (learning rate drop brings boost the accuracy). However, learning rate schedule for fine-tuning (especially for significantly pruned model as for reply#6) is not well explored. I wonder whether that a carefully tuned learning rate/hyperparameters for fine-tuning may get the same or better performance as scratch training.

Questions:
- Are both methods using the same learning rate schedule between epoch 160 and epoch 320?
- The ResNets-56 A/B results in the reply#8 does not match the reported performance in reply#5. e.g., it shows 92.67(0.09) for ResNet-56-B with 40-epochs fine-tuning in reply5,  but it turns out to be 92.68(±0.19) in reply#8.
- It would be great if the authors can add convergence curves for fine-tuning and scratch training for easier comparison.


#6 The failure case for sparse pruning on ImageNet is interesting and it would be great to have the imageNet result reported and discussed. 

The authors find that “when the pruned ratio is large enough, training from scratch is better by a even larger margin than fine-tuning”.  This could be due to following reasons: 
      1. When the pruning ratio is large, the pruned model with preserved weights is significantly different from the original model, and fine-tuning with small learning rate and limited number of epochs is not enough to recover the accuracy. As mentioned earlier, tuning the hyperparameters for fine-tuning based on pruning ratio might improve the performance of fine-tuning. 
      2. Though the pruning ratio is large, the model used in this experiment may still have large capacity to reach good performance. How about pruning ResNet-56 with significant pruning ratios? 

Finally, based on above observations, it seems to me that the preserved weights is more essential for fast fine-tuning but less useful for significant pruning ratios.

-------- update ----------------

The authors addressed most of my concerns. Some questions are still remaining in my comment “Review after rebuttal”,  specifically, fine-tuning a pruned network may still get good performance if the hyperparameters are carefully tuned based on the pruning ratios, or in other words, the preserved weights is more essential for fast fine-tuning but less useful for significant pruning ratios. The authors may need to carefully made the conclusion from the observations. I would hope the authors can address these concerns in the future version.

However, I think the paper is overall well-written and existing content is inspiring enough for readers to further explore the trainability of the pruned network. Therefore I raised my score to 7.
","The sentiment of the review is generally positive, as the reviewer acknowledges that the paper is well-organized, well-written, and makes interesting contributions to the field. The reviewer also appreciates the extensive experiments conducted by the authors. However, the reviewer has several concerns and suggestions for improvement, which are detailed in a constructive manner. The politeness of the language is high, as the reviewer uses polite phrases such as 'it would be great if,' 'I understand that,' and 'I would hope.' The reviewer also offers to revise their score if the authors address the concerns, indicating a willingness to engage in a constructive dialogue.",70,90
"This paper proposes a method to synthesize speech from text input, with the style of an input voice provided with the text. Thus, we provide content - text - and style - voice. It leverages recent - phenomenal - progress in TTS with Deep Neural Networks as seen from exemplar works such as Tacotron (and derivatives), DeepVoice, which use seq2seq RNNs and Wavenet families of models. The work is extremely relevant in that audio data is hard to generate (expensive) and content-style modeling could be useful in a number of practical areas in synthetic voice generation. It is also quite applicable in the related problem of voice conversion. The work also uses some quite complex - (and very interesting!) - proposals to abstract style, and paste with content using generative modeling. I am VERY excited by this effort in that it puts together a number of sophisticated pieces together, in what I think is a very sensible way to implement a solution to this very difficult problem. However, I would like clarifications and explanations, especially in regards to the architecture.  

Description of problem: The paper proposes a fairly elaborate setup to inject voice style (speech) into text. At train time it takes in text samples $x_{txt}$, paired voice samples (utterances that have $x_{txt}$ as content) $s+$ and unpaired voice samples $s-$, and produces two voice samples $x+$ (for paired  <txt, utterance>) and $x-$ (for unpaired txt/utterance). The idea is that at test time, we pass in a text sample $x_{txt}$ and an UNPAIRED voice sample $x_{aud}$ and the setup produces voice in the style of $x_{aud}$ but whose content is $x_{txt}$, in other words it generates synthetic speech saying $x_{txt}$. The paper goes on to show performance metrics based on an autoencoder loss, WER and t-SNE embeddings for various attributes. 

Context:  The setup seems to be built upon the earlier work by Taigman et al (2016) which has the extremely interesting conception of using a {\it ternary} discriminator loss to carry out domain adaptation between images. This previous work was prior to the seminal CycleGAN work for image translation, which many speech works have since used. Interestingly, the Taigman work also hints at a 'common' latent representation a la UNIT using coupled VAE-GANs with cycle consistency (also extremely pertinent), but done differently. In addition to the GAN framework by Taigman et al, since this work is built upon Tacotron and the GST (Global Style Tokens) work that followed it, the generative setup is a sophisticated recurrent attention based seq2seq model.

Formulation:
A conditional formulation is used wherein the content c (encoding generated by text) is passed along with other inputs in the generator and discriminator. The formulation in Taigman assumes that there is an invariant representation in both (image) domains with shared features. To this, style embeddings (audio) gets added on and then gets passed into the generator to generate the speech. Both c and s seem to be encoder outputs in the formulation. The loss components of what they call ‘adversarial’, ‘collaborative’ and ‘style’ losses. 

Adversarial losses
The ternary loss for D consists of 

Discriminator output from ‘paired’ style embedding (i.e. text matching the content of paired audio sample)
Discriminator output from ‘unpaired’ style embedding (i.e text paired with random sample of some style)
Discriminator output from target ground truth style. The paper uses x_+, so I would think that it uses the paired sample (i.e. from the source) style.

Generator loss (also analogous to Taigman et al) consists of generations from paired and unpaired audio, possibly a loose analogue to source and target domains, although in this case we can’t as such think of ‘+’ as the source domain, since the input is text. 

Collaborative losses 
This has two components, one for style (Gatys et al 2016) and a reconstruction component. The reconstruction component again has two terms, one to reconstruct the paired audio output ‘x+=x_audio+’ - so that the input content is reproduced -  and the other to encourage reconstruction of the latent code. 

Datasets and Results:
They use two datasets: one, an internal ‘EMT-4’ dataset with 20k+ English speakers, and the other, the VCTK corpus. Comparisons are made with a few good baselines in Tacotron2, GST and DeepVoice2. 

One comparison technique to test disentanglement ability is to compare autoencoder reconstructions with the idea that a setup that has learnt to disentangle would produce higher reconstruction error because it has learnt to separate style and content. 

t-SNE embeddings are presented to show visualizations of various emotion styles (neutral, angry, sad and happy), and separation of male and female voices. A WER metric is also presented so that generations are passed into a classifier (an ASR system trained on Wavenet). All the metrics above seem to compare excellently (better than?) with the others. 

Questions and clarifications:

(Minor) There’s a typo in page 2, line 2. x_{aud}^+ should be x_{aud}^-.

Clarification on formulation: Making the analogy (is that even the right way of looking at this?) that the ‘source’ domain is ‘+’, and the target domain is ‘-’, in equation (5), the last term of the ternary discriminator has the source domain (x_{aud}^+) in it, while the Taigman et al paper uses the target term. Does this matter? I would think ‘no’, because we have a large number of terms here and each individual term in and of itself might not be relevant, nor is the current work a direct translation of the Taigman et al work. Nevertheless, I would like clarification, if possible, on the discrepancy and why we use the ‘+’ samples. 

Clarification on reconstruction loss: I think the way it is presented, equation (8) is misleading. Apparently, we are sampling from the latent space of style and content embeddings for paired data. The notation seems to be quite consistent with that of the VAE, where we have a reconstruction and a recognition model, and in effect the equation (8) is sampling from the latent space in a stochastic way. However, as far as I can see, the latent space here produces deterministic embeddings, in that c = f(x_{txt}) and s = g(x_{aud}^+), with the distribution itself being a delta function. Also, the notation q used in this equation most definitely indicates a variational distribution, which I would think is misleading (unless I have misinterpreted what the style tokens mean). At any rate, it would help to show how the style token is computed and why it is not deterministic. 

Clarification on latent reconstruction loss: In equation (9), how is the latent representation ‘l’ computed? While I can intuitively see that the latent space ‘l’ (or z, in more common notation) would be the ‘same’ between real audio samples and the ‘+’, ‘-’ fake samples, it seems to me that they would be related to s (as the paper says, ‘C’ and ‘Enc_s’ share all conv layers) and the text. But what, in physical terms is it producing? Is it like the shared latent space in the UNIT work, or the invariant representation in Taigman? This could be made clearer with an block diagram for the architecture. 

(Major) Clarification on network architecture
The work references Tacotron’s GST work (Wang et al 2018) and the related Skerry-Ryan work as the stem architecture with separate networks for style embeddings and for content (text). While the architecture itself might be available in the stem work by Wang et al, I think we need some diagrams for the current work as well for a high level picture. Although it is mentioned in words in section 3.3, I do not get a clear idea of what the encoder/decoder architectures look like. I was also surprised in not seeing attention plots which are ubiquitous in this kind of work. Furthermore, in the notes to the ‘inference’ network ‘C’ it is stated that C and Enc_s share all conv layers. Again, a diagram might be helpful - this also applies for the discriminator. 

Clarification on stability/mode collapse: Could the authors clarify how easily this setup trained in this adversarial setup? 

Note on latent representation: To put the above points in perspective, a small note on what this architecture does in regards to the meaning of the latent codes would be useful. The Taigman et al 2016 paper talks about the f-constancy condition (and 'invariance'). Likewise, in the UNIT paper by Ming-Yu Liu - which is basically a set of coupled VAEs + cycle consistency losses, there is the notion of a shared latent space. A little discussion on these aspects would make the paper much more insightful to the domain adaptation practitioner.

Reference: This reference - Adversarial feature matching for text generation - (https://arxiv.org/abs/1706.03850) contains a reconstruction stream (as perhaps many other papers) and might be useful for instruction. 

Other relevant works in speech and voice conversion: This work comes to mind, using the StarGAN setup, also containing a survey of relevant approach in voice conversion. Although the current work is for TTS, I think it would be useful to include speech papers carrying out domain adaptation for other tasks.

StarGAN-VC: Non-parallel many-to-many voice conversion with star generative adversarial networks. 
https://arxiv.org/abs/1806.02169 

I would rate this paper as being acceptable if the authors clarify my concerns, and in particular, about the architecture. It is also hard to hard to assess reproducibility in a complex architecture such as this. ","The sentiment of the review is highly positive, as evidenced by the enthusiastic language used to describe the work (e.g., 'extremely relevant,' 'VERY excited,' 'phenomenal progress'). The reviewer clearly appreciates the significance and sophistication of the proposed method. However, the review also contains several requests for clarification and minor corrections, which are presented in a constructive and polite manner. The reviewer uses phrases like 'I would like clarifications,' 'it would help to show,' and 'a diagram might be helpful,' which indicate a polite and respectful tone.",90,85
"The authors propose in this paper a series of results on the approximation capabilities of neural networks based on ReLU using quantized weights. Results include upper bounds on the depth and on the number of weights needed to reach a certain approximation level given the number of distinct weights usable. The paper is clear and as far as I know the results are both new and significant. My only negative remark is about the appendix that could be clearer. In particular, I think that figure 2 obscures the proof of Proposition 1 rather than the contrary. I think it might be much clearer to give an explicit neural network approximation of x^2 for say r=2, for instance.","The review starts with a positive assessment of the paper, highlighting the clarity, novelty, and significance of the results. The only negative remark is minor and constructive, focusing on improving the clarity of the appendix. The language used is polite and constructive, suggesting improvements without being harsh or dismissive.",80,90
"The English, grammar and writing style is very good, as are the citations.
The technical quality appears to me to be very good (I am not an expert in Poincare spaces).
The authors demonstrate a good knowledge of the mathematical theory with the constructions made in Section 6.
The experimental write-up has been abbreviated.  The lexical entailment results Tables 6 and 7 are just sitting there without discussion, as far as I can see, as are the qualitative results Tables 4 and 5.   The entailment results are quite complex and really need supporting interpretation.  For instance, for Hyperlex, WN-Poincare is 0.512, above yours.
For your entailment score you say ""For simplicity, we propose dropping the dependence in μ"".  This needs more justification and discussion as it is counter-intuitive for those not expert in  Poincare spaces.
Section 6.2 presents the entailment score.  Note Nickel etal. give us a nice single formula.  You however, provide 4 paragraphs of construction from which an astute reader would then have to work on to extract your actual method.  I would prefer to see a summary algorithm given somewhere.  Perhaps you need another appendix.
RADAGRAD is discussed in Section 5, but I'd have preferred to see it discussed again in Section 8 and discussed to highlight what was indded done and the differences.  It certainly makes the paper non-reproducible.
A significant part of the theory in earlier sections is about the 50x2D method, but in experiments this doesn't seem to work as well.  Can you justify this some other how:  its much faster, its more interpretable?  Otherwise, I'm left thinking, why not delete this stuff?
The paper justifies its method with a substantial and winning comparison against vanilla GloVe.  That by itself is a substantial contribution.
But now, one is then hit with a raft of questions.  Embedding methods are popping up like daisies all over the fields of academia.  Indeed, word similarity and lexical entailment tasks themselves are proliferating too.  To me, its really unclear what one needs to achieve in the empirical section of a paper.  To make it worse, some folks use 500D, some 100D, some 50D, so results aren't always comparible.  Demonstrating one's work is state-of-the-art against all comers is a massive implementation effort.  I notice some papers now just compare against one other (e.g., Klami etal. ECML-PKDD, 2018).

My overall feeling is that this paper tries to compress too much into a small space (8 pages).
I think it really needs to be longer to present what is shown.   Moreover, I would want to see the inclusion of the work on 50x2D justified. So my criticisms are about the way the paper is written, not about the quality of the work.  
Moroever, though, one needs to consider comparisons against models other than GloVe.

Addendum:  You know, what I really love about ICLR is the effort authors make to refresh their paper and respond to reviewers.  You guys did a great job.  Really impressed.  50x2D now clarified and some of the hasty/unexplained bits fixed.","The sentiment of the review is generally positive, as the reviewer praises the English, grammar, writing style, technical quality, and the authors' knowledge of the mathematical theory. The reviewer also appreciates the substantial contribution of the method against vanilla GloVe and the effort made by the authors to refresh their paper. However, there are several criticisms regarding the presentation and the need for more justification and discussion in certain sections. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, even when pointing out areas for improvement.",70,90
"# 1. Summary
This paper presents a model for future video prediction, which integrates 3D convolutions into RNNs. The internal operations of the RNN are modified by adding historical records controlled via a gate-controlled self-attention module. The authors show that the model is effective also for other tasks such as early activity recognition.

Strengths:
* Nice extensive experimentation on video prediction and early activity recognition tasks and comparison with recent papers
* Each choice in the model definition are motivated, although some clarity is still missing (see below)

Weaknesses:
* Novelty: the proposed model is a small extension of a previous work (Wang et al., 2017) 


# 2. Clarity and Motivation
In general, the paper is clear and general motivation makes sense, however some points need to be improved with further discussion and motivation:

A) Page 2 “Unlike the conventional memory transition function, it learns the size of temporal interactions. For longer sequences, this allows attending to distant states containing salient information”: This is not obvious. Can the authors add more details and motivate these two sentences? How is long-term relations are learned given Eq. 1? 
B) Page 5 “These two terms are respectively designed for short-term and long-term video modeling”: How do you make sure that Recall(.) does not focus on the short-term modeling instead? Not clear why this should model long-term relations.
C) Page 5 and Eq 1: motivation why layer norm is required when defining C_t^k is not clear
D) What if the Recall is instead modeled as attention? The idea is to consider only C_{1:t-1}^k (not consider R_t) and have an attentional model that learn what to recall based only on C. Also, why does Recall need to depend on R_t?
E) Page 5 “to minimize the l1 + l2 loss over every pixel in the frame”: this sentence is not clear. How does it relate to Eq. 2?


# 3. Novelty
Novelty is the major concern of this paper. Although the introduced new concepts and ideas are interesting, the work seems to be an extension of ST-LSTM and PredRNN where Eq 1 is slightly modified by introducing Recall. 
In addition the existing relation between the proposed model and ST-LSTM is not clearly state. Page 2, first paragraph: here the authors should state that model is and extension of ST-LSTM and highlight what are the difference and advantage of the new model.


# 4. Significance of the work
This paper deals with an interesting and challenging topic (video prediction) as well as it shows some results on the early activity recognition task. These are definitively nice problem which are far to be solved. From the application perspective this work is significant, however from the methodological perspective it lacks a bit of significance because of the novelty issues highlighted above.


# 5. Experimentation
The experiments are robust with nice comparisons with recent methods and ablation study motivating the different components of the model (Table 1 and 2). Some suggested improvements:

A) Page 7 “Seq 1 and Seq 2 are completely irrelevant, and ahead of them, another sub-sequence called prior context is given as the input, which is exactly the same as Seq 2”: The COPY task is a bit unclear and need to be better explained. Why are Seq. 1 and 2 irrelevant? I would suggest to rephrase this part.
B) Sec. 4.2, “Dataset and setup”: which architecture has been used here?
C) Sec. 4.3, “Hyper-parameters and Baselines“: the something-something dataset is more realising that the other two “toy” dataset. Why did the authors choose to train a 2 layers 3D-CNN encoders, instead of using existing pretrained 3D CNNs? I would suspect that the results can improve quite a bit.


# 6. Others
* The term “self-supervised auxiliary learning” is introduced in the abstract, but at this point it’s meaning is not clear. I’d suggest to either remove it or explain its meaning.
* Figure 1(a): inconsistent notation with 2b. Also add citation (Wang et al., 2017) since it ie the same model of that paper

-------
# Post-discussion
I increased my rating: even if novelty is not high, the results support the incremental ideas proposed by the authors.
","The sentiment of the review is generally positive, as indicated by the acknowledgment of the extensive experimentation and the interesting and challenging topic. However, there are concerns about the novelty of the work, which slightly tempers the overall positive sentiment. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer provides constructive feedback and suggestions in a respectful and professional manner. The reviewer uses phrases like 'I would suggest' and 'Can the authors add more details,' which indicate a polite tone. Therefore, the politeness score is 80.",60,80
"The paper aims to come up with a criterion for evaluating the quality of samples produced by a Generative Adversarial Network. The main goal is that the criterion should not reward trivial sample generation algorithms such as the one which generates samples uniformly at random from the samples in the training set. I personally feel that if sample generation is the only goal, then this trivial algorithm is perfectly fine because, statistically, the empirical distribution is in many, though not all, ways, a good estimator of the underlying true probability measure (this is the idea that is used in the statistical technique of Bootstrap for example). However the underlying goal in unsupervised learning problems where GANs are used is hardly sample generation. The GANs also output a whole function in the form of a generative network which converts random samples into samples from the underlying generating distribution. This generative network is arguably more important and more useful than just the samples that it generates. An evaluation scheme for GANs should focus on the generative network directly rather than on a set of its generating samples. 

Even if one were to regard the premise of the paper as valuable, the paper still does a poor job meeting its objective. A measure D_CNN is proposed as a benchmark. It must be remarked that D_CNN is not even properly defined (for example, there is a function \Delta in its definition but it is never explained what this function is). D_CNN is a variant of the existing notion of Neural Network Divergences. Only a numerical study (with no theory) is done to illustrate the utility of D_CNN for evaluating samples generated by GANs. The entire paper is very anecdotal with very little rigorous theory. ","The sentiment of the review is largely negative. The reviewer criticizes the paper for not meeting its objective, for being poorly defined, and for lacking rigorous theory. The sentiment score is -70 because the review is critical but not entirely dismissive. The politeness score is 10 because the language, while critical, is not rude or offensive. The reviewer uses phrases like 'it must be remarked' and 'the entire paper is very anecdotal,' which are critical but not impolite.",-70,10
"The paper is nice thread, easy to follow.

The paper proposed to apply SWA (Stochastic Weight Averaging) Izmailov et al. 2018 to the semi-supervised approached based on consistency regularization. The paper first describes the related work nicely and offers a succinct explanation of two semi-supervised approaches they study. The paper then present an analysis on SGD trajectories of these 2 approaches, drawing comparisons with the supervised training and then building a case of why SWA is a valid idea to apply. The analysis section is very well described, the theoretical explanations are easy to follow and Figure 1, Figure 2 are really helpful to understand this analysis. 

Overall, the paper offers a useful insight into semi-supervised model trainings and offers recipe of converging to supervised results which is a valid contribution.

I have following questions to the authors:
1. Did the authors do the analysis and apply SWA on ImageNet training besides Cifar-10 and Cifar-100
2. The accuracy number reported in abstract (5.0% error) is top-1 error or top-5 error? I think it's top-5 but explicit mention would be great.
3. In section 3.2, authors offer an analysis by chosing epoch 170, 180. How are these epochs chosen?
4. In section 3.1, authors consider a simple model version where only small additive perturbations to student inputs are applied. Is this a practical setup i.e. is this ever the case in actual model training?
5. In section 3.3, pg 6, do authors have intuition into why weight averaging has better improvement (1.18) vs ensembling (0.94)?
6. In section 5.2, page 8 , can authors provide their intuition behind the results: ""We found that the improvement on VAT is not drastic – our base implementation obtains 11.26% error where fast-SWA reduces it to 10.97%"" - why did fast-SWA not improve much?","The sentiment of the review is quite positive, as indicated by phrases like 'nice thread,' 'easy to follow,' 'nicely,' 'succinct explanation,' 'very well described,' and 'useful insight.' The reviewer appreciates the paper's contributions and clarity. Therefore, the sentiment score is high. The politeness of the language is also very high; the reviewer uses polite language throughout, such as 'I have following questions to the authors' and 'explicit mention would be great,' indicating a respectful and constructive tone.",90,95
"The paper presents a methodology for improved program synthesis by generating datasets for program induction and synthetic tasks from uniform distributions. This method is evaluate on two problem settings. 

The methodology is presented in section 3. Even though the outline does not seem to be complicated, the presentation in section 3 leaves me puzzled. The the second paragraph two sets of silent variables are introduced X_1,...,X_n and Z_1,...,Z_m but never used again the rest of the paper. In the third and forth paragraph details about the Karel domain are presented without the Karel domain having been introduced. It seems you are using rejection sampling to sample from a uniform distribution. Why can you not sample from a uniform distribution directly? What do you mean with the notation X(s)? What are you proving in Appendix? Would maybe be clearer if you presented it as a theorem/lemma.

The remaining part of the paper evaluates this methodology on two specific problem settings, the Karel domain and Calculator domain. The generalization performance is increased when trained on datasets generated by the method presented in the paper. However, I cannot find and strong arguments in the paper why this property should generalize to other problem settings. To me the analysis and experimental results seems to be tailored to the two problems settings used in the paper.

==== After revision ====

The authors have done a great job addressing the concerns I had about the clarity. Consequently, I have raised my score, whereas my fairly low confidence still remains.","The sentiment of the review appears to be slightly positive, especially after the revision, as the reviewer acknowledges the improvements made by the authors. However, the initial part of the review contains several critical points and questions, indicating some dissatisfaction with the original submission. Therefore, the sentiment score is 20. The politeness of the language is generally neutral to slightly polite. The reviewer uses phrases like 'leaves me puzzled' and 'I cannot find any strong arguments,' which are direct but not rude. The final comment 'The authors have done a great job' is positive and polite. Therefore, the politeness score is 30.",20,30
"1) Summary
This paper presents a graph neural network based architecture that is trained to locate and model the interactions of agents in an environment directly from pixels. They propose an architecture that is a composition of recurrent neural networks where each models a single object independently and communicate with other for the overall environment modeling. The model is trained with a variational recurrent neural network objective that allows for stochasticity in the predictions while at the same time allows to model the current and future steps simultaneously. In experiments, they show the advantage of using the proposed model for tasks of tracking as well as forecasting of agents locations.



2) Pros:
+ Novel recurrent neural network architecture to model structured dynamics of agents in an environment.
+ Outperforms baseline methods.
+ New dataset for partially observable prediction research.

3) Cons:

Forecasting task:
- The authors argue that a discretization needs to be performed because of the many possible futures given the past, and also provide an error measure based on likelihood. However, if trajectories are actually generated from these distributions, I suspect the many possible futures generated will be very shaky. Can the authors provide trajectories sampled from this? If sampling trajectories does not make sense somehow, can the authors comment on how we can sample multiple trajectories?

Lack of baselines:
- The authors mention social LSTM and social GAN in the related work, however, no comparison is provided. From a quick glance, the authors of these papers work on trajectories. However, the “social” principle in those papers is general since it’s done from the computed feature vector. Could it have not been used on top of one of the baselines? If not, could the authors provide a reason why this is not the case?


Additional comments:
As the authors mention, it would be nice to extend this paper to an unsupervised or semi-supervised task. Here are a couple of papers that may interest you:
https://arxiv.org/abs/1804.04412
https://arxiv.org/abs/1705.02193
https://arxiv.org/abs/1806.07823

4) Conclusion
Overall, the paper is well written, easy to understand, and seems to be simple enough to quickly reproduce. Additionally, the proposed dataset may be of use for the community. If the authors are able to successfully address the issues mentioned, I am willing to improve my score.","The sentiment of the review is generally positive, as indicated by the praise for the novel architecture, outperforming baseline methods, and the introduction of a new dataset. However, there are some critical points raised, particularly regarding the forecasting task and lack of baselines, which slightly temper the overall positivity. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer uses polite language, provides constructive feedback, and even offers additional resources for improvement. The reviewer also expresses willingness to improve their score if the issues are addressed, indicating a collaborative and supportive tone. Thus, the politeness score is 90.",60,90
"The paper is very well written. The proposed approach is appropriate on modeling the node representations when the two types of events happen in the dynamic networks. Authors also clearly discussed the relevance and difference to related work. Experimental results show that the presented method outperforms the other baselines.
Overall, it is a high-quality paper. 
There are only some minor comments for improving the paper:
ν	Page 6, there is a typo. “for node v by employing …”  should be “for node u”
ν	Page 6, “Both GAT and GaAN has”   should be  “Both GAT and GaAN have”
ν	In section 5.1, it will be great if authors can explain more what are the “association events” and “communication events” with more details in these two evaluation datasets.
","The review is highly positive, praising the paper's writing quality, the appropriateness of the proposed approach, and the clarity of the discussion on related work. The reviewer also commends the experimental results. The minor comments provided are constructive and aimed at improving the paper further. The language used is polite and respectful, indicating a positive and professional tone.",90,100
"Paper summary: 

This paper presents a deep-learning based method for super-resolving low-resolution labels into high-resolution labels given the joint distribution between those low- and high- resolution labels. This is useful for many semantic segmentation tasks where high-resolution ground truth data is hard and expensive to collect. Its main contribution is a novel loss function that allows to minimize the distance between the distribution determined by a set of model outputs and the corresponding distribution given by low-resolution label over the same set of outputs. The paper also thoroughly evaluates the proposed method for two main tasks, the first being a land cover mapping task and the second being a medical imaging problem.

For the land cover application, adding low-resolution data to high-resolution data worsens the results when evaluating on the geographic area from which the high-resolution data was taken. However, when testing the model on new geographic areas and only adding the low-resolution data from this new area in training makes significant improvements.

Generally the paper is very well written, well structured, all explanations are clear, examples and figures are presented when needed and convey helpful information for the reader. The overall idea is fun, original, useful (especially in remote sensing) and is presented in a a convincing way. All major claims are supported by experimental evaluation. There are nevertheless a few concerns:

Major Concerns:

On a conceptual level, the main concern is that the paper assumes we are given a joint distribution of low and high resolution labels, “where we are given the joint distribution P(Y,Z)”, which seems the main limitation of this method. In fact, to correctly estimat this joint distribution either requires additional knowledge about low-resolution data such as the example presented on the NCLD data : “For instance, the “Developed, Medium Intensity” class [...] of the coarse classes”, or it requires actual high-resolution labelled data to correctly estimate this joint distribution. I think the paper would greatly benefit from including a section that discusses the impact of this limitation.

Another point is footnote 3 on page 5. This argument is valid but it would be more convincing to give a thorough explanation on why the choice of the presented loss function is better compared to the KL divergence based loss function or at least some evidence that the two perform similarly when evaluating the method.

Minor Concerns: 

-	“such as CRFs or iterative evaluation” I would include a citation on this type of work.
-	Format of some references in the text need to be corrected, e.g. “into different land cover classes Demir et al. (2018); Kuo et al. (2018); Davydow et al. (2018); Tian et al. (2018).” 

","The sentiment of the review is generally positive, as indicated by phrases like 'very well written,' 'well structured,' 'clear explanations,' 'fun, original, useful,' and 'presented in a convincing way.' The reviewer acknowledges the paper's strengths and contributions before mentioning concerns, which are framed constructively. The politeness of the language is high, as the reviewer uses polite and constructive language throughout, such as 'I think the paper would greatly benefit' and 'it would be more convincing to give a thorough explanation.' The reviewer also uses polite suggestions for minor concerns, indicating a respectful tone.",80,90
"This paper introduces a new framework to interactively interact document retriever and reader for open-domain question answering. While retriever-reader framework was often used for open-domain QA, this bi-directional interaction between the retriever and the reader is novel and effective because
1) If the retriever fails to retrieve the right document at the first step, the reader can give a signal to the retriever so that the retriever can recover its mistake at the next step
2) The idea of `reader state` from the reader to the retriever is new
3) The retriever use question-independent representation of paragraphs, which does not require different representation depending on the question and makes the framework easily scalable.

Strengths
1) The idea of multi-step & bi-directional interaction between the retriever and the reader is novel enough (as mentioned above). The paper contains enough literature studies on existing retriever-reader framework in open-domain setting, and clearly demonstrates how their framework is different from them.
2) The authors run the experiments on 4 different dataset, which supports the argument about the framework’s effectiveness.

Weakness
1) The authors seem to highlight multi-step `reasoning`, while it is not `reasoning` in my opinion. Multi-step reasoning refers to the task which you need evidence from different documents, and/or you need to find first evident to find the second evidence from a different document. I don’t think the dataset here are not multi-step reasoning dataset, and the authors seem not to claim it either. Therefore, I recommend using another term (maybe `multi-step interaction`?) instead of `multi-step reasoning`.
2) While the idea of multi-step interaction and how it benefits the overall performance is interesting, the analysis is not enough. Figure 3 in the paper does not have enough description — for example, I got the left example means step 2 recovers the mistake from step 1, but what does the right example mean?

Questions on result comparison
1) On TriviaQA (both open and full), the authors mentioned the result is on hidden test set — did you submit it to the leaderboard? I don’t see the same numbers on the TriviaQA leaderboard. Also, the authors claim they are SOTA on TriviaQA, but there are higher numbers on the leaderboard (which are submitted prior to the ICLR deadline).
2) There are other published papers with higher result on Quasar-T, SearchQA and TriviaQA (such as https://aclanthology.info/papers/P18-1161/p18-1161 and https://arxiv.org/abs/1805.08092) which the authors did not compare with.
3) In Section 4.2, is there a reason for the specific comparison to AQA (5th line), though AQA is not SOTA on SearchQA? I don’t think it means latent space is better than natural language space. They are totally different model and the only intersection is they contains interaction between two submodules.
4) In Section 5, the authors mentioned their framework outperforms previous SOTA by 15% margin on TriviaQA, but what is that? I don’t see 15% margin in Table 2.

Marginal comments:
1) If I understood correctly, `TriviaQA-open` and `TriviaQA-full` in the paper are officially called `TriviaQA-full` and `open-domain TriviaQA`. How about changing the term for readers to better understand the task? Also, in Section 4, the authors said TriviaQA-open is larger than web/wiki setting, but to my knowledge, this setting is part of the wiki setting.
2) It would be great if the authors make the capitalization consistent. e.g. EM, Quasar-T, BiDAF. Also, the authors can use EM instead of `exact match` after they mentioned EM refers to exact match in Section 4.2.

Overall comment
The idea in the paper is interesting, and their model and experiments are concrete. My only worries is that the terms in the paper are confusing and performance comparison are weak. I would like to update the score when the authors update the paper.


Update 11/27/2018
Thanks for the authors for updating the paper. The updated paper have more clear comparisons with other models, with more & stronger experiments with the additional dataset. Also, the model is claimed to perform multi-step interaction rather than multi-step reasoning, which clearly resolves my initial concern. The analysis, especially ablations in varying number of iterations, was helpful to understand how their framework benefits. I believe these make the paper stronger along with its initial novelty in the framework. In this regard, I vote for acceptance.","The sentiment of the review is generally positive, as the reviewer acknowledges the novelty and effectiveness of the proposed framework and appreciates the thorough literature review and experimental validation. However, there are some concerns about terminology and performance comparisons, which are addressed in the updated version of the paper. The politeness of the language is high, as the reviewer provides constructive feedback and recommendations in a respectful and professional manner.",80,90
"The authors design a program synthesizer that tries to satisfy per-instance specific syntactic and functional constraints,
based on sampling trajectories from an RL agent that at each time-step expands a partial-program.

The agent is trained with policy gradients with a reward shaped as the ratio of input/output examples that the synthesized program satisfies.

With the 'out-of-box' evaluation, the authors show that their agent can explore more efficiently the harder problems than their non-learning alternatives even from scratch.
(My intuition is that the agent learns to generate the most promising programs)
It would be good to have a Monte Carlo Tree Search baseline on the'out-of-box' evaluation, to detect exploration exploitation trade-offs.

The authors show with the 'meta-solver' approach that the agent can generalize to and also speed up unseen (albeit easy-ish in the authors words) instances.

Clarity: Paper is clear and nicely written.

Significance: Imagine a single program synthesizer that could generate C++/Java/Python/DSLs  programs and learn from all its successes and failures! This is a step towards that.

Pros:
+ Generating spec-following programs for different grammars.
+ partial tree expansion takes care of syntactic constraints.
Neutral
· The grammar and specification diversity may be too low to feel impressive.
· It would have been nicer by computing likelihood for unseen instances with unique and known solutions (that is, without finetuning).
Cons:
- No Tree Search baseline.
- No results on programs with control flow/internal state.","The sentiment of the review is generally positive, as the reviewer acknowledges the clear and nicely written paper, the significance of the work, and the pros of the approach. However, there are some neutral and negative points mentioned, such as the lack of a Tree Search baseline and results on programs with control flow/internal state. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, even when pointing out the cons and areas for improvement.",70,90
"In many machine learning applications, sorting is an important step such as ranking. However, the sorting operator is not differentiable with respect to its inputs. The main idea of the paper is to introduce a continuous relaxation of the sorting operator in order to construct an end-to-end gradient-based optimization. This relaxation is introduced as \hat{P}_{sort(s)} (see Equation 4). The paper also introduces a stochastic extension of its method 
using Placket-Luce distributions and Monte Carlo. Finally, the introduced deterministic and stochastic methods are evaluated experimentally in 3 different applications: 1. sorting handwritten numbers, 2. Quantile regression, and 3. End-to-end differentiable k-Nearest Neighbors.

The introduction of the differentiable approximation of the sorting operator is interesting and seems novel. However, the paper is not well-written and it is hard to follow the paper especially form Section 4 and on. It is not clear how the theoretical results in Section 3 and 4 are used for the experiments in Section 6. For instance:
** In page 4, what is ""s"" in the machine learning application?
** In page 4, in Equation 6, what are theta, s, L and f exactly in our machine learning applications?

Remark: 
** The phrase ""Sorting Networks"" in the title of the paper is confusing. This term typically refers to a network of comparators applied to a set of N wires (See e.g. [1])
** Page 2 -- Section 2 PRELIMINARIES -- It seems that sort(s) must be [1,4,2,3].

[1] Ajtai M, Komlós J, Szemerédi E. An 0 (n log n) sorting network. InProceedings of the fifteenth annual ACM symposium on Theory of computing 1983 Dec 1 (pp. 1-9). ACM
","The review starts by acknowledging the novelty and interest of the paper's main idea, which is a positive sentiment. However, it quickly shifts to criticism regarding the clarity and writing quality of the paper, particularly from Section 4 onwards. Specific issues are pointed out, such as unclear definitions and confusing terminology. The language used is direct but not impolite, offering constructive feedback and specific examples of where improvements are needed. Therefore, the sentiment score is slightly negative due to the criticisms, and the politeness score is positive as the feedback is given in a constructive manner.",-20,60
"The paper proposes a method for neural network training under a hard energy constraint (i.e. the method guarantees the energy consumption to be upper bounded). Based on a systolic array hardware architecture the authors model the energy consumption of transferring the weights and activations into different levels of memory (DRAM, Cache, register file) during inference. The energy consumption is therefore determined by the number of nonzero elements in the weight and activation tensors. To minimize the network loss under an energy constraint, the authors develop a training framework including a novel greedy algorithm to compute the projection of the weight tensors to the energy constraint.

Pros:

The proposed method allows to accurately impose an energy constraint (in terms of the proposed model), in contrast to previous methods, and also yields a higher accuracy than these on some data sets. The proposed solution seems sound (although I did not check the proofs in detail, and I am not very familiar with hardware energy consumption subtleties).

Questions:

The experiments in Sec. 6.2 suggest that the activation mask is mainly beneficial when the data is highly structured. How are the benefits (in terms of weight and activation sparsity) composed in the experiments on Imagenet? How does the weight sparsity of the the proposed method compare to the related methods in these experiments? Is weight sparsity in these cases a good proxy for energy consumption?

How does the activation sparsity (decay) parameter (\delta) q affect the accuracy-energy consumption tradeoff for the two data sets?

The authors show that the weight projection problem can be solved efficiently. How does the guarantee translate into wall-clock time?

Filter pruning methods [1,2] reduce both the size of the weight and activation tensors, while not requiring to solve a complicated projection problem or introducing activation masks. It would be good to compare to these methods, or at least comment on the gains to be expected under the proposed energy consumption model.

Knowledge distillation has previously been observed to be quite helpful when constraining neural network weights to be quantized and/or sparse, see [3,4,5]. It might be worth mentioning this.

Minor comments:
- Sec. 3.4. 1st paragraph: subscript -> superscript
- Sec. 6.2 first paragraph: pattens -> patterns, aliened -> aligned

[1] He, Y., Zhang, X., & Sun, J. (2017). Channel pruning for accelerating very deep neural networks. ICCV 2017.
[2] Li, H., Kadav, A., Durdanovic, I., Samet, H., & Graf, H. P. Pruning filters for efficient convnets. ICLR 2017.
[3] Mishra, A., & Marr, D. Apprentice: Using knowledge distillation techniques to improve low-precision network accuracy. ICLR 2018.
[4] Tschannen, M., Khanna, A., & Anandkumar, A. StrassenNets: Deep learning with a multiplication budget. ICML 2018.
[5] Zhuang, B., Shen, C., Tan, M., Liu, L., & Reid, I. Towards effective low-bitwidth convolutional neural networks. CVPR 2018.","The sentiment of the review is generally positive, as the reviewer acknowledges the novelty and potential benefits of the proposed method. However, the reviewer also raises several questions and suggests comparisons with other methods, indicating a critical but constructive approach. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer uses polite and respectful language throughout the review, even when pointing out areas for improvement. Therefore, the politeness score is 90.",60,90
"** Summary **
The authors propose a new training scheme with a learned auxiliary reward function to optimise transition policies, i.e. policies that connect the ending state of a previous macro action/option with good initiation states of the following macro action/option.

** Quality & Clarity **
The paper is well written and features an extensive set of experiments.

** Originality **
I am not aware of similar work and believe the idea is novel.

** Significance **
Several recent papers have proposed to approach the topic of learning hierarchical policies not by training the hierarchy end-to-end, but by first learning useful individual behavioural patterns (e.g. skills) which then later can be used and sequentially chained together by higher-level policies. I believe the here presented work can be quite helpful to do so as the individual skills are not optimised for smooth composition and are therefore likely to fail when naively used sequentially.","The review is generally positive, highlighting the novelty and potential significance of the work. The reviewer appreciates the quality and clarity of the writing and the extensive set of experiments. The language used is polite and constructive, aiming to provide helpful feedback to the authors.",80,90
"This is a paper that communicates a large scale experiment on human object/semantic representations and a model of such representations.   The experiment could have been more carefully controlled (and described in the paper) and the modeling work is inconclusive.

Quality, 
The experiment design is conventional, based on rating pair-wise similarity among triplets. Compared to earlier experiments, this data has more objects and more triplets.  Additional control experiments on smaller subsets have been carried out to further address hypotheses.  The description of the experiment could have been more careful: What are the precise instructions, how are the object/images presented (it is well known that relative positions, asymmetry, etc can play an important role), are there any temporal/learning effects (how clear is the task to the workers?).
The modeling work is basic and contains a number of steps that have unknown influence on the final outcome. For example model dimension: Is you claim that ""D=49"" is a law of human nature?  Model predictive performance seems excellent, that is interesting! But we do not know how robust this is to the many heuristics

Clarity, 
The presentation of the inference process is clear. Not so clear what the uncertainties are

Originality 
Limited. Mainly related to scale. But the data quality is unclear. The modeling approach involves a number of untested heuristics (non-negative, exponentiation etc). 

Significance 
Mostly related to the data.  I did not understand if it is planned to release the data.

Pros and cons 

+Large scale experiment
+simple model, seem to have good accuracy

-experiment needs more careful description
-too many heuristics in model and inference, unclear how general the conclusions are

Other comments:
References have many issues

The authors have done a good job in the revision and have clarified points that were unclear in the first version. 
I have remaining reservations on significance, but move rating up a notch to reflect the extensive improvements and  the authors' confirmation that they will release the data.


","The sentiment of the review is mixed but leans towards positive due to the acknowledgment of improvements and the decision to move the rating up. The initial critique is balanced with positive remarks about the scale of the experiment and the model's accuracy. Therefore, the sentiment score is 20. The politeness score is high as the reviewer uses polite language, acknowledges the authors' efforts, and provides constructive feedback without being rude. Thus, the politeness score is 80.",20,80
"The algorithm described in this paper is part of the one-shot family of architecture search algorithms. In practice this means training an over-parameterized architecture, of which the architectures being searched for are sub-graphs. Once this bigger network is trained it is pruned into the desired sub-graph. The algorithm is similar to DARTS in that it it has weights that determine how important the various possible nodes are, but the interpretation here is stochastic, in that the weight indicates the probability of the component being active. Two methods to train those weights are being suggested, using REINFORCE and using BinaryConnect, both having different trade offs.

- (minor) *cumbersome* network seems the wrong term, maybe over-parameterized network?
- (minor) I do not think that the size of the search space a very meaningful metric

Pros:
- Good exposition
- Interesting and fairly elegant idea
- Good experimental results

Cons
- tested on a limited amount of settings, for something that claims that helps to automate the creation of architecture. I think this is the main shortcoming, although shared by many NAS papers
- No source code available

Some typos:

- Fo example, when proxy strategy -> Fo*r* example
- normal training in following ways. -> in *the* following ways
- we can then derive optimized compact architecture.","The sentiment of the review is generally positive, as indicated by the pros listed (good exposition, interesting idea, good experimental results) and the constructive nature of the cons (limited settings tested, no source code available). The sentiment score is therefore 70. The language used is polite, with suggestions framed in a constructive manner and minor issues pointed out without harsh criticism. The politeness score is 80.",70,80
"The paper presents a novel end-to-end mechanistic generative model of electron flow in a particular type of chemical reaction (“Linear Electron Flow” reactions) . Interestingly, modeling the flow of electrons aids in the prediction of the final product of a chemical reaction over and above problems which attack this “product prediction problem” directly. The method is also shown to generalize well to held-out reactions (e.g. from a chemistry textbook).

General Impressions

+ For me the biggest selling point is that it improves performance in predicting the ultimate reaction outcome. It should do because it provides strictly more supervision, but it’s great that it actually does. 
+ Because it models the reaction mechanism the model is interpretable, and it’s possible to enforce constraints, e.g. that dynamics are physically possible.
+ Generalises outside of the dataset to textbook problems :-)
+ Well-founded modeling choices and neural network architectures.
- Only applies to a very particular type of reaction (heterolytic LEF). 
- Requires supervision on the level of electron paths. This seems to inhibit applying the model to more datasets or extending it to other types of reactions.
- Furthermore the supervision extraction does not seem take advantage of symmetries noted in the section(s) about difficulty evaluating inference. 
- It would be nice to margin out the electron flow model and just maximize the marginal likelihood for the product prediction problem.

Novelty
I’m not an expert on the literature of applying machine learning to the problems of reaction {product, mechanism} prediction but the paper appears to conduct a thorough review of the relevant methods and occupy new territory in terms of the modeling strategy while improving over SOTA performance.

Clarity
The writing/exposition is in general extremely clear. Nicely done. There are some suggestions/questions which I think if addressed would improve clarity.

Ways to improve the paper
1. Better motivate the use of machine learning on this problem. What are the limitations of the arrow-pushing models? 

2. Explain more about the Linear Electron Flow reactions, especially:
- Why does the work only consider “heterolytic” LEF reactions, what other types of LEF reactions are omitted?
- Is the main blocker to extending the model on the modeling front or the difficulties of extracting ground-truth targets? It appears to be the latter but this could be made more clear. Also that seems to be a pretty severe limitation to making the algorithm more general. Could you comment on this?

Questions
1. Is splitting up the electron movement model into bond “removal” and “addition” steps just a matter of parameterization or is that physically how the movements work? 

2. It appears that Jin et al reports Top 6/8/10 whereas this work reports Top 1/3/5 accuracy on the USPTO dataset. It would be nice if there was overlap :-). Do your Top 6/8/10 results with the WLDN model agree with the Jin et al paper?


Nits
Section 2.3, first paragraph “...(LEF) topology is by far the most important”: Could you briefly say why? It’s already noted that they’re the most common in the database. Why?

Section 3.ElectionMovement, first paragraph. “Observer that since LEF reactions are a single path of electrons…”. Actually, it’s not super clear what this means from the brief description of LEF. Can you explain these reactions in slightly more detail?

Section 3.ElectionMovement, second paragraph. “Differently, the above distribution can be split…”. Awkward phrasing. How about “In contrast, the above distribution can be split…”. 

Section 3.Training, last sentence “...minibatches of size one reaction”. Slightly awkward phrasing. Maybe “...minibatches consisting of a single reaction”?

Section 5.2, second sentence. “However, underestimates the model’s actual predictive accuracy…”. It looks like a word accidentally got deleted here or something.

Section 5.2, paragraph 4. “To evaluate if our model predicts the same major project”... Did you mean “the same major product”?

","The sentiment of the review is generally positive, as indicated by the praise for the novel approach, the interpretability of the model, and the clear writing. However, there are some criticisms regarding the limitations of the model's applicability and the need for specific supervision. The sentiment score is therefore 60, reflecting a positive but balanced view. The politeness of the language is very high, as the reviewer uses polite and constructive language throughout, even when pointing out areas for improvement. The politeness score is 90.",60,90
"This paper presents a methodology to infer shape programs that can describe 3D objects. The key intuition of the shape programs is to integrate bottom-up low-level feature recognition with symbolic high-level program structure, which allows the shape programs to capture both high-level structure and the low-level geometry of the shapes. The paper proposes a domain-specific language for 3D shapes that consists of “For” loops for capturing high-level regularity, and associates objects with both their geometric and semantic attributes. It then proposes an end-to-end differentiable architecture to learn such 3D programs from shapes using an interesting self-supervised mechanism. The neural program generator proposes a program in the DSL that is executed by a neural program execution module to render the corresponding output shape, which is then compared with the original shape and the difference loss is back-propagated to improve the program distribution. The technique is evaluated on both synthetic and ShapeNet tasks, and leads to significant improvements compared to Tulsiani et al. that embed a prior structure on learning shape representations as a composition of primitive abstractions. In addition, the technique is also paired with MarrNet to allow for a better 3D reconstruction from 2D images.

Overall, this paper presents an elegant idea to describe 3D shapes as a DSL program that captures both geometric and spatial abstractions, and at the same time captures regularities using loops. CSGNet [Sharma et al. 2018] also uses programs to describe 2D and 3D shapes, but the DSL used here is richer as it captures more high-level regularities using loops and also semantic relationships such as top, support etc. The idea of training a neural program executor and using it for self-supervised training is quite elegant. I also liked the idea of guided adaption to make the program generator generalize beyond the synthetic template programs. Finally, the results show impressive improvements and generalization capability of the model.

Can the authors comment on some notion of completeness of the proposed DSL? In other words, is this the only set of operators, shapes, and semantics needed to represent all of ShapeNet objects? Also, it might be interesting to comment more on how this particular DSL was derived. Some of the semantics operator such as “Support”, “Locker”, etc. look overly specific to chair and tables. Is there a way to possibly learn such abstractions automatically?

What is the total search space of programs in this DSL? How would a naive random search perform in this synthesis task?

I also particularly liked the decomposition of programs into draw and compound statements, and the corresponding program generator decomposition into 2 steps BlockLSTM and StepLSTM. At inference time, does the model use some form of beam search to sample block programs or are the results corresponding to top-1 prediction?

Would it be possible to compare the results to the technique presented in CSGNet [Sharma et al. 2018]?  There are some key differences in terms of using lower-level DSL primitives and using REINFORCE for training the program generator, but it would be good to measure how well having higher-level primitives improve the results.

I presume the neural program executor module was trained using a manually-written shape program interpreter. How difficult is it to write such an interpreter? Also, how easy/difficult is to extend the DSL with new semantics operator and then write the corresponding interpreter extension?

Minor typos:
page 3: consists a variable → consists of a variable
page 5: We executes → We execute
page 6: synthetica dataset → synthetic dataset
","The sentiment of the review is highly positive, as evidenced by the use of words like 'elegant idea,' 'impressive improvements,' and 'generalization capability.' The reviewer appreciates the methodology, the results, and the innovative aspects of the paper. Therefore, the sentiment score is 90. The politeness of the language is also very high. The reviewer uses polite phrases such as 'I also liked,' 'Can the authors comment,' and 'Would it be possible,' which indicate a respectful and constructive tone. Therefore, the politeness score is 95.",90,95
"This paper describes a method for improving the (sequence-length) scalability of the Transformer architecture, with applications to modeling long-range interactions in musical sequences. The proposed improvement is applied to both global and local relative attention formulations of self-attention, and consists of a clever re-use (and re-shaping) of intermediate calculations. The result shaves a factor of L (sequence length) from the (relative) memory consumption, facilitating efficient training of long sequences. The method is evaluated on MIDI(-like) data of Bach chorales and piano performances, and compares favorably to prior work in terms of perplexity and a human listener evaluation.

The results in this paper seem promising, though difficult to interpret.  The quantitative evaluation consists of perplexity
scores (Tables 2 and 3), and the qualitative listening study is analyzed by pairwise comparisons between methods. While the proposed method achieves the highest win-rate in the listening study, other results in the study (LSTM vs Transformer) run contrary to the ranking given by the perplexity scores in Table 3. This immediately raises the question of how perceptually relevant the (small) differences in perplexity might be, which in turn clouds the overall interpretation of the results. Of course, perplexity is not the whole story here: the focus of the paper seems to be on efficiency, not necessarily accuracy, but one might expect improved efficiency to afford higher model capacity and improve on accuracy.


The core contributions of this work are described in sections 3.4 and 3.5, and while I get the general flavor of the idea, I find the exposition here both terse and difficult to follow. Figures 1 and 2 should illustrate the core concept, but they lack axis labels (and generally sufficient detail to decode properly), and seem to use the opposite color schemes from each-other to convey the same ideas.  Concrete image maps using real data (internal feature activations) may have been easier to read here, along with an equation that describes how the array indices map after skewing.

The description in 3.4 of the improved memory enhancement is also somewhat difficult to follow.  The claim is a reduction from O(DL^2) to O(DL), but table 1 lists this as O(DL^2) to O(DL + L^2).  In general, I would expect L to dominate D, which still leaves the memory usage in quadratic space, so it's not clear how or why this constitutes an improvement. The improvement due to moving from global to local attention is clear, but this does not appear to be a contribution of this work.","The sentiment of the review is mixed. The reviewer acknowledges the promising nature of the results and the cleverness of the proposed method, which suggests a positive sentiment. However, the reviewer also points out significant issues with the interpretation of results and the clarity of the exposition, which introduces a negative sentiment. Overall, the sentiment score is slightly positive. The language used in the review is polite and constructive, with the reviewer providing specific feedback and suggestions for improvement without being rude or dismissive.",20,80
"After the rebuttal and the authors providing newer experimental results, I've increased my score. They have addressed both the issue with the phrasing of the auxiliary loss, which I'm very happy they did as well as provided more solid experimental results, which in my opinion make the paper strong enough for publication. 

#####
The paper proposes a variational framework for learning a Model of both the environment and the actor's policy in Reinforcement Learning. Specifically, the model is a deterministic RNN which at every step takes as input also a new stochastic latent variable z_t. Compared to more standard approaches, the prior over z_t is not standard normal but depends on the previously hidden state. The inference model combines information from the forward generative hidden state and a backward RNN that looks only at future observations. Finally, an auxiliary loss is added to the model that tries to predict the future states of the backward RNN using the latent variable z_t.  The idea of the paper is quite well presented and concise. 

The paper tests the proposed framework on several RL benchmarks. Using it for imitation learning outperforms two baseline models: behaviour cloning and behaviour cloning trained with an auxiliary loss of predicting the next observation. Although the results are good, it would have been much better if there was also a comparison against a Generative model (identical to the one proposed) without the auxiliary loss added? The authors claim that the results of the experiment suggest that the auxiliary loss is indeed helping, where I find the evidence unconvincing given that there is no comparison against this obvious baseline. Extra comparison against the method from [1] or GAIL would make the results even stronger, but it is understandable that one can not compare against everything, hence I do not see this as a major issue. 
The authors also compare on long-horizon video prediction. Although their method outperforms the method proposed in Ha & Schmidhuber, this by no means suggests that the method is really that superior. I would argue that in terms of future video prediction that [3] provides significantly better results than the World Models, nevertheless, at least one more baseline would have supported the authors claims much better. 
On the Model-Based planning, the authors outperform SeCTAR model on the BabyAI tasks and the Wheeled locomotion. This result is indeed interesting and shows that the method is viable for planning. However, given that similar result has been shown in [1] regarding the planning framework it is unclear how novel the result is. 

In conclusion, the paper presents a generative model for training a model-based approach with an auxiliary loss. The results look promising, however, stronger baselines and better ablation of how do different components actually contribute would make the paper significantly stronger than it is at the moment. Below are a few further comments on some specific parts of the paper. 

A few comments regarding relevant literature: 

Both in the introduction and during the main text the authors have not cited [1] which I think is a very closely related method. In this work similarly, a generative model of future segments is learned using a variational framework. In addition, the MPC procedure that the authors present in this paper is not novel, but has already been proposed and tried in [1] - optimizing over the latent variables rather than the actions directly, and there have been named Latent Action Priors. 

The data gathering process is also not a new idea and using the error in a dynamics model for exploration is a well-known method, usually referred to as curiosity, for instance see [2] and some of the cited papers as Pathak et. al., Stadie et. al. - these all should be at least cited in section 3.2.2 as well not only in the background section regarding different topics. 


On the auxiliary loss:

The authors claim that they train the auxiliary loss using Variational Inference, yet they drop the KL term, which is ""kinda"" an important feature of VI. Auxiliary losses are well understood that often help in RL, hence there is no need to over-conceptualize the idea of adding the extra term log p(b|z) as a VI and then doing something else. It would be much more clear and concise just to introduce it as an extra term and motivate it without referring to the VI framework, which the authors do not use for it (they still use it for the main generative model). The only way that this would have been acceptable if the experiment section contained experiments with the full VI objective as equation (6) suggest and without the sharing of the variational priors and posteriors and compared them against what they have done in the current version of the manuscript. 


A minor mistake seems to be that equation (5) and (7) have double counted log p(z_t|h_t-1) since they are written as an explicit term as well as they appear in the KL(q(z_t|..)|p(z_t|h_t-1)). 



[1] Prediction and Control with Temporal Segment Models [Nikhil Mishra, Pieter Abbeel, Igor Mordatch, 2017]

[2] Large-Scale Study of Curiosity-Driven Learning [Yuri Burda, Harri Edwards, Deepak Pathak, Amos Storkey, Trevor Darrell, Alexei A. Efros, 2018]

[3] Action-Conditional Video Prediction using Deep Networks in Atari Games [Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard Lewis, Satinder Singh, 2015]
","The sentiment of the review is generally positive, as the reviewer mentions being happy with the addressed issues and considers the paper strong enough for publication. However, there are still some critical points and suggestions for improvement, which slightly temper the overall positivity. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, even when pointing out areas for improvement. Thus, the politeness score is 80.",60,80
"Pros:
The paper formulates the driving policy problem as a model-based RL problem. Most related work on driving policy has been traditional robotics planning methods such as RRT or model-free RL such as policy gradient methods.

The policy is learned through unrolling a learned model of the environment dynamics over multiple time steps, and training a policy network to minimize a differentiable cost over this rolled-out trajectory.

The cost combine the objective the policy seeks to optimize (proximity to other cars) and an uncertainty cost representing the divergence from the states it is trained on.

Cons:

The model based RL formulation is pretty standard except that the paper has a additional model uncertainty cost.

Realistically, the output of driving policy should be planning decision, i.e. the waypoints instead of steering angles and acceleration / deceleration commands. There does not seem to be a need to solve the control problem using learning since PID and iLQR has solved the control problem very well. 

The paper did not seem to reach a conclusion on why stochastic forward model does not yield a clear improvement over the deterministic model. This may be due to the limitation of the dataset or the prediction horizon which seems to be 2 second. 

The dataset is only 45 minutes which captured by a camera looking down a small section of the road. So the policies learned might only do lane following and occasionally doing collision avoidance. I would encourage the authors to look into more diverse dataset. See the paper DESIRE: Distant Future Prediction in Dynamic Scenes with Interacting Agents, CVPR 2017.

Overall, the paper makes an interesting contribution: formulate the driving policy problem as a model-based RL problem. The techniques used are pretty standard. There are some insights in the experimental section. However, due to the limitation of the dataset, it is not clear how much the results can generalize to complex settings such as nudging around other cars, cutting in, pedestrian crossing, etc.

Response to rebuttal:
It is good to know that the authors have a new modified VAE posterior distribution for the stochastic model which can achieve significant gain over the deterministic model. Is this empirical and specific to this dataset? Without knowing the details, it is not clear how general this new stochastic model is.

I agree that it is worthwhile to test the model using the 45 minute dataset. However, I still believe the dataset is very limiting and it is not clear how much the experimental results can apply to other large realistic datasets.

My rating stays the same.

","The sentiment of the review is mixed but leans slightly positive. The reviewer acknowledges the interesting contribution of formulating the driving policy problem as a model-based RL problem and notes some insights in the experimental section. However, they also point out several limitations, such as the standard nature of the techniques used, the limitations of the dataset, and the lack of clear improvement from the stochastic model. Therefore, the sentiment score is 10. The politeness of the language is generally high. The reviewer provides constructive criticism and suggestions for improvement without being rude or dismissive. They encourage the authors to look into more diverse datasets and acknowledge the authors' efforts in the rebuttal. Therefore, the politeness score is 80.",10,80
"The paper proposes a way to speed up softmax at test time, especially when top-k words are needed. The idea is clustering inputs so that we need only to pick up words from a learn cluster corresponding to the input. The experimental results show that the model looses a little bit accuracy in return of much faster inference at test time. 

* pros: 
- the paper is well written. 
- the idea is simple but BRILLIANT. 
- the used techniques are good (especially to learn word clusters). 
- the experimental results  (speed up softmax at test time) are impressive. 

* cons: 
- the model is not end-to-end because word clusters are not continuous. But it not an important factor. 
- it can only speed up softmax at test time. I guess users are more interesting in speeding up at both test and training time.
- it would be better if the authors show some clusters for both input examples and corresponding word clusters.


","The sentiment of the review is highly positive, as evidenced by the use of words like 'BRILLIANT' and 'impressive' to describe the idea and results. The reviewer also lists several pros and only minor cons, indicating a strong overall approval of the paper. The politeness of the language is also high, as the reviewer uses polite and constructive language throughout, even when pointing out the cons. The cons are presented in a way that suggests they are minor issues and not significant drawbacks.",90,90
"In the submitted manuscript, the authors introduce a novel deep learning architecture to solve the  problem of supervised learning with sparse and irregularly sampled multivariate time series, with a specific interest in EHRs. The architecture is based on the use of a semi-parametric interpolation network followed by the application of a prediction network, and it is tested on two classification/regression tasks.

The manuscript is interesting and well written: the problem is properly located into context with extensive bibliography, the method is sufficiently detailed and the experimental comparative section is rich and supportive of the authors’ claim. However, there are a couple of issues that need to be discussed: 

	▪	the reported performances represent only a limited improvement over the comparing baselines, indicating that the proposed model is promising but it is still immature
	▪	the model is sharing many characteristics with (referenced) published methods, which the proposed algorithm is a smart combination of - thus, overall, the novelty of the introduced method is somewhat limited.


######### 

After considering the proposed improvements, I decided to raise my mark to 6. Thanks for the good job done!
","The sentiment of the review is generally positive, as indicated by phrases like 'interesting and well written' and 'good job done.' However, it also includes some constructive criticism, such as the limited improvement over baselines and the limited novelty of the method. Therefore, the sentiment score is 50. The language used is polite, with phrases like 'Thanks for the good job done!' and 'there are a couple of issues that need to be discussed,' indicating a polite tone. Therefore, the politeness score is 80.",50,80
"[Summary]:
This paper tackles the problem of automatic robot design. The most popular approach to doing this has been evolutionary methods which work by evolving morphology of agents in a feed-forward manner using a propagation and mutation rules. This is a non-differentiable process and relies on maintaining a large pool of candidates out of which best ones are chosen with the highest fitness. In robot design for a given task using rewards, training each robot design using RL with rewards is an expensive process and not scalable. This paper uses graph network to train each morphology using RL. Thereby, allowing the controller to share parameters and reuse information across generations. This expedites the score function evaluation improving the time complexity of the evolutionary process.

[Strengths]:
This paper shows some promise when graph network-based controllers augmented with evolutionary algorithms. Paper is quite easy to follow.

[Weaknesses and Clarifications]:
=> Robot design area has been explored extensively in classical work of Sims (1994) etc. using ES. Given that, the novelty of the paper is fairly incremental as it uses NerveNet to evaluate fitness and ES for the main design search.
=> Environment: The experimental section of the paper can be further improved. The approach is evaluated only in three cases: fish, walker, cheetah. Can it be applied to more complex morphologies? Humanoid etc. maybe?
=> Baselines: The comparison provided in the paper is weak. At first, it compares to random graph search and ES. But there are better baselines possible. One such example would be to have a network for each body part and share parameters across each body part. This network takes some identifying information (ID, shape etc.) about body part as input. As more body parts are added, more such network modules can be added. How would the given graph network compare to this? This baseline can be thought of a shared parameter graph with no message passing.
=> The results shown in Figure-4 (Section-4.2) seems unclear to me. As far as I understand, the model starts with hand-engineered design and then finetuned using evolutionary process. However, the original performance of the hand-engineered design is surprisingly bad (see first data point in any plot in Figure-4). Does the controller also start from scratch? If so, why? Also, it is not clear what is the meaning of generations if the graph is fixed, can't it be learned altogether at once?

[Recommendation]:
I request the authors to address the comments raised above. Overall, this is a reasonable paper but experimental section needs much more attention.","The sentiment of the review is moderately positive. The reviewer acknowledges the promise of the paper and finds it easy to follow, but also points out several areas for improvement, particularly in the experimental section and the novelty of the approach. Therefore, the sentiment score is 20. The politeness of the language is quite high. The reviewer uses polite language, such as 'I request the authors to address the comments raised above' and 'Overall, this is a reasonable paper,' which indicates a respectful and constructive tone. Therefore, the politeness score is 80.",20,80
"[REVISION]
The work is thorough and some of my minor concerns have been addressed, so I am increasing my score to 6. I cannot go beyond because of the incremental nature of the work, and the very limited applicability of the used continual learning setup from this paper.

[OLD REVIEW]
The paper proposes a novel, regularization based, approach to the sequential learning problem using a fixed size model. The main idea is to add extra terms to the loss encouraging representation sparsity and combating catastrophic forgetting. The approach fairs well compared to other regularization based approaches on MNIST and CIFAR-100 sequential learning variants.

Pros:
Thorough experiments, competitive baselines and informative ablation study.
Good performance on par or superior to baselines.
Clear paper, well written.

Cons:
The approach, while competitive in performance, does not seem to fix any significant issues with baseline methods. For example, task boundaries are still used, which limits applicability; in many scenarios which do have a continual learning problem there are no clear task boundaries, such as data distribution drift in both supervised and reinforcement learning.
Since models used in the work are very different from SOTA models on those particular tasks, it is hard to determine from the paper how the proposed method influences these models. In particular, it is not clear whether these changes to the loss would still allow top performance on regular classification tasks, e.g. CIFAR-10 or MNIST even without sequential learning, or in multitask learning settings. 

Summary:
Although the work is substantial and experiments are thorough, I have reservations about extrapolating from the results to settings which do have a continual learning problem. Although I am convinced results are slightly superior to baselines, and I appreciate the lengthy amount of work which went into proving that, the paper does not go sufficiently beyond previous work.
","The sentiment of the review is moderately positive. The reviewer acknowledges the thoroughness of the work and the competitive performance of the proposed approach, which is reflected in the increased score. However, the sentiment is tempered by concerns about the incremental nature of the work and its limited applicability, resulting in a sentiment score of 20. The language used in the review is polite and professional, with constructive feedback provided in a respectful manner, leading to a politeness score of 80.",20,80
"This work demonstrates that a particle dynamics model can be learned to approximate the interaction of various objects. The resulting differentiable simulator has a strong inductive bias, which makes it possible to efficiently solve complex manipulation tasks over deformable objects.

# Quality

This work is an impressive proof-of-concept of the capabilities of differentiable programming for learning complex (physical) processes, such as particle dynamics. In my opinion, the resulting particle interaction network would deserve publication for itself. However, this work goes already one step further and demonstrates that the resulting differentiable simulator can be used for the manipulation of deformable objects.

The method is evaluated on a well-rounded set of experiments which demonstrates its potential. More real-world experiments would be welcome to leave any doubt.

EDIT: However, the current manuscript lacks a proper comparison with (cited) previous work, such as 1806.08047. 

# Clarity

The paper is well written, although I do feel it was difficult to remain within the 8-page limit given the breadth of the work.

# Originality

As far as I know, this work is (very) original. (That being said, I am not too familiar with the related work.)

EDIT: This work is actually quite similar to 1806.08047. A proper discussion of the differences should be included. 

# Significance

This work will certainly be of interest for several research communities, including deep learning, physics, control and robotics.
","The sentiment of the review is generally positive, as indicated by phrases like 'impressive proof-of-concept,' 'deserve publication,' and 'well-rounded set of experiments.' However, there are some critical remarks, particularly about the lack of comparison with previous work, which slightly tempers the overall positivity. Therefore, the sentiment score is 70. The politeness of the language is very high, with the reviewer using polite and constructive language throughout, even when pointing out areas for improvement. Thus, the politeness score is 90.",70,90
"Paper Summary -
The authors trained RNNs to recognize formal languages defined by random regular expressions, then measured the accuracy of decoders that predict states of the minimal deterministic finite automata (MDFA) from the RNN hidden states. They then perform a greedy search over partitions of the set of MDFA states to find the groups of states which, when merged into a single decoder target, maximize prediction accuracy. For both the MDFA and the merged classes prediction problems, linear decoders perform as well as non-linear decoders.
Clarity - The paper is very clear, both in its prose and maths.
Originality - I don't know of any prior work that approaches the relationship between RNNs and automata in quite this way.
Quality/Significance - I have one major concern about the interpretation of the experiments in this paper.

The paper seems to express the following logic:
1 - linear (and non-linear) decoders aren't so good at predicting MDFA states from RNN hidden states
2 - if we make an ""abstract"" finite automata (FA) by merging states of the MDFA to optimize decoder performance, the linear (and non-linear) decoders are much better at predicting this new, smaller FA's states.
3 - thus, trained RNNs implement something like an abstract FA to recognize formal languages.

However, a more appropriate interpretation of these experiments seems to be:
1 - (same)
2 - if we find the output classes the decoder is most often confused between, then merge them into one class, the decoder's performance increases -- trivially. in other words, you just removed the hardest parts of the classification problem, so performance increased. note: performance also increases because there are fewer classes in the merged-state FA prediction problem (e.g., chance accuracy is higher).
3 - thus, from these experiments it's hard to say much about the relationship between trained RNNs and finite automata.

I see that the ""accuracy"" measurement for the merged-state FA prediction problem, \rho, is somewhat more complicated than I would have expected; e.g., it takes into account \delta and f(h_t) as well as f(h_{t+1}). Ultimately, this formulation still asks whether any state in the merged state-set that contains f(h) transitions under the MDFA to the any state in the merged state-set that contains f(h_{t+1}). As a result, as far as I can tell the basic logic of the interpretation I laid out still applies.

Perhaps I've missed something -- I'll look forward to the author response which may alleviate my concern.

Pros - very clearly written, understanding trained RNNs is an important topic
Cons - the basic logic of the conclusion may be flawed (will await author response)

Minor -
The regular expression in Figure 6 (Top) is for phone numbers instead of emails.
""Average linear decoding accuracy as a function of M in the MDFA"" -- I don't think ""M"" was ever defined. From contexts it looks like it's the number of nodes in the MDFA.
""Average ratio of coarseness"" -- It would be nice to be explicit about what the ""ratio of coarseness"" is. I'm guessing it's (number of nodes in MDFA)/(number of nodes in abstracted DFA).
What are the integers and percentages inside the circles in Figure 6?
Figures 4 and 5 are difficult to interpret because the same (or at least very similar) colors are used multiple times.
I don't see ""a"" (as in a_t in the equations on page 3) defined anywhere. I think it's meant to indicate a symbol in the alphabet \Sigma. Maybe I missed it.","The sentiment of the review is generally positive, as the reviewer acknowledges the clarity and originality of the paper and expresses interest in the topic. However, the reviewer also has a significant concern about the interpretation of the experiments, which tempers the overall positivity. Therefore, the sentiment score is 50. The politeness of the language is high, as the reviewer uses polite phrases such as 'I have one major concern,' 'Perhaps I've missed something,' and 'I'll look forward to the author response,' indicating a respectful and constructive tone. Therefore, the politeness score is 90.",50,90
"This paper aims at matching people's voices to the images of their faces. It describes a method to train shared embeddings of voices and face images. The speech and image features go through separate neural networks until a shared embedding layer. Then a classification network is built on top of the embeddings from both networks.  The classification network predicts various combinations of covariates of faces and voices: gender, nationality, and identity.  The input to the classification network is then used as a shared representation for performing retrieval and matching tasks.

Compared with similar work from Nagrani et al (2018) who generate paired inputs of voices and faces and train a network to classify if the pair is matched or not, the proposed method doesn't require paired inputs.  It does, however, require inputs that are labeled with the same covariates across modalities.  My feeling is that paired positive examples are easier to obtain (e.g., from unlabeled video) than inputs labeled with these covariates, although paired negative examples require labeling and so may be as difficult to obtain.

Several different evaluations are performed, comparing networks that were trained to predict all subsets of identity, gender, and nationality.  These include identifying a matching face in a set of faces (1,2 or N faces) for a given voice, or vice versa. Results show that the network that predicts identity+gender tends to work best under a variety of careful examinations of various stratifications of the data.  These stratifications also show that while gender is useful overall, it is not when the gender of imposters is the same as that of the target individual.  The results also show that even when evaluating the voices and faces not shown in the training data, the model can achieve 83.2% AUC on unseen/unheard individuals, which outperforms the state-of-the-art method from Nagrani et al (2018).

An interesting avenue of future work would be using the prediction of these covariates to initialize a network and then refine it using some sort of ranking loss like the triplet loss, contrastive loss, etc.


Writing:
* Overall, ciations are all given in textual form Nagrani et al (2018) (in latex this is \citet{} or \cite{}), when many times parenthetical citations (Nagrani et al, 2018) (in latex this is \citep{}) would be more appropriate.
* The image of the voice waveform in Figures 1 and 2 should be replaced by log Mel-spectrograms in order to illustrate the network's input.
* ""state or art"" instead of ""state-of-the-art"" on page 3. 
* In subsection 2.4: ""mGiven"" is written instead of ""Given"". 
* On Page 6 Section 3.1 ""1:2 matching"" paragraph. ""Nagrani et al."" is written twice. * * Page 6 mentions that there is a row labelled ""SVHF-Net"" in table 2, but there is no such row is this table. 
* Page 7 line 1, “G,N” should be ""G, N"".
","The sentiment of the review is generally positive, as the reviewer acknowledges the novelty and effectiveness of the proposed method, especially in comparison to the state-of-the-art method by Nagrani et al. (2018). The reviewer also highlights the impressive performance metrics achieved by the model. However, there are some minor criticisms regarding the practicality of obtaining labeled inputs and some suggestions for future work. The politeness of the language is high, as the reviewer provides constructive feedback and specific recommendations in a respectful and professional manner. The reviewer uses polite language and avoids any harsh or dismissive comments.",70,90
"This paper describes two distinct contributions: a new compound criterion for comparing a temporal clustering to a ground truth clustering and a new bayesian temporal clustering method. Globally the paper is clear and well illustrated. 
1) About the new criterion:
*pros: *
 a) as clearly pointed out by the authors, using standard non temporal clustering comparison metrics for temporal clustering evaluation is in a way ""broken by design"" as standard metrics disregard the very specificity of the problem. Thus the introduction of metrics that take explicitly into account time is extremely important.
 b) the proposed criterion combines two parts that are very important: finding the length of the stable intervals (i.e. intervals whose instants are all classified into a single cluster) and finding the sequence of labels. 
*cons:*
 a) while the criterion seems new it is also related to criteria used in the segmentation literature (see among many other https://doi.org/10.1080/01621459.2012.737745) and it would have been a good idea to discuss the relation between temporal clustering and segmentation, even briefly.
b) the reliance on a tradeoff parameter in the final criterion is a major problem: how shall one chose the parameter (more on this below)? The paper does not explore the effect of modifying the parameter.
c) in the experimental section, TSS is mostly compared to NMI and to optimal matching (called Munkres here). Even considering the full list of criteria in the appendix, the normalized rand index (NRI) seems to be missing. This is a major oversight as the NRI is very adapted to comparing clusterings with different number of clusters, contrarily to NMI. In addition, the authors claim that optimal matching is completely opaque and difficult to analyse, while on the contrary it gives a proper way of comparing clusters from different clusterings, enabling fine grain analysis. 

2) about the new model
*pros*: 
 a) as far as I know, this is indeed a new model
 b) the way the model is structured emphasizes segmentation rather than temporal dependency: the so called procedure is arbitrary and no dependency is assumed from one segment to another. In descriptive analysis this is highly desirable (as opposed to say HMM which focuses on temporal dependencies). 
*cons*
a) the way the length of the segments in the sequence are generated (with sorting) this a bit convolved. Why not generating directly those lengths? What is the distribution of those lengths under the sampling model? Is this adapted? 
b) I find the experimental evaluation acceptable but a bit poor. In particular, nothing is said on how a practitioner would tune the parameters. I can accept that the model will be rather insensitive to hyper-parameters alpha and beta, but I've serious doubt about the number of clusters, especially as the evaluation is done here in the best possible setting. In addition, the other beta parameter (of TSS) is not studied. 

Minor point:
- do not use beta for two different things (the balance in TSS and the prior parameter in the model)","The review starts with a positive sentiment, appreciating the clarity and illustration of the paper. The reviewer acknowledges the importance of the new criterion and model, which contributes positively to the sentiment score. However, the review also includes several critical points, particularly regarding the reliance on a tradeoff parameter, the omission of the normalized rand index, and the complexity of the segment length generation. These criticisms lower the overall sentiment score but do not make it entirely negative. The language used is polite and constructive, with suggestions for improvement rather than dismissive comments, which results in a high politeness score.",30,80
"General comment
==============
The authors used policy gradient optimization for generating RNA sequences that fold into a target secondary structure, reporting clear accuracy and runtime improvements over the previous state-of-the-art. The authors used BOHR for optimizing hyper-parameters and present a new dataset for evaluating RNA design methods. The paper is well motivated and mostly clearly written. However, the methodological contributions are limited and I have some important concerns about their evaluation. Overall, I feel it’s a good paper for an ICLR workshop or biological journal if the authors address the outstanding comments.

Major comments
=============
1. The methodological contributions are limited. The authors used existing approaches (policy gradient optimization and BOHR for hyperparameter optimization) but do not report new methods, e.g. for sequence modeling. Performing hyper-parameter optimization is in my eyes not novel, but common practice in the field. It would me more informative if the authors compared reinforcement learning to other approaches for (conditional) sequence generations, e.g. RNNs, autoregressive models, VAEs, or GANs, which have been previously reported for biological sequence generation (e.g. http://arxiv.org/abs/1804.01694).

2. Did the authors split all three datasets (Eterna, Rfam-Taneda, Rfam-learn-test) into train, eval, and test set, trained their method on the training set, optimized hyper-parameters on the eval set, and measured generalization and runtime on the test set? This is not described clearly enough in section 5. I suggest to summarize the number of sequences for each dataset and split in a table.

3. Did the authors also optimize the most important hyperparameters of RL-LS and other methods? Otherwise it is unclear if the performance gain is due to hyperparameter optimization or the method itself.

4. The time measurement (x-axis figure 3) is unclear. Is it the time that methods were given to solve a particular target structure and does figure 3 show the average number of solved structures in the test for a the time shown on the x-axis? 

5. Were all methods compared on the same hardware (section 5; 20 cores; Broadwell E5-2630v4 2.2 GHz CPUs) and can they be parallelized over multiple CPU or GPU cores? This is essential for a fair runtime comparison.

6. The term ‘run’ (“unreliable outcomes in single runs”, section 4) is unclear. Is it a single sample from the model (one rollout), a particular hyperparameter configuration, or training the model once for a single target structure? This must be clarified for understanding the evaluation.

7. How does the accuracy and runtime or LEARNA scale depending on the sequence (structure) length?

8. How sensitive is the model performance depending on the context size k for representing the current state? Did the authors try to encode the entire target structure with, e.g. recurrent models, instead of using a window centered on the current position?

9. The authors should more clearly describe the local optimization step (section 3.1; reward). Were all nucleotides that differ mutated independently, or enumerated exhaustively? The latter would have a high runtime of O(3^d), where d is the number of nucleotides that differ. When do the authors start with the local optimization? 

Minor comments
=============
10. The authors should replace ‘450x’ faster in the abstract by ‘clearly’ faster since the evaluation does not show that LEARNA is 450x faster than all other methods.

11. Does “At its most basic form” (introduction) mean that alternative RNA nucleotides exist? If so, this should be cited.

12. The authors should more clearly motive in the introduction why they created a new dataset.

13. The authors should mention in section 2.1 that the dot-bracket notation is not the only notation for representing RNA structures (https://www.tbi.univie.ac.at/RNA/ViennaRNA/doc/html/rna_structure_notations.html).

14. The authors should define the hamming distance (section 2.1). Do other distance metrics exist?

15. For the Traveling Salesman Problem (section 2.2) should the reward be the *negative* tour length?

16. The authors should more clearly describe the embedding layer (section 4). Are nucleotides one-hot encoded or represented as integers (0, 1  for ‘(‘ and ‘.’)?","The sentiment of the review is generally positive but with notable reservations. The reviewer acknowledges the clear accuracy and runtime improvements, as well as the well-motivated and mostly clearly written paper. However, they express concerns about the limited methodological contributions and the evaluation process. Therefore, the sentiment score is 40. The politeness of the language is high; the reviewer uses polite and constructive language throughout, providing specific recommendations and suggestions for improvement without being dismissive or harsh. Thus, the politeness score is 80.",40,80
"** review score incremented following discussion below **

Strengths:

Well written and clear paper
Intuition is strong: not all source-target class pairs are as beneficial to find adversarial examples for 

Weaknesses:

Cost matrices choices feel a bit arbitrary in experiments
CIFAR experiments still use very small norm-balls

The submission builds on seminal work by Dalvi et al. (2004), which studied cost-sensitive adversaries in the context of spam detection. In particular, it extends the approach to certifiable robustness introduced by Wong and Kolter with a cost matrix that specifies for each pair of source-target classes whether the model should be robust to adversarial examples that are able to take an input from the source class to the target (or conversely whether these adversarial examples are of interest to an adversary).

While the presentation of the paper is overall of great quality, some elements from the certified robustness literature could be reminded in order to ensure that the paper is self-contained. For instance, it is unclear how the guaranteed lower bound is derived without reading prior work. Adding this information in the present submission would make it easier for the reader to follow not only Sections 3.1 and 3.2 but also the computations behind Figure 1.b. 

The experiments results are clearly presented but some of the details of the experimental setup are not always justified. If you are able to clarify the following choices in your rebuttal, this would help revise my review. First, the choice of cost matrices feels a bit arbitrary and somewhat cyclical. For instance, binary cost matrices for MNIST are chosen according to results found in Figure 1.b, but then later the same bounds are used to evaluate the performance of the approach. Yet, adversarial incentives may not be directly correlated with the “hardness” of a source-target class pair as measured in Figure 1.b. The real-valued cost matrices are better justified in that respect. Second, would you be able to provide additional justification or analysis of the choice of the epsilon parameter for CIFAR-10? For MNIST, you were able to improve the epsilon parameter from epsilon=0.1 to epsilon=0.2 but for CIFAR-10 the epsilon parameter is identical to Wong et al. Does that indicate that the results presented in this paper do not scale beyond simple datasets like MNIST?

Minor comments:


P2: The definition of adversarial examples given in Section 2.2 is a bit too restrictive, and in particular only applies to the vision domain. Adversarial examples are usually described as any test input manipulated by an adversary to force a model to mispredict.
P3: typo in “optimzation” 
P5: trade off -> trade-off 
P8: the font used in Figure 2 is small and hard to read when printed.
","The sentiment of the review is generally positive, as indicated by phrases like 'well written and clear paper' and 'intuition is strong.' However, there are some criticisms and suggestions for improvement, which slightly temper the overall positivity. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer uses polite and constructive language throughout, such as 'If you are able to clarify' and 'this would help revise my review.' Therefore, the politeness score is 80.",60,80
"This paper presents an algorithm to find adversarial attacks to binary neural networks.  Binary neural networks uses sign functions as nonlinearities, making the network essentially discrete.  Previous attempts at finding adversarial attacks for binary neural networks either rely on relaxation which cannot find very good adversarial examples, or calling a mixed integer linear programming (MILP) solver which doesn’t scale.  This paper proposes to decompose the problem and iteratively find desired representations layer by layer from the top to the input.  This so called Integer Propagation (IProp) algorithm is more efficient than solving the full MILP as it solves much smaller MILP problems, one for each layer, thus each step can be solved relatively quickly.  The authors then proposed a few more improvements to the IProp algorithm, including ways to do local adjustments to the solutions, and warming starting from an existing solution.  Experiments on binary neural nets trained for MNIST and Fashion MNIST show the superiority of the proposed method over MILP and relaxation based algorithms.

Overall I found the paper to be very clear and the proposed method is sound.  I think combining ideas from discrete / combinatorial optimization with deep learning is an important research direction and can shed light on training and verifying models with discrete components, like the hard nonlinearities in the binary neural nets studied in this paper.

In terms of the particular proposed approach, it is hard for me to imagine the blind IProp that does not take the input into account until the last layer is ever going to work.  The small step size modifications make a lot more sense.  Regarding the selection of the set S, in the paper the authors simply sampled elements to be in S uniformly, but it seems possible to make use of the information from the forward pass, and choose the hidden units that are the closed to reaching the desired activations.  Would that be any better?

A few minor comments:
- when reporting warm start results, it would be good to also show the performance of the FGSM solution used for warm starting, in addition to the other two results shown in Figure 6 to have a more complete comparison
- the hidden units h_{l,j} were formulated to be in {0, 1} in equation (7), but everywhere else in the paper they are assumed to be in {-1, +1}, which is not consistent and slightly confusing.

Overall I think this is a solid paper and support accepting it for publication.","The sentiment of the review is generally positive. The reviewer praises the clarity and soundness of the paper, and acknowledges the importance of the research direction. The reviewer also supports the acceptance of the paper for publication. However, there are some critical comments regarding specific aspects of the proposed approach, which slightly temper the overall positivity. Therefore, the sentiment score is 80. The language used in the review is polite and constructive, offering suggestions for improvement in a respectful manner. The reviewer uses phrases like 'it would be good to' and 'slightly confusing,' which indicate a polite tone. Therefore, the politeness score is 90.",80,90
"Overall, the paper is well-written and of high quality, therefore I recommend acceptance. 

Pros:
+ The work gives an accessible but still rigorous introduction to the literature on VIs which I find highly valuable, as it creates a bridge between the classical mathematical programming literature and applications in AI. 

+ The theory for optimization of VIs with stochastic gradients (though only in monotone setting) was very interesting to me and contains some novel results (Theorem 2, Theorem 4)

Cons:
- I'm a bit skeptical about the experiments on GANs. They indicate that for the specific choice of architectures and hyper-parameters ""ExtraAdam"" works better, but the chosen architectures are not state-of-the art. What would convince me if the algorithm can be used to improve a current best inception score of 8.2 reached with SNGANs. Also with WGAN-GP, scores of ~7.8 are reported which are much higher than the 6.4 reported in the paper. But I understand that producing state-of-the-art inception scores is not the focus of the paper, therefore I would suggest that the authors release an implementation of the proposed new optimizers (ExtraAdam) for a popular DL framework (e.g. pytorch) such that practitioners working with GANs can quickly try them out in a ""plug-and-play"" fashion.

- Proposition 2 is a bit misleading. While for \eta \in (0, 1) implicit and extrapolation are similar, adding the remark that implicit method is stable for any \eta > 0 (and therefore can lead to an arbitrary fast convergence) would give a more balanced view. Right now, only the advantages of extrapolation method and disadvantages of implicit method are mentioned which I find unfair for the implicit method.

- The theory is presented for variational inequalities with monotone operators. For clarity it should be mentioned that GANs parametrized with neural nets lead to non-monotone VIs. A provably convergent algorithm for that setting is still an open problem, no?
","The sentiment of the review is positive, as indicated by the initial statement recommending acceptance and the praise for the paper's quality and contributions. The sentiment score is 80 because the reviewer expresses strong approval but also notes some areas for improvement. The politeness score is 90 because the language used is very polite and constructive, with suggestions framed in a helpful manner rather than as harsh criticisms.",80,90
"The paper proposes ""style transfer"" approaches for text rewriting that allow for controllable attributes. For example, given one piece of text (and the conditional attributes associated with the user who generated it, such as their age and gender), these attributes can be changed so as to generate equivalent text in a different style.

This is an interesting application, and somewhat different from ""style transfer"" approaches that I've seen elsewhere. That being said I'm not particularly expert in the use of such techniques for text data.

The architectural details provided in the paper are quite thin. Other than the starting point, which as I understand adapts machine translation techniques based on denoising autoencoders, the modifications used to apply the technique to the specific datasets used here were hard to follow: basically just a few sentences described at a high level. Maybe to somebody more familiar with these techniques will understand these modifications fully, but to me it was hard to follow whether something methodologically significant had been added to the model, or whether the technique was just a few straightforward modifications to an existing method to adapt it to the task. I'll defer to others for comments on this aspect.

Other than that the example results shown are quite compelling (both qualitatively and quantitatively), and the experiments are fairly detailed.
","The sentiment of the review is mixed but leans towards positive. The reviewer acknowledges the interesting application and compelling results, which suggests a positive sentiment. However, the reviewer also points out significant issues with the architectural details, indicating some level of dissatisfaction. Therefore, the sentiment score is 30. The politeness of the language is high; the reviewer uses polite phrases such as 'That being said,' 'I'll defer to others,' and 'Other than that,' which indicates a respectful and considerate tone. Therefore, the politeness score is 80.",30,80
"Summary:
The authors propose an extension of dual learning (DL). In DL, one leverages the duality of a dataset, by predicting both forward and backward, e.g. English to German, and German back to English. It’s been shown that training models using this duality is beneficial. This paper extends DL by introducing multiple models for the forward and backward, and using their output to regularise the training of the two main agents.

The authors show that this setup improves on the SotA, at only a training computation expense (inference/test time remains the same).

Review:
The paper shows extensive experimentation and improves the previous result in all cases. The proposed method is a straightforward extension and can be readily implemented and used.

I have difficulty understanding equation 8 and the paragraph below. It seems like the authors use an equal weighting for the additional agents, however they mention using Monte Carlo to “tackle the intractability resulting form the summation over the exponentially large space y”. According to the paper the size of y is the dataset, is it exponentially large? Do the authors describe stochastic gradient descent? Also what do the authors mean by offline sampling? Do they compute the targets for f_0 and g_0 beforehand using f_1…n and g_1…n?

The results mention computational cost a few times, I was wondering if the authors could comment on the increase in computational cost? e.g. how long does “pre-training” take versus training the dual? Can the training of the pre-trained agents be parallelised? Would it be possible to use dropout to more computationally efficient obtain the result of an ensemble?

In general I think the authors did an excellent job validating their method on various different datasets. I also think the above confusions can be cleared up with some editing. However the general contribution of the paper is not enough, the increase in performance is minimal and the increased computational cost/complexity substantial. I do think this is a promising direction and encourage the authors to explore further directions of multi-agent dual learning.

Textual Notes:
- Pg2, middle of paragraph 1: “which are pre-trained with parameters fixed along the whole process”. This is unclear, do you mean trained before optimising f_0 and g_0 and subsequently held constant?
- Pg2, middle last paragraph: “typical way of training ML models”. While the cross entropy loss is a popular loss, it is not “typical”.
- Pg 3, equation 4, what does “briefly” mean above the equal sign?
- Perhaps a title referring to ensemble dual learning would be more appropriate, given the possible confusion with multi agent reinforcement learning. 


################
Revision:

I would like to thank the authors for the extensive revision, additional explanations/experiments, and pointing out extensive relevant literature on BLUE scores. The revision and comments are much appreciated. I have increased my score from 4 to 6.","The sentiment of the review is generally positive, as the reviewer acknowledges the extensive experimentation, validation on various datasets, and the potential of the proposed method. However, the sentiment is tempered by concerns about the clarity of certain explanations and the overall contribution of the paper. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer uses polite phrases such as 'I would like to thank the authors' and 'the revision and comments are much appreciated,' and provides constructive feedback in a respectful manner. Therefore, the politeness score is 90.",60,90
"
SUMMARY:
This work is about learning state-transition models in complex domains represented as sets of objects, their properties, ``""deictic"" reference functions between sets of objects, and possible actions (or action templates). A parametric model for the actions is assumed, and these parameters act on a neural net that learns the transition model (probabilistic rule) from the current state to the next one.  It is basically this nonlinear transition model implemented by a network which makes this work different from previous models described in the literature. The relational transition model proposed is sparse, based on the assumption that actions have only ``local effects on related objects. The prediction model itself is basically a Gaussian distribution whose mean and variances are represented by neural nets. For jointly learning multiple rules, a clustering strategy is presented which assigns experience samples to transition rules. The method is applied to simulated data in the context of predicting pushing stacks of blocks on a cluttered table top.

EVALUATION: 
The type of problems addressed in this paper is challenging and highly relevant for solving problems in the ``real'' world. Although the method proposed is in some sense a direct generalization of the work in [Pasula et al.], it still contains many novel and interesting aspects.Any single part of the model (like the use of Gaussians parametrized by functions implemented via neural nets) is somehow ``standard in deep latent variable models, but in complex real-world rule-learning problems the whole system presented  defines  certainly a big improvement over the state-of-the-art, which in my opinion has the potential to indeed advance this field of research.     
 ","The review starts by summarizing the work in a neutral and factual manner. The evaluation section, however, contains positive remarks about the relevance and novelty of the work. The reviewer acknowledges the challenging nature of the problem and the potential of the proposed method to advance the field, which indicates a positive sentiment. The language used is respectful and acknowledges the contributions of the authors, indicating a high level of politeness.",80,90
"This article addresses the problem of domain adaptation of semantic segmentation methods from autonomous vehicle simulators to the real world. The key contribution of this paper is the use of privileged information for performing the adaptation. The method is of those called unsupervised domain adaptation as no labels from the target domain are used for the adaptation. The method is based on a GAN with: a) A generator that transforms the simulation images to real appearance; b) A discriminator that distinguish between real and fake images;  c) a privileged network that learns to perform depth estimation; and d) the task networks that learns to perform semantic segmentation. Privileged information is very few exploited in simulations and I consider it an important way of further exploit these simulators.

The article is clear, short, well written and very easy to understand. The method is effective as it is able to perform domain adaptation and improve over the compared methods. There is also an ablation study to evaluate the contribution of each module. This ablation study shows that the privileged information used helps to better perform the adaptation. The state of the art is comprehensive and the formulation seams correct.  The datasets used for the experiments (Synthia, Cityscapes and Vistas) is very adequate as they are the standard ones.

Some minor concerns:
 - The use of 360x640 as resolution
 - The use of FCN8 instead of something based on Resnet or densenet

I would like some more details on what is happening with Vistas dataset. SPIGAN-no-PI underperforms the source model. By looking at Figure 4 we can observe that the transformation of the images is not working properly as many artifacts appear. In SPIGAN those artifacts does not appear and then the adaptation works better. Could it be a problem in the training?


","The review is highly positive about the article, praising its clarity, effectiveness, and the importance of its contributions. The sentiment score is high because the reviewer expresses strong approval of the work. The politeness score is also high as the language used is respectful and constructive, even when pointing out minor concerns.",90,95
"In the paper, the authors try to analyze the convergence of stochastic gradient descent based method with stagewise learning rate and average solution in practice. The paper is very easy to follow, and the experimental results are clear. The following are my concerns:

1. In function (3), for any x in R^d, if \hat x  = prox_\gamma f (x), then f(\hat x ) <= f(x). This inequality looks not correct to me. If x = argmin_x f(x), the above inequality is obviously wrong.  It looks like that function (3) is a very important basis for the whole paper.
 
2. By using the weakly convex assumption and solving f_s, the authors transform a nonconvex nonsmooth problem to a convex problem. However, the paper didn't mention how to select \gamma in the algorithm. This parameter is nontrivial, if you set a small value, the problem is not convex and the analysis does not hold. In the experiment, the authors tune \gamma from 1 to 2000, which means that u < 1 or u < 1/2000.  Given neural network is a u-weakly convex problem or u-smooth problem, the theory does not match the experiment. 

3. The authors propose a universal stagewise optimization framework and mention that the stagewise ADAGRAD obtains faster convergence than other analysis. My question is that, if it is a generic framework, how about the convergence rate for other methods? is there also acceleration for SGD or momentum SGD? 
","The review starts with a positive sentiment, noting that the paper is easy to follow and the experimental results are clear. However, it quickly transitions to specific concerns and criticisms about the paper's methodology and theoretical basis. The language used is polite and constructive, aiming to provide helpful feedback to improve the paper. The reviewer uses phrases like 'looks not correct to me' and 'My question is that,' which are polite ways to express disagreement and seek clarification.",20,80
"edit:  the authors nicely revised the submission, I think it is a very good paper. I increased my rating.

-----

This paper presents a method that learns to reproduce 'block towers' from a given image. A perception model, a physics engine model, and a rendering engine are first trained together on pairs of images.
The perception model predicts a representation of the scene decomposed into objects;  the physics engine predicts the object representation of a scene from an initial object representation; the rendering engine predicts an image given an object representation.

Each training pair of images is made of the first image of a sequence when introducing an object into a scene, and of the last image of the sequence, after simulating the object's motion with a physics engine. The 3 parts of the pipeline (perception, physics, rendering) are trained together on this data.

To validate the learned pipeline, it is used to recreate scenes from reference images, by trying to introduce objects in an empty scene until the given scene can be reproduced. It outperforms a related pipeline that lacks a scene representation based on objects.

This is a very interesting paper, with new ideas:
- The object-based scene representation makes a lot of sense, compared to the abstract representation used in recent work. 
- The training procedure, based on observing the result of an action, is interesting as the examples are easy to collect (except for the fact that the ground truth segmentation of the images is used as input, see below).

However, there are several things that are swept 'under the carpet' in my opinion, and this should be fixed if the paper is accepted.

* the input images are given in the form of a set of images, one image corresponding to the object segmentation. This is mentioned only once (briefly) in the middle of the paragraph for Section 2.1, while this should be mentioned in the introduction, as this makes the perception part easier. There is actually a comment in the discussion section and the authors promised to clarify this aspect, which should indeed be more detailed. For example, do the segments correspond to the full objects, or only the visible parts?

* The training procedure is explained only in Section 4.1. Before reaching this part, the method remained very mysterious to me. The text in Section 4.1 should be moved much earlier in the paper, probably between current sections 2.3 and 2.4, and briefly explained in the introduction as well.
This training procedure is in fact fully supervised - which is fine with me: Supervision makes learning 'safer'. What is nice here is that the training examples can be collected easily - even if the system was not running in a simulation.

* if I understand correctly the planning procedure, it proceeds as follows:
- sampling 'actions' that introduce 1 object at a time (?)
- for each sampled action, predicting the scene representation after the action is performed, by simulating it with the learned pipeline, 
- keeping the action that generates a scene representation close to the scene representation computed for the goal image of the scene.
- performing the selected action in a simulator, and iterate until the number of performed actions is the same as the number of objects (which is assumed to be known).

-> how do you compare the scene representation of the goal image and the predicted one before the scene is complete? Don't you need some robust distance instead of the MSE?
-> are the actions really sampled randomly?  How many actions do you need to sample for the examples given in the paper?

I also have one question about the rendering engine:  Why using the weighted average of the object images? Why not using the intensity of the object with the smallest predicted depth?  It should generate sharper images. Does using the weighted average make the convergence easier?
","The sentiment of the review is positive, as indicated by phrases such as 'the authors nicely revised the submission,' 'I think it is a very good paper,' and 'This is a very interesting paper, with new ideas.' The reviewer appreciates the novel aspects of the paper and acknowledges the improvements made by the authors. However, the reviewer also points out several areas that need clarification and improvement, which is typical in a constructive review. Therefore, the sentiment score is 80. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review. Phrases like 'should be fixed if the paper is accepted,' 'which should indeed be more detailed,' and 'which is fine with me' indicate a respectful and considerate tone. Therefore, the politeness score is 90.",80,90
"Pros:
1. This work presents a novel construction of the popularly-used attention modules. It points out the problems lied in existing design that attention vectors are only computed based on parametric functions, instead of considering the interactions among each attention step and output variables. To achieve that, the authors re-write the joint distribution as a product of tractable terms at each timestamp and fully exploit the dependencies among attention and output variables across the sequence. The motivation is clear, and the proposed strategy is original and to the point. This makes the work relative solid and interesting for a publication. Furthermore, the authors propose 3 different formulation for prior attention, making the work even stronger.
2. The technical content looks good, with each formula written clearly and with sufficient deductive steps. Figure 1 provides clear illustration on the comparison with traditional attentions and shows the advantage of the proposed model.
3. Extensive experiments are conducted including 5 machine translation tasks as well as another morphological inflection task. These results make the statement more convincing. The authors also conducted further experiments to analyze the effectiveness, including attention entropy evaluation.

Cons:
1. The rich information contained in the paper is not very well-organized. It takes some time to digest, due to some unclear or missing statements. Specifically, the computation for prior attention should be ordered in a subsection with a section name. The 3 different formulations should be first summarized and started with the same core formula as (4). In this way, it will become more clear of where does eq(6) come from or used for. Currently, this part is confusing.
2. Many substitutions of variables take place without detailed explanation, e.g., y_{<t} with s_t, a with x_{a} in (11) etc. Could you explain before making these substitutions?
3. As mentioned, the PAM actually computes hard attentions. It should be better to make the statement more clear by explicitly explaining eq(11) on how it assembles hard attention computation.

QA:
1. In the equation above (3) that computes prior(a_t), can you explain how P(a_{t-1}|y_{<t}) approximates P(a_{<t}|y_{<t})? What's the assumption?
2. How is eq(5) computed using first order Taylor expansion? How to make Postr inside the probability? And where does x_a' come from?
3. Transferring from P(y) on top of page 3 to eq(11), how do you substitute y_{<t}, a_t with s_t, x_j? Is there a typo for x_j?
4. Can you explain how is the baseline Prior-Joint constructed? Specifically, how to compute prior using soft attention without postr?","The review starts with a positive sentiment, highlighting the novelty, clarity, and thoroughness of the work. The reviewer appreciates the original strategy, clear technical content, and extensive experiments. However, the review also points out several areas for improvement, such as the organization of information and the need for clearer explanations of certain substitutions and computations. The language used is constructive and polite, offering specific suggestions for improvement without being harsh or dismissive.",70,80
"The main contributions of this work are essentially on the theoretical aspects. It seems that the proposed algorithm is not very original because its two parts, namely prediction (coefficient estimation) and learning (dictionary update) have been widely used in the literature, using respectively a IHT and a gradient descent. The authors need to describe in detail the algorithmic novelty of their work.

The definition of “recovering true factor exactly” need to be given. The proposed algorithm involves several tuning parameters, when alternating between two updating rules, an IHT-based update for coefficients and a gradient descent-based update for the dictionary. Therefore, an appropriate choice of their values need to be given.

In the algorithm, the authors need to define the HT function in (3) and (4).

In the experiments, the authors compare the proposed method to only the one proposed by Arora et al. 2015. We think that this is not enough, and more extensive experimental results would provide a better paper. 

There are some typos that can be easily found, such as “of the out algorithm”.","The sentiment of the review is moderately negative. The reviewer points out that the main contributions are theoretical and criticizes the lack of originality in the proposed algorithm. They also highlight several areas where the paper needs improvement, such as the need for detailed descriptions, definitions, and more extensive experimental results. The politeness of the language is neutral to slightly polite. The reviewer uses phrases like 'need to' and 'we think that this is not enough,' which are direct but not rude. They also provide constructive feedback and specific recommendations for improvement.",-40,20
"==========================
I have read the authors' response and other reviewers' comments. Thanks the authors for taking great effort in answering my questions. Generally, I feel satisfied with the repsonse, and prefer an acceptance recommendation. 
==========================
Contributions:

The main contribution of this paper is the proposed RelGAN. First, instead of using a standard LSTM as generator, the authors propose using a relational memory based generator. Second, instead of using a single CNN as discriminator, the authors use multiple embedded representations. Third, Gumbel-softmax relaxation is also used for training GANs on discrete textual data. The authors also claim the proposed model has the ability to control the trade-off between sample quality and diversity via a single adjustable parameter. 

Detailed comments:

(1) Novelty: This paper is not a breakthrough paper, mainly following previous work and propose new designs to improve the performance. However, it still contains some novelty inside, for example, the model choice of the generator and discriminator. I think the observation that the temperature control used in the Gumbel-softmax can reflect the trade-off between quality and diversity is interesting. 

However, I feel the claim in the last sentence of the abstract and introduction is a little bit strong. Though this paper seems to be the first to really use Gumbel-softmax for text generation, similar techniques like using annealed softmax to approximate argmax has already been used in previous work (Zhang et al., 2017). Since this is similar to Gumbel-softmax, I think this may need additional one or two sentences to clarify this for more careful discussion.  

Further, I would also recommend the authors discuss the following paper [a] to make this work more comprehensive as to the discussion of related work. [a] also uses annealed softmax approximation, and also divide the GAN approaches as RL-based and RL-free, similar in spirit as the discuss in this paper. 

[a] Adversarial Text Generation via Feature-Mover's Distance, NIPS 2018.

(2) Presentation: This paper is carefully written and easy to follow. I enjoyed reading the paper. 

(3) Evaluation: Experiments are generally well-executed, with ablation study also provided. However, human evaluation is lacked, which I think is essential for this line of work. I have a few questions listed below. 

Questions:

(1) In section 2.4, it mentions that the generator needs pre-training. So, my question is: does the discriminator also need pre-training? If so, how the discriminator is pre-trained?

(2) In Table 1 & 2 & 3, how does your model compare with MaskGAN? If this can be provided, it would be better. 

(3) Instead of using NLL_{gen}, a natural question is: what are the self-BLEU score results since it was used in previous work?

(4) The \beta_max value used in the synthetic and real datasets is quite different. For example, \beta_max = 1 or 2 in synthetic data, while \beta_max = 100 or 1000 is used in real data. What is the observation here? Can the authors provide some insights into this?

(5) I feel Figure 3 is interesting. As the authors noted, NLL_gen measures diversity, NLL_oracle measures quality. Looking at Figure 3, does this mean GAN model produces higher quality samples than MLE pretrained models, while GAN models also produces less diverse samples than MLE models? This is due to NLL_gen increases after pretraining, while NLL_oracle further decreases after pretraining. However, this conclusion also seems strange. Can the authors provide some discussion on this? 

(6) Can human evaluation be performed since automatic metrics are not reliable enough?
","The sentiment of the review is generally positive, as indicated by the reviewer's satisfaction with the authors' responses and preference for acceptance. The sentiment score is therefore 80. The language used is polite and constructive, with the reviewer thanking the authors and providing detailed feedback without any negative or harsh language. The politeness score is 90.",80,90
"Pros:
- The finding that SVHN has larger likelihood than CIFAR according to networks is interesting. 
- The empirical and theoretical analyses are clear, seem thorough, and make sense.
- Section 5 can provide some insight when the model is too rigid and too log-concave (e.g. Gaussian).
Cons:
- The premises of the analyses are not very convincing, limiting the significance of the paper.
- In particular, Section 4 is a series of empirical analyses, based on one dataset pair. In 3/4 of the pairs the author tried, this phenomenon is not there. Whether the findings generalize to other situations where the phenomenon appears is uncertain. 
- It is good that Section 5 has some theoretical analysis. But I personally find it very disturbing to base it on a 2nd order approximation of a probability density function of images when modeling something as intricate as models that generate images. At least this limitation should be pointed out in the paper.
- Some parts of the paper feel long-winded and aimless.

[Quality]
See above pros and cons.
A few less important disagreement I have with the paper:
- I don't think Glow necessarily is encouraged to increase sensitivity to perturbations. The bijection needs to map training images to a high-density region of the Gaussian, and that aspect would make the model think twice before making the volume term too large.
- Figure 6(a) clearly suggests that the data mean for SVHN and CIFAR are very different, instead of similar.

[Clarity]
In general, the paper is clear and easy to understand given enough reading time, but feels at times long-winded.
Section 2 background takes too much space.
Section 3 too much redundancy -- it just explains that SVHN has a higher likelihood when trained on CIFAR, and a few variations of the same experiment.
Section 4 seems to lack a high-level idea of what it want to prove -- the hypothesis around the volume term is dismissed shortly after, and it ultimately proves that we do not know what is the reason behind the high SVHN likelihood, making it look like a distracting side-experiment.
A few editorial issues:
- On page 4 footnote 2, as far as I know the paper did not define BPD.
- There are two lines of text between Fig. 4 and Fig. 5, which is confusing.

[Originality]
I am not an expert in this specific field (analyzing generative models), but I believe this analysis is novel.
However, there are papers empirically analyzing novelty detection using generative model -- should analyze or at least cite:
    Vít Škvára et al. Are generative deep models for novelty detection truly better? 
    ^ at first glance, their AUROC is never under 0.5, indicating that this phenomenon did not appear in their experiments although a lot of inlier-novelty pairs are tried.
A part of the paper's contribution (section 5 conclusion) seem to overlap with others' work. The section concludes that if the second dataset has small variances, it will get higher likelihood. But this is too similar to the cited findings on page 6 (models assign high likelihood to constant images).

[Significance] 
The paper has a very interesting finding; pointing out and in-depth analysis of negative results should benefit the community greatly.
However, only 1 dataset pair is experimented -- there should be more to ensure the findings generalize, since Sections 3 and 4 rely completely on empirical analysis. According to the conclusions of the paper, such dataset pairs should be easy to find -- just find a dataset that ""lies within"" another. Did you try e.g. CIFAR-100 train and CIFAR-10 test?
Section 5 is based on a 2nd order expansion on the $log p(x)$ given by a deep network -- I shouldn't be the judge of this, but from a realistic perspective this does not mean much.
","The review starts with positive comments about the interesting findings and thorough analyses, but quickly moves to significant criticisms regarding the premises and generalizability of the analyses. The reviewer also points out specific issues with the theoretical basis and the clarity of the paper. The sentiment score is slightly negative due to the balance of positive and negative comments, with more emphasis on the latter. The language used is polite, with constructive feedback and suggestions for improvement, without any rude or harsh language.",-20,80
"Summary: the paper introduces a new way of fine-tuning neural networks. Instead of re-training the whole model or fine-tuning the last few layers, the authors propose to fine-tune a small set of model patches that affect the network at different layers. The results show that this way of fine-tuning is superior to above mentioned typical ways either in accuracy or in the number of tuned parameters in three different settings: transfer learning, multi-task learning and domain adaptation.

Quality: the introduced way of fine-tuning is interesting alternative to the typical last layer re-training. I like that the authors present an intuition behind their approach and justify it by an illustrative example. The experiments are fair, assuming the authors explain the choice of hyper-parameters during the revision.

Clarity: in general the paper is well-written. The discussion of multi-task and domain adaptation parts can be improved though.

Originality: the contributions are novel to my best knowledge.

Significance: high, I believe the paper may facilitate a further developments in the area.

I ask the authors to address the following during the rebuttal stage:
* explain the choice of the hyper-parameters of RMSProp (paragraph under Table 1).
* fix Figure 3, it's impossible to read in the paper-printed version
* explain how the average number of parameters per model in computed in Tables 4 and 5. E.g. 700K params/model in the first column of Table 4 is misleading - I suppose the shared parameters are not taken into account. The same holds for 0 in the second column, etc.
* add a proper discussion for domain adaptation part. The simple ""The results are shown in Table 5"" is not enough. 
* consider leaving the discussion of cost-efficient model cascades out. The presented details are too condensed and do not add value to the paper.
* explain how different resolutions are managed by the same model in the domain adaptation experiments.","The sentiment of the review is positive, as the reviewer appreciates the novel approach and its potential significance in the field. The reviewer also acknowledges the clarity and quality of the paper, although they suggest some improvements. Therefore, the sentiment score is 80. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, making suggestions rather than demands. Thus, the politeness score is 90.",80,90
"This paper proposed a new network structure to learn GAN with incomplete data, and it is a nice extension of AmbientGAN. Two theorems are provided for better understanding the potential effect of the missing values. Improved results compared with state-of-the-art methods on MNIST, CIFAR-10 and CelebA are presented. Overall, the paper is well organized, and the experiment results are sufficient to demonstrate the advantages of the proposed method. I particular like figure5 where AmbientGAN failed in this case.

 Several suggestions about improving the paper. I notice the images used in the experiments are small size. It would be interesting to test the performance on a larger image. Another direction would be testing the robustness of the model, for example, what will happen if the observation is also noisy? Some discussion about the potential extensions will also be helpful. For example, can the proposed network be used to solve the compressive sensing problem with a real value mask instead of binary valued. 

I did not dive into the detail of the prove of theorems. And it seems valid by reading through each step.  Although these two theorems are not directly related to the properties of the proposed network structure. But it does provide some nice intuition.
","The sentiment of the review is positive, as evidenced by phrases like 'nice extension,' 'well organized,' and 'sufficient to demonstrate the advantages.' The reviewer also expresses specific appreciation for Figure 5. Therefore, the sentiment score is 80. The politeness of the language is high, with the reviewer offering constructive suggestions and using polite language throughout, such as 'It would be interesting' and 'Some discussion about the potential extensions will also be helpful.' Thus, the politeness score is 90.",80,90
"Review:

This paper proposes a method for finding optimal architectures for deep neural networks based on a teacher network. The optimal network is found by removing or shrinking layers or adding skip connections. A Bayesian Optimization approach is used by employing a Gaussian Process to guide the search and the acquisition function expected improvement. A special kernel is used in the GP to model the space of network architectures. The method proposed is compared to a random search strategy and a method based on reinforcement learning.
	
Quality: 

	The quality of the paper is high in the sense that it is very well written and contains exhaustive experiments with respect to other related methods

Clarity: 

	The paper is well written in general with a few typos, e.g., 

	""The weights of the Bi-LSTM θ, is learned during the search process. The weights θ determines""

Originality: 

	The proposed method is not very original in the sense that it is a combination of several known techniques. May be the most original contribution is the proposal of a kernel for network architectures based on recurrent neural networks.

	Another original idea is the use of sampling to avoid the problem of doing kernel over-fitting. Something that can be questioned, however, in this regard is the fact that instead of averaging over kernels the GP prediction to account for uncertainty in the kernel parameters, the authors have suggested to optimize a different acquisition function per each kernel. This can be problematic since for each kernel over-fitting can indeed occur, although the experimental results suggest that this is not happening.
	
Significance:

	Why N2N does not appear in all the CIRFAR-10 and CIFAR-100 experiments? This may question the significance of the results.

	It also seems that the authors have not repeated the experiments several times since there are no error bars in the results.
	This may also question the significance of the results. An average over several repetitions is needed to account for the randomness in for example the sampling of the network architectures to learn the kernels.

	Besides this, the authors may want to cite this paper

	Hernández-Lobato, D., Hernandez-Lobato, J., Shah, A., & Adams, R. (2016, June). Predictive entropy search for multi-objective Bayesian optimization. In International Conference on Machine Learning (pp. 1492-1501).	

	which does multi-objective Bayesian optimization of deep neural networks (the objectives are accuracy and prediction time).

Pros:

	- Well written paper.
		
	- Simply idea.

	- Extensive experiments.

Cons:
	
	- The proposed  approach is a combination of well known methods.

	- The significance of the results is in question since the authors do not include error bars in the experiments.","The sentiment of the review is generally positive, as the reviewer acknowledges the high quality of the paper, its well-written nature, and the extensive experiments conducted. However, there are some criticisms regarding the originality of the method and the significance of the results due to the lack of error bars. Therefore, the sentiment score is not fully positive but leans towards the positive side. The politeness of the language used in the review is high, as the reviewer provides constructive feedback and suggestions in a respectful manner without any rude or harsh language.",60,80
"

UPDATE: I've increased my rating based on the authors' thorough responses and the updates they've made to the paper. However, I still have a concern over the static nature of the experimental environments.

=====================

This paper proposes the use of iterative, linguistic corrections to guide (ie, condition and adjust) an RL policy. A major challenge in learning language-guided policies is grounding the language in environment states and agent actions. The authors tackle this challenge with a meta-learning approach.

The approach is fairly complex, blending imitation and supervised learning. It operates on a training set from a distribution of virtual pick-move-place tasks. The policy to be learned operates on this set and collects data, via something close to DAgger, for later supervised learning on the task distribution. The supervised-learning data comprises trajectories augmented with linguistic subgoal annotations, which are referred to as policy ""corrections."" By ingesting its past trajectories and the correction information, the policy is meant to learn to solve the task and to ground the corrections at the same time, end-to-end. Correction annotations are derived from an expert policy.

The idea of guiding a policy through natural language and the requisite grounding of language in environment states and policy actions have been investigated previously: for example, by supervised pretraining on a language corpus, as in the cited work of Andreas et al. (2018). The alternative meta-learning approach proposed here is both well-motivated and original.

Generally, I found the paper clear and easy to read. The authors explain convincingly the utility of guiding policies through language, especially with respect to the standard mechanisms of reward functions (sparse, engineered) and demonstrations (expertise required). The paper is also persuasive on the utility of iterative, interactive correction versus a fully-specified language instruction given a priori. The meta-learning algorithm and training/test setup are both explained well, despite their complexity. On the other hand, most architectural details necessary to reproduce the work are missing, at least from the main text. This includes various tensor dimensions, the structure of the network for perceiving the state, etc.

I like the proposed experimental setting. It enables meta-learning on sequential decision making problems in a partially observable environment, which seems useful to the research community at large. Ultimately, however, this paper's significance is not evident to me, mainly because the proposed method lacks thorough experimental validation. No standard baselines are evaluated on the task (with or without meta-learning), nor is a detailed analysis of the learned policies undertaken. The ablation study is useful, and a good start, but insufficient in my opinion. Unfortunately, the results are merely suggestive rather than convincing.

Some things I'd like to see in an expanded results section before recommending this paper include:
- Comparison to an RL baseline that attempts to learn the full task, without meta-training or language corrections.
- Comparison to a baseline that learns from intermediate rewards. Instead of annotating data with corrections, you could provide +/- scalar rewards throughout each trajectory based on progress towards the goal (since you know the optimal policy). How effective might this be compared to using the corrections?
- Comparison to a baseline that does some kind of pretraining on the language corrections, as in Andreas et al. (2018).
- Quantification of how much meta-training data is required. What is the sample complexity like with/without language corrections?

I also have concerns about the need for near-optimal agents on each task -- this seems very expensive and inefficient. The expert policy is obtained via RL on each individual task using ""ground truth"" rewards. It is not specified what these rewards are, nor is it stated how near to optimal the resulting policy is nor how this nearness affects the overall meta-learning process.

Its unclear to me how the ""full information"" baseline processes and conditions on the full set of subgoals/corrections. Are they read as a single concatenated string converted to one vector by the bi-LSTM?

There also might be an issue with the experimental setup, unless I've misunderstood it. The authors state that ""the agent only needs 2 corrections where the first correction is the location of the goal object and the second is the location of the goal square."" But if the specific rooms, indicated by colors, do not change location from task to task (and they appear not to from all the figures), then the agent can learn the room locations during meta-training and these two ""corrections"" tell it everything it needs to know to solve the task.

Pros:
- Appealing, well-motivated idea for training policies via language.
- Clear, pleasant writing and good communication of a complicated algorithm.
- Good experimental setup that should be useful in other research (except for possible issue with static room locations).

Cons:
- The need for a near-optimal policy for each task. 
- Overall complexity of the training process.
- The so-called corrections are actually linguistic statements of subgoals computed from the optimal policy. There is much talk in the introduction of interactive policy correction by humans, which is an important goal and interesting problem, but the present paper does not actually investigate human interaction. This comes as a letdown after the loftiness of the introduction.
- Various details needed for reproduction are lacking. Maybe they're in the supplementary material; if so, please state that in the main text.
- Major lack of comparisons to alternative approaches.","The sentiment of the review is generally positive, as the reviewer appreciates the well-motivated idea, clear writing, and good experimental setup. However, there are notable concerns about the lack of experimental validation, missing architectural details, and the need for near-optimal policies. Therefore, the sentiment score is 40. The politeness of the language is high, as the reviewer uses polite and constructive language throughout, even when pointing out significant issues. Thus, the politeness score is 80.",40,80
"The paper considers a multiclass classification problem in which labels are grouped in a given number M of subsets c_j, which contain all individual labels as singletons. Training takes place through an active learning setting in which all training examples x_i are initially provided without their ground truth labels y_i. The learner issues queries of the form (x_i,c_j) where c_j is one of the given subsets of labels. The annotator only replies yes/no according to whether the true label y_i of x_i belongs to c_j or not. Hence, for each training example the learner maintains a ""version space"" containing all labels that are consistent with the answers received so far for that example. The active learning process consists of the following steps: (1) use the current learning model to score queries (x_i,c_j); (2) query the best (x_i,c_j); (3) update the model.
In their experiments, the authors use a mini-batched version, where queries are issued and re-ranked several times before updating the model. Assuming the learner generates predictive models which map examples to probability distributions over the class labels, several uncertainty measures can be used to score queries: expected info gain, expected remaining classes, expected decrease in remaining classes. Experiments are run using the Res-18 neural network architecture over CIFAR10, CIFAR100, and Tiny ImageNet, with training sets of 50k, 50k, and 100k examples. The subsets c_j are computed using the Wordnet hierarchy on the label names resulting in 27, 261, and 304 subsets for the three datasets. The experiments show the advantage of performing adaptive queries as opposed to several baselines: random example selection with binary search over labels, active learning over the examples with binary search over the labels, and others. 

This paper develops a natural learning strategy combining two known approaches: active learning and learning with partial labels. The main idea is to exploit adaptation in both choosing examples and queries. The experimental approach is sound and the results are informative. In general, a good experimental paper with a somewhat incremental conceptual contribution.

In (2) there is t+1 on the left-hand side and t on the right-hand side, as if it were an update. Is it a typo?

In 3.1, how is the standard multiclass classifier making use of the partially labeled examples during training?

How are the number of questions required to exactly label all training examples computed? Why does this number vary across the different methods?

What specific partial feedback strategies are used by AQ for labeling examples?

EDC seems to consistently outperform ERC for small annotation budgets. Any intuition why this happens?","The sentiment of the review is generally positive, as it acknowledges the soundness of the experimental approach and the informativeness of the results, despite noting that the conceptual contribution is somewhat incremental. This suggests a sentiment score of around 60. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, making suggestions and asking questions in a respectful manner. This suggests a politeness score of around 90.",60,90
"This paper provides several theoretical and practical insights on the impact of data distribution to adversarial robustness of trained networks. The paper reads well and provides analysis on two datasets MNIST and CIFAR10. I particularly like the result demonstrating that a lossless transformation on the data distribution could significantly impact the robustness of an adversarial trained models. The idea of using smoothness and saturation to bridge the gap between the MNIST and CIFAR10 datasets was also very interesting. One thing that is not clear from the paper is how one could use the findings from this paper and put it into practice. In other words, it would help if the authors could provide some insights on how to improve a model robustness w.r.t the changes in the data distribution. The authors did an attempt toward this in section 5, but that seems to only cover three factors that do not cause the difference in robustness.","The sentiment of the review is generally positive, as the reviewer appreciates the theoretical and practical insights provided by the paper, as well as specific results and ideas. However, there is a constructive criticism regarding the practical application of the findings, which slightly tempers the overall positivity. Therefore, the sentiment score is 70. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, making suggestions in a respectful manner. Thus, the politeness score is 90.",70,90
"
- Summary
This paper proposes a multi-objective evolutionary algorithm for the neural architecture search. Specifically, this paper employs a Lamarckian inheritance mechanism based on network morphism operations for speeding up the architecture search. The proposed method is evaluated on CIFAR-10 and ImageNet (64*64) datasets and compared with recent neural architecture search methods. In this paper, the proposed method aims at solving the multi-objective problem: validation error rate as a first objective and the number of parameters in a network as a second objective.

- Pros
  - The proposed method does not require to be initialized with well-performing architectures.
  - This paper proposes the approximate network morphisms to reduce the capacity of a network (e.g., removing a layer), which is reasonable property to control the size of a network for multi-objective problems.

- Cons
  - Judging from Table 1, the proposed method does not seem to provide a large contribution. For example, while the proposed method introduced the regularization about the number of parameters to the optimization, NASNet V2 and ENAS outperform the proposed method in terms of the accuracy and the number of parameters.
  - It would be better to provide the details of the procedure of the proposed method (e.g., Algorithm 1 and each processing of Algorithm 1) in the paper, not in the Appendix.
  - In the case of the search space II, how many GPU days does the proposed method require? 
  - About line 10 in Algorithm 1, how does the proposed method update the population P? Please elaborate on this procedure.
","The sentiment of the review is mixed. The reviewer acknowledges some positive aspects of the paper, such as the proposed method not requiring initialization with well-performing architectures and the reasonable property of network morphisms. However, the reviewer also points out significant drawbacks, such as the proposed method not providing a large contribution compared to other methods and the lack of detailed procedural information in the main text. Therefore, the sentiment score is slightly negative. The language used in the review is polite and constructive, with suggestions for improvement rather than harsh criticism, leading to a high politeness score.",-20,80
"In this paper, the authors claim that they are able to update the generator better to avoid generator mode collapse and also increase the stability of GANs training by indirectly increasing the entropy of the generator until it matches the entropy of the original data distribution using functional gradient methods.

The paper is interesting and well written. However, there is a lot of work coming out in the field of GANs currently, so I am not able to comment on the novelty of this regularization approach, and I am interested to know how this method performs when compared to other techniques to avoid mode collapse such as feature matching and mini-batch discrimination, etc.  

","The sentiment of the review is generally positive, as indicated by phrases like 'interesting and well written.' However, the reviewer does express some reservations about the novelty of the approach and requests comparisons with other techniques, which slightly tempers the overall positivity. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer uses polite and constructive language throughout, such as 'I am interested to know,' which indicates a polite tone. Therefore, the politeness score is 80.",60,80
"This paper investigates adversarial examples for audio data. The standard defense techniques proposed for images are studied in the context of audio. It is shown that these techniques are somewhat robust to adversarial attacks, but fail against adaptive attacks. A method exploiting the temporal dependencies of the data is then presented and shown to be robust to adversarial examples and to adaptive attacks.

The paper addresses an important issue, and the two main findings of the paper, the transformation methods used in computer vision are not useful against audio adversarial example and using temporal dependencies improves the defense capability are significant. The proposed TD method is novel.

The first part of the paper is easy to read (Section 1-3), but Section 4 is hard to follow, for the following reasons:
* Section 4.1 presents the metrics used in the evaluation, which is nice. But in the following subsections, other metrics are used: effectiveness ratio, detection rate and relative perturbation. They should be clearly defined in 4.1, and the authors should discuss why they used these metrics.
* Section 4.2 should be reorganized as it is hard to follow: there are three types of attack, so one subsection per attack should make the section clearer.
* It's not always clear what each attack is doing and why it is used. I suggest the authors to have a separate subsection with the description of each attack and the motivation of why it is used.

Because of the above, it's hard to clearly assess the performance of each method for each attack, it would be better to have a Table that summarizes the results for the transformation methods. Also, I don't understand the statement in 4.2: ""We report that the autoencoder works fine for transforming benign instances (57.6% WER in Common Voice compared to 27.5%)"": given that it's not an attack, the PER should be the same with and without transform, as we don't want the transform to affect non-adversarial examples ? Please clarify that.
The experiments on the proposed TD method are clear enough to show the viability of the approach.

Overall, the findings of this paper are significant and it is good step towards audio adversarial examples defense. But the experimental part is hard to follow and does not bring a clear picture. I am still willing to accept the paper if the authors improve and clarify Section 4.

Revision after rebuttal:
The new version is definitely clearer and easier to read, hence I support the paper for acceptance and change my rating to 7. 
There are still minor improvements that can be done in Section 4 to improve the overall clarity:
* About the metrics, the ""Average attack success rate"" and the ""Target command recognition rate"" should be clearly defined, probably under the description of the attack methods.
* The Adaptive attack approach could be introduced unter ""Attack methods"" in 4.1.
* Table 4 is not easy to read, the authors should improve it.
* The first paragraph in Section 4 (""The presentation flows ..."") is very interesting, but almost reads like a conclusion, so maybe the authors could move that to the end of Section 4 or to Section 5.","The review starts with a positive sentiment, acknowledging the importance of the issue addressed and the significance of the findings. The reviewer appreciates the novelty of the proposed method and the clarity of the first part of the paper. However, the reviewer points out several issues with Section 4, providing detailed suggestions for improvement. The language used is constructive and polite, aiming to help the authors improve their work. After the rebuttal, the reviewer acknowledges the improvements made and supports the paper's acceptance, while still suggesting minor improvements. This indicates a positive sentiment overall and a polite tone throughout the review.",70,90
"This is an exciting paper with a simple idea for better representing audio data so that convolutional models such as generative adversarial networks can be applied. The authors demonstrate the reliability of their method on a large dataset of acoustic instruments and report human evaluation metrics. I expect their proposed method of preprocessing audio to become standard practice.

Why didn't you train a WaveNet on the high-resolution instantaneous frequency representations? In addition to conditioning on the notes, this seems like it would be the right fair comparison. 

I'm still not clear on unrolled phase which is central to this work. If you can, spend more time explaining this in detail, maybe with examples / diagrams? In figure 1,  in unrolled phase, why is time in reverse?

Small comments:

- Figure 1 & 2: label the x-axis as time. Makes it a lot easier to understand.

- I appreciate the plethora of metrics. The inception score you propose is interesting. Very cool that number of statistically-different bins tracks human eval!

- sentence before sec 2.2, and other small grammatical mistakes. Reread every sentence carefully for grammar. 

- Figure 5 is low-res. Please fix. All the other figures are beautiful - nice work!","The review starts with a very positive sentiment, describing the paper as 'exciting' and the idea as 'simple' yet effective. The reviewer expects the proposed method to become standard practice, which indicates a high level of approval. The sentiment score is therefore very high. The language used is polite and constructive, with phrases like 'I appreciate' and 'nice work,' even when pointing out areas for improvement. The reviewer also provides specific, actionable feedback in a respectful manner, which contributes to a high politeness score.",90,85
"This paper tries to analyze the impact of the staleness on machine learning models in different settings, including model complexity, optimization methods or the number of workers. In this work, they study the convergence behaviors of a wide array of ML models and algorithms under delayed updates, and propose a new convergence analysis of asynchronous SGD method for non-convex optimization.

The following are my concerns:
1. ""For CNNs and DNNs, the staleness slows down deeper models much more than shallower counterparts."" I think it is straightforward. I want to see the theoretical analysis of the relation between model complexity and staleness.  
2. ""Different algorithms respond to staleness very differently"".  This finding is quite interesting. Is there any theoretical analysis of this phenomenon?  
3. The ""gradient coherence""  in the paper is not new. I am certain that ""gradient coherence"" is very similar to the ""sufficient direction"" in [1]. 
4. What is the architecture of the network? in the paper, each worker p can communicate with other workers p'. Does it mean that it is a grid network? or it is just a start network. 
5. in the top of page 3, why the average delay under the model is 1/2s +1, isn't it (s-1)/2? 
6.  on page 5, ""This is perhaps not surprising, given the fact that deeper models pose more optimization challenges even under the sequential settings."" why it is obvious opposite to your experimental results in figure 1(a)? Could you explain why shallower CNN requires more iterations to get the same accuracy? it is a little counter-intuitive.
7. I don't understand what does ""note that s = 0 execution treats each worker’s update as separate updates instead of one large batch in other synchronous systems"" mean in the footnote of page 5.


Above all, this paper empirically analyzes the effect of the staleness on the model and optimization methods. It would be better if there is some theoretical analysis to support these findings.

[1] Training Neural Networks Using Features Replay  https://arxiv.org/pdf/1807.04511.pdf


===after rebuttal===
All my concerns are addressed. I will upgrade the score.
","The sentiment of the review is initially neutral to slightly negative, as the reviewer lists several concerns and requests for additional theoretical analysis. However, the sentiment improves significantly after the rebuttal, indicating that the reviewer's concerns were addressed satisfactorily. Therefore, the overall sentiment score is around 20. The politeness of the language is generally neutral to polite. The reviewer uses phrases like 'I want to see,' 'Is there any,' and 'Could you explain,' which are polite requests for clarification and additional information. Therefore, the politeness score is around 50.",20,50
"This paper proposes a List Conditional Variational Autoencoder approach for the slate recommendation problem. Particularly, it learns the joint document distribution on the slate conditioned on user responses, and directly generates full slates. The experiments show that the proposed method surpasses greedy ranking approaches.

Pros:
+ nice motivation, and the connection with industrial recommendation systems where candidate nomination and ranker is being used is engaging
+ it provides a conditional generative modeling framework for slate recommendation
+ the simulation experiments very clearly show that the expected number of clicks as obtained by the proposed List-CVAE is much higher compared to the chosen baselines. A similar story is shown for the YOOCHOOSE challenge dataset.

Cons: 
- Do the experiments explicitly compare with the nomination & ranking industry standard?
- Comparison with other slate recommendation approaches besides the greedy baselines?
- Comparison with non-slate recommendation models of Figure 1?

Overall, this is a very nicely written paper, and the experiments both in the simulated and real dataset shows the promise of the proposed approach.","The sentiment of the review is generally positive, as indicated by phrases like 'nice motivation,' 'engaging,' and 'very nicely written paper.' The reviewer acknowledges the strengths of the paper and provides constructive criticism in a balanced manner. Therefore, the sentiment score is 80. The politeness of the language is also high, as the reviewer uses polite and respectful language throughout, such as 'nice motivation' and 'very nicely written paper,' while offering suggestions for improvement in a non-confrontational way. Thus, the politeness score is 90.",80,90
"This paper proposed a new training algorithm, G-SGD, by exploring the positively scale-invariant space for relu neural networks. The basic idea is to identify the basis paths in the path graph, and convert the weight update in SGD to the weight rescaling in G-SGD. My major concerns are as follows:

1. Empirical significance of G-SGD: While the idea of exploring the structures of relu neural networks for training based on group theory on graphs is interesting, I do not see significant improvement over SGD. The accuracy differences in Table 1 are marginal, training/testing behaviors in Fig. 3 are very similar, and more importantly there is no evidence to support the claims ""with little extra cost"" in the abstract/Sec. 4.3 in terms of computation. Therefore, I do not see the truly contribution of the proposed method.

PS: After reading the revision, I am happy to see the results on computational time that support the authors' claim. However, I still have doubts on the significance of the improvement on CIFAR10 and CIFAR100, because the performance is heavily dependent on network architectures. In my experience, using resnet101 it can easily achieve >96% accuracy. So can you achieve better than this using G-SGD? The training and testing behaviors on both datasets somehow show improvement over SGD, which I take it more importantly than just those numbers. Therefore, I am glad to raise my score.

2. In Alg. 3 I do not quite understand how to apply step 3 to step 4. The connection needs more explanation.","The sentiment of the review is mixed but leans towards positive. Initially, the reviewer expresses significant concerns about the empirical significance of the proposed method, which would suggest a negative sentiment. However, after reading the revision, the reviewer acknowledges improvements and raises their score, indicating a more positive sentiment. Therefore, the sentiment score is 20. The politeness of the language is quite high. The reviewer uses phrases like 'I am happy to see,' 'I still have doubts,' and 'I am glad to raise my score,' which are polite and constructive. Therefore, the politeness score is 80.",20,80
"The paper presents a new convolution-like operation for parameterized manifolds, and demonstrates its effectiveness on learning problems involving spherical signals. The basic idea is to define the MeshConvolution as a linear combination (with learnable coefficients) of differential operators (identity, gradient, and Laplacian). These operators can be efficiently approximated using the 1-hop neighbourhood of a vertex in the mesh.

In general I think this is a strong paper, because it presents a simple and intuitive idea, and shows that it works well on a range of different problems. The paper is well written and mostly easy to follow. The appendix contains a wealth of detail on network architectures and training procedures.

What is not clear to me is how exactly the differential operators are computed, and how the MeshConvolution layer is implemented. The authors write that ""differential operators can be efficiently computed using Finite Element basis, or derived by Discrete Exterior Calculus"", but no references or further detail is provided. The explanation of the derivative computation is:
""The first derivative can be obtained by first computing the per-face gradients, and then using area-weighted average to obtain per-vertex gradients. The dot product between the per-vertex gradient value and the corresponding x and y vector fields are then computed to acquire grad_x F and grad_y F.""
What are per-face gradients and how are they computed? Is the signal sampled on vertices or on faces? What area is used for weighting? What is the exact formula? What vector fields are you referring to? (I presume these are the coordinate vector fields). In eq. 5, what are F_i and F_j? What is the intuition behind the cotangent formula (eq. 5), and where can I read more? etc.

Please provide a lot more detail here, delegating parts to an appendix if necessary. Providing code would be very helpful as well.

A second (minor) concern I have is to do with the coordinate-dependence of the method. Because the MeshConvolution is defined in terms of (lat / lon) coordinates in a non-invariant manner, and the sphere does not admit a global chart, the method will have a singularity at the poles. This is confirmed by the fact that in the MNIST experiment, digits are rotated to the equator ""to prevent coordinate singularity at the poles"". I think that for many applications, this is not a serious problem, but it would still be nice to be transparent and mention this as a limitation of the method when comparing to related work.

In ""Steerable CNNs"", Cohen & Welling also used a linear combination of basis kernels, so this could be mentioned in the related work under ""Reparameterized Convolutional Kernel"".

To get a feel for the differential operators, it may be helpful to show the impulse response (at different positions on the sphere if it matters).

In experiment 4.1 as well as in the introduction, it is claimed that invariant/equivariant models cannot distinguish rotated versions of the same input, such as a 6 and a 9. Although indeed an invariant model cannot, equivariant layers do preserve the ability to discriminate transformed versions of the same input, by e.g. representing a 9 as an upside-down 6. So by replacing the final invariant pooling layer and instead using a fully connected one, it should be possible to deal with this issue in such a network. This should be mentioned in the text, and could be evaluated experimentally.

In my review I have listed several areas for improvement, but as mentioned, overall I think this is a solid paper.","The sentiment of the review is generally positive, as indicated by phrases like 'I think this is a strong paper' and 'overall I think this is a solid paper.' The reviewer acknowledges the strengths of the paper, such as its simplicity, intuitiveness, and effectiveness across different problems. However, the reviewer also points out several areas for improvement, particularly regarding the clarity of the differential operators' computation and the coordinate-dependence of the method. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, making suggestions and asking questions in a respectful manner.",70,90
"This paper introduces a verifier that obtains improvement on both the precision of the incomplete verifiers and the scalability of the complete verifiers. The proposed approaches combines over-parameterization, mixed integer linear programming, and linear programming relaxation. 

This paper is well written and well organized. I like the simple example exposed in section 2, which is a friendly start. However, I begun to lose track after that. As far as I can understand, the next section listed several techniques to be deployed. But I failed to see enough justification or reasoning why these techniques are important. My background is more theoretical, but I'm looking for theorems here, considering the complicatedness of neural network. All I am looking for is probably some high level explanation.

Empirically, the proposed approach is more robust while time consuming that the AI2 algorithm. However, the contribution and the importance of this paper still seems incremental to me.  I probably have grumbled too much about the lack of reasonings. As this paper is purely empirical, which is totally fine and could be valuable and influential as well.  In that case, I found the current experiment unsatisfying and would love to see more extensive experimental results. 
","The sentiment of the review is mixed. The reviewer appreciates the writing and organization of the paper, as well as the friendly start in section 2. However, they express confusion and dissatisfaction with the lack of justification for the techniques used and the incremental nature of the contribution. They also find the experimental results unsatisfying and request more extensive experiments. Therefore, the sentiment score is slightly positive but tempered by significant criticism. The politeness of the language is high, as the reviewer uses polite phrases such as 'I like,' 'I probably have grumbled too much,' and 'which is totally fine and could be valuable and influential as well,' even when expressing criticism.",20,80
"Summary:
This paper presents a Two-Timescale Network (TTN) that enables linear methods to be used to learn values. On the slow timescale non-linear features are learned using a surrogate loss. On the fast timescale, a value function is estimated as a linear function of those features. It appears to be a single network, where one head drives the representation and the second head learns the values.  They investigate multiple surrogate losses and end up using the MSTDE for its simplicity, even though it provides worse value estimates than MSPBE as detailed in their experiments.  They provide convergence results - regular two-timescale stochastic approximation results from Borkar, for the two-timescale procedure and provide empirical evidence for the benefits of this method compared to other non-linear value function approximation methods.

Clarity and Quality:
The paper is well written in general, the mathematics seems to be sound and the experimental results appear to be thorough. 

Originality:
Using two different heads, one to drive the representation and the second to learn the values appears to be an architectural detail. The surrogate loss to learn the features coupled with a linear policy evaluation algorithm appear to be novel, but does not warrant, in my opinion, the novelty necessary for publication at ICLR. 

The theoretical results appear to be a straightforward application of Borkar’s two-timescale stochastic approximation algorithm to this architecture to get convergence. This therefore, does not appear to be a novel contribution.

You state after equaltion (3) that non-linear function classes do not have a closed form solution. However, it seems that the paper Convergent Temporal-Difference Learning with Arbitrary Smooth Function Approximation does indeed have a closed form solution for non-linear function approximators when minimizing the MSPBE (albeit making a linearity assumption, which is something your work seems to make as well). 

The work done in the control setting appears to be very similar to the experiments performed in the paper: Shallow Updates for Deep Reinforcement Learning.

Significance:
Overall, I think that the paper is well written and the experimental evaluation is thorough. However, the novelty is lacking as it appears to be training using a multi-headed approach (which exists) and the convergence results appear to be a straightforward application of Borkars two-timescale proof. The novelty therefore appears to be using a surrogate loss function for training the features which does not possess the sufficient novelty in my opinion for ICLR. 

I would suggest the authors' detail why their two-timescale approach is different from that of Borkars. Or additionally add some performance guarantee to the convergence results to extend the theory. This would make for a much stronger paper.","The sentiment of the review is mixed but leans towards the positive side. The reviewer acknowledges that the paper is well-written, the mathematics is sound, and the experimental results are thorough. However, they also express concerns about the novelty of the work, which they believe is insufficient for publication at ICLR. Therefore, the sentiment score is 20. The language used in the review is polite and constructive, offering specific suggestions for improvement without being dismissive or rude. Therefore, the politeness score is 80.",20,80
"This paper proposed a variational autoencoder-based method for semi-supervised dependency parsing. Given an input sentence s, an LSTM-based encoder generates a sentence embedding z, and a NN of Kiperwasser & Goldberg (2016) generates a dependency structure T. Gradients over the tree encoder are approximated by (1) adding a perturbation matrix over the weight matrix and (2) relax dynamic programming-based parsing algorithm to a differentiable format. The decoder combines standard LSTM and Graph Convolutional Network to generate the input sentence from z and T. The authors evaluated the proposed method on three languages, using 10% of the original training data as labeled and the rest as unlabeled data.

Pros
1. I like the idea of this sentence->tree->sentence autoencoder for semi-supervised parsing. The authors proposed a novel and nice way to tackle key challenges in gradient computation. VAE involves marginalization over all possible dependency trees, which is computationally infeasible, and the proposed method used a Gumbel-Max trick to approximate it. The tree inference procedure involves non-differentiable structured prediction, and the authors used a peaked-softmax method to address the issue. The whole model is fully differentiable and can be thus trained end to end.

2. The direction of semi-supervised parsing is useful and promising, not only for resource-poor languages, but also for popular languages like English. A successful research on this direction could be potentially helpful for lots of future work.

Cons, and suggestions on experiments
My main concerns are around experiments. Overall I think they are not strong enough to demonstrate that this paper has sufficient contribution to semi-supervised parsing. Below are details.

1. The current version only used 10% of original training data as labeled and the rest as unlabeled data. This makes the reported numbers way below existing state-of-the-art performance. For example, the SOTA UAS on English PTB has been >95%. Ideally, the authors should be able to train a competitive supervised parser on full training data (English or other languages), and get huge amount of unlabeled data from other sources (e.g. News) to further push up the performance. The current setting makes it hard to justify how useful the proposed method could be in practice.

2. The best numbers from the proposed model is lower than baseline (Kipperwasser & Goldberg) on English, and only marginally better on Swedish. This probably means the supervised baseline is weak, and it's hard to tell if the gains from VAE will retain if applied to a stronger supervised.

3. A performance curve with different amount of labeled and unlabeled data would be useful to better understand the impact of semi-supervised learning.

4. What's the impact of perturbation? One could simply use T=Eisner(W) as approximation. Did you observe any significant benefits from sampling?

Other questions
1. What's the impact of keeping the tree constraint on dependencies during backpropagation?  Have you tried removing the tree constraint like previous work?

2. Are sentence embedding and trees generated from two separate LSTM encoders? Are there any parameter sharing between the two?

","The sentiment of the review is generally positive, as the reviewer appreciates the novel approach and the potential impact of the research. This is evident from the positive remarks in the 'Pros' section. However, the sentiment is tempered by significant concerns about the experimental validation, which are detailed in the 'Cons' section. Therefore, the sentiment score is 30. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, even when pointing out weaknesses. This is reflected in the use of phrases like 'I like the idea,' 'The direction is useful and promising,' and 'My main concerns are around experiments,' which indicate a respectful and considerate tone. Therefore, the politeness score is 80.",30,80
"This paper proposed a new class of meta-algorithm for reinforcement learning and proved the monotone improvement for a local maximum of the expected reward, which could be used in deep RL setting. The framework seems to be quite general but does not include any specific example, like what non-linear dynamical model in detail could be included and will this framework cover the classical MDP setting? In theory, the dynamical model needs to satisfy L-Lipschitz. So which dynamical model in reality could satisfy this assumption? It seems that the focus of this paper is theoretical side. But the only guarantee is the non-decreasing value function of the policy. In RL, people may be more care about the regret or sample complexity. Previous model-based work with simpler model already can have such strong guarantees, such as linear dynamic (Y. Abbasi-Yadkori and Cs. Szepesvari (2011)), MDP (Agrawal and Jia (2017)). What kind of new insights will this framework give when the model reduces to simpler one (linear model)?

In practical implementation, the authors designed a Stochastic Lower Bound Optimization. Is there any convergence rate guarantee for this stochastic optimization? And also neural network is used for deep RL. So there is also no guarantee for the actual algorithm which is used?

Minor:

1. In (3.2), what norm is considered here?
2. In page 4, the authors mentioned their algorithm can be viewed as an extension of the optimism-in-face-of-uncertainty principle to non-linear parameterized setting. This is a little bit confused. How this algorithm can be viewed as OFU principle? How does it recover the result in linear setting (Y. Abbasi-Yadkori and Cs. Szepesvari (2011))?
3. The organization could be more informative. For example, Section 1 has 13 paragraphs but without any subsection.

Y. Abbasi-Yadkori and Cs. Szepesvari, Regret Bounds for the Adaptive Control of Linear Quadratic Systems, COLT, 2011.
Shipra Agrawal and Randy Jia. Optimistic posterior sampling for reinforcement learning: worst-case regret bounds. NIPS, 2017","The sentiment of the review is mixed, leaning towards neutral. The reviewer acknowledges the generality and theoretical contributions of the paper but raises significant concerns about the lack of practical examples, specific guarantees, and comparisons to existing work. This results in a sentiment score of -20. The language used is polite and constructive, with the reviewer asking questions and making suggestions without being dismissive or rude, leading to a politeness score of 80.",-20,80
"The authors develop a tree structured extension to the recently proposed recurrent switching linear dynamical systems. Like switching linear dynamical systems (sLDS) the proposed models capture non-linear dynamics by switching between a collection of linear regimes. However, unlike SLDS, the transition between the regimes is a function of a latent tree as well as the preceding continuous latent state. Experiments on synthetic data as well as neural spike train data are presented to demonstrate the utility of the model.

The paper is clearly written and easy to read. The tree structured model (TrSLDS) is a sensible extension to rSLDS. While one wouldn’t expect TrSLDS to necessarily fit the data any better than rSLDS, the potential for recovering multi-scale, possibly more interpretable decompositions of the dynamic process is compelling. 

While the authors do provide some evidence of being able to recover such multi-scale structures, overall the experiments are underwhelming and somewhat sloppy. First, to understand whether the sampler is mixing well, it would be nice to include an experiment where the true dynamics and the entire latent structure (including the discrete states) are known, and then to examine how well this ground-truth structure is recovered. Second, for the results presented in section 5, how many iterations was the sampler run for? In the figures, what is being visualized?, the last sample?, the MAP sample? or something else? I am not sure what to make of the real data experiment in section 5.3. Wouldn’t rSLDS produce nearly identical results? What is TrSLDS buying us in this scenario? Do the higher levels of the tree capture interesting low resolution dynamics that are not shown for some reason? 

My other big concern is scalability. To use larger number of discrete states one would need deeper (or wider if the binary requirement is relaxed) trees. How well does the sampler scale with the number of discrete states? How long did the sampler take for the various 4-state results presented in the paper? 

Minor:
a) There is a missing citation in the first para fo Section 5. 
b) Details of message passing claimed to be in the supplement are missing.

============
There are interesting ideas in this paper. However, experimental section could better highlight the benefits afforded by the model and scalability concerns need to be addressed.

","The sentiment of the review is mixed. The reviewer acknowledges the clarity and potential of the proposed model, which is positive, but also points out significant shortcomings in the experimental section and scalability concerns, which are negative. Therefore, the sentiment score is around 20. The language used is polite and constructive, offering specific suggestions for improvement without being rude or dismissive, so the politeness score is 80.",20,80
"Overview:
I thank the authors for their interesting and detailed work in this paper. I believe it has the potential to provide strong value to the community interested in using VAEs with an explicit and simple parameterization of the approximate posterior and likelihood as Gaussian. Gaussianity can be appropriate in many cases where no sequential or discrete structure needs to be induced in the model. I find the mathematical arguments interesting and enlightening. However, the authors somewhat mischaracterize the scope of applicability of VAE models in contemporary machine learning, and don't show familiarity with the broad literature around VAEs outside of this case (that is, where a Gaussian model of the output would be manifestly inappropriate). Since the core of the paper is valuable and salvageable from a clarity standpoint, my comments below are geared towards what changes the authors may make to move this paper into the ""pass"" category.

Pros: 
- Mathematical insights are well reasoned and interesting. Based on the insight from the analysis in the supplementary materials, the authors propose a two-stage VAE which separate learning the a parsimonious representation of the low-dimensional (lower than the ambient dimension of the input space), and the training a second VAE to learn the unknown approximate posterior. The two-stage training procedure is both theoretically motivated and appears to enhance the output quality of VAEs w.r.t. FID score, making them rival GAN architectures on this metric.

Cons:
- The title and general tone of the paper is too broad: it is only VAE models with Gaussian approximate posteriors and likelihoods. This is hardly the norm for most applications, contrary to the claims of the authors. VAEs are commonly used for discrete random variables, for example. Many cases where VAEs are applied cannot use a Gaussian assumption for the likelihood, which is the key requirement for the proofs in the supplement to be valid (then, the true posterior is also Gaussian, and the KL divergence between that and the approximate posterior can be driven to zero during optimization--clearly a Gaussian approximate posterior will never have zero KL divergence with a non-Gaussian true posterior).
- None of the proofs consider the approximation error garnered by only having access to empirical samples through a sample of the ground truth population. (The ground-truth distribution must be defined with respect to the population rather just the dataset in hand, otherwise we lose all generalizability from a model.) Moreover, the proofs hold asymptotically. Generalization bounds and error from finite time approximations are very pertinent issues and these are ignored by the presented analyses. Such concerns have motivated many of the recent developments in approximate posterior distributions. Overall, the paper contains little evidence of familiarity with the recent advances in approximate Bayesian inference that have occurred over the past two years.
- A central claim of the paper is that the two-stage VAE obviates the need for highly adaptive approximate posteriors. However, no comparison against those models is done in the paper. How does a two-stage VAE compare against one with, e.g., a normalizing flow approximate posterior? I acknowledge that the purpose of the paper was to argue for the Gaussianity assumption as less stringent than previously believed, but all of the mathematical arguments take place in an imagined world with infinite time and unbounded access to the population distribution. This is not really the domain of interest in modern computational statistics / machine learning, where issues of generalization and computational efficiency are paramount.
- While the mathematical insights are well developed, the specifics of the algorithm used to implement the two-stage VAE are a little opaque. Ancestral sampling now takes place using latent samples from a second VAE. An algorithm box is badly needed for reproducibility.

Recommendations / Typos

I noted a few typos and omissions that need correction.

- Generally, the mathematical proofs in section 7 of the supplement are clear. At the top of page 11, though, the paragraph correctly begins by stating that the composition of invertible functions is invertible, but fails to establish that G is also invertible. Clearly it is so by construction, but the explicit reasons should be stated (as a prior sentence promises), and so I assume this is an accidental omission.
- The title of Section 8.1 has a typo: clearly is it is the negative log of p_{theta_t} (x) which approaches its infimum rather than p_{theta_t} (x) approaching negative infinity.
- Equation (4): the true posterior has an x as its argument instead of the latent z.
- Missing parenthesis under Case 2 and wrong indentation. This analysis also seems to be cut off. Is the case r > d relevant here?

* EDIT: I have read the authors' detailed response. It has clarified a few key issues, and convinced me of the value to the community for publication in its present (slightly edited according to the reviwers' feedback) form. I would like to see this published and discussed at ICLR and have revised my score accordingly. *","The sentiment of the review is generally positive, as the reviewer acknowledges the interesting and detailed work of the authors and believes it has strong value to the community. The sentiment score is slightly tempered by the constructive criticism provided, but the overall tone remains positive, especially with the final edit indicating support for publication. The politeness of the language is very high, as the reviewer uses polite and respectful language throughout, even when pointing out areas for improvement. The reviewer thanks the authors, acknowledges their efforts, and provides detailed and constructive feedback in a courteous manner.",70,90
"The paper proposes a method to check if a given point is a stationary point or not (if not, it provides a descent direction), and then classify stationary points as either local min or second-order stationary. The method works for a specific non-differentiable loss.  In the worst case, there can be exponentially many flat directions to check (2^L), but usually this is no the case.

Overall, I'm impressed. The analysis seems solid, and a lot of clever ideas are used to get around issues (such as exponential number of regions, and non-convex QPs that cannot be solved by the S-procedure or simple tricks). A wide-variety of techniques are used: non-smooth analysis, recent analysis of non-convex QPs, copositive optimization.

The writing is clear and makes most arguments easy to follow.

There are some limitations:

(1) the technical details are hard to follow, and most are in a lengthy appendix, which I did not check

(2) there was no discussion of robustness. If I find a direction eta for which the directional derivative is zero, what do you mean by ""zero""? This is implemented on a computer, so we don't really expect to find a directional derivative that is exactly zero.  I would have liked to see some discussions with epsilons, and give me a guarantee of an epsilon-SOSP or some kind of notion.  In the experiments, this isn't discussed (though another issue is touched on a little bit: you wanted to find real stationary points to test, but you don't have exactly stationary points, but rather can get arbitrarily close).  To make this practical, I think you need a robust theory.

(3) The numerical simulations mainly provided some evidence that there are usually not too many flat directions, but don't convince us that this is a useful technique on a real problem.  The discussion about possible loss functions at the end was a bit opaque.  Furthermore, if you can't find a dataset/loss, then why is this technique useful?

The paper is interesting and novel enough that despite the limitations, I am supportive of publishing it. It introduces new ideas that I find refreshing. The technique many not ever make it into the state-of-the-art algorithms, but I think the paper has intellectual value regardless of practical value.

In short, quality = high, clarity=high, originality=very high, and significance=hard-to-predict","The sentiment of the review is highly positive, as indicated by phrases such as 'Overall, I'm impressed,' 'The analysis seems solid,' and 'The writing is clear.' The reviewer acknowledges the limitations but still supports the publication, which further underscores the positive sentiment. Therefore, the sentiment score is 80. The politeness of the language is also high, as the reviewer uses polite and constructive language throughout the review, such as 'I would have liked to see,' 'To make this practical, I think,' and 'The paper is interesting and novel enough.' Therefore, the politeness score is 90.",80,90
"## Overview

This paper proposes a new way to stabilize the training process of GAN by regularizing the Discriminator to be robust to adversarial examples. Specifically, this paper proves that a discriminator which is robust to adversarial attacks also leads to a robust minimax objects. Authors provide theoretical analysis about the how the robustness of the Discriminator affects the properties of the objective function, and the proposed regularization term provides an efficient and effective way to regularize the discriminator to be robust. However, it does not build connection between the robustness of the Discriminator and why it can provide meaningful gradient to the Generator. Experimental results demonstrate the effectiveness of the proposed method. This paper is easy to understand.


## Drawbacks
There are some problems in this paper. First, this paper is not highly motivated and lacks of intuition. I can hardly understand why the robustness can stabilize the training of GAN. Will it solve the problem of gradient vanishing problem or speed up the convergence of GAN? The toy example in Sec. 4.2 shows that it can regularize the Discriminator to provide a meaningful gradient to Generator, but no theoretical analysis is provided. The main gap between them is that the smoothness of D around the generated data points does not imply the effectiveness of gradients. Second, the theoretical analysis is inconsistent with the experimental settings. Theorem 4.3 holds true when f is non-positive, but WGAN’s loss function can be positive and this paper does not give any details about this part. Third, in Sec. 4.2, I can hardly distinguish the difference between robust loss, robust discriminator and regularized objectives.

Besides, there are lots of typos in this paper. In Sec 3, Generative Adversarial Networks part, the notations of x and z are quiet confusing. In Definition 3.2, d which measures the distance between network outputs is not appeared above.

## Summarization
Generally, this paper provides a novel way to stabilize the training of GAN. However, it does not illustrate its motivation clearly and no insight is provided.

## After rebuttal
Some of the issues are addressed. So I change my rating to 6.
","The sentiment of the review is mixed but leans towards positive. The reviewer acknowledges the novelty and effectiveness of the proposed method but points out several significant drawbacks and areas for improvement. Therefore, the sentiment score is 20. The language used is generally polite and constructive, with phrases like 'I can hardly understand' and 'there are some problems' indicating a respectful tone while providing critical feedback. Thus, the politeness score is 60.",20,60
"PAPER SUMMARY
-------------

The paper proposes a method for evaluating the failure probability of a learned agent, which is important in safety critical domains. 

Using plain Monte Carlo for this evaluation can be too expensive, since discovering a failure probability of epsilon requires on the order of 1/epsilon samples. Therefore the authors propose an adversarial approach, which focuses on scenarios which are difficult for the agent, while still yielding unbiased estimates of failure probabilities. 

The key idea of the proposed approach is to learn a failure probability predictor (FPP). This function attempts to predict at which initial states the system will fail. This function is then used in an importance sampling scheme to sample the regions with higher failure probability more often, which leads to higher statistical efficiency.
Finding the FPP is itself a problem which is just as hard as the original problem of estimating the overall failure probability. However, the FPP can be trained using data from different agents, not just the final agent to be evaluated (for instance the data from agent training, containing typically many failure cases). The approach hinges on the assumption that these agents tend to fail in the same states as the final agent, but with higher probability. 

The paper shows that the proposed method finds failure cases orders of magnitude faster than standard MC in simulated driving as well as a simulated humanoid task. Since the proposed approach uses data acquired during the training of the agent, it has more information at its disposal than standard MC. However, the paper shows that the proposed method is also orders of magnitudes more efficient than a naive approach using the failure cases during training.


REVIEW SUMMARY
--------------

I believe that this paper addresses an important problem in a novel manner (as far as I can tell) and the experiments are quite convincing.
The main negative point is that I believe that the proposed method has some flaws which may actually decrease statistical efficiency in some cases (please see details below).


DETAILED COMMENTS
-----------------

- It seems to me that a weak point of the method is that it may also severly reduce the efficiency compared to a standard MC method. If the function f underestimates the probability of failure at certain x, it would take a very long time to correct itself because these points would hardly ever be evaluated. It seems that the paper heuristically addresses this to some extent using the exponent alpha of the function. However, I think there should be a more in-depth discussion of this issue. An upper-confidence-bound type of algorithm may be a principled way of addressing this problem.

- The proposed method relies on the ability to initialize the system in any desired state. However, on a physical system, where finding failure cases is particularly important, this is usually not possible. It would be interesting if the paper would discuss how the proposed approach would be used on such real systems.

- On page 6, in the first paragraph, the state is called s instead of x as before. Furthermore, the arguments of f are switched.","The sentiment of the review is generally positive, as the reviewer acknowledges the importance and novelty of the paper's approach and finds the experiments convincing. However, the reviewer also points out some flaws and areas for improvement, which slightly tempers the overall positive sentiment. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, even when pointing out flaws. The reviewer suggests improvements in a respectful manner, which warrants a politeness score of 90.",60,90
"The paper is well written and easy to read. Exploration is one of the fundamental problems in RL, and the idea of using two agents for better exploration is interesting and novel. However, an explanation of the intuition behind the method would be useful. The experimental results show that the method works well in complex tasks. Since states are compared to each other in L2 distance, the method might not generalize to other domains where L2 distance is not a good distance metric.

Pros:
- well written
- a simple and novel idea tackling a hard problem
- good results on hard tasks

Cons: 
- an explanation of why the method should work is missing
- plot text is too small (what is the unit of X-axis?)

Questions:
- what is the intuition behind the method?
- during training, randomly sampled two states are compared. why it is a good idea? how the replay buffer size will affect it?
- since it is a two-player game, is there anything you can say about its Nash equilibrium? 
- why A is better than B at the task?
- when comparing states, are whole raw observations (including velocity etc.) used?
- section 4.2 doesn't seem to be that relevant or helpful. is it really necessary? 
- fig 4 is missing CER alone results? why is that? it doesn't work by itself on those tasks? ","The sentiment of the review is generally positive, as indicated by phrases like 'well written,' 'easy to read,' 'interesting and novel,' and 'good results on hard tasks.' However, there are some critical points and questions raised, which slightly temper the overall positivity. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer uses polite and constructive language throughout, such as 'would be useful,' 'might not generalize,' and 'is there anything you can say.' Therefore, the politeness score is 80.",60,80
"This paper builds upon recent work by Balestriero and Baraniuk (ICML 2018) that concern max-affine spline opertaor (MASO) interpretation of a substantial class of deep networks. In the new paper a special focus is put on Recurrent Neural Networks (RNNs), and it is highlighted based on theoretical considerations leveraging the MASO and numerical experiments that in the case of a piecewise affine and convex activation function, using noise in initial hidden state acts as regularization.  
Overall I was impressed by the volume of contributions presented throughout the paper and also I very muched like the light shed on important classes of models that turn out to be not as black box as they could seem. My enthouasiasm was somehow tempered when discovering that the MASO modelling here was in fact a special case of Balestriero and Baraniuk (ICML 2018), but it seems that despite this the specific contribution is well motivated and justified, especially regarding application results. Yet, the other thing that has annoyed me and is causing me to only moderately champion the paper so far is that I found the notation heavy, not always well introduced nor explained, and while I believe that the authors have a clear understanding of things, it appears to me that the the opening sections 1 and 2 lack notation and/or conceptual clarity, making the paper hard to accept without additional care. To take a few examples:
a) In equation (3), the exponent (\ell) in A and B is not discussed. On a different level, the term ""S"" is used here but doesn't seem to be employed much in next instances of MASOs...why? 
b) In equation (4), sure you can write a max as a sum with an approxiate indicator (modulo unicity I guess) but then what is called Q^{(\ell)} here becomes a function of A^{(\ell)}, B^{(\ell)}, z^{(\ell-1)}...?
c) In proposition 1, the notation A_sigma is not introduced. Of course, there is a notation table later but this would help (to preserve the flow and sometimes clarify things) to introduce notations upon first usage...
d) Still in prop 1, braket notation not so easy to grasp. What is A[z]z? 
e) Still in prop 1, recall that sigma is assumed piecewise-linear and convex? 
f) In th1, abusive to say that the layer ""is"" a mapping, isn't it?  
g) In Theorem 2, what is f? A generic term for a deterministic function? 
Also, below the Theorem, ""affine"" or ""piecewise affine""? 
h) I found section 4 somehow disconnected and flow-breaking. Put in appendix and use space to better explain the rest? 
i) Section 5 is a strong and original bit, it seems. Should be put more to the fore in abstract/intro/conclusion? ","The sentiment of the review is generally positive, as the reviewer expresses being impressed by the volume of contributions and the light shed on important classes of models. However, the sentiment is tempered by some criticisms regarding the notation and conceptual clarity, leading to a sentiment score of 50. The politeness of the language is quite high, as the reviewer uses polite phrases such as 'I very much liked,' 'it appears to me,' and 'I believe that the authors have a clear understanding of things,' even when pointing out issues. Therefore, the politeness score is 80.",50,80
"In this paper the authors propose a neural model that, given a logical formula as input, predicts whether the formula is a tautology or not. Showing that a formula is a tautology is important because if we can classify a formula A -> B as a tautology then we can say that B is a logical consequence of A. The structure of the formula is a feedforward neural network built in a top-down manner. The leaves of this network are vectors (each of them represents a particular occurrence of an atom) which, after the construction of the formula, are processed by some recurrent neural networks.

The proposed approach seems interesting. However, my main doubt concerns the model. It seems to outperform the state-of-the-art, but the authors do not give any explanations why. There is no theoretical or intuitive explanation of why the model works. Why we need RNNs and not feedforward NNs? I think this is an big issue.
In conclusion, I think that the paper is a bit borderline. The model should be better explained. However, I think that the approach is compelling and, after a minor revision, the paper could be considered for acceptance.

[Minor comments]
Page 4. 
“The dataset contains train (99876 pairs)”, pairs of what?

Page 5. 
What is the measure of the values reported in Table 1? Precision? 
","The sentiment of the review is moderately positive, as the reviewer acknowledges the interesting nature of the proposed approach and suggests that the paper could be considered for acceptance after minor revisions. However, the review also highlights a significant concern regarding the lack of explanation for the model's performance, which tempers the overall positivity. Therefore, the sentiment score is 30. The politeness of the language is high, as the reviewer uses polite phrases such as 'I think' and 'could be considered,' and provides constructive feedback without being harsh or dismissive. Thus, the politeness score is 80.",30,80
"update: The authors' feedback has addressed some of my concerns. I update my rating to 6.
=================
original:
This paper provides some new insights into classification bias. On top of the well known unbalanced group size, it shows that a large number of weak but asymmetry weak features also leads to bias. This paper also provides a method to reduces bias and remain the prediction accuracy.

In general, the paper is well written, but some description can be clearer. Some notation seems inconsistent. For example, D in equation (1) denotes the joint distribution (x,y), but it also refers to the marginal distribution of x somewhere else. 

In the high level, I am not totally convinced of how significant the result is.  In particular, the bias this paper defines is on the probability (softmax) scale, but logistic regression is on logit scale--   not even aimed at the unbiasedness in the original scale. So the result in section 2 seems to be expected.  Given the fact that unbiasedness is not invariant under transformation, I am wondering why it should be the main target in the first place.  

In the bias reduction methods in equation 5 and 6, both the objective function and the constraint are empirical estimations. Will it be too noisy to adapt to the high dimensional setting? On the other hand, adding some sparsity regularization improves prediction seems well known in practice.

I would also encourage the authors to have extended work both theoretically and experimentally.  The asymmetry feature is only illustrated by a single logistic regression. Is it a problem of weak features, or indeed a problem of logistic regression? What will happen in a more general case beyond mean-field Gaussian?  I would imagine in this simple case the authors may even derive the closed form expression to verify their heuristics.  

Based on the evaluations above, I would recommend a weak reject. 
","The sentiment of the review is mixed but leans slightly positive. The reviewer acknowledges that the paper provides new insights and methods to reduce bias while maintaining prediction accuracy, which is a positive aspect. However, the reviewer also expresses significant concerns about the clarity of the descriptions, the consistency of notation, and the significance of the results. The recommendation for a 'weak reject' indicates a slightly negative overall sentiment. Therefore, the sentiment score is set to -10. The language used in the review is generally polite and constructive, offering specific suggestions for improvement without being harsh or dismissive. Therefore, the politeness score is set to 80.",-10,80
"This is in a visual active tracking application. The paper proposes a novel reward function - ""partial zero sum"", which only encourages the tracker-target competition when they are close and penalizes whey they are too far.

This is a very interesting problem and I see why their contribution could improve the system performance. 

Clarity: the paper is well-written. I also like how the author provides both formulas and a lot of details on implementation of the end-to-end system. 

Originality: Most of the components are pretty standard, however I value the part that seems pretty novel to me - which is the ""partial zero-sum"" idea.

Evaluation: the result obtained from the simulated environment in 2d and 3d are convincing. However, if 1) real-world test and results  2) a stronger baseline can be used, that would be a stronger acceptance. ","The sentiment of the review is positive, as the reviewer expresses interest in the problem and acknowledges the potential improvement in system performance due to the novel contribution. The reviewer also appreciates the clarity and detailed implementation provided in the paper. However, there are some suggestions for improvement, which are presented constructively. The politeness of the language is high, as the reviewer uses polite and respectful language throughout the review, even when providing critical feedback.",80,90
"This paper proposes a new few-shot learning method with class dependencies. To consider the structure in the label space, the authors propose to use conditional batch normalization to help change the embedding based on class-wise statistics. Based on which the final classifier can be learned by the gradient-based meta-learning method, i.e., MAML. Experiments on MiniImageNet show the proposed method can achieve high-performance, and the proposed part can be proved to be effective based on the ablation study.

There are three main concerns about this paper, and the final rating depends on the authors' response.
1. The motivation
The authors claim the label structure is helpful in the few-shot learning. If the reviewer understands correctly, it is the change of embedding network based on class statistics that consider such a label structure. From the objective perspective, there are no terms related to this purpose, and the embedding space learning is also based on the same few-shot objective. Will it introduces more information w.r.t. only using embedding space to do the classification?

2. The novelty.
This paper looks like a MAML version of TADAM. Both of the methods use the conditional batch normalization in the embedding network, while CAML uses MAML to learn another classifier based on the embedding. Although CAML uses the CBN at the example level and considers the class information in a transductive setting, it is not very novel. From the results, the proposed method uses a stronger network but does not improve a lot w.r.t. TADAM.

3. Method details
3.1 Since CBN is example induced, will it prone to overfitting?
3.2 About the model architecture. 
CAML uses a 4*4 skip connection from input to output. It is OK to use this improve the final performance, but the authors also need to show the results without the skip connection to fairly compare with other methods. Is this skip connection very important for this particular model? Most methods use 64 channel in the convNet while 30 channels are used in this paper. Is this computational consideration or to avoid overfitting? It is a bit strange that the main network is just four layers but the conditional network is a larger and stronger resNet.
3.3 About the MAML gradients
How to compute the gradient in the MAML flow? Will the embedding network be updated simultaneously? In other words, will the MAML objective influences the embedding network?
3.4 The training details are not clear. 
The concrete training setting is not clear. For example, does the method need model pre-train? What is the learning rate, and how to adapt it? For the MAML, we also need the inner-update learning rate. How many epochs does CAML need?
3.5 How about build MAML directly on the embedding space?","The sentiment of the review is mixed. The reviewer acknowledges the high performance of the proposed method and its effectiveness based on the ablation study, which is positive. However, the reviewer also raises significant concerns about the motivation, novelty, and method details, indicating a level of skepticism. Therefore, the sentiment score is slightly negative. The politeness of the language is generally high. The reviewer uses formal and respectful language, even when pointing out concerns and asking for clarifications, which indicates a polite tone.",-20,80
"This paper provides a visualization framework to understand the generative neural network in GAN models. To achieve this, they first find a group of interpretable units and then quantify the causal effect of interpretable units. Finally, the contextual relationship between these units and their surrounding is examined by inserting the discovered object concepts into new images. Extensive experiments are presented and a video is provided.

Overall, I think this paper is very valuable and well-written. The experiments clearly show the questions proposed in the introduction are answered. Two concerns are as follows.

Cons:
1) The visualization seems to be very heuristic. What I want to know is the theoretical interpretation of the visualization. For example, the Class Activation Maps (CAM) can be directly calculated by the output values of softmax function. How about the visual class for the generative neural networks?
2) I am also very curious, how is the rate of finding the correct sets of units for a particular visual class?
","The sentiment of the review is positive, as indicated by phrases like 'very valuable and well-written' and the acknowledgment that the experiments clearly address the questions posed in the introduction. Therefore, the sentiment score is 80. The politeness of the language is also high, as the reviewer uses polite language and constructive feedback, such as 'I am also very curious' and 'What I want to know is,' which suggests a politeness score of 90.",80,90
"Summary
-------
This paper describes a model for musical timbre transfer.
The proposed method uses constant-Q transform magnitudes as the input representation, transfers between domains (timbres) by a CycleGAN-like architecture, and resynthesizes the generated CQT representation by a modified WaveNET-like decoder. The system is evaluated by human (mechanical turk) listening studies, and the results indicate that the proposed system is effective for pitch and tempo transfer, as well as timbre adaptation.


High-level comments
-------------------

This paper is extremely well written, and the authors clearly have a great attention to detail in both the audio processing and machine learning domains.  Each of the modifications to prior work was well motivated, and the ablation study at the end, while briefly presented, provides a good sense of the contributions of each piece.

I was unable to listen to the examples provided by the link in section 6, which requires a Microsoft OneDrive login to access.  However, the youtube link provided in the ICLR comments gave a reasonable sample of the results of the system.  Overall, the outputs sound compelling, and match my expectations given the reported results of the listening studies.

On the quantitative side, it would have been nice to see a measurement of phase retrieval of the decoder component, which could be done in isolation from the transfer components by feeding in original CQT magnitudes.  This might help give a sense of how well the model can be expected to perform, particular as it breaks down along target timbres.  I would expect some timbres to be easier to model than others, and having a quantitative handle on that could help put the listener study in a bit more perspective.

Detailed comments
-----------------

The paper contains numerous typos and grammatical quirks, e.g.:
    - page 5: ""GP can stable GAN training""
    - page 7: ""CQT is equivalent to pitch""


The reverse-generation trick in section 3.2 was clever!
","The sentiment of the review is highly positive, as evidenced by phrases like 'extremely well written,' 'great attention to detail,' and 'outputs sound compelling.' The reviewer appreciates the modifications to prior work and the ablation study, indicating a strong approval of the paper's contributions. The sentiment score is therefore 90. The politeness of the language is also very high, with the reviewer using polite and constructive language throughout the review. Phrases like 'it would have been nice to see' and 'I would expect' show a considerate and respectful tone. The politeness score is therefore 95.",90,95
"This paper provides a new dynamic perspective on deep neural network. Based on Gaussian weights and biases, the paper investigates the evolution of the covariance matrix along with the layers. Eventually the matrices achieve a stationary point, i.e., fixed point of the dynamic system. Local performance around the fixed point is explored. Extensions are provided to include the batch normalization. I believe this paper may stimulate some interesting ideas for other researchers.

Two technical questions:

1. When the layers tends to infinity, the covariance matrix reaches stationary (fixed) point. How to understand this phenomenon? Does this mean that the distribution of the layer outputs will not change too much if the layer is deep enough? This somewhat conflicts the commonsense of ""the deeper the better?"" 

2. Typos: the weight matrix in the end of page 2 should be N_l times N_{l-1}. Also, the x_i's in the first line of page 3 should be bold.","The sentiment of the review is positive, as indicated by the statement 'I believe this paper may stimulate some interesting ideas for other researchers.' This suggests that the reviewer sees value in the paper and its potential impact on the field. The sentiment score is therefore 80. The politeness of the language is also high. The reviewer uses polite language, such as 'I believe' and 'may stimulate,' and provides constructive feedback without any harsh criticism. The politeness score is 90.",80,90
"The focus on novelty (mentioned in both the abstract, and conclusion as a direct claim) in the presentation hurts the paper overall. Without stronger comparison to other closely related work, and lack of citation to several closely related models, the claim of novelty isn't defined well enough to be useful. Describing what parts of this model are novel compared to e.g. Stochastic WaveNet or the conditional dilated convolutional decoder of ""Improved VAE for Text ..."" (linked below, among many others) would help strengthen the novelty claim, if the claim of novelty is needed or useful at all. Stochastic WaveNet in particular seems very closely related to this work, as does PixelVAE. In addition, use of autoregressive models conditioned on (non-variational, in some sense) latents have been shown in both VQ-VAE and ADA among others, so a discussion would help clarify the novelty claim.

Empirical results are strong, though (related to the novelty issue) there should be greater comparison both quantitatively and qualitatively to further work. In particular, many of the papers linked below show better empirical results on the same datasets. Though the results are not always directly comparable, a discussion of *why* would be useful - similar to how Z-forcing was included.

In the qualitative analysis, it would be good to see a more zoomed out view of the text (as in VRNN), since one of the implicit claims of the improvement from dense STCN is improved global coherence by direct connection to the ""global latents"". As it stands now the text samples are a bit too local to really tell. In addition, the VRNN samples look quite a bit different than what the authors present in their work - what implementation was used for the VRNN samples (they don't appear to be clips from the original paper)? 

On the MNIST setting, there are many missing numbers in the table from related references (some included below), and the >= 60.25 number seems so surprising as to be (possibly) incorrect - more in-depth analysis of this particular result is needed. Overall the MNIST result needs more description and relation to other work, for both sequential and non-sequential models.

The writing is well-done overall, and the presented method and diagrams are clear. My primary concern is in relation to related work, clarification of the novelty claim, and more comparison to existing methods in the results tables. 

Variational Bi-LSTM https://arxiv.org/abs/1711.05717

Stochastic WaveNet https://arxiv.org/abs/1806.06116

PixelVAE https://arxiv.org/abs/1611.05013

Filtering Variational Objectives https://github.com/tensorflow/models/tree/master/research/fivo

Improved Variational Autoencoders for Text Modeling using Dilated Convolutions https://arxiv.org/abs/1702.08139

Temporal Sigmoid Belief Networks for Sequential Modeling http://papers.nips.cc/paper/5655-deep-temporal-sigmoid-belief-networks-for-sequence-modeling

Neural Discrete Representation Learning (VQ-VAE) https://arxiv.org/abs/1711.00937

The challenge of realistic music generation: modelling raw audio at scale (ADA) https://arxiv.org/abs/1806.10474

Learning hierarchical features from Generative Models https://arxiv.org/abs/1702.08396

Avoiding Latent Variable Collapse with Generative Skip Models https://arxiv.org/abs/1807.04863

EDIT: Updated score after second revisions and author responses","The sentiment of the review is moderately negative, as it highlights several significant issues with the paper, particularly regarding the novelty claim and the need for stronger comparisons to related work. However, it does acknowledge the empirical results as strong and the writing as well-done. Therefore, the sentiment score is -40. The politeness of the language is relatively high; the reviewer uses constructive criticism and provides specific recommendations without being rude or dismissive. Thus, the politeness score is 60.",-40,60
"The paper propose a framework to deal with large state and action
spaces with sparse rewards in reinforcement learning. In particular,
they propose to use a meta-learner to generate experience to the agent
and to decompose the learning task into simpler sub-tasks. The authors
train a DQN with a novel architecture to navigate the Web.
In addition the authors propose to use several strategies: shallow
encoding (SE), reward shaping (AR) and curriculum learning (CI/CG). 
It is shown how the proposed method outperforms state-of-the-art
systems on several tasks.

In the first set of experiments it is clear the improved performance
of QWeb over Shi17 and Liu18, however, it is not clear why QWeb is not
able to learn in the social-media-all problem. The authors tested only
one of the possible variants (AR) of the proposed approach with good
performance. 

It is not clear in the book-flight-form environment, why the
QWeb+SE+AR obtained 100% success while the MetaQWeb, which includes
one of main components in this paper, has a lower performance.

The proposed method uses a large number of components/methods, but it
is not clear the relevance of each of them. The papers reads like, ""I
have a very complex problem to solve so I try all the methods that I
think will be useful"". The paper will benefit from an individual
assessment of the different components.

The authors should include a section of conclusions and future work.
","The sentiment of the review is moderately positive as it acknowledges the proposed framework and its performance improvements over state-of-the-art systems. However, it also points out several unclear aspects and areas for improvement, which slightly tempers the overall positivity. Therefore, the sentiment score is 40. The politeness of the language is quite high, as the reviewer uses constructive criticism and suggests specific improvements without being harsh or dismissive. Thus, the politeness score is 80.",40,80
"While most works consider embedding as the problem of mapping an input into a point in an embedding space, paper 1341 considers the problem of mapping an input into a distribution in an embedding space. Computing the matching score of two inputs (e.g. two images) involves the following steps: (i) assuming a Gaussian distribution in the embedding space, computing the mean and standard deviation for each input, (ii) drawing a set of samples from each distribution, (3) computing the normalized distances between the samples and (iv) averaging to obtain a global score.

The proposed approach is validated on a new benchmark built on MNIST.

On the positive side:
-	The topic of injecting uncertainty in neural networks should be of broad interest to the ICLR community.
-	The paper is generally clear.
-	The qualitative evaluation provides intuitive results.

On the negative side:
-	The whole idea of drawing samples to compute the distance between two Gaussian distributions seems unnecessarily complicated. Why not computing directly a distance between distributions? There exist kernels between distributions, such as the Probability Product Kernel (PPK). See Jebara, Kondor, Howard “Probability product kernels”, JMLR’04. The PPK between two distributions p(x) and q(x) writes as: \int_x p^a(x) q^a(x) dx, where a is a parameter. When a=1, it is known as the Expected Likelihood Kernel (ELK). When a=1/2, this is known as the Hellinger or Bhattacharyya kernel (BK). In p and q are Gaussian distributions, then the PPK can be computed in closed form. If p and q are mixtures of Gaussians, then the ELK can be computed in closed form. 
-	The Mixture of Gaussians embedding extension is lacking in details. How does the network generate C Gaussian distributions? By having 2C output branches generating C means and C standard deviation vectors? 
-	It might be useful to provide more details about why the self-similarity measure makes sense as an uncertainty measure. In its current state, the paper does not provide much intuition and it took me some time to understand (I actually understood when I made the connection with the ELK). Also, why not using a simpler measure of uncertainty such as the trace of the covariance matrix?
-	The experiments are lacking in some respects:
o	It would be useful to report results without the VIB regularization.
o	The focus on the cases D=2 and D=3 (embedding in a 2D or 3D space) shades some doubt on the practical usefulness of this framework in a higher-dimensional case.

Miscellaneous:
-	It seems there is a typo between equations (4) and (5). It should write z_1^{(k_1)} \sim p(z_1|x_1)

--- 

In their rebuttal, the authors satisfyingly addressed my concerns. Hence, I am upgrading my overall rating.
","The sentiment score is derived from the overall tone of the review. Initially, the reviewer acknowledges the positive aspects of the paper, such as the broad interest of the topic, clarity, and intuitive results, which suggests a positive sentiment. However, the reviewer also lists several significant criticisms and suggestions for improvement, which tempers the overall sentiment. The final note about the authors satisfactorily addressing concerns and upgrading the rating indicates a positive resolution. Therefore, the sentiment score is moderately positive. The politeness score is high because the reviewer uses polite language throughout, even when pointing out flaws. The reviewer provides constructive feedback and references to relevant literature, which is a hallmark of polite and professional communication.",40,80
"==== Summary ====

This paper proposes a model for learning problems that exhibit compositional and recursive structure, called Compositional Recursive Learner (CRL). The paper approaches the subject by first defining a problem as a transformation of an input representation x from a source domain t_x to a target domain t_y. If t_x = t_y then it is called a recursive problem, and otherwise a translational problem. A composite problem is the composition of such transformations. The key observation of the paper is that many real-world problems can be solved iteratively by either recursively transforming an instance of a problem to a simpler instance, or by translating it to a similar problem which we already know how to solve (e.g., translating a sentence from English to French through Spanish). The CRL model is essentially composed of two parts, a set of differential functions and a controller (policy) for selecting functions. At each step i, the controller observes the last intermediate computation x_i and the target domain t_y, and then selects a function and the subset of x_i to operate on. For each instance, the resulting compositional function is trained via back-propagation, and the controller is trained via policy gradient. Finally, the paper presents experiments on two synthetic datasets, translating an arithmetic expression written in one language to its outcome written in another language, and classifying MNIST digits that were distorted by an unknown random sequence of affine transformations. CRL is compared to RNN on the arithmetic task and shown to be able to generalize both to longer sequences and to unseen language pairs when trained on few examples, while RNN can achieve similar performance only using many more examples. On MNIST, it is qualitatively shown that CRL can usually (but not always) find the sequence of transformations to restore the digit to its canonical form.

==== Detailed Review ====

I generally like this article, as it contains a neat solution to a common problem that builds on and extends prior work. Specifically, the proposed CRL model is a natural evolution of previous attempts at solving problems via compositionally, e.g. Neural Programmer [1] that learns a policy for composing predefined commands, and Neural Module Networks [2] that learns the parameters of shared differential modules connected via deterministically defined structure (found via simple parse tree). The paper contains a careful review of the related works and highlights the similarities and differences from prior approaches. Though the experiments are mostly synthetic, the underlying method seems to be readily applicable to many real-world problems.

However, the true contributions of the paper are somewhat muddied by presenting CRL as more general than what is actually supported by the experiments. More specifically, the paper presents CRL as a general method for learning compositional problems by decomposing them into simpler sub-problems that are automatically discovered, but in practice, a far more limited version of CRL is used in the experiments, and the suggested translational capabilities of CRL, which are important for abstract sub-problem discovery, are not properly validated:

1. In both experiments, the building-block functions are hand-crafted to fit to the prior knowledge on the compositionally of the problem. For the arithmetic task, the functions are limited to operate each step just on a single window of encompassing 3 symbols (e.g., <number> <op> <number>,  <op> <number> <op>) and return a distribution over the possible symbols, which heavily forces the functions to represent simple evaluators for simple expressions of the form <number> <op> <number>. For the distorted MNIST task, the functions are limited to neural networks which choose the parameter of predetermined transformations (scaling, translation, or rotation) of the input. In both cases, CRL did not *found* sub-problems for reducing the complexity of the original instance but just had to *fine tune* loosely predefined sub-problems. Incorporating expert knowledge into the model like so is actually an elegant and useful trick for solving real problems, and it should be emphasized far clearly in the article. The story of “discovering subproblems” should be left for the discussion / future research section, because though it might be a small step towards that goal, it is not quite there yet.
2. The experiments very neatly show how recursive transformations offer a nice framework for simplifying an instance of a problem. However, the translation capabilities of the model are barely tested by the presented experiments, and it can be argued that all transformations used by the model are recursive in both experiments. First, only the arithmetic task has a translation aspect to it, i.e., the task is to read an expression in one language and then output the answer in a different language. Second, this problem is only weakly related to translation because it is possible to translate the symbols independently, word by word, as opposed to written language that has complex dependencies between words. Third, the authors report that in practice proper translation was only used in the very last operation for translating the computed value of the input expression to the requested language, and not as a method to translate one instance that we cannot solve into another that we can. Finally, all functions operate and return on all symbols and not ones limited to a specific language, and so by the paper’s own definition, these are all recursive problems and not translational ones.

In conclusion, I believe this paper should be accepted even with the above issues, mostly because the core method is novel, clearly explained, and appears to be very useful in practice. Nevertheless, I strongly suggest to the authors to revise their article to focus on the core qualities of their method that can be backed by their current experiments, and correctly frame the discussion on possible future capabilities as such.

[1] Reed et al. Neural Programmer-Interpreters. ICLR 2016.
[2] Andreas et al. Neural Module Networks. CVPR 2016.","The sentiment of the review is generally positive, as indicated by phrases like 'I generally like this article' and 'the core method is novel, clearly explained, and appears to be very useful in practice.' However, the reviewer also points out several significant limitations and areas for improvement, which tempers the overall positivity. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer uses polite and constructive language throughout, such as 'I strongly suggest to the authors' and 'it should be emphasized far clearly in the article.' The reviewer provides detailed feedback without being harsh or dismissive, so the politeness score is 90.",60,90
"This paper presents a targeted empirical evaluation of generalization in models
for visual reasoning. The paper focuses on the specific problem of recognizing
(object, relation, object) triples in synthetic scenes featuring letters and
numbers, and evaluates models' ability to generalize to the full distribution of
such triples after observing a subset that is sparse in the third argument. It
is found that (1) NMNs with full layout supervision generalize better than other
state-of-the art visual reasoning models (FiLM, MAC, RelNet), but (2) without
supervised layouts, NMNs perform little better than chance, and without
supervised question attentions, NMNs perform better than the other models but
fail to achieve perfect generalization.

STRENGTHS
- thorough analysis with a good set of questions

WEAKNESSES
- some peculiar evaluation and presentation decisions
- introduces *yet another* synthetic visual reasoning dataset rather than
  reusing existing ones

I think this paper would have been stronger if it investigated a slightly
broader notion of generalization and had some additional modeling comparisons.
However, I found it interesting and think it successfully addresses the set of
questions it sets out to answer. If it is accepted, there are a few things that
can be done to improve the experiments.

MODELING AND EVALUATION

- Regarding the dataset: the proliferation of synthetic reasoning datasets is
  annoying because it makes it difficult to compare results without downloading
  and re-running a huge amount of code. (The authors have, to their credit, done
  so for this paper.) I think all the experiments here could have been performed
  successfully using either the CLEVR or ShapeWorld rendering engines: while the
  authors note that they require a ""large number of different objects"", this
  could have been handled by treating e.g. ""red circle"" and ""red square"" as
  distinct atomic primitives in questions---the fact that redness is a useful
  feature in both cases is no different from the fact that a horizontal stroke
  detector is useful for lots of letters.

- I don't understand the motivation behind holding out everything on the
  right-hand side. For models that can't tell that the two are symmetric, why
  not introduce sparsity everwhere---hold out some LHSs and relations?
  
- Table 1 test accuracies: arbitrarily reporting ""best of 3"" for some model /
  dataset pairs and ""confidence interval of 5"" for others is extremely
  unhelpful: it would be best to report (mean / max / stderr) for 5. Also, it's
  never stated which convidence interval is reported.

- Table 1 baselines: why not run Conv+LSTM and RelNet with easier #rhs/lhs data?

- How many MAC cells are used? This can have significant performance
  implications. I think if you used their code out of the box you'll wind up
  with way bigger structures than you need for this task.

- I'm not sure how faithful the `find` module used here is to the one in the
  literature, and one of the interesting claims in this work is that module
  implementation details matter! The various Hu papers use an attentional
  parameterization; the use of a ReLU and full convolution in Eq. 14 suggest
  that that one here can pass around more general feature maps. This is fine but
  the distinction should be made explicit, and it would be interesting to see
  additional comparisons to an NMN with purely attentional bottlenecks.

- Why do all the experiments after 4.3 use #rhs/lhs of 18? If it was 8 it would
  be possible to make more direct comparisons to the other baseline models.

- The comparison to MAC in 4.2 is unfair in the following sense: the NMN
  effectively gets supervised textual attentions if the right parameters are
  always plugged into the right models, while the MAC model has to figure out
  attentions from scratch. A different way of structuring things would be to
  give the MAC model supervised parameterizations in 4.2, and then move the
  current MAC experiment to 4.3 (since it's doing something analogous to
  ""parameterization induction"".
  
- The top-right number in Table 4---particularly the fact that it beats MAC and
  sequential NMNs under the same supervision condition---is one of the most
  interesting results in this paper. Most of the work on relaxing supervision
  for NMNs has focused on (1) inducing new question-specific discrete structures
  from scratch (N2NMN) or (2) finding fixed sequential structures that work well
  in general (SNMN and perhaps MAC). The result this paper suggests an
  alternative, which is finding good fixed tree-shaped structures but continuing
  to do soft parameterization like N2NMN.

- The ""sharpness ratio"" is not super easy to interpret---can't you just report
  something standard like entropy? Fig 4 is unnecessary---just report the means.

- One direction that isn't explored here is the use of Johnson- or Hu-style
  offline learning of a model to map from ""sentences"" to ""logical forms"". To the
  extent that NMNs with ground-truth logical forms get 100% accuracy, this turns
  the generalization problem studied here into a purely symbolic one of the kind
  studied in Lake & Baroni 18. Would be interesting to know whether this makes
  things harder (b/c no grounding signal) or easier (b/c seq2seq learning is
  easier.)

PRESENTATION

- Basically all of the tables in this paper are in the wrong place. Move them
  closer to the first metnion---otherwise they're confusing.

- It's conventional in this conference format to put all figure captions below
  the figures they describe. The mix of above and below here makes it hard to
  attach captions to figures.

- Some of the language about how novel the idea of studying generalization in
  these models is a bit strong. The CoGenT split of the CLEVR dataset is aimed
  at answering similar questions. The original Andreas et al CVPR paper (which btw
  appears to have 2 bib entries) also studied generalization to structurally
  novel inputs, and Hu et al. 17 notes that the latent-variable version of this
  model with no supervision is hard to train.

MISCELLANEOUS

- Last sentence before 4.4: ""NMN-Chain"" should be ""NMN-Tree""?

- Recent paper with a better structure-induction technique:
  https://arxiv.org/abs/1808.09942. Worth citing (or comparing if you have
  time!)","The sentiment of the review is moderately positive. The reviewer acknowledges the strengths of the paper, such as the thorough analysis and the interesting findings, but also points out several weaknesses and areas for improvement. The sentiment score is 30 because the review is generally supportive but not overwhelmingly positive. The politeness score is 50 because the language used is constructive and respectful, though it includes some critical feedback. The reviewer provides detailed suggestions for improvement without being rude or dismissive.",30,50
"This article presents experiments on medium- and large-scale language modeling when the ideas of adaptive softmax (Grave et al., 2017) are extended to input representations.

The article is well written and I find the contribution simple, but interesting. It is a reasonable and well supported increment from adaptive softmax of Grave et al. (2017).

My question is a bit philosophical: The only thing which I was concerned about when reading the paper is projection of the embeddings back to the d-dimensional space. I understand that for two matrices A and B we have rank(AB) <= min(rank(A), rank(B)), and we are not making the small-sized embeddings richer when backprojecting to R^d, but have you thought about how it would be possible to avoid this step and keep the original variable-size embeddings?

References
Joulin, A., Cissé, M., Grangier, D. and Jégou, H., 2017, July. Efficient softmax approximation for GPUs. In International Conference on Machine Learning (pp. 1302-1310).","The sentiment of the review is positive. The reviewer describes the article as 'well written' and the contribution as 'simple, but interesting,' which indicates a positive sentiment. The reviewer also acknowledges that the work is a 'reasonable and well supported increment' from previous work, further supporting a positive sentiment score. The politeness of the language is also high. The reviewer uses polite language throughout, including phrases like 'I find the contribution,' 'My question is a bit philosophical,' and 'have you thought about,' which are respectful and considerate ways to provide feedback and ask questions.",80,90
"The paper proposes neural networks which are convex on inputs data to control problems. These types of networks, constructed based on either MLP or RNN, are shown to have similar representation power as their non-convex versions, thus are potentially able to better capture the dynamics behind complex systems compared with linear models. On the other hand, convexity on inputs brings much convenience to the later optimization part, because there are no worries on global/local minimum or escaping saddle points. In other words, convex but nonlinear provides not only enough search space, but also fast and tractable optimization. The compromise here is the size of memory, since 1) more weights and biases are needed to connect inputs and hidden layers in such nets and 2) we need to store also the negative parts on a portion of weights. 

Even though the idea of convex networks were not new, this work is novel in extending input convex RNN and applying it into dynamic control problems. As the main theoretical contribution, Theorem 2 shows that to have same representation power, input convex nets use polynomial number of activation functions, compared with exponential from using a set of affine functions. Experiments also show such effectiveness. The paper is clearly and nicely written. These are reasons I suggest accept.


Questions and suggestions:

1) For Lemma 1 and Theorem 1, I wonder whether similar results can be established for non-convex functions. Intuitively, it seems that as long as assuming Lipschiz continuous, we can always approximate a function by a maximum of many affine functions, no matter it is convex or not. Is this right or something is missing?

2) In the main paper, all experiments were aimed to address ICNN and ICRNN have good accuracy, but not they are easier to optimize due to convexity. In the abstract, it is mentioned ""... using 5X less time"", but I can only see this through appendix. A suggestion is at least describing some results on the comparison with training time in the main paper.

3) In Appendix A, it seems the NN is not trained very well as shown in the left figure. Is this because the number of parameters of NN is restricted to be the same as in ICNN? Do training on both spend the same resource, ie, number of epoch? Such descriptions are necessary here.

4) In Table 2 in appendix, why the running time of ICNN increases by a magnitude for large H in Ant case?


Typos:
	Page 1 ""simple control algorithms HAS ...""
	Page 7 paragraph ""Baselines"": ""Such (a) method"".
	In the last line of Table 2, 979.73 should be bold instead of 5577.
	There is a ?? in appendix D.4.
	
","The review starts with a positive sentiment towards the paper, highlighting the novelty and effectiveness of the proposed neural networks. The reviewer appreciates the clear and well-written nature of the paper and suggests acceptance. The language used is polite and constructive, offering specific suggestions and questions to improve the paper. The reviewer also points out some minor typos, which is done in a helpful manner.",90,95
"This paper studies the forgetting behavior of the training examples during SGD. Empirically it shows there are forgettable and unforgettable examples, unforgettable examples are like ""support examples"", one can achieve similar performance by training only on these ""support examples"". The paper also shows this phenomenon is consistent across different network architectures.

Pros:
This paper is written in high quality, clearly presented. It is original in the sense that this is the first empirical study on the forgettability of examples in during neural network training.

Comments and Questions on the experiment details:
1. Is the dataset randomly shuffled after every epoch? One concern is that if the order is fixed, some of the examples will be unforgettable simply because the previous batches have similar examples , and training the model on the previous batches makes it good on some examples in the current batch.
2. It would be more interesting to also include datasets like cifar100, which has more labels. The current datasets all have only 10 categories.
3. An addition figure can be provided which switches the order of training in figure 4b. Namely, start with training on b.2.

Cons:
Lack of insight. Subjectively, I usually expect empirical analysis papers to either come up with unexpected observations or provide guidance for practice. In my opinion, the findings of this work is within expectation, and there is a gap for practice.

Overall this paper is worth publishing for the systematic experiments which empirically verifies that there are support examples in neural networks.","The sentiment of the review is generally positive, as the reviewer acknowledges the high quality and originality of the paper, and ultimately recommends it for publication. However, the reviewer also points out some areas for improvement, particularly in the experimental details and the lack of insightful findings. Therefore, the sentiment score is not fully positive but leans towards the positive side. The language used is polite and constructive, offering specific suggestions for improvement without being harsh or dismissive.",60,80
"This paper introduces the General Language Understanding Evaluation (GLUE) benchmark and platform, which aims to evaluate representations of language with an emphasis on generalizability. This is a timely contribution and GLUE will be an impactful resource for the NLP community. This is mitigated, perhaps, somewhat by the recent release of decaNLP. But, as discussed the authors, this has a different focus (re-framing all tasks as QQ) and further does not feature the practical tools released here (leaderboard, error analysis) that will help drive progress.

Some comments below. 

- The inclusion of the small diagnostic dataset was a nice addition and it would be nice if future corpora included similar. 

- Implicit in this and related efforts is the assumption that parameter sharing ought to be possible and fruitful across even quite diverse tasks. While I do not object to this, it would be nice if the authors could make an explicit case here as to why should we believe this to be the case.

- The proposed platform is touted as one of the main contributions here, but not pointed to -- I assume for anonymity preserving reasons, but still would have been nice for this to be made explicit. 

- I would consider pushing Table 5 (Appendix) into the main text. 
","The sentiment of the review is generally positive. The reviewer acknowledges the timeliness and potential impact of the GLUE benchmark and platform, which suggests a positive sentiment. However, there is a slight mitigation mentioned due to the recent release of decaNLP, which introduces a minor neutral element. The politeness of the language used is high. The reviewer uses polite and constructive language, providing specific recommendations without any negative or rude remarks.",80,90
"This paper proposes to combine fitted value iteration with model predictive control (MPC) to speed up the learning process. The value iteration is the ""Learn offline"" subsystem while MPC is the ""Plan online"" subsystem. In addition, this paper also proposes an exploration technique that increases exploration if the multiple value function estimators disagree. The evaluation is complete and shows nice results.

However, I did not rank this paper high for two reasons. First, it is not clear to me how the model is acquired in MPC. Does the method learn the model? Does the method linearize the dynamics and assume a linear model? I am not sure. I suspect that the method just uses the simulator as the model. If it is the case, the method is not so useful because for complexity systems, such as humanoids, we do not know the model. And the comparisons with model-free learning algorithms are not fair because the paper assumes that the model is given. If this is not the case, I suggest that a more detailed description of MPC should be presented in Section 2.3.

Second, the technical contributions are lean. The three main components, 1) fitted value iteration, 2) MPC and 3) exploration based on multiple value function estimates, are not novel. The combination of them seems straight forward. For example, the H-step Bellman update (Section 2.3) is a blend between Monte-Carlo method and Q learning. It seems to be similar to the TD(\lambda) method. Thus, it is not surprising that it can accelerate convergence of value function.

For the above reasons, I would not recommend accepting this paper at this time.","The sentiment score is derived from the overall tone of the review. The reviewer acknowledges the completeness and positive results of the evaluation but ultimately does not recommend the paper for acceptance due to significant concerns. This mixed feedback results in a sentiment score of -20, indicating a slightly negative sentiment. The politeness score is based on the language used throughout the review. The reviewer uses polite and professional language, even when pointing out flaws and making suggestions for improvement. Therefore, the politeness score is 80.",-20,80
"

=Major Comments=
The prior work on grid cells and deep learning makes it clear that the goal of the work is to demonstrate that a simple learning system equipped with representation learning will produce spatial representations that are grid-like. Finding grid-like representations is important because these representations occur in the mammalian brain. 

Your paper would be improved by making a similar argument, where you would need to draw much more explicitly on the neuroscience literature. Namely, the validation of your proposed representations for position and velocity are mostly validated by the fact that they yield grid-like representations, not that they are useful for downstream tasks.

Furthermore, you should better justify why your simple model is better than prior work? What does the simplicity provide? Interpretability? Ease if optimization? Sample complexity for training?

This is important because otherwise it is unclear why you need to perform representation learning. The tasks you present (path integral and planning) could be easily performed in basic x-y coordinates. You wouldn’t need to introduce a latent v. Furthermore, this would mprove your argument for the importance of the block-diagonal M, since it would be more clear why interpretability matters.


Finally, you definitely need to discuss the literature on randomized approximations to RBF kernels (random Fourier features). Given the way you pose the representation learning objective, I expect that these would be optimal. With this, it is clear why grid-like patterns would emerge.

=Additional Comments=
What can you say about the quality of the path returned by (10)? Is it guaranteed to converge to a path that ends at y? Is it the globally optimal path? 

I don’t agree with your statement that your approach enables simple planning by steepest descent. First of all, are the plans that your method outputs high-quality? Second, if you had solved (10) directly in x-y coordinates, you could have done this easily since it is an optimization problem in just 2 variables. That could be approximately solved by grid search.

I would remove section 5.4. The latent vector v is a high-dimensional encoding of low-dimensional data, so of-course it is robust to corruptions. The corruptions you consider don’t come from a meaningful noise process, however? I can imagine, for example, that the agent observes corrupted versions of (x,y), but why would v get corrupted?

","The sentiment of the review is moderately positive. The reviewer acknowledges the importance of the work and provides constructive feedback aimed at improving the paper. However, the tone is critical and points out several areas that need significant improvement. Therefore, the sentiment score is 20. The politeness of the language is quite high. The reviewer uses polite language and phrases such as 'Your paper would be improved by' and 'you should better justify,' which are constructive and respectful. Therefore, the politeness score is 80.",20,80
"The paper proposes a method to prevent posterior collapse, which refers to the phenomenon that VAEs with powerful autoregressive decoders tend to ignore the latent code, i.e., the decoder models the data distribution independently of the code. Specifically, the encoder, decoder, and prior distribution families are chosen such that the KL-term in the ELBO is bounded away from 0, meaning that the encoder output cannot perfectly match the prior. Assuming temporal data, the authors employ a 1-step autoregressive (across) prior with an encoder whose codes are independent conditionally on the input. Furthermore, they propose to use a causal decoder together with an anti-causal or non-causal encoder, which translates into a PixelSNAIL/PixelCNN style decoder and an anti-causal version thereof as encoder in the case of image data. The proposed approach is evaluated on CIFAR10, Imagenet 32x32, and the LM1B data set (text).

Pros:

The method obtains state-of-the-art performance in image generation. The paper features extensive ablation experiments and is well-written. Furthermore, it is demonstrated that the code learns an abstract representation by repeatedly sampling form the decoder conditionally on the code.

Cons:

One question that remains is the relative contribution of 1) lower-bounding the KL-term 2) using causal decoder/anti-causal encoder to the overall result. Is the encoder-decoder structure alone enough to prevent posterior collapse? In this context it would also be interesting to see how the encoder-decoder structure performs without \delta-constraint, but with regularization as in \beta-VAE.

What data set are the ablation experiments performed on? As far as I could see this is not specified.

Also, I suggest toning down the claims that the proposed method works ""without altering the ELBO training objective"" in the introduction and conclusion. After all, the encoding and decoding distributions are chosen such that the KL term in the ELBO is lower-bounded by \delta. In other words the authors impose a constraint to the ELBO.

Minor comments:
- Space missing in the first paragraph of p 5: \kappaas
- ""Auxiliary prior""-paragraph on p 5: marginal posterior -> aggregate posterior?","The sentiment of the review is generally positive, as the reviewer acknowledges the state-of-the-art performance, extensive ablation experiments, and the well-written nature of the paper. However, there are some critical points and suggestions for improvement, which slightly temper the overall positivity. Therefore, the sentiment score is 70. The politeness of the language is high, as the reviewer uses polite suggestions and constructive criticism without any rude or harsh language. Thus, the politeness score is 90.",70,90
"The paper introduces a method for online adaptation of a model that is expected to adapt to changes in the environment the model models. The method is based on a mixture model, where new models are spawned using a Chinese restaurant process, and where each newly spawned model starts with weights that have been trained using meta-learning to quickly adapt to new dynamics. The method is demonstrated on model-based RL for a few simple benchmarks.

The proposed method is well justified, clearly presented, and the experimental results are convincing. The paper is generally clear and well written. The method is clearly most useful for situations where the environment suddenly changes, which is relevant in some real-world problems. As a drawback, using a mixture model (that also grows with time) for such modelling can be considered quite heavy in some situations. Nevertheless, the idea of combining a spawning process with meta-learned priors is neat, and clearly works well.

Minor comments:
- Algorithm 1: is the inequality correct, and is T* supposed to be an argmin instead of argmax?","The sentiment of the review is positive, as indicated by phrases such as 'well justified,' 'clearly presented,' 'convincing,' and 'well written.' The reviewer acknowledges the usefulness of the method and its relevance to real-world problems, despite noting a potential drawback. The politeness of the language is high, as the reviewer uses respectful and constructive language, even when pointing out the minor issue with Algorithm 1. The overall tone is supportive and encouraging.",80,90
"Update:

I thank the authors for the response. Unfortunately, the response does not mention modifications made to the paper according to the comments. According to pdfdiff, modifications to the paper are very minor, and none of my comments are addressed in the paper. I think the paper shows good results, but it could very much benefit from improved presentation and evaluation. I do recommend acceptance, but if the authors put more work in improving the paper, it could have a larger impact.

------

The paper proposes a learnable planning model based on value iteration. The proposed methods can be seen as modifications of Value Iteration Networks (VIN), with some improvements aimed at improving sample efficiency and generalization to large environment sizes. The method is validated on gridworld-type environments, as well as on a more complex StarCraft-based domain with raw pixel input.

Pros:
1) The topic of the paper is interesting: combining the advantages of learning and planning seems like a promising direction to achieving adaptive and generalizable systems.
2) The presentation is quite good, although some details are missing.
3) The proposed method can be effectively trained with reinforcement learning and generalizes well to much larger environments than trained on. It beats vanilla VIN by a large margin. The MVProp variant of the method is especially successful.

Cons:
1) I would like to see a more complete discussion of the MVProp method. Propagation of only positive rewards seems like somewhat of a hack. Is this a general solution or is it only applicable to gridworld navigation-type tasks? Why? If not, is the area of applicability of MVProp different from VProp? Also, is the area of applicability of VProp different from VIN? It’s important to discuss this in detail.
2) I wonder how would the method behave in more realistic gridworld environments, for instance similar in layout to those used in RL navigation literature (DMLab, ViZDoom, MINOS, etc). The presented environments are quite artificial and seem to basically only require “obstacle avoidance”, not so much deliberate long-distance planning.
3) Some details are missing. For instance, I was not able to find the exact network architectures used in different tasks. 
Related to this, I was confused by the phrase “As these new environments are not static, the agent needs to re-plan at every step, forcing us to train on 8x8 maps to reduce the time spent rolling-out the recurrent modules.” I might be misunderstanding something, but is there any recurrent network in VProp? Isn’t it just predicting the parameters once and then rolling our value iteration forward without any learning? Is this so time-consuming?
4) Why does the performance even of the best method not reach 100% even in the simpler environments in Figure 2? Why is the performance plateauing far from 100% in the more difficult case? It would be interesting to see more analysis of how the method works, when it fails, and which parts still need improvement. On a related topic, it would be good to see more qualitative results both in MazeBaze and StarCraft - in the form of images or videos.
5) Novelty is somewhat limited: the method is conceptually similar to VIN. 

To conclude, I think the paper is interesting and the proposed method seems to perform well in the tested environments. I am quite positive about the paper, and I will gladly raise the rating if my questions are addressed satisfactorily.","The sentiment of the review is generally positive, as indicated by the recommendation for acceptance and the acknowledgment of good results and interesting topics. However, the sentiment is tempered by the fact that the reviewer's comments were not addressed, and there are several suggestions for improvement. Therefore, the sentiment score is 40. The politeness of the language is high, as the reviewer thanks the authors, provides constructive feedback, and expresses willingness to raise the rating if questions are addressed. Thus, the politeness score is 80.",40,80
"This paper considers how to augment training data by applying class-preserving transformations to selected datapoints.
It proposes improving random datapoint selection by selection policies based on two metrics: the training loss 
associate with each datapoint (""Loss""), and the influence score (from Koh and Liang that approximates leave-one-one test loss). The authors consider two policies based on these metrics: apply transformations to training points in decreasing 
order of their score, or to training points sampled with probability proportional to score. They also consider two 
refinements: downweighting observations that are selected for transformation, and updating scores everytime 
transformations associated with an observation are added. 

The problem the authors tackle is important and their approach is natural and promising. On the downside, the theoretical 
contribution is moderate, and the empirical studies quite limited. 

The stated goals of the paper are quite modest: ""In this work, we demonstrate that it is possible to significantly reduce the 
number of data points included in data augmentation while realizing the same accuracy and invariance benefits of 
augmenting the entire dataset"". It is not too surprising that carefully choosing observations according suitable policies 
is an improvement over random subsampling, especially, when the test data has been ""poisoned"" to highlight this effect. 
The authors have demonstrated that two intuitive policies do indeed work, have quantified this on 3 datasets. 

However they do not address the important question of whether doing so can improve training time/efficiency. In other words, the authors have not attempted to investigate the computational cost of trying to assign importance scores to each observation. Thus this paper does not really demonstrate the overall usefulness of the proposed methodology.

The experimental setup is also limited to (I think) favor the proposed methodology. Features are precomputed on images using a CNN, and the different methods are compared on a logistic regression layer acting on the frozen features. The existence of such a pretrained model is necessary for the proposed methods, otherwise one cannot assign selection scores to different datapoints. However, this is not needed for random selection, where the transformed inputs can directly be input to the system. A not unreasonable baseline would be to train the entire CNN with the augmented 5%,10%, 25% datasets, rather than just the last layer. Of course this now involves training the entire CNN on the augmented dataset, rather than just the last layer, but how relevant is the two stage training approach that the authors propose?

In short, while I think the proposed methodology is promising, the authors missed a chance to include a more thorough analysis of the trade-offs of their method.

I also think the paper makes only a minimal effort to understand the policies, the experiments could have helped shed some more light on this.

Minor point:
The definition of ""influence"" is terse e.g. I do not see the definition of H anywhere (the Hessian of the empirical loss)","The sentiment of the review is mixed. The reviewer acknowledges the importance of the problem and finds the approach promising, which is positive. However, they also point out several significant limitations, such as the moderate theoretical contribution, limited empirical studies, and lack of analysis on computational cost and training efficiency. This results in a sentiment score of around 10. The politeness of the language is quite high. The reviewer uses polite and constructive language throughout, even when pointing out the paper's shortcomings. They offer specific recommendations and maintain a respectful tone, leading to a politeness score of 80.",10,80
"The papers studies neural network-based sparse signal recovery, and derives many new theoretical insights into the classical LISTA model. The authors proposed Analytic LISTA (ALISTA), where the weight matrix in LISTA is pre-computed with a data-free coherence minimization, followed by a separate data-driven learning step for merely (a very small number of) step-size and threshold parameters. Their theory is extensible to convolutional cases. The two-stage decomposed pipeline was shown to keep the optimal linear convergence proved in (Chen et al., 2018). Experiments observe that ALISTA has almost no performance loss compared to the much heavier parameterized LISTA, in contrast to the common wisdom that (brutal-force) “end-to-end” always outperforms stage-wise training. Their contributions thus manifest in both novel theory results, and the practical impacts of simplifying/accelerating LISTA training.  Besides, they also proposed an interesting new strategy called Robust ALISTA to overcome the small perturbations on the encoding basis, which also benefits from this decomposed problems structure. 

The proofs and conclusions are mathematically correct to my best knowledge. I personally worked on similar sparse unfolding problems before so this work looks particularly novel and interesting to me. My intuition then was that, it should not be really necessary to use heavily parameterized networks to approximate a simple linear sparse coding form (LISTA idea). Similar accelerations could have been achieved with line search for something similar to steepest descent (also computational expensive, but need learn step-sizes only, and agnostic to input distribution). Correspondingly, there should exist a more elegant network solution with very light learnable weights. This work perfectly coincides with the intuition, providing very solid guidance on how a LISTA model could be built right. Given in recent three years, many application works rely on unfold-truncating techniques (compressive sensing, reconstruction, super resolution, image restoration, clustering…), I envision this paper to generate important impacts for practitioners pursuing those ideas. 

Additionally, I like Theorem 3 in Section 3.1, on the provable efficient approximation of general convolution using circular convolution. It could be useful for many other problems such as filter response matching. 

I therefore hold a very positive attitude towards this paper and support for its acceptance. Some questions I would like the authors to clarify & improve in revision:

1.	Eqn (7) assumes noise-free case. The author stated “The zero-noise assumption is for simplicity of the proofs.” Could the authors elaborate which part of current theory/proof will fail in noisy case? If so, can it be overcome (even by less “simpler” way)? How about convolutional case, the same? Could the authors at least provide some empirical results for ALISTA’s performance under noise?

2.	Section 5.3. It is unclear to me why Robust ALISTA has to work better than the data augmented ALISTA. Is it potentially because that in the data augmentation baseline, the training data volume is much amplified, and one ALISTA model might become underfitting? It would be interesting to create a larger-capacity ALISTA model (e.g., by increasing unfolded layer numbers), train it on the augmented data, and see if it can compare more favorably against Robust ALISTA?

3.	The writeup is overall very good, mature, and easy to follow. But still, typos occur from time to time, showing a bit rush. For example, Section 5.1, “the x-axes denotes is the indices of layers” should remove “is”. Please make sure more proofreading will be done.

","The review expresses a very positive sentiment towards the paper, highlighting its novel contributions, practical impacts, and solid theoretical foundations. The reviewer also mentions their personal interest and alignment with the paper's findings, further reinforcing the positive sentiment. The language used is polite and constructive, offering specific suggestions for improvement without any negative or rude remarks. The reviewer acknowledges the strengths of the paper and provides detailed, respectful feedback on areas that could be clarified or improved.",90,95
"The authors propose a bidirectional model for learning a policy. In particular, a backtracking model was proposed to start from a high-value state and sample back the sequence of actions and states that could lead to the current high-value state. These traces can be used later for learning a good policy. The experiments show the effectiveness of the model in terms of increase the expected rewards in different tasks. However, learning the backtracking model would add some computational efforts to the entire learning phase. I would like to see experiments to show the computational time for these components. 
","The sentiment of the review is generally positive, as the reviewer acknowledges the effectiveness of the proposed model and its ability to increase expected rewards in different tasks. However, there is a slight concern about the added computational efforts, which is a minor critique. Therefore, the sentiment score is 60. The language used in the review is polite and constructive, as the reviewer uses phrases like 'I would like to see' rather than more demanding language. This indicates a polite tone, so the politeness score is 80.",60,80
"
This paper shows that with a clever initialization method ResNets can be trained without using batch-norm (and other normalization techniques).  The network can still reach state-of-the-art performance.


The authors propose a new initialization method called ""ZeroInit"" and use it to train very deep ResNets (up to 10000 layers). They also show that the test performance of their method matches the performance of state-of-the-art results on many tasks with the help of strong data augmentation. This paper also indicates that the role of normalization in training deep resnets might not be as important as people thought. In sum, this is a very interesting paper that has novel contribution to the practical side of neural networks and new insights on the theoretical side. 

Pros:
1. The analysis is not complicated and the algorithm for ZeroInit is not complicated.  
2. Many people believe normalization (batch-norm, layer-norm, etc. ) not only improves the trainability of deep NNs but also improves their generalization. This paper provides empirical support that NNs can still generalize well without using normalization. It might be the case that the benefits from the data augmentation (i.e., Mixup + Cutout) strictly contain those from normalization. Thus it is interesting to see if the network can still generalize well (achieving >=95% test accuracy on Cifar10)  without using strong data-augmentation like mixup or cutout. 
3.Theoretical analysis of BatchNorm (and other normalization methods) is quite challenging and often very technical. The empirical results of this paper indicate that such analysis, although very interesting, might not be necessary for the theoretical understanding of ResNets.  


Cons:
1.The analysis works for positively homogeneous activation functions i.e. ReLU, but not for tanh or Swish. 
2.The method works for Residual architectures, but may not be applied to Non-Residual networks (i.e. VGG, Inception)  ","The sentiment of the review is highly positive, as evidenced by phrases like 'very interesting paper,' 'novel contribution,' and 'state-of-the-art performance.' The reviewer highlights several strengths of the paper and provides constructive feedback on its limitations. The politeness of the language is also high, as the reviewer uses respectful and encouraging language throughout the review, even when pointing out the cons.",90,95
"The paper proposes a method for generating diverse outputs for various conditional GAN frameworks including image-to-image translation, image-inpainting, and video prediction. The idea is quite simple, simply adding a regularization term so that the output images are sensitive to the input variable that controls the variation of the images. (Note that the variable is not the conditional input to the network.) The paper also shows how the regularization term is related to the gradient penalty term. The most exciting feature about the work is that it can be applied to various conditional synthesis frameworks for various tasks. The paper includes several experiments with comparison to the state-of-the-art. The achieved performance is satisfactory. 

To the authors, wondering if the framework is applicable to unconditional GANs.","The sentiment of the review is positive, as indicated by phrases like 'The most exciting feature about the work' and 'The achieved performance is satisfactory.' The reviewer appreciates the simplicity and applicability of the proposed method. The politeness of the language is also high, as the reviewer uses respectful and constructive language, and even poses a question in a polite manner to further engage the authors.",80,90
"
-- Originality --

This paper studies how to use KL-regularization with information asymmetry to speed up and improve reinforcement learning (RL). Compared with existing work, the major novelty in the proposed algorithm is that it uses a default policy learned from data, rather than a fixed default policy. Moreover, the proposed algorithm also limits the amount of information the default policy receives, i.e., there is an ""information asymmetry"" between the agent policy and the default policy. In many applications, the default policy is purposely chosen to be ""goal agnostic"" and hence conducts the ""transfer learning"". To the best of my knowledge, this ""informationally asymmetric"" KL-regularization approach is novel.

-- Clarify --

The paper is well written in general and is easy to follow.

-- Significance --

I think the idea of regularizing RL via an informationally asymmetric default policy is interesting. It might be an efficient way to do transfer learning (generalization) in some RL applications. This paper has also done extensive and rigorous experiments. Some experiment results are thought-provoking.

-- Pros and Cons

Pros:

1)  The idea of regularizing RL via an informationally asymmetric default policy is interesting. To the best of my knowledge, this ""informationally asymmetric"" KL-regularization approach is novel.

2) The experiment results are extensive, rigorous, and thought-provoking.

Cons:

1) My understanding is that this ""informationally asymmetric"" KL-regularization approach is a general approach and can be combined with many policy learning algorithms. It is not completely clear to me why the authors choose to combine it with an actor-critic approach (see Algorithm 1)? Why not combine it with other policy learning algorithms? Please explain.

2) This paper does not have any theoretical results. I fully understand that it is highly non-trivial or even impossible to analyze the proposed algorithm in the general case. However, I recommend the authors to analyze (possibly a variant of) the proposed algorithm in a simplified setting (e.g. the network has only one layer, or even is linear) to further strengthen the results.

3) The experiment results of this paper are interesting, but I think the authors can do a better job of intuitively explaining the experiment results. For instance, the experiment results show that when the reward is ""dense shaping"", the proposed method and the baseline perform similarly. Might the authors provide an intuitive explanation for this observation? I recommend the authors to try to provide intuitive explanation for all such interesting observations in the paper. 

","The sentiment of the review is generally positive. The reviewer appreciates the novelty of the approach and the thoroughness of the experiments, which is reflected in statements like 'the idea of regularizing RL via an informationally asymmetric default policy is interesting' and 'the experiment results are extensive, rigorous, and thought-provoking.' However, the reviewer also points out some areas for improvement, such as the need for theoretical results and better explanations of experimental outcomes. The politeness of the language is high, as the reviewer uses phrases like 'please explain' and 'I recommend,' which are courteous and constructive. The reviewer also acknowledges the difficulty of providing theoretical results, showing an understanding and respectful tone.",80,90
"In this paper, authors proposed a so-called FLOWQA for conversational question answering (CoQA). Comparing with machine reading comprehension (MRC),  CoQA includes a conversation history. Thus, FLOWQA makes use of this property of CoQA and adds an additional encoder to handle this. It also includes one classifier to handle with no-answerable questions.

Pros:
The idea is pretty straightforward which makes use of the unique property of CoQA.

Results are strong, e.g., +7.2 improvement over current state-of-the-art on the CoQA dataset. 

The paper is well written.

Cons:
It is lack of detailed analysis how the conversation history affects results and what types of questions the proposed model are handled well.

Limited novelty. The model is very similar to FusionNet (Huang et al, 2018) with an extra history encoder and a no-answerable classifier. 

Questions:
One of simple baseline is to treat this as a MRC task by combining the conversation history with documents. Do you have this result?

The model uses the full history. Have you tried partial history? What's the performance? 
","The sentiment of the review is generally positive, as indicated by the praise for the straightforward idea, strong results, and well-written paper. However, there are some criticisms regarding the lack of detailed analysis and limited novelty, which slightly temper the overall positive sentiment. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, providing specific feedback and questions without being harsh or dismissive. Thus, the politeness score is 80.",60,80
"The authors formulate planning as sampling from an intractable distribution motivated by control-as-inference, propose to approximately sample from the distribution using a learned model of the environment and SMC, then evaluate their approach on 3 Mujoco tasks. They claim that their method compares favorably to model-free SAC and to CEM and random shooting (RS) planning with model-based RL.

This is an interesting idea and an important problem, but there appear to be several inconsistencies in the proposed algorithm and the experimental results do not provide compelling support for the algorithm. In particular,

Levine 2018 explains that with stochastic transitions, computing the posterior leads to overly optimistic behavior because the transition dynamics are not enforced, whereas the variational bound explicitly enforces that. Is that an issue here?

The value function estimated in SAC is V^\pi the value function of the current policy. The value function needed in Sec 3.2 is a different value function. Can the authors clarify on this discrepancy?

The SMC procedure in Alg 1 appears to be incorrect. It multiplies the weights by exp(V_{t+1}) before resampling. This needs to be accounted for by setting the weights to exp(-V_{t+1}) instead of uniform. See for example auxiliary particle filters.

The experimental section could be significantly improved by addressing the following points: 
* How was the planning horizon h chosen? Is the method sensitive to this choice? What is the model accuracy?
* Does CEM use a value function? If not, it seems like a reasonable baseline to consider CEM w/ a value function to summarize the values beyond the planning horizon. This will evaluate whether SMC or including the value function is important. 
* Comparing to state-of-the-art model-based RL (e.g., one of Chua et al. 2018, Kurutach et al. 2018, Buckman et al. 2018). 
* How were the task # of steps chosen? They seem arbitrary. What is the performance at 1million and 5million steps?
* Was SAC retuned for this small number of samples/steps?
* Clarify where the error bars come from in Fig 5.2 in the caption.
At the moment, SMCP is within the error bars of a baseline method.

Comments:

In the abstract, the authors claim that the major challenges in planning are: 1) model compounding errors in roll-outs and 2) the exponential search space. Their method only attempts to address 2), is that correct? If so, can the authors state that explicitly.

Recent papers (Chua et al. 2018, Kurutach et al. 2018, Buckman et al. 2018, Ha and Schmidhuber 2018) all show promising model-based results on continuous state/action tasks. These should be mentioned in the intro.

The connection between Gu et al.'s work on SMC and SAC was unclear in the intro, can the authors clarify?

For consistency, ensure that sums go to T instead of \infty.

I found the discussion of SAC at the end of Sec 2.1 confusing. As I understand SAC, it does try to approximate the gradient of the variational bound directly. Can the authors clarify what they mean?

At the end of Sec 2.2, the authors claim that the tackle the particle degeneracy issue (a potentially serious issue) by ""selecting the temperature of the resampling distribution to not be too low."" I could not find further discussion of this anywhere in the paper or appendix.

Sec 3.2, mentions an action prior for the first time. Where does this come from?

Sec 3.3 derives updates assuming a perfect model, but we learn a model. What are the implications of this?

Please ensure the line #'s and the algorithm line #'s match.

Model learning is not described in the main text though it is a key component of the algorithm. The appendix lacks details (e.g., what is the distribution used to model the next state?) and contradicts itself (e.g., one place says 3 layers and another says 2 layers).

In Sec 4.1, a major difference between MCTS and SMC is that MCTS runs serially, whereas SMC runs in parallel. This should be noted and then it's unclear whether SMC-Planning should really be thought of as the maximum entropy tree search equivalent of MCTS.

In Sec 4.1, the authors claim that Alpha-Go and SMCP learn proposals in similar ways. However, SMCP minimizes the KL in the reverse direction (from stated in the text). This is an important distinction.

In Sec 4.3, the authors note that Gu et al. learn the proposal with the reverse KL from SMCP. VSMC (Le et al. 2018, Naesseth et al. 2017, Maddison et al. 2017) is the analogous work to Gu et al. that learn the proposal using the same KL direction as SMCP. The authors should consider citing this work as it directly relates to their algorithm.

In Sec 4.3, the authors claim that their direction of minimizing KL is more appropriate for exploration. Gu et al. suggest the opposite in their work. Can the author's justify their claim?

In Sec 5.1, the authors provide an example of SMCP learning a multimodal policy. This is interesting, but can the authors explain when this will be helpful?

====

11/26
At this time, the authors have not responded to reviews. I have read the other reviews. Given the outstanding issues, I do not recommend acceptance.

12/7
After reading the author's response, I have increased my score. However, baselines that establish the claim that SMC improves planning which leads to improved control are missing (such as CEM + value function). Also, targeting the posterior introduces an optimism bias that is not dealt with or discussed.","The sentiment of the review is mixed. The reviewer acknowledges that the idea is interesting and the problem is important, but they also point out several inconsistencies and areas for improvement. This results in a sentiment score of -20. The language used is generally polite and constructive, with specific recommendations and questions aimed at improving the paper, leading to a politeness score of 80.",-20,80
"The idea, transforming the input data to an output space in which the data is distributed uniformly and thus indexing is easier, is interesting. 

My main concerns come from experimental results.

(1) Table 1: where are the results of OPQ and LSQ from? run the codes by the authors of this paper? or from the original paper?

It is not consistent to the LSQ paper (https://www.cs.ubc.ca/~julm/papers/eccv16.pdf). For BigANN1M, from the LSQ paper, the result is >29 recall at 1 for 64 bits. 

(2) Figure 5: similarly, how did you get the results of PQ and OPQ?

(3) There are some other advanced algorithms: e.g.,  additive quantization (Babenko & Lempitsky, 2014) and composite quantization (https://arxiv.org/abs/1712.00955)

The above points make it hard to judge this paper.","The sentiment of the review is slightly negative, as the reviewer expresses concerns about the experimental results and the consistency of the data presented. The reviewer does acknowledge that the idea is interesting, which prevents the sentiment from being entirely negative. Therefore, the sentiment score is -30. The politeness score is 20, as the reviewer uses polite language but is direct and critical in their feedback. The reviewer asks questions and points out inconsistencies without using harsh or rude language.",-30,20
"The authors propose a novel method for learning graph convolutional networks. The core idea is to use the Lanczos algorithm to obtain a low-rank approximation of the graph Laplacian. The authors propose two ways to include the Lanczos algorithm. First, as a preprocessing step where the algorithm is applied once on the input graph and the resulting approximation is fixed during learning. Second, by including a differentiable version of the algorithm into an end-to-end trainable model. 

The proposed method is novel and achieves good results on a set of experiments. 

The authors discuss related work in a thorough and meaningful manner. 

There is not much to criticize. This is a very good paper. The almost 10 pages are perhaps a bit excessive considering there was an (informal) 8 page limit. It might make sense to provide a more accessible discussion of the method and Theorem 1, and move some more detailed/technical parts in pages 4, 5, and 6 to an appendix. 
","The review is highly positive, as indicated by phrases such as 'novel method,' 'good results,' 'thorough and meaningful,' and 'very good paper.' The only criticism is minor and constructive, suggesting a slight reduction in length and reorganization of content. The language used is very polite, with no negative or harsh words, and the suggestions are framed in a helpful manner.",90,100
"PAPER SUMMARY
-------------
This paper proposes an approach to video prediction which autonomously finds an action space encoding differences between subsequent frames. This approach can be used for action-conditioned video prediction and visual servoing. 
Unlike related work, the proposed method is initially trained on video sequences without ground-truth actions. A representation for the action at each time step is inferred in an unsupervised manner. This is achieved by imposing that the representation of this action be as small as possible, while also being composable, i.e. that that several actions can be composed to predict several frames ahead.
Once such a representation is found, a bijective mapping to ground truth actions can be found using only few action-annotated samples. Therefore the proposed approach needs much less annotated data than approaches which directly learn a prediction model using actions and images as inputs.

The approach is evaluated on action-conditioned video prediction and visual servoing. The paper shows that the learned action-space is meaningful in the sense that applying the same action in different initial condition indeed changes the scenes in the same manner, as one would intuitively expect. Furthermore, the paper shows that the approach achieves state of the art results on a action-conditioned video prediction dataset and on a visual servoing task.

POSITIVE POINTS
---------------
The idea of inferring the action space from unlabelled videos is very interesting and relevant.

The paper is well written.

The experimental results are very interesting, it is impressive that the proposed approach manages to learn meaningful actions in an unsupervised manner (see e.g. Figure 3).

NEGATIVE POINTS
---------------
It is not exactly clear to me how the model is trained for the quantitative evaluation. On which sequences is the bijective mapping between inferred actions and true actions learned? Is is a subset of the training set? If yes, how many sequences are used? Or is this mapping directly learned on the test set? This, however, would be an unfair comparison in my opinion, since then the actions would be optimized in order to correctly predict on the tested sequences.

The abstract and introduction are too vague and general. It only becomes clear in the technical and experimental section what problem is addressed in this paper.","The sentiment of the review is generally positive, as the reviewer highlights several strengths of the paper, such as the interesting and relevant idea, the well-written nature of the paper, and the impressive experimental results. However, the reviewer also points out some areas of improvement, particularly regarding the clarity of the training process and the vagueness of the abstract and introduction. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, even when pointing out negative aspects. Therefore, the politeness score is 80.",60,80
"General:
The paper tackles one of the most important problems of learning VAEs, namely, the posterior collapse. Typically, this problem is attacked by either proposing a new model or modifying the objective. Interestingly, the authors considered a third option, i.e., changing the training procedure only, leaving the model and the objective untouched. Moreover, they show that in fact the modified objective (beta-VAE) could drastically harm training a VAE.

I find the idea very interesting and promising. The proposed algorithm is very easy to be applied, thus, it could be easily reproduced. I believe the paper should be presented at the ICLR 2019.

Pros:
+ The paper is written in a lucid manner. All ideas are clearly presented. I find the toy problem (Figure 2) very illuminating.
+ It might seem that the idea follows from simple if not even trivial remarks. But this impression is fully due to the fashion the authors presented their idea. I am truly impressed by the writing style of the authors.
+ I find the proposed approach very appealing because it requires changes only in the optimization procedure while the model and the objective remain the same. Moreover, the paper formalizes some intuition that could be found in other papers (e.g., (Alemi et al., 2018)).
+ The presented results are fully convincing.

Cons:
- It would be beneficial to see samples for the same latent variables to verify whether the model utilizes the latent code. Additionally, a latent space interpolation could be also presented.
- The choice of the stopping criterion seems to be rather arbitrary. Did the authors try other methods? If yes, what were they? If not, why the current stopping criterion is so unique?
- The proposed approach was applied to the case when the prior is a standard Normal. What would happen if a different prior is considered?

Neutral remark:
* Another problem, next to the posterior collapse, is the “hole problem” (see Rezende & Viola, “Taming VAEs”, 2018). A natural question is whether the proposed approach also helps to solve this issue? One possible solution to that problem is to take the aggregated posterior as the prior (e.g., (Tomczak & Welling, 2018)) or to ensure that the KL between the aggregated posterior and the prior is small. In Figure 4 it seems it is the case, however, I am really curious about the authors’ opinion on this matter.
* Can the authors relate the proposed algorithm to the wake-sleep algorithm? Obviously, the motivation is different, however, I find these two approaches a bit similar in spirit.

--REVISION--
I would like to thank the authors for their comments. In my opinion the paper is very interesting and opens new directions for further research (as discussed by the authors in their reply). I strongly believe the paper should be accepted and presented at the ICLR.","The review is highly positive, praising the novelty and applicability of the proposed approach. The reviewer finds the idea interesting, promising, and well-presented, and strongly recommends the paper for acceptance. The language used is very polite and appreciative, with constructive feedback provided in a respectful manner. The reviewer also thanks the authors for their comments and reiterates their support for the paper's acceptance.",90,95
"This paper considers the problem of inferring unspecified costs in an RL problem (e.g., inferring that vases in a room should not be broken). The primary insight is that the initial state of the environment conveys rich information about such unspecified costs since environments are often optimized for humans. The paper frames the problem of inferring unspecified costs from the initial condition as an inverse reinforcement learning (IRL) problem and applies the Maximum Causal Entropy IRL framework to solve this problem. Two methods are proposed for combining the inferred unspecified costs with specified costs. The efficacy of the proposed approach is demonstrated on a number of simulated examples.

Overall, I was impressed by this paper and I believe that it makes a strong contribution. The paper presents an interesting perspective on a relatively old problem (the frame problem in AI). The primary intuition of the paper (that the initial state conveys information about unspecified costs) and the framing of this problem in terms of IRL is novel. The simulated examples (while relatively simple in terms of the number of states and actions) are informative and demonstrate the strengths of the approach (and also some of the weaknesses; the paper is explicit about the current challenges). The paper is very clearly written and is easy to read.

My concerns are relatively minor:
- Perhaps the weakest bit of the paper is Section 5 (combining the specified reward with the inferred reward). As presented, the Additive method is somewhat hard to justify. However, the simulated results suggest that the Additive method performs slightly better than the Bayesian method. I would suggest either presenting a bit more intuition and justification for the Additive method or getting rid of this method altogether (since the results are not too different from the Bayesian method, which seems a bit more justifiable).
- One practical (and potentially important) question that the paper does not directly address is the problem of choosing the time horizon T (i.e., the time horizon for the past). In the standard IRL setting, it is reasonable to assume that the time horizon is given (since the demonstrations have an associated horizon). However, it is not entirely clear how to choose T in the setting considered in this paper. It is possible that if one chooses T to be too small, the inferred rewards will not be accurate (and one may have to look further back in the past to correctly infer rewards). A discussion of this issue and possible ways to choose T would be helpful.
- In Section 6.1 (baselines), the paper mentions that ""while relative reachability makes use of known dynamics, it does not benefit from our handcoded featurization"". Is it possible to modify the relative reachability method to also take advantage of the handcoded features, perhaps by considering dynamics over the feature space? If not, a sentence explaining that this is not straightforward would be helpful.
- In the related work section (and also in the introduction), I would recommend being more explicit about precisely what the differences are between the presented work and the approaches presented in (Krakovna et al. 2018) and (Turner, 2018). The paper is currently slightly vague about the differences.
- Currently, the title of the paper is a bit uninformative. On first reading the title, I expected a paper on control theory; the title makes no mention of unspecified costs, or reinforcement learning, or humans, etc. I believe that this is a good paper and that the paper would have more readers if the title was more inline with the content of the paper. Of course, this is at the discretion of the authors. My suggestion would be something along the lines of ""Inferring Unspecified Rewards in RL from the Initial State"".

Typos:
- Pg. 1, second paragraph, 3rd line: there is a placeholder for citations.
- Periods are missing at the end of equations.
","The sentiment of the review is highly positive, as indicated by phrases like 'I was impressed by this paper' and 'I believe that it makes a strong contribution.' The reviewer appreciates the novelty and clarity of the paper, as well as the informative examples. The sentiment score is therefore 90. The politeness of the language is also very high. The reviewer uses polite and constructive language throughout, offering suggestions and minor concerns in a respectful manner. Phrases like 'I would suggest' and 'My concerns are relatively minor' indicate a high level of politeness. The politeness score is therefore 95.",90,95
"The paper proposed a defensive mechanism against adversarial attacks using GANs. The general network structure is very much similar to a standard GANs -- generated perturbations are used as adversarial examples, and a discriminator is used to distinguish between them. The performance on MNIST, SVHN, and CIFAR10 demonstrate the effectiveness of the approach, and in general, the performance is on par with carefully crafted algorithms for such task. 

pros
- the presentation of the approach is clean and easy-to-follow.
- the proposed network structure is simple, but it surprisingly works well in general. 
- descriptions of training details are reasonable, and the experimental results across several datasets are extensive

cons
- the network structure may not be novel, though the performance is very nice. 
- there are algorithms that are carefully crafted to perform the network defense mechanism, such as Samangouei et al, 2018. However, the method described in this paper, despite simple, works very good. It would be great if authors can provide more insights on why it works well (though not the best, but still reasonable), besides only demonstrating the experimental results.
- it would also be nice if authors can visualize the behavior of their design by showing some examples using the dataset they are working on, and provide side-to-side comparisons against other approaches.","The sentiment of the review is generally positive, as the reviewer acknowledges the effectiveness of the proposed approach and highlights several strengths such as the clean presentation, simplicity, and extensive experimental results. However, there are some minor criticisms regarding the novelty of the network structure and the need for more insights and visualizations. Therefore, the sentiment score is 70. The language used in the review is polite and constructive, with suggestions framed in a helpful manner rather than as harsh criticisms. Therefore, the politeness score is 90.",70,90
"This paper addresses the generalization of adversarial training by proposing a new domain adaptation method. In order to have robust defense for adversarial examples, they combine supervised and unsupervised learning for domain adaptation. The idea of domain adaptation is to increase the similarity between clear and adversarial examples. For this purpose, in their objective, they are minimizing the domain shift by aligning the covariance matrix and mean vector of the clean and adversarial examples.

From experimental viewpoint, they have lower performance than almost all competitors on clean data, but they are beating them when there is white-box as well as the back-box threats. This means their method gives a good generalization. In CIFAR-100 they do not have this trade-off for accuracy and generalization; they are beating other competitors in clean data as well.

The paper is clear and well-written. The introduction and background give useful information. 

In general, I think the paper has a potential for acceptance, but I have to mention that I am not an expert in Adversarial networks area.

 ","The sentiment of the review is generally positive, as the reviewer acknowledges the potential for acceptance and highlights the strengths of the paper, such as its clear writing and useful introduction and background. However, there is a slight reservation due to the reviewer's self-admitted lack of expertise in the specific area of Adversarial networks. Therefore, the sentiment score is 70. The politeness of the language is very high, as the reviewer uses respectful and constructive language throughout the review, making the politeness score 90.",70,90
"The paper proposes a subgradient descent method to learn orthogonal, squared /complete n x n  dictionaries under l1 norm regularization. The problem is interesting and relevant, and the paper, or at least the first part, is clear.

The most interesting property is that the solution does not depend on the dictionary initialization, unlike many other competing methods. 

The experiments sections in disappointingly short. Could the authors play with real data? How does sparsity affect the results? How does it change with different sample complexities? Also, it would be nice to have a final conclusion section. I think the paper contains interesting material but, overall, it gives the impression that the authors rushed to submit the paper before the deadline!","The sentiment of the review is mixed. The reviewer acknowledges the interesting and relevant problem addressed by the paper and appreciates the clarity of the first part and the unique property of the solution not depending on dictionary initialization. However, the reviewer expresses disappointment with the brevity of the experiments section and suggests several improvements, including using real data, exploring the effect of sparsity, and adding a conclusion section. The sentiment score is therefore moderately positive but tempered by the criticisms. The language used is polite, with constructive feedback and suggestions for improvement, although the final comment about rushing to meet the deadline is slightly blunt.",30,60
"The authors propose three improvements to the DNC model: masked attention, erasion of de-allocated elements, and sharpened temporal links --- and show that this allows the model to solve synthetic memory tasks faster and with better precision. They also show the model performs better on average on bAbI than the original DNC.

The negatives are that the paper does not really show this modified DNC can solve a task that the original DNC could not. As the authors also admit, there have been other DNC improvements that have had more dramatic improvements on bAbI.

I think the paper is particularly clearly written, and I would vote for it being accepted as it has implications beyond the DNC. The fact that masked attention works so much better than the standard cosine-weighted content-based attention is pretty interesting in itself. The insights (e.g. Figure 5) are interesting and show the study is not just trying to be a benchmark paper for some top-level results, but actually cares about understanding a problem and fixing it. Although most recent memory architectures do not seem to have incorporated the DNC's slightly complex memory de-allocation scheme, any resurgent work in this area would benefit from this study.","The sentiment of the review is generally positive. The reviewer acknowledges the improvements proposed by the authors and their effectiveness, particularly highlighting the clarity of the paper and its broader implications. However, the reviewer also points out some limitations, such as the lack of evidence that the modified DNC can solve tasks that the original DNC could not, and mentions other improvements with more dramatic results. Despite these negatives, the overall sentiment leans towards acceptance. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer uses respectful and constructive language throughout the review, even when pointing out the negatives. The reviewer appreciates the authors' efforts and provides specific feedback in a courteous manner. Therefore, the politeness score is 90.",60,90
"
In this paper, the authors address the problem of learning to achieve perceptually specified goals in a fully unsupervised way. For doing so, they simultaneously learn a goal-conditioned policy and a goal achievement reward function based on the mutual information between goals sampled from an a priori distribution and states achieved using the goal-conditioned policy. These two learning processes are coupled through the mutual information criterion, which seems to result in efficient state representation learning for the visual specified goal space. A key feature is that the resulting metrics in the visual goal space helps the agent focus on what it can control and ignore distractors, which is critical for open-ended learning.

Overall, the idea looks very original and promissing, but the methods are quite difficult to understand under the current form, the messages from the results are not always clear, and the lack of ablative studies makes it difficult to determine which of the mechanisms are crucial in the system performance and which are not.

* Clarification of the methods:

Given the key features outlined above, I believe the work described in this paper has a lot of potential, but the main issue is that the methods are not easy to get, and the authors could do a better job in that respect. Here is a list of remarks meant to help the authors write a clearer presentation of their method:

- the ""problem formulation"" section contains various things. Part of it could be inserted as a subsection in Section 3, and the last paragraph may rather come into the related work section.

- in Section 3, optimization paragraph, the details given after ""As will be discussed""... might rather come in Section 4 were most of all other details are given.

- in Section 4, I would refer to Algorithm 1 only in the end of the section after all the details have been explained: I went first to the algorithm and could not understand many details that are explained only afterwards.

- in Algorithm 1, shouldn't the two procedures be called ""Imitator"" and ""Teacher"", rather than ""actor"" and ""learner"", to be consistent with the end of Section 3?

- there must be a mathematical relationship between $\xsi_\phi$ and $\hat{q}$, but I could not find this relationship anywhere in the text. What is $\xsi_\phi$ is never introduced clearly...

- p4: we treat h as fixed ... => explain why.

- I don't have a strong background about variational methods, and it is unclear to me why using an expanding set of goals corresponding to already seen states recorded in a buffer makes it that maximizing the log likelihood given in (4) is easier than something else.

More generally, the above are local remarks from a reader who did not succeed in getting a clear picture of what is done exactly and why. Anything you can do to give a more didactic account of the methods is welcome.

* Related work:

The related work section is too poor for a strong paper like this one. Learning to reach goals and learning goal representations are two extremely active domains at the moment and the authors should position themselves with respect to more of these works. Here is a short list in which the authors may find many more relevant papers:

 (Machado and Bowling, 2016), (Machado et al., 2017), GoalGANs (Florensa et al., 2018), RIG (Nair et al., 2018), Many-Goals RL (Veeriah et al., 2018), DAYN (Eysenbach et al., 2018), FUN (Vezhnevets et al., 2017), HierQ, HAC (Levy et al., 2018), HIRO (Nachum et al., 2018), IMGEP (Pere et al., 2018), MUGL IMGEP (Laversanne-Finot et al., 2018).

It would also be useful to position yourself with respect to Sermanet et al. : ""Unsupervised Perceptual Rewards for Imitation Learning"".

About state representation learning, if you consider the topic as relevant for your work, you might have a look at the recent survey from Lesort et al. (2018).

External comments on ICLR web site also point to missing references. The authors should definitely consider doing a much more serious job in positioning their work with respect to the relevant literature.

* Experimental study:

The algorithm comes with a lot of mechanisms and small tricks (at the end of Section 3 and in Section 4) whose importance is never assessed by specific experimental studies. This matters all the more than some of the details do not seem to be much principled. It would be nice to have elements to figure out how important they are with ablative studies putting them aside and comparing performance. Among other things, I would be glad to know how well the system performs without its HER component. Is it critical?

The same about the goal sampling strategy, as mentioned in the discussion: how critical is it in the performance of the algorithms?

- Fig. 1b is not so easy to exploit: it is hard to figure out what the reader should actually extract from these figures

- difficult tasks like cartpole: other papers mention cartpole as a rather easy task.

In the begining of Section 4, the authors mention that the mechanisms of DISCERN naturally induce a form of curriculum (which may be debated), but this aspect is not highlighted clearly enough in the experimental study.

In my opinion, studying fewer environments but giving a more detailed analysis of the performance of DISCERN and its variations in these environment would make the paper stronger.



* typos:

p3: the problem (of) learning a goal achievement reward function

In (3), p_g should most probably be p_{goal}

p4: we treated h(.) ... and did not adapt => treat, do not

p9: needn't => need not
","The sentiment of the review is mixed. The reviewer acknowledges the originality and potential of the work, but also points out significant issues with the clarity of the methods, the lack of ablative studies, and insufficient related work. This results in a sentiment score of 20. The language used is constructive and aims to help the authors improve their paper, with specific suggestions and a polite tone, resulting in a politeness score of 80.",20,80
"First, the writing can be better. I had a hard time to understand the paper. It has many symbols, but some of them are not explained. For instance, in  formula (9), what are Q or s? Also, formula (14). I probably can guess them. Is it possible to simplify the notations or use a table to list the symbols? 

Finding good models is a bi-level or tri-level optimization problem. The paper describes a gradient-based hyperparameter optimization method, which finds model parameters, hyperparameter schedules, and network structure (limited) the same time. It is a interesting idea. Comparing random search, grid search and Spearmint, it seems to be better them. The paper rules out the performance gain is from the randomness of the hyperparameters, which is a good thought. 

More evidences are needed to show this method is superior. The paper doesn't explain well why it works, and the experimental results are just ok. The network architecture search part is limited to number of filters in the experiments. Certainly, the results is not as good as  PNASNet or NASNet. 

Evolution algorithm or GA shows good performance in hyperparameter optimization or neural architecture search. Why not compare with them? Random and grid search are not good generally, and Bayesian optimization is expensive and its performance depends on implementation.   

In Table 2 and figure 4, should ""Loss"" be ""Error""? 


","The sentiment of the review is mixed. The reviewer acknowledges the interesting idea and some positive aspects of the paper, such as ruling out performance gain from randomness. However, the review also points out several significant issues, such as poor writing, lack of explanation for symbols, and insufficient evidence for the method's superiority. Therefore, the sentiment score is slightly negative. The politeness score is neutral to slightly negative as well. The reviewer uses direct language and points out flaws without much cushioning, which can come across as somewhat blunt.",-20,-10
"Summary: This paper aims to find important regions to classify an image. The main algorithm, FIDO, is trained to find a saliency map based on SSR or SDR objective functions. The main novelty of this work is that it uses generative models to in-fill masked out regions by SSR or SDR. As such, compared to existing algorithms, FIDO can synthesize more realistic samples to evaluate.

I like the motivation of this paper since existing algorithms have clear limitations, i.e., using out-of-distribution samples. This issue can be addressed by using a generative network as described in this paper.

However, I think this approach yields another limitation: the performance of the algorithm is bound by the generative network. For example, let’s assume that a head region is important to classify birds. Also assume that the proposed algorithm somehow predicts a mask for the head region during training. If the generative network synthesizes a realistic bird from the mask, then the proposed algorithm will learn that the head region is a supporting region of SSR. In the other case, however, the rendered bird is often not realistic and classified incorrectly. Then, the algorithm will seek for other regions. As a result, the proposed method interprets a classifier network conditioned on the generative network parameters. Authors did not discuss these issues importantly in the paper.

Although the approach has its own limitation, I still believe that the overall direction of the paper is reasonable. It is because I agree that using a generative network to in-fill images to address the motivation of this paper is the best option we have at this current moment. In addition, authors report satisfactory amount of experimental results to support their claim.

Quality: The paper is well written and easy to follow.

Clarify: The explanation of the approach and experiments are clear. Since the method is simple, it also seems that it is easy to reproduce their results.

Originality: Authors apply off-the-shelf algorithms to improve the performance of a known problem. Therefore, I think there is no technical originality except that authors found a reasonable combination of existing algorithms and a problem.

Significance: The paper has a good motivation and deals with an important problem. Experimental results show improvements. Overall, the paper has some amount of impact in this field.

Pros and Cons are discussed above. As a summary,
Pros: 
+ Good motivation.
+ Experiments show qualitative and quantitative improvements.

Cons: 
- Lack of technical novelty and justification of the approach.
","The sentiment of the review is generally positive, as the reviewer appreciates the motivation and direction of the paper, acknowledges the satisfactory experimental results, and finds the paper well-written and easy to follow. However, the reviewer also points out a significant limitation related to the dependency on the generative network and mentions a lack of technical novelty. Therefore, the sentiment score is not fully positive but leans towards the positive side. The language used in the review is polite and constructive, providing balanced feedback without being harsh or dismissive.",60,80
"After author response, I have increased my score. I'm still not 100% sure about the interpretation the authors provided for the negative distances. 

The paper is well written and is mostly clear. (1st line on page 4 has a typo, \bar{x}_k in eq (4) should be \bar{x}^l?)

Novelty: I am not sure whether the paper adds any significant on top of what we know from Bartlett et al., Elsayed et al. since:

(i). The fact that ""normalized"" margins are strongly correlated with the test set accuracy was shown in Bartlett et al. (figure 1.). A major part of the definition comes from there or from the reference they cite; 
(ii). Taylor approximation to compute the margin distribution is in Elsayed et al.; 
(iii). I think the four points listed in page 2 (which make the distinction between related work) is misleading: the way I see it is that the authors use the margin distribution in Elsayed et al which simply overcomes some of the obstacles that norm based margins may face. The only novelty here seems to be that the authors use the margin distribution at each layer. 

Technical pitfalls: Computing the d_{f,x,i,j} using Equation (3) is missing an absolute value in the numerator as in equation (7) Elsayed et al.. The authors interpret the negative values as misclassification: why is it true? The margin distribution used in Bartlett et al. (below Figure 4 on page 5 in arxiv:1706.08498) uses labeled data and it is obvious in this case to interpreting negative values as misclassification. I don't see how this is true for eq (3) here in this paper. Secondly, why are negative points ignored?? Misclassified points in my opinion are equally important, ignoring the information that a point is misclassified doesn't sound like a great idea. How do the experiments look if we don't ignore them?

Experiments: Good set of experiments. However I find the results to be mildly taking the claims of the authors made in four points listed in page 2 away: Section 4.1, ""Empirically, we found constructing this only on four evenly-spaced layers, input, and 3 hidden layers, leads to good predictors."". How can the authors explain this? 

By using linear models, authors implicitly assume that the relationship between generalization gaps and signatures are linear (in Eucledian or log spaces). However, from the experiments (table 1), we see that log models always have better results than linear models. Even assuming linear relationship, I think it is informative to also provide other metrics such as MSE, AIC, BIC etc..","The sentiment of the review is moderately positive, as the reviewer acknowledges that the paper is well-written and clear, and appreciates the good set of experiments. However, the reviewer also expresses significant concerns about the novelty and technical aspects of the paper, which tempers the overall positive sentiment. Therefore, the sentiment score is 30. The politeness of the language is quite high, as the reviewer uses polite language throughout, even when pointing out issues and making suggestions for improvement. The reviewer avoids harsh or rude language and maintains a constructive tone. Therefore, the politeness score is 80.",30,80
"The manuscript entitled ""Kernel Change-Point Detection with Auxiliary Deep Generative Models"" describes a novel approach to optimising the choice of kernel towards increased testing power in this challenging machine learning problem.  The proposed method is shown to offer improvements over alternatives on a set of real data problems and the minimax objective identified is well motivated, however, I am not entirely convinced that (a) the performance improvements arise for the hypothesised reasons, and (b) that the test setting is of wide applicability.

A fundamental distinction between parametric and non-parametric tests for CPD in timeseries data is that the adoption of parametric assumptions allows for an easier introduction of strict but meaningful relationships in the temporal structure---e.g. a first order autoregressive model introduces a simple Markov structure---whereas non-parametric kernel tests typically imagine samples to be iid (before and after the change-point).  For this reason, the non-parametric tests may lack robustness to certain realistic types of temporal distributional changes: e.g. in the parameter of an autoregressive timeseries.  On the other hand, it may be prohibitively difficult to design parametric models to well characterise high dimensional data, whereas non-parametric models can typically do well in high dimension when the available data volumes are large.  In the present application it seems that the setting imagined is for low dimensional data of limited size in which there is likely to be non-iid temporal structure (i.e., outside the easy relative advantage of non-parametric methods).  For this reason it seems to me the key advantage offered by the proposed approach with its use of a distributional autoregressive process for the surrogate model may well be to introduce robustness against Type 1 errors due to otherwise unrepresented temporal structure in the base distribution (P).  In summarising the performance results by AUC it is unclear whether it is indeed the desired improvement in test power that offers the advantages or whether it is in fact a decrease in Type 1 errors.

Another side of my concern here is that I disagree with the statement: ""As no prior knowledge of Q ... intuitiviely, we have to make G as close to P as possible"" interpretted as a way to maximise test power; as a way to minimise Type 1 errors, yes.

Across change-point detection methods it is also important to distinguish key aspects of the problem formulation.  One particular specification here is that we have already some labelled instances of data known to come from the P distribution, and perhaps also some fewer instances of data labelled from Q.  This is distinct from fully automated change point detection methods for time series such as automatic scene selection in video data.  Another dissimilarity to that archetypal scenario is that here we suppose the P and Q distributions may have subtle differences that we're interested in; and it would also seem that we assume there is only one change-point to detect.  Or at least the algorithm does not seem to be designed to be applied in a recursive sense as it would be for scene selection.

Finally there is no discussion here of computational complexity and cost?","The sentiment of the review is mixed, with both positive and negative elements. The reviewer acknowledges the novelty and potential improvements of the proposed method but expresses significant concerns about its applicability and the reasons behind its performance improvements. Therefore, the sentiment score is slightly negative. The language used is formal and polite, with the reviewer providing constructive criticism and suggestions without being rude or dismissive.",-20,80
"Summary:
The paper proposes a fast method for generating visual metamers – physically different images that cannot be told apart from an original – via foveated, fast, arbitrary style transfer. The method achieves the same goal as an earlier approach (Freeman & Simoncelli 2011): locally texturizing images in pooling regions that increase with eccentricity, but is orders of magnitude faster. The authors perform a psychophysical evaluation to test how (in)discriminable their synthesized images are amongst each other and compared with originals. Their experiment replicates the result of Freeman & Simoncelli of a V2-like critical scaling in the synth vs. synth condition, but shows that V1-like or smaller scaling is necessary for the original vs. synth condition.

I reviewed an earlier version of this paper for a different venue, where I recommended rejection. The authors have since addressed some of my concerns, which is why I am more positive about the paper now.

Strengths:
+ The motivation for the work is clear and the implementation straightforward, combining existing tools from style transfer in a novel way.
+ It's fast. Rendering speed is indeed a bottleneck in existing methods, so a fast method is useful.
+ The perceptual quality of the rendered images is quantified by psychophysical testing.
+ The role of the scaling factor for the pooling regions is investigated and the key result of Freeman & Simoncelli (pooling regions scale with 0.5*eccentricity) is replicated with the new method. In addition, the result of Wallis et al. (2018) that lower scale factors are required for original vs. synth is replicated as well.


Weaknesses:
- Compared with earlier work, an additional fudge parameter (alpha) is introduced. It is not clear why it is necessary and it complicates interpretation.
- The paper contains a number of sections with obscure mathiness and figures that I can't follow and whose significance is unclear.


Conclusion:
The work is well motivated, the method holds up to its promise of being fast and is empirically validated. However, it feels quite ad-hoc and the writing of the paper is very obscure at various places, which leaves room for improvement.


Details:

- The motivation for introducing alpha not clear to me. Wasn't the idea of F&S that you can reduce the image to its summary statistics within a pooling region whose size scales with eccentricity? Why do you need to retain some content information in the first place? How do images with alpha=1 (i.e. keep only texture) look?

- Related to above, why does alpha need to change with eccentricity? Experiment 1 seems to suggest that changing alpha leads to similar SSIM differences between synths and originals as F&S does, but what's the evidence that SSIM is a useful/important metric here?

- Again related to above, why do you not use the same approach of blending pooling regions like F&S did instead of introducing alpha?

- I would like to know some details about the inference of the critical scaling. It seems surprisingly spot on 0.5 as in F&S for synth vs. synth, but looking at the data in Fig. 12 (rightmost panel), I find the value 0.5 highly surprising given that all the blue points lie more or less on a straight line and the point at a scaling factor of 0.5 is clearly above chance level. Similarly, the fit for original vs. synth does not seem to fit the data all that well and a substantially shallower slope seems equally plausible given the data. How reliable are these estimates, what are the confidence intervals, and was a lapse rate included in the fits (see Wichmann & Hill 2001)?

- I don't get the point of Figs. 4, 13 and 14. I think they could as well be removed without the paper losing anything. Similarly, I don't think sections 2.1 and the lengthy discussion (section 5) are useful at all. Moreover, section 3 seems bogus. I don't understand the arguments made here, especially because the obvious options (alpha=1 or overlapping pooling regions; see above) are not even mentioned.

- How is the model trained? Do the authors use the pre-trained model of Huang & Belongie or is the training different in the context of the proposed method? I could only find the statement that the decoder is trained to invert the encoder, but that doesn't seem to be what Huang & Belongie's model does and the paper does not say anything about how it's trained to invert. Please clarify.

- At various places the writing is somewhat sloppy (missing words, commas, broken sentences), which could have been avoided by carefully proof-reading the paper.","The sentiment of the review is moderately positive. The reviewer acknowledges that the authors have addressed some of their previous concerns and highlights several strengths of the paper, such as its clear motivation, straightforward implementation, and fast rendering speed. However, the reviewer also points out significant weaknesses, including the introduction of an additional parameter (alpha) and sections with obscure mathematics and unclear figures. The conclusion reiterates that the work is well-motivated and empirically validated but feels ad-hoc and obscure in places. Therefore, the sentiment score is 30. The politeness of the language is quite high. The reviewer uses polite language throughout, even when pointing out weaknesses and areas for improvement. They provide constructive feedback and specific questions to help the authors improve their work. Therefore, the politeness score is 80.",30,80
"The paper makes a nice contribution to solving Circuit-SAT problem from a Neuro-Symbolic approach, particularly, 1) a novel DAG embedding with a forward layer and a reverse layer that captures the structural information of a circuit-sat input. 2) Compared with Selsam et al.'s work of Neuro-SAT, the proposed model in this paper, DG-DAGRNN, directly produces an assignment of variables, and the method is unsupervised and end-to-end differentiable. 3) Empirical experiments on random k-SAT and random graph k-coloring instances that support the authors' claim on better generalization ability.

The paper is lucid and well written, I would support its acceptance at ICLR. Though I have a few comments and questions for the authors to consider.

- In figure 1 (a), what are x11, x12, etc?

- When comparing the two approaches of Neuro-Symbolic methods, besides the angles of optimality and training cost, it is worth to mention that the first one that based on classical algorithms always has a correctness guarantee, while the second one (learning the entire solution from scratch) usually does not.

- Section 4.1, as a pure decision problem, solving SAT means that giving a yes/no answer (i.e., a classification); while for practical purposes, solving SAT means that producing a model (i.e., a witness) of the formula if it is SAT. This can be misleading for some readers when the authors mentioning ""solving SAT"", and it would be clear if the authors could make a distinction when using such terms.

- Section 4.1, ""without requiring to see the actual SAT solutions during training"", again, what is the meaning of ""solutions"" is not very clear at this point. Readers may realize the experiments in the paper only train with satisfiable formulae from the afterward description, so the ""solutions"" indicates the assignments of variables. But it would be better to make it clear.

- Section 4.1/The Evaluator Network, ""one can show also show that min() < S_min() <= S_max() < max()"", what is the ordering relation (i.e., < and <=) here? It is a bit confusing if a forall quantifier for inputs (a_1, ... a_n) is required here.

- Section 4.1/The Evaluator Network, how does the temperature affect the results of R_G? It would be helpful to show their dynamics.

- Section 4.1/Optimization, ""if the input circuit is UNSAT, one can show that the maximum achievable values for S_\theta is 0.5"", it would be better to provide a brief description of how it is guaranteed. Also, this seems to be suggesting the DG-SAGRNN solver has no false positives, i.e., it will never produce a satisfiable result for unsatisfiable formulae? This would be interesting toward some semi-correctness if the answer is yes.

- Section 5.1, are the testing data all satisfiable formulae? If yes, then the figure 2 shows there is a number of satisfiable formulae but both the models cannot produce correct results -- is that a correct understanding of figure 2? If not, then what is the ground truth?

- I would love to see more experiments on SAT instances with a moderate number of variables but from real-world applications. It would be interesting to see how the model utilizes the rich structural information of instances from real applications (instead of randomly generated formulae).

- The training time and testing time(per instance) are not reported in the experiments.
","The sentiment of the review is positive, as indicated by phrases like 'nice contribution,' 'lucid and well written,' and 'I would support its acceptance at ICLR.' The reviewer acknowledges the strengths of the paper and provides constructive feedback. The politeness of the language is high, as the reviewer uses polite phrases such as 'I would love to see,' 'it would be better to,' and 'it would be helpful to.' The feedback is given in a respectful and constructive manner, aiming to improve the paper rather than criticize it harshly.",80,90
"

Summary: This work considers the problem of learning in input-driven environments -- which are characterized by an addition stochastic variable z that can affect the dynamics of the environment and the associated reward the agent might see. The authors show how the PG theorem still applied for a input-aware critic and then they show that the best baseline one can use in conjecture with this critic is a input-dependent one. My main concerns are highlighted in points (3) and (4) in the detailed comments below. 

Clarity: Generally it reads good, although I had to go back-and-forth between the main text and appendix several times to understand the experimental side. Even with the supplementary material, examples in Section 3 and Sections 6.2 could be improved in explanation and discussion.

Originality and Significance: Limited in this version, but could be improved significantly by something like point (3)&(4) in detailed comments. Fairly incremental extension of the PG (and TRPO) with the conditioning on the potentially (unobserved) input variables. The fact that a input-aware critic could benefit from a input-aware baseline is not that surprising. The fact that it reduces variance in the PG update is an interesting result; nevertheless I strongly feel the link or comparison needed is with the standard PG update. 

Disclaimer: I have not checked the proofs in the appendix.

Detailed comments:

1) On learning the input-dependent baselines: Generalising over context via a parametric functional approximation, like UVFAs [1] seems like a more natural first choice. Also these provide a zero-shot generalisation, bypassing the need for a burn-in period of the task. Can you comment on why something like that was not used at least as baseline?

2) Motivating example. The exposition of this example lacks a bit of clarity and can use some more details as it is not a standard MDP example, so it’s harder to grasp the complexity of this task or how standard methods would do on it and where would they struggle. I think it’s meant to be an example of high variance but the performance in Figure 2 seems to suggest this is actually something manageable for something like A2C. It is also not clear in this example how the comparison was done. For instance, are the value functions used, input-dependent? Is the policy input aware? 

3) Input-driven MDP. Case 1/Case 2 : As noted by the authors, in case 1 if both s_t and z_t are observed, this somewhat uninteresting as it recovers a particular structured state variable of a normal MDP. I would argue that the more interesting case here, is where only s_t is observed and z_t is hidden, at least in acting. This might still be information available in hindsight and used in training, but won’t be available ‘online’ -- similar to slack variable, or privileged information at training time.  And in this case it’s not clear to me if this would still result in a variance reduction in the policy update. Case 2 has some of that flavour, but restricts z_t to an iid process. Again, I think the more interesting case is not treated or discussed at all and in my opinion, this might add the best value to this work.
  
4) Now, as mentioned above the interesting case, at least in my opinion, is when z is hidden. From the formulae(eq. (4),(5)), it seems to be that the policy is unaware of the input variables. Thus we are training a policy that should be able to deal with a distribution of inputs z. How does this compare with the normal PG update, that would consider a critic averaged over z-s and a z-independent baseline? Is the variance of the proposed update always smaller than that of the standard PG update when learning a policy that is unaware of z? 

References:
[1] Schaul, T., Horgan, D., Gregor, K. and Silver, D., 2015, June. Universal value function approximators. In International Conference on Machine Learning (pp. 1312-1320).

[POST-rebuttal] I've read the author's response and it clarified some of the concerns. I'm increase the score accordingly.","The sentiment of the review can be considered moderately positive. The reviewer acknowledges the interesting results and potential improvements but also highlights significant concerns and areas for improvement. This suggests a sentiment score of around 30. The politeness of the language is quite high; the reviewer uses polite and constructive language throughout, even when pointing out flaws. This suggests a politeness score of around 80.",30,80
"This paper presents a study of the community detection problem via graph neural networks. The presented results open the possibility that neural networks are able to discover the optimal algorithm for a given task. This is rather convincingly demonstrated on the example of the stochastic block model, where the optimal performance is known (for 2 symmetric groups) or strongly conjectured (for more groups). The method is rather computationally demanding, and also somewhat unrealistic in the aspect that the training examples might not be available, but for a pioneering study of this kind this is well acceptable.

Despite my overall very positive opinion, I found a couple of claims that are misleading and overall hurt the quality of the paper, and I would strongly suggest to the authors to adjust these claims:

** The method is claimed to ""even improve upon current computational thresholds in hard regimes."" This is misleading, because (as correctly stated in the body of the paper) the computational threshold to which the paper refers apply in the limit of large graph sizes whereas the observed improvements are for finite sizes. It is shown here that for finite sizes the present method is better than belief propagation. But this clearly does not imply that it improves the conjectured computational thresholds that are asymptotic. At best this is an interesting hypothesis for future work, not more. 

** The energy landscape is analyzed ""under certain simplifications and assumptions"". Conclusions state ""an interesting transition from rugged to simple as the size of the graphs increase under appropriate concentration conditions."" This is very vague. It would be great if the paper could offer intuitive explanation of there simplifications and assumptions that is between these unclear remarks and the full statement of the theorem and the proof that I did not find simple to understand. For instance state the intuition on in which region of parameters are those results true and in which they are not. 

** ""multilinear fully connected neural networks whose landscape is well understood (Kawaguchi, 2016)."" this is in my opinion grossly overstated. While surely that paper presents interesting results, they are set in a regime that lets a lot to be still understood about landscape of fully connected neural networks. It is restricted to specific activation functions, and the results for non-linear networks rely on unjustified simplifications, the sample complexity trade-off is not considered, etc. 


Misprint: Page 2: cetain -> certain. 
","The sentiment of the review is generally positive, as indicated by phrases like 'overall very positive opinion' and 'convincingly demonstrated.' However, the reviewer also points out several significant issues that need to be addressed, which slightly tempers the overall positivity. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer uses phrases like 'I would strongly suggest' and 'It would be great if,' which are polite ways to offer criticism and suggestions. Therefore, the politeness score is 80.",60,80
"Summary: The paper proposes a new smoothness constraint in the original cycle-gan formulation. The cycle-gan formulation minimizes reconstruction error on the input, and there is no criterion other than the adversarial loss function to ensure that it produce a good output (this is in sync with the observations from Gokaslan et al. ECCV'18 and Bansal et al. ECCV'18). A smoothness constraint is defined across random patches in input image and corresponding patches in transformed image. This enables the translation network to preserve edge discontinuities and variation in the output, and leads to better outputs for medical imaging, image to labels task, and horse to zebra and vice versa.

Pros: 

1.  Additional smoothness constraints help in improving the performance over multiple tasks. This constraint is intuitive.

2. Impressive human studies for medical imaging.

3. Improvement in the qualitative results for the shown examples in paper and appendix.

Things not clear from the submission: 

1. The paper is lacking in technical details: 

a. what is the patch-size used for RGB-histogram?

b. what features or conv-layers are used to get the features from VGG (19?) net? 

c. other than medical imaging where there isn't a variation in colors of the two domains, it is not clear why RGB-histogram would work?

d. the current formulation can be thought as a variant of perceptual loss from Johnson et al. ECCV'16 (applied for the patches, or including pair of patches). In my opinion, implementing via perceptual loss formulation would have made the formulation cleaner and simpler? The authors might want to clarify as how it is different from adding perceptual loss over the pair of patches along with the adversarial loss. One would hope that a perceptual loss would help improve the performance. Also see, Chen and Koltun, ICCV'17.

2. The proposed approach is highly constrained to the settings where structure in input-output does not change. I am not sure how would this approach work if the settings from Gokaslan et al. ECCV'18 were considered (like cats to dogs where the structure changes while going from input to output)? 

3. Does the proposed approach also provide temporal smoothness in the output? E.g. Figure-6 shows an example of man on horse being zebrafied. My guess is that input is a small video sequence, and I am wondering if it provides temporal smoothness in the output? The failure on human body makes me wonder that smoothness constraints are helping learn the edge discontinuities. What if the edges of the input (using an edge detection algorithm such as HED from Xie and Tu, ICCV'15) were concatenated to the input and used in formulation? This would be similar in spirit to the formulation of deep cascaded bi-networks from Zhu et al . ECCV'16.","The sentiment of the review is generally positive, as indicated by the praise for the smoothness constraints and their intuitive nature, as well as the impressive human studies and qualitative results. However, the review also contains several critical points and questions, which slightly temper the overall positive sentiment. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer uses polite and constructive language throughout, even when pointing out areas that need clarification or improvement. The reviewer uses phrases like 'The authors might want to clarify' and 'I am wondering if,' which are polite ways to request additional information or suggest improvements. Therefore, the politeness score is 80.",60,80
"Summary:
The authors propose a method to learn and improve problem-tailored PDE solvers from existing ones. The linear updates of the target solver, specified by the problem's geometry and boundary conditions, are computed from the updates of a well-known solver through an optimized linear map.  The obtained solver is guaranteed to converge to the correct solution and
achieves a considerable speed-up compared to solvers obtained from alternative state-of-the-art methods.   

Strengths:
Solving PDEs is an important and hard problem and the proposed method seems to consistently outperform the state of the art. I ve liked the idea of learning a speed-up operator to improve the performance of a standard solver and adapt it to new boundary conditions or problem geometries. The approach is simple enough to allow a straightforward proof of correctness. 

Weaknesses:
The method seems to rely strongly on the linearity of the solver and its deformation (to guarantee the correctness of the solution). The operator H is a matrix of finite dimensions and it is not completely clear to me what is the role of the multi-layer parameterization. Based on a grid approach, the idea applies only to one- or two-dimensional problems. 

Questions:
- in the introduction, what does it mean that generic solvers are effective 'but could be far from optimal'?  Does this refer to the convergence speed or to the correctness of the solution? 
- other deep learning approaches to PDE solving are mentioned in the introduction. Is the proposed method compared to them somewhere in the experiments? 
- given a PDE and some boundary conditions, is there any known method to choose the liner iterator T optimally? For example, since u* is the solution of a linear system, could one choose the updates to be the gradient descent updates of a least-squares objective such as || A u - f||^2?
- why is the deep network parameterization needed? Since no nonlinearities are present, isn t this equivalent to fix the rank of H?
- given the `  interpretation of H' sketched in Section 3.3, is there any relationship between the proposed accelerated update and the update of second-order coordinated descent methods (like Newton or quasi-Newton)?","The sentiment of the review is generally positive, as the reviewer acknowledges the importance of the problem being addressed and appreciates the novel approach and its performance improvements. This is evident from phrases like 'consistently outperform the state of the art' and 'liked the idea of learning a speed-up operator.' However, the reviewer also points out some weaknesses and poses several questions, indicating a balanced and constructive critique rather than unreserved praise. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer uses polite and respectful language throughout the review, even when pointing out weaknesses and asking questions. Phrases like 'it is not completely clear to me' and 'could one choose' show a considerate and non-confrontational tone. Therefore, the politeness score is 80.",60,80
"This paper proposes a meta-learning approach for the problem of few-shot classification. Their method, based on parametrizing the learner for each task by a closed-form solver, strikes an interesting compromise between not performing any adaptation for each new task (as is the case in pure metric learning methods) and performing an expensive iterative procedure, such as MAML or Meta-Learner LSTM where there is no guarantee that after taking the few steps prescribed by the respective algorithms the learner has converged. For this reason, I find that leveraging existing solvers that admit closed-form solutions is an attractive and natural choice. 

Specifically, they propose ridge regression as their closed-form solver (R2-D2 variant). This is easily incorporated into the meta-learning loop with any hyperparameters of this solver being meta-learned, along with the embedding weights as is usually done. The use of the Woodbury equation allows to rewrite the closed form solution in a way that scales with the number of examples instead of the dimensionality of the features; therefore taking advantage of the fact that we are operating in a few-shot setting. While regression may seem to be a strange choice for eventually solving a classification task, it is used as far as I understand due to the availability of this widely-known closed-form solution. They treat the one-hot encoded labels of the support set as the regression targets, and additionally calibrate the output of the network (via a transformation by a scale and bias) in order to make it appropriate for classification. Based on the loss of ridge regression on the support set of a task, a parameter matrix is learned for that task that maps from the embedding dimensionality to the number of classes. This matrix can then be used directly to multiply the embedded (via the fixed for the purposes of the episode embedding function) query points, and for each query point, the entry with the maximum value in the corresponding row of the resulting matrix will constitute the predicted class label.

They also experimented with a logistic regression variant (LR-D2) that does not admit a closed-form solution but can be solved efficiently via Newton’s Method under the form of Iteratively Reweighted Least Squares. When using this variant they restrict to tackling the case of binary-classification.

A question that comes to mind about the LR-D2 variant: while I understand that a single logistic regression classifier is only capable of binary classification, there seems to be a straightforward extension to the case of multiple classes, where one classifier per class is learned, leading to a total of N one-vs-all classifiers (where N is the way of the episode). I’m curious how this would compare in terms of performance against the ridge regression variant which is naturally multi-class. This would allow to directly apply this variant in the common setting and would enable for example still oversampling classes at meta-training time as is done usually.

I would also be curious to see an ablation where for the LR-D2 variant SGD was used as the optimizer instead of Newton’s method. That variant may require more steps (similar to MAML), but I’m curious in practice how this performs.

A few other minor comments:
- In the related work section, the authors write: “On the other side of the spectrum, methods that optimize standard iterative learning algorithms, [...] are accurate but slow.” Note however that neither MAML nor MetaLearner LSTM have been showed to be as effective as Prototypical Networks for example. So I wouldn’t really present this as a trade-off between accuracy and speed.
- I find the term multinomial classification strange. Why not use multi-class classification?
- In page 8, there is a sentence that is not entirely grammatically correct: ‘Interestingly, increasing the capacity of the other method it is not particularly helpful’.

Overall, I think this is good work. The idea is natural and attractive. The writing is clear and comprehensive. I enjoyed how the explanation of meta learning and the usual episodic framework was presented. I found the related work section thorough and accurate too. The experiments are thorough as well, with appropriate ablations to account for different numbers of parameters used between different methods being compared. This approach is evidently effective for few-shot learning, as demonstrated on the common two benchmarks as well as on a newly-introduced variant of cifar that is tailored to few-shot classification. Notably, the ridge regression variant can reach results competitive with SNAIL that uses significantly more weights and is shown to suffer when its capacity is reduced. Interestingly, other models such as MAML actually suffer when given additional capacity, potentially due to overfitting.
","The sentiment of the review is generally positive. The reviewer appreciates the natural and attractive idea, clear and comprehensive writing, and thorough experiments. They also highlight the effectiveness of the approach for few-shot learning. However, there are some minor criticisms and suggestions for improvement, which are presented constructively. Therefore, the sentiment score is 80. The politeness of the language is very high. The reviewer uses polite and respectful language throughout, even when pointing out areas for improvement. They use phrases like 'I’m curious,' 'I would also be curious,' and 'A few other minor comments,' which indicate a polite and constructive tone. Therefore, the politeness score is 90.",80,90
"The paper addresses a challenging problem of predicting the states of entities over the description of a process. The paper is very well written, and easily understandable. The authors propose a graph structure for entity states, which is updated at each step using the outputs of a machine comprehension system. The approach is novel and well motivated. I will suggest a few improvements: 

1. the NPN model seems a good alternative, will be good to have a discussion about why your model is better than NPN. Also, NPN can probably be modified to output spans of a sentence. I will be curious to know how it performs.

2. A more detailed illustration of the system / network is needed. Would have made it much easier to understand the paper. 

3. What are the results when using the whole training set of Recipes ?


","The sentiment of the review is highly positive, as indicated by phrases such as 'very well written,' 'easily understandable,' 'novel and well motivated.' The reviewer appreciates the approach and provides constructive feedback. The politeness of the language is also high, as the reviewer uses polite suggestions like 'I will suggest a few improvements' and 'will be good to have a discussion.' The tone is respectful and encouraging.",90,95
"UPDATE:

I've read the revised version of this paper, I think the concernings have been clarified.

-------

This paper proposes to employ the bandit optimization based approach for the generation of adversarial examples under the loss accessible black-box situation. The authors examine the feasibility of using the step and spatial dependence of the image gradients as the prior information for the estimation of true gradients. The experimental results show that the proposed method out-performs the Natural evolution strategies method with a large margin.

Although I think this paper is a decent paper that deserves an acceptance, there are several concernings:

1. Since the bound given in Theorem 1 is related to the square root of k/d, I wonder if the right-hand side could become ""vanishingly small"", in the case such as k=10000 and d=268203. I wish the authors could explain more about the significance of this Theorem, or provide numerical results (which could be hard).

2. Indeed I am not sure if Section 2.4 is closely related to the main topic of this paper, these theoretical results seem to be not helpful in convincing the readers about the idea of gradient priors. Also, the length of the paper is one of the reasons for the rating.

3. In the experimental results, what is the difference between one ""query"" and one ""iteration""? It looks like in one iteration, the Algorithm 2 queries twice?","The sentiment of the review is generally positive, as indicated by the statement 'I think this paper is a decent paper that deserves an acceptance.' However, the reviewer also points out several concerns that need to be addressed, which slightly tempers the overall positivity. Therefore, the sentiment score is 60. The language used in the review is polite and constructive, with phrases like 'I wish the authors could explain more' and 'Indeed I am not sure if,' which indicate a respectful tone. Therefore, the politeness score is 80.",60,80
"In this paper, the authors extend the HER framework to deal with dynamical goals, i.e. goals that change over time.
In order to do so, they first need to learn a model of the dynamics of the goal, and then to select in the replay buffer experience reaching the expected value of the goal at the expected time. Empirical results are based on three (or four, see the appendix) experiments with a Mujoco UR10 simulated environment, and one experiment is successfully transfered to a real robot.

Overall, the addressed problem is relevant (the question being how can you efficiently replay experience when the goal is dynamical?), the idea is original and the approach looks sound, but seems to suffer from a fundamental flaw (see below).

Despite some merits, the paper mainly suffers from the fact that the implementation of the approach described above is not explained clearly at all.
Among other things, after reading the paper twice, it is still unclear to me:
- how the agent learns of the goal motion (what substrate for such learning, what architecture, how many repetitions of the goal trajectory, how accurate is the learned model...)
- how the output of this model is taken as input to infer the desired values of the goal in the future: shall the agent address the goal at the next time step or later in time, how does it search in practice in its replay buffer, etc.

These unclarities are partly due to unsufficient structuring of the ""methodology"" section of the paper, but also to unsufficient mastery of scientific english. At many points it is not easy to get what the authors mean, and the paper would definitely benefit from the help of an experienced scientific writer.

Note that Figure 1 helps getting the overall idea, but another Figure showing an architecture diagram with the main model variables would help further.

In Figures 3a and 5, we can see that performance decreases. The explanation of the authors just before 4.3.1 seem to imply that there is a fundamental flaw in the algorithm, as this may happen with any other experiment. This is an important weakness of the approach.

To me, Section 4.5 about transfer to a real robot does not bring much, as the authors did nothing specific to favor this transfer. They just tried and it happens that it works, but I would like to see a discussion why it works, or that the authors show me with an ablation study that if they change something in their approach, it does not work any more.

In Section 4.6, the fact that DHER can outperform HER+ is weird: how can a learn model do better that a model given by hand, unless that given model is wrong? This needs further investigation and discussion.

In more details, a few further remarks:

In related work, twice: you should not replace an accurate enumeration of papers with ""and so on"".

p3: In contrary, => By contrast, 

which is the same to => same as

compare the above with the static goals => please rephrase

In Algorithm 1, line 26: this is not the algorithm A that you optimize, this is its critic network.

line 15: you search for a trajectory that matches the desired goal. Do you take the first that matches? Do you take all that match, and select the ""best"" one? If yes, what is the criterion for being the best?

p5: we find such two failed => two such failed

that borrows from the Ej => please rephrase

we assign certain rules to the goals so that they accordingly move => very unclear. What rules? Specified how? Please give a formal description.

For defining the reward, you use s_{t+1} and g_{t+1}, why not s_t and g_t?

p6: the same cell as the food at a certain time step. Which time step? How do you choose?

The caption of Fig. 6 needs to be improved to be contratsed with Fig. 7.

p8: the performance of DQN and DHER is closed => close?

DHER quickly acheive(s)

Because the law...environment. => This is not a sentence.

Mentioning in the appendix a further experiment (dy-sliding) which is not described in the paper is of little use.
","The sentiment of the review is mixed. The reviewer acknowledges the relevance, originality, and soundness of the approach, which contributes positively to the sentiment score. However, the review also highlights significant flaws and unclarities in the implementation and explanation, which detracts from the overall sentiment. Therefore, the sentiment score is moderately positive at 30. The politeness of the language is generally high. The reviewer provides constructive criticism and offers specific suggestions for improvement without being rude or dismissive. Phrases like 'the paper would definitely benefit from the help of an experienced scientific writer' and 'please rephrase' indicate a polite tone. Therefore, the politeness score is high at 80.",30,80
"Optimization algorithms such as stochastic gradient descent (SGD) and stochastic mirror descent (SMD) have found wide applications in training deep neural networks. In this paper the authors provide some theoretical studies to understand why SGD/SMD can produce a solution with good generalization performance when applied to high-parameterized models. The authors developed a fundamental identity for SGD with least squares loss function, based on which the minimax optimality of SGD is established, meaning that SGD chooses the best estimator that safeguards against the worst-case disturbance. Implicit regularization of SGD is also established in the interpolating case, meaning that SGD iterates converge to the one with minimal distance to the starting point in the set of models with no errors. Results are then extended to SMD with general loss functions.

Comments:

(1) Several results are extended from existing literature. For example, Lemma 1 and Theorem 3 have analogues in (Hassibi et al. 1996). Proposition 8 is recently derived in (Gunasekar et al., 2018). Therefore, it seems that this paper has some incremental nature. I am not sure whether the contribution is sufficient enough.

(2) The authors say that they show the convergence of SMD in Proposition 9, while (Gunasekar et al., 2018) does not. It seems that the convergence may not be surprising since the interpolating case is considered there.

(3) Implicit regularization is only studied in the over-parameterized case. Is it possible to say something in the general setting with noises?

(4) The discussion on the implicit regularization for over-parameterized case is a bit intuitive and based on strong assumptions, e.g., the first iterate is close to the solution set. It would be more interesting to present a more rigorous analysis with relaxed assumptions.","The sentiment of the review is moderately negative, as the reviewer points out several limitations and incremental nature of the contributions, questioning the sufficiency of the paper's contributions. The sentiment score is -40. The politeness of the language is relatively high, as the reviewer uses polite and constructive language, such as 'I am not sure whether the contribution is sufficient enough' and 'It would be more interesting to present a more rigorous analysis with relaxed assumptions.' The politeness score is 80.",-40,80
"The reviewer feels that the paper is hard to follow. The abstract is confusing enough and raises a number of questions.  The paper talks about `""local maxima"" without defining an optimization problem. What is the optimization problem are we talking about? Is it a maximization problem or minimization problem? If we are dealing with a minimization problem, why do we care about maxima?

The first several paragraphs did not make the problem of interest clearer. But at least the fourth paragraph starts talking about training networks (the reviewer guesses this ""network"" refers to neural network, not other types network (e.g., Bayesian network) arising in machine learning). This paragraph talks about random initialization for minimizing a loss function, does this mean we are considering a minimization problem's local maxima? In addition, random initialization-based neural network training algorithms like back propagation cannot guarantee giving local maxima or local minima of the problem of interest (which is the loss function for training). It is even not clear if a stationary point can be achieved. So if the method in this paper wishes to work with local maxima of an optimization problem, this may not be a proper example.

The next paragraph brings out a notion of value function, which is hard to follow what it is. A suggestion is to give a much more concrete example to enlighten the readers.

The next two paragraphs seem to be very disconnected. It is not properly defined what is x and how to obtain it. If they are local maxima of a problem, please give us an example: what is the optimization problem, and why this is an interesting setup?

Since the problem setup of this paper is very hard to decode, it is also very hard to appreciate why the papers in the ""related work"" section are really related.

The motivation and intuition behind the formulations in (1) and (2) are hard to follow, perhaps because the goal and objective of the paper is unclear.

Overall, there is no formal problem definition or statement, and the notions and terminologies in this paper are not properly defined or introduced. This makes evaluating this work very hard.


========= after author feedback =======
After discussing with the authors through OpenReview, the reviewer feels that a lot of things have been clarified. The paper is interesting in its setting, and seems to be useful in different applications.  The clarity can still be improved, but this might be more of a style matter.  The analysis part is a bit heavy and overwhelming and not very insightful at this moment. Overall, the reviewer appreciate the effort for improving the readability of the paper and would like to change the recommendation to ````   accept.","The initial sentiment of the review is quite negative, as the reviewer finds the paper hard to follow and confusing, with many questions raised about the definitions and clarity of the concepts. This is reflected in the detailed critique of various sections of the paper. However, after discussing with the authors and gaining clarification, the sentiment improves significantly, leading to a recommendation to accept the paper. Therefore, the overall sentiment score is a mix of the initial negative feedback and the later positive feedback, resulting in a neutral score. The politeness of the language is generally neutral to slightly polite, as the reviewer provides constructive criticism without being rude, and acknowledges the authors' efforts to improve the paper.",0,20
"The paper introduces Neural Logic Machines, a particular way to combine neural networks and first order but finite logic. 

The paper is very well written and structured. However, there are also some downsides.

First of all, Section 2.1 is rather simple from a logical perspective and hence it is not clear what this gets a special term. Moreover, why do mix Boolean logic (propostional logic) and first order logic? Any how to you deal with the free variables, i.e., the variables that are not bounded by a quantifier? The semantics you define later actually assumes that all free variables (in your notation) are bounded by all quantifiers since you apply the same rule to all ground instances. Given that you argue that you want a neural extension of symbolic logic (""NLM is a neural realization of (symbolic) logic machines"") this has to be clarified as it would not be an extension otherwise. 

Furthermore, Section 2.2 argues that we can use a MLP with a sigmoid output to encode any joint distribution. This should be proven. It particular, given that the input to the network are the marginals of the ground atoms. So this is more like a conditional distribution? Moreover, it is not clear how this is different to other approaches that encode the weight of weighted logical rule (e.g. in a MLN) using neural networks, see
e.g. 

Marco Lippi, Paolo Frasconi:
Prediction of protein beta-residue contacts by Markov logic networks with grounding-specific weights. 
Bioinformatics 25(18): 2326-2333 (2009)

Now of course, and this is the nice part of the present paper, by stacking several of the rules, we could directly specify that we may need a certain number of latent predicates. 
This is nice but it is not argued that this is highly novel. Consider again the work by Lippi and Frasconi. We unroll a given NN-parameterized MLN for s fixed number of forward chaining steps. This gives us essentially a computational graph that could also be made differentiable and hence we could also have end2end training. The major difference seems to be that now objects are directly attached with vector encodings, which are not present in Lippi and Frasconi's approach. This is nice but also follows from Rocktaeschel and Riedel's differentiable Prolog work (when combined with Lippi and Frasconi's approach).
Moreover, there have been other combinations of tensors and logic, see e.g. 

Ivan Donadello, Luciano Serafini, Artur S. d'Avila Garcez:
Logic Tensor Networks for Semantic Image Interpretation. 
IJCAI 2017: 1596-1602
 
Here you can also have vector encodings of constants. This also holds for 

Robin Manhaeve, Sebastijan Dumancic, Angelika Kimmig, Thomas Demeester, Luc De Raedt:
DeepProbLog: Neural Probabilistic Logic Programming. CoRR abs/1805.10872 (2018)

The authors should really discuss this missing related work. This should also involve
a clarification of the ""ILP systems do not scale"" statement. At least if one views statistical relational learning methods as an extension of ILP, this is not true. Probabilistic ILP aka statistical relational learning has been used to learn models on electronic health records, see e.g., the papers collectively discussed in 

Sriraam Natarajan, Kristian Kersting, Tushar Khot, Jude W. Shavlik:
Boosted Statistical Relational Learners - From Benchmarks to Data-Driven Medicine. Springer Briefs in Computer Science, Springer 2014, ISBN 978-3-319-13643-1, pp. 1-68

So the authors should either discuss SRL and its successes, separating SRL from ILP, or they cannot argue that ILP does not scale. In the related work section, they decided to view both as ILP, and, in turn, the statement that ILP does not scale is not true. Moreover, many of the learning tasks considered have been solved with ILP, too, of course in the ILP setting. Any ILP systems have been shown to scale beyond those toy domains.   
This also includes the blocks world. Here relational MDP solvers can deal e.g. with BW worlds composed of 10 blocks, resulting in MDPs with several million states. And the can compute relational policies that solve e.g. the goal on(a,b) for arbitrary number of blocks. This should be incorporated in the discussion of the introduction in order to avoid the wrong impression that existing methods just work for toy examples. 

Coming back to scaling, the current examples are on rather small datasets, too, namely <12 training instances. Moreover, given that we learn a continuous approximation with a limit depth of reasoning, it is also very likely that the models to not generate well to larger test instances. So the scaling issue has to be qualified to avoid to give the wrong impression that the present paper solves this issue. 

Finally, the BW experiments should indicate some more information on the goal configuration. This would help to understand whether an average number of moves of 84 is good or bad. Moreover, some hints about the MDP formulation should be provided, given that there have been relational MDPs that solve many of the probabilistic planning competition tasks. And, given that the conclusions argue that NLMs can learn the ""underlying logical rules"", the learned rules should actually be shown. 

Nevertheless, the direction is really interesting but there several downsides that have to be addressed. ","The sentiment of the review is generally positive, as indicated by phrases like 'The paper is very well written and structured' and 'the direction is really interesting.' However, the reviewer also points out several downsides and areas for improvement, which tempers the overall positivity. Therefore, the sentiment score is 40. The politeness of the language is quite high, as the reviewer uses polite and constructive language throughout, such as 'The authors should really discuss this missing related work' and 'This should be incorporated in the discussion.' There are no rude or harsh comments, so the politeness score is 80.",40,80
"imho, this manuscript is clearly written, addresses a confusing point in the current literature, clarifies some issues, and provides a novel and useful approach to mitigate those issues. 
reading the other comments online, the authors seem to have addressed those concerns as well.","The sentiment of the review is positive as it praises the manuscript for being clearly written, addressing confusing points in the literature, clarifying issues, and providing a novel and useful approach. The politeness of the language is also high, as the reviewer uses respectful and appreciative language, acknowledging the authors' efforts in addressing concerns.",80,90
"Summary:

This paper introduces a new supervised dimensionality reduction model. Supervision is provided in the form of class probabilities and the learning algorithm learns low-dimensional representations such that posterior cluster assignment probabilities given the representations match the observed class probabilities. The representations can be learned directly or the parameters of a neural network can be learned which maps inputs to the lower-dimensional space. The authors provide an extensive theoretical analysis of the proposed method and evaluate it on dimensionality reduction, visualization, and zero-shot learning tasks.

Review:

Overall, I thought this was an excellent paper. The idea is well-motivated, the presentation is clear, and the evaluations are both comprehensive and provide insight into the behavior of the proposed methods (I will not comment on the theoretical analysis, as it is entirely contained in the supplemental materials). I was honestly impressed by the shear volume of content in this paper, particularly since I found none of it to be superfluous. Frankly, this paper might be better served as two papers or a longer journal paper, but that is hardly a reason not to accept it. I strongly recommend acceptance and have only a couple of comments on presentation.

Comments:

- When trying to understand the proposed method, I found it useful to expand out the full objective function and derive the gradients w.r.t. to f_i. If my maths were correct, the gradient of the objective w.r.t. f_i can be written as the difference between the expected gradient of the divergence w.r.t Y and the expected gradient of the divergence w.r.t. the posterior cluster assignment probabilities. Though not surprising in and of itself, the authors might consider including this equation as it really helped me understand what the learning algorithm was doing. 

- The authors might consider adding a more complete description of the zero-shot learning task. My understanding of the task was that there are text descriptions of each category and at test time new text descriptions are added that were not in the training set. The goal is to map an unseen image to a class based on the text descriptions of the classes. A couple of sentences explaining this in the first paragraph of section 4.2 would help those who are not familiar with this zero-shot learning setup.","The sentiment of the review is highly positive. The reviewer describes the paper as 'excellent,' praises the motivation, clarity, and comprehensiveness of the evaluations, and strongly recommends acceptance. This indicates a sentiment score of 100. The politeness of the language is also very high. The reviewer uses phrases like 'I thought this was an excellent paper,' 'I was honestly impressed,' and 'the authors might consider,' which are all very polite and respectful. This indicates a politeness score of 100.",100,100
"This work adds to a growing literature on biologically plausible (BP) learning algorithms. Building off a study by Bartunov et al. that shows the deficiencies of some BP algorithms when scaled to difficult datasets, the authors evaluate a different algorithm, sign-symmetry, and conclude that there are indeed situations in which BP algorithms can scale. This seemingly runs counter to the conclusions of Bartunov et al.; while the authors state that their results are ""complementary"", they also state that the findings “directly conflict” with the results of Bartunov, concluding that BP algorithms remain viable options for both learning in artificial networks and the brain.

To reach these conclusions the authors report results on a number of experiments. First, they show successful training of a ResNet-18 architecture on ImageNet using sign-symmetry, with their model performing nearly as well as one trained with backpropagation. Next, they demonstrate decent performance on MS COCO object detection using RetinaNet. Finally, they end with a discussion that seeks to explain the differences in their approach and the approach of Batunov et al, and with a potential biological implementation of sign symmetry.

Overall the clarity of the writing is sufficient. The algorithm is properly explained, and there are sufficient citations to reference prior work. The results are generally clear (though there is an incomplete experiment, I agree with the authors that it is unlikely for the preliminary results to change). I believe that there is enough detail for this work to be reproducible. The work is also sufficiently novel in that experiments using sign-symmetry on difficult datasets have not been undertaken, to my knowledge.

Unfortunately, the clarity and rigor of the *scientific argument* is insufficient for a number of reasons. These will be enumerated below.

First, the explicit writing and underlying tone of the paper reveal a misrepresentation of the scientific argument in Bartunov et al. The scientific question in Bartunov et al. is not a matter of whether BP algorithms can be useful in purely artificial settings, but rather whether they can say anything about the way in which the brain learns. In this work, on the other hand, there seems to be two scientific questions: first, to assess whether BP algorithms can be useful in artificial settings, and second, to determine whether they can say anything about how the brain learns, as in Bartunov (indeed, the author’s conclusions highlight precisely these two points). Unfortunately, the experiments and underlying experimental logic push towards addressing the first question, and use this as evidence towards a conclusion to the second question. More concretely, experiments are run on biologically problematic architectures such as ResNet-18, often with backpropagation in the final layer (though admittedly this doesn’t seem to be an important detail with sign-symmetry, for reasons explained below). This is fine under the pretense of answering the first question, but to seriously engage with the results of Bartunov et al. and assess sign-symmetry’s merit as a BP algorithm for learning in the brain, the work requires the authors the algorithms to be tested under similar conditions before claiming that there is a “direct conflict”. To this end, though the authors claim that the conditions on which Bartunov et al tested are “somewhat restrictive”, this logic can equally be flipped on its head: the conditions under which this paper tests sign-symmetry are not restrictive enough to productively move in the direction of assessing sign-symmetry’s usefulness as a description of learning in the brain, and so the conclusion that the algorithm remains a viable option for describing learning in the brain is not sufficiently supported. On the other hand, I think the conclusions regarding the first question -- whether sign-symmetry can be useful in artificial settings -- are fine given the experiments.

Second, the work does not sufficiently weigh the “degree” of implausibility of sign-symmetry compared to the other algorithms, and implicitly speaks of feedback alignment, target propagation, and sign-symmetry as equally realistic members of a class of BP algorithms. Of course, one doesn’t want to go down the road of declaring that “algorithm A is more plausible than algorithm B!”, but the nuances should at least be seriously discussed if the algorithms are to be properly compared. In backpropagation the feedback connections must be similar in sign and magnitude. Sign-symmetry eliminates the requirement that the connections be similar in magnitude. However, this factor is arguably the least important of the two (the direction of the gradient is more important than the magnitudes), and we are still left with feedback weights that somehow have to tie their sign to their feedforward counterparts, which is not an issue in target propagation or feedback alignment. The authors try to explain away this difficulty with an appeal to molecular biology, which leads into my third point.

Third, the appeal to molecular mechanisms to explain how sign-symmetry can arise is not rigorous. There is a plethora of molecular mechanisms at play in our cells; indeed, there are enough mechanisms to hand-craft *any* sort of circuit one likes. Thus, it is somewhat vacuous to conclude that a particular circuit can be “easily implemented” in the brain simply by appealing to a hand-crafted circuit. For this argument to hold one needs to appeal to biological data to demonstrate that such a circuit either a) exists already, b) most probably exists because of reasons X, Y, Z. Unfortunately there is no biological backing, rendering this argument a possibly fun thinking exercise, but not a serious scientific proposal. But perhaps most problematic, the argument leaves the problem of sign-switching in the feedforward network to “future work”. This is perhaps *the most* important problem at play here, and until it is answered, these arguments don’t have sufficient impact.

Altogether the scientific argument of this work needs tightening. The tone, the title, and the overall writing should be modified to better tackle the nuances underlying the arguments of biologically plausible learning algorithms. The claims and conclusions need to be more explicit, and the work needs to better seated in the context of both the previous literature, and the important questions at play for assessing biologically plausible learning algorithms.
","The sentiment of the review is mixed. The reviewer acknowledges the novelty and clarity of the work but criticizes the scientific rigor and argumentation. The sentiment score is therefore slightly negative. The language used is polite and constructive, offering detailed feedback and suggestions for improvement without being rude or dismissive.",-20,80
"# Summary

The paper proposes training generative models that produce multi-agent trajectories using heuristic functions that label variables that would otherwise be latent in training data. The generative models are hierarchical, and these latent variables correspond to higher level goals in agent behavior. The paper focuses on basketball offenses as a motivating scenario in which multiple agents have coordinated high-level behavior. The generative models are RNNs where each output is fed into the decoder of a variational autoencoder to produce observed states. The authors add an intermediate layer to capture the latent variables, called macro-intents. The parameters are learned by maximizing an evidence lower bound.

Experiments qualitatively and quantitatively show that the hierarchical model produces realistic multi-agent traces.

# Comments

The paper presents a sensible solution for heuristically labeling latent variables. It is not particularly surprising that the model then learns useful behavior because it no longer has to maximize the marginal likelihood over all possible macro-intents. What is more interesting is that a heuristic labeling function is sufficient to label macro-intents that lead to learning realistic basketball offenses and swarm behavior.

Are any of the baselines (VRNN-single, VRNN-indep, and VRNN-mi) equivalent to training the hierarchical model by maximizing an ELBO on the marginal likelihood? I do not think this comparison is done, which might be interesting to quantify how much of a difference heuristic labeling makes. Of course, the potentially poor fit of a variational distribution would confound the results.

# Minor things

1) In the caption of Table 1, it says ""Our hierarchical model achieves higher log-likelihoods than baselines for both datasets."" Are not the reported scores evidence lower-bounds? So it achieves a higher evidence lower bound, but without actually computing the true likelihood, could not the other models have higher likelihoods?

2) Under ""Human preference study"" it says ""All judges preferred our model over the baselines with 98% statistical significance."" I am not familiar with this terminology. Does that mean that a p value for some null hypothesis is .02?

3) Something is wrong with the citation commands. Perhaps \citep should be used.","The sentiment of the review is generally positive, as it acknowledges that the paper presents a sensible solution and finds the results interesting. However, it also points out areas for improvement and raises questions about the methodology, which tempers the overall positivity. Therefore, the sentiment score is 60. The language used in the review is polite and constructive, offering suggestions and clarifications without being dismissive or harsh. Thus, the politeness score is 80.",60,80
"This paper proposes a GCN variant that addresses a limitation of the original model, where embedding is propagated in only a few hops. The architectural difference may be explained in the following: GCN interleaves the individual node feature transformation and the single-hop propagation, whereas the proposed architecture first transforms the node features, followed by a propagation with an (in)finite number of hops. The propagation in the proposed method follows personalized PageRank, where in addition to following direct links, there is a nonzero probably jumping to a target node.

I find the idea interesting. The experiments are comprehensive, covering important points including data split, training set size, number of hops, teleport probability, and ablation study. Two interesting take-home messages are that (1) GCN-like propagation without teleportation leads to degrading performance as the number of hops increases, whereas propagation with teleportation leads to converging performance; and (2) the best-performing teleport probability generally falls within a narrow range.

Question: The current propagation approach uses the normalized adjacency matrix proposed by GCN, which is, strictly speaking, not the transition matrix used by PageRank. What prevents from using the transition matrix? Note that this matrix naturally handles directed graphs.
","The sentiment of the review is positive, as indicated by phrases like 'I find the idea interesting' and the acknowledgment of comprehensive experiments and interesting take-home messages. Therefore, the sentiment score is 80. The politeness of the language is also high, as the reviewer uses polite and constructive language throughout, including the use of 'interesting' and 'comprehensive,' and poses a question in a respectful manner. Thus, the politeness score is 90.",80,90
"The submission presents a reinforcement learning method for exploring/probing the environment to determine an environment’s properties and exploit these during later tasks. The method relies on jointly learning an embedding that simplifies prediction of future states and a policy that maximises a curiosity/intrinsic motivation like reward to learn to explore areas where the prediction model underperforms. In particular, the reward is based on the difference between prediction based on the learned embedding and prediction based on a prior collected dataset, such that the reward optimises to collect data with a large difference between the prediction accuracy of both models. The subsequently frozen policy and embedding are then used in other domains in a system identification like manner with the embedding utilised as input for a final task policy. The method is evaluated on a striker and hopper environment with varying dynamics parameters and shown to outperform a broad set of baselines. 

In particular the broad set of baselines and small performed ablation study on the proposed method are quite interesting and beneficial for understanding the approach. However, the ablation study could be in more detail with respect to the additional training variations (Section 4.1.3; e.g. without all training tricks). Additionally, information about the baselines should be extended in the appendix as e.g. different capacities alone could have an impact where the performances of diff. algorithms are comparably similar. In particular, additional information about the training procedure for the UP-OSI (Yu et al 2017) baseline is required as the original approach relies on iterative training and it is unclear if the baseline implementation follows the original implementation (similar to Section 4.1.3.). 

Overall the submission provides an interesting new direction on learning system identification approaches, that while quite similar to existing work (Yu et al 2017), provides increased performance on two benchmark tasks. The contribution of the paper focuses on detailed evaluation and, overall, beneficial details of the proposed method. The novelty of the submission is however limited and highly similar to current methods.

Minor issues:
- Related work on learning system identification:
Learning to Perform Physics Experiments via Deep Reinforcement Learning
Misha Denil, Pulkit Agrawal, Tejas D Kulkarni, Tom Erez, Peter Battaglia, Nando de Freitas
","The review starts by summarizing the submission in a neutral to positive tone, highlighting the method's novelty and performance. The sentiment is slightly positive as the reviewer acknowledges the interesting aspects and benefits of the approach, despite noting its similarity to existing work. The language used is polite and constructive, offering specific recommendations for improvement without being dismissive or harsh.",40,80
"Summary:
The paper reformulates the task of video prediction/interpolation so that a predictor is not forced to generate frames at fixed time intervals, but instead it is trained to generate frames that happen at any point in the future. The motivation for such approach is that there might be future states that are highly uncertain – and thus, difficult to predict – that might not be useful for other tasks involving video prediction such as planning. The authors derive different loss functions for such Time-Agnostic Prediction (TAP), including extensions to the Variational AutoEncoders (VAE) and Generative Adversarial Networks (GAN) frameworks, and conduct experiments that suggest that the frames predicted by TAP models correspond to ‘subgoal’ states useful for planning.

Strenghts:
[+] The idea of TAP is novel and intuitively makes sense. 
It is clear that there are frames in video prediction that might not be interesting/useful yet are difficult to predict, TAP allows to skip such frames.
[+] The formulation of the TAP losses is clear and well justified. 
The authors do a good job at showing a first version of a TAP loss, generalizing it to express preferences, and its extension to VAE and GAN models, showing that 

Weaknesses:
[-] The claim that the model discovers meaningful planning subgoals might be overstated. 
The hierarchical planning evaluation experiment seems like it would clearly favor TAP compared to a fixed model (why would the middle prediction in time of the fixed model correspond to reasonable planning goals?). Furthermore, for certain tasks and environments it seems like the uncertain frames might be the ones that correspond to important subgoals. For example, for the BAIR Push Dataset, usually the harder frames to predict are the arm-object interactions, which probably would correspond to the relevant subgoals.

Overall I believe that the idea in this paper is a meaningful novel contribution. The paper is well-written and the experiments support the fact that TAP might be a better choice for training frame predictors for certain tasks.","The sentiment of the review is generally positive, as indicated by phrases such as 'meaningful novel contribution' and 'well-written'. The reviewer acknowledges the strengths of the paper and provides constructive criticism without being overly harsh. The politeness of the language is high, as the reviewer uses polite and respectful language throughout, even when pointing out weaknesses. The use of phrases like 'might be overstated' and 'seems like' shows a considerate approach to criticism.",80,90
"This paper presents a method to reconstruct images using only noisy measurements. This problem is practically interesting, since the noiseless signal may be unavailable in many applications. The approach combines ideas from recent development in compressed sensing and GANs. However, the model’s presentation is confusing, and many important details of the experiments are missing.

Pros:

* The problem is interesting and important
* The combination of compressed sensing and GANs for image reconstruction is novel

Cons:

* The model structure is unclear: for example, what is the role of the variable \theta? Section 2.1 says it is known, but the algorithm samples from its prior(?). Since there is no further explanation with respect to the experiments, I am not sure how the values of \theta or its distributions were determined. Although \theta is formally similar to the \theta parameters of the measurement function in ambientGANs, this interpretation is at odds with the example given in the paper (below eq.1, saying \theta can be positions or sizes).
* A few important details of the model are missing. For example, what is the exact structure of the measurement function F?
* The baseline models are a bit confusing. More detail about unpaired vs paired supervision would also be helpful for understanding how these baseline models use the additional information.
* Although the paper mentioned parameters are obtained from cross-validation, it would still be helpful to describe a few important ones (e.g., neural network size, weight \lambda) for comparison with other models.The experiments on only CelebA dataset are too limited.","The sentiment of the review is mixed, with both positive and negative aspects highlighted. The reviewer acknowledges the importance and novelty of the problem and approach, which contributes to a positive sentiment. However, the reviewer also points out several significant issues with the clarity and completeness of the model's presentation and experimental details, which contributes to a negative sentiment. Therefore, the sentiment score is balanced at 0. The politeness of the language is quite high, as the reviewer uses polite and constructive language to point out the issues and provide recommendations for improvement. Therefore, the politeness score is 80.",0,80
"The paper builds upon Deep Image Prior (DIP) - work which shows that one can optimize a neural generator to fit a single image without learning on any dataset, and the output of the generator (which approximates the image) can be used for denoising / super resolution / etc. The paper proposes a new architecture for the DIP method which has much less parameters, but works on par with DIP. Another contribution of the paper is theoretical treatment of (a simplified version of) the proposed architecture showing that it can’t fit random noise (and thus maybe better suited for denoising).

The paper is clearly written, and the proposed architecture has too cool properties: it’s compact enough to be used for image compression; and it doesn’t overfit thus making early stopping notnesesary (which was crucial for the original DIP model).

I have two main concerns about this paper.
First, it is somewhat misleading about its contributions: it's not obvious from abstract/introduction that the whole model is the same as DIP except for the proposed architecture. Specifically, the first contribution listed in the introduction makes it look like this paper introduces the idea of not learning the decoder on the dataset (the one that starts with “The network is not learned and itself incorporates all assumptions on the data.”).

My second concern is about the theoretical contribution. On the one hand, I enjoyed the angle the authors tackled proving that the network architecture is underparameterized enough to be a good model for denoising. On the other hand, the obtained results are very weak: only one layered version of the paper is analysed and the theorem applies only to networks with less than some threshold of parameters. Roughly, the theorem states that if for example we fix any matrix B of size e.g. 256 x k and matrix U of size 512 x 256 and then compute U relu(B C) where C is the vector of parameters of size k x 1, AND if k < 2.5 (i.e. if we use at most 2 parameters), then it would be very hard to fit 512 iid gaussian values (i.e. min_C ||U relu(B C) - eta|| where eta ~ N(0, 1)). This restriction of the number of parameters to be small is only mentioned in the theorem itself, not in the discussion of its implications.
Also, the theorem only applies to the iid noise, while most natural noise patterns have structure (e.g. JPEG artifacts, broken pixels, etc) and thus can probably be better approximated with deep models.

Since the paper manages to use very few parameters (BTW, how many parameters in total do you have? Can you please add this number to the text?), it would be cool to see if second order methods like LBFGS can be applied here.

Some less important points:

Fig 4 is very confusing.
First, it doesn’t label the X axis.
Second, the caption mentions that early stopping is beneficial for the proposed method, but I can’t see it from the figure.
Third, I don’t get what is plotted on different subplots. The text mentions that (a) is fitting the noisy image, (b) is fitting the noiseless image, and (c) is fitting noise. Is it all done independently with three different models? Then why does the figure says test and train loss? And why DIP loss goes up, it should be able to fit anything, right? If not and it’s a single model that gets fitted on the noisy image and tested on the noiseless image, then how can you estimate the level of noise fitting? ||G(C) - eta|| should be high if G(C) ~= x.
Also, in this quote “In Fig. 4(a) we plot the Mean Squared Error (MSE) over the number of iterations of the optimizer for fitting the noisy astronaut image x + η (i.e., FORMULA ...” the formula doesn’t correspond to the text.
And finally, the discussion of this figure makes claims about the behaviour of the model that seems to be too strong to be based on a single image experiment.

I don’t get the details of the batch normalization used: with respect to which axis the mean and variance are computed?

The authors claim that the model is not convolutional. But first, it’s not obvious why this would be a good thing (or a bad thing for that matter). Second, it’s not exactly correct (as noted in the paper itself): the architecture uses 1x1 convolutions and upsampling, which combined give a weak and underparametrized analog of convolutions.

> The deep decoder is a deep image model G: R N → R n, where N is the number of parameters of the model, and n is the output dimension, which is typically much larger than the number of parameters (N << n).
I think it should be vice versa, N >> n

The following footnote
> Specifically, we took a deep decoder G with d = 6 layers and output dimension 512×512×3, and choose k = 64 and k = 128 for the respective compression ratios.
Uses unintroduced (at that point) notation and is very confusing.

It would be nice to have a version of Figure 6 with k = 6, so that one can see all feature maps (in contrast to a subset of them).

I’m also wondering, is it harder to optimize the proposed architecture compared to DIP? The literature on distillation indicates that overparameterization can be beneficial for convergence and final performance.
","The sentiment of the review is mixed. The reviewer acknowledges the clear writing and the interesting properties of the proposed architecture, which is positive. However, the reviewer also expresses significant concerns about the contributions and the theoretical results, which are negative. Therefore, the sentiment score is slightly negative. The politeness of the language is generally high. The reviewer uses polite language, such as 'I enjoyed,' 'it would be cool to see,' and 'I’m also wondering,' and provides constructive feedback without being rude or dismissive.",-20,80
"Thanks the authors for an interesting work!
The paper studies the differences between human and DNN vision via means of minimal images (i.e. smallest image crops that can be correctly classified).

There are a few notable take-away messages:
1. DNNs are not invariant to even tiny (1-2 px) translations of small image crops.
    - It would be more insightful if authors added comparison between DNN sensitivity to tiny translations of small image crops vs. full-size images (i.e. translation-based adversarial examples https://openreview.net/forum?id=BJfvknCqFQ ).
2. The smaller the image crops, the more sensitive DNNs become (here, more FRIs)
3. DNNs and human vision misclassify the image crops differently: (1) DNNs have almost twice more FRI(s) and (2) FRIs of human and DNNs differ in location.

Questions:

- ""After extracting the region from the image, the region is resized to be of the size required by the network.""
Would upscaling say a small 28x28 crop into 224x224 image here naturally negatively impact the DNN predictive performance?
That is, because typically image classifiers are trained on one (or a few) fixed resolution(s) of images.

One hypothesis here is that fragile recognition may be because the test image resolution does not match the training image resolution.
Human on the other hands, have been trained on images of variable resolutions.

An alternative to upscaling here is to zero-pad the crop region. Can you help us understand your choice of upscaling here?

+ Originality
The originality is limited as it is a close extenstion work of Ullman et al. 2016

+ Clarity
The paper is well written and presented.

+ Significance
This work extends our understanding of the differences between DNNs and human vision.
However, given what we learned from the adversarial example research area, the contribution of this work is low because results might not be too surprising.","The sentiment of the review is generally positive, as indicated by the opening statement 'Thanks the authors for an interesting work!' and the positive remarks about the clarity and significance of the paper. However, there are also some critical points, such as the limited originality and the low contribution due to the predictability of the results. Therefore, the sentiment score is moderately positive. The politeness of the language is high, as the reviewer uses polite phrases and constructive criticism without any rude or harsh language.",60,90
"This paper proposed a novel approach for learning disentangled representation from supervised data (x as the input image, y as different attributes), by learning an encoder E and a decoder D so that (1) D(E(x)) reconstructs the image, (2) E(D(x)) reconstruct the latent vector, in particular for the vectors that are constructed by mingling different portion of the latent vectors extracted from two training samples, (3) the Jacobian matrix matches and (4) the predicted latent vector matches with the provided attributes. In addition, the work also proposes to progressively add latent nodes to the network for training. The claim is that using this framework, one avoid GAN-style training (e.g., Fader network) which could be unstable and hard to tune. 

Although the idea is interesting, the experiments are lacking. While previous works (e.g., Fader network) has both qualitative (e.g., image quality when changing attribute values) and quantitative results (e.g., classification results of generated image with novel combination of attributes), this paper only shows visual comparison (Fig. 4 and Fig. 5), and its comparison with Fader network is a bit vague (e.g., it is not clear to me why Fig. 5(e) generated by proposed approach is “more natural” than Fig. 5(d), even if I check the updated version mentioned by the authors' comments). Also in the paper there are five hyperparameters (Eqn. 14) and the center claim is that using Jacobian loss is better. However, there is no ablation study to support the claim and/or the design choice. From my opinion, the paper should show the performance of supervised training of attributes, the effects of using Jacobian loss and/or cycle loss, the inception score of generated images, etc. 

I acknowledge the authors for their honesty in raising the issues of Fig. 4, and providing an updated version. ","The sentiment of the review is mixed but leans towards the negative side. The reviewer acknowledges the interesting idea proposed by the paper but criticizes the lack of experimental validation and comparison with previous works. The sentiment score is therefore -30. The politeness of the language is quite high; the reviewer uses polite phrases such as 'I acknowledge the authors for their honesty' and 'from my opinion,' which indicates a respectful tone. The politeness score is 80.",-30,80
"Mode collapse in the context of GANs occurs when the generator only learns one of the 
multiple modes of the target distribution. Mode collapsed can be tackled, for instance, using Wasserstein distance instead of Jensen-Shannon divergence. However, this sacrifices accuracy of the generated samples.

This paper is positioned in the context of Bayesian GANs (Saatsci & Wilson 2017) which, by placing a posterior distribution over the generative and discriminative parameters, can potentially learn all the modes. In particular, the paper proposes a Bayesian GAN that, unlike previous Bayesian GANs, has theoretical guarantees of convergence to the real distribution.

The authors put likelihoods over the generator and discriminator with logarithms proportional to the traditional GAN objective functions. Then they choose a prior in the generative parameters which is the output of the last iteration. The prior over the discriminative parameters is a uniform improper prior (constant from minus to plus infinity). Under this specifications, they demonstrate that the true data distribution is an equilibrium under this scheme. 

For the inference, they adapt the Stochastic Gradient HMC used by Saatsci & Wilson. To approximate the gradient of the discriminator, they take samples of the generator parameters. To approximate the gradient of the generator they take samples of the discriminator parameters but they also need to compute a gradient of the previous generator distribution. However, because this generator distribution is not available in close form they propose two simple approximations.

Overall, I enjoyed reading this paper. It is well written and easy to follow. The motivation is clear, and the contribution is significant. The experiments are convincing enough, comparing their method with Saatsci's Bayesian GAN and with the state of the art of GAN that deals with mode collapse. It seems an interesting improvement over the original Bayesian GAN with theoretical guarantees and an easy implementation.

Some typos:

- The authors argue that compare to point mass...
+ The authors argue that, compared to point mass...

- Theorem 1 states that any the ideal generator
+ Theorem 1 states that any ideal generator

- Assume the GAN objective and the discriminator space are symmetry
+ Assume the GAN objective and the discriminator space have symmetry

- Eqn. 8 will degenerated as a Gibbs sampling
+ Eqn. 8 will degenerate as a Gibbs sampling","The sentiment of the review is highly positive. The reviewer expresses enjoyment in reading the paper, describes it as well-written and easy to follow, and acknowledges the clear motivation and significant contribution of the work. The experiments are also described as convincing. Therefore, the sentiment score is 90. The politeness of the language is also very high. The reviewer uses polite and respectful language throughout the review, providing constructive feedback and suggestions for improvement without any negative or harsh comments. Therefore, the politeness score is 100.",90,100
"* Description

The work is motivated by the empirical performance of Batch Normalization and in particular the observed better robustness of the choice of the learning rate.  Authors analyze theoretically the asymptotic convergence rate for objectives involving normalization, not necessarily BN, and show that for scale-invariant groups of parameters (appearing as a result of normalization) the initial learning rate may be set arbitrary while still asymptotic convergence is guaranteed with the same rate as the best known in the general case. Offline gradient descent and stochastic gradient descent cases are considered.

* Strengths

The work addresses better theoretical understanding of successful heuristics in deep learning, namely batch normalization and other normalizations. The technical results obtained are non-trivial and detailed proofs are presented. Also I did not verify the proofs the paper appears technically correct and technically clear. The result may be interpreted in the following form: if one chooses to use BN or other normalization, the paper gives a recommendation that only the learning rate of scale-variant parameters need to be set, which may have some practical advantages. Perhaps more important than the rate of convergence, is the guarantee that the method will not diverge (and will not get stuck in a non-local minimum). 

* Criticism
This paper presents non-trivial theoretical results that are worth to be published but as I argue below its has a weak relevance to practice and the applicability of the obtained results is unclear.
-- Concerns regarding the clarity of presentation and interpretation of the results.
 
The properties of BN used as motivation for the study, are observed non-asymptotically with constant or empirically decreased learning rate schedules for a limited number of iterations. In contrast, the studied learning rates are asymptotic and there is a big discrepancy. SGD is observed to be significantly faster than batch gradient when far from convergence (experimental evidence), and this is with or without normalization. In practice, the training is stopped much before convergence, in the hope of finding solutions close to minimum with high probability. There is in fact no experimental evidence that the practical advantages of BN are relevant to the results proven. It makes a nice story that the theoretical properties justify the observations, but they may be as well completely unrelated. 

As seen from the formal construction, the theoretical results apply equally well to all normalization methods. It occludes the clarity that BN is emphasized amongst them. 

Considering theoretically, what advantages truly follow from the paper for optimizing a given function? Let’s consider the following cases.
1. For optimizing a general smooth function with all parameters forming a single scale-invariant vector. In this case, the paper proves that no careful selection of the learning rate is necessary. This result is beyond machine learning and unfortunately I cannot evaluate its merit. Is it known / not known in optimization?

2. The case of data-independent normalization (such as weight normalization).
Without normalization, we have to tune learning rate to achieve the optimal convergence. With normalization we still have to tune the learning rate (as scale-variant parameters remain or are reintroduced with each invariance to preserve the degrees of freedom), then we have to wait for the phase two of Lemma 3.2 so that the learning rate of scale-invariant parameters adapts, and from then on the optimal convergence rate can be guaranteed.

3. The case of Batch Normalization. Note that there is no direct correspondence between the loss of BN-normalized network (2) and the loss of the original network because of dependence of the normalization on the batches. In other words, there is no setting of parameters of the original network that would make its forward pass equivalent to that of BN network (2) for all batches. The theory tells the same as in case 2 above but with an additional price of optimizing a different function.

These points remain me puzzled regarding either practical or theoretical application of the result. It would be great if authors could elaborate. 


-- Difference from Wu et al. 2018

This works is cited as a source of inspiration in several places in the paper. As the submission is a theoretical result with no immediate applicability, it would be very helpful if the authors could detail the technical improvements over this related work. Note, ICLR policy says that arxiv preprints earlier than one month before submission are considered a prior art. Could the authors elaborate more on possible practical/theoretical applications?
 

* Side Notes (not affecting the review recommendation)

I believe that the claim that “BN reduces covariate shift” (actively discussed in the intro) was an imprecise statement in the original work. Instead, BN should be able to quickly adapt to the covariate shift when it occurs. It achieves this by using the parameterization in which the mean and variance statistics of neurons (the quantities whose change is called the covariate shift) depend on variables that are local to the layer (gamma, beta in (1)) rather than on the cumulative effect of all of the preceding layers.

* Revision
I took into account the discussion and the newly added experiments and increased the score. The experiments verify the proven effect and make the paper more substantial. Some additional comments about experiments follow.
Training loss plots would be more clear in the log scale.
Comparison to ""SGD BN removed"" is not fair because the initialization is different (application of BN re-initializes weight scales and biases). The same initialization can be achieved by performing one training pass with BN with 0 learning rate and then removing it, see e.g. Gitman, I. and Ginsburg, B. (2017). Comparison of batch normalization and weight normalization algorithms for the large-scale image classification.
The use of Glorot uniform initializer is somewhat subtle. Since BN is used, Glorot initialization has no effect for a forward pass. However, it affects the gradient norm. Is there a rationale in this setting or it is just a more tricky method to fix the weight norm to some constant, e.g. ||w||=1?

","The review starts with a positive sentiment, acknowledging the non-trivial and technically correct results of the paper. However, it transitions into a more critical tone, highlighting concerns about the practical relevance and clarity of the results. The language used is polite and constructive, offering specific suggestions for improvement and additional elaboration. The reviewer also acknowledges the improvements made in the revision, which indicates a balanced and fair assessment.",20,80
"This paper discusses the effect of weight decay on the training of deep network models with and without batch normalization and when using first/second order optimization methods. 

First, it is discussed how weight decay affects the learning dynamics in networks with batch normalization when trained with SGD. The dominant generalization benefit due to weight decay comes from increasing the effective learning rate of parameters on which batch normalization is applied. The authors therefore hypothesize that a larger learning rate has a regularization effect.

Second, the role of weight decay is discussed when training with second order methods without batch normalization. Under the approximation of not differentiating the curvature matrix used in second order method, it is shown that using weight decay is equivalent to adding to the loss an L2 regularization in the metric space of the curvature matrix considered. It is then shown that if the curvature matrix is the Gauss-Newton matrix, this L2 regularization (and hence the weight decay) is equivalent to the Frobenius norm of the input-output Jacobian when the input has a spherical Gaussian distribution. Similar arguments are made about KFAC with Gauss-Newton norm. The generalization benefit due to weight decay in this case is claimed based on the recent paper by Novak et al 2018 which empirically shows a strong correlation between input-output Jacobian norm and generalization error.


Finally, the role of weight decay is discussed for second order methods when using batch normalization. In this case it is discussed for Gauss-Newton KFAC that the benefit mostly comes from the application of weight decay on the softmax layer and the effect of weight decay on other weights cancel out due to batch normalization. A comparison between Gauss-Newton KFAC and Fischer KFAC is also made. Thus the generalization benefit is presumably attributed to the second order properties of KFAC and a smaller norm of softmax layer weights.

Comments:
The paper is technically correct and proofs look good.

I have mixed comments about this paper. I find the analysis in section 4.2 and 4.3 which discuss about the role of weight decay for second order methods (with and without batch-norm) to be novel and insightful (described above). 

But on the other hand, I feel section 4.1 is more of a discussion on existing work rather than novel contribution. Most of what is said, both analytically and experimentally, is a repetition of van Laarhoven 2017, except for a few details. It would have been interesting to carefully study the effect of weight decay on the gamma parameter of batch-norm which controls the complexity of the network along with the softmax layer weights as it was left for future work in van Laarhoven 2017. But instead the authors brush it under the carpet by saying they did not find the gamma and beta parameters to have significant impact on performance, and fixed them during training.  I also find the claim of section 4.1 to be a bit mis-leading because it is claimed that weight decay applied with SGD and batch normalization only has benefits due to batch-norm dynamics, and not due to complexity control even though in Fig 2 and 4, there is a noticeable difference between training without weight decay, and training with weight decay only on last layer. Furthermore, when hypothesizing the regularization effect of large learning rate in section 4.1, a large body of literature that has studied this effect has not been cited. Examples are [1], [2], [3]. 

I have other concerns which mainly stem from lack of clarity in writing:

1. In the line right above remark 1, it is not clear what “assumption” refer to. I am guessing the distribution of the input being spherical Gaussian?
2. In remark 1, regarding the claim about the equivalence of L2 norm of theta under Gauss-Newton metric and the Frobenius norm of input-output Jacobian, why does f_theta need to be a linear function without any non-linearity? I think the linearity part is only needed for the KFAC result.
3. In remark 1, what does it mean by “Furthermore, if G is approximated by KFAC”? For linear f_theta, given lemma 1 and theorem 1, the claimed equivalence always holds true, no?
4. In the 1st line of last paragraph of page 6, what are the general conditions under which the connection between Gauss-Newton norm and Jacobian norm does not hold true?
5. In figure 5, how are the different points in the plots achieved? By varying hyper-parameters?

A minor suggestion: in theorem 1 (and lemma 1), instead of assuming network has no bias, it can be said that the L2 regularization term does not have bias terms. This is more reasonable because bias terms have no effect on complexity and so it is reasonable to not apply weight decay on bias.

Overall I think the paper is good *if* section 4.1 is sorted out and writing (especially in section 4.2) is improved. For these reasons, I am currently giving a score of 6, but I will increase it if my concerns are addressed.

[1] a bayesian perspective on generalization and stochastic gradient descent
[2] Train longer, generalize better: closing the generalization gap in large batch training of neural networks
[3] Three Factors Influencing Minima in SGD","The sentiment of the review is mixed, with positive remarks about the novelty and insightfulness of sections 4.2 and 4.3, but negative comments about section 4.1 being repetitive and lacking in novel contributions. The reviewer also points out several concerns and areas for improvement, indicating a critical but constructive stance. Therefore, the sentiment score is 10. The politeness of the language is quite high, as the reviewer uses polite and constructive language throughout, even when pointing out flaws and suggesting improvements. The reviewer acknowledges the technical correctness and potential of the paper, which reflects a respectful and professional tone. Therefore, the politeness score is 80.",10,80
"This is an experimental paper that investigates how spatial ordering of patches influences the classification performances of CNNs. To do so, the authors design CNNs close to ResNets that almost only consist in a simple cascade of 1x1 convolutions, obtaining relatively small receptive field. It is an interesting read, and I recommend it as a valuable contribution to ICLR, that might lead to nice future works.

I have however several comments and questions, that I would like to be addressed.

1) First I think a reference is missing. Indeed, to my knowledge, it is not the first work to use this kind of techniques. Cf [1]. This does not alterate however the novelty of the approach.

2) « We construct a linear DNN-based BoF » : I do not like this formulation. Here, you assume that you build a ResNet-50 with 1x1 as a representation and have a last final linear layer as a classifier. One could also claim it is a ResNet-48 as a representation followed by 2 layers of 1x1 as a classifier.

3) « our proposed model architecture is simpler » this is very subjective because for instance the FV models are learned in a layer-wise fashion, which makes their learning procedure more interpretable because each layer objective is specified. Furthermore, analyzing these models is now equivalent to analyze a cascade of fully connected layers, which is not simple at all.

4) Again, the interpretability mentioned in Sec. 3  is in term of spatial localization, not mapping. I think it is important to make clear this consideration. Indeed, this work basically leaves the problem of understanding general CNNs to the problem of understanding MLPs.

5) The graphic of the Appendix A is a bit misleading : it seems 13 downsampling are performed whereas it is not the case, because the first element of each group of block is actually only done once.(if I understood correctly)

6) I think the word feature is sometimes mis-used: sometimes it seems it can refer to a patch, sometimes to the code for a patch. (« Surprisingly, feature sizes assmall as 17 × 17 pixels »)

I got also few questions:
Q1 : I was wondering if you did try manifold learning on the patches ? Do you expect it to work ?
Q2 : Is there a batch normalization in the FC or a normalization? Did you try to threshold the heat maps before feeding them to the linear layer? I'm wondering indeed if the amplitude of those heatmaps is really key.
Q3 : do you think it would be easy to exploit the non-overlapping patches for a better parallelization of computations ?

Finally, I find very positive the amount of experiments to test the similarity with standard CNNs. Of course, it’s far from being a formal proof, but I think it is a very nice first step.

[1] Oyallon, Edouard, Eugene Belilovsky, and Sergey Zagoruyko. ""Scaling the scattering transform: Deep hybrid networks."" International Conference on Computer Vision (ICCV). 2017.","The sentiment of the review is generally positive, as indicated by phrases like 'interesting read' and 'valuable contribution.' The reviewer also appreciates the amount of experiments conducted. However, there are several critical comments and questions, which slightly temper the overall positivity. Therefore, the sentiment score is 60. The politeness of the language is quite high, as the reviewer uses polite phrases such as 'I think,' 'I would like to be addressed,' and 'I find very positive.' The reviewer also avoids harsh language and provides constructive feedback, leading to a politeness score of 80.",60,80
"This paper proposes learning exploration policies for navigation. The problem is motivated well. The learning is conducted using reinforcement learning, bootstrapped by imitation learning. Notably, RL is done using sensor-derived intrinsic rewards, rather than extrinsic rewards provided by the environment. The results are good.

I like this paper a lot. It addresses an important problem. It is written well. The approach is not surprising but is reasonable and is a good addition to the literature.

One reservation is that the method relies on an oracle for state estimation. In some experiments, synthetic noise is added, but this is not a realistic noise model and the underlying data still comes from an oracle that would not be available in real-world deployment. I recommend that the authors do one of the following: (a) use a real (monocular, stereo, or visual-inertial) odometry system for state estimation, or (b) acknowledge clearly that the presented method relies on unrealistic oracle odometry.

Even with this reservation, I support accepting the paper.

Minor: In Section 3.4, ""existing a room"" -> ""exiting a room""","The sentiment of the review is highly positive, as indicated by phrases like 'I like this paper a lot,' 'addresses an important problem,' and 'is written well.' The reviewer also supports accepting the paper despite a noted reservation. Therefore, the sentiment score is 90. The politeness of the language is also very high, as the reviewer uses polite and constructive language throughout, even when pointing out a reservation. The reviewer offers specific recommendations in a respectful manner and acknowledges the strengths of the paper. Thus, the politeness score is 100.",90,100
"The fundamental idea proposed in this paper is a sensible one:  design the functional form of a policy so that there is an initial parameterized stage that operates on perceptual input and outputs some ""symbolic"" (I'd be happier if we could just call them ""discrete"") characterization of the input, and then an arbitrary program that operates on the symbolic output of the first stage.

My fundamental problem is with equation 3.  If you want to talk about the factoring of the probability distribution p(a | s) that's fine, but, to do it in fine detail, it should be:
P(a | s) = \sum_sigma P(a, sigma | s) = \sum_sigma P(a | sigma, s) * P(sigma | s)
And then by conditional independence of a from s given sigma
 = \sum_sigma P(a | sigma) * P(sigma | s)
But, critically, there needs to be a sum over sigma!  Now, it could be that I am misunderstanding your notation and you mean for p(a | sigma) to stand for a whole factor and for the operation in (3) to be factor multiplication, but I don't think that's what is going on.

Then, I think, you go on to assume, that p(a | sigma) is a delta distribution.  That's fine.

But then equation 5 in Theorem 1 again seems to mention delta without summing over it, which still seems incorrect to me.

And, ultimately, I think the theorem doesn't make sense because the transformation that the program performs on its input is not included in the gradient computation.  Consider the case where the program always outputs action 0 no matter what its symbolic input is.   Then the gradient of the log prob of a trajectory with respect to theta should be 0, but instead you end up with the gradient of the log prob of the symbol trajectory with respect to theta.

I got so hung up here that I didn't feel I could evaluate the rest of the paper.  

One other point is that there is a lot of work that is closely related to this at the high level, including papers about Value Iteration Networks, QMDP Networks, Particle Filter Networks, etc.  They all combine a fixed program with a parametric part and differentiate the whole transformation to do gradient updates.  It would be important in any revision of this paper to connect with that literature.","The sentiment of the review is mixed but leans towards negative. The reviewer acknowledges the sensible idea proposed in the paper but expresses significant concerns about the mathematical formulation and theorems presented. The sentiment score is -40 because the review highlights fundamental issues that prevented the reviewer from evaluating the rest of the paper. The politeness score is 50 as the reviewer uses polite language, such as 'I'd be happier if' and 'it would be important,' while pointing out the issues and providing constructive feedback.",-40,50
"The paper proposes a dimensionality reduction method that embeds data into a product manifold of spherical, Euclidean, and hyperbolic manifolds. The proposed algorithm is based on matching the geodesic distances on the product manifold to graph distances. I find the proposed method quite interesting and think that it might be promising in data analysis problems. Here are a few issues that would be good to clarify:

- Could you please formally define K in page 3?

- I find the estimation of the signature very interesting. However, I am confused about how the curvature calculation process is (or can be) integrated into the embedding method proposed in Algorithm 1. How exactly does the sectional curvature estimation find use in the current results? Is the “Best model” reported in Table 2 determined via the sectional curvature estimation method? If yes, it would be good to see also the Davg and mAP figures of the best model in Table 2 for comparison.

- I think it would also be good to compare the results in Table 2 to some standard dimensionality reduction algorithms like ISOMAP, for instance in terms of Davg. Does the proposed approach bring advantage over such algorithms that try to match the distances in the learnt domain with the geodesic distances in the original graph?

- As a general comment, my feeling about this paper is that the link between the different contributions does not stand out so clearly. In particular, how are the embedding algorithm in Section 3.1, the signature estimation algorithm in Section 3.2, and the Karcher mean discussed in Section 3.3 related? Can all these ideas find use in an overall representation learning framework? 

- In the experimental results in page 7, it is argued that the product space does not perform worse than the optimal single constant curvature spaces. The figures in the experimental results seem to support this. However, I am wondering whether the complexity of learning the product space should also play a role in deciding in what kind of space the data should be embedded in. In particular, in a setting with limited availability of data samples, I guess the sample error might get too high if one tries to learn a very high dimensional product space.  


Typos: 

Page 3: Note the “analogy” to Euclidean products
Page 7 and Table 1: I guess “ring of cycles” should have been “ring of trees” instead
Page 13: Ganea et al formulates “basic basic” machine learning tools …","The sentiment of the review is generally positive, as the reviewer finds the proposed method interesting and promising for data analysis problems. However, the reviewer also points out several areas that need clarification and improvement, which slightly tempers the overall positivity. Therefore, the sentiment score is 60. The language used in the review is polite and constructive, with phrases like 'Could you please' and 'I think it would also be good,' indicating a respectful and helpful tone. Thus, the politeness score is 90.",60,90
"This paper proposes fine-tuning an out-of-distribution detector using an Outlier Exposure (OE) dataset. The novelty is in proposing a model-specific rather than dataset-specific fine-tuning. Their modifications are referred to as Outlier Exposure. OE includes the choice of an OE dataset for fine-tuning and a regularization term evaluated on the OE dataset. It is a comprehensive study that explores multiple datasets and improves dataset-specific baselines.

Suggestions and clarification requests:
- The structure of the writing does not clearly present the novel aspects of the paper as opposed to the previous works. I suggest moving the details of model-specific OE regularization terms to section 3 and review the details of the baseline models. Then present the other set of novelties in proposing OE datasets in a new section before presenting the results. Clearly presenting two sets of novelties in this work and then the results. If constrained in space, I suggest squeezing the discussion, conclusion, and 4.1.
- In the related work section Radford et al., 2016 is references when mentioning GAN. Why not the original reference for GAN?
- Maybe define BPP, BPC, and BPW in the paragraphs on PixelCNN++ and language modeling or add a reference.
- Numbers in Table 3 column MSP should match the numbers in Table 1, right? Or am I missing something?","The sentiment of the review is generally positive, as it acknowledges the novelty and comprehensiveness of the study. However, it also points out areas for improvement and clarification, which is typical in constructive peer reviews. Therefore, the sentiment score is 60. The language used in the review is polite and constructive, offering suggestions and requests for clarification without being harsh or dismissive. Thus, the politeness score is 80.",60,80
"I enjoyed reading this manuscript. The paper is based on a simple idea used by others as well (i.e., the image has two components, one  that encode content which is shared across domains and another one characterizing the domain specific style). The other important idea is the use of feature masks that steer the translation process without requiring semantic labels. This is similar to attention models used by others but I think it is novel when applying to this specific application domain. I was a bit disappointed by the evaluation part. The authors decided to perform ablation and to show the importance of each component using only the MNIST-Single dataset. While this is good as a toy example I would have expected to see such analysis on a more complex example, e.g., street-view translation. This is also surprising considering that it is not even present in the supplementary material. Overall, this is a solid submission with interesting ideas and good implementation.   ","The sentiment of the review is generally positive, as indicated by phrases like 'I enjoyed reading this manuscript' and 'Overall, this is a solid submission with interesting ideas and good implementation.' However, there is a note of disappointment regarding the evaluation part, which slightly tempers the overall positivity. Therefore, the sentiment score is 70. The politeness of the language is high, as the reviewer uses polite and constructive language throughout, even when pointing out areas for improvement. Phrases like 'I was a bit disappointed' and 'I would have expected' are gentle and considerate. Thus, the politeness score is 90.",70,90
"Overall:
This paper works on improving the gradient estimator of the ELBO. Author experimentally found that the estimator of the existing work(STL) is biased and proposed to reduce the bias by using the technique like  REINFORCE.
The problem author focused on is unique and the solution is simple, experiments show that proposed method seems promising.

Clarity:
The paper is clearly written in the sense that the motivation of research is clear, the derivation of the proposed method is easy to understand.

Significance:
I think this kind of research makes the variational inference more useful, so this work is significant. But I cannot tell the proposed method is really useful, so I gave this score.
The reason I doubt the reason is that as I written in the below, the original STL can handle the mixture of Gaussians as the latent variable but the proposed method cannot. So I do not know which is better and whether I should use this method or use the original STL with flexible posterior distribution to tighten the evidence lower bound. I think additional experiments are needed. I know that motivation is a bit different for STL and proposed method but some comparisons are needed.

Question and minor comments:
In the original paper of STL, the author pointed out that by freezing the gradient of variational parameters to drop the score function term, we can utilize the flexible variational families like the mixture of Gaussians.
In this work, since we do not freeze the variational parameters, we cannot utilize the mixture of Gaussians as in the STL. IWAE improves the lower bound by increasing the samples, but we can also improve the bound by specifying the flexible posteriors like the mixture of Gaussians in STL.
Faced on this, I wonder which strategy is better to tighten the lower bound, should we use the STL with the mixture of Gaussians or use the proposed method?  
To clarify the usefulness of this method, I think the additional experimental comparisons are needed.

About the motivation of the paper, I think it might be better to move the Fig.1 about the Bias to the introduction and clearly state that the author found that the STL is biased ""experimentally"".

Followings are minor comments.
In experiment 6.1, I'm not sure why the author present the result of K ELBO estimator in the plot of Bias and Variance.
I think author want to point that when K=1, STL is unbiased with respect to the 1 ELBO, but when k>1, it is biased with respect to IWAE estimator.
However, the objective of K ELBO and IWAE are different, it may be misleading. So this should be noted in the paper.

In Figure 3, the left figure, what each color means? Is the color assignment is the same with the middle figure?
(Same for Figure 4)","The sentiment of the review is generally positive, as the reviewer acknowledges the uniqueness of the problem and the promise of the proposed solution. However, there are some reservations about the usefulness of the method without additional experiments, which slightly tempers the overall positivity. The sentiment score is therefore 60. The politeness of the language is high, as the reviewer uses polite and constructive language throughout, providing specific suggestions and minor comments without being dismissive or harsh. The politeness score is 90.",60,90
"This paper studied data poisoning attacking for graph neural networks. The authors proposed treating graph structures as hyperparameters and leveraged recent progress on meta-learning for optimizing the adversarial attacks. Different from some recent work on adversarial attacks for graph neural networks (Zuigner et al. 2018; Dai et al. 2018), which focus on attacking specific nodes, this paper focuses on attacking the  overall performance of graph neural networks. Experiments on a few data sets prove the effectiveness of the proposed approach. 

Strength:
- the studied problem is very important and recently attracting increasing attention
- Experiments show that the proposed method is effective.

Weakness:
- the complexity of the proposed method seems to be very high
- the data sets used in the experiments are too small
Details:
-- the complexity of the proposed method seems to be very high. The authors should explicitly discuss the complexity of the proposed method. 
-- the data sets in the experiments are too small. Some large data sets would be much more compelling.
-- Are the adversarial examples identified by the proposed method transferrable to other graph embedding algorithms (e.g., the unsupervised node embedding methods, DeepWalk, LINE, and node2vec)?
-- I like Figure 3, though some concrete examples would be more intuitive. ","The sentiment of the review is generally positive, as it acknowledges the importance of the studied problem and the effectiveness of the proposed method. However, it also points out some weaknesses, such as the high complexity of the method and the small size of the data sets used in the experiments. The language used is polite and constructive, offering specific suggestions for improvement without being harsh or dismissive.",60,80
"*Summary :
The paper explores variants of popular adaptive optimization methods.
The idea is to clip the magnitude of the gradients from above and below in order to prevent too aggressive/conservative updates.
The authors provide regret bound to this algorithm in the online convex setting and perform several illustrative experiments.


*Significance:
-There is not much novelty in Theorems 1,2,3 since similar results already appeared in Reddi et al.

-Also, the theoretical part does not demonstrate the benefit of the clipping idea. Concretely, the regret bounds seem to be similar to the bounds of AMSBound.
Ideally, I would like to see an analysis that discusses a situation where AdaGrad/AMSBound fail or perfrom really bad, yet the clipped versions do well.

-The experimental part on the other hand is impressive, and the results illustrate the usefulness of the clipping idea.

*Clarity:
The idea and motivation are very clear and so are the experiments.


*Presentation:
The presentation is mostly good.

Summary of review:
The paper suggests a simple idea to avoid extreme behaviour of the learning rate in standard adaptive methods. The theory is not so satisfying, since it does not illustrate the benefit of the method over standard adaptive methods. The experiments are more thorough and illustrate the applicability of the method.

","The sentiment of the review is mixed. The reviewer acknowledges the value of the experimental part but criticizes the lack of novelty and theoretical contribution. Therefore, the sentiment score is slightly negative at -20. The politeness of the language is quite high; the reviewer uses polite and constructive language, even when pointing out weaknesses. Thus, the politeness score is 80.",-20,80
"This paper describes a novel method for solving inverse problems in imaging.

The basic idea of this approach is use the following steps:
1. initialize with nonnegative least squares solution to inverse problem (x0)
2. compute m different projections of x0
3. estimate x from the m different projections by solving ""reformuated"" inverse problem using TV regularization.

The learning part of this algorithm is in step 2, where m different convolutional neural networks are used to learn m good projections. The projections correspond to computing a random Delaunay triangulation over the image domain and then computing pixel averages within each triangle. It's not clear exactly what the learning part is doing, i.e. what makes a ""good"" triangulation, why a CNN might accurately represent one, and what the shortcomings of truly random triangulations might be.

More specifically, for each projection the authors start with a random set of points in the image domain and compute a Delaunay triangulation. They average x0 in each of the Delaunay triangles. Then since the projection is constant on each triangle, the projection into the lower-dimensional space is given by the magnitude of the function over each of the triangular regions. Next they train a convolutional neural network to approximate the above projection. The do this m times. It's not clear why the neural network approximation is necessary or helpful. 

Empirically, this method outperforms a straightforward use of a convolutional U-Net to invert the problem.

The core novelty of this paper is the portion that uses a neural network to calculate a projection onto a random Delaunay triangulation. The idea of reconstructing images using random projections is not especially new, and much of the ""inverse-ness"" of the problem here is removed by first taking the pseudoinverse of the forward operator and applying it to the observations. Then the core idea at the heart of the paper is to speed up this reconstruction using a neural network by viewing the projection onto the mesh space as a set of special filter banks which can be learned.

At the heart of this paper is the idea that for an L-Lipschitz function f : R^k → R the sample complexity
is O(L^k), so the authors want to use the random projections to essentially reduce L. However, the Cooper sample complexity bound scales with k like k^{1+k/2}, so the focus on the Lipschitz constant seems misguided.
This isn't damning, but it seems like the piecewise-constant estimators are a sort of regularizer, and that's where we
really get the benefits.

The authors only compare to another U-Net, and it's not entirely clear how they even trained that U-Net. It'd be nice to see if you get any benefit here from their method relative to other approaches in the literature, or if this is just better than inversion using a U-Net. Even how well a pseudoinverse does would be nice to see or TV-regularized least squares.

Practically I'm quite concerned about their method requiring training 130 separate convolutional neural
nets. The fact that all the different datasets give equal quality triangulations seems a bit odd, too. Is
it possible that any network at all would be okay? Can we just reconstruct the image from regression
on 130 randomly-initialized convolutional networks? 

The proposed method isn't bad, and the idea is interesting. But I can't help but wonder whether it works just because what we're doing is denoising the least squares reconstruction, and regression on many random projections might be pretty good for that. Unfortunately, the experiments don't help with developing a deeper understanding. 
","The sentiment of the review is mixed. The reviewer acknowledges the novelty and interesting aspects of the proposed method but also raises several concerns and questions about its implementation and effectiveness. The sentiment score is therefore slightly positive. The language used in the review is polite and constructive, aiming to provide helpful feedback rather than being dismissive or rude.",20,80
"This is a paper of the verification of neural networks, i.e. check their robustness, 
and the main contribution here is to tackle it as a statistical problem adressed with 
multi-level splitting Monte Carlo approach. I found the paper well motivated and original, 
resulting in a publishable piece of research up to a few necessary adjustments. These 
concern principally notation issues and some potential improvements in the writing. 
Let me list below some main remarks along the text, including also some typos. 

* In the introduction, ""the classical approach"" is mentioned but to be the latter is 
insufficiently covered. Some more detail would be welcome. 

* page 2, ""predict the probability"": rather employ ""estimate"" in such context? 

* ""linear piecewise"": ""piecewise linear""? 

* what is ""an exact upper bound""? 

* In related work, no reference to previous work on ""statistical"" approaches to NN 
verification. Is it actually the case that this angle has never been explored so far?

* I am not an expert but to me ""the density of adversarial examples"" calls for further 
explanation. 

* From page 3 onwards: I was truly confused by the use of [x] throughought the text 
(e.g. in Equation (4)). x is already present within the indicator, no need to add yet 
another instance of it. Here and later I suffered from what seems to be like an awkward 
attempts to stress dependency on variables that already appear or should otherwise 
appear in a less convoluted way. 

* In Section 4, it took me some time to understand that the considered metrics do not 
require actual observations but rather concern coherence properties of the NN per se. 
While this follows from the current framework, the paper might benefit from some more 
explanation in words regarding this important aspect. 

* In page 6, what is meant by ""more perceptually similar to the datapoint""? 

* In the discussion: is it really ""a new measure"" that is introduced here? 

* In the appendix: the MH acronym should better be introduced, as should the notation 
g(x,|x') if not done elsewhere (in which case a cross-reference would be welcome). 
Besides this, writing ""the last samples"" requires disambiguation (using ""respective""?). 


 

 ","The sentiment of the review is generally positive, as the reviewer finds the paper well-motivated, original, and publishable with some necessary adjustments. This is reflected in the positive comments about the paper's motivation and originality. However, the reviewer also points out several areas for improvement, which slightly tempers the overall positivity. Therefore, the sentiment score is 70. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, offering suggestions and asking questions in a respectful manner. The reviewer avoids harsh criticism and instead focuses on how the paper can be improved. Therefore, the politeness score is 90.",70,90
"In this paper, the authors propose a new approach to representation learning in the context of reinforcement learning.
The main idea is that two states should be distinguished *functionally* in terms of the actions that are needed to reach them,
in contrast with generative methods which try to capture all aspects of the state dynamics, even those which are not relevant for the task at hand.
The method of the authors assumes that a goal-conditioned policy is already learned, and they use a Kullback-Leibler-based distance
between policies conditioned by these two states as the loss that the representation learning algorithm should minimize.
The experimental study is based on 6 simulated environments and outlines various properties of the framework.

Overall, the idea is interesting, but the paper suffers from many weaknesses both in the framework description and in the experimental study that make me consider that it is not ready for publication at a good conference like ICLR.

The first weakness of the approach is that it assumes that a learned goal-conditioned policy is already available, and that the representation extracted from it can only be useful for learning ""downstream tasks"" in a second step. But learning the goal-conditioned policy from the raw input representation in the first place might be the most difficult task. In that respect, wouldn't it be possible to *simultaneously* learn a goal-conditioned policy and the representation it is based on? This is partly suggested when the authors mention that the representation could be learned from only a partial goal-conditioned policy, but this idea definitely needs to be investigated further.

A second point is about unsufficiently clear thoughts about the way to intuitively advocate for the approach. The authors first claim that two states are functionally different if they are reached from different actions. Thinking further about what ""functionally"" means, I would rather have said that two states are functionally different if different goals can be reached from them. But when looking at the framework, this is close to what the authors do in practice: they use a distance between two *goal*-conditioned policies, not *state*-conditioned policies. To me, the authors have established their framework thinking of the case where the state space and the goal space are identical (as they can condition the goal-conditioned policy by any state=goal). But thinking further to the case where goals and states are different (or at least goals are only a subset of states), probably they would end-up with a different intuitive presentation of their framework. Shouldn't finally D_{act} be a distance between goals rather than between states?

Section 4 lists the properties that can be expected from the framework. To me, the last paragraph of Section 4 should be a subsection 4.4 with a title such as ""state abstraction (or clustering?) from actionable representation"". And the corresponding properties should come with their own questions and subsection in the experimental study (more about this below).

About the related work, a few remarks:
- The authors do not refer to papers about using auxiliary tasks. Though the purpose of these works is often to supply for additional reward signals in the sparse reward context, then are often concerned with learning efficient representations such as predictive ones.
- The authors refer to Pathak et al. (2017), but not to the more recent Burda et al. (2018) (Large-scale study of curiosity-driven learning) which insists on the idea of inverse dynamical features which is exactly the approach the authors may want to contrast theirs with. To me, they must read it.
- The authors should also read Laversanne-Finot et al. (2018, CoRL) who learn goal space representations and show an ability to extract independently controllable features from that.

A positive side of the experimental study is that the 6 simulated environments are well-chosen, as they illustrate various aspects of what it means to learn an adequate representation. Also, the results described in Fig. 5 are interesting. A side note is that the authors address in this Figure a problem pointed in Penedones et al (2018) about ""The Leakage Propagation problem"" and that their solution seems more convincing than in the original paper, maybe they should have a look.
But there are also several weaknesses:
- for all experiments, the way to obtain a goal-conditioned policy in the first place is not described. This definitely hampers reproducibility of the work. A study of the effect of various optimization effort on these goal-conditioned policies might also be of interest.
- most importantly, in Section 6.4, 6.5 and 6.6, much too few details are given. Particularly in 6.6, the task is hardly described with a few words. The message a reader can get from this section is not much more than ""we are doing something that works, believe us!"". So the authors should choose between two options:
* either giving less experimental results, but describing them accurately enough so that other people can try to reproduce them, and analyzing them so that people can extract something more interesting than ""with their tuning (which is not described), the framework of the authors outperforms other systems whose tuning is not described either"".
* or add a huge appendix with all the missing details.
I'm clearly in favor of the first option.

Some more detailed points or questions about the experimental section:
- not so important, Section 6.2 could be grouped with Section 6.1, or the various competing methods could be described directly in the sections where they are used.
- in Fig. 5, in the four room environment, ARC gets 4 separated clusters. How can the system know that transitions between these clusters are possible?
- in Section 6.3, about the pushing experiment, I would like to argue against the fact that the block position is the important factor and the end-effector position is secundary. Indeed, the end-effector must be correctly positioned so that the block can move. Does ARC capture this important constraint?
- Globally, although it is interesting, Fig.6 only conveys a quite indirect message about the quality of the learned representation.
- Still in Fig. 6, what is described as ""blue"" appears as violet in the figures and pink in the caption, this does not help when reading for the first time.
- In Section 6.4, Fig.7 a, ARC happens to do better than the oracle. The authors should describe the oracle in more details and discuss why it does not provide a ""perfect"" representation.
- Still in Section 6.4, the authors insist that ARC outperforms VIME, but from Fig.7, VIME is not among the best performing methods. Why insist on this one? And a deeper discussion of the performance of each method would be much more valuable than just showing these curves.
- Section 6.5 is so short that I do not find it useful at all.
- Section 6.6 should be split into the HRL question and the clustering question, as mentioned above. But this only makes sense if the experiments are properly described, as is it is not useful.

Finally, the discussion is rather empty, and would be much more interesting if the experiments had been analyzed in more details.

typos:

p1: that can knows => know
p7: euclidean => Euclidean
","The review starts with a neutral to slightly positive sentiment, acknowledging the interesting idea behind the paper. However, it quickly shifts to a more critical tone, highlighting several weaknesses in both the framework description and the experimental study. The overall sentiment score is therefore slightly negative. The language used is generally polite, with constructive criticism and suggestions for improvement, although it does contain some direct and firm statements about the paper's shortcomings.",-30,60
"Authors propose a parameter sharing scheme by allowing parameters to be reused across layers. It further makes connection between traditional CNNs with RNNs by adding additional regularization and using hard sharing scheme.

The way of parameter sharing is similar to the filter prediction method proposed in Rebuff et al’s work, where they model a convolutional layer’s parameters as a linear combination of a bank of filters and use that to address difference among multiple domains.

Sylvestre-Alvise Rebuffi, Hakan Bilen, Andrea Vedaldi, Learning multiple visual domains with residual adapters, NIPS 2017.

The discussion on the connection between coefficients for different layers and a network’s structure and visualization of layer similarity matrix is interesting. Additional regularization can further encourage a recurrent neural network to be learned. 

However, they only experiment with one or two templates and advantage on accuracy and model size  over other methods is not very clear.","The review starts by summarizing the authors' work and acknowledges the interesting aspects of their approach, such as the connection between coefficients for different layers and the visualization of the layer similarity matrix. However, it also points out a significant limitation regarding the limited experimentation with templates and the unclear advantage in accuracy and model size. The sentiment score is slightly positive as the review appreciates the novel aspects but also highlights areas for improvement. The language used is polite and constructive, providing specific feedback without being harsh or dismissive.",20,80
"This paper presents the following approach to domain adaptation. Train a source domain RNN. While doing inference on the target domain, first you run the source domain RNN on the sequence. Then while running the target domain RNN, set the hidden state at time step i, h^t_i, to be a function 'f' of  h^t_{i-1} and information from source domain \psi_i; \psi_i is computed as a convex combination of the state of the source domain RNN, h^s_{i}, and an attention-weighted average of all the states h^s{1...n}. So in effect, the paper transfers information from each of source domain cells -- the cell at time step i and all the ""collocated"" cells (collocation being defined in terms of attention). This idea is then extended in a straightforward way to LSTMs as well. 
 
Doing ""cell-level"" transfer enables more information to be transferred according to the authors, but it comes at a higher computation since we need to do O(n^2) computations for each cell.

The authors show that this beats a variety of baselines for classification tasks (sentiment), and for sequence tagging task (POS tagging over twitter.)

Pros:
1. The idea makes sense and the experimental results show solid 

Cons:
1. Some questions around generalization are not clearly answered. E.g. how are the transfer parameters of function 'f' (that controls how much source information is transferred to target) trained? If the function 'f' and the target RNN is trained on target data, why does 'f' not overfit to only selecting information from the target domain? Would something like dropping information from target domain help?

2. Why not also compare with a simple algorithm of transferring parameters from source to target domain? Another simple baseline is to just train final prediction function (softmax or sigmoid) on the concatenated source and target hidden states. Why are these not compared with? Also, including the performance of simple baselines like word2vec/bow is always a good idea, especially on the sentiment data which is very commonly used and widely cited. 

3. Experiments: the authors cite the hierarchical attention transfer work of Li et al (https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/download/16873/16149) and claim their approach is better, but do not compare with them in the experiments. Why?

Writing:
The writing is quite confusing at places and is the biggest problem with this paper. E.g.

1. The authors use the word ""collocated"" everywhere, but it is not clear at all what they mean. This makes the introduction quite confusing to understand. I assumed it to mean words in the target sentences that are strongly attended to. Is this correct? However, on page 4, they claim ""The model needs to be evaluated O(n^2) times for each sentence pair."" -- what is meant by sentence pair here? It almost leads me to think that they consider all source sentence and target sentences? This is quite confusing. 

2. The authors keep claiming that ""layer-wise transfer learning mechanisms lose the fine-grained cell-level information from the source domain"", but it is not clear exactly what do they mean by layer-wise here. Do they mean transferring the information from source cell i to target cell i as it is? In the experiments section on LWT, the authors claim that ""More specifically, only the last cell of the RNN layer transfers information. This cell works as in ART. LWT only works for sentence classification."" Why is it not possible to train a softmax over both the source hidden state and the target hidden state for POS tagging? 

nits:
page 4 line 1: ""i'th cell in the source domain"" -> ""i'th cell in the target domain"". ""j'th cell in target"" -> ""j'th cell in sourcE"".


Revised: increased score after author response.
","The sentiment of the review is mixed but leans towards positive. The reviewer acknowledges the solid experimental results and the sensible idea behind the approach, which suggests a positive sentiment. However, the review also contains several critical points and questions about the methodology and writing, which temper the overall positivity. Therefore, the sentiment score is 20. The politeness of the language is generally high. The reviewer uses polite language, such as 'the idea makes sense' and 'the experimental results show solid,' and provides constructive criticism without being rude. Therefore, the politeness score is 80.",20,80
"Summary:
The authors propose to apply the Deep Variational Information Bottleneck (VIB) method of [1] on discriminator networks in various adversarial-learning-based scenarios. They propose a way to adaptively update the value for the bêta hyper-parameter to respect the constraint on I(X,Z). Their technique is shown to stabilize/allow training when P_g and P_data do not overlap, similarly to WGAN and gradient-penalty based approaches, by essentially pushing their representation distributions (p_z) to overlap with the mutual information bottleneck. It can also be considered as an adaptive version of instance noise, which serves the same goal. The method is evaluated on different adversarial learning setup (imitation learning, inverse reinforcement learning and GANs), where it compares positively to most related methods. Best results for ‘classical’ adversarial learning for image generation are however obtained when combining the proposed VIB with gradient penalty (which outperforms by itself the VGAN in this case).


Pros :
- This paper brings a good amount of evidence of the benefits to use the VIB formulation to adversarial learning by first showing the effect of such approach on a toy example, and then applying it to more complex scenarios, where it also boosts performance. The numerous experiments and analyses have great value and are a necessity as this paper mostly applies the VIB to new learning challenges. 

- The proposition of a principled way of adaptively varying the value of Bêta to actually respect more closely the constraint I(X,Z) < I_c, which to my knowledge [1] does not perform, is definitely appealing and seems to work better than fixed Bêtas and does also bring the KL divergence to the desired I_c.

- The technique is fairly simple to implement and can be combined with other stabilization techniques such as gradient penalties on the discriminator.


Cons:

- In my view, the novelty of the approach is somewhat limited, as it seems like a straightforward application of the VIB from [1] for discriminators in adversarial learning, with the difference of using an adaptive Bêta.

- I think the Bêta-VAE [2] paper is definitely related to this paper and to the paper on which it is based [1] and should thus be cited as the authors use a similar regularization technique, albeit from a different perspective, that restricts I(X,Z) in an auto-encoding task.

- I think the content of batches used to regularize E(z|x) w.r.t. to the KL divergence should be clarified, as the description of p^tilde “being a mixture of the target distribution and the generator” (Section 4) can let the implementation details be ambiguous. I think batches containing samples from both distributions can cause problems as the expectation of the KL divergence on a batch can be low even if the samples from both distributions are projected into different parts of the manifold. This makes me think batches are separated? Either way, this should be more clearly stated in the text.

- The last results for  the ‘traditional’ GAN+VIB show that in this case, gradient penalty (GP) alone outperforms the proposed VGAN, and that both can be combined for best results. I thus wonder if the results in all other experiments could show similar trends if GP had been tested in these cases as well. In the imitation learning task, authors compare with instance noise, but not with GP, which for me are both related to VIB in what they try to accomplish. Was GP tested in Imitation Learning/Inverse RL ? Was it better? Could it still be combined with VIB for better results? 

- In the saliency map of Figure 5, I’m unclear as to what the colors represent (especially on the GAIL side). I doubt that this is simply due to the colormap used, but this colormap should be presented.

Overall, I think this is an interesting and relevant paper that I am very likely to suggest to peers working on adversarial learning, and should therefore be presented. I think the limited novelty is counterbalanced by the quality of empirical analysis. Some clarity issues and missing citations should be easy to correct. I appreciate the comparison and combination with a competitive method (Gradient Penalty) in Section 5.3, but I wish similar results were present in the other experiments, in order to inform readers if, in these cases as well, combining VIB with GP leads to the best performance.

[1] Deep Variational Information Bottleneck, (Alemi et al. 2017)
[2] beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework (Higgins et al. 2017)
","The sentiment of the review is generally positive, as the reviewer acknowledges the value and relevance of the paper, despite noting some limitations in novelty and clarity. The reviewer appreciates the empirical analysis and suggests that the paper should be presented. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer provides constructive feedback in a respectful and professional manner, making suggestions for improvement without being harsh or dismissive. Thus, the politeness score is 90.",60,90
"This paper proposes a new approach to use more informative signals (than only class labels), specifically, regions humans deem important on images, to improve deep convolutional neural networks. They collected a large dataset by implementing a game on clickme.ai and showed that using this information results in both i) improved classification accuracy and ii) more interpretable features. 

I think this is good work and should be accepted. The main contribution is three fold: i) a publicly available dataset that many researchers can use, ii) a network module to incorporate this human information that might be inserted into many networks to improve performance, and iii) some insights on the effect of such human supervision and the relation between features that humans deem important to those that neural nets deem important. 

Some suggestions on how to improve the paper:
1. I find Sections 3 & 4 hard to track - some missing details and notation issues. Several variables are introduced without detailing the proper dimensions, e.g., the global feature attention vector g (which is shown in the figure actually). The relation between U and u_k isn't clear. Also, it will help to put a one-sentence summary of what this module does at the beginning of Section 3, like the last half-sentence in the caption of Figure 3. I was quite lost until I see that. Some more intuition is needed, on W_expand and W_shrink; maybe moving some of the ""neuroscience motivation"" paragraph up into the main text will help. Bold letters are used to denote many different things - in  Section 4 as a set of layers, in other places a matrix/tensor, and even an operation (F). 

2. Is there any explanation on why you add the regularization term to every layer in a network? This setup seems to make it easy to explain what happens in Figure 4. One interesting observation is that after your regularization, the GALA features with ClickMe maps exhibit minimal variation across layers (those shown). But without this supervision the features are highly different. What does this mean? Is this caused entirely by the regularization? Or there's something else going on, e.g., this is evidence suggesting that with proper supervision like human attention regions, one might be able to use a much shallower network to achieve the same performance as a very deep one?

3. Using a set of 10 images to compute the correlation between ClickMe and Clicktionary maps isn't ideal - this is even less than the number of categories among the images. I'm also not entirely convinced that ""game outcomes from the first and second half are roughly equal"" says much about humans not using a neural net-specific strategy, since you can't rule out the case that they learned to play the game very quickly (in the first 10 of the total 380 rounds). 

4. Title - this paper sound more ""human feedback"" to me than ""humans-in-the-loop"", because the loop has only 1 iteration.  Because you are collecting feedback from humans but not yet giving anything back to them. Maybe change the title?","The sentiment of the review is positive, as the reviewer explicitly states that the work is good and should be accepted. The reviewer highlights the main contributions of the paper and provides constructive feedback for improvement. The politeness of the language is high, as the reviewer uses polite and respectful language throughout the review, offering suggestions in a helpful manner without being dismissive or harsh.",90,95
"General comment
==============
The authors describe two loss functions for learning embeddings of protein amino acids based on i) predicting the global structural similarity of two proteins, and ii) predicting amino acid contacts within proteins. As far as I know, these loss functions are novel and the authors show clear improvements when using the learned embeddings in downstream tasks. The paper is well motivated and mostly clearly written. However, the evaluation must be strengthened and some aspects of it clarified. Provided that the authors address my comments below, I think it is a good ICLR application paper.

Major comments
=============
1. The authors should describe how they optimized hyperparameters such as the learning, lambda (loss section 3.3), or the smoothing factor (section 3.4). These should be optimized on an evaluation set, but the authors only mentioned that they split the dataset into training and holdout (test) set (section 4.1).

2. The way the authors present results in table 1 and table 2 is unclear. Both table 1 and table 2 contain results of the structural similarity tasks but with different baselines. ‘SSA w/ contact predictions’ is also undefined and can be interpreted as ‘with’ or ‘without’ contacts predictions. I therefore strongly recommend to show structural similarity results in table 1 and secondary structure results in table 2 and include in both tables i) ‘SSA full’, ‘SSA without contact predictions’, and ‘SSA without language model’.

3. The authors should compare SSA to the current state-of-the art in structure prediction in addition to baseline models.

4. The authors should evaluate how well their method predicts amino acid contact maps.

5. The authors should describe how they were dealing with variable-length protein sequences. Are sequences truncated and embedded to a fixed length? What is the mean and variance in protein sequence lengths in the considered datasets? The authors should point out that their method is limited to fixed length sequences.

6. The authors should briefly describe the training and inference time on a single GPU and CPU. How much memory is required for training with a certain sequence length, e.g. 400 amino acids per sequence? Does the model fit on a single GPU?

7. The authors should discuss limitations of their method, e.g. that it cannot handle variable length sequences and that the memory scales quadratically by the the sequence length.

8. CRF (SSA) (table 3) includes a biLSTM layer between SSA and the CRF. However, the biLSTM can learn a non-linear projection of embeddings learned by SSA such that it is unclear if improvements are due to the embeddings learned by SSA or the biLSTM+CRF architecture. The authors should therefore train a biLSTM+CRF model on one-hot encoded amino-acids and include it as baseline in table 3.


Minor comments
=============
9. The way the similarity score s’ is computed (section 3.2.1) should be motivated more clearly. Why do the authors compute the score s’ manually instead of predicting it, e.g. using a model that takes the embeddings z of both proteins as input and predicts a single scalar s’? 

10. How does ordinal regression (section 3.2.2) perform compared with a softmax layer? Why do the authors compute s’ and then train logistic regression classifiers on s’ to predict the similarity level, instead of predicting the similarity level directly based on the embeddings z?

11. Why do the authors use a distance threshold of 8A (section 3.3)? Is this common practice in the field?

12. Why do the authors use the not product and the absolute difference as features instead of the embeddings z directly? Which activation function is used to predict contact probabilities (sigmoid, softmax, …)?

13. The authors should reference and describe the results presented in table 1 more clearly.

14. Optional: the authors should analyze if learned embeddings are correlated with amino acid and structural properties such as their size, charge, or solvent accessibility. Do embeddings clusters by certain properties? This can be analyzed, e.g., using a tSNE plot. 

15. How does TMalign perform when using the maximum or geometric average instead of the arithmetic average of the two scores (section 4.1)","The review starts with a positive sentiment, acknowledging the novelty of the loss functions and the clear improvements shown by the authors. The reviewer appreciates the motivation and clarity of the paper but points out that the evaluation needs strengthening and clarification. The language used is polite and constructive, offering specific recommendations for improvement without being dismissive or rude. The reviewer provides detailed feedback and suggestions in a respectful manner, indicating a willingness to see the paper improved and accepted.",70,80
"I have no major complaints with this work.  It is well presented and easily understandable. I agree with the claim that the largest gains are largely syntactic, but this leads me to wonder about more tail phenomena.   PP attachment is a classic example of a syntactic decision requiring semantics, but one could also imagine doing a CCG supertagging analysis to see how well the model captures specific long-tail phenomena.  Though a very different task Vaswani et al 16, for example, showed how bi-LSTMs were necessary for certain constructions (presumably current models would perform much better and may capture this information already).

An important caveat of these results is that the evaluation (by necessity) is occurring in English.  Discourse in a pro-drop language would presumably require longer contexts than many of these approaches currently handle.","The sentiment of the review is generally positive, as indicated by phrases like 'I have no major complaints with this work' and 'It is well presented and easily understandable.' The reviewer does raise some questions and suggestions for further analysis, but these are framed constructively rather than critically. Therefore, the sentiment score is 80. The politeness of the language is also high, as the reviewer uses polite and respectful language throughout, such as 'I agree with the claim' and 'one could also imagine.' There are no harsh or rude comments, so the politeness score is 90.",80,90
"
The authors make a case that deep networks are biased
toward fitting data with simple functions.

The start by examining the priors on classifiers obtained by sampling
the weights of a neural network according to different distributions.  They do this
in two ways.  First, they examine properties of the distribution
on binary-valued functions on seven boolean inputs obtained by
sampling the weights of a small neural network.  They also empirically compare
the labelings obtained by sampling the weights of a network with
labelings obtained from a Gaussian process model arising from earlier
work.

Next, they analyze the complexity of the functions produced, using
different measures of the complexity of boolean functions.  A
favorite of theirs is something that they call Lempel-Ziv complexity,
which is measured by choosing an arbitrarily ordering of the
domain, writing the outputs of the function in that ordering,
and looking at how well the Lempel-Ziv algorithm compresses this
sequence.  I am not convinced that this is the most meaningful
and fundamental measure of the complexity of functions.
(In the supplementary material, they examine some others.
They show plots relating the different measures in the body
of the paper.  None of the measures is specified in detail in the
body of the paper. They provide plots relating these complexity
measures, but they don't demonstrate a very close connection.)

The authors then evaluate the generalization bound obtained by
applying a PAC Bayes bound, together with the assumption that
the training process produces weights sampled from the distribution
obtained by conditioning weights chosen according to the random
initialization on the event that they fit they fit the training
data perfectly.  They do this for small networks and simple datasets.
They bounds are loose, but not vacuous, and follow the same order
of difficulty on a handful of datasets as the true generalization
error.

In all of their experiments, they stop training when the training
accuracy reaches 100%, where papers like https://arxiv.org/pdf/1706.08947.pdf
have found that continuing training past this point further improves test
accuracy.  The experiments all use architectures that are
quite dissimilar to what is commonly used in practice, and
achieve much worse accuracy, so that a reader is concerned
that the results differ qualitatively in other respects.

I do not find it surprising that randomly sampling parameters
of deep networks leads to simple functions.

Papers like the Soudry, et al paper cited in this submission are
inconsistent with the assumption in the paper that SGD samples
parameters uniformly.

It is not clear to me how many hidden layers were used for the
results in Table 1 (is it four?).  

I did find it interesting to see exactly how concentrated the
distribution of functions obtained in their 7-input experiment
was, and also found results on the agreement of the Gaussian process
models with the randomly sampled weight interesting, as far as they
went.  Overall, I am not sure that this paper provided enough
fundamental new insight to be published in ICLR.
","The sentiment of the review is mixed but leans slightly negative. The reviewer acknowledges some interesting aspects of the paper but expresses significant doubts about its contributions and methodology. The sentiment score is therefore -20. The politeness of the language is generally neutral to slightly polite. The reviewer uses phrases like 'I am not convinced' and 'It is not clear to me,' which are polite ways of expressing criticism. The politeness score is 20.",-20,20
"Adversarial attacks and defences are of growing popularity now a days. As AI starts to be present everywhere, more and more people can start to try to attack those systems. Critical systems such as security systems are the ones that can suffer more from those attacks. In this paper the case of vehicles that attack an object detection system by trying to not be detected are tackled.

The proposed system is trained and evaluated in a simulation environment. A set of possible camouflage patterns are proposed and the system learns how to setup those in the cars to reduce the performance of the detection system. Two methods are proposed. Those methods are based on Expectation over transformation method. This method requires the simulator to be differentiable which is not the case with Unity/Unreal environments. The methods proposed skip the need of the simulator to be differentiable by approximating it with a neural network.

The obtained results reduce the effectivity of the detection system. The methods are compared with two trivial baselines. Isn't there any other state of the art methods to compare with?

The paper is well written, the results are ok, the related work is comprehensive and the formulation is correct. The method is simply but effective. Some minor comments:
 - Is the simulator used CARLA? Or is a new one? Where are the 3D assets extracted from?
 - Two methods are proposed but I only find results for one","The sentiment of the review is generally positive, as the reviewer acknowledges the growing importance of the topic, the effectiveness of the proposed methods, and the quality of the writing. However, there are some critical points raised, such as the lack of comparison with state-of-the-art methods and the incomplete presentation of results for both proposed methods. Therefore, the sentiment score is 50. The politeness of the language is quite high, as the reviewer uses polite language and phrases like 'well written' and 'minor comments,' even when pointing out issues. Thus, the politeness score is 80.",50,80
"In this paper the authors present mutual posterior divergence regularization, a data-dependent regularization for the ELBO that enforces diversity and smoothness of the variational posteriors. The experiments show the effectiveness of the model for density estimation and representation learning.
This is an interesting paper dealing with the important issues of fully exploiting the stochastic part of VAE models and avoiding inactive latent units in the presence of very expressive decoders. The paper reads well and is well motivated. 

The authors claim that their method is ""encouraging the learned variational posteriors to be diverse"". While it is important to have models that can use well the latent space, the constraints that are encoded seem too strong. If two data points are very similar, why should there be a term encouraging their posterior approximation to be different? In this case, their true posteriors will be in fact be similar, so it seems counter-intuitive to force their approximations to be different.

The numerical results seem promising, but I think they could be further improved and made more convincing.
- For the density estimation experiments, while there is an improvement in terms of NLL thanks to the new regularizer, it is not clear which is the additional computational burden. How much longer does it takes to train the model when computing all the regularization terms in the experiments with batch size 100? 
- I am not completely convinced by the claims on the ability of the regularizer to improve the learned representations. K-means implicitly assumes that the data manifold is Euclidean. However, as shown for example by [Arvanitidis et al. Latent space oddity: on the curvature of deep generative models, ICLR 2018] and other authors, the latent manifold of VAEs is not Euclidean, and curved riemannian manifolds should be used when computing distances and performing clustering. Applying k-means in the high dimensional latent spaces of ResNet VAE and VLAE does not seem therefore a good idea.
One possible reason why your MAE model may perform better in the unsupervised clustering of table 2 is that the terms added to the elbo by the regularizer may force the space to be more Euclidean (e.g. the squared difference term in the Gaussian KL) and therefore more suitable for k-means. 
- The semi-supervised classification experiment is definitely better to assess the representation learning capabilities, but KNN suffers with the same issues with the Euclidean distance as in the k-means experiments, and the linear classifier may not be flexible enough for non-euclidean and non-linear manifolds. Have you tried any other non-linear classifiers?
- Comparisons with other methods that aim at making the model learn better representation (such as the kl-annealing of the beta-vae) would be useful.
- The lack of improvements on the natural image task is a bit concerning for the generalizability of the results.

Typos and minor comments:
- devergence -> divergence in introduction
- assistant -> assistance in 2.3
- the items (1) and (2) in 3.1 are not very clear
- set -> sets in 3.2
- achieving -> achieve below theorem 1
- cluatering -> clustering in table 2","The sentiment of the review is generally positive, as indicated by phrases like 'interesting paper,' 'important issues,' and 'reads well and is well motivated.' However, the reviewer also raises several critical points and suggestions for improvement, which slightly temper the overall positivity. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer uses polite and constructive language throughout, such as 'I think they could be further improved,' 'I am not completely convinced,' and 'Have you tried any other non-linear classifiers?' This indicates a high level of politeness, so the politeness score is 90.",60,90
"Summary: 
 
The paper provides the convergence analysis at linear rate of gradient descent to global minima for deep linear neural networks – the fully-connected neural networks with linear activation with l2 loss. The convergence only works under two necessary assumptions on initialization: “weight matrices at initialization are approximately balanced” and “the initial loss is smaller than the loss of any rank-deficient solution”. The result of this work is similar to that of Barlett et al. 2018, but the difference is that, in Barlett et al. 2018, they consider a subclass of linear neural networks (linear residual networks – a subclass of linear neural networks which the input, output and all hidden layers are the same dimensions). 
 
Comments: 
 
This paper focuses on theoretical aspect of Deep Learning. Yes, theoretical study of gradient-based optimization in deep learning is still open and needs to spread more. I have the following comments and questions to the author(s) and hope to discuss further during the rebuttal period: 
 
1) Most of the deep learning applications are well-known used the neural networks with non-linear activation (specifically ReLU). Could you please provide any successful applications that linear neural networks could achieve better performance over the “non-linear” one? Yes, more layers may lead to better performance since we have more parameters. However, it is still not clear that which one is better between “linear” and “non-linear” with the same size of networks. I am not sure if this linear neural networks could generalize well. 
 
2) For N=1, the problem should become linear regression with strongly convex loss, which means that there exists a unique W: y = W*x in order to minimize the loss. Hence, if W = W_N*....*W_1, the problem becomes non-convex w.r.t parameters W_N, ...., W_1 but all the minima could be global. Can you please provide some intuitions why the loss function could have saddle points? Also, is not easier to just solve the minimization problem on W?

3) Similar with l2 loss, it seems that the problem needs to be restricted on l2 loss. In understand that it could have in some applications. Do you try to think of different loss for example in binary classification problems? 
 
4) I wonder about the constant “c > 0” in the definition 2 and it would use it to determine the learning rate. Do you think that in order to satisfy the definition 2 for the most cases, constant c would be (arbitrarily) small or may be very close to 0? If so, the convergence rate may be affected in this case. 
 
5) The result of Theorem 2 is nice and seems new in term of probabilistic bound.  I did not see the similar result in the existing literature for neural networks. 
 
6)  It would be nice if the author(s) could provide some experiments to verify the theory. I am also curious to know what performance it could achieve for this kind of networks. 
 
I would love to discuss with the author(s) during the rebuttal period. ","The sentiment of the review is generally positive, as the reviewer acknowledges the importance of the theoretical study and appreciates the novelty of the results, particularly Theorem 2. However, the reviewer also raises several critical questions and concerns, which slightly temper the overall positive sentiment. Therefore, the sentiment score is 40. The politeness of the language is high, as the reviewer uses polite phrases such as 'Could you please,' 'I wonder,' and 'I would love to discuss,' indicating a respectful and constructive tone. Therefore, the politeness score is 90.",40,90
"Strengths:
- Good coverage of related work
- Clear presentation of the methods
- Evaluation using established SemEval datasets


Weaknesses:
1. It is not entirely clear what is the connection between fuzzy bag of words and DynaMax. In principle DynaMax can work with other methods too. This point should be elaborated a bit more.
2. It is claimed that the this paper shows that max-pooled word vectors are a special case of fuzzy bag of words. This is not correct. The paper shows how to ""convert"" one to the other. 
3. It is also claimed that point 2 above implies that max-pooled vectors should be compared with the fuzzy Jaccard index instead of cosine similarity. There is no proof or substantial justification to support this. 
4. Some relevant work that is missing:
- De Boom, C., Van Canneyt, S., Demeester, T., Dhoedt, B.: Representation learning for very
short texts using weighted word embedding aggregation. Pattern Recognition Letters 80,
150–156 (2016)
- Kenter, T., De Rijke, M.: Short text similarity with word embeddings. In: International on
Conference on Information and Knowledge Management. pp. 1411–1420. ACM (2015)","The review starts by acknowledging the strengths of the paper, such as good coverage of related work, clear presentation of methods, and evaluation using established datasets. This indicates a generally positive sentiment towards the paper's contributions. However, the weaknesses section points out several critical issues that need to be addressed, including unclear connections between concepts, incorrect claims, lack of proof or justification, and missing relevant work. The language used in the review is constructive and polite, offering specific recommendations for improvement without being harsh or dismissive.",20,80
"It is shown empirically that common algorithms used in supervised learning (SGD) yield networks for which such upper bound decreases as the number of hidden units increases. This might explain why in some cases overparametrized models have better generalization properties.

This paper tackles the important question of why in the context of supervised learning, overparametrized neural networks in practice generalize better. First, the concepts of \textit{capacity} and \textit{impact} of a hidden unit are introduced. Then, {\bf Theorem 1} provides an upper bound for the empirical Rademacher complexity of the class of 1-layer networks with hidden units of bounded \textit{capacity} and \textit{impact}. Next, {\bf Theorem 2} which is the main result, presents a new upper bound for the generalization error of 1-layer networks. An empirical comparison with existing generalization bounds is made and the presented bound is the only one that in practice decreases when the number of hidden units grows. Finally {\bf Theorem 3} is presented, which provides a lower bound for the Rademacher complexity of a class of neural networks, and such bound is compared with existing lower bounds.

## Strengths
- The paper is theoretically sound, the statement of the theorems
    are clear and the authors seem knowledgeable when bounding the
    generalization error via Rademacher complexity estimation.

- The paper is readable and the notation is consistent throughout.

- The experimental section is well described, provides enough empirical
    evidence for the claims made, and the plots are readable and well
    presented, although they are best viewed on a screen.

- The appendix provides proofs for the theoretical claims in the
    paper. However, I cannot certify that they are correct.

- The problem studied is not new, but to my knowledge the
    presented bounds are novel and the concepts of capacity and
    impact are new. Theorem 3 improves substantially over
    previous results.

- The ideas presented in the paper might be useful for other researchers
    that could build upon them, and attempt to extend and generalize
    the results to different network architectures.

- The authors acknowledge that there might be other reasons
    that could also explain the better generalization properties in the
    over-parameterized regime, and tone down their claims accordingly.

## Weaknesses
\begin{itemize}
- The abstract reads ""Our capacity bound correlates with the behavior
    of test error with increasing network sizes ..."", it should
    be pointed out that the actual bound increases with increasing
    network size (because of a sqrt(h/m) term), and that such claim
    holds only in practice.

- In page 8 (discussion following Theorem 3) the claim
    ""... all the previous capacity lower bounds for spectral
        norm bounded classes of neural networks (...) correspond to
        the Lipschitz constant of the network. Our lower bound strictly
    improves over this ..."", is not clear. Perhaps a more concise
    presentation of the argument is needed. In particular it is not clear
    how a lower bound for the Rademacher complexity of F_W translates into a
    lower bound for the rademacher complexity of l_\gamma F_W. This makes the claim of tightness of Theorem 1 not clear. Also this makes
    the initial claim about the tightness of Theorem 2 not clear.
","The sentiment of the review is generally positive, as it highlights several strengths of the paper, such as its theoretical soundness, readability, and the novelty of the presented bounds. The reviewer also acknowledges the potential usefulness of the ideas for other researchers. However, there are some criticisms in the weaknesses section, which are presented constructively. The politeness of the language is high, as the reviewer uses polite and respectful language throughout the review, even when pointing out weaknesses.",80,90
"The main theory points out two scenarios causing Adam-type optimizers to diverge, which extends Reddi et al's results. 

The theorem in this paper applies to all Adam-type algorithms, which combine momentum with adaptive learning rates and thus are more general as compared to the recent papers, such as Zhou et al's. The relationship between optimizers' effective step size, step size oscillation and convergence is well demonstrated and is interesting.

Remarks:
1. The main theorem and proof are based on the non-convex settings while the examples to demonstrate the convergence condition are simple convex functions.

2. The message delivered by MNIST experiment is limited, is not clear and is not very relevant to the main part of the paper. It would be better to compare these algorithms in larger deep learning tasks.

Typo:
Page 5, section 3.1: Term A is a generalization of term alpha^2 g^2 (instead of just alpha^2) for SGD.","The review starts with a positive sentiment, highlighting the extension of Reddi et al's results and the general applicability of the theorem to all Adam-type algorithms. The reviewer finds the relationship between optimizers' effective step size, step size oscillation, and convergence to be well demonstrated and interesting. However, the review also points out specific areas for improvement, such as the use of non-convex settings for the main theorem and proof, and the limited relevance of the MNIST experiment. The language used is polite and constructive, offering suggestions for improvement without being harsh or dismissive.",60,80
"Summary:
This paper studies the properties of applying gradient flow and gradient descent to deep linear networks on linearly separable data. For strictly decreasing loss like the logistic loss, this paper shows 1) the loss goes to 0, 2) for every layer the normalized weight matrix converges to a rank-1 matrix 3) these rank-1 matrices are aligned. For the logistic loss, this paper further shows the linear function is the maximum margin solution.

Comments:
This paper discovers some interesting properties of deep linear networks, namely asymptotic rank-1, and the adjacent matrix alignment effect. These discoveries are very interesting and may be useful to guide future findings for deep non-linear networks. The analysis relies on many previous results in Du et al. 2018, Arora et al. 2018 and Soudry et al. 2017  authors did a good job in combing them and developed some techniques to give very interesting results. 
There are two weaknesses. First, there is no convergence rate. Second, the step size assumption (Assumption 5) is unnatural. If the step size is set proportional to 1/t or 1/t^2  does this setup satisfies this assumption? 

Overall I think there are some interesting findings for deep linear networks and some new analysis presented, so I think this paper is above the bar.
However, I don't think this is a strong theory people due to the two weakness I mentioned.","The sentiment of the review is generally positive, as the reviewer acknowledges the interesting findings and new analysis presented in the paper. However, the sentiment is tempered by the mention of two weaknesses, leading to a sentiment score of 50. The language used is polite and constructive, with phrases like 'interesting properties,' 'good job,' and 'interesting findings,' leading to a politeness score of 80.",50,80
"This paper presents an approach for hierarchical RL based on an ensemble of low-level controllers.
From what I can tell, you train K randomly initialized models to maximize displacement (optionally with a periodic implementation).
This ensemble of low-level models is then presented to a high-level controller, that can use them for actions.
When you do this, the resultant algorithm performs well on a selection of deep RL tasks.

There are several things to like about this paper:
- Hierarchical RL is an important area of research, and this algorithm appears to make progress beyond the state of the art.

- The ideas of using ensemble of low-level policies is intuitive and appealing.

- The authors provide a reasonable explanation of their ""periodicity"" ideas, together with evidence that it can be beneficial, but is not always essential to the algorithm.

- Overall the writing is good... but I did find the main statement of the algorithm confusing! I think this deserves a proper appendix with everything spelled out.


There are several places this paper could be improved:
- First, the statement of the *main* algorithm needs to be brought together so that people can follow it clearly. I understand one of the main reasons this is complicated is because the authors have tried to make this ""general"" or to be used with DQN/PPO/A3C... but if you present a clear implementation for *one* of them (PPO?) then I think this will be a huge improvement.

- Something *feels* a little hacky about this... why are there only two timescales? Is this a general procedure that we should always expect to work? *why* are we doing this... and what can its downsides be? The ablation studies are good, but I think a little more thought/discussion on how this fits in with a bigger picture of RL/control would be good.

Overall, I hope that I understood the main idea correctly... and if so, I generally like it.
I think it will be possible to make this much clearer even with some simple amendments.","The sentiment of the review is generally positive, as the reviewer highlights several strengths of the paper, such as its contribution to hierarchical RL, the intuitive and appealing idea of using an ensemble of low-level policies, and the reasonable explanation of periodicity. However, the reviewer also points out areas for improvement, such as the need for a clearer statement of the main algorithm and a more thorough discussion of the approach's generality and potential downsides. The politeness of the language is high, as the reviewer uses constructive and respectful language throughout the review, offering specific suggestions for improvement without being dismissive or harsh.",70,90
"This paper presents an approach to creating word representations that operate at both the sub-word level and generalise across languages. The paper presents soft decoupled encoding as a method to learn word representations from weighted bags of character-n grams, a language specific transformation layer, and a ""latent semantic embedding"" layer. The experiments are conducted over low-resource languages from the multilingual TED corpus. The experiments show consistent improvements compared to existing approaches to training translation models with sub-word representations. The ablation studies in Section 4.4 are informative about the relative importance of different parts of the proposed model.

Can you comment on how your model is related to the character-level CNN of Lee et al. (TACL 2017)?

In the experiments, do you co-train the LRLs with the HRLs? This wasn't completely clear to me from the paper. In Section 4.2 you use phrases like ""concatenated bilingual data"" but I couldn't find an explicit statement that you were co-training on both language pairs.

What does it mean for the latent embedding to have a size of 10,000? Does that mean that W_s is a 10,000 x D matrix?

Is Eq (4) actually a residual connection, as per He et al. (CVPR 2016)? It looks more like a skip connection to me.

Why do you not present results for all languages in Section 4.6?

What is the total number of parameters in the SDE section of the encoder? The paper states that you encode 1--5 character n-grams, and presumably the larger the value of N, the sparser the data, and the larger the number of parameters that you need to estimate.

For which other tasks do you think this model would be useful?","The sentiment of the review is generally positive, as it acknowledges the consistent improvements shown by the proposed method and finds the ablation studies informative. However, the review also contains several critical questions and requests for clarification, which slightly temper the overall positive sentiment. Therefore, the sentiment score is 60. The politeness of the language used is high, as the reviewer uses polite phrases such as 'Can you comment,' 'This wasn't completely clear to me,' and 'Why do you not present,' which indicate a respectful and constructive tone. Therefore, the politeness score is 80.",60,80
"This work presents an extension of the MAML framework for ""learning to learn."" This extension changes the space in which ""inner-loop"" gradient steps are taken to adapt the model to a new task, and also introduces stochasticity. The authors validate their proposed method with regression experiments in a toy setting and few-shot classification experiments on mini- and tiered-Imagenet. The latter are well known and competitive benchmarks in few-shot learning.

The primary innovations that distinguish this work from previous gradient-based approaches to meta-learning (namely MAML) are that (i) the initial set of parameters is data-dependent and drawn from a generative distribution; and (ii) the adaptation of model parameters proceeds in a lower-dimensional latent space rather than in the higher-dimensional parameter space. Specifically, model parameters are generated from a distribution parameterized by an adapted latent code at each adaptation step. I find both of these innovations novel.

The experimental results, in which LEO outperforms the state of the art on two benchmarks derived from ImageNet by ""comfortable margins,"" and the ablation study demonstrate convincingly that these innovations are also significant. I also found the curvature analysis and embedding visualization illuminating of the model's function. My one suggestion would be to test the model on realistic data from beyond the image domain, perhaps on something sequential like language (consider the few-shot PTB setting from Vinyals et al. (2016)). I'm aware anecdotally that MAML struggles with adapting RNNs and I wonder if LEO overcomes that weakness.

The paper is clearly written and I had little difficulty in following the algorithmic details, although I'm sure it helped to be familiar with the convoluted meta-learning and inner-/outer- loop frameworks. I recommend it for publication.

Pros:
- Natural, novel extension to gradient-based meta-learning
- state of the art results on two competitive few-shot benchmarks
- good analysis
- clear writing

Cons:
- realistic, high-dim data is only from the image domain

Minor questions for the authors:
- Relation Networks are computationally intensive, although in few-shot learning the sets encoded are fairly small. Can you discuss the computational cost and training time of the full framework?
- What happens empirically when you generate parameters for more than just the output layer in, eg, your convolutional networks?
- What happens if you try to learn networks from scratch through the meta-learning process rather than pre-training and fine-tuning them? Some of the methods you compare against do so, to my understanding.","The review is highly positive, highlighting the novelty and significance of the innovations presented in the paper. The reviewer appreciates the clear writing and the strong experimental results, and recommends the paper for publication. The sentiment score is therefore very high. The language used is polite and constructive, with suggestions and questions framed in a respectful manner, indicating a high politeness score.",90,95
"This work proposes to use semantic knowledge about the relationships and functionality of different objects, to help in navigation tasks, in both familiar and unfamiliar situations. The paper is very well written and it is clear what the authors did. The approach seems sound, and while it combines two existing approaches (actor-critic reinforcement learning for navigation, and belief propagation using graph convolution networks) is sufficiently novel to be of interest to at least some members of the community. The experimental evaluation is good, and the proposed method outperforms Mnih 2016 by a significant margin, especially in the more interesting settings. A good ablation study is provided. 

My main concern is that there seems to be a larger pool of work in semantic navigation than what the evaluation includes. Anderson 2018, Zhu 2017 and Gupta 2017 seem relevant. While none of these use knowledge graphs, some of these show they outperform Mnih 2016 so would be stronger baselines. 

I am also curious whether the proposed work generalizes across scene type categories (e.g. if it learns on kitchens but it tested on living rooms). This would be an experiment in the spirit of unknown object/scene but even more challenging. ","The review starts with a positive sentiment, praising the clarity, soundness, and novelty of the paper. The reviewer also appreciates the experimental evaluation and ablation study. However, the reviewer raises a concern about the evaluation pool and suggests additional relevant works that should be included. The language used is polite and constructive, offering specific recommendations without being dismissive or harsh.",80,90
"This manuscript contributes a new online gradient descent algorithm with adaptation to local curvature, in the style of the Adam optimizer, ie with a diagonal reweighting of the gradient that serves as an adaptive step size. First the authors identify a limitation of Adam: the adaptive step size decreases with the gradient magnitude. The paper is well written.

The strengths of the paper are a interesting theoretical analysis of convergence difficulties in ADAM, a proposal for an improvement, and nice empirical results that shows good benefits. In my eyes, the limitations of the paper are that the example studied is a bit contrived and as a results, I am not sure how general the improvements.

# Specific comments and suggestions

Under the ambitious term ""theorem"", the results of theorem 2 and 3 limited to the example of failure given in eq 6. I would have been more humble, and called such analyses ""lemma"". Similarly, theorem 4 is an extension of this example to stochastic online settings. More generally, I am worried that the theoretical results and the intuitions backing the improvements are built only on one pathological example. Are there arguments to claim that this example is a prototype for a more general behavior?


Ali Rahimi presented a very simple example of poor perform of the Adam optimizer in his test-of-time award speech at NIPS this year (https://www.youtube.com/watch?v=Qi1Yry33TQE): a very ill-conditioned factorized linear model (product of two matrices that correspond to two different layers) with a square loss. It seems like an excellent test for any optimizer that tries to be robust to ill-conditioning (as with Adam), though I suspect that the problem solved here is a different one than the problem raised by Rahimi's example.


With regards to the solution proposed, temporal decorrelation, I wonder how it interacts with mini-batch side. With only a light understanding of the problem, it seems to me that large mini-batches will decrease the variance of the gradient estimates and hence increase the correlation of successive samples, breaking the assumptions of the method.


Using a shared scalar across the multiple dimensions implies that the direction of the step is now the same as that of the gradient. This is a strong departure compared to ADAM. It would be interesting to illustrate the two behaviors to optimize an ill-conditioned quadratic function, for which the gradient direction is not a very good choice.


The performance gain compared to ADAM seems consistent. It would have been interesting to see Nadam in the comparisons.



I would like to congratulate the authors for sharing code.

There is a typo on the y label of figure 4 right.
","The sentiment of the review is generally positive, as indicated by phrases like 'The paper is well written,' 'interesting theoretical analysis,' 'nice empirical results,' and 'I would like to congratulate the authors for sharing code.' However, there are some critical points and suggestions for improvement, which slightly temper the overall positivity. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer uses polite phrases such as 'I would have been more humble,' 'I wonder,' and 'It would have been interesting,' and ends with a congratulatory note. Thus, the politeness score is 80.",60,80
"This paper presents an extension of Wasserstein autoencoder (WAE) by modifying the regularization term in learning objective of variational autoencoder. This term measures the divergence between the distribution of the encoded training samples and the samplable prior distribution. The modification is based on the sliced-Wasserstein distance where the distance between two distributions is measured through slicing or projecting the high-dimensional distributions into one-dimensional marginal distributions. As a result, a closed-form solution to the integral in Eq. (9) is obtained via a numerical method. The adversarial learning in WAE, designed to fulfill the calculation of high-dimensional distance, can be avoided. In general, this is an interesting work by introducing new idea of sliced-Wasserstein distance.

Remarks:
1. A theoretical paper which addresses how and why the sliced-Wasserstein distance between p_z and q_z is reasonable to build a new variant of variational auto-encoder.
2.  Reformulating the Wasserstein distance into Monge primal formulation with the assumption based on the property of diffeomorphic mapping.
3. As a result, the implementation based on the unstable adversarial training or the maximum mean discrepancy (MMD) training can be avoided. Computational attractiveness is assured. MMD needs the choice of kernel function which is basically a data-dependent design parameter.
4. Provide an empirical numerical solution which is compatible with SGD optimization.
5. The key idea of this paper is shown in Eq. (14). Learning objective is expressed in a deterministic way. However, the style of objective in Eq. (14) involves the stochastic learning.
6. This paper is not actually doubly-blind reviewed. Authors have exposed their identities in arXiv.
","The sentiment of the review is generally positive, as indicated by phrases like 'interesting work' and the detailed, constructive feedback provided. The reviewer acknowledges the novelty and computational attractiveness of the proposed method. However, there are some critical remarks, particularly regarding the exposure of the authors' identities, which slightly temper the overall positivity. The politeness of the language is high, as the reviewer uses formal and respectful language throughout the review, providing specific and constructive feedback without any derogatory or dismissive comments.",70,90
"The authors analyze the local SGD algorithm, where $K$ parallel chains of SGD are run, and the iterates are occasionally synchronized across machines by averaging. For sufficiently short intervals between synchronization, the algorithm achieves the same convergence rate in terms of the number of gradient evaluations as parallel minibatch SGD, but with the advantage that communication can be significantly reduced.

The algorithm is simple and practical, and the analysis is concise and seems like it could be applicable more generally to other parallel SGD variants.

I am curious about what happens for the analysis of the algorithm when $H$ becomes large. As the authors point out, when $H=T$, this is one-shot averaging which is known to converge. The authors mention not working too hard to optimize the bounds for extreme values of $H$, which is fine, but I wonder if this is possible using their analysis technique, or whether new tools would be necessary.","The review starts with a positive assessment of the algorithm's simplicity, practicality, and the general applicability of the analysis. The reviewer also expresses curiosity about the behavior of the algorithm under certain conditions, which indicates a constructive and inquisitive approach rather than a critical one. The language used is polite and professional, with phrases like 'I am curious' and 'I wonder if this is possible' showing a respectful tone towards the authors' work.",80,90
"The paper extends an existing topic model - DocNADE - by replacing the feedforward part of the network which combines the textual context with an LSTM sequence model. Hence this paper fits in a long tradition of work which aims to extend the bad-of-words model from the original LDA paper with some sequence information.

The authors do a commendable job in thoroughly evaluating the proposed extension, using a number of evaluations based on perplexity, topic coherence, and text retrieval and categorization.

My main problem with the paper as it stands is that it a) arguably oversells the contribution, and b) is unclear when explaining certain crucial aspects of the model.

It would also help to have a clearer statement of whether the contribution here is on the document modeling side, or the language modeling side. Motivation is provided from both angles, but the evaluation focuses largely on the topic modeling (which is fine, just need to say it).

More specific comments:
--
The abstract should mention that the DocNADE model already exists, and that the contribution of the current work is to extend that existing model in a particular way. For those readers unfamiliar with DocNADE, this will help situate the work with regard to the existing literature.

Using existing word embeddings as a 'prior' for the LSTM word embeddings is a completely standard alternative now to learning those embeddings from scratch. I'm not sure that can count as a second, major contribution of the paper. (I'm also not sure that either extension to DocNADE warrants a new name, but I'll leave that to the authors' judgement.)

I'm confused by one aspect of the DocNADE model: ""the topic assigned ...equally depends all the other words appearing in the same document"". But the model is generative, no? And eqn 1 suggests that each word is generated conditioned on the *previous* words in the document, or did I miss something? 

Related to this point, DocNADE transforms its BoWs into a sequence. But what's the order? Is it just the order of the words in the document? In which case it's very similar to the LSTM extension, except the LSTM keeps the order in the history, whereas the bag-of-words model doesn't.

Relation to generative models: LDA is a generative model with a generative story. It's not completely obvious to me what the generative story is in the new model. Talking about ""distributed compositional priors"" doesn't help, since I'm assuming these aren't priors in a Bayesian modeling sense? (It's also not clear in what sense these ""priors"" are compositional, but that's a separate question.)

Equation 2: what's the motivation for mixing the LSTM history with the bag-of-words (esp. if the history is from the same bag of words in each case). Why not just use the LSTM?

It would be useful to state in the main body of the text what the value of lambda ends up being. In 3.1 there's a suggestion this might be 0.01, but that effectively ignores the LSTM?

Similar question: how can the DocNADE model provide a *global* context if the model is generative?

Perplexity is a reasonable thing to measure, but presumably the auto-regressive nature of the LSTM means that it's more-or-less guaranteed to do better than a bag-of-words model? I wonder if it's worth acknowledging this fact?

I don't understand why lambda has to be zero ""to compute the exact log-likelihood"".

The first line of the conclusion doesn't say much: it's pretty obvious that the ordering of the words is going to help better estimate the probability of a word in a given context; 50 years of language modeling research has already taught us that.

Minor presentational comments:
--
Some of the hyphenation looks odd, eg in the title. Are you using the standard LaTeX hyphenation settings?

Strictly speaking, I'm not sure that 'bear' in the example is a proper noun.

""orderless sets of words"": bags, not sets, since the counts matter, no?

The tables are too small, with a lot of numbers in them. One option is to move some of the details to the Appendix. Either way there needs to be more summary in the main body explaining what the numbers tell us.
","The sentiment of the review is generally positive, as the reviewer acknowledges the commendable job done by the authors in evaluating the proposed extension and provides constructive feedback. However, there are some critical points raised, such as overselling the contribution and unclear explanations of crucial aspects, which slightly lower the sentiment score. The politeness of the language is high, as the reviewer uses polite and respectful language throughout the review, even when pointing out issues and making suggestions for improvement.",50,90
"Several recent works propose to discover bugs in code by creating dataset of presumably correct code and then to augment the data by introducing a bug and creating a classifier that would discriminate between the buggy and the correct version. Then, this classifier would be used to predict at each location in a program if a bug is present.

This paper hypothetizes that when running on buggy code (to discover the bug) would lead to such classifier misbehave and report spurious bugs at many other locations besides the correct one and would fail at precisely localizing the bug. Then, they propose a solution that essentially create a different classifier that is trained to localize the bug.

Unfortunatley this leads to a number of weaknesses:
 - The implementation and evaluation are only on a quite syntactic system with low precision and that needs to sift through a huge amount of weak and irrelevant signals to make predictions.
 - The gap here is huge: the proposed system is only based on program syntax and gets 62.3% accuracy, but state-of-the-art has 85.5% (there is actually another recent technique [1] also with accuracy in the >80% range)
 - It is not clear that the entire discussed problem is orthogonal to the selection of such weak baselines to build the improvements on.
 - Trade-offs are not clear: is the proposed architecture slower to train and query than the baselines?

Strengths of the paper are:
 - Well-written and easy to follow and understand.
 - Evaluation on several datasets.
 - Interesting architecture for bug-localization if the idea really works.

[1] Michael Pradel, Koushik Sen. DeepBugs: a learning approach to name-based bug detection","The sentiment of the review is mixed but leans towards negative. The reviewer acknowledges some strengths of the paper, such as it being well-written and having an interesting architecture, but the majority of the review focuses on significant weaknesses, including low precision, a large gap in accuracy compared to state-of-the-art methods, and unclear trade-offs. Therefore, the sentiment score is -40. The politeness of the language is relatively neutral. The reviewer uses formal and professional language without being overly harsh or rude, but also does not go out of their way to be particularly polite. Therefore, the politeness score is 0.",-40,0
"This manuscript describes a deep convolutional neural network for
assigning proteins to subcellular compartments on the basis of
microscopy images.

Positive points:

- This is an important, well-studied problem.

- The results appear to improve significantly on the state of the art.

- The experimental comparison is quite extensive, including
  reimplementations of four, competing state-of-the-art methods, and
  lots of details about how the comparisons were carried out.

- The manuscript also includes a human-computer competition, which the
  computer soundly wins.

- The manuscript is written very clearly.

Concerns:

There is not much here in the way of new machine learning methods.
The authors describe a particular neural network architecture
(""GapNet-PL"") and show empirical evidence that it performs well on a
particular dataset.  No claims are made about the generalizability of
the particular model architecture used here to other datasets or other
tasks.

A significant concern is one that is common to much of the deep
learning literature these days, namely, that the manuscript fails to
separate model development from model validation. We are told only
about the final model that the authors propose here, with no
discussion of how the model was arrived at.  The concern here is that,
in all likelihood, the authors had to try various model topologies,
training strategies, etc., before settling on this particular setup.
If all of this was done on the same train/validation/test split, then
there is a risk of overfitting.

The dataset used here is not new; it was the basis for a competition
carried out previously.  It is therefore somewhat strange that the
authors chose to report only the results from their reimplementations
of competing methods.  There is a risk that the authors'
reimplementations involve some suboptimal choices, relative to the
methods used by the originators of those methods.

Another concern is the potential circularity of the labels.  At one
point, we are told that ""Most importantly, these labels have not been
derived from the given microscopy images, but from other
biotechnologies such as microarrays or from literature.""  However,
earlier we are told that the labels come from ""a large battery of
biotechnologies and approaches, such as microarrays, confocal
microscopy, knowledge from literature, bioinformatics predictions and
additional experimental evidence, such as western blots, or small
interfering RNA knockdowns.""  The concern is that, to the extent that
the labels are due to bioinformatics predictions, then we may simply
be learning to re-create some other image processing tool.

The manuscript contains a fair amount of biology jargon (western
blots, small interfering RNA knockdowns, antibodies, Hoechst staining,
etc.) that will not be understandable to a typical ICLR reader.

At the end, I think it would be instructive to show some examples
where the human expert and the network disagreed.

Minor:

p. 2: ""automatic detection of malaria"" -- from images of what?

p. 2: Put a semicolon before ""however"" and a comma after.

p. 2: Change ""Linear Discriminant"" to ""linear discriminant."" Also, remove
the abbreviations (SVM and LDA), since they are never used again in
this manuscript.

p. 5: Delete comma in ""assumption, that.""

p. 8: ""nearly perfect"" -> ""nearly perfectly""

The confusion matrices in Figure 5 should not be row normalized --
just report raw counts.  Also, it would be better to order the classes
so that confusable ones are nearby in the list.
","The sentiment of the review is generally positive, as indicated by the numerous positive points listed at the beginning, such as the importance of the problem, significant improvement on the state of the art, extensive experimental comparison, and clear writing. However, the review also contains several concerns and criticisms, particularly regarding the lack of new machine learning methods, potential overfitting, and issues with dataset and label circularity. These concerns temper the overall positive sentiment. The language used in the review is polite and constructive, with suggestions for improvement and minor corrections provided in a respectful manner.",50,80
"Summary:

This paper proposes learning reward functions via inverse reinforcement learning (IRL) for vision-based instruction following tasks like ""go to the cup"". The agent receives the language instruction (generated via grammar templates) and a set of four images (corresponding to four cardinal directions) from virtual cameras mounted on the agent as input at every time step and its aim is either to 1. navigate to the goal location (navigation task) or 2. move an object from one place to another (pick task). 

The really interesting part in this paper is learning reward functions such that they generalize across different tasks and environments (e.g. indoor home layouts). This differentiates it from the standard IRL setting where reward functions are learnt and then policies optimized on this reward function on the *same* environment. 

In order to generalize across tasks and environments a slight modification to the max-ent IRL gradient equations are made: 1. Similar to meta-learning the gradient is taken with respect to multiple tasks (in a sampling-based manner) and 2. Making the reward function a function of not just states and actions but also language context. The overall algorithm (Algorithm 1) is simple and the critical step of computing an optimal policy to compute the IRL gradient is done by assuming that one has access to full state and dynamics and essentially running a planner on the MDP. This assumption is not unreasonable since in a simulator one has access to the full dynamics and can hence one can compute the optimal trajectories by planning. 

Experiments are presented on the SUNCG dataset of indoor environments. Two baselines are presented: One using behavior cloning (BC) and an oracle baseline which simply regresses to the ground truth reward function which is expected to be an upper bound of performance. Then DQN is used (with and without reward shaping) using the learnt reward functions to learn policies which are shown to have better performance on different tasks. 

Comments and Questions:

- The paper is generally well-written and easy to understand. Thanks!

- The idea of using IRL to learn generalizable reward functions to learn policies so as to aid transfer between environments in such vision-language navigation tasks is interesting and clearly shows benefits to behavior cloning.

- One of my main concerns (and an interesting question that this paper naturally raises) is how does this approach compare to imitation learning (not vanilla behavior cloning which is straight-up supervised learning and has been theoretically and empirically shown to have worse performance due to distribution shifts. See Ross and Bagnell, 2011, Ross, Gordon, Bagnell 2012 (DAgger, Ross and Bagnell 2014 (AggreVate), Chang et al., 2015 (LOLS), etc). If the same budget of 10 demonstrations per environment is used via DAgger (where say each iteration of DAgger gets say 2 or 3 demonstrations until the budget is exhausted) how does it compare? Note online version of DAgger has already been used in similar settings in ""Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments"" by Anderson et al, CVPR 2018. The main difference from their setting is that this paper considers higher level tasks instead of taking as input low-level turn-by-turn language inputs. 

- The following papers are relevant and should be cited and discussed:
""Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments"" by Anderson et al, CVPR 2018.

""Embodied Question Answering"", Das et al, CVPR 2018.

Update:
------------
After looking at other reviews and author rebuttals to all reviews I am raising my grade. 
","The sentiment of the review is generally positive. The reviewer appreciates the interesting aspects of the paper, such as the generalization of reward functions across different tasks and environments, and acknowledges the benefits over behavior cloning. The reviewer also thanks the authors for the clarity of the writing. However, there are some critical questions and suggestions for improvement, which are presented in a constructive manner. The politeness of the language is high, as the reviewer uses polite phrases like 'Thanks!' and provides constructive feedback without any negative or rude language.",80,90
"Paper summary:

The paper presents two distinct contributions in text-to-speech systems:
a) It describes a method for distilling a Gaussian WaveNet into a Gaussian Inverse Autoregressive Flow that uses an analytically computed KL between their conditionals.
b) It presents a text-to-speech system that is trained end-to-end from text to waveforms.

Technical quality:

The distillation method presented in the paper is technically correct. The evaluation is based on Mean Opinion Score and seems to follow good practices.

The paper makes three claims:
a) A WaveNet with Gaussian conditionals can model speech waveforms equally well as WaveNets with other types of conditionals.
b) Analytically computing KL divergence stabilizes distillation.
c) A text-to-speech system trained end-to-end from text to waveforms outperforms one that has separately trained text-to-spectrogram and spectrogram-to-waveform subsystems.

Claims (a) and (c) are clearly demonstrated in the experiments. However, there is nothing in the paper that substantiates claim (b). I think the paper would be strengthened if the performance of sample-based KL distillation was added into Table 2, and if learning curves were reported that evaluate the amount of stabilization that an analytical KL may offer vs a sample-based KL.

Further points about the experiments:
- It wasn't clear to me whether distillation happens at the same time as the autoregressive WaveNet is trained on data, or after it has been fully trained. I think the paper should make this clear.
- The paper says that distillation makes generation three orders of magnitude faster. I think it would be good if actual generation times (e.g. in seconds) were reported.

Clarity:

The paper is generally well-written. Sections 1 and 2 in particular are excellent.

However, section 3 contains several notational errors and technical inaccuracies, that makes it rather confusing to read. In particular:
- q(x_t | z_{<=t}) is used in several places to mean the Gaussian conditional q(x_t | z_{<t}) (e.g. in Eqs (6) and (7), and elsewhere). This is confusing, as q(x_t | z_{<=t}) is actually a delta distribution.
- q(x | z) is used in several places to mean q(x) (e.g. in Eq. (7), in Alg. 1 and elsewhere). This is confusing, as q(x | z) is also a delta distribution.
I believe that section 3, especially subsections 3.2 and 3.3.1, should be reworked to be made clearer, and the notation should be carefully revised.

I don't think the paper needs to span 9 pages. Section 3 is rather wordy, and should be compressed to the important points.

Originality:

Distilling a Gaussian autoregressive model to another Gaussian autoregressive model by matching their Gaussian conditionals with an analytical KL is rather straightforward, and, methodologically speaking, I wouldn't consider it an original contribution on its own. However, I think its application and demonstration in text-to-speech constitutes an original contribution.

Significance:

The paper contains a substantial amount of significant work that I think is important to be communicated to the ICLR community, especially the text-to-speech community.

Review summary:

Pros:
+ Substantial amount of good work.
+ Significant improvement in text-to-speech end-to-end software.
+ Generally well-written (with the exception of section 3 which needs work).

Cons:
- Some more experiments would be good to substantiate the claim that analytical KL is better.
- Notational errors and confusion in section 3.
- Too wordy, no need for 9 pages.

Nitpicks:
- As I said above, I wouldn't consider distillation of models with Gaussian conditionals using analytical KLs methodologically novel, so I think the phrase ""novel regularized KL divergence"" should be moderated.
- Eq. (1) should contain theta on the left hand side too.
- Page 3: ""at Appendix B"" --> ""in Appendix B"".
- Page 4: In flows we don't just ""suppose z has the same dimension as x""; rather, it's a necessary condition that must hold.
- Footnote 5: It's unclear to me what it means to ""make the loss less sensitive"".
- References: Real NVP, Fourier, Bayes, PixelCNN, WaveNet, VoiceLoop should be properly capitalized.","The review starts with a positive sentiment, acknowledging the substantial contributions and technical correctness of the paper. However, it also points out several areas for improvement, such as the need for additional experiments to substantiate certain claims, notational errors, and verbosity in section 3. The language used is constructive and polite, offering specific suggestions for improvement without being overly critical or harsh.",60,80
"The goal of this paper is to use deep generative models for missing data imputation. This paper proposes learning a latent variable deep generative model over every randomly sampled subset of observed features. First, a masking variable is sampled from a chosen prior distribution. The mask determines which features are observed. Then, the likelihood of the observed features is maximized via a lower bound. Inference in this latent variable model is achieved through the use of an inference network which conditions on the set of ""missing"" (to the generative model) features.

Novelty:
Generative models have a long history of being used to impute missing data. e.g. http://www.cs.toronto.edu/~fritz/absps/ranzato_cvpr2011.pdf, https://arxiv.org/pdf/1610.04167.pdf,
https://arxiv.org/pdf/1808.01684.pdf, https://arxiv.org/pdf/1401.4082.pdf [Appendix F]
It is a little difficult to guage what the novelty of this work is.

Clarity
This is a poorly written paper. Distilling the proposed methodology down to one paragraph was challenging since the text meanders through several concepts whose relevance to the overarching goal is questionable. For example, it is not clear what Section 3.2 adds to the discussion. The text describes a heuristic used in learning GSNNs only to say that the loss function used by GSNNs is not used in the experimental section for this paper -- this renders most of 4.3.2 redundant. There are issues like awkward grammar, sloppy notation, and spelling mistakes (please run spell check!) throughout the manuscript. Please use a different notation when referring to the variational distributions (do not re-use ""p"").

Experimental Results
The model is evaluated against MICE and MissForest on UCI datasets. RMSE and accuracy of classification (from imputed data is compared). The complexity of data considered is simplistic (and may not make use of the expressivity of the deep generative model). Why not run these experiments on datasets like MNIST and Omniglot?
Beyond that:
(a) was there any comparison to how classification performance behaves when using another neural network based imputation baseline (e.g. the method in Yoon et. al)?
(b) the *kind* of missingness considered here appears to be MCAR (the easiest kind to tackle) -- did you consider experiments with other kinds of missingess?

The qualitative results presented in this work are interesting. The method does appear to produce more diverse in-paintings than the method from Yeh et. al (though the examples considered are not aligned).

Table 5 claims negative log-likelihood numbers on MNIST as low as 61 and 41 (I assume nats...). These numbers do not make sense. How were they computed?


Priors on b:
What kind of priors on b did you experiment with? ","The sentiment of the review is generally negative, as it highlights several issues with the paper, including lack of novelty, poor clarity, and questionable experimental results. The reviewer points out that the paper is poorly written, with awkward grammar, sloppy notation, and spelling mistakes. Additionally, the reviewer questions the novelty of the work and the complexity of the data used in the experiments. However, the reviewer does acknowledge some interesting qualitative results. The politeness of the language is relatively neutral to slightly negative. While the reviewer is critical, they do not use overly harsh or rude language, but they do make direct and blunt statements about the paper's shortcomings.",-60,-20
"I really enjoyed this paper. It takes an idea which at first glance seems to be obviously bad (if you want permutation invariance, build a model that considers all permutations) and uses it to make the important point that the universal approximation results contained in Deep Sets [Zaheer et al. 2017] are not the last word on pooling. Janossy Pooling is intractable for most problems of interest (because it sums over all n! permutations of the input set) so the authors suggest 3 tractable alternatives: canonical orderings, k-ary dependencies and SGD / sampling-based approaches. Only the latter two are explored in detail, so I’ll focus on them:

K-ary dependencies
Functions that are restricted to k-ary dependencies in Janossy Pooling require summing over only |h|! / (|h| - k)! terms - that is they sum over the permutations of subsets of h of length k. In the experimental section, the authors show that this recovers some of the performances lost by using sum / mean pooling (as in Deep Sets), but this suggests the natural question: is it the fact that you’re explicitly modelling higher-order interactions that improves performance? Or is it that you’re doing Janossy pooling over the higher order interactions (i.e. summing over permutations of non-invariant functions)? 

These two effects could be separated by comparing to invariant models that allow higher order interactions. E.g. you could compare against Santoro et al. [2017] who explicitly model pairwise interactions (or similarly any of the graph convolutional models [Kipf and Welling 2016, Hamilton et al 2017, etc.] with a fully connected graph would do the same); similarly Hartford, et al. [2018] allow for k-wise interactions by extending Deep Sets to exchangeable tensors - the permutation invariant analog of k-ary Janossy Pooling. All of these approaches model k-wise interactions through sum-pooling over permutation invariant functions so this lets you address the question - is it the permutation invariance that’s the problem (necessitating k-ary Janossy pooling) or is it the lack of higher-order interaction terms? 

SGD approaches:
I think that the point that the sampling-based approaches are bias with respect to the Janossy sum is important to make and I liked the discussion around it, but I don’t follow the relevance of Proposition 2.2? I see that it gives conditions under which we can expect \pi-SGD to converge, but we aren’t provided with any guidance about how likely those conditions are to be satisfied? Furthermore - these conditions don’t seem to be specific to \pi-SGD - any SGD algorithm with “slightly biased” gradients that satisfy these conditions would converge. The regularization idea is interesting, but it isn’t evaluated so we’re left with theory that doesn’t provide guidance and isn’t evaluated.

Summary:
There are two ways to read this paper:
 1. Janossy pooling as a framework & proposed pooling approach implemented in one of the two ways discussed above.
 2. Janossy pooling as an intractable upper bound on what we might want from a pooling method (with approximations in the form of the LSTM approaches) and a demonstration that our current invariant pooling methods are insufficient.

I liked the paper based on reading (2). Janossy pooling clearly demonstrates limitations to sum / mean pooling which is widely used in practice which shows the need for better alternatives and it is on this basis that I’m arguing for it’s acceptance. My view is that the experimental section is too limited to support reading (1) which asserts that k-ary pooling or LSTM + sampling approaches are the right solution to this problem. 

[Zaheer et al. 2017] - Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan Salakhutdinov, and
Alexander Smola. Deep Sets
[Santoro et al. 2017] - Adam Santoro, David Raposo, David G Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, and Tim Lillicrap. A simple neural network module for relational reasoning.
[Kipf and Welling 2016] - Thomas N. Kipf and Max Welling. Semi-Supervised Classification with Graph Convolutional Net- works
[Hamilton et al 2017] - William L. Hamilton, Rex Ying, and Jure Leskovec. Inductive Representation Learning on Large Graphs
[Hartford, et al. 2018] - Jason S. Hartford, Devon R. Graham, Kevin Leyton-Brown, and Siamak Ravanbakhsh. Deep models of interactions across sets","The sentiment of the review is generally positive, as the reviewer expresses enjoyment of the paper and acknowledges its important contributions. However, there are also critical points and suggestions for improvement, particularly regarding the experimental section and the relevance of Proposition 2.2. Therefore, the sentiment score is not at the maximum positive level. The language used is polite and constructive, with the reviewer providing detailed feedback and suggestions without being rude or dismissive.",70,90
"This paper considers learning informative priors for convolutional neural network models based on fits to data sets from similar problem domains.  For trained networks on related datasets the authors use autoencoders to obtain an expressive prior on the filter weights, with independence assumed between different layers.  The resulting prior is generative and its density has no closed form expression, and a novel variational method for dealing with this is described.  Some empirical comparisons of the deep weight prior with alternative priors is considered, as well as a comparison of deep weight samples for initialization with alternative initialization schemes.  

This is an interesting paper.  It is mostly clearly written, but there is a lack of detail in Section 4 that makes it hard for me, at least, to understand exactly what was done there.  I think the originality level of the paper is high.  The issue of informative priors in these complex models seems wide open and the authors provide an interesting approach both conceptually and computationally.  I did wonder whether there was any link between the suggested priors and the idea of modelling the current and related data sets used in constructing the prior jointly, with data set specific parameters given an exchangeable prior?  This would be a standard hierarchical modelling approach.  Such an approach would not be computationally attractive, I just wondered if there is some conceptual link with the current method being an approximation of that approach in some sense.  In Section 4.1, it seems that for the trained networks on the source datasets, point estimates of the filter weights are treated as data for learning the variational autoencoder - is that correct?  Could you model dataset heterogeneity here as well?  Presumably the p_l(z) density is N(0,I)?  Details of the inference and reconstruction networks are sketchy.  In Section 4.2, you say that the number of filters is proportional to the scale parameter k and that you vary k.  What scale parameter do you mean?  


","The sentiment of the review is generally positive, as indicated by phrases like 'This is an interesting paper,' 'mostly clearly written,' and 'the originality level of the paper is high.' However, there are some criticisms regarding the lack of detail in Section 4 and specific questions about the methodology, which slightly temper the overall positive sentiment. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, such as 'I did wonder,' 'Could you model,' and 'Presumably.' The reviewer also provides specific questions and suggestions in a respectful manner, leading to a politeness score of 90.",60,90
"This paper studies the problem of *learning* online combinatorial algorithms via Reinforcement Learning. In particular, the paper studies three different well-studied problems, namely AdWords/Online Matching, Online Knapsack, and Secretary Problem. The common thread to all three problems is that they are special cases of Online Packing problem and that there exist optimal algorithms with theoretical guarantees. Moreover, all these problems have an algorithm based on the unifying Primal-Dual framework (*). The paper runs extensive experiments and shows that the learned RL policies resemble the algorithms created in theory by comparing many properties of the returned policy. Overall I think this paper tackles a very interesting question, is well-written and has extensive experiments. 

I will detail my technical comments and questions to authors. I would especially appreciate detailed answers to (1), (2), (3). (4) and (5) are more open-ended questions and/or beyond the scope of the current paper. It can be viewed as feedback for some future work.

(1) (*) The paper starts with the claim that one of the key insights to this work is the primal-dual framework. Yet this has not been exploited in the paper; at least I can't see where it is used! Can the authors give more details? For example, one way I could see this being used is as follows. In the AdWords problem, the evolution of the Dual variable is well understood (See [Devanur, Kleinberg, Jain '13]). One can experiment to see how the dual values change over the run of the algorithm for the learned policy and compare that with how it changes in the analysis. If they are similar, then that is yet another evidence that the two algorithms are similar.

(2) One key point to note is that all three algorithms include only the ""primal"" observations in the state. This strengthens this work since all these algorithms, in theory, are analyzed by realizing that the primal algorithm can be interpreted as an appropriate process on the evolution of the dual variables.  Thus it seems like the RL policy is actually learning the optimal way to set the dual variables in the online phase. Is this true? I guess the experiments above can indeed verify this. If this is true, it implies that the key message of this paper is that RL algorithms can be used to learn algorithms that can be analyzed via the primal-dual framework. Right now, the authors stop short of this by saying this work is inspired from it. It would be good to see this taken to completion.

(3) It seems like there has been some work on using RL to learn algorithms in combinatorial optimization (see [Dai et al., NIPS 2017]). Can the authors discuss both in the rebuttal and in the paper on how their work compares and differs from this work? 

(4) I wonder if the authors experimented with the i.i.d. arrival process for Knapsacks and/or Online Matching/Adwords. It is known that the theoretical algorithms for both these problems do much better than the pessimistic adversarial arrival order. It will be interesting to see if the RL policies also find this. On a related note, did the authors try this on b-matching with b=1? The problem tends to get easier as b is large and/or when bid/budget ratio is small in Adwords. However even when b=1, in theory, we can get 1-1/e [KVV '90].

(5) Finally, I am curious if the authors tested this on problems that are not packing but covering and/or mixed packing and covering problems. Online Set Cover is a candidate example. The other direction is also to test Online Minimization problems. Note that Online Min-cost matching is significantly harder than Online maximum weight matching. Moreover, Online Min-cost matching does not have a primal-dual analysis (to the best of my knowledge). The latter helps because if the RL policy fails to learn the best-known algorithm, then it is further evidence that it is indeed learning through the primal-dual framework.

Some minor comments:

Styling is not consistent throughout the paper. For example, there are places with the header followed by a colon sometimes, a period other times and sometimes no punctuation. Please make these consistent throughout the paper.

The fonts on the figures are very small to read. It would be easy on the reader if the sizes were made larger.


","The sentiment of the review is generally positive, as the reviewer appreciates the interesting question tackled by the paper, its well-written nature, and the extensive experiments conducted. This is evident from phrases like 'Overall I think this paper tackles a very interesting question, is well-written and has extensive experiments.' However, the reviewer also provides several critical technical comments and questions, which slightly temper the overall positivity. Therefore, the sentiment score is 70. The politeness of the language is very high, as the reviewer uses polite phrases such as 'I would especially appreciate detailed answers,' 'It would be good to see this taken to completion,' and 'I wonder if the authors experimented.' The reviewer also frames their comments as suggestions and questions rather than direct criticisms, which indicates a high level of politeness. Therefore, the politeness score is 90.",70,90
"DOM-Q-NET: GROUNDED RL ON STRUCTURED LANGUAGE 

This paper presents a somewhat novel graph-based Q-learning method for web navigation benchmark tasks. The authors show that multi-task learning helps in this case and their method is able to learn without BC as previous works have needed. While this work is interesting and to my knowledge somewhat novel. I concerns with one aspect of the evaluation. In some part it was stated that they show the highest success rate for testing on 100 episodes, if this is indeed the maximum success rate, it is unclear if these results are misleading or not. It is possible that there was a lucky seed in those 100 episodes leading to a higher max that is not representative of the algorithm performance. Also, please have the submission proof-read for English style and grammar issues. There are many minor mistakes, some of which are pointed out below. I am rating marginally below due mainly to the potentially misleading results from the comment on using the highest success rate to report results and to a minor extent due to the novelty aspect (though this is an interesting application).


Comments:

- “Evaluation metric: we plot the moving average of the reward for last 100 episodes, and report the highest success rate for testing on 100 episodes.” —> This is unclear, do you mean you only displayed the maximum success rate out of all 100 episodes? So if the success rates are [0, 100, 0, 0, 0], Figure 2 shows 100% success? If so, this is somewhat misleading and a better metric may have been the average success rate with confidence intervals. Otherwise you may have just gotten a lucky random seed potentially.
- I would’ve liked to see if this is the only method which benefits from multitask learning or do DOMNETs also benefit. This however, is just a nice to have.
- I appreciate the inclusion of hyper parameters and commitment to releasing the code in an effort to promote reproducibility! Great job there. 
- I really like the idea of using graph networks with RL, though I’m not sure if it’s novel to this work. Interesting line of work!
- While this is an interesting application, I’m not sure about the novelty. I suggest spending a bit more time discussing how this work contrasts with methods like Wang et al., or others cited here.

Typos:

“MiniWoB(Shi et al., 2017) benchmark tasks. “ —> missing space between citation
“Q network architecture with graph neural network” —> with a graph neural network
""MiniWoB(Shi et al., 2017)” —> MiniWoB (Shi et al., 2017) (missing space)
“achieved the state” —> achieved state of the art 
“2016; Wang et al., 2018)as main” —> missing space
“series of attentions between DOM elements and goal” —> series of attention (modules?) between the DOM elements and the goal (?)
“constrained action set” —> constrained action sets
“In appendix, we define our criteria for difficulties of different tasks.” —> In the appendix","The sentiment score is determined by the overall tone and content of the review. The reviewer acknowledges the novelty and interesting aspects of the work but expresses concerns about the evaluation method and the novelty. This mixed feedback results in a sentiment score of 20, indicating a slightly positive but cautious sentiment. The politeness score is high because the reviewer uses polite language, provides constructive feedback, and includes positive remarks about the reproducibility efforts and the interesting application. Therefore, the politeness score is 80.",20,80
"This paper proposes a well-designed instance level unsupervised image-to-image translation method which can handle the arbitrary number of instances in a permutation-invariant way. The idea is interesting and the results on various translation datasets are reasonable.  

Pros:
* The proposed method process each instance separately to handle multiple instances. The summarization operation is a simple but effective way to achieve the permutation-invariant property. The context preserving loss is suitable for preserving the background information.
* The paper is well written and easy to follow.

Cons:
* My main concern is about the comparisons with CycleGAN in Figure 4 to 6. Although the CycleGAN+Seg results are shown in Figure 9 indicating that the proposed method can handle multiple instances better. I think there should also be CycleGAN+Seg results in Figure 4 to 6, since the instance segmentation is an extra information. And in my opinion, the CycleGAN+Seg can handle the situation where there are only a few instances (also can be observed in the 1st row in Figure 9). Besides, CycleGAN+Seg can naturally handle the arbitrary number of instances without extra computation cost.

Questions:
*  I wonder what will happen if the network does not permutation-invariant. Except that the results will vary for different the input order, will the generated quality decrease? Since the order information may be useful for some applications.

Overall, I think the proposed method is interesting but the comparison should be fairer in Figure 4 to 6. 
","The sentiment of the review is generally positive, as the reviewer acknowledges the interesting idea, reasonable results, and well-written nature of the paper. However, there are some concerns about the fairness of the comparisons with CycleGAN, which slightly tempers the overall positivity. Therefore, the sentiment score is 70. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, providing specific feedback and questions without being rude or dismissive. Thus, the politeness score is 90.",70,90
"This paper presents a system that infers programs describing 3D scenes composed of simple primitives. The system consists of three stages each of which is trained separately. First, the perceptual module extracts object masks and their attributes. The objects are then are split into several groups. Finally, each group is mapped to a corresponding DSL program using a sequence-to-sequence network similar to the ones typically employed in neural machine translation.

Pros:
+ The paper is written clearly and easy to read.
+ Visual program synthesis is very exciting and important direction both for image understanding and generation.
+ The results on synthetic datasets are good. The authors also demonstrate the applicability of the approach to real-world data (albeit significantly constrained).
+ I find it surprising that a seq2seq is good at producing an accurate program for a group of objects.
+ Visual analogy making experiments are impressive.

Cons:
- The proposed model requires rich annotation of training data since all the components of the systems are trained in a supervised fashion. It’s not clear how to use the method on the in-the-wild data without such annotation.
- Related to the previous point, even when it’s possible to synthesize data, it is non-trivial to obtain the ground-truth grouping of objects. Judging by Table 2, it seems that the system breaks in absence of the grouping information.
- The data used in the paper is quite simplistic (limited number of primitives located in a regular grid). I’m wondering if there is a natural way to extend the approach to more complex settings. My guess is that the performance will drop significantly.

Notes/questions:
* Section 2, paragraph 1: The paper by [Ganin et al., 2018] presents both a system for reproducing an image as well as for sampling from a distribution; moreover, it presents experiments on 3D data (i.e., not limited to drawing).
* Section 3.4, paragraph 2: I’m not sure I understand the last sentence. How can we know that we successfully recovered the scene at test time? Could the authors elaborate on the stopping criterion for sampling?
* Section 4.2, paragraph 2: Do I understand correctly that the main difference between the test set and the generalization set is the number of groups? (i.e., 2 vs 3). If so, it’s a fairly limited demonstration of generalization capabilities of the system.
* Section 4.2, paragraph 4: “we search top 3 proposals ...” – How do we decide which one is better? Do we somehow have an access to the ground truth program at test time?
* Could the authors explain the representation of a program more clearly? How are loops handled? How can one subtract/add programs in the analogy making experiment?

Overall, I think it is a interesting paper and can be potentially accepted on the condition that the authors address my questions and concerns.","The sentiment of the review is generally positive, as indicated by the numerous pros listed and the overall conclusion that the paper is interesting and potentially acceptable. The reviewer appreciates the clarity of the writing, the importance of the research direction, and the impressive results on synthetic datasets. However, there are also several cons and questions raised, which are presented in a constructive manner. The politeness of the language is high, as the reviewer uses polite phrases and constructive feedback without any harsh or rude language.",70,90
"Given an additively decomposable function F(X, Q) = sum_over_x_in_X cost(x, Q), one can approximate it using either random sampling of x in X (unbiased, possibly high variance), or using importance sampling and replace the sum_over_x with a sum_over_coreset importance_of_a_point * cost(x, Q) which if properly defined can be both unbiased and have low variance [1]. In this work the authors consider the weighted sum of activations as F and suggest that for each neuron we can subsample the incoming edges. To construct the importance sampling strategy the authors adapt the classic notion of sensitivity from the coreset literature. Then, one has to carefully balance the approximation quality from one layer to the next and essentially union bound the results over all layers and all sampled points. The performed analysis is sound (up to my knowledge).

Pro:
- I commend the authors for a clean and polished writeup.
- The analysis seems to be sound (apart from the issues discussed below)
- The experimental results look promising, at least in the limited setup.

Con:
- There exists competing work with rigorous guarantees, for example [2].
- The analysis hinges on two assumptions which, in my opinion, make the problem feasible: having (sub) exponential tails allows for strong concentration results, and with proper analysis (as done by the authors), the fact that the additively decomposable function can be approximated given well-behaving summands is not surprising. The analysis is definitely non-trivial and I commend the authors for a clean writeup.
- While rigorous guarantees are lacking for some previous work, previously introduced techniques were shown to be extremely effective in practice and across a spectrum of tasks. As the guarantees arguably stem from the assumptions 1 and 2, I feel that it’s unfair to not compare to those results empirically. Hence, failing to compare to results of at least [2, 3] is a major drawback of this work.
- The result holds for n points drawn from P. However, in practice the network might receive essentially arbitrary input from P at inference time. Given that we need to decide on the number of edges to preserve apriori, what are the implications?
- The presented bounds should be discussed on an intuitive level (i.e. the number of non zero entries is approximately cubic in L).

I consider this to be a well-executed paper which brings together the main ideas from the coreset literature and shows one avenue of establishing provable results. However, given that no comparison to the state-of-the-art techniques is given I'm not confident that the community will apply these techniques in practice. On the other hand, the main strength -- the theoretical guarantees -- hinge on the introduced assumptions. As such, without additional empirical results demonstrating the utility with respect to the state-of-the-art methods (for the same capacity in terms of NNZ) I cannot recommend acceptance.

[1] https://arxiv.org/abs/1601.00617
[2] papers.nips.cc/paper/6910-net-trim-convex-pruning-of-deep-neural-networks-with-performance-guarantee
[3] https://arxiv.org/abs/1510.00149


========
Thank you for the detailed responses. Given the additional experimental results and connections to existing work, I have updated my score from 5 to 6. ","The sentiment of the review is generally positive, as indicated by phrases like 'commend the authors for a clean and polished writeup' and 'the analysis seems to be sound.' However, the reviewer also points out significant drawbacks, such as the lack of comparison to state-of-the-art techniques, which tempers the overall positivity. Therefore, the sentiment score is 40. The politeness of the language is high, with the reviewer using polite and constructive language throughout, such as 'I commend the authors' and 'I cannot recommend acceptance' rather than more direct or harsh criticism. Thus, the politeness score is 80.",40,80
"The paper proposes a method of regularising goal-conditioned policies with a mutual information term. While this is potentially useful, I found the motivation for the approach and the experimental results insufficient. On top of that the presentation could also use some improvements. I do not recommend acceptance at this time.

The introduction is vague and involves undefined terms such as ""useful habits"". It is not clear what problems the authors have in mind and why exactly they propose their specific method. The presentation of the method itself is not self-contained and often relies on references to other papers to the point where it is difficult to understand just by reading the paper. Some symbols are not defined, for example what is Z and why is it discrete?

The experiments are rather weak, they are missing comparison to strong exploration baselines and goal-oriented baselines.","The sentiment score is -60 because the reviewer expresses significant dissatisfaction with the paper's motivation, experimental results, and presentation. The reviewer explicitly states that they do not recommend acceptance, which indicates a strong negative sentiment. The politeness score is 20 because, while the reviewer is critical, they maintain a professional tone and do not use rude or disrespectful language. The feedback is direct but not impolite.",-60,20
"This paper proposes an unsupervised style transfer method uses two-pathway encoder and a decoder for both domains. The loss function can be written using reconstruction losses and the confusion term. Experimental results are very promising comparing to state of the art methods. 

The methodology presented in this paper is simple yet powerful according to the experimental results. However I do have a few concerns:   

1. The writing can certainly be improved.  I had a difficult time understanding Section 2. For example the function Q is upper cased but later the f and g are all lower cased. Why domains A and B are defined using the space and the probability measure? ""our framework assumes that the distribution of persons with sunglasses and that of persons without them is the same,"" The ""distribution of persons"" is not a rigorous definition and is hard to infer what does it actually mean. ""f"" does not appear in the loss terms although it appears under ""min"". 

2. I like the simplicity of the objective function, but it is hard for me to understand that why the algorithm does not pick up spurious differences between A and B. For example, what if there are lighting differences and glasses/no-glasses differences between A and B? See 3rd row of figure 2 for an example. 

3. Given the huge differences in performance between the proposed method and MUNIT and DRIT, some analysis/discussion on the reason of success/failure should be given.

--------------------------------------------------------

I have read authors' response. ","The sentiment of the review is generally positive, as indicated by phrases like 'Experimental results are very promising' and 'The methodology presented in this paper is simple yet powerful.' However, the reviewer also expresses some concerns and provides specific recommendations for improvement. Therefore, the sentiment score is 60. The politeness of the language is quite high, as the reviewer uses polite language such as 'I do have a few concerns' and 'I like the simplicity of the objective function,' even when pointing out issues. Therefore, the politeness score is 80.",60,80
"The paper proposes embedding the data into low-dimensional Wasserstein spaces. These spaces are larger and more flexible than Euclidean spaces and thus, can capture the underlying structure of the data more accurately. However, the paper simply uses the automatic differentiation to calculate the gradients. Thus, it offers almost no theoretical contribution (for instance, how to calculate these embeddings more efficiently (e.g. faster or more efficient calculation of the Sinkhorn divergence), how to motivate loss functions than can benefit the structure of the Wasserstein spaces, what the interpretation of phi(x) is for each problem, e. g. word embedding, etc.). Additionally, the experiments are unclear and need further improvement. For instance, which method is used to find the Euclidean embedding of the datasets? Have you tried any alternative loss functions for the discussed problems? Are these embeddings useful (classification accuracy, other distortion measure)? How does the 2-D visualizations compare to DR methods such as t-SNE? What is complexity of the method? How does the runtime compare to similar methods?","The sentiment of the review is moderately negative, as it highlights several shortcomings of the paper, such as the lack of theoretical contributions and unclear experiments. The reviewer does acknowledge the potential of embedding data into low-dimensional Wasserstein spaces but focuses more on the deficiencies. Therefore, the sentiment score is -40. The politeness of the language is neutral to slightly polite. The reviewer uses formal and respectful language without any derogatory remarks, but the tone is more critical than encouraging. Thus, the politeness score is 20.",-40,20
"Paper overview: Model ensembling techniques aim at improving machine learning model prediction results by i) executing several different algorithms on the same task and ii) solving the discrepancies in the responses of all the algorithms, for each task. Some common methods are voting and averaging (arithmetic or geometric average) on the results provided by the different algorithms. 
Since averaging amounts to computing barycenters with different distance functions, this paper proposes to use the Wassertein barycenter instead of the L2 barycenter (arithmetic average) or the extended KL barycenter (geometric mean). 

Remarks, typos and experiences that would be interesting to add: 
     1) Please define the acronyms before using them, for instance DNN (in first page, 4th line), KL (also first page), NLP, etc. 
    2) In practice, when ensembling different methods, the geometric and arithmetic mean are not computed with equal weights ($\lambda_l$ in Definition 1). Instead, these weights are computed as the optimal values for a given small dev-set. It would be interesting to see how well does the method compare to these optimal weighted averages, and also if it improves is we also compute the optimal $\lambda_l$ for the Wasserstein barycenter. 
    3) How computationally expensive are these methods? 
    4) So the output of the ensembling method is a point in the word embedding space, but we know that not all points in this space have an associated word, thus, how are the words chosen?
    5) The image captioning example of Fig.4 is very interesting (although the original image should be added to understand better the different results), can you show also some negative examples? That is to say, when is the Wassertein method is failing but not the other methods.


Points in favor: 
     1)Better results: The proposed model is not only theoretically interesting, but it also improves the arithmetic and geometric mean baselines.
    2) Interesting theoretical and practical properties: semantic accuracy, diversity and robustness (see Proposition 1). 

Points against: The paper is not easy to read. Ensembling methods are normally applied to the output of a classifier or a regression method, so it is not evident to understand why the 'underlying geometry' is in the word embedding space (page 2 after the Definition 1). I think this is explained in the second paragraph of the paper, but that paragraph is really not clear. I assume that is makes sense to use the word-embedding space for the image caption generation or other ML tasks where the output is a word, but I am not sure how this is used in other cases. 

Conclusion: The paper proposes a new method for model assembling by rethinking other popular methods such as the arithmetic and geometric average. It also shows that it improves the current methods. Therefore, I think it presents enough novelties to be accepted in the conference.","The sentiment of the review is generally positive, as indicated by the concluding statement that the paper presents enough novelties to be accepted in the conference and the acknowledgment of better results and interesting theoretical and practical properties. However, there are some criticisms regarding the clarity of the paper, which slightly tempers the overall positivity. Therefore, the sentiment score is 70. The politeness of the language is high, as the reviewer uses polite phrases such as 'please' and 'it would be interesting to see,' and provides constructive feedback without being harsh or dismissive. Thus, the politeness score is 90.",70,90
"This paper is well set-up to target the interesting problem of degraded generalisation after adversarial training. The proposal of applying spectral normalisation (SN) is well motivated, and is supported by margin-based bounds. However, the experimental results are weak in justifying the paper's claims.

Pros:
* The problem is interesting and well explained
* The proposed method is clearly motivated
* The proposal looks theoretically solid

Cons:

* It is unclear to me whether the ""efficient method for SN in convolutional nets"" is more efficient than the power iteration algorithm employed in previous work, such as Miyato et al. 2018, which also used SN in conv nets with different strides. There is no direct comparison of performance.

* Fig. 3 needs more explanation. The horizontal axes are unlabelled, and ""margin normalization"" is confusing when shown together with SN without an explanation. Perhaps it's helpful to briefly introduce it in addition to citing Bartlett et al. 2017.

* The epsilons in Fig. 5 have very different scales (0 - 0.5 vs. 0 - 5). Are these relevant to the specific algorithms and why?

* Section 5.3 (Fig. 6) is the part most relevant to the generalisation problem. However, the results are unconvincing: only the results for epsilon = 0.1 are shown, and even so the advantage is marginal. Furthermore, the baseline models did not use other almost standard regularisation techniques (weight decay, dropout, batch-norm). It is thus unclear whether the advantage can be maintained after applying these standard regularsisers.

A typo in page 6, last line: wth -> with","The sentiment of the review is generally positive, as the reviewer acknowledges the interesting problem addressed by the paper, the clear motivation of the proposed method, and its theoretical solidity. However, the sentiment is tempered by the critique of the experimental results, which are described as weak in justifying the paper's claims. Therefore, the sentiment score is 50. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, providing specific feedback and suggestions for improvement without being rude or dismissive. Therefore, the politeness score is 90.",50,90
"Overview:

This paper proposes a new approach to do unsupervised phoneme recognition by learning from unlabelled speech in combination with a trained phoneme language model. The proposed loss function is a combination of a term encouraging the language model of predicted phonemes to match the given language model distribution, and a term to encourage adjacent speech frames to be assigned to the same phoneme class. Phoneme boundaries are iteratively refined using a separate model. Experiments where a hidden Markov model is applied on top of the predicted phonemes are also performed.


Main strengths:

The paper is clear and addresses a very important research problem. The approach and losses proposed in Section 2 have also not been proposed before, and given that an external language model is available, are very natural choices.


Main weaknesses:

The main weakness of this paper is that it does not situate itself within the rich body of literature on this problem.  I give several references below, but I think the authors can include even more studies -- there are several studies around ""zero-resource"" speech processing, and I would encourage the authors to work through the review papers [1, 6].

Concretely, I do not think the authors can claim that ""this is the first fully unsupervised speech recognition method that does not use any oracle segmentation or labels."" I think it could be argued that the system of [3] is doing this, and there are even earlier studies. I also don't think this claim is actually necessary since the paper has enough merit to stand on its own, as long as the related work is discussed properly.

For instance, the proposed approach shares commonalities with several other approaches: [2] also used two separate steps for acoustic modelling and boundary segmentation; [4, 7, 8] builds towards the setting where non-matching text data is available (for language model training) together with untranscribed speech for model development; the approach of [5] uses a very similar refinement step to the one described in Section 3, where an HMM model is initialised and retrained on noisy predicted labels.

In the experiments (Section 4), it would also be useful to report more fine-grained metrics. [6] gives an overview of several of the standard metrics used in this area, but at a minimum phoneme boundary recall, precision and F-scores should be reported in order to allow comparisons to other studies.


Overall feedback:

Given that this paper is situated within the broader context of this research area, which already has a small community around it, I think the novelty in the approach is strong enough to warrant publication given that the additional metrics are reported in the experiments.


Papers/links that should be reviewed and cited:

1. E. Dunbar et al., ""The Zero Resource Speech Challenge 2017,"" in Proc. ASRU, 2017.
2. H. Kamper, K. Livescu, and S. Goldwater. An embedded segmental k-means model for unsupervised segmentation and clustering of speech. in Proc. ASRU, 2017.
3. Lee, C.-y. and Glass, J. R. A nonparametric Bayesian approach to acoustic model discovery. ACL, 2012.
4. Ondel, Lucas, Lukaš Burget, Jan Černocký, and Santosh Kesiraju. ""Bayesian phonotactic language model for acoustic unit discovery."" In Acoustics, Speech and Signal Processing (ICASSP), 2017 IEEE International Conference on, pp. 5750-5754. IEEE, 2017.
5. Walter, O., Korthals, T., Haeb-Umbach, R., and Raj, B. (2013). A hierarchical system for word discovery exploiting DTW-based initialization. ASRU, 2013.
6. M. Versteegh, X. Anguera, A. Jansen, and E. Dupoux, ""The Zero Resource Speech Challenge 2015: Proposed approaches and results,” in Proc. SLTU, 2016.
7. https://www.clsp.jhu.edu/wp-content/uploads/sites/75/2018/05/jsalt2016-burget-building-speech-recognition.pdf
8. https://www.clsp.jhu.edu/workshops/16-workshop/building-speech-recognition-system-from-untranscribed-data/

","The sentiment of the review is generally positive, as the reviewer acknowledges the clarity of the paper and the importance of the research problem. They also appreciate the novelty of the approach and suggest that the paper has enough merit to warrant publication. However, the reviewer points out some weaknesses, particularly the lack of situating the work within the existing literature and the need for more fine-grained metrics in the experiments. The language used is polite and constructive, offering specific recommendations for improvement without being overly critical or harsh.",70,90
"This work is an extension to the work of Sukbaatar et al. (2016) with two main differences:
1) Selective communication: agents are able to decide whether they want to communicate.
2) Individualized reward: Agents receive individual rewards; therefore, agents are aware of their contribution towards the goal.
These two new extensions enable their model to work in either cooperative or a mix of competitive and competitive/collaborative settings. The authors also claim these two extensions enable their model to converge faster and better. 
The paper is well written, easy to follow, and everything has been explained quite well. The experiments are competent in the sense that the authors ran their model in four different environments (predator and prey, traffic junction, StarCraft explore, and StarCraft combat). The comparison between their model with three baselines was extensive; they reported the mean and variance over different runs. I have some concerns regarding their method and the experiments which are brought up in the following:
 
Method:

In a non-fully-cooperative environment, sharing hidden state entirely as the only option for communicate is not very reasonable; I think something like sending a message is a better option and  more realistic (e.g., something like the work of Mordatch & Abbeel, 2017)

Experiment:

The experiment ""StarCraft explore"" is similar to predator-prey; therefore, instead of explaining StarCraft explore, I would like to see how the model works in StarCraft combat. Right now, the authors explain a bit about the model performance in Starcraft combat, but I found the explanation confusing.
 
Authors provide 3 baselines:
1) no communication, but IR
2) no communication, no IR
3) global communication, no IR (commNet)

I think having a baseline that has global communication with IR can show the effect of selective communication better. 

There are some questions in the experiment section that have not been addressed very well. For example:
 Is there any difference between the results of table 1, if we look at the cooperative setup? 
Does their model outperform a model which has global communication with IR? 
Why do IRIC and IC work worst in the medium in comparison to hard in TJ in table1? 
Why is CommNet work worse than IRIC and IC in table 2?","The sentiment of the review is generally positive, as the reviewer acknowledges that the paper is well-written, easy to follow, and the experiments are competent. However, the reviewer also raises several concerns and suggestions for improvement, which slightly tempers the overall positive sentiment. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, making suggestions and asking questions in a respectful manner. Therefore, the politeness score is 90.",60,90
"After the rebuttal:

1.  Still, the novelty is limited. The authors want to tell a more motivated storyline from Nestrove-dual-average, but that does not contribute to the novelty of this paper. The real difference to the existing works is ""using soft instead of hard constraint"" for BNN. 

2. The convergence is a decoration. It is easy to be obtained from existing convergence proof of proximal gradient algorithms, e.g. [accelerated proximal gradient methods for nonconvex programming. NIPS. 2015].

---------------------------
This paper proposes solving binary nets and it variants using proximal gradient descent. To motivate their method, authors connect lazy projected SGD with straight-through estimator. The connection looks interesting and the paper is well presented. However, the novelty of the submission is limited.

1. My main concern is on the novelty of this paper. While authors find a good story for their method, for example,
- A Proximal Block Coordinate Descent Algorithm for Deep Neural Network Training
- Training Ternary Neural Networks with Exact Proximal Operator
- Loss-aware Binarization of Deep Networks

All above papers are not mentioned in the submission. Thus, from my perspective, the real novelty of this paper is to replace the hard constraint with a soft (penalized) one (section 3.2). 

2. Could authors perform experiments with ImageNet?

3. Could authors show the impact of lambda_t on the final performance? e.g., lambda_t = sqrt(t) lambda, lambda_t = sqrt(t^2 lambda","The sentiment of the review is mixed but leans towards the negative side. The reviewer acknowledges that the paper is well-presented and finds the connection between lazy projected SGD and the straight-through estimator interesting. However, the reviewer repeatedly emphasizes the limited novelty of the paper, which significantly impacts the overall sentiment score. Therefore, the sentiment score is -40. The politeness of the language is generally neutral to polite. The reviewer provides constructive feedback and specific recommendations without using harsh or rude language. Thus, the politeness score is 50.",-40,50
"The paper considers a shortcoming of sequence to sequence models trained using maximum likelihood estimation. In particular, a model trained in this way can be biased in the sense that training sequences typically have different sets of prefixes compared to test sequences. As a result, at the prediction time the model does not generalize well and for a given input sequence the decoder constructs a label sequence which reflects the training label sequences rather than the actual target label.

To address this shortcoming, the authors propose an approach based on edit distances and the implicit use of given label sequences during training. The main idea is to generate a label sequence with respect to the current parameter vector of a conditional probabilistic model (see Eqs. 2 & 3, as well as the objective in Eq. 6) and then based on the edit distance find the best possible completions for any prefix of that model-based label sequence. The training objective is then defined in Eq. (6): to each element in the output sequence the objective assigns the KL-divergence between a conditional distribution of the next element in the label sequence given a label prefix generated using the current model and the exponential family model based on edit distances given by the prefixes and optimal completions after the position of interest in the label sequence. The objective and the corresponding gradient can be computed efficiently using dynamic programming.

Intuitively, the approach tries to find a parameter vector such that the decoder at a particular instance is likely to construct a label sequence with a small edit distance to the target label. As the training objective now considers all possible elements of the vocabulary given a prefix sequence, it is reasonable to expect that it performs better than MLE which only considers target vocabulary elements given target prefix sequences (e.g., compare Eqs. 3 & 6).

The experiments were conducted on the `Wall Street Journal' and `Librispeech' datasets and the reported results are a significant improvement over the state-of-the-art. I am not an expert in the field and cannot judge the related work objectively but can say that the context for their approach is set appropriately. I would, however, prefer more clarity in the presentation of the approach. This especially applies to the presentation of the approach around Eq. (6). It might not be straightforward for a reader to figure out how the tilde-sequences are obtained. As the objective is non-convex, in order to be able to reproduce the results it would be useful to provide some heuristics for choosing the initial solutions for the parameter vector. In Section 3, please also provide a reference to the appendix so that a reader can understand the conditional probabilistic model.","The sentiment of the review is generally positive, as the reviewer acknowledges the significance of the improvement over the state-of-the-art and appreciates the context set for the approach. However, the reviewer also points out areas for improvement, such as the clarity of the presentation and the need for additional heuristics and references. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer uses polite phrases like 'I would, however, prefer,' 'please also provide,' and 'it would be useful,' indicating a respectful and constructive tone. Therefore, the politeness score is 90.",60,90
"This paper proposes a novel approach with the hypothesis that the reliable features can guide the less reliable ones. This approach is applied to the object detection task and show consistent performance improvements.

pros)
(+) This paper is well-written and easy to follow.
(+) The base idea that divides the learned features into two sets; the reliable feature set and the less reliable one is very interesting and looks novel. Plus, the hypothesis, which is that reliable features can guide the features in the less reliable set is also interesting.
(+) The performance improvements are quite large.
(+) Extensive ablative studies are provided to support the proposed method well.

cons)
(-) The method of obtaining the representative in buffer B is not clearly presented.
(-) The overall training and inference procedure are not clearly presented. 
(-) Some notations and descriptions are vague and confusing.
(-) More than two datasets are necessary to show the effectiveness of the methods

comments)
- What is the higher level feature map P_m? and How did you choose the higher level feature map at the m-th level in option (b) and (c) in Section 3.3.
- What is the meaning of the ""past"" features in Section 3.2?
- It is better to show the exact architecture of the make-up module and the critic module.
- Can this method apply to the other backbones such as VGG or ResNets without FPN?
- The sentences at the bottom of p.4 starting with ""Note that only~"" looks ambiguous. 
- f_critic^j may be the j-th element of F_critic, please denote what f_critic^j stands for.

Even if the paper needs to be revised for better readability, I think this paper is above the standard of ICLR because the idea is interesting and novel. Furthermore, the experimental studies are properly designed and well support the main idea. I am leaning toward acceptance, but I would like to see the other reviewers' comments.","The sentiment of the review is generally positive, as indicated by the praise for the paper's writing quality, novelty, and performance improvements. The reviewer also expresses a leaning towards acceptance. However, there are several critical points raised, which slightly temper the overall positivity. Therefore, the sentiment score is 70. The politeness of the language is high, as the reviewer uses polite and constructive language throughout, even when pointing out the cons and making suggestions for improvement. Thus, the politeness score is 90.",70,90
"The major contribution of this work is extending routing networks (Rosenbaum et al., ICLR 2018) to use diverse architectures across routed modules. I view this as an important contribution and am very impressed by the experiment on Omniglot where it shows big performance gain on a split with very few examples. This idea of incorporating in architectural bias and not just parameter bias for small data problems is very compelling and intuitive to me on the surface. The ablation study was also very interesting in this regard. I really like the discourse and found it to be filled with interesting insights throughout ranging from the connection between routing networks and neural architecture search to the heuristic for selecting k.  However, after the great discourse, I was quite disappointed by the breadth of the experiments. 

The paper is positioned as exploring two parallel ideas that are independently interesting 1) diversity in the architecture of modules in routing models 2) the effect of increasing depth in routing models. For the first idea, this is shown very well by the Omniglot experiment but is not evaluated in any other setting. Showing this in a few other experiments would have really driven this point home in my opinion.  The second idea is not really executed in a convincing way to me. The authors call it a ‘negative result’ in the end, but I’m not sure I really feel like I learned anything from this experiment. I wonder about statistical significance. I also feel like the authors are trying to turn it into a commentary that this is a pain point for all variants of routing models while they only actually tried it for their proposed architecture which makes quite a few decisions along the way. I would have liked to see more model variants and datasets before really feeling like I can make any empirical determinations about the fundamental limitations of all routing models in this regard.  Additionally, if there were such a fundamental scaling limitation, you would imagine that an experiment could be constructed that really highlighted this fact where all routing models do way worse.

In short, I think there are some really good idea in this paper and vote for acceptance on that basis. Had the authors provided more empirical evidence about architectural diversity, I would have given it a very high score. The analysis of depth is also a very interesting topic, but it could possibly even serve as another paper considering that the current results don’t really come to concrete conclusions for the community. 
","The sentiment of the review is generally positive, as the reviewer expresses admiration for the main contributions and the experiment on Omniglot, despite some criticisms regarding the breadth of experiments and the execution of the second idea. The sentiment score is therefore 60. The politeness of the language is high, as the reviewer uses polite and constructive language throughout, even when pointing out areas for improvement. The politeness score is 90.",60,90
"The paper proposes a method for learning regression models through evolutionary
algorithms that promise to be more interpretable than other models while
achieving similar or higher performance. The authors evaluate their approach on
99 datasets from OpenML, demonstrating very promising performance.

The authors take a very interesting approach to modeling regression problems by
constructing complex algebraic expressions from simple building blocks with
genetic programming. In particular, they aim to keep the constructed expression
as small as possible to be able to interpret it easier. The evaluation is
thorough and convincing, demonstrating very good results.

The presented results show that the new method beats the performance of existing
methods; however, as only very limited hyperparameter tuning for the other
methods was performed, it is unclear to what extent this will hold true in
general. As the main focus of the paper is on the increased interpretability of
the learned models, this is only a minor flaw though.

The interpretability of the final models is measured in terms of their size.
While this is a reasonable proxy that is easy to measure, the question remains
to what extent the models are really interpretable by humans. This is definitely
something that should be explored in future work, as a small-size model does not
necessarily imply that humans can understand it easily, especially as the
generated algebraic expressions can be complex even for small trees.

The description of the proposed method could be improved; in particular it was
unclear to this reviewer why the features needed to be differentiable and what
the benefit of this was (i.e. why was this the most appropriate way of adjusting
weights).

In summary, the paper should be accepted.","The sentiment of the review is positive, as indicated by phrases like 'very interesting approach,' 'thorough and convincing evaluation,' and 'very good results.' The reviewer also concludes with a recommendation for acceptance, which further supports a positive sentiment. Therefore, the sentiment score is 80. The politeness of the language is also high, as the reviewer uses polite and constructive language throughout the review, such as 'should be explored in future work' and 'could be improved,' without any harsh or rude comments. Therefore, the politeness score is 90.",80,90
"Quality: This submission claims to present a model that can control non-annotated attributes such as speaking style, accent, background noise, etc. Though empirical evidence in the form of numerical measurements is presented for some controllable attributes more evidence other than individual samples and authors claims is needed. For example a reliable numerical evidence is needed on page 4 following ""We also found..."", page 5 following ""We discovered...."", page 5 following ""It clearly presents..."", page 5 following ""Drawing samples..."" evidence is given only for 1 dimension, page 6 following ""Figure 7(b)..."". 

Clarity: The model is simple though the exact form and nature of observed and latent class variables could be made more explicit. Including how they are computed/initialised/set. What are different modes using the proposed model? Why both negative results are in the appendix? 

Originality: moderately

Significance: moderately
","The sentiment of the review appears to be neutral to slightly negative. The reviewer acknowledges the claims made by the authors but emphasizes the need for more empirical evidence and clarity in the presentation. The sentiment score is therefore set at -20. The politeness of the language used is neutral to slightly polite. The reviewer uses formal and constructive language without being overly harsh or rude, thus the politeness score is set at 20.",-20,20
"This paper introduces a generative model for question answering.  Instead of modeling p(a|q,c), the authors propose to model p(q,a|c), factorized as p(a|c) * p(q|a,c).  This is a great idea, it was executed very well, and the paper is very well written.  I'm glad to see this idea implemented and working.                                                       
                                                                                                     
Reactions:                                                                                           
- Section 2.1: Is there a bias problem here, where you're only ever training with the correct answer?  Oh, I see you covered that in section 2.6.  Great.
- Section 2.4: what happens when there are multiple QA pairs per paragraph or image?  Are you just getting conflicting gradients at different batches, so you'll end up somewhere in the middle of the two answers?  Could you do better here?
- Section 2.6: The equation you're optimizing there reduces to -log p(a|q,c), which is exactly the loss function used by typical models.  You should note that here.  It's a little surprising (and interesting) that training on this loss function does so poorly compared to the generative training.  This is because of how you've factorized the distributions, so the model isn't as strong a discriminator as it could be, yes?
- Section 3.1 (and section 2.6): Can you back up your claim of ""modeling more complex dependencies"" in the generative case?  Is that really what's going on?  How can we know?  What does ""modeling more complex dependencies"" even mean?  I don't think these statements really add anything currently, as they are largely vacuous without some more description and analysis.
- Section 3.3: Your goal here seems similar to the goal of Clark and Gardner (2018), trying to correctly calibrate confidence scores in the face of SQuAD-like data, and similar to the goals of adding unanswerable questions in SQuAD 2.0.  I know that what you're doing isn't directly comparable to either of those, but some discussion of the options here for addressing this bias, and whether your approach is better, could be interesting.
                                                                                                     
Clarity issues:                                                                                      
- Bottom of page 2, ""sum with a vector of size d"" - it's not clear to me what this means.            
- Top of page 3, ""Answer Encoder"", something is off with the sentence ""For each word representation"" 
- Section 2.5, ""we first embed words independently of the question"" - did you mean ""of the _context_""?
- Section 2.5.2 - it's not clear to me how that particular bias mechanism ""allows the model to easily filter out parts of the context which are irrelevant to the question"".  The bias mechanism is independent of the question.
- Section 2.7 - when you said ""beam search"", I was expecting a beam over the question words, or something.  I suppose a two-step beam search is still a beam search, it just conjured the wrong image for me, and I wonder if there's another way you can describe it that better evokes what you're actually doing.
- Section 3.1 - ""and are results..."" - missing ""competitive with""?                                   
- Last sentence: ""we believe their is"" -> ""we believe there is"" ","The sentiment of the review is highly positive, as indicated by phrases like 'This is a great idea,' 'executed very well,' and 'the paper is very well written.' The reviewer expresses enthusiasm and satisfaction with the work. Therefore, the sentiment score is 90. The politeness of the language is also very high. The reviewer uses polite and constructive language throughout, even when pointing out areas for improvement. Phrases like 'Great,' 'Could you do better here?' and 'I wonder if there's another way you can describe it' indicate a respectful and considerate tone. Therefore, the politeness score is 95.",90,95
"This paper first identifies an inequivalence between L2 regularization and the original weight decay in adaptive stochastic gradient methods, e.g., the Adam method, and then proposes two decoupled variants, SGDW and AdamW, respective. The authors also cited a recent work to provide a justification of their proposed update rules from the perspective of Bayesian filtering. To demonstrate the effectiveness of both methods, experiments on CIFAR10 and ImageNet32x32 are conducted to compare with the original methods. Results show that the proposed methods consistently lead to faster convergence. Overall the paper is well written and easy to follow, with enough details describing the experimental settings. 

First of all I appreciate the authors pointing out that weight decay is not equal to L2 regularization in general. This is evident once the original definition of weight decay is given. The main motivation comes from the argument that instead of using L2 regularization, weight decay should be used in adaptive gradient methods. The Bayesian filtering interpretation helps to justify the proposed method. But it is not clear to me why the hyperparameters w and \alpha are decoupled in the proposed methods? For example, in Line 6 of Alg. 1, g_t is a function of w, and later in Line 8, g_t is coupled with \alpha which naturally introduces a term w \alpha into m_t. So both w and \alpha are still coupled together in the proposed algorithm. If this is the case why the authors still call w and \alpha decoupled? 

To me the most interesting result is Proposition 3 where the authors show that weight decay actually corresponds to preconditioned L2 regularization. This helps to explain what's the algorithmic difference between these two methods in adaptive gradient methods, and provides an intuitive insight on why weight decay may lead to better results compared with the vanilla L2 regularization. 

Experiments on image recognition tasks basically confirm the authors' claims. However, as the authors have already pointed out, it is better to have more thorough experiments on other kinds of tasks, e.g., in text classification, etc. If the improvement does come from the difference between weight decay vs L2, then I would also expect the same improvement on other tasks. It would be great to see more experimental results on other tasks to have a better understanding of this problem. So far it is not clear whether the same improvement holds in general or not. 
","The sentiment of the review is generally positive, as the reviewer appreciates the authors' work and acknowledges the clarity and effectiveness of the proposed methods. The sentiment score is 70 because the reviewer expresses appreciation and acknowledges the value of the work, but also points out areas for improvement. The politeness score is 90 because the language used is very polite, with phrases like 'I appreciate,' 'it would be great,' and 'it is better to,' which indicate a respectful and constructive tone.",70,90
"The paper introduces a decentralized training method for multi-agent reinforcement learning, where the agents infer the policies of other agents and use the inferred models for decision making. The method is intuitively straightforward and the paper provides some justification for convergence. I think the underlying theory is okay (new but not too surprising, a lot of the connections can be made with single agent RL), but the paper would be much stronger with experiments that have more than two players, one state and one dimensional actions.

(
Update: the new version of the paper addresses most of my concerns. There are a lot more experiments, and I think the paper is good for ICLR. 

However, I wonder if the reasoning for PR2 is limited to ""self-play"", otherwise Theorem 1 could break because of the individual Q_i functions will not be symmetric. This could limit the applications to other scenarios.

Also, maybe explain self-play mathematically to make the paper self contained?
)

1. From the abstract, "" PR2-Q and PR2-Actor-Critic, that are proved to converge in the self-play scenario"". Theorem 2 only shows that under relatively strong assumptions (e.g. single Nash equilibrium), the soft value iteration operator is a contraction. This seems to have little to do with the actual convergence of PR2-Q and PR2-AC, especially AC which uses gradient-based approach. Also here the ""convergence"" in the abstract seem to imply convergence to the (single) global optimal solution (as is shown in the experiments), for which I thought you cannot prove even for single agent AC -- the best you can do is to show that gradient norm converges to zero, which gives you a local optima. Maybe things are different with the presence of the (concave) entropy regularization?

2. Theorem 2 also assumes that the opponent model $\rho$ will find the global optimal solution (i.e. (11, 12) can be computed tractably). However, the paper does not discuss the case where $\rho$ or $Q_\theta$ in question is imperfect (similar to humans over/underestimate its opponents), which might cause the actual solution to deviate significantly from the (single) NE. This would definitely be a problem in more high-dimensional MARL scenarios. I wonder if one could extend the convergence arguments by extending Prop 2.

3. The experiments mostly demonstrates almost the simplest non-trivial Markov games, where it could be possible that (11, 12) are true for PR2. However, the effectiveness of the method have not been demonstrated in other (slightly higher-dimensional) environments, such as the particle environments in the MADDPG paper. It does not seem to be very hard to implement this, and I wonder if this is related to the approximation error in (11, 12). The success in such environments would make the arguments much stronger, and provide sound empirical guidance to MARL practitioners.

Minor points:
- Are the policies in question stationary? How is PR2 different from the case of single agent RL (conditioned on perfect knowledge of a stationary opponent policy)?
- I have a hard time understanding why PR2 would have different behavior than IGA even with full knowledge of the opponent policy, assuming each policy is updated with infinitesimally small (but same) learning rates. What is the shape of the PR2 optimization function wrt agent 1?
- I wonder if using 2 layer neural networks with 100 units each on a 1 dimensional problem is overkill.
- Figure 4(a): what are the blue dots?
- Does (11) depend on the amount of data collected from the opponents? If so, how?
- I would recommend combining Prop 1 and prop 2 to save space. Both results are straightforward to prove, but the importance sampling perspective might be useful.
- Have you tried to compare with SGA (Balduzzi et al) or Optimistic mirror descent?
- I am also curious about an ablation study over the components used to infer opponent policies. A much simpler case would be action-dependent baselines, which seem to implicitly use some information about the opponents.","The sentiment of the review is generally positive, as indicated by the statement that the paper is good for ICLR and that the new version addresses most concerns. However, there are still some critical points and suggestions for improvement, which slightly temper the overall positivity. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer uses polite phrases such as 'I think,' 'I wonder,' and 'I would recommend,' and provides constructive feedback without being harsh or dismissive. Thus, the politeness score is 80.",60,80
"The main contributions of the paper are an edit encoder model similar to (Guu et al. 2017 http://aclweb.org/anthology/Q18-1031), a new dataset of tree-structured source code edits, and thorough and well thought-out analysis of the edit encodings. The paper is clearly written, and provides clear support for each of their main claims.

I think this would be of interest to NLP researchers and others working on sequence- and graph-transduction models, but I think the authors could have gone further to demonstrate the robustness of their edit encodings and their applicability to other tasks. This would also benefit greatly from a more direct comparison to Guu et al. 2017, which presents a very similar ""neural editor"" model.

Some more specific points:

- I really like the idea of transferring edits from one context to another. The one-shot experiment is well-designed, however it would benefit from also having a lower bound to get a better sense of how good the encodings are.

- If I'm reading it correctly, the edit encoder has access to the full sequences x- and x+, in addition to the alignment symbols. I wonder if this hurts the quality of the representations, since it's possible (albeit not efficient) to memorize the output sequence x+ and decode it directly from the 512-dimensional vector. Have you explored more constrained versions of the edit encoder (such as the bag-of-edits from Guu et al. 2017) or alternate learning objectives to control for this?

- The WikiAtomicEdits corpus has 13.7 million English insertions - why did you subsample this to only 1M? There is also a human-annotated subset of that you might use as evaluation data, similar to the C#Fixers set.

- On the human evaluation: Who were the annotators? The categories ""similar edit"", and ""semantically or syntactically same edit"" seem to leave a lot to interpretation; were more specific instructions given? It also might be interesting, if possible, to separately classify syntactically similar and semantically similar edits.

- On the automatic evaluation: accuracy seems brittle for evaluating sequence output. Did you consider reporting BLEU, ROUGE, or another ""soft"" sequence metric?

- It would be worth citing existing literature on classification of Wikipedia edits, for example Yang et al. 2017 (https://www.cs.cmu.edu/~diyiy/docs/emnlp17.pdf). An interesting experiment would be to correlate your edit encodings with their taxonomy.","The sentiment of the review is generally positive. The reviewer acknowledges the main contributions of the paper, appreciates the clear writing, and finds the work to be of interest to the research community. However, the reviewer also provides several constructive criticisms and suggestions for improvement. Therefore, the sentiment score is 60. The politeness of the language is high; the reviewer uses polite and respectful language throughout the review, even when pointing out areas for improvement. Therefore, the politeness score is 90.",60,90
"I like the idea of the paper and I believe it addressing a very relevant problem. While the authors provide a good formalization of the problem and convincing demonstration of the generalization bound, the evaluation could have been better by including some more challenging experiments to really prove the point of the paper. It is surely good to present the toy example with the MNIST dataset but the ethnicity domain is less difficult than what the authors claim. This is also pretty evident from the results presented (e.g., in Table 3). The proposed approach provides maybe slightly better results than the state of the art but the results do not seem to be statistically significant. This is probable also due to the fact that the problem itself is made simpler by the cropped faces, no background, etc. I would have preferred to see an application domain where the improvement would be more substantial. Nevertheless, I think the theoretical presentation is good and I believe the manuscript has very good potential. ","The sentiment of the review is generally positive, as the reviewer appreciates the idea of the paper and acknowledges its relevance and good theoretical presentation. However, there are some criticisms regarding the evaluation and the significance of the results, which slightly temper the overall positive sentiment. Therefore, the sentiment score is 60. The language used in the review is polite and constructive, offering specific recommendations for improvement without being harsh or dismissive. Thus, the politeness score is 80.",60,80
"This paper extends the recent results concerning GP equivalence of infinitely wide FC nets to the convolutional case. This paper is generally of a high quality (notwithstanding the lack of keys on figures) and provides insights to an important class of model. I recommend that this paper be accepted, but I think it could be improved in a few ways. 

Firstly, and rather mundanely: the figures. Fig 1 is not easy to read due to the density of plotting, and as there is no key it isn’t possible to tell what it shows. Figure 2 is rather is called a ‘graphical model’ but the variables (weights and biases) are not shown. It should be specified that this is the graphical model of the infinite limit, in which case the K variables should not be random. Also, the caption on this figure refers to variables that aren’t in the figure, and is grammatically incorrect (perhaps something like ‘the limit of an infinitely wide convolutional’ is missing?). Figure 3 has a caption which seems to be inconsistent with the coloring (for example green is center pixel in the text, but blue in the key). Figure 6 is also missing a key. In Figure 5, what does the tick symbol denote? Finally, the value some of Table 1 is questionable as so many entries are missing. For example, the Fashion-MNIST column has only two values, which seems to me of little use. [I would have given the paper a rating of 7 were it not for these issues]

Regarding the presentation of the content, I found this paper generally easy to follow and the arguments sound. Here are few points:

There is an important distinction between finite width Bayesian-CNNs and the infinite limit, and this distinction is indeed made in the paper but not clearly enough in my view. I would anticipate that some readers might come away after a cursory reading thinking that Bayesian-CNNs are fundamentally worse than their parametric counterparts, but this is emphatically not the message of the paper. It seems that the infinite limit that is the cause of two problems. The first problem (or perhaps benefit) is that the infinite limit gives Gaussian inner layers, just as in the fully connected case. The second problem (and I’d say this is definitely a problem this time) is that the infinite limit loses the covariance between the pixels, at least with a fully connected final layer. I would recall [Matthews 2018, long version] section 7, which discusses that point that taking the infinite limit in the fully connected is actually potentially undesirable. To quote Matthews 2018, “MacKay (2002, p. 547) famously reflected on what is lost when taking the Gaussian process limit of a single hidden layer network, remarking that Gaussian processes will not learn hidden features”. Some discussion of this would enhance the presented paper, in my view. 

The discussion of eq (7) could be made more clear. Eq (7) is only defined on K, and not in composition with A. It is important that the alpha dependency is preserved by the A operation, and while I suppose this is obvious I would welcome a bit more detail. It would help to demonstrate the application of the results of [Cho and Saul 2009] to the convolution case explicitly (i.e. for C o A), in my view. 

Regarding results, effort has clearly gone to keep the comparisons as fair as possible, but with these large datasets it is difficult to disentangle the many factors that might effect performance (as acknowledged on p9). It is a weakness of the paper that there is no toy example. An example demonstrating a situation which can only be solved with hierarchical features (e.g. features that are larger than the receptive field of a single layer) would be particularly interesting, as in this case I think the GP-CNN would fail, even with the average pooling, whereas the finite Bayesian-CNN would succeed (with a sufficiently accurate inference method).  

It would improve readability to stress the 1D notation in the main text rather than in a footnote. On first reading I missed this detail and was confused as I was trying to interpret everything as a 2D convolution. On reflection I think notation is used in the paper is good, but I think the generalization to 2D should be elevated to something more than the footnote. Perhaps a paragraph explaining how the 2D case works would be appropriate, especially as all the experiments are in 2D cases. 

Some further smaller points on specific [section, paragraph, line]s

1,2,4 I think ‘easily’ is a bit of an overstatement. In this work the kernel is itself defined via a recursive convolutional operation, which doesn’t seem to me much more interpretable than the parametric convolution. At least the filters can be examined in parametric case, which isn’t the case here. I do agree with the sentiment that a function prior is better than an implicit weight prior, however.

1,2,-1 This seems too vague to me, as at least to some extent, Matthew 2018 did indeed consider using NN-GPs to gain insight about equivalent NN models (e.g. section 5.3)

1.1,:,: I find it very surprising that there are no references to Cho and Saul 2009 in this section (one does appear in 2.2.2, however). 

1.1,3,-2:-1 ‘Our work differs from all of these in that our GP corresponds exactly to a fully Bayesian CNN in the many channel limit’ I do not think this is completely true, as the deep convolution GP does correspond to an infinite limit of a Bayesian CNN, just not the same limit as the one taken in this paper. Similarly a DGP following the Danianou and Lawrence 2013 is an infinite limit of a NN, but one with bottlenecks between layers. It is important that readers appreciate that infinite limits can be taken in different ways, and the resulting models may be very different. This certain limit taken in this work has desirable computational properties, but arguably undesirable modelling implications.

1.1,-1,-2 It should be made more clear here that the SGD trained models are non-Bayesian. 

Figure 3 The MC-CNN-GP appears to have performance that is nearly independent of the depth, even including 1 layer. Could this be explained?

2.2,2,: The z^l variables are zero mean Gaussian with a fixed covariance, not delta functions, as I understand it. They are independent of each other due to the deterministic K^l, certainly, but they are not themselves deterministic. Could this be clarified? 
","The sentiment of the review is generally positive, as indicated by the recommendation to accept the paper and the acknowledgment of its high quality and insights into an important class of model. However, the reviewer also points out several areas for improvement, particularly regarding the figures and some aspects of the presentation. Therefore, the sentiment score is not extremely high but still positive. The politeness of the language is quite high; the reviewer uses polite language and constructive criticism throughout the review, even when pointing out issues. The reviewer also provides specific suggestions for improvement, which is a hallmark of polite and constructive feedback.",70,90
"This paper proposes a method for estimating non-linear similarities between items using Gramian estimation. This is achieved by having two separate neural networks defined for each item to be compared, which are then combined via a dot product. The proposed innovation in this paper is to use Gramian estimation for the penalty parameter of the optimization which allows for the non-linear case. Two algorithms are proposed which allow for estimation in the stochastic / online setting. Experiments are presented which appear to show good performance on some standard benchmark tasks. 

Overall, I think this is an interesting set of ideas for an important problem. I have two reservations. First, the organization of the paper needs to be addressed in order to aid user readability. The paper often jumps across sections without giving motivation or connecting language. This will limit the audience of the paper and the work. Second (and more importantly), I found the experiments to be slightly underwhelming. The hyperparameters (batch size, learning rate) and architecture don’t have any rationale attached to them. It is also not entirely clear whether the chosen comparison methods fully constitute the current state of the art. Nonetheless, I think this is an interesting idea and strong work with compelling results. 

Editorial comments:

The organization of this paper leaves something to be desired. The introductions ends very abruptly, and then appears to begin again after the related work section. From what I can tell the first three sections all constitute the introduction and should be merged with appropriate edits to make the narrative clear.

“where x and y are nodes in a graph and the similarity is wheter an edge” → typo and sentence ends prematurely. 
","The sentiment of the review is generally positive, as the reviewer describes the paper as 'interesting' and 'strong work with compelling results,' despite having some reservations about the organization and the experiments. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer provides constructive feedback in a respectful manner, using phrases like 'needs to be addressed' and 'nonetheless, I think this is an interesting idea.' Thus, the politeness score is 80.",60,80
"# [Updated after author response]
Thank you for your response. I am happy to see the updated paper. In particular, the added item in section 1.3 highlights where the novelty of the paper lies, and as a consequence, I think the significance of the paper is increased. Furthermore, the clarity of the paper has increased. 

In its current form, I think the paper would be a valuable input to the deep learning community, highlighting an important issue (CF) for neural networks. I have therefore increased my score.

------------------------------------------

# Summary
The authors present an empirical study of catastrophic forgetting (CF) in deep neural networks. Eight models are tested against nine datasets with 10 classes each but a varying number of samples. The authors construct a number of sequential learning tasks to test the model performances in different scenarios. The main conclusion is that CF is still a problem in all models, despite claims in other papers.

# Quality
The paper shows healthy criticism of the methods used to evaluate CF in previous works. I very much like this.

While I like the different experimental set-ups and the attention to realistic scenarios outlined in section 1.2, I find the analysis of the experiments somewhat superficial. The accuracies of each model for each task and dataset are reported, but there is little insight into what causes CF. For instance, do some choices of hyperparameters consistently cause a higher/lower degree of CF across models? I also think the metrics proposed by Kemker et al. (2018) are more informative than just reporting the last and best accuracy, and that including these metrics would improve the quality of the paper.

# Clarity
The paper is generally clearly written and distinct paragraphs are often highlighted, which makes reading and getting an overview much easier. In particular, I like the summary given in sections 1.3 and 1.4.

Section 2.4 describing the experimental setup could be clearer. It takes a bit of time to decipher Table 2, and it would have been good with a few short comments on what the different types of tasks (D5-5, D9-1, DP10-10) will tell us about the model performances. E.g. what do you expect to see from the experiments of D5-5 that is not covered by D9-1 and vice versa? And why are the number of tasks in each category so different (8 vs 3 vs 1)?

I am not a huge fan of 3D plots, and I don't think they do anything good in section 4. The perspective can make it tricky to compare models, and the different graphs overshadow each other. I would prefer 2D plots in the supplementary, with a few representative ones shown in the main paper. I would also experiment with turning Table 3 into a heat map.

# Originality
To my knowledge, the paper presents the largest evaluation of CF in terms of evaluated datasets. Kemker et al. (2018) conduct a somewhat similar experiment using fewer datasets, but a larger number of classes, which makes the CF even clearer. I think it would be good to cite this paper and briefly discuss it in connection with the current work.

# Significance
The paper is mostly a report of the outcome of a substantial experiment on CF, showing that all tested models suffer from CF to some extent. While this is interesting and useful to know, there is not much to learn in terms of what can cause or prevent CF in DNNs. The paper's significance lies in showing that CF is still a problem, but there is room for improvement in the analysis of the outcome of the experiments.

# Other notes
The first sentence of the second paragraph in section 5 seems to be missing something.

# References
Kemker, R., McClure, M., Abitino, A., Hayes, T., & Kanan, C. (2018). In AAAI Conference on Artificial Intelligence. https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16410","The sentiment of the review is positive, as the reviewer expresses satisfaction with the updated paper and acknowledges improvements in clarity and significance. This is evident from phrases like 'I am happy to see the updated paper' and 'I think the paper would be a valuable input to the deep learning community.' The sentiment score is therefore 80. The politeness of the language is also high, as the reviewer uses polite expressions such as 'Thank you for your response' and 'I very much like this.' The reviewer provides constructive feedback in a respectful manner, leading to a politeness score of 90.",80,90
"This paper presents a VAE approach in which a dependency structure on the latent variable is learned during training.  Specifically, a lower-triangular random binary matrix c is introduced, where c_{i,j} = 1 for i>j, indicates that z_i depends on z_j, where z is the latent vector.  Each element of c is separately parametrized by a Bernoulli distribution whose means are optimized for during training, using the target \mathbb{E}_{p(c)}[\mathcal{L}_c] where \mathcal{L}_c indicates the ELBO for a particular instance of c.  The resulting ""Graph-VAE"" scheme is shown to train models with improved marginal likelihood than a number of baselines for MNIST, Omniglot, and CIFAR-10.

The core concept for this paper is good, the results are impressive, and the paper is, for the most part, easy to follow.  Though I think a lot of people have been thinking about how to learn dependency structures in VAEs, I think this work is the first to clearly lay out a concrete approach for doing so.  I thus think that even though this is not the most novel of papers, it is work which will be of significant interest to the ICLR community.  However, the paper has a number of technical issues and I do not believe the paper is suitable for publication unless they are addressed, or at the vest least acknowledged. I further have some misgivings with the experiments and the explanations of some key elements of the method.  Because of these issues, I think the paper falls below the acceptance threshold in its current form, but I think they could potentially be correctable during the rebuttal period and I will be very happy to substantially increase my score if they are; I feel this has the potential to be a very good paper that I would ultimately like to see published.

%%% Lower bound %%%

My first major concern is in the justification of the final approach (Eq 8), namely using a lower bound argument to move the p(c) term outside of the log.  A target being a lower bound on something we care about is never in itself a justification for that target -- it just says that the resulting estimator is provably negatively biased.  The arguments behind the use of lower bounds in conventional ELBOs are based on much more subtle arguments in terms of the bound becoming tight if we have good posterior approximations and implicit assumptions that the bound will behave similarly to the true marginal.  The bound derived in A.1 of the current paper is instead almost completely useless and serves little purpose other than adding ""mathiness"" of the type discussed in https://arxiv.org/abs/1807.0334. Eq 8 is not a variational end-to-end target like you claim.  It is never tight and will demonstrably behave very differently to the original target.

To see why it will behave very differently, consider how the original and bound would combine two instances of c for the MNIST experiment, one corresponding to the MAP values of c in the final trained system, the other a value of c that has an ELBO which is, say, 10 nats lower.  Using Eq 8, these will have similar contributions to the overall expectation and so a good network setup (i.e. theta and phi) is one which produces a decent ELBO for both.  Under the original expectation, on the other hand, the MAP value of c corresponds to a setup that has many orders of magnitude higher probability and so the best network setup is the one that does well for the MAP value of c, with the other instance being of little importance.  We thus see that the original target and the lower bound behave very differently for a given p(c).

Thankfully, the target in Eq 8 is a potentially reasonable thing to do in its own right (maybe actually more so that the original formulation), because the averaging over c is somewhat spurious given you are optimizing its mean parameters anyway.  It is easy to show that the ""optimum"" p(c) for a given (\theta,\phi) is always a delta function on the value of c which has the highest ELBO_c.  As Fig 3 shows, the optimization of the parameters of p(c) practically leads to such a collapse.  This is effectively desirable behavior given the overall aims and so averaging over values of c is from a modeling perspective actually a complete red herring anyway.  It is very much possible that the training procedure represented by Eq 8 is (almost by chance) a good approach in terms of learning the optimal configuration for c, but if this is the case it needs to be presented as such, instead of using the current argument about putting a prior on c and constructing a second lower bound, which is a best dubious and misleading, and at worst complete rubbish.  Ideally, the current explanations would be replaced by a more principled justification, but even just saying you tried Eq 8 and it worked well empirically would be a lot better than what is there at the moment.

%%% Encoder dependency structure does not match the generative model %%%

My second major concern is that the dependency structure used for the encoder is incorrect from the point of view of the generative model.  Namely, a dependency structure on the prior does not induce the same dependency structure on the posterior.  In general, just because z_1 and z_2 are independent, doesn't mean that z_1 and z_2 are independent given x (see e.g. Bishop).  Consequently, the encoder in your setup will be incapable of correctly representing the posterior implied by the generative model.  This has a number of serious practical and theoretical knock-on effects, such as prohibiting the bound becoming tight, causing the encoder to indirectly impact the expressivity of the generative model etc.  Note that this problem is not shared with the Ladder VAE, as there the Markovian dependency structure means produces a special case where the posterior and prior dependency structure is shared.

As shown in https://arxiv.org/abs/1712.00287 (a critical missing reference more generally), it is actually possible to derive the dependency structure of the posterior from that of the prior.  I think in your case their results imply that the encoder needs to be fully connected as the decoder can induce arbitrary dependencies between the latent variables.  I am somewhat surprised that this has not had more of an apparent negative impact on the empirical results and I think at the very very least the paper needs to acknowledge this issue.  I would recommend the authors run experiments using a fully connected encoder and the Graph-VAE decoder (and potentially also vice verse).  Should this approach perform well, it would represent a more principled approach to replace the old on from a generative model perspective.  Should it not, it would provide an empirical justification for what is, in essence, a different restriction to that of the learned prior structure: it is conceivably actually the case that these encoder restrictions induce the desired decoder behavior, but this is distinct to learning a particular dependency structure in the generative model.

%%% Specifics of model and experiments %%%

Though the paper is generally very easy to read, there as some key areas where the explanations are overly terse.  In particular, the explanation surrounding the encoding was difficult to follow and it took me a while to establish exactly what was going on; I am still unsure how \tilde{\psi} and \hat{\psi} are combined.  I think a more careful explanation here and a section giving more detail in the appendices would both help massively.

I was not clear on exactly what was meant by the FC-VAE.  I do not completely agree with the assertion that a standard VAE has independent latents.  Though the typical choice that the prior is N(0,I) obviously causes the prior to have independent latents, as explained earlier, this does not mean the latents are independent in the posterior.  Furthermore, the encoder implicitly incorporates these dependencies through its mean vector, even if it uses a diagonal covariance (which is usually rather small anyway).  What is actually changed from this by the FC-VAE?  Are you doing some kind of normalizing flow approach here?  If so this needs proper explanation.

Relatedly, I am also far from convinced by the arguments presented about why the FC-VAE does worse at the end of the experiments.  VAEs attempt to maximize a marginal likelihood (through a surrogate target) and a model which makes no structural assumptions will generally have a lower marginal likelihood than one which makes the correct structural assumptions.  It is thus perfectly reasonable that when you learn dependency structures, you will get a higher marginal likelihood than if you presume none.  I thus find your arguments about local optima somewhat speculative and further investigation is required.

%%% Experiments %%%

Though certainly not terrible, I felt that the experimental evaluation of the work could have been better.  The biggest issue I have is that no error bars are given for the results, so it is difficult to assess the robustness of the Graph-VAE.  I think it would be good to add convergence plots with error bars to see how the performance varies with time and provide an idea of variability.  More generally, the experiment section overall feels more terse and rushed than the rest of the paper, with some details difficult to find or potentially even straight up missing.

Though Fig 3 is very nice, it would be nice to have additional plots seeing qualitatively what happens with the latent space.  E.g. on average what proportion of the c tend to zero?  Is the same dependency structure always learned?  What do the dataset encodings look like?  Are there noticeable qualitative changes in samples generated from the learned models?  I would be perfectly happy for the paper to extend over the 8 pages to allow more results addressing these questions.","The sentiment of the review is mixed but leans towards positive. The reviewer acknowledges the core concept as good and the results as impressive, but also points out significant technical issues and misgivings with the experiments. The sentiment score is therefore 20. The politeness of the language is generally high, as the reviewer uses phrases like 'I think,' 'I would recommend,' and 'I would be very happy,' which indicate a polite and constructive tone. The politeness score is therefore 80.",20,80
"- Summary: This paper proposes verification algorithms for a class of convex-relaxable specifications to evaluate the robustness of the network under adversarial examples. Experimental results are shown for semantic specifications for CIFAR, errors in predicting sum of two digits and conservation of energy in a simple pendulum. 

- Clarity and correctness: It is a well-written and well-organized paper. Notations and expressions are clear. The math seems to be correct. 

- Significance: The paper claims to have introduced a class of convex-relaxable specifications which constitute specifications that can be verified using a convex relaxation. However, as described later in the paper, it is limited to feed-forward neural networks with ReLU and softmax activation functions and quadratic parts (it would be better to tone down the claims in the abstract and introduction parts.)

- Novelty: The idea of accounting for label semantics and quadratic expressions when training a robust neural network is important and very practical. This paper introduces some nice ideas to generalize linear verification functions to a larger class of convex-relaxable functions, however, it seems to be more limited in practice than it claims and falls short in presenting justifying experimental results.

** More detailed comments:

** The idea of generalizing verifications to a convex-relaxable set is interesting, however, applying it in general is not very clear -- as the authors worked on a case by case basis in section 3.1. 

** One of my main concerns is regarding the relaxation step. There is no discussion on the effects of the tightness of the relaxation on the actual results of the models; when in reality, there is an infinite pool of candidates for 'convexifying' the verification functions. It would be nice to see that analysis as well as a discussion on how much are we willing to lose w.r.t. to the tightness of the bounds -- especially when there is a trade-off between better approximation to the verification function and tightness of the bound. 

** I barely found the experimental results satisfying. To find ""reasonable"" inputs to the model, authors considered perturbing points in the test set. However, I am not sure if this is a reasonable assumption when there would be no access to test data points when training a neural network with robustness to adversarial examples. And if bounding them is a very hard task, I am wondering if that is a reasonable assumption to begin with.

** It is hard to have a sense of how good the results are in Figure 1 due to lack of benchmark results (I could not find them in the Appendix either.)

** The experimental results in section 4.4 are very limited. I suggest that the authors consider running more experiments on more data sets and re-running them with more settings (N=2 for digit sums looks very limited, and if increasing N has some effects, it would be nice to see them or discuss those effects.)

** Page 2, ""if they do a find a proof"" should be --> ""if they do find a proof"" 
** Page 5, ""(as described in Section (Bunel et al., 2017; Dvijotham et al., 2018)"", ""Section"" should be omitted.

******************************************************
After reading authors' responses, I decided to change the score to accept. It got clear to me that this paper covers broader models than I originally understood from the paper. Changing the expression to general forms was a useful adjustment in understanding of its framework. Comparing to other relaxation technique was also an interesting argument (added by the authors in section H in the appendix). Adding the experimental results for N=3 and 4 are reassuring.
One quick note: I think there should be less referring to papers on arxiv. I understand that this is a rapidly changing area, but it should not become the trend or the norm to refer to unpublished/unverified papers to justify an argument.","The sentiment of the review is generally positive, as indicated by phrases like 'well-written and well-organized paper' and 'nice ideas to generalize linear verification functions.' However, there are also several critical points and suggestions for improvement, which slightly temper the overall positivity. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer uses polite and constructive language throughout, such as 'it would be better to tone down the claims' and 'I suggest that the authors consider running more experiments.' Thus, the politeness score is 90.",60,90
"This paper propose to add an OT-based regularization term to seq-2-seq models in order to better take into account the distance between the generated and the reference and/or source sentences, allowing one to capture the semantic meaning of the sequences. Indeed, it allows the computation of a distance between embeddings of a set of words, and this distance is then used to define a penalized objective function.
The main issue with this computation is that it provides a distance between a set of words but not a sequence of words. The ordering is then not taken into account. Authors should discuss this point in the paper.
Experiments show an improvement of the method w.r.t. not penalized loss.

Minor comments:
- in Figure 1, the OT matching as described in the text is not the solution of eq (2) but rather the solution of eq. (3) or the entropic regularization (the set of ""edges"" is higher than the admissible highest number of edges).
- Introduction ""OT [...] providing a natural measure of distance for sequences comparisons"": it is not clear why this statement is true. OT allows comparing distributions, with no notion of ordering (see above).  
- Table 1: what is NMT?
- first paragraph, p7: how do you define a ""substantial"" improvement of the scores?
- how do you set parameter $\gamma$ in the experiments? Why did you choose \beta=0.5 for the ipot algorithm?

","The sentiment of the review is moderately positive. The reviewer acknowledges the contribution of the paper and the improvement shown in the experiments, but also points out a significant issue regarding the computation of distance between sets of words rather than sequences. The politeness of the language is high, as the reviewer uses polite and constructive language, providing specific suggestions for improvement without being harsh or dismissive.",40,80
"The paper studies few-host learning in a transductive setting: using meta learning to learn to propagate labels from training samples to test samples. 

There is nothing strikingly novel in this work, using unlabeled test samples in a transductive way seem to help slightly. However, the paper does cover a setup that I am not aware that was studied before. The paper is written clearly, and the experiments seem solid. 

Comments: 
-- What can be said about how computationally demanding the procedure is? running label propagation within meta learning might be too costly. 
-- It is not clear how the  per-example scalar sigma-i is learned. (for Eq 2)
-- solving Eq 3 by matrix inversion does not scale. Would be best to also show results using iterative optimization 




","The sentiment of the review is moderately positive. The reviewer acknowledges that the paper covers a setup that has not been studied before and appreciates the clarity of writing and solid experiments, but also notes that there is nothing strikingly novel and that the improvements are slight. Therefore, the sentiment score is 30. The politeness of the language is quite high; the reviewer uses polite language and constructive criticism without any harsh or rude remarks. Therefore, the politeness score is 80.",30,80
"The authors made several claims and provide suggestions on training binary networks, however, they are not proved or theoretically analyzed.  The empirical verification of the proposed hypothesis was viewed as weak as the only two datasets used are small datasets MNIST and CIFAR-10, and the used network architectures are also limited. Much more rigorous and thorough testing is required for an empirical paper which proposes new claims. 

Take the first claim ""end-to-end training of binary networks crucially relies on the optimiser taking advantage of second moment gradient estimates"" as an example. As it is known that choice of optimizer is highly dependent on the specific dataset and network structure, it is not convincing to jump to this conclusion using the observations on two small datasets and limited network architectures.  E.g, many binarization papers use momentum for ImageNet dataset with residual networks. Does Adam also outperforms momentum in this case? Similarly, it is also hard for me to judge whether the other conclusions made about weight/gradient clipping, the momentum in batch normalization and learning rate, are correct or not.

Some minor issues are:
1. In Figure 4, different methods are not run to convergence, and the comparison may not be fair.
2. The second paragraph in section 4: ""It can be seen that not clipping weights when learning rates are large can completely halt the optimisation (red curve in Figure 5)."" However, in figure 5, the red curve is ""Clipping gradients"", which one is correct?
3. The authors propose a recipe for faster training of binary networks, is there experiments supporting that training networks with the proposed recipe is faster than the original counterpart? ","The sentiment of the review is generally negative, as the reviewer points out several significant weaknesses in the paper, such as the lack of theoretical analysis and insufficient empirical verification. The reviewer also questions the validity of the claims made by the authors due to the limited datasets and network architectures used. Therefore, the sentiment score is -60. The politeness of the language used is relatively neutral to slightly polite. The reviewer does not use harsh or rude language and provides constructive criticism, but the tone is firm and critical. Therefore, the politeness score is 20.",-60,20
"In distributed optimisation, it is well known that asynchronous methods outperform synchronous methods in many cases. However, the questions as to whether (and when) asynchronous methods can be shown to have any speed-up, as the number of nodes increases, has been open. The paper under review answers the question in the affirmative and does so very elegantly.

I have only a few minor quibbles and a question. There are some recent papers that could be cited:
http://proceedings.mlr.press/v80/zhou18b.html
http://proceedings.mlr.press/v80/lian18a.html
https://nips.cc/Conferences/2018/Schedule?showEvent=11368
and the formatting of the bibliography needs to be improved. 

In the synchronous case, some of the analyses extend to partially separable functions, e.g.:
https://arxiv.org/abs/1406.0238
and citations thereof. Would it be possible to extend the present work in that direction?","The review starts with a positive sentiment, acknowledging the significance and elegance of the paper's contribution. The reviewer mentions only a few minor quibbles and provides constructive feedback, which indicates a generally positive sentiment. The language used is polite and respectful, suggesting improvements and asking questions in a considerate manner.",90,95
"The paper extends previous work on differentiable placticity to include neuro modulation by parameterizing the learning rate of Hebbs update rule. In addition, the authors introduce retroactive modulation that basically allows the system to delay incorporation of plasticity updates via so eligibility traces. Experiments are performaed on 2 simple toy datasets and a simple language modeling task. A newly developed cue-reward association task shows the clear limitations of basic plasticity and how modulation can resolve this. Slight improvements can also be seen on a simple maze navigation task as well as on a basic language modeling dataset.

Overall I like the motivation, provided background information and simplicity of the approach. Furthermore, the cue-reward experiment seems to be a well designed show case for neuro-modulation. However, as the authors acknowledge the overall simplicity of the tasks being evaluated with mostly marginal improvements makes the overall evaluation fall short. Unfortunately the paper doesn't provide any qualitative analysis on how modulation is employed by the models after training. Therefore, although I would like to see an extended version of this paper at the conference, without further experiments and analysis I see the current version rather as an interesting workshop contribution.


Strengths:
- motivation: the natural extension of previous work on differentiable plasticity based on existing knowledge from neuro science is an important next step
- cue reward experiment exemplifies limitations of current plasticity approaches and clearly shows the potential benefits of neuro modulation
- maze navigation shows incremental benefits over non-modulated plasticity
- thorough experimentation
- clipping-trick is a neat observation 


Weaknesses:
- evaluation: only on toy tasks (which includes PTB), no real world tasks
- very incremental improvements on PTB over a very simple baseline (far from SotA)
- evaluated models (feed-forward NNs and LSTMs) are very basic and far from current SotA architectures
- no qualitative analysis on how modulation is actually use by the systems. E.g., when is modulation strong and when is it not used 


Comments:
- perplexity improvements of less than 1.3 points over plasticity alone (which is the actual baseline for this paper) can hardy be called ""significant"". Even though they might be statistically significant (meaning nothing more than the two models being statistically different), minor architectural changes can lead to such improvements. Furthermore PTB is not a ""challenging"" LM benchmark.

","The sentiment of the review is mixed but leans towards positive. The reviewer appreciates the motivation, background information, and simplicity of the approach, as well as the well-designed cue-reward experiment. However, they also point out significant weaknesses, such as the simplicity of the tasks, marginal improvements, and lack of qualitative analysis. Therefore, the sentiment score is 20. The language used in the review is polite and constructive, offering specific recommendations and acknowledging the strengths of the paper while pointing out areas for improvement. Therefore, the politeness score is 80.",20,80
"The paper is well written and structured, presenting the problem clearly and accurately. It contains considerable relevant references and enough background knowledge. It nicely motivates the proposed approach, locates the contributions in the state-of-the-art and reviews related work. It is also very honest in terms of how it differs on the technical level from existing approaches. 
The paper presents interesting and novel findings to further state-of-the-art’s understanding on how language concepts are represented in the intermediate layers of deep convolutional neural networks, showing that channels in convolutional representations are selectively sensitive to specific natural language concepts. It also nicely discusses how concepts granularity evolves with layers’ deepness in the case of natural language tasks.
What I am missing, however, is an empirical study of concepts coverage over multiple layers, studying the multiple occurrences of single concepts at different layers, and a deeper dive on the rather noisy elements of natural language and the layers’ activation dynamics towards such elements.
Overall, however, the ideas presented in the paper are interesting and original, and the experimental section is convincing. My recommendation is to accept this submission.
","The sentiment of the review is highly positive, as evidenced by phrases like 'well written and structured,' 'interesting and novel findings,' and 'ideas presented in the paper are interesting and original.' The reviewer also recommends accepting the submission, which further indicates a positive sentiment. Therefore, the sentiment score is 90. The politeness of the language is also very high, with the reviewer using polite and constructive language throughout, such as 'nicely motivates,' 'honest in terms of,' and 'my recommendation is to accept.' There are no rude or harsh comments, so the politeness score is 100.",90,100
"This paper proposed a framework that can improve the performances of reinforcement learning algorithms in tasks that involve long time horizons and sparse rewards. The proposed method is a hierarchical reinforcement learning framework that can use policy hierarchies with an arbitrary number of levels. To improve the sample efficiency in the learning process, the authors proposed to apply the hindsight experience replay mechanism at each level. Also, in order to avoid the actor function to output an unrealistic subgoal, the authors proposed the subgoal testing technique. 

The proposed framework is interesting. And the example in Section 3.5 clearly demonstrate how this framework works. The authors proposed to solve a UMDP by solving a hierarchy of k UMDPs, where k is a hyperparameter. Each level (except for the bottom most level) will output subgoal states for the next level to achieve. This hierarchy is reasonable and easy to understand. However, from the definition on Page 3, it seems that all of the intermediate levels i (the case where 0 < i < k - 1) has the same state and action spaces. They are all equal to the state set of the original UMDP. Under this setting, will adding more intermediate levels help improve the performance a lot? We only see results with at most one intermediate level in the experiment. It will be better if the authors can show results on more levels (i.e. at least 4 levels in total). 

Moreover, the proposed framework has a policy limit parameter T, meaning that we only consider if a goal can be achieved within T steps or not, at each level. Is this parameter necessary to be the same for all levels? Also, it will be better if the author can show some results on the performances of the proposed method according to different values for T. The authors also proposed the subgoal testing technique. It is also better if the authors can show some performance comparisons on the cases with and without this technique.

The authors claimed that their method has the advantage over some existing HRL methods (e.g. the Option-Critic Architecture [1]) that their method can use policy hierarchies with an arbitrary number of levels while these methods can only use policy hierarchies with two levels. In the experiments, the authors also showed that, in some of their experiments, the 3-layer agent (with 2 subgoal layers) outperforms the 2-layer agent (with 1 subgoal layer), under their framework. However, the authors did not compare their 2-layer agent's performance with these existing HRL methods, which means that we do not know if their 3-layer agent's performance is better than that of some of the existing 2-layer agent methods. In addition to that, as I mentioned before, it is better if the authors can show experiment results on more levels (e.g. 4 levels and more) to show that their method can perform well in practice for policy hierarchies with many levels.


References:

[1] Pierre-Luc Bacon, Jean Harb, and Doina Precup. The option-critic architecture. CoRR, abs/1609.05140, 2016.

","The sentiment of the review is generally positive, as the reviewer finds the proposed framework interesting and acknowledges the clarity of the example provided. However, the reviewer also raises several concerns and suggestions for improvement, which slightly temper the overall positive sentiment. Therefore, the sentiment score is 50. The politeness of the language is high, as the reviewer uses polite phrases such as 'it will be better if' and 'it is also better if,' and provides constructive feedback without being harsh or dismissive. Thus, the politeness score is 80.",50,80
"The paper first examines the objective function optimized in MAML and E-MAML and interprets the terms as different credit assignment criteria. MAML takes into account the dependences between pre-update trajectory and pre-update policy, post-update trajectory and post-update policy by forcing the gradient of the two policies to be aligned, which results in better learning properties. 
Thought better, the paper points out MAML has incorrect estimation for the hessian in the objective. To address that, the paper propose a low variance curvature estimator (LVC). However, naively solving the new objective with LVC with TRPO is computationally prohibitive. The paper addresses this problem by proposing an objective function that combines PPO and a slightly modified version of LVC.

Quality: strong, clarity:strong, originality:strong, significance: strong,

Pros:
- The paper provides strong theoretical results. Though mathematically intense, the paper is written quite well and is easy to follow.
- The proposed method is able to improve in sample complexity, speed and convergence over past methods.
- The paper provides strong empirical results over MAML, E-MAML. They also show the effective of the LVC objective by comparing LVC over E-MAML using vanilla gradient update.
- Figure 4 is particularly interesting. The results show different exploration patterns used by different method and is quite aligned with the theory.  
Cons:
- It would be nice to add more comparison and analysis on the variance. Since LVC is claimed to reduce variance of the gradient, it would be nice to show more empirical evidences that supports this. (By looking at Figure 2, although not directly related, LVC-VPG seems to have pretty noisy behaviour)

","The sentiment of the review is highly positive, as indicated by the use of words like 'strong' and 'quite well' to describe the quality, clarity, originality, and significance of the paper. The reviewer also highlights several pros, such as strong theoretical and empirical results, and only provides one minor con, which is a suggestion for additional analysis. Therefore, the sentiment score is 90. The politeness of the language is also very high, as the reviewer uses polite and constructive language throughout the review, even when pointing out the single con. Therefore, the politeness score is 90.",90,90
"
========
Summary
========

The paper deals with the training of neural networks for classification or sequence generation tasks, using a cross-entropy loss. Minimizing the cross-entropy means maximizing the predicted probabilities of the ground-truth classes (averaged over the samples). The authors introduce a ""complementary entropy"" loss with the goal of minimizing the predicted probabilities of the complementary (incorrect) classes. To do that, they use the average of sample-wise entropy over the complement classes. By maximizing this entropy, the predicted complementary probabilities are encouraged to be equal and therefore, the authors claim that it neutralizes them as the number of classes grows large. The proposed training procedure, named COT, consists of alternating between the optimization of the two losses.

The procedure is tested on image classification tasks with different datasets (CIFAR-10, CIFAR-100, Street View House Numbers, Tiny ImageNet and ImageNet), machine translation (training using IWSLT dataset, validation and test using TED tst2012/2013 datasets), and speech recognition (Gooogle Commands dataset). In the experiments, COT outperforms state-of-the-art models for each task/dataset.

Adversarial attacks are also considered for the classification of images of CIFAR-10: using the Fast Gradient Sign and Basic Iterative Fast Gradient Sign methods on different models, adversarial examples specifically designed for each model, are generated. Then results of these models are compared to COT on these examples. The authors admit
that the results are biased since the adversarial attacks only target part of the COT objective, hence more accurate comparisons should be done in future work.

===========================
 Main comments and questions
===========================

End of page 1: ""the model behavior for classes other than the ground  truth stays unharnessed and not well-defined"". The probabilities  should still sum up to 1, so if the ground truth one is maximized,  the others are actually implicitly minimized. No?

Page 3, sec 2.1: ""optimizing on the complement entropy drives ŷ_ij to 1/(K − 1)"". I believe that it drives each term ŷ_ij /(1 − ŷ_ig ) to be equal to 1/(K-1). Therefore, it drives ŷ_ij to (1 − ŷ_ig)/(K-1) for j!=g.

This indeed flattens the ŷ_ij for j!=g, but the effect on ŷ_ig is not controlled. In particular this latter can decrease. Then in the next step of the algorithm, ŷ_ig will be maximized, but with no explicit control over the complementary probabilities. There are two objectives that are optimized over the same variable theta. So the question is, are we sure that this procedure will converge? What prevents situations where the probabilities will alternate between two values? 

For example, with 4 classes, we look at the predicted probabilities of a given sample of class 1:
Suppose after step 1 of Algo 1, the predicted probabilities are:  0.5 0.3 0.1 0.1 
After step 2:  0.1 0.3 0.3 0.3
Then step 1: 0.5 0.3 0.1 0.1
Then step 2: 0.1 0.3 0.3 0.3
And so on... Can this happen? Or why not? Did the algorithm have trouble converging in any of the experiments?

Sec 3.1:
""additional efforts for tuning hyper-parameters might be required for optimizers to achieve the best performance"": Which hyper-parameters are considered here? If it is the learning rate, why not use a different one, tuned for each objective?

Sec 3.2:
The additional optimization makes each training iteration more costly. How much more? How do the total running times of COT compare to the ones of the baselines? I think this should be mentioned in the paper.

Sec 3.4:
As the authors mention, the results are biased and so the comparison is not fair here. Therefore I wonder about the  relevance of this section. Isn't there an easy way to adapt the attacks to the two objectives to be able to illustrate the conjectured robustness of COT? For example, naively having a two steps perturbation of the input: one based on the gradient of the primary objective and then perturb the result using the gradient of the complementary objective?

===========================
Secondary comments and typos
===========================

Page 3, sec 2.1: ""...the proposed COT also optimizes the complement objective for neutralizing the predicted probabilities..."", using maximizes instead of optimizes would be clearer.

In the definition of the complement entropy, equation (2), C takes as parameter only y^hat_Cbar but then in the formula, ŷ_ig appears. Shouldn't C take all \hat_y as an argument in this case?

Algorithm 1 page 4: I find it confusing that the (artificial) variable that appears in the argmin (resp. argmax) is theta_{t-1}
(resp. theta'_t) which is the previous parameter. Is there a reason for this choice?

Sec 3:
""We perform extensive experiments to evaluate COT on the tasks"" --> COT on tasks

""compare it with the baseline algorithms that achieve state-of-the-art in the respective domain."" --> domainS

""to evaluate the model’s robustness trained by COT when attacked"" needs reformulation.

""we select a state- of-the-art model that has the open-source implementation"" --> an open-source implementation

Sec 3.2:
Figure 4: why is the median reported and not the mean (as in Figure 3, Tables 2 and 3)?

Table 3 and 4: why is it the validation error that is reported and not the test error?

Sec 3.3:
""Neural machine translation (NMT) has populated the use of neural sequence models"": populated has not the intended meaning.

""We apply the same pre-processing steps as shown in the model"" --> in the paper?

Sec 3.4:
""We believe that the models trained using COT are generalized better"" --> ""..using COT generalize better""

""using both FGSM and I-FGSM method"" --> methodS

""The baseline models are the same as Section 3.2."" --> as in Section 3.2.

""the number of iteration is set at 10."" --> to 10

""using complement objective may help defend adversarial attacks."" --> defend against

""Studying on COT and adversarial attacks.."" --> could be better formulated

References: there are some inconsistencies (e.g.: initials versus first name)


Pros
====
- Paper is clear and well-written
- It seems to me that it is a new original idea
- Wide applicability
- Extensive convincing experimental results

Cons
====
- No theoretical guarantee that the procedure should converge
- The training time may be twice longer (to clarify)
- The adversarial section, as it is,  does not seem relevant for me

","The sentiment of the review is generally positive, as indicated by the pros listed at the end, such as the paper being clear, well-written, and presenting a new original idea with wide applicability and extensive experimental results. However, there are some critical points raised, such as the lack of theoretical guarantee for convergence and the relevance of the adversarial section. Therefore, the sentiment score is not fully positive but leans towards the positive side. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, providing specific feedback and suggestions without being rude or dismissive.",60,90
"Summary

The authors introduce BabyAI, a platform with the aim to study grounded language learning with a human in the loop. The platform includes a *simulated* human expert (bot) that teaches a neural learner. The current domain used in a 2D gridworld and the synthetic instructions require the agent to navigate the world (including unlocking doors) and move objects to specified locations. They also introduce ""Baby Language"" to give instructions to the agent as well as to automatically verify their execution.

The paper includes a detailed description of the minigrid env with the included tasks and instruction language set. 

Authors trained small and large LSTM models on the tasks on a variety of standard learning approaches, using pure exploration (RL) and imitation from a synthetic bot (IL). They show IL is much more data efficient than RL in this domain as well. Also, a curriculum approach is evaluated (pre-train on task 1-N, then train on task N+1).  

Pro
- Human-in-the-loop research is an exciting direction.
- The language instruction set is a starting point for high-level human instructions. 

Con
- It is still unclear how to effectively learn with human-in-the-loop. The authors don't actually evaluate 
1) how well the bot imitates a human, or 
2) how an actual human would interact and speed up learning. 
All experiments are done with standard learning approaches with a synthetic bot. 
- The authors assume that human feedback comes as instructions or demonstrations. These are not the only forms of feedback possible (e.g., preferences). (Does the platform easily support those?)

Reproducibility
- Open-sourcing the platform is a good contribution to the community.
","The sentiment of the review is generally positive, as indicated by the acknowledgment of the exciting direction of human-in-the-loop research and the contribution of open-sourcing the platform. However, there are notable criticisms regarding the lack of evaluation with actual human interaction and the limited forms of feedback considered. Therefore, the sentiment score is 40. The language used in the review is polite and constructive, offering specific recommendations without being harsh or dismissive. Thus, the politeness score is 80.",40,80
"Overall impression: 
I think that this is a well written interesting paper with strong results. One thing I’d have liked to see a bit more is an explanation of why self imitation is more effective than standard policy gradient? Where does the extra supervision/stability come from, and can this be explained intuitively? I’ve suggested some small changes/clarifications to be made inline, and a few more comparisons to add. But overall, I very much like this line of work and I recommend accepting this paper. 


Abstract:
We demonstrate its effectiveness on a number of challenging tasks. -> be more specific.

The term single-timestep optimization is not very clear. Can this be clarified?

they are more widely applicable in the sparse or episodic reward settings -> it is likely important to mention that they are agnostic to horizon of the task.

Related works: 
Guided Policy Search also does divergence minimization. GAIL considers the imitation learning work as a sort of divergence minimization problem as well, which should be explicitly mentioned. Other work for good exploration include DIAYN (Eysenbach et al 2018). The difference in resulting updates between (Oh et al) and this work should be clearly discussed in the methods section. 

“we learn shaped, dense rewards”-> too early in the paper for this to make sense. can provide some contextt

Section 2.2:
fully decides the expected return -> clarify this a bit. I think what you mean is that the dynamics are wrapped into this already, so it accounts for this, but this can be made explicit.

Small typos in appendix 5.1 (r should be replaced by the density ratio)

The update in (3) seems quite similar to what GAIL would do. What is the difference there? Or is the difference just in the fact that the experts are chosen from “self” experiences. 

How is the priority list threshold and size chosen?
 Would a softer version of the priority queue update do anything useful? Or would it just reduce to policy gradient when weighted by rewards?

Appendices are very clear and very informative while being succinct!

I would have liked to see Appendix 5.3 in the main text (maybe a shorter form) to clarify the whole algorithm 

What is psi in appendix 5.3? The algorithm remains a bit unclear without this clarification

Experiments. 
Only 1 question to answer in this section is labelled? Put 2) and 3) appropriately. 

Can a comparison to Oh et al 2018 be added to this for the sake of completeness? Also can this be compared to using novelty/curiosity based exploration schemes?

Can the authors comment on why the method reaches higher asymptotic performance but is often slower in the beginning than the other methods in Fig 3. ","The review starts with a positive overall impression of the paper, highlighting that it is well-written and interesting with strong results. The reviewer expresses a desire for more explanation in certain areas but still recommends accepting the paper. This indicates a positive sentiment. The language used throughout the review is constructive and polite, with suggestions for improvements and clarifications presented in a respectful manner.",80,90
"This paper proposes to accelerate architecture search by replacing the expensive inner loop (wherein candidate architectures are trained to completion) with a HyperNetwork which predicts the weights of candidate architectures, as in SMASH. Contrary to SMASH, this work employs a Graph neural network to allow for the use of any feedforward architecture, enabling fast architecture search through parameter prediction using highly performant search spaces. The authors test their system and show that performance using Graph HyperNet-generated weights correlates with performance when trained normally. The authors benchmark their method against competing approaches (""traditional"" NAS techniques which incur the full expense of the inner loop, and one-shot techniques which learn a large model then select architectures by searching for paths in said model) and show competitive performance.

This is a solid technical contribution with a well-designed set of experiments. While the novelty is not especially high, the paper does a good job of synthesizing existing tools and achieves reasonably strong results with much less compute, making for a strong entry into the growing table of fast architecture search methods. I argue in favor of acceptance.

Notes:

-Whereas SMASH is limited to architectures which can be described with its proposed encoding scheme, GHNs only requires that the architecture be represented as a graph (which, to my knowledge, means it can handle any feedforward architecture). 

-Section 4.2: It's not entirely clear how this setup allows for variable sized kernels or variable #channels. Is the output of H simply as large as the largest allowable parameter tensor, and sliced as necessary? A snippet of code might be more illuminating here than a set of equations. Additionally (I may have missed this in the text) is the #channels in each node held fixed with a predfined pattern, or also searched for? Are the channels for each node within a block allowed to vary relative to one another?

-Do you sample a new, random architecture at every SGD step during training of the GHN?

-I have no expertise in graph neural networks, and I cannot judge the efficacy of this scheme wrt other GNN techniques, nor can I judge the forward-backward message passing scheme of section 4.4. If another reviewer has expertise in this area and can provide an evaluation that would be great.
 
-GPU-days is an okay metric, but it's also problematic, since it will of course depend on the choice of GPU (e.g. you can achieve a 10x speedup just from switching from a 600-series to a V100! How does using 4 GPUS for 1 hour compare to 1 GPU for 4 hours? How does this change if you have more CPU power and can load data faster? What if you're using a DL framework which is faster than your competitor's?) Given that the difference here is an order of magnitude, I don't think it matters, but if authors begin to optimize for GPU-milliseconds then it will need to be better standardized.
 
-Further empirical evidence showing the correlation between approximate performance and true performance is also strong. I very much like that this study has been run for a method based on finding paths in a larger model (ENAS) and shows that ENAS' performance does indeed correlate with true performance, *but* not perfectly, something which (if I recall correctly) is not addressed in the original paper.
 
-It is worth noting that for ImageNet-Mobile and CIFAR-10 they perform on par with the top methods but tend to use more parameters.  

-I like figures 3 and 4, the comparisons against MSDNet and random networks as a function of op budget is good to see.

-Table 4 shows that the correlation is weaker (regardless of method) for the top architectures, which I don't find surprising as I would expect the variation in performance amongst top architectures to be lower. It would be interesting to also see what the range of error rates are; I would expect that the correlation is higher when the range of error rates across the population of architectures is large, as it is easier to distinguish very bad architectures from very good architectures. Distinguishing among a set of good-to-very-good architectures is likely to be more difficult.

-For Section 5.3, I found the choice to use unseen architectures a little bit confusing. I think that even for this study, there's no reason to use a held-out set, as we seek to scrutinize the ability of the system to approximate performance even with architectures it *does* see during training. 

-How much does the accuracy drop when using GHN weights? I would like to see a plot showing true accuracy vs. accuracy with GHN weights for the random-100 networks, as using approximations like this typically results in the approximated weights being substantially worse. I am curious to see just how much of a drop there is.

-Section 5.4: it's interesting that performance is stronger when the GHN only sees a few (7) nodes during training, even though it sees 17 nodes during testing. I would expect that the best performance is attained with training-testing parity. Again, as I do not have any expertise in graph neural networks, I'm not sure if this is common (to train on smaller graphs and generalize to larger ones), so if the authors or another reviewer would like to comment and further illuminate this behavior, that would be helpful.

Some typos:

Abstract: ""prematured""  should be ""premature""

Introducton, last paragraph: ""CNN networks."" CNN already stands for Convolutional Neural Network.","The review starts with a positive sentiment, highlighting the solid technical contribution and well-designed experiments of the paper. The reviewer acknowledges that the novelty is not especially high but still argues in favor of acceptance, indicating a positive overall sentiment. The language used throughout the review is polite and constructive, with specific suggestions and questions aimed at improving the paper rather than criticizing it harshly. The reviewer also admits their limitations in expertise in certain areas, which is a humble and respectful approach.",80,90
"This paper presents a very interesting interpretation of the neural network architecture.

I think what is remarkable is that the author presents the general results (beyond the dense layer) including a convolutional layer by using the higher-order tensor operation.
Also, this research gives us new insight into the network architecture, and have the potential which leads to many interesting future directions. 
So I think this work has significant value for the community.

The paper is clearly written and easy to follow in the meaning that the statement is clear and enough validation is shown. (I found some part of the proof are hard to follow.)

\questions
In the experiment when you mention about ""embed solvers as a replacement to their corresponding blocks of layers"", I wonder how they are implemented. About the feedforward propagation, I guess that for example, the prox operator is applied multiple times to the input, but I cannot consider what happens about the backpropagation of the loss.

In the experiment, the author mentioned that  ""what happens if the algorithm is applied for multiple iterations?"". From this, I guess the author iterate the corresponding algorithms several times, but actually how many times were the iterations or are there any criterion to stop the algorithm?

\minor comments
The definition of \lambda_max below Eq(3) are not shown, thus should be added.","The sentiment of the review is highly positive, as indicated by phrases like 'very interesting interpretation,' 'remarkable,' 'significant value for the community,' and 'clearly written and easy to follow.' The reviewer appreciates the novelty and potential impact of the work. The politeness of the language is also very high, with the reviewer using respectful and constructive language throughout. Even when pointing out areas that were hard to follow or needed clarification, the reviewer did so in a polite and considerate manner.",90,95
"For binary layers, how to calculate and backpropagate gradients is a big problem, particularly for the binary neural networks. To solve the problem, this paper proposes an unbiased and low variance augment-REINFORCE-merge (ARM) estimator. With the help of an appropriate reparameterization, the antithetic sampling in an augmented space can be used to drive a variance-reduction mechanism. The experimental results show that ARM estimator converges fast, has low computational complexity, and provides advanced prediction performance.

This paper is well-organized. The motivation of the proposed model is well-driven and algorithm is articulated clearly. Meanwhile, the derivations and analysis of the proposed algorithm are correct. The experimental results show that the proposed model is better than the other existing methods.

A few minor revision are list below.
1) In figure 1, it seems difficult to decide which one is better from the trace plots of the true/estimated gradients. Also, why the author choose to compare the REINFORCE instead of REBAR and RELAX, since REBAR and RELAX improve on REINFORCE by introducing stochastically estimated control variates. Also, about trace plots of the loss functions, I am curious why REINFORCE has a big vibration during 1500~2000 iterations. 
2) About Table 2, are all compared methods in the same experimental settings?
","The review is highly positive about the paper, praising its organization, motivation, clarity, correctness, and experimental results. The sentiment score is therefore high. The language used is polite and constructive, offering specific suggestions for improvement without any negative or rude remarks.",90,90
"### post rebuttal### authors addressed most of my concerns and greatly improved the manuscript and hence I am increasing my score. 
 
Summary: 

The paper introduces a static formulation for unbalanced optimal transport by learning simultaneously a transport map T and scaling factor xi .

Some theory is given to relate this formulation to unbalanced transport metrics such as Wasserstein Fisher Rao metrics  for e.g. Chizat et al 2018.  

The paper proposes to  relax the constraint in the proposed static formulation using a divergence.  furthermore using a bound on the divergence , the final discrepancy proposed  is written as a min max problem between the witness function f of the divergence and the transport map T , and scaling factor xi. 

An algorithm is given to find the optimal map T as a generator in GAN and to learn the scaling factor  and the witness function of the divergence with a neural network paramterization , the whole optimized with stochastic gradient. 

Small experimentation on image to image transportation with unbalance in the classes is given and show how the scaling factor behaves wrt to this kind of unbalance. 


Novelty and  Originality:

The paper claims that there are no known static formulations known with a scaling factor and a transport map learned simultaneously. We refer the authors to Unbalanced optimal Transport: Geometry and Kantrovich Formulation Chizat et al 2015. In page 19 in this paper Equation 2.33 a similar formulation to Equation 4 in this paper is given. (Note that phi corresponds to T and lambda to xi). This is known as the monge formulation of unbalanced optimal transport. The main difference is that the authors here introduce a stochastic map T and an additional probabilty space Z. Assuming that the mapping is deterministic those two formulations are equivalent. 

Correctness: 

The metric defined in this paper can be written as follow and corresponds to a generalization of the monge formulation in chizat 2015 :
L(mu,nu)= inf_{T, xi}  int   c_1(x,T_x(z) ) xi(x) lambda(z) dmu(x)  + int c_2(x_i(x)) dmu(x)
                        		 s.t T_# (xi mu)=nu
In order to get a kantorovich formulation out of this chizat et al 2015 defines semi couplings and the formulation is given in Equations 3.1 page 20. 

This paper proposes to relax  T_# (xi mu)=nu with D_psi (xi \mu, \nu) and hence proposes to use:

L(mu,nu)= inf_{T, xi} int   c_1(x,T_x(z) ) xi(x) lambda(z) dmu(x)  + int c_2(x_i(x)) dmu(x)+  D_psi (xi \mu, \nu)

Lemma 3.2 of the paper claims that the formulation above corresponds to the Kantrovich formulation of unbalanced transport. I doubt the correctness of this:

Inspecting the proof of Lemma 3.2 L \geq W seems correct to me, but it is unclear what is going on in the proof of the other direction? The existence of T_x is not well supported by rigorous proof or citation? Where does xi come from in the third line of the equalities in the end of page 14? I don’t follow the equalities written at the end of page 14. 

Another concern is the space Z, how does the metric depend on this space? should there be an inf on all Z?

Other comments:

- Appendix A is good wish you baselined your experiments with those algorithms. 

- The experiments don’t show any benefit for learning the scaling factor, are there any applications in biology that would make a better case for this method?

- What was the architecture used to model T, xi, and f?

- Improved training dynamics in the appendix, it seems you are ignoring the weighting while optimizing on theta? than how would the weighing be beneficial ?","The sentiment of the review is generally positive, as indicated by the initial statement that the authors have addressed most concerns and improved the manuscript. However, the reviewer does express some doubts and concerns about specific aspects of the paper, such as the correctness of Lemma 3.2 and the dependence on space Z. Therefore, the sentiment score is not fully positive but leans towards the positive side. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, even when pointing out issues and making suggestions.",60,80
"This paper presents a method for learning about the parts and motion dynamics of a video by trying to predict future frames.  Specifically, a model based on optical flow is defined, noting that the motion of hierarchically related parts are additive.  Flow fields are represented using an encoder/decoder architecture and a binary structural matrix encodes the representations between parts.  This matrix is predicted given the previous frame and flow field.  This is then used to estimate a new flow field and generate a possible future frame.  The system is trained to predict future frames using an L2 loss on the predicted image and motion field and regularized to prefer more compact and parsimonious representations.

The method is applied to synthetic datasets generated by moving shapes or MNIST digits and shown to work well compared to some recent baseline methods for part segmentation and hierarchy representation.  It is also applied and qualitatively evaluated for future frame prediction on an atari video game and human motion sequences.  The qualitative evaluation shows that part prediction is plausible but the results for future frame prediction are somewhat unclear as there are no baseline comparisons for this aspect of the task.

Overall the approach seems very interesting and well motivated.   However, the experimental comparisons are limited and baselines are lacking.  Further, some relevant related work is missing.

Specific concerns:
- Motion segmentation has been studied for a long time in computer vision, a comparison against some of these methods may be warranted.  See, e.g., Mangas-Flores and Jepson, CVPR 2013.
- There is some missing related work on learning part relations.  See, e.g., Ross, et al IJCV 2010 and Ross and Zemel JMLR 2006.
- There is also some missing work on future frame prediction.  In particular, PredNet seems relevant to discuss in the context of this work and as a baseline comparison method.  See Lotter et al ICLR 2017.
- A reasonable baseline might be simply to apply the previous frames motion field to generate the next frame.  This would be a good comparison to include.
- The ""Human Studies"" section is very unclear.  How is ""same tree structure"" defined exactly and how were humans asked to annotate the tree structure?  If it's about the hierarchical relationship, then I would expect humans to always be pretty consistent with the hierarchy of body parts and suggests that the model is doing relatively poorly.  If it's some other way, then this needs to be clarified.  Further, how was this study performed?  If this section can't be thoroughly explained it should be removed from the paper as it is at best confusing and potentially very misleading.
- The system only considers a single frame and flow-field for part prediction.  From this perspective, the effectiveness of the method seems somewhat surprising.
- The system takes as input both a frame and a flow field.  I assume that flow field is computed between I0 and I1 and not I1 and I2, however this is never specified anywhere I can find in the manuscript.  If this is not the case, then the problem setup is (almost) trivial.

","The sentiment of the review is generally positive, as indicated by phrases like 'the approach seems very interesting and well motivated.' However, the reviewer also points out several significant limitations and missing comparisons, which tempers the overall positivity. Therefore, the sentiment score is 30. The politeness of the language is quite high, as the reviewer uses polite and constructive language throughout, such as 'may be warranted' and 'if this section can't be thoroughly explained it should be removed.' Therefore, the politeness score is 80.",30,80
"As a reviewer I am expert in learning in structured data domains. 
The paper proposes a quite complex system, involving many different choices and components, for obtaining chemical compounds with improved properties starting from a given corpora. 
Overall presentation is good, although some details/explanations/motivations are missing. I guess this was due to the need to keep the description of a quite complex system in the given space limit. Such details/explanations/motivations could, however, have been inserted in the appendix. As an example, let consider the description of the decoding of the junction tree. In that section, it is not explained when the decoding process stops. My understanding is that this is when, being in the root node, the choice is to go back to the parent (that does not exist). In the same section, it is not explicitly discussed that the probability to select between adding a node or going back to the parent should have a different distribution according to ""how many"" nodes have been generated before, i.e. we do not want to have a high probability to ""go back"" at the beginning of the decoding, while I guess it is desirable that such probability increases proportionally with the number of generated nodes. This leads to an issue that I personally think is important: the paper does lack an explicit probabilistic modelling of the different involved components, which may help for a better understanding of all the assumptions made in the construction of the proposed system. 
The complexity of the proposed system is actually an issue since the author(s) do not attempt (except for  the presence or absence of the adversarial scaffold regularization and the number of trials in appendix) an analysis of the influence of the different components (and corresponding hyper-parameters). 
Reference to previous relevant work seems to be complete.
I think the paper is relevant for ICLR (although there is no explicit analysis of the obtained hidden representations) and of interest for a good portion of attendees.

Minor issues:
- Tree and Graph Encoding: asynchronous update implies that T should be a multiple of the diameter of the input graph to guarantee a proper propagation of information across the graph. A discussion about that would be needed.
- eq.(6): \mathbb{u}^d is not defined.
- Section 3.3:
   - first paragraph is not clear. An example and/or figure is needed to understand the argument, which is related to the presence of cycles.
  - the definition of f(G_i) involves  \mathbb{x}_u. I guess they should be  \mathbb{x}_u^G.
  - not clear how the log-likelihood of ground truth subgraphs is computed given that the predicted junction tree, especially at the beginning of training, may be way different from the correct one. Moreover, what is the assumed bias of this choice ?
- Table I: please provide an explanation of why using a larger value for \delta does provide worst performance than a smaller value. From an optimisation point of view it should provide at least an as good performance. This is a clear indication that the used procedure is suboptimal.
- diversity could be influenced by the cardinality of the sample. Is this false ? please discuss why diversity is (not) biased versus larger sets.","The review starts with a positive acknowledgment of the paper's overall presentation and relevance to the field, which indicates a positive sentiment. However, it also points out several missing details and areas for improvement, which tempers the positivity. Therefore, the sentiment score is moderately positive. The language used in the review is constructive and provides specific recommendations without being harsh or dismissive, indicating a high level of politeness.",40,80
"The paper proposes a modification to the traditional conditional GAN objective (which minimizes GAN loss as well as either L1 or L2 pixel-wise reconstruction losses) in order to promote diverse, multimodal generation of images. The modification involves replacing the L1/L2 reconstruction loss -- which predicts the first moment of a pixel-wise gaussian/laplace (respectively) likelihood model assuming a constant spherical covariance matrix -- with a new objective that matches the first and second moments of a pixel-wise gaussian/laplace likelihood model with diagonal covariance matrix. Two models are proposed for matching the first and second moments - the first one involves using a separate network to predict the moments from data which are then used to match the generator’s empirical estimates of the moments (using K samples of generated images). The second involves directly matching the empirical moment estimates using monte carlo.

The paper makes use of a well-established idea - modeling pixel-wise image likelihood with a diagonal covariance matrix i.e. heteroscedastic variance (which, as explained in [1], is a way to learn data-dependent aleatoric uncertainty). Following [1], the usage of first and second moment prediction is also prevalent in recent deep generative models (for example, [2]) i.e. image likelihood models predict the per-pixel mean and variance in the L2 likelihood case, for optimizing Equation 4 from the paper. Recent work has also attempted to go beyond the assumption of a diagonal covariance matrix (for example, in [3] a band-diagonal covariance matrix is estimated). Hence, the only novel idea in the paper seems to be the method for matching the empirical estimates of the first and second moments over K samples. The motivation for doing this makes intuitive sense since diversity in generation is desired, which is also demonstrated in the results.

Section specific comments:
- The loss of modality of reconstruction loss (section 3.2) seems like something which doesn’t require the extent of mathematical and empirical detail presented in the paper. Several of the cited works already mention the pitfalls of using reconstruction loss.

- The analyses in section 4.4 are sound in derivation but not so much in the conclusions drawn. It is not clear that the lack of existence of a generator that is an optimal solution to the GAN and L2 loss (individually) implies that any learnt generator using GAN + L2 loss is suboptimal. More explanation on this part would help.

The paper is well written, presents a simple idea, complete with experiments for comparing diversity with competing methods. Some theoretical analyses do no directly support the proposition - e.g. sections 3.2 and 4.4 in my specific comments above. Hence, the claim that the proposed method prevents mode collapse (training stability) and gives diverse multi-modal predictions is supported by experiments and intuition for the method, but not so much theoretically. However, the major weakness of the paper is the lack of novelty of the core idea.

=== Update after rebuttal:
Having read through the other reviews and the author's rebuttal, I am unsatisfied with the rebuttal and I do not recommend accepting the paper. My rating has decreased accordingly.

The reasons for my recommendation, after discussion with other reviews, are -- (1) lack of novelty and (2) weak theoretical results (some justification of which was stated in my initial review above). Elaborating more on the second point, I would like to mention some points which came up during the discussion with other reviewers: The theoretical result which states that not using reconstruction loss given that multi-modal outputs are desired is a weaker result than proving that the proposed method is actually effective in what it is designed to do. There are empirical results to back that claim, but I strongly believe that the theoretical results fall short and feel out of place in the overall justification for the proposed method. This, along with my earlier point of lack of novelty are the basis for my decision.


References:
[1] Kendall, Alex, and Yarin Gal. ""What uncertainties do we need in bayesian deep learning for computer vision?."" Advances in neural information processing systems. 2017.
[2] Bloesch, M., Czarnowski, J., Clark, R., Leutenegger, S., & Davison, A. J. (2018). CodeSLAM-Learning a Compact, Optimisable Representation for Dense Visual SLAM. CVPR 2018.
[3] Dorta, G., Vicente, S., Agapito, L., Campbell, N. D., & Simpson, I. (2018, February). Structured Uncertainty Prediction Networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.","The sentiment of the review is mixed but leans towards the negative side. Initially, the reviewer acknowledges the paper's well-written nature and the intuitive sense of the proposed method. However, the reviewer points out significant weaknesses, such as the lack of novelty and weak theoretical results, which ultimately lead to a recommendation against accepting the paper. The sentiment score is therefore slightly negative. The politeness of the language is high; the reviewer provides constructive criticism and specific recommendations without being rude or dismissive. The language is professional and respectful throughout the review.",-30,80
"This paper used a complex-valued network to learn the modified complex ratio mask with a weighted SDR loss for the speech enhancement task. It can get good enhancement performance.

For me, the complex-valued network is already there and weighted SDR loss is not difficult to think. The modified complex ratio mask is a bit interesting. However, I think it better to compare with [Donald S Williamson et al] where the hyperbolic tangent compression is used.

Apart from the objective metrics, a human listening test using MOS or preference score should be conducted.

On Fig 3, the unbounded complex mask might suffer from the infinity problem leading to training failure. However, on table 2, the performance of the unbounded mask is quite close to your method. It is a bit strange for me.

The total idea is good, but the novelty is not much.

","The sentiment of the review is somewhat mixed. The reviewer acknowledges that the paper achieves good enhancement performance and finds the modified complex ratio mask interesting. However, they also express that the complex-valued network and weighted SDR loss are not novel and suggest additional comparisons and tests. Therefore, the sentiment score is slightly positive. The language used is generally polite, with phrases like 'I think it better to compare' and 'It is a bit strange for me,' indicating a respectful tone, even though the reviewer points out several areas for improvement.",20,60
"In this paper, authors analyze the performance of neural networks and polynomial kernels in a natural regression setting where the data enjoys sparse latent structure, and the labels depend in a simple way on the latent variables. They give an almost-tight theoretical analysis of the performance and verify them with simulations.
 
Authors motivated the theoretical analysis from typical applications, for which the desired function can be only important to be approximated well on the relevant part of domains. Instead of formalizing the above problem, authors tackle a particular simple question. However, it is not easy to understand the relationships between the two problems.
 
A regression task is studied where the data has a sparser latent structure. Authors measure the performance of estimators via the expected reconstruction error from theoretical perspectives for both two-layer ReLU network and polynomial kernel. Empirical experiments will be even better to show the performance of some applications consistent with the theoretical results.","The review starts with a neutral to slightly positive sentiment, acknowledging the authors' efforts in analyzing the performance of neural networks and polynomial kernels and verifying their theoretical analysis with simulations. However, the sentiment becomes more neutral as the reviewer points out the difficulty in understanding the relationship between the problems tackled and the motivation behind the theoretical analysis. The reviewer also suggests that empirical experiments would improve the paper, indicating a constructive but critical stance. The language used is polite, as the reviewer provides constructive feedback without using harsh or negative language.",10,80
"This paper considers the problem of transferring motor skills from multiple experts to a student policy. To this end, the paper proposes two approaches: (1) an approach for policy cloning that learns to mimic the (local) linear feedback behavior of an expert (where the expert takes the form of a neural network), and (2) an approach that learns to compress a large number of experts via a latent space model. The approaches are applied to the problem of one-shot imitation from motion capture data (using the CMU motion capture database). The paper also considers an extension of the proposed approach to the problem of high-level planning; this is done by treating the learned latent space as a new action space and training a high-level policy that operates in this space. 

Strengths:
S1. The supplementary video was clear and helpful in understanding the setup.
S2. The paper is written in a generally readable fashion.
S3. The related work section does a thorough job of describing the context of the work.  

However, I have some significant concerns with the paper. These are described below. 

Significant concerns:
C1. My biggest concern is that the paper does not make a strong case for the benefits of LPFC over simpler strategies. The results in Figure 3 demonstrate that a linear feedback policy computed along the expert's nominal trajectory performs as well as (and occasionally even better than) LPFC. This is quite concerning.
C2. Moreover, as the authors themselves admit, ""while LPFC did not work quite as well in the full-scale model as cloning from noisy rollouts, we believe it holds promise insofar as it may be useful in rollout-limited settings..."". However, the paper does not present any theoretical/experimental evidence that would suggest this.
C3. Another concern has to do with the two-step procedure for LPFC (Section 2.2), where the first step is to learn an expert policy (in the form of a neural network) and the second step is to perform behavior cloning by finding a policy that tries to match the local behavior of the expert (i.e., finding a policy that attempts to produce similar actions as the expert policy linearized about the nominal trajectory). This two-step procedure seems unnecessary; the paper does not make a case for why the expert policies are not chosen as linear feedback controllers (along nominal trajectories) in the first place.
C4. The linearization of the expert policy produced in (1) may not lead to a stabilizing feedback controller and could easily destabilize the system. It is easy to imagine cases where the expert neural network policy maintains trajectories of the system in a tube around the nominal trajectory, but whose linearization does not lead to a stabilizing feedback controller. Do you see this in practice? If not, is there any intuition for why this doesn't occur? If this doesn't occur in practice, this would suggest that the expert policies are not highly nonlinear in the neighborhood of states under consideration (in which case, why learn neural network experts in the first place instead of directly learning a linear feedback controller as the expert policy as suggested in C3?)
C5. I would have liked to have seen more implementation details in Section 3. In particular, how exactly was the linear feedback policy along the expert's nominal trajectory computed? Is this the same as (2)? Or did you estimate a linear dynamical model (along the expert's nominal trajectory) and then compute an LQR controller? More details on the architecture used for the behavioral cloning baseline would also have been helpful (was this a MLP? How many layers?)

Minor comments:
- There are some periods missing at the end of equations (eqs. (1), (2), (6), (8), (9)).","The sentiment of the review is mixed. The reviewer acknowledges some strengths of the paper, such as the clarity of the supplementary video, the readability of the paper, and the thoroughness of the related work section. However, the reviewer also expresses significant concerns about the paper's contributions, the necessity of the proposed methods, and the lack of theoretical or experimental evidence supporting the claims. Therefore, the sentiment score is slightly negative. The politeness of the language is high; the reviewer uses polite and constructive language throughout the review, even when pointing out significant concerns.",-20,80
"This is a strong theory paper and I recommend to accept.

Paper Summary:
This paper studies the problem of learning a two-layer fully connected neural network where both the output layer and the first layer are unknown. In contrast to previous papers in this line which require the input distribution being standard Gaussian, this paper only requires the input distribution is symmetric. This paper proposes an algorithm which only uses polynomial samples and runs in polynomial time. 
The algorithm proposed in this paper is based on the method-of-moments framework and several new techniques that are specially designed to exploit this two-layer architecture and the symmetric input assumption.
This paper also presents experiments to illustrate the effectiveness of the proposed approach (though in experiments, the algorithm is slightly modified).

Novelty:
1. This paper extends the key observation by Goel et al. 2018 to higher orders (Lemma 6). I believe this is an important generalization as it is very useful in studying multi-neuron neural networks.
2. This paper proposes the notation, distinguishing matrix, which is a natural concept to study multi-neuron neural networks in the population level.
3. The “Pure Neuron Detector” procedure is very interesting, as it reduces the problem of learning a group of weights to a much easier problem, learning a single weight vector. 

Clarity:
This paper is well written.

Major comments:
My major concern is on the requirement of the output dimension. In the main text, this paper assumes the output dimension is the same as the number of neurons and in the appendix, the authors show this condition can be relaxed to the output dimension being larger than the number of neurons. This is a strong assumption, as in practice, the output dimension is usually 1 for many regression problems or the number of classes for classification problems. 
Furthermore, this assumption is actually crucial for the algorithm proposed in this paper. If the output dimension is small, then the “Pure Neuron Detection” step does work. Please clarify if I understand incorrectly. If this is indeed the case, I suggest discussing this strong assumption in the main text and listing the problem of relaxing it as an open problem. 


Minor comments:
1. I suggest adding the following papers to the related work section in the final version:
https://arxiv.org/abs/1805.06523
https://arxiv.org/abs/1810.02054
https://arxiv.org/abs/1810.04133
https://arxiv.org/abs/1712.00779
These paper are relatively new but very relevant. 

2. There are many typos in the references. For example, “relu” should be ReLU.




","The review starts with a strong positive sentiment, recommending the paper for acceptance and highlighting its strengths and contributions. The language used is polite and constructive, offering specific suggestions for improvement without being dismissive or harsh. The reviewer acknowledges the novelty and clarity of the paper, while also providing major and minor comments in a respectful manner.",90,95
"In this paper, the authors proposed a unified framework which computes spectral decompositions by stochastic gradient descent. This allows learning eigenfunctions over high-dimensional spaces and generating to new data without Nystrom approximation. From technical perspective, the paper is good. Nevertheless, I feel the paper is quite weak from the perspective of presentation. There are a couple of aspects the presentation can be improved from. 

(1) I feel the authors should formally define what a Spectral inference network is, especially what the network is composed of, what are the nodes, what are the edges, and the semantics of the network and what's motivation of this type of network.

(2) In Section 3, the paper derives a sequence of formulas, and many of the relevant results were given without being proven or a reference. Although I know the results are most likely to be correct, it does not hurt to make them rigorous. There are also places in the paper, the claim or statement is inclusive. For example, in the end of Section 2.3, ""if the distribution p(x) is unknown, then constructing an explicitly orthonormal function basis may not be possible"". I feel the authors should avoid this type of handwaving claims.  

(3) The authors may consider summarize all the technical contribution in the paper. 

One specific question:

What's Omega above formula (6)? Is it the support of x? Is it continuous or discrete? Above formula (8), the authors said ""If omega is a graph"". It is a little bit confusing there. ","The sentiment of the review is mixed. The reviewer acknowledges the technical soundness of the paper but expresses significant concerns about its presentation. This results in a sentiment score of -20, as the overall tone leans slightly negative due to the emphasis on weaknesses. The politeness score is 60, as the reviewer uses polite language, such as 'I feel' and 'the authors may consider,' which softens the critique and maintains a respectful tone.",-20,60
"The paper examines an architectural feature in GAN generators -- self-modulation -- and presents empirical evidence supporting the claim that it helps improve modeling performance. The self-modulation mechanism itself is implemented via FiLM layers applied to all convolutional blocks in the generator and whose scaling and shifting parameters are predicted as a function of the noise vector z. Performance is measured in terms of Fréchet Inception Distance (FID) for models trained with and without self-modulation on a fairly comprehensive range of model architectures (DCGAN-based, ResNet-based), discriminator regularization techniques (gradient penalty, spectral normalization), and datasets (CIFAR10, CelebA-HQ, LSUN-Bedroom, ImageNet). The takeaway is that self-modulation is an architectural feature that helps improve modeling performance by a significant margin in most settings. An ablation study is also performed on the location where self-modulation is applied, showing that it is beneficial across all locations but has more impact towards the later layers of the generator.

I am overall positive about the paper: the proposed idea is simple, but is well-explained and backed by rigorous evaluation. Here are the questions I would like the authors to discuss further:

- The proposed approach is a fairly specific form of self-modulation. In general, I think of self-modulation as a way for the network to interact with itself, which can be a local interaction, like for squeeze-and-excitation blocks. In the case of this paper, the self-interaction allows the noise vector z to interact with various intermediate features across the generation process, which for me appears to be different than allowing intermediate features to interact with themselves. This form of noise injection at various levels of the generator is also close in spirit to what BigGAN employs, except that in the case of BigGAN different parts of the noise vector are used to influence different parts of the generator. Can you clarify how you view the relationship between the approaches mentioned above?
- It’s interesting to me that the ResNet architecture performs better with self-modulation in all settings, considering that one possible explanation for why self-modulation is helpful is that it allows the “information” contained in the noise vector to better propagate to and influence different parts of the generator. ResNets also have this ability to “propagate” the noise signal more easily, but it appears that having a self-modulation mechanism on top of that is still beneficial. I’m curious to hear the authors’ thoughts in this.
- Reading Figure 2b, one could be tempted to draw a correlation between the complexity of the dataset and the gains achieved by self-modulation over the baseline (e.g., Bedroom shows less difference between the two approaches than ImageNet). Do the authors agree with that?
","The sentiment of the review is positive, as indicated by phrases like 'I am overall positive about the paper' and the acknowledgment that the proposed idea is 'well-explained and backed by rigorous evaluation.' The reviewer also provides constructive feedback and asks for clarifications in a respectful manner, which suggests a polite tone. The language used is considerate and aims to engage the authors in a meaningful discussion about their work.",80,90
"This paper addresses a novel variant of AutoML, to automatically learn and generate optimization schedules for iterative alternate optimization problems. The problem is formulated as a RL problem, and comprehensive experiments on four various applications have demonstrated that the optimization schedule produced can guide the task model to achieve better quality of convergence, more sample-efficient, and the trained controller is transferable between datasets and models. Overall, the writing is quite clear, the problem is interesting and important, and the results are promising. 

Some suggestions:

1. What are the key limitations of AutoLoss ? Did we observe some undesirable behavior of the learned optimization schedule, especially when transfer between different datasets or different models ? More discussions on these questions can be very helpful to further understand the proposed method.  

2. As the problem is formulated as an RL problem, which is well-known for its difficulty in training, did we encounter similar issues? More details in the implementation can be very helpful for reproducibility. 

3. Any plan for open source ? ","The review starts with a positive sentiment, highlighting the novelty, clarity, importance, and promising results of the paper. The suggestions provided are constructive and aimed at improving the paper, without any negative or harsh language. Therefore, the sentiment score is high. The language used is polite and professional, asking questions and making suggestions in a respectful manner, which warrants a high politeness score.",90,95
"The authors propose in this paper an approach for learning models with tractable approximate posterior inference. The paper is well motivated (fast and accurate posterior inference) and the construction of the solutions (invertible architecture, appending vectors to input and output, choice of cost function) well described. From my understanding, it seems this method is also to be compatible with other methods of approximate Bayesian Computation (ABC).
Concerning the experimental section:
- The Mixture of Gaussians experiment is a good illustration of how the choice of cost functions influences the solution. However, I do not understand how are the *discrete* output y is handled. Is it indeed a discrete output (problem with lack of differentiability)? Softwax probability? Other modelling choice? 
- The inverse kinematics is an interesting illustration of the potential advantage of this method over conditional VAE and how close it is to ABC which can be reasonably computed for this problem.
- For the medical application, INN outperforms other methods (except sometimes for ABC, which is far more expensive, or direct predictor, which doesn’t provide uncertainty estimates) over some metrics such as the error on parameters recovery (Table 1), calibration error, and does indeed have a approximate posterior which seems to correspond to the ABC solution better. I’m not sure I understand what we are supposed to learn from the astrophysics experiments.
The method proposed and the general problem it aims at tackling seem interesting enough, the toy experiments demonstrates well the advantage of the method. However, the real-world experiments are not necessarily the easiest to read. 
EDIT: the concerns were mostly addressed in the revision. ","The review starts with a positive sentiment, highlighting that the paper is well-motivated and the solutions are well-described. The reviewer also acknowledges the compatibility of the method with other approaches. However, the review does point out some areas of confusion and suggests improvements, particularly in the experimental section. The language used is polite and constructive, aiming to help the authors improve their work. The reviewer also notes that most concerns were addressed in the revision, which adds to the positive sentiment.",70,80
"RNNs are difficult to explain, understand and analyze due to the continuous-valued memory vectors and observations features they use. Thus, this paper attempts to extract finite representation from RNNs so as to better interpret or understand RNNs. They introduce a new technique called Quantized Bottleneck Insertion to extract Moore Machines (MM). The extracted MM can be analyzed to improve the understanding of memory use and general behavior on the policies. The experiments on synthetic datasets and six Atari games validate the effectiveness of the proposal.

Here are my detailed comments:
Interpreting or understanding RNNs is a very interesting and important topic since RNNs and their variants like LSTM, GRU are widely used in different domains such as reinforcement learning, sentiment analysis, stock market prediction, natural language processing, etc. The more understandable on RNNs, the more trustful on them. In this paper, the authors try to extract more interpretable representation of RNNs, namely Moore Machines (MM). MM is actually a classical finite state automaton. The authors mention that (Zeng et al., 1993) is the most similar work to theirs. In fact a series of works have been proposed to extract finite state automaton, which is similar to (Zeng et al., 1993) such as [1], [2], [3], etc. I think the authors could make the related works more complete by incorporating these literatures I mentioned.
 
Besides, I think this work is a good application of the idea of extraction of RNNs on reinforcement learning since no works have introduced this idea into this domain as far as I know. The authors use the autoencoder named as QBN to quantize the space of hidden states. This is a good operation of clustering or quantizing the space of hidden states since it can be tuned to make the final performance better. The authors also incorporate the minimization of MM to show the probability of shrinking memory which can also make the extracted MM more interpretable. As a result, the policy represented by MM is intuitive and vivid.
 
Nevertheless, there is an obvious weak point in this paper. Specifically, the authors claim that the main contribution of this paper is to introduce an approach for transforming RNNs to finite state representations. But I do not see any comparisons between the proposed methods and other relative methods such as the method proposed by (Zeng et al., 1993) to show the effectiveness or improvement of the proposed method. I suggest the authors could incorporate comparisons to make the results more convincing.
 
[1] C. W. Omlin and C. L. Giles, ""Extraction of rules from discrete-time recurrent neural networks,"" Neural Networks, vol. 9, no. 1, pp. 41–52, 1996.
[2] C. W. Omlin and C. L. Giles, ""Constructing deterministic finite-state automata in recurrent neural networks,"" Journal of the ACM, vol. 43, no. 6, pp. 937-972, 1996.
[3] A. Cleeremans, D. Servan-Schreiber, and J. L. McClelland. ""Finite state automata and simple recurrent networks."" Neural computation, vol. 1, no. 3, pp. 372-381, 1989.","The review starts with a positive sentiment towards the paper, highlighting the importance and interest in the topic of interpreting RNNs. The reviewer acknowledges the novelty of applying the idea of extracting finite state representations to reinforcement learning and appreciates the techniques used by the authors. However, the review also points out a significant weakness, suggesting the need for comparisons with other methods to validate the effectiveness of the proposed approach. The language used is constructive and polite, offering specific suggestions for improvement without being dismissive or harsh.",70,80
"This paper proposes a method for functional regularization for training neural nets, such that the sequence of neural nets during training is stable in function space. Specifically, the authors define a L2 norm (i.e., a Hilbert norm), which can be used to measure distances in this space between two functions. The authors argue that this can aid in preventing catastrophic forgetting, which is demonstrated in a synthetic multi-task variant of MNIST.   The authors also show how to regularize the gradient updates to be conservative in function space in standard stochastic gradient style learning, but with rather inconclusive empirical results.  The authors also draw upon a connection to the natural gradient.


***Clarity***

The paper is reasonably well written.  I think the logical flow could be improved at places.   I think the major issue with clarity is the title.  The authors use the term ""regularizing"" in a fairly narrow sense, in particular regularizing the training trajectory to be stable in function space.  However, the more dominant usage for regularizing is to regularize the final learned function to some prior, which is not studied or even really discussed in the paper.

Detailed comments:

-- The notation in Section 2 could be cleaned up.  The use of \mu is a bit disconnected from the rest of the notation.  

-- Computing the empirical L2 distance accurately can also be NP hard.  There's no stated guarantee of how large N needs to be to have a good empirical estimate.  Figure 3 is nice, but I think a more thorough discussion on this point could be useful.

-- L2-Space was never formally defined.  

-- Section 2.1 isn't explained clearly.  For instance, in the last paragraph, the first sentence states ""the networks are initialized at very different point"", and halfway into the paragraph a sentence states ""all three initializations begin at approximately the same point in function space."".  The upshot is that Figure 1 doesn't crisply capture the intuition the authors aim to convey.


***Originality***

Strictly speaking, the proposed formulation is novel as far as I am aware.  However, the basic idea has been the air for a while.  For instance, there are some related work in RL/IL on functional regularization:
-- https://arxiv.org/abs/1606.00968

The proposed formulation is, in some sense, the obvious thing to try (which is a good thing).  The detailed connection to the natural gradient is nice.  I do wish that the authors made stronger use of properties of a Hilbert space, as the usage of Hilbert spaces is fairly superficial.  For instance, one can apply operators in a Hilbert space, or utilize an inner product.  It just feels like there was a lost opportunity to really explore the implications.


***Significance***

This is the place where the contributions of this paper are most questionable.  While the multi-task MNIST experiments are nice in demonstrating resilience against catastrophic forgetting, the experiments are pretty synthetic.  What about a more ""real"" multi-task learning problem?

More broadly, it feels like this paper is suffering from a bit of an identity crisis.  It uses regularizing in a narrow sense to generate conservative updates.  It argues that this can help in catastrophic forgetting.  It also shows how to employ this to construct the standard bounded-update gradient descent rules, although without much rigorous discussion for the implications.  There are some nice empirical results on a synthetic multi-task learning task, and inconclusive results otherwise.  There's a nice little discussion on the connection to the natural gradient.  It argues that that this form of regularization lives in a Hilbert space, but the usage of a Hilbert space is fairly superficial.  All in all, there are some nice pieces of work here and there, but it's all together neither here or there in terms of an overall contribution.    


***Overall Quality***

I think if the authors really pushed one of the angles to a more meaningful contribution, this paper would've been much stronger.  As it stands, the paper just feels too scattered in its focus, without a truly compelling result, either theoretically or empirically.","The sentiment of the review is mixed, leaning towards neutral with some positive and negative elements. The reviewer acknowledges the novelty and some strengths of the paper but also points out significant weaknesses and areas for improvement. Therefore, the sentiment score is slightly negative. The language used is generally polite and constructive, with suggestions for improvement rather than harsh criticism, so the politeness score is high.",-20,80
"This paper tests a number of untrained sentence representation models - based on random embedding projections, randomly-initialized LSTMs, and echo state networks - and compares the outputs of these models against influential trained sentence encoders (SkipThought, InferSent) on transfer and probing tasks. The paper finds that using the trained encoders yields only marginal improvement over the fully untrained models.

I think this is a strong paper, with a valuable contribution. The paper sheds important light on weaknesses of current methods of sentence encoding, as well as weaknesses of the standard evaluations used for sentence representation models - specifically, on currently-available metrics, most of the performance achievements observed in sentence encoders can apparently be accomplished without any encoder training at all, casting doubt on the capacity of these encoders - or existing downstream tasks - to tap into meaningful information about language. The paper establishes stronger and more appropriate baselines for sentence encoders, which I believe will be valuable for assessment of sentence representation models moving forward. 

The paper is clearly written and well-organized, and to my knowledge the contribution is novel. I appreciate the care that has been taken to implement fair and well-controlled comparisons between models. Overall, I am happy with this paper, and I would like to see it accepted. 

Additional comments:

-A useful addition to the reported results would be confidence intervals of some kind, to get a sense of the extent to which the small improvements for the trained encoders are statistically significant.

-I wonder about how the embedding projection method would compare to simply training higher-dimensional word embeddings from the start. Do we expect substantial differences between these two options?","The sentiment of the review is highly positive. The reviewer describes the paper as 'strong' and 'valuable,' and expresses a desire to see it accepted. The reviewer also appreciates the novelty, clarity, and organization of the paper. Therefore, the sentiment score is 100. The politeness of the language is also very high. The reviewer uses phrases like 'I appreciate the care,' 'Overall, I am happy with this paper,' and 'I would like to see it accepted,' which are all very polite and respectful. Therefore, the politeness score is 100.",100,100
"This paper introduced a proximal approach to optimize neural networks by linearizing the network output instead of the loss function. They demonstrate their algorithm on multi-class hinge loss, where they can show that optimal step size can be computed in close form without significant additional cost. Their experimental results showed competitive performance to SGD/Adam on the same network architectures. 

1. Figure 1 is crucial to the algorithm design as it aims to prove that Loss-Preserving Linearization (LPL) preserves information on loss function. While the authors provided numerical plots to compare it with the SGD linearization, I personally prefer to see some analytically comparsion between SGD linearization and LPL even on the simplest case. An appendix with more numerical comparisons on other loss functions might also be insightful. 
2. It seems LPL is mainly compared to SGD for convergence (e.g. Fig 2). In Table 2 I saw some optimizers end up with much lower test accuracy. Can the authors show the convergence plots of these methods (similar to Figure 2)?","The sentiment of the review is generally positive, as the reviewer acknowledges the introduction of a new approach and its competitive performance compared to established methods like SGD and Adam. However, the reviewer also provides constructive criticism and suggestions for improvement, which indicates a balanced and thoughtful perspective. Therefore, the sentiment score is 60. The language used in the review is polite and professional, with phrases like 'I personally prefer' and 'might also be insightful,' which show respect for the authors' work while suggesting improvements. Therefore, the politeness score is 80.",60,80
"The authors introduce a class of quasi-hyperbolic algorithms that mix SGD with SGDM (or similar with Adam) and show improved empirical results. They also prove theoretical convergence of the methods and motivate the design well. The paper is well-written and contained the necessary references. Although I did feel that the authors could have better compared their method against the recent AggMom (Aggregated Momentum: Stability Through Passive Damping by Lucas et al.). Seems like there are a few similarities there. 

I enjoyed reading this paper and endorse it for acceptance. The theoretical results presented and easy to follow and state the assumptions clearly. I appreciated the fact that the authors aimed to keep the paper self-contained in its theory. The numerical experiments are thorough and fair. The authors test  the algorithms on an extremely wide set of problems ranging from image recognition (including CIFAR and ImageNet), natural language processing (including the state-of-the-art machine translation model), and reinforcement learning (including MuJoCo). I have not seen such a wide comparison in any paper proposing training algorithms before. Further, the numerical experiments are well-designed and also fair. The hyperparameters are chosen carefully, and both training and validation errors are presented. I also appreciate that the authors made the code available during the reviewing phase. Out of curiosity, I ran the code on some of my workflows and found that there was some improvement in performance as well. 


","The sentiment of the review is highly positive, as indicated by phrases such as 'I enjoyed reading this paper and endorse it for acceptance' and 'The numerical experiments are thorough and fair.' The reviewer also appreciates the theoretical results, the wide range of numerical experiments, and the availability of the code. Therefore, the sentiment score is 90. The politeness of the language is also very high, with the reviewer using appreciative and respectful language throughout the review, such as 'I appreciated the fact that the authors aimed to keep the paper self-contained in its theory' and 'I also appreciate that the authors made the code available during the reviewing phase.' Therefore, the politeness score is 100.",90,100
"This paper proposes to parameterize the weight matrices of neural nets using the SVD, with approximate orthogonality enforced on the singular vectors using Orthogonal Regularization (as opposed to e.g. the Cayley transform or optimizing on the Stiefel manifold), allowing for direct, efficient control over the spectra. The method is applied to GAN discriminators to stabilize training as a natural extension of Spectral Normalization. This method incurs a slight memory and compute cost and achieves a minor performance improvement over Spectral Normalization on two benchmark image generation tasks.

I'm a bit back and forth on this paper. On the one hand, I think the ideas this paper proposes are very interesting and could provide a strong basis off which future work can be built--the extension of spectral normalization to further study and manipulation of the spectra is natural and very promising. However, the results obtained are not particularly strong, and as they stand do not, in my opinion, justify the increased compute and memory cost of the proposed methods. The paper's presentation also wavers between being strong (there were some sections I read and immediately understood) and impenetrable (there were other sections which I had to read 5-10 times just to try and grip what was going on).

Ultimately, my vote is for acceptance. I think that we should not throw out a work with interesting and potentially useful ideas just because it does not set a new SOTA, especially when the current trend with GANs seems to suggest that top performance comes at a compute cost that all but a few groups do not have access to. With another editing pass to improve language and presentation this would be a strong, relevant paper worthy of the attention of the ICLR community.

My notes:

-The key idea of parameterizing matrices as the SVD by construction, but using a regularizer to properly constrain U and V (instead of the expensive Cayley transform, or trying to pin the matrices to the Stifel manifold) is very intriguing, and I think there is a lot of potential here.

-This paper suffers from a high degree of mathiness, substituting dense notation in places where verbal explanation would be more appropriate. There are several spots where explaining the intuition behind a given idea (particularly when proposing the various spectrum regularizers) would be far more effective than the huge amount of notation. In the author's defense, the notation is generally used as effectively as it could be. My issue is that it often is just insufficient, and communication would be better served with more illustrative figures and/or language.

-I found the way the paper references Figure 1 confusing. The decays are substantially different for each layer--are these *all* supposed to be examples of slow decay? Layer 6 appears to have 90% of its singular values below 0.5, while layer 0 has more than 50%. If this is slow decay, what does an undesirable fast decay look like? Isn't the fast decay as shown in figure 2 almost exactly what we see for Layer 6 in figure 1? What is the significance of the sharp drop that occurs after some set number of singular values? The figure itself is easy to understand, but the way the authors repeatedly refer to it as an example of smooth singular decays is confusing.

-what is D-optimal design? This is not something commonly known in the ML literature. The authors should explain what exactly that D-optimal regularizer does, and elucidate its backward dynamics (in an appendix if space does not permit it in the main body). Does it encourage all singular values to have similar values? Does it push them all towards 1? I found the brief explanation (""encourages a slow singular value decay"") to be too brief--consider adding  a plot of the D-optimal spectrum to Figure 1, so that the reader can easily see how it would compare to the observed spectra. Ideally, the authors would show an example of the target spectra for each of the proposed regularizers in Figure 1. This might also help elucidate what the authors consider a desirable singular value decay, and mollify some of the issues I take with the way the paper references figure 1.

-The explanation of the Divergence Regularizer is similarly confusing and suffers from mathiness, a fact which I believe is further exacerbated by its somewhat odd motivation. Why, if the end result is a reference curve toward which the spectra will be regularized, do the authors propose (1) a random variable which is a transformation of a gaussian (2) to take the PDF of that random variable (3) discretize the PDF  (4) take the KL between a uniform discrete distribution and the discretized PMF and (5) ignore the normalization term? If the authors were actually working with random variables and proposing a divergence this might make sense, but the items under consideration are singular values which are non-stochastic parameters of a model, so treating them this way seems very odd. Based on figure 2 it looks like the resulting reference curves are fine, but the explanation of how to arrive there is quite convoluted--I would honestly have been more satisfied if the authors had simply designed a function (a polynomial logarithmic function perhaps) with a hyperparameter or two to control the curvature.

-""Our experimental results show that both combinations achieve an impressive results on CIFAR10 and STL-10 datasets""
Please do not use subjective adjectives like ""impressive."" A 6.5% improvement is okay, but not very impressive, and when you use subjective language you run the risk of readers and reviewers subjectively disagreeing with you, as is the case with this reviewer. Please also fix the typo in this sentence, it should at least be ""...achieve [impressive] results"" or ""achieve an [impressive] improvement on..."" 

Section 3:
-What is generalization supposed to mean in this context? It's unclear to me why this is at all relevant--is this supposed to be indicating the bounds for which the Discriminator will correctly distinguish real vs generated images? Or is there some other definition of generalization which is relevant? Does it actually matter for what we care about (training implicit generative models)? 

-What exactly is the use of this generalization bound? What does it tell us? What are the actual situations in which it holds? Is it possible that it will ever be relevant to training GANs or to developing new methods for training GANs?

Experiments:
-I appreciate that results are taken over 10 different random seeds.

-If the choice of gamma is unimportant then why is it different for one experiment? I found footnote 4 confusing and contradictory.  

-For figure 3, I do not think that the margin is ""significant""--it constitutes a relative 6.5% improvement, which I do not believe really justifies the increased complexity and compute cost of the method.

-I appreciate Table 1 and Figure 4 for elucidating (a) how orthogonal the U and V matrices end up and (b) the observed decay of the spectra.

Appendix:
-Please change table 7 to be more readable, with captions underneath each figure rather than listed at the top and forcing readers to count the rows and match them to the caption. What is the difference between SN-GAN and Spectral Norm in this table? Or is that a typo, and it should be spectral-constraint?

-I Would like to see a discussion of table 7 / interpretation of why the spectra look that way (and why they evolve that way over training) for each regularizer.  

Minor:
-Typos and grammatical mistakes throughout.
-As per the CIFAR-10/100 website (https://www.cs.toronto.edu/~kriz/cifar.html) the Torralba citation is not the proper one for the CIFAR datasets, despite several recent papers which have used it.
-Intro, last paragraph, ""Generation bound"" should be generalization bound?
-Page 4, paragraph 2, last sentence, problem is misspelled.","The sentiment of the review is generally positive, as the reviewer acknowledges the interesting and promising ideas presented in the paper and ultimately votes for its acceptance. However, the reviewer also points out several weaknesses, such as the lack of strong results and the need for improved presentation. Therefore, the sentiment score is 40, reflecting a moderately positive but balanced view. The politeness of the language is high, as the reviewer provides constructive criticism and suggestions for improvement in a respectful and professional manner. The reviewer uses phrases like 'I think,' 'I appreciate,' and 'please,' which indicate a polite tone. Therefore, the politeness score is 80.",40,80
"The big-little module is an extension of the multi-scale module. Different scales takes different complexities: higher complexity for low-scale, and lower complexity for high scale. Two schemes of merging two branches are also discussed, and the linear combination is empirically better. 

As expected, the results are better than ResNets, ResNexts, SEResNexts. I do not have  comments except ablation study is needed to show the results for more choices of alpha, beta, e.g., alpha =1, beta =1.","The sentiment of the review is generally positive, as the reviewer acknowledges that the results are better than those of ResNets, ResNexts, and SEResNexts. However, the review is brief and lacks detailed praise, which tempers the positivity. The politeness of the language is neutral to slightly polite, as the reviewer does not use any harsh or overly critical language and makes a simple recommendation for an ablation study without any negative connotations.",50,20
"I like the simplicity of the approach in this paper (especially compared to very computationally hungry methods such as Deepmind's ""Graph Networks as Learnable Physics Engines for Inference and Control""). The fact that the approach allows for online learning is also interesting. I very much appreciate that you tested your approach on a real robot arm!

I have a number of questions, which I believe could help strengthen this paper:
- The decomposition of H into L^TL ensures H is positive definite, however there are no constraints on g (gravity/external forces). How do you ensure the model doesn't degenerate into only using g and ignoring H? In the current formulation g only depends on q, however this seems insufficient to model velocity dependent external forces (e.g. contact dynamics). Please elaborate.
- How would you handle partial observability of states? Have you tried this? 
- How would you extend this approach to soft robots or robots for which the dimensionality of the state space is unknown?
- Have you tested your method on systems that are not kinematic chains? How would complex contact dynamics be handled (e.g. legged robots)?
- It would be interesting to see more comparisons with recent work (e.g. Deepmind's).

Some figures (e.g. Figure 6) are missing units on the axes. Please fix.","The sentiment of the review is generally positive, as indicated by phrases like 'I like the simplicity of the approach' and 'I very much appreciate that you tested your approach on a real robot arm.' The reviewer expresses genuine interest and appreciation for the work. However, the review also contains several critical questions and suggestions for improvement, which slightly temper the overall positivity. The politeness of the language is high, as the reviewer uses polite phrases such as 'I have a number of questions, which I believe could help strengthen this paper' and 'Please elaborate.' The tone is constructive and respectful throughout.",70,90
"

*Pros:*
-	Easily accessible paper with good illustrations and a mostly fair presentation of the results (see suggestions below).
-	It is a first attempt to generate audio with GANs which results in an efficient scheme for generating short, fixed-length audio segments of reasonable (but not high) quality.
-	Human evaluations (using crowdsourcing) provides empirical evidence that the approach has merit.
-	The paper appears reproducible and comes with data and code.

*Cons*:
-	Potentially a missing comparison with existing generative methods (e.g. WaveNet). See comments/questions below ** 
-	The underlying idea is relatively straightforward in that the proposed methods is a non-trivial application of already known techniques from ML and audio signal processing.

*Significance*: The proposed GAN-based audio generator is an interesting step in the development of more efficient audio generation and it is of interest to a subcommunity of ICLR as it provides a number of concrete techniques for applying GANs to audio.

*Further comments/ questions:*
-	Abstract/introduction: I’d suggest being more explicit about the limitations of the method, i.e. you are currently able to generate short and fixed-length audio.
-	SpecGAN (p 4): I’d suggest including some justification of the chosen pre-processing of spectrograms (p. 4, last paragraph). 
-	** Evaluation:  The paper dismisses existing generative methods early in the evaluation phase but the justification for doing so is not entirely clear to me: Firstly, if the inception score is used as an objective criterion it would seem reasonable to include the values in the paper. Secondly, as inception scores are based on spectrograms it could potentially favour methods using spectrograms directly (SpecGAN) or indirectly (WaveGAN, via early stopping) thus putting the purely sample based methods (e.g. WaveNet) at a disadvantage. It would seem fair to pre-screen the audio before dismissing competitors instead of solely relying on potentially biased inception scores (which was probably also done in this work, but not clearly stated…)? Finally, while not the aim of the paper, it would have been beneficial to discuss and understand the failures of existing methods in more detail to convince the reader that a fair attempt has been made to getting competitors to work before leaving them out entirely. 
-	Results/analysis: It is unclear to me how many people annotated the individual samples? What is the standard deviation over the human responses (perhaps include in tab 1)? Consider including a reflection on (or perhaps even test statistically) the alignment between the qualitative diversity/quality scores and the subjective ratings to justify the use of the objective scores in the training/selection process.
-	Related work: I think it would provide a better narrative if the existing techniques are outlined earlier on in the paper.
","The review starts with a positive sentiment, highlighting the accessibility of the paper, the novelty of using GANs for audio generation, and the empirical evidence supporting the approach. The reviewer also appreciates the reproducibility of the work. However, the review also points out some significant cons, such as the lack of comparison with existing methods and the straightforwardness of the underlying idea. The language used is polite and constructive, offering specific suggestions for improvement without being dismissive or harsh.",60,80
"Authors case the problem of finding informative measurements by using a maximum likelihood formulation and show how a data-driven dimensionality reduction protocol is built for sensing signals using convolutional architectures. A novel parallelization scheme is discussed and analyzed for speeding up the signal recovery process.
 
Previous works have been proposed to jointly learn the signal sensing and reconstruction algorithm using convolutional networks. Authors do not consider them as the baseline methods due to the fact that the blocky reconstruction approach is unrealistic such as MRI. However, there is no empirical result to support his conclusion.  In addition, the comparisons to these methods can further convince the readers about the advantage of the proposed method.
 
It is not clear where the maximum deviation from isometry in Algorithm 1 is discussed since the MSE is used as a loss function.
 
Authors provided theoretical insights for the proposed algorithm. It indicates that the lower-bound of the mutual information is maximized and minimizing the mean squared error is a special case, but it is unclear why this can provide theoretical guarantee for the proposed method. More details are good for the connections between the theory and the proposed algorithm.
 
One of the contributions in this paper is the speed, so the results on the speed should be put in the main paper.","The review starts by summarizing the authors' contributions and acknowledges the novelty of the parallelization scheme. However, it quickly shifts to critique, pointing out the lack of empirical results to support the authors' claims and the need for comparisons to baseline methods. The reviewer also highlights unclear aspects of the theoretical insights and suggests that more details are needed. The language used is constructive and polite, offering specific recommendations for improvement without being harsh or dismissive.",-20,80
"The authors propose a Laplacian in the context of reinforcement learning, together with learning the representations. Overall the authors make a nice contribution. The insight of defining rho to be the stationary distribution of the Markov chain P^pi and connecting this to eq (1) is interesting. Also the definition of the reward function on p.7 in terms of the distance between phi(s_{t+1}) and phi(z_g) looks original. The method is also well illustrated and compared with other methods, showing the efficiency of the proposed method.

On the other hand I also have further comments and suggestions:

- it would be good if the authors could comment on the choice of d. This is in fact a model selection problem. According to which criterion is this selected?

- the authors define D(u,v) in eq (4). Why this choice? Is there some intuition or interpretation possible related to this expression?

- in (6) beta is called a Lagrange multiplier. Given that a soft constraint (not a hard constraint) is added for the orthonormality constraint it is not a Lagrange multiplier.

How sensitive are the results with respect to the choice of beta in (6) (or epsilon in the eq above)? The orthonormality constraint will only be approximately satisfied. Isn't this a problem?

Wouldn't it be better in this case to rely on optimization algorithm on Grassmann and Stiefel manifolds?

- The authors provide a scalable approach related to section 2 by stochastic optimization. Other scalable methods related to kernel spectral clustering (related to subsets/subgraphs and making out-of-sample extensions) were proposed in literature, e.g.

Multiway Spectral Clustering with Out-of-Sample Extensions through Weighted Kernel PCA, IEEE Transactions on Pattern Analysis and Machine Intelligence, 32(2), 335-347, 2010.

Kernel Spectral Clustering for Big Data Networks, Entropy, Special Issue: Big Data, 15(5), 1567-1586, 2013.


","The review starts with a positive sentiment, appreciating the authors' contribution and highlighting interesting and original aspects of the work. This sets a positive tone. The subsequent comments and suggestions are constructive and aimed at improving the paper, without any negative or rude language. The reviewer asks for clarifications and suggests additional references, which is typical in a constructive peer review process. Therefore, the sentiment score is high, and the politeness score is also high due to the respectful and constructive nature of the feedback.",80,90
"Updated rating after author response from 8 to 7 because I agree that Figure 1 and some discussions were confusing in the original manuscript.
--------------------------------------------------------------------------

This paper investigates the relationship between the eigenvectors of the Hessian. This paper investigates characteristics of Hessian of the empirical losses of DNNs through comprehensive experiments. These experiments showed many important insights, 1) the top-K eigenvalues become bigger in the early stage, and decrease in later stage. 2) Bigger SGD steps and smaller batch-size leads to smaller and earlier peak of eigenvalues. 3) The sharpest direction update does not contribute to the loss value decrease in the normal step size (or bigger). From these analyses, this paper proposes to decrease the SGD step length on top-K eigenvectors for speeding up the convergence. Experimental results showed that the proposed method could converge to local minima in a fewer epoch and obtain better result, which means higher test accuracy.

This paper is well-written and well-organized. Findings about eigenvalues and these relationship between the SGD step length are very impressive. Although the step length adjustment on the top-K eigenvector directions are not realistic solution for improving the current SGD-based optimization on DNNs due to heavy computational cost, I think these findings and insights are very helpful to ICLR and other ML communities.","The sentiment of the review is quite positive, as the reviewer praises the paper for being well-written and well-organized, and finds the findings impressive and helpful to the community. However, there is a minor critique regarding the computational cost of the proposed method. Therefore, the sentiment score is 80. The politeness of the language is very high, as the reviewer uses respectful and appreciative language throughout the review, even when pointing out the limitations. Thus, the politeness score is 100.",80,100
"This paper investigates two issues regarding Adversarial Imitation Learning. They identify a bias in commonly used reward functions and provide a solution to this. Furthermore they suggest to improve sample efficiency by introducing a off-policy algorithm dubbed ""Discriminator-Actor-Critic"". They key point here being that they propose a replay buffer to sample transitions from. 

It is well written and easy to follow. The authors are able to position their work well into the existing literature and pointing the differences out. 

Pros:
	* Well written
	* Motivation is clear
	* Example on biased reward functions 
	* Experiments are carefully designed and thorough
Cons:
	* The analysis of the results in section 5.1 is a bit short

Questions:
	* You provide a pseudo code of you method in the appendix where you give the loss function. I assume this corresponds to Eq. 2. Did you omit the entropy penalty or did you not use that termin during learning?

	* What's the point of plotting the reward of a random policy? It seems your using it as a lower bound making it zero. I think it would benefit the plots if you just mention it instead of plotting the line and having an extra legend

	* In Fig. 4 you show results for DAC, TRPO, and PPO for the HalfCheetah environment in 25M steps. Could you also provide this for the remaining environments?

	* Is it possible to show results of the effect of absorbing states on the Mujoco environments?

Minor suggestions:
In Eq. (1) it is not clear what is meant by pi_E. From context we can assume that E stands for expert policy. Maybe add that. Figures 1 and 2 are not referenced in the text and their respective caption is very short. Please reference them accordingly and maybe add a bit of information. In section 4.1.1 you reference figure 4.1 but i think your talking about figure 3.","The review starts with a positive sentiment, highlighting that the paper is well-written, easy to follow, and well-positioned within existing literature. The pros listed further reinforce the positive sentiment by mentioning clear motivation, well-designed experiments, and thoroughness. The cons and questions are presented constructively and politely, aiming to improve the paper rather than criticize it harshly. The language used is polite and professional throughout, with suggestions and questions framed in a helpful manner.",80,90
"The paper focuses on adversarial vulnerability of neural networks, and more specifically on perturbation-based versus invariance-based adversarial examples and how using bijective networks (with so-called metameric sampling) may help overcoming issues related to invariance. The approach is used to get around insufficiencies of cross-entropy-based information-maximization, as illustrated on experiments where the proposed variation on CE outperforms CE. 

While I am not a neural network expert, I felt that the ideas developed in the paper are worthwhile and should eventally lead to useful contributions and be published. This being said, I did not find the paper in its present form to be fit for publication in a high-tier conference or journal. The main reason for this is the disbalance between the somehow heavy and overly commented first four pages (especially in Section 2) contrasting with the surprisingly moderate level of detail when it comes to bijective networks, supposedly the heart of the actual original contribution. To me this is severely affecting the overall quality of the paper. The contents of sections 3 and 4 seem relevant, but I struggled find out what precisely is the main contribution in the end, probably because of the lack of detail on bijective networks mentioned before. Again, I am not an expert, and I will indicate that in the system of course, but while I cannot completely judge all aspects of the technical relevance and the originality of the approach, I am fairly convinced that the paper deserves to be substantially revised before it can be accepted for publication.   

Edit: After paper additions I am changing my score to a 6. ","The sentiment of the review is mixed but leans towards positive. The reviewer acknowledges the potential of the ideas and contributions of the paper, indicating that they are worthwhile and should eventually lead to useful contributions. However, the reviewer also points out significant issues with the current form of the paper, particularly the imbalance in content and lack of detail in crucial sections. This results in a sentiment score of 20. The politeness of the language is quite high. The reviewer uses phrases like 'I felt,' 'should eventually lead to useful contributions,' and 'I am fairly convinced,' which are considerate and respectful. The critique is constructive and aimed at helping the authors improve their work, leading to a politeness score of 80.",20,80
"This paper describes approximation and estimation error bounds for functions in Besov spaces using estimators corresponding to deep ReLU networks. The general idea of connecting network parameters such as depth, width, and sparsity to classical function spaces is interesting and could lead to novel insights into how and why these networks work and under what settings. The authors carefully define Besov spaces and related literature, and overall the paper is clearly written. 

Despite these strengths, I'm left with several questions about the results. The most critical is this: piecewise polynomials are members of the Besov spaces of interest, and ReLU networks produce piecewise linear functions. How can piecewise linear approximations of piecewise polynomial functions lead to minimax optimal rates? The authors' analysis is based on cardinal B-spline approximations, which generally makes sense, but it seems like you would need more terms in a superposition of B-splines of order 2 (piecewise linear) than higher orders to approximate a piecewise polynomial to within a given accuracy. The larger number of terms should lead to worse estimation errors, which is contrary to the main result of the paper. I don't see how to reconcile these ideas. 

A second question is about the context of some broad claims, such as that the rates achieved by neural networks cannot be attained by any linear or nonadaptive method. Regarding linear methods, I agree with the author, but I feel like this aspect is given undue emphasis. The key paper cited for rates for linear methods is the Donoho and Johnstone Wavelet Shrinkage paper, in which they clearly show that nonlinear, nonadaptive wavelet shrinkage estimators do indeed achieve minimax rates (within a log factor) for Besov spaces. Given this, how should I interpret claims like ""any linear/non-linear approximator
with fixed N -bases does not achieve the approximation error ... in some parameter settings such as 0 < p < 2 < r ""?
Wavelets provide a fixed N-basis and achieve optimal rates for Besov spaces. Is the constraint on p and r a setting in which wavelet optimality breaks down? If not, then I don't think the claim is correct. If so, then it would be helpful to understand how relevant this regime for p and r is to practical settings (as opposed to being an edge case). 

The work on mixed Besov spaces (e.g. tensor product space of 1-d Besov spaces) is a fine result but not surprising.

A minor note: some of the references are strange, like citing a 2015 paper for minimax rates for Besov spaces that have been known for far longer or a 2003 paper that describes interpolation spaces that were beautifully described in DeVore '98. It would be appropriate to cite these earlier sources. ","The sentiment of the review is mixed. The reviewer acknowledges the strengths of the paper, such as the interesting idea of connecting network parameters to classical function spaces and the clear writing. However, the reviewer also raises several critical questions and concerns about the results and claims made in the paper. Therefore, the sentiment score is slightly positive but not overwhelmingly so. The politeness of the language is high; the reviewer uses polite and constructive language throughout the review, even when pointing out issues and asking questions.",20,80
"This is an interesting paper which proposes a novel angle on the problem of learning long-term dependencies in recurrent nets. The authors argue that most of the action should be in the imaginary part of the eigenvalues of the Jacobian J=F' of the new_state = old_state + epsilon F(old_state, input) incremental type of recurrence, while the real part should be slightly negative. If they were 0 the discrete time updates would still not be stable, so slightly negative (which leads to exponential loss of information) leads to stability while making it possible for the information decay to be pretty slow. They also propose a gated variant which sometimes works better. 

This is similar to earlier work based on orthogonal or unitary Jacobians of new_state = H(old_state,input) updates, since the Jacobian of H(old_state,input) = old_state + epsilon F( old_state,input) is I + epsilon F'. In this light, it is not clear why the proposed architecture would be better than the partially orthogonal / unitary variants previously proposed. My general concern with this this type of architecture is that they can store information in 'cycles' (like in fig 1g, 1h) but this is a pretty strong constraint. For example, in the experiments, the authors did not apparently vary the length of the sequences (which would break the trick of using periodic attractors to store information). In practical applications this is very important. Also, all of the experiments are with classification tasks with few categories (10), i.e., requiring only storing 4 bits of information. Memorization tasks requiring to store many more bits, and with randomly varying sequence lengths, would better test the abilities of the proposed architecture.

","The review starts with a positive note, describing the paper as 'interesting' and acknowledging the novelty of the proposed approach. However, it quickly transitions into a critical analysis, questioning the advantages of the proposed architecture over existing methods and pointing out potential weaknesses in the experimental design. The language used is polite and professional, with no signs of rudeness or hostility.",20,80
"This paper proposed a post-processing rejection sampling scheme for GANs, named Discriminator Rejection Sampling (DRS), to help filter ‘good’ samples from GANs’ generator. More specifically, after training GANs’ generator and discriminator are fixed; GANs’ discriminator is further exploited to design a rejection sampler, which is used to reject the ‘bad’ samples generated from the fixed generator; accordingly, the accepted generated samples have good quality (better IS and FID results). Experiments of SAGAN model on GMM toys and ImageNet dataset show that DRS helps further increases the IS and reduces the FID.

The paper is easy to follow, and the experimental results are convincing. However, I am curious about the follow questions.

(1)	Besides helping generate better samples, could you list several other applications where the proposed technique is useful? 

(2)	In the last paragraph of Page 4, I don’t think the presented Discriminator Rejection Sampling “addresses” the issues in Sec 3.2, especially the first paragraph of Page 5.

(3)	The hyperparameter gamma in Eq. (8) is of vital importance for the proposed DRS. Actually, it is believed the key to determining whether DRS works or not. Detailed analysis/experiments about hyperparameter gamma are considered missing. 
","The sentiment of the review is generally positive, as the reviewer acknowledges that the paper is easy to follow and the experimental results are convincing. However, the reviewer also raises some critical questions and points out areas that need further clarification or improvement. Therefore, the sentiment score is not fully positive but leans towards the positive side. The language used in the review is polite and constructive, as the reviewer uses phrases like 'I am curious about' and 'could you list,' which are respectful and suggest a collaborative tone.",60,80
"This paper proposes to construct multiple classification tasks from unsupervised data.

Quality:
The detail of the proposed method is not mathematically presented and its performance is not theoretically analyzed.
Although the proposed method is empirically shown to be superior to other approaches, the motivation is not clearly presented.
Hence the overall quality of this paper is not high.

Clarity:
The readability of this paper is not high as it is redundant or unclear at several points.
For example, Sections 2.1, 2.3 and Sections 2.2, 2.4 can be integrated, respectively, and more mathematical details can be included instead.

Originality:
The proposal of constructing meta-learning based on unsupervised learning seems to be original.

Significance:
- The motivation is not clear. The proposed method artificially generates a number of classification tasks. But how to use such classifiers for artificially generated labels in real-world applications is not motivated.
  It is better to give a representative application, to which the proposed method fits.
- There is no theoretical analysis on the proposed method.
  For example, why is the first embedding step required? Clustering can be directly performed on the give dataset D = {x_i}.
- Although the paper discusses using unsupervised learning for meta-learning, only k-means is considered in the proposed method.
  There are a number of types of unsupervised learning, including other clustering algorithms and other tasks such as outlier detection, hence analyzing them is also interesting.
- The proposed method includes several hyper-parameters. But how to set them in practice it not clear.

Pros:
- An interesting approach to meta-learning is presented.

Cons:
- Motivation is not clear.
- There is no theoretical analysis.
","The sentiment of the review is moderately negative. The reviewer points out several significant issues with the paper, such as the lack of mathematical detail, unclear motivation, and absence of theoretical analysis. However, the reviewer does acknowledge the originality of the approach and provides constructive feedback. Therefore, the sentiment score is -60. The politeness of the language is relatively high. The reviewer uses formal and respectful language, even when pointing out flaws. The feedback is constructive and aimed at helping the authors improve their work. Therefore, the politeness score is 80.",-60,80
"In this submission, the authors investigate using recurrent networks in distributed DQN with prioritized experience replay on the Atari and DMLab benchmarks. They experiment with several strategies to initialize the recurrent state when processing a sub-sequence sampled from the replay buffer: the best one consists in re-using the initial state computed when the sequence was originally played (even if it may now be outdated) but not doing any network update during the first k steps of the sequence (“burn-in” period). Using this scheme with LSTM units on top of traditional convolutional layers, along with a discount factor gamma = 0.997, leads to a significant improvement on Atari over the previous state-of-the-art, and competitive performance on DMLab.

The proposed technique (dubbed R2D2) is not particularly original (it is essentially “just” using RNNs in Ape-X), but experiments are thorough, investigating several important aspects related to recurrence and memory to validate the approach. These findings are definitely quite relevant to anyone using recurrent networks on RL tasks. The results on Atari are particularly impressive and should be of high interest to researchers working on this benchmark. The fact that the same network architecture and hyper-parameters also work pretty well on DMLab is encouraging w.r.t. the generality of the method.

I do have a couple of important concerns though. The first one is that a few potentially important changes were made to the “traditional” settings typically used on Atari, which makes it difficult to perform a fair comparison to previously published results. Using gamma = 0.997 could by itself provide a significant boost, as hinted by results from “Meta-Gradient Reinforcement Learning” (where increasing gamma improved results significantly compared to the usual 0.99). Other potentially impactful changes are the absence of reward clipping (replaced with a rescaling scheme) and episodes not ending with life loss: I am not sure whether these make the task easier or harder, but they certainly change it to some extent (the “despite this” above 5.1 suggests it would be harder, but this is not shown empirically). Fortunately, this concern is partially alleviated by Section 6 that shows feedforward networks do not perform as well as recurrent ones, but this is only verified on 5 games: a full benchmark comparison would have been more reassuring (as well as running R2D2 with more “standard” Atari settings, even if it would mean using different hyper-parameters on DMLab).

The second important issue I see is that the authors do not seem to plan to share their code to reproduce their results. Given how time consuming and costly it is to run such experiments, and all potentially tricky implementation details (especially when dealing with recurrent networks), making this code available would be tremendously helpful to the research community (particularly since this paper claims a new SOTA on Atari). I am not giving too much weight to this issue in my review score since (unfortunately) the ICLR reviewer guidelines do not explicitly mention code sharing as a criterion, but I strongly hope the authors will consider it.

Besides the above, I have a few additional small questions:
1. “We also found no benefit from using the importance weighting that has been typically applied with prioritized replay”: this is potentially surprising since this could be “wrong”, mathematically speaking. Do you think this is because of the lack of stochasticity in the environments? (I know Atari is deterministic, but I am not sure about DMLab)
2. Fig. 3 (left) shows R2D2 struggling on some DMLab tasks. Do you have any idea why? The caption of Table 3 in the Appendix suggests the absence of specific reward clipping may be an issue for some tasks, but have you tried adding it back? I also wonder if maybe training a unique network per task may make DMLab harder, since IMPALA has shown some transfer learning occurring between DMLab tasks? (although the comparison might be to the “deep experts” version of IMPALA — this is not clear in Fig. 3 — in which case this last question would be irrelevant)
3. In Table 1, where do the IMPALA (PBT) numbers on DMLab come from? Looking at the current arxiv version of their paper, their Fig. 4 shows it goes above 70% in mean capped score, while your Table 1 reports only 61.5%. I also can’t find a median score being reported on DMLab in their paper, did you try to compute it from their Fig. 9? And why don’t you report their results on Atari?
4. Table 4’s caption mentions “30 no-op starts” but you actually used the standard “random starts” setting, right? (not a fixed number of 30 no-ops)

And finally a few minor comments / suggestions:
- In the equation at bottom of p. 2, it seems like theta and theta- (the target network) have been accidentally swapped (at least compared to the traditional double DQN formula)
- At top of p. 3 I guess \bar{delta}_i is the mean of the delta_i’s, but then the index i should be removed
- In Fig. 1 (left) please clarify which training phase these stats are computed on (whole training? beginning / middle / end?)
- p. 4, “the true stored recurrent states at each step”: “true” is a bit misleading here as it can be interpreted as “the states one would obtain by re-processing the whole episode from scratch with the current network” => I would suggest to remove it, or to change it (e.g. “previously”). By the way, I think it would have been interesting to also compare to these states recomputed “from scratch”, since they are the actual ground truth.
- I think you should mention in Table 1’s caption that the PBT IMPALA is a single network trained to solve all tasks
- Typo at bottom of p. 7, “Indeed, Table 1 that even...”

Update: score updated to 8 (from 7) following discussion below","The sentiment of the review is generally positive, as the reviewer acknowledges the thoroughness of the experiments and the relevance of the findings, particularly highlighting the impressive results on Atari and the encouraging performance on DMLab. However, the reviewer also raises some important concerns, such as the changes to traditional settings and the lack of code sharing, which slightly temper the overall positive sentiment. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer provides constructive feedback and suggestions in a respectful and professional manner, even when pointing out issues. Thus, the politeness score is 90.",60,90
"This paper addresses black-box classifier attacks in the “hard-label” setting, meaning that the only information the attacker has access to is single top-1 label predictions. Relative to even the standard black-box setting where the attacker has access to the per-class logits or probabilities, this setting is difficult as it makes the optimization landscape non-smooth. The proposed approach reformulates the optimization problem such that the outer-loop optimizes the direction using approximate gradients, and the inner-loop estimates the distance to the nearest attack in a given direction. The results show that the proposed approach successfully finds both untargeted and targeted adversarial examples for classifiers of various image datasets (including ImageNet), usually with substantially better query-efficiency and better final results (lower distance and/or higher success rate) than competing methods.

=====================================

Pros:

Very well-written and readable paper with good background and context for those (like me) who don’t closely follow the literature on adversarial attacks. Figs. 1-3 are nice visual aids for understanding the problem and optimization landscape.

Novel formulation and approach that appears to be well-motivated from the literature on randomized gradient-free search methods. Novel theoretical analysis in Appendix that generalizes prior work to approximations (although, see notes below).

Good empirical results showing that the method is capable of query-efficiently finding attacks of classifiers on real-world datasets including ImageNet. Also shows that the model needn’t be differentiable to be subject to such attacks by demonstrating the approach on a decision-tree based classifier. Appears to compare to and usually outperform appropriate baselines from prior work (though I’m not very familiar with the literature here).

=====================================

Cons/questions/suggestions/nitpicks:

Eq 4-5: why \texttt argmin? Inconsistent with other min/maxes.

Eq 4-5: Though I understood the intention, I think the equations are incorrect as written: argmin_{\lambda} { F(\lambda) } of a binary-valued function F would produce the set of all \lambdas that make F=0, rather than the smallest \lambda that makes F=1. I think it should be something like:

min_{\lambda>0} {\lambda}
s.t. f(x_0+\lambda \theta/||\theta||) != y_0

Sec 3.1: why call the first search “fine-grained”? Isn’t the binary search more fine-grained? I’d suggest changing it to “coarse-grained” unless I’m misunderstanding something.

Algorithm 2: it would be nice if this included all the tricks described as “implementation details” in the paragraph right before Sec. 4 -- e.g. averaging over multiple sampled directions u_t and line search to choose the step size \eta. These seem important and not necessarily obvious to me.

Algorithm 2: it could be interesting to show how performance varies with number of sampled directions per step u_t.

Sec: 4.1.2: why might your algorithm perform worse than boundary-attack on targeted attacks for CIFAR classifiers? Would like to have seen at least a hypothesis on this.

Sec 6.3 Theorem 1: I think the theorem statement is a bit imprecise. There is an abuse of big-O notation here -- O(f(n)) is a set, not a quantity, so statements such as \epsilon ~ O(...) and \beta <= O(...) and “at most O(...)” are not well-defined (though common in informal settings) and the latter two are redundant given the meaning of O as an upper bound. The original theorem from [Nesterov & Spokoiny 2017] that this Theorem 1 would generalize doesn’t rely on big-O notation -- I think following the same conventions here might improve the theorem and proof.

=====================================

Overall, this is a good paper with nice exposition, addressing a challenging but practically useful problem setting and proposing a novel and well-motivated approach with strong empirical results.","The sentiment of the review is highly positive. The reviewer praises the paper for being well-written, providing good background and context, and presenting a novel and well-motivated approach with strong empirical results. The reviewer also acknowledges the visual aids and the theoretical analysis. The sentiment score is therefore 90. The politeness of the language is very high. The reviewer uses polite and constructive language throughout, even when pointing out issues or making suggestions. The use of phrases like 'it would be nice if,' 'I’d suggest,' and 'would like to have seen' indicates a high level of politeness. The politeness score is therefore 95.",90,95
"The work is a special case of density estimation problems in Statistics, with a use of conditional independence assumptions to learn the joint distribution of nodes. While the work appears to be impressive, such ideas have typically been used in Statistics and machine learning very widely over the years(Belief Propagation,  Topic modeling with anchor words assumptions etc...). This work could be easily extended to multi-class classifications where each node belongs to multiple classes. It would be interesting to know the authors' thoughts on that. The hard classification rule in the paper seems to be too restrictive to be of use in practical scenarios, and soft classification would be a useful pragmatic alternative. 
","The sentiment of the review is moderately positive. The reviewer acknowledges the impressiveness of the work but also points out that the ideas have been widely used in the field, which slightly tempers the enthusiasm. The sentiment score is therefore 40. The politeness of the language is quite high. The reviewer uses polite language, such as 'appears to be impressive,' 'it would be interesting to know,' and 'a useful pragmatic alternative,' which indicates a respectful and constructive tone. The politeness score is 80.",40,80
"
=================
Updated Thoughts
=================

I was primarily concerned about a lack of analysis regarding the technical contributions moving from AQM to AQM+. The revisions and author comments here have addressed the specific experiments I've asked for and more generally clarified the contributions made as part of AQM+. I've increased my rating to reflect my increased confidence in this paper. Overall, I think this is a good paper and will be interesting to the community.

I also thank the authors for their substantial efforts to revise the paper and address these concerns.


===========
Strengths:
===========

The approach is a sensible application of AQM to the GuessWhich setting and results in significant improvements over existing approaches both in terms of quantitative results and qualitative examples. 

===========
Concerns:
===========

[A] Technical Novelty is Limited Compared to AQM 
The major departures from the AQM approach claimed in the paper (Section 3.3) are:
	[1] the generation of candidate questions through beam search rather than predefined set 
	[2.1] The approximate answerer being an RNN generating free-form language instead of a binary classifier. 
	[2.2] Dropping the assumption that \tilde p(a_t | c, q_t) = \tilde p (a_t | c, q_t, h_{t-1}). 
	[3] Estimate approximate information gain using subsets of the class and answer space corresponding to the beam-search generated question set and their corresponding answers.

I have some concerns about these:

For [1], the original AQM paper explores this exact setting for GuessWhat in Section 5.2 -- generating the top-100 questions from a pretrained RNN question generator via beam search and ranking them based on information gain. From my understand, this aspect of the approach is not novel.

For [2.1] I disagree that this is a departure from the AQM approach, instead simply an artifact of the experimental setting. The original AQM paper was based in the GuessWhat game in which the answerer could only reply with yes/no/na; however, the method itself is agnostic of this choice. In fact, the detailed algorithm explanation in Appendix A of the AQM paper explicitly discusses the possibility of the answer generator being an RNN model. 

Generally, the modifications to AQM largely seem like necessary, straight-forward adjustments to the problem setting of GuessWhich and not algorithmic advances. That said, the changes make sense and do adapt the method to this more complex setting where it performs quite well!


[B] Design decisions are not well justified experimentally
Given that the proposed changes seem rather minor, it would be good to see strong analysis of their effect. Looking back at the claimed difference from AQM, there appear to be a few ablations missing:
- How useful is generating questions? I would have liked to see a comparison to a Q_fix set samples from training. (This corresponds to difference [1] above.)
- How important is dialog history to the aprxAns model? (This corresponds to difference [2.2] above).
- How important is the choice to restrict to |C| classes? Figure 4b begins to study this question but conflates the experiment by simultaneously increasing |Q| and |A|. (This correspond to difference [3] above.)

[C] No evaluation of Visual Dialog metrics
It would be useful to the community to see if this marked improvement in GuessWhich performance also results in improved ability to predict human response to novel dialogs. I (and I imagine many others) would like to see evaluation on the standard Visual Dialog test metrics. If this introspective inference process improves these metrics, it would significantly strengthen the paper!

[D] No discussion of inference time
It would be useful to include discussion of relative inference time. The AQM framework requires substantially more computation than an non-introspective model. Could authors report this relative increase in inference efficiency (say at K=20)? 


[E] Lack of Comparison to Base AQM
I would expect explicit comparison to AQM for a model named AQM+ or a discussion on why this is not possible.


===========
Minor Things:
===========

- I don't understand the 2nd claimed contribution from the introduction ""At every turn, AQM+ generates a question considering the context of the previous dialog, which is desirable in practice."" Is this claim because the aprxAns module uses history? 

- Review versions of papers often lack polished writing. I encourage the authors to review their manuscript for future versions with an eye for clarity of terminology, even if it means a departure from established notation in prior work. 

- The RL-QA qualitative results, are these from non-delta or delta? Is there a difference between the two in terms of interpretability? 

===========
Overview:
===========

The modifications made to adapt AQM to the GuessWhich setting presented here as AQM+ seem to be somewhat minor technical contributions. Further, where these difference could be explored in greater detail, there is a lack of analysis. That said, the proposed approach does make significant qualitative and quantitative improvements in the target problem. I'm fairly on the fence for this paper and look forward to seeing additional analysis and the opinions of other reviewers.



","The sentiment of the review is generally positive, as the reviewer acknowledges the authors' efforts to address concerns and improve the paper, ultimately stating that it is a good paper that will be interesting to the community. However, the reviewer also points out several areas for improvement and expresses some reservations, which tempers the overall positivity. Therefore, the sentiment score is 50. The politeness of the language is high, as the reviewer thanks the authors for their efforts, provides constructive feedback, and uses polite language throughout. Therefore, the politeness score is 90.",50,90
"This is a work regarding the alignment of word embedding for multiple languages.Though there are existing works similar to this one, most of them are only considering a pair of two languages, resulting in the composition issue mentioned in this work. The authors proposed a way of using a regularization term to reduce such degraded accuracy and demonstrate the validity of the proposed algorithm via experiments. I find the work to be interesting and well written. Several points that I want to bring up:

1. The language tree at the end of section 5 is very interesting. Does it change if the initialization/parameter is different?

2. The matrix P in (1) is simply a standard permutation matrix. I think the definitions are redundant.

3. The experiment results are expected since the algorithms are designed for better composition quality. An additional experiment, e.g. classification of instances in multiple languages, could further help demonstrate the strength of the proposed technic.

4. How to choose the regularization parameter \mu and what's the effect of \mu?

5. Some written issues like the notation of orthogonal matrix set, both \mathcal{O} and \mathbb{O} are used.","The review starts with a positive sentiment towards the work, mentioning that it is interesting and well-written. The reviewer provides constructive feedback and suggestions for improvement without any negative or harsh language. The tone is polite and professional throughout the review.",80,90
"This paper proposes a method for learning skills in absence of a reward function. These skills are learned so that the diversity of the trajectories produced by each skill is maximised. This is achieved by having a discriminator attempting to tell these skills apart. The agent is rewarded for visiting states that are easy to distinguish and the discriminator is trained to better infer the skills from states visited by the agent. Furthermore, a maximum entropy policy is used to force the skills to be diverse. The proposed method is general and any RL algorithm with entropy maximisation in the objective can be used, the implementation in the paper uses the Soft Actor Critic method.

The problem that they are tackling is interesting and is of clear value for obtaining more generalisable RL algorithms. The paper is overall clear and easy to follow, the results are interesting and potentially useful, although I have some reservations regarding how they assess this usefulness in the current version of this paper.
Structure-wise, I would say that the choice of writing the paper in the form of a Q&A, with very brief explanations and details was more distracting and at times unnecessary than I liked (e.g. Question 7 could move to Appendix as it is quite trivial).

I really appreciated how much care has been taken to discuss differences with the closest prior work, Variational Intrinsic Control (VIC) by Gregor et al. 
One such difference is that their prior distribution over skills is not learnt. While there are good arguments by the authors about why this is appealing (e.g. it prevents collapsing to sampling only a few skills), I feel this could be also quite a limitation of their method. This assumes that you have a good a-priori knowledge and assumptions regarding how many skills are useful or needed in the environment. This is unlikely to be the case in complex environments, where you first need to learn simple skills in order to explore the environment, and later learn to form new more complex skills. During this process, you might want to prune simplistic skills after you learnt more abstract and complex ones, for instance in the context of continual learning. I understand this could be investigated in future work, but I feel they take a rather optimistic take on this problem.

Overall, the use case for the proposed method is slightly unclear to me. While the paper claims to allow diverse set of skills to be learnt, it is highly dependent on learning varied action sequences that help you visit different part of state space, regardless of their usefulness. This means there could be learn a lot of skills that capture part of the state space that is not useful or desirable for downstream tasks. While there is a case made for DIAYN being a stepping stone for imitation learning and hierarchical RL, I don’t find the reported experiments for imitation learning and HRL convincing. In the imitation learning experiment, the distance (KL divergence) between all skills and the expert data is computed and the closest skill is then chosen as the policy imitating the expert. The results are weak and no comparisons with any LfD baselines are reported. The HRL experiments also lack comparisons to any other HRL baseline. I feel that this section is rather weak, especially compared to the rest of the paper, and I am not sure it achieves much.

As a general comment, the choice of reporting the training progress using “hours spent training” is an peculiar choice which is never discussed. I understand that for methods with varying computational costs this might be a fairer comparison but it would be perhaps good to also report progress against number of required environment interactions (including pre-training).
Another assumption made is that the method is valuable in situations where the reward function is expensive to compute and the unsupervised pre-training is free (somewhat easing the large amount of pre-training required). However, it would have been interesting to see examples of such environments in their experiments supporting these claims, as this assumption is not valid for the chosen MuJoCo environments.

Despite these comments, I still feel this is valuable work, that can clearly inspire further relevant work and deserves to be presented at ICLR.
It presents a solid contribution, given its technical novelty, proposed applications and its overall generality. 
However, the paper could use more convincing experiments to support its claims.

Additional comments and typos:
- Figure 5 lack error bars across the 5 random seeds and are crucial to assess whether this performance difference is indeed significant given the amount of pre-training required.
- Figure 7’s title and caption is missing...
- typo: page 3, last paragraph “...mutual information between skills and states, **I(S; Z )**” not I(A; Z)
- typo: page 7 paragraph next to Figure 6 “...whereas DIAYN explicitly **learns** skills that effectively partition the state space”
- typo: page 7 above Figure 8 “...make them **exceedingly** difficult for non- hierarchical RL algorithms.”
","The sentiment of the review is generally positive, as the reviewer acknowledges the value and potential of the work, despite pointing out several areas for improvement. The reviewer appreciates the clarity of the paper, the interesting problem tackled, and the care taken to discuss differences with prior work. However, they also express reservations about certain aspects of the methodology and the strength of the experimental results. The politeness of the language is high, as the reviewer provides constructive criticism in a respectful and professional manner, using phrases like 'I really appreciated,' 'I feel,' and 'it would have been interesting to see.' The reviewer also acknowledges the potential for future work and the overall contribution of the paper.",60,90
"This paper introduces a deep RL algorithm to solve the Rubik's cube. The particularity of this algorithm is to handle the huge state space and very sparse reward of the Rubik's cube. To do so, a) it ensures each training batch contains states close to the reward by scrambling the solution; b) it computes an approximate value and policy for that state using the current model and c) it weights data points based by the inverse of the number of random moves from the solution used to generate that training point. The resulting model is compared to two non-ML algorithms and shown to be competitive either on computational speed or on the quality of the solution.  

This paper is well written and clear. To the best of my knowledge, this is the first RL-based approach to handle the Rubik's cube problem so well. The specificities of this problem make it interesting. While the idea of starting from the solution seemed straightforward at first, the paper describes more advanced tricks claimed to be necessary to make the algorithm work. The algorithm seems to be quite successful and competitive with expert algorithms, which I find very nice. Overall, I found the proposed approach interesting and sparsity of reward is an important problem so I would rather be in favor of accepting this paper. 

On the negative side, I am slightly disappointed that the paper does not link to a repository with the code. Is this something the authors are considering in the future? While it does not seem difficult to code, it is still nice to have the experimental setup.

There has been (unsuccessful) attempts to solve the Rubik's cube using deep RL before. I found some of them here: https://github.com/jasonrute/puzzle_cube . I am not sure whether these can be considered prior art as I could not find associated accepted papers but some are quite detailed. Some could also provide additional baselines for the proposed methods and highlight the challenges of the Rubik's cube.

I am also curious whether/how redundant positions are handled by the proposed approach and wished this would be discussed a bit. Considering the nature of the state space and the dynamics, I would have expected this to be a significant problem, unlike in Go or chess. Does the algorithm forbid the reverse of the last action? Is the learned value/policy function good enough that backwards moves are seldom explored? Since the paper mention that BFS is interesting to remove cycles, I assume identical states are not duplicated. Is this correct?","The sentiment of the review is generally positive, as the reviewer praises the paper for being well-written, clear, and innovative in its approach to solving the Rubik's cube using deep RL. The reviewer also appreciates the competitiveness of the algorithm and expresses a favorable inclination towards accepting the paper. However, there are minor criticisms regarding the lack of a code repository and some additional questions about the handling of redundant positions. The politeness of the language is high, as the reviewer uses polite phrases such as 'I am slightly disappointed' and 'I am curious,' and provides constructive feedback without being harsh or dismissive.",80,90
"Summary: 

This paper proposed a feature boosting and suppression method for dynamic channel pruning. To be specific, the proposed method firstly predicts the importance of each channel and then use an affine function to amplify/suppress the importance of different channels. However, the idea of dynamic channel pruning is not novel. Moreover, the comparisons in the experiments are quite limited. 

My detailed comments are as follows.


Strengths:

1. The motivation for this paper is reasonable and very important. 

2. The authors proposed a new method for dynamic channel pruning.

Weaknesses:

1. The idea of dynamic channel pruning is not novel. In my opinion, this paper is only an extension to Network Slimming (Liu et al., 2017). What is the essential difference between the proposed method and Network Slimming?

2. The writing and organization of this paper need to be significantly improved. There are many grammatical errors and this paper should be carefully proof-read.

3. The authors argued that the importance of features is highly input-dependent. This problem is reasonable but the proposed method still cannot handle it. According to Eqn. (7), the prediction of channel saliency relies on a data batch rather than a single data. Given different inputs in a batch, the selected channels should be different for each input rather than a general one for the whole batch. Please comment on this issue.

4. The proposed method does not remove any channels from the original model. As a result, both the memory and the computational cost will not be reduced. It is confusing why the proposed method can yield a significant speed-up in the experiments.

5. The authors only evaluate the proposed method on shallow models, e.g., VGG and ResNet18. What about the deeper model like ResNet50 on ImageNet?

6. It is very confusing why the authors only reported top-5 error of VGG. The results of top-1 error for VGG should be compared in the experiments.

7. Several state-of-the-art channel pruning methods should be considered as the baselines, such as ThiNet (Luo et al., 2017), Channel pruning (He et al., 2017) and DCP (Zhuang et al., 2018)
[1] Channel pruning for accelerating very deep neural networks. CVPR 2017.
[2] Thinet: A filter level pruning method for deep neural network compression. CVPR 2017.
[3] Discrimination-aware Channel Pruning for Deep Neural Networks. NIPS 2018.
","The sentiment of the review is mixed but leans towards negative. The reviewer acknowledges the importance of the motivation and the introduction of a new method, but they also highlight significant weaknesses, including lack of novelty, poor writing, and insufficient experimental comparisons. Therefore, the sentiment score is -40. The politeness of the language is relatively high; the reviewer uses polite language and constructive criticism without being rude or dismissive. Thus, the politeness score is 80.",-40,80
"Summary:
This work presents a method to generate adversary examples capable of fooling a neural network classifier. Szegedy et al. (2013) were the first to expose the weakness of neural networks against adversarial attacks, by adding a human-imperceptible noise to images to induce misclassification. Since then, several works tackled this problem by modifying the image directly in the pixel space: the norm-balls convention. The authors argue that this leads to non-realistic attacks and that a network would not benefit from training with these adversarial images when performing in the real world. Their solution and contributions are parametric norm-balls: unlike state-of-the-art methods, they perform perturbations in the image formation space, namely the geometry and the lighting, which are indeed perturbations that could happen in real life. For that, they defined a differentiable renderer by making some assumptions to simplify its expression compared to solving a light transport equation. The main simplifications are the direct illumination to gain computation efficiency and the distant illumination and diffuse material assumptions to represent lighting in terms of spherical harmonics as in Ramamoorthi et al. (2001), which require only 9 parameters to approximate lighting. This allows them to analytically derivate their loss function according to the geometry and lighting and therefore generate their adversary examples via gradient descent. They show that their adversary images generalize to other classifiers than the one used (ResNet). They then show that injecting these images into the training set increase the robustness of WideResNet against real attacks. These real attack images were taken by the authors in a laboratory with varying illumination.

Strength:
- The proposed perturbations in the image formation space simulate the real life scenario attacks.
- The presented results show that the generated adversary images do fool the classifier (used to compute the loss) but also new classifiers (different than the one used to compute the loss). As a consequence the generated adversary images increase the robustness of the considered classifier. 
- Flexibility in their cost function allows for diverse types of attacks: the same modified geometry can fool a classifier in several views, either into detecting the same object or detecting different false objects under different views. 

Major comments:
- Method can only compute synthetic adversary examples, unlike state-of-the-art.
- The main contribution claimed by the author is that their perturbations are realistic and that it would help better increase the robustness of classifiers against real attacks. However, they do not give any comparison to the state-of-the-art methods as is expected. 

Minor comments:
- Even if the paper is well written, they are still some typos. 
","The sentiment of the review is generally positive, as it highlights the strengths of the proposed method and acknowledges its contributions to the field. The reviewer appreciates the realistic nature of the perturbations and the robustness improvements shown in the results. However, the sentiment is slightly tempered by the major comments, which point out the lack of comparison to state-of-the-art methods and the limitation of only computing synthetic adversary examples. The politeness of the language is high, as the reviewer uses constructive and respectful language throughout the review, even when pointing out areas for improvement.",70,90
"The authors demonstrate the generalization bound for deep neural networks using the PAC-Bayesian approach. They adopt the idea of noise resilience in the analysis and obtain a result that has improved dependence in terms of the network dimensions, but involves parameters (e.g., pre-activation) that may be large potentially. 

My major concern is also regarding the dependence on the pre-activation that can be very large in practice. This is also shown in the numerical experiments. Therefore, the overall generalization bound can be larger than existing results, though the later have stronger dependence on the network sizes. By examining the analysis for the main result, it seems to me that the reason the authors can induce weaker dependence on network sizes is essentially they involved the pre-activation parameters. This can be viewed as a trade-off how strong the generalization bound depend on the network sizes and other related parameters (like the pre-activation here) rather than strictly tighten the error bound from a more refined/structured way. I also suggest that the authors provide the comparison of their bound and existing ones to see the quantitative difference of the results. 

Regarding the noise resilience, it is not clear to where the noise resilience shows up from the analysis or the result. From the proof of the main result, the analysis seems to be standard as in the PAC-Bayesian analysis, which is based on bounding the difference of the network before and after injecting randomness into the parameters. The difference with respect to the previous result due to the different way of bounding such a gap, where the Jacobian, the pre-activation and function output pop up. But this does not explain how well a network can tolerate the noise, either in the parameter space of the data space. This is different with the previous analysis based on the noise resilience, such as [1]. So, the title and the way the authors explain as noise resilience is somewhat misleading. More detailed explanation will help.

[1] Arora et al. Stronger generalization bounds for deep nets via a compression approach. 
","The sentiment of the review is moderately negative. The reviewer expresses significant concerns about the dependence on the pre-activation parameter and the clarity of the noise resilience aspect. However, the reviewer acknowledges the improved dependence on network dimensions. The politeness of the language is high; the reviewer uses polite and constructive language, suggesting improvements and providing specific feedback without being harsh or dismissive.",-40,80
"This paper introduces the study of the problem of frequency estimation algorithms with machine learning advice. The problem considered is the standard frequency estimation problem in data streams where the goal is to estimate the frequency of the i-th item up to an additive error, i.e. the |\tilde f_i - f_i| should be minimized where \tilde f_i is the estimate of the true frequency f_i.

Pros:
-- Interesting topic of using machine learned advice to speed up frequency estimation is considered
-- New rigorous bounds are given on the complexity of frequency estimation under Zipfian distribution using machine learned advice
-- Experiments are given to justify claimed improvements in performance

Cons:

-- While the overall claim of the paper in the introduction seems to be to speed up frequency estimation using machine learned advice, results are only given for the Zipfian distribution.

-- The overall error model in this paper, which is borrowed from Roy et al. is quite restrictive as at it assumes that the queries to the frequency estimation data structure are coming from the same distribution as that given by f_i’s themselves. While in some applications this might be natural, this is certainly very restrictive in situations where f_i’s are updated not just by +/-1 increments but through arbitrary +/-Delta updates, as in this case it might be more natural to assume that the distribution of the queries might be proportional to the frequency that the corresponding coordinate is being updated, for example.

-- The algorithm proposed in the paper is very straightforward and just removes heavy hitters using oracle advice and then hashes everything else using the standard CountMin sketch.

-- Since CounMin is closely related to Bloom filters the idea of using machine learning to speed it up appears to be noticeably less novel given that for Bloom filters this has already been done by Mitzenmacher’18.

-- The analysis is relatively straightforward and boils down to bucketing the error and integration over the buckets.


Other comments:
-- The machine learned advice is assumed to be flawless at identifying the Heavy Hitters, authors might want to consider incorporating errors in the analysis.



","The sentiment of the review appears to be slightly positive, as the reviewer acknowledges the interesting topic, new rigorous bounds, and experiments provided. However, the review also contains several critical points regarding the limitations and straightforwardness of the algorithm, which tempers the overall positivity. Therefore, the sentiment score is 20. The politeness of the language is quite high, as the reviewer uses polite and constructive language throughout the review, even when pointing out the cons. Therefore, the politeness score is 80.",20,80
"Interesting work, extending previous work by Balestriero and Baraniuk in a relevant and non-trivial direction. The presentation could be cleaner and clearer, 

The paper contains solid work and contributes to an interesting perspective/interpretation of deep networks. The presentation is reasonably clear, although somewhat cluttered by a large number of subscripts and superscripts, which could be avoided by using a more modular formulation; e.g., in equation (1), when referring to a specific layer l, the superscript l can be dropped as it adds no useful information. By the way, when l is first used, just before equation (1), it is undefined, although the reader can guess what it stands for.

It is not clear why $[\pi^{(l)}]_{k,t}$ is defined after equation (5), as these quantities are not mentioned in Theorem 2. Another confusion issue is that it is not clear if the assumption made in Proposition 1 concerning is only valid there of if it is assued to hold elsewhere in the paper.

Proposition 2 is simply a statement of the well-known relationship between between soft-max (a.k.a. logistic regression) and the maximum entropy principle (see, for example, http://www.win-vector.com/dfiles/LogisticRegressionMaxEnt.pdf).

","The sentiment of the review is generally positive, as it acknowledges the interesting and relevant extension of previous work and the solid contribution to the interpretation of deep networks. However, it also points out areas for improvement in the presentation and clarity. Therefore, the sentiment score is 60. The language used in the review is polite and constructive, offering specific suggestions for improvement without being harsh or dismissive. Thus, the politeness score is 80.",60,80
"The authors propose an exploration bonus that is aimed to aid in sparse reward RL problems. The bonus is given by an auxillary network which tries to score whether a candidate observation is difficult to reach with respect to all previously observed novel observations which are stored in a memory buffer. The paper considers many experiments on complex 3D environments. 

The paper is well written and very well illustrated. The method can be clearly understood from the 3 figures and the examples are nice. I think the method is interesting and novel and it is evaluated on a realistic and challenging problem.

It would be good if the authors could further elaborate on the scalability of the method in terms of compute/memory requirements and related to that if the implementation is cumbersome. I didn’t understand well how the method avoids the issue of old memories leaving the buffer. It seems for a large enough environment important observations will eventually become discarded causing a poor approximation of the curiosity bonus? For the large scale experiments I would like to know more rough details of the number of the compute time needed for the method relative to the PPO baseline and the other baseline (e.g. number of nodes for example and how long they run approximately)

Are there any potential issues with adapting the method on 2D environments like Atari? this could permit direct comparisons with several other recently proposed techniques in this area.

The Grid-Oracle result is very interesting and a contribution on it’s own if similar results for complex 3D environments are not published anywhere else. It demonstrates well that exploration bonuses can help drastically in these tasks. I think if possible it would be interesting to have an idea how fast this method converges (number of training steps) and not just the final reward as reported in the tables. Indeed as a general problem the current number of training steps of any methods shown seem to indicate these techniques are too data hungry for non-simulated environments. For some applications (e.g. aimed at sim-to-real transfer) the grid-oracle approach might be a good alternative to consider. I would be interested to know if the authors had some thoughts on this.

Overall I lean towards accept, the method is shown to work on relatively very complex problems in DMLab and VizDoom while most sparse reward solutions proposed are typically evaluated on relatively simpler and unrealistic tasks. I would consider to further increase my score if the authors can address some of the comments. 
","The sentiment of the review is quite positive, as evidenced by phrases like 'well written and very well illustrated,' 'interesting and novel,' and 'I lean towards accept.' The reviewer appreciates the clarity and novelty of the method and its evaluation on complex problems. However, there are some constructive criticisms and requests for further elaboration, which are presented in a polite and respectful manner. The reviewer uses phrases like 'It would be good if the authors could further elaborate' and 'I would be interested to know,' which indicate a polite tone.",80,90
"The paper presents an application of Bayesian neural networks in predicting 
future street scenes. The inference is done by using variational approximation 
to the posterior. Moreover, the authors propose to using a synthetic (approximate)
likelihood and the optimization step in variational approxiation is based on a regularization.
These modifications are claimed by the authors that it yields a better results in practice 
(more stable, capture the multi-modal nature). Numerical parts in the paper support
the authors' claims: their method outperforms some other state-of-the-art methods.

The presentation is not too hard to follow.
I think this is a nice applied piece, although I have never worked on this applied side.

Minor comment:
In the second sentence, in Section 3.1, page 3, 
$f: x \mapsto y$    NOT $f: x \rightarrow y$. 
We use the ""\rightarrow"" for spaces X,Y not for variables.  


","The sentiment of the review is generally positive, as the reviewer acknowledges the paper's contributions and supports the authors' claims with numerical evidence. The reviewer also describes the paper as a 'nice applied piece,' which indicates a positive sentiment. Therefore, the sentiment score is 80. The politeness of the language is also high, as the reviewer uses polite language and provides constructive feedback without any negative or harsh comments. The minor comment is presented in a helpful and respectful manner. Therefore, the politeness score is 90.",80,90
"This paper studies convergence of gradient descent on a two-layer fully connected ReLU network with binary output and square loss. The main result is that if the number of hidden units is polynomially large in terms of the number of training samples, then under suitable randomly initialization conditions and given that the output weights are fixed, gradient descent necessarily converge to zero training loss.

Pros:
The paper is presented clearly enough, but I still urge the authors to carefully check for typos and grammatical mistakes as they revise the paper. As far as I have checked, the proofs are correct. The analysis is quite simple and elegant. This is one thing that I really like about this paper compared to previous work. 

Cons:
The current setting and conditions for the main result to hold are quite a bit limited. If one has polynomially large number of neurons (i.e. on the order of n^6 where n is number of training samples) as stated in the paper, then the weights of the hidden layer can be easily chosen so that the outputs of all training samples become linearly independent in the hidden layer (see e.g. [1] for the construction, which requires only n neurons even with weight sharing) , and thus fixing these weights and optimizing for the output weights would lead directly to a convex problem with the same theoretical guarantee. At this point, it would be good to explain why this paper is focusing on the opposite setting, namely fixing the output weights and learning just the hidden layer weights, because it seems that this just makes the problem become more non-trivial compared to the previous case while yielding almost the same results . Either way, this is not the way how practical neural networks are trained as only a subset of the weights are optimized. Thus it's hard to conclude from here why the commonly used GD w.r.t. all variables converges to zero loss as stated in the abstract.

The condition on the Gram matrix H_infty in Theorem 3.1 seems to be critical. I would like to see the proof that this condition can be fulfilled under certain conditions on the training data.

In Lemma 3.1, it seems that ""log^2(n/delta)"" should be ""log(n^2/delta)""? 

Despite the above limitations, I think that the analysis in this paper is still interesting (mainly due to its simplicity) from a theoretical perspective. Given the difficulty of the problem, I'm happy to vote for its acceptance.

[1] Optimization landscape and expressivity of deep CNNs","The sentiment of the review is generally positive, as indicated by the clear appreciation for the simplicity and elegance of the analysis, and the overall recommendation for acceptance. However, there are notable criticisms regarding the limitations of the current setting and conditions, which slightly temper the overall positive sentiment. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer uses polite suggestions and constructive feedback without any rude or harsh language. The reviewer also acknowledges the difficulty of the problem and expresses happiness to vote for acceptance, which further indicates politeness. Therefore, the politeness score is 80.",60,80
"Evaluation:
This is a solid paper: The idea is clear, it is well communicated and put into context of the existing literature, and the results are promising. The experiments are well chosen and illustrate the method well. The connection between the chosen setting (BAMDPs) to POMDPs is explained well and explored in the empirical evaluation as well. I think that the methods section could go into a bit more detail, and the underlying assumptions that the authors make could be discussed more critically.

Summary:
This paper looks at Bayes-Adaptive MDPs (BAMDPs) in which the latent parameter space is either
- a discrete finite set or
- a bounded continuous set that can be approximated via discretization.
Consequently, the authors choose to represent the belief as a categorical distribution, which can be represented by a vector of weights.
They further assume that the environment model is known. Hence, the posterior belief can be computed exactly.
If I understand correctly, the main contribution is that the authors represent the policy as a neural network and train it using a policy gradient algorithm.
This is a good first step towards scalable Bayesian policy optimisation.

Main Feedback:
- In the Introduction, first paragraph, you say one of the aspects of real-world robotics is that there's ""(1) an underlying dynamical system with unknown latent parameters"". I would argue that the dynamic system itself is typically also unknown, including how it is parametrized by these latent parameters. I think it is important to point this out more explicitly in the introduction (it is mentioned in sec 2 and 5, but maybe it's worth mentioning it in 4 again as well): for the problems that you look at, you assume that the form of the transition function is known (just not its parameters phi). 
- In the main methods section (4), it would be nice to see some more detail about the Bayes filter. Can you write out the distribution over the latent parameters, and write out how the filtering is done? Explain how to compute the normalising constant (and mention explicitly why this is possible for your set-up, and why it would be infeasible if the latent space cannot be discretized). How exactly is the posterior distribution represented and fed to the policy? Seeing this done explicitly in Section 4 (even if it repeats some things that are explained in 2) would help someone that is interested in (re-)implementing the proposed method.
- I would like to see a more critical discussion in Section 7 about the assumptions that the authors make: that the environment models are known, and that the latent space can be discretized. How realistic are those assumptions (and in which kind of real-world problems can we make them), and what are ways forward to drop these assumptions?

Other Comments:
- Introduction: Using an encoder for the state/belief is an implementation choice, and (as I see it) not part of the main contribution. I would focus on explaining the intuition behind BPO in the introduction, and only mention the architecture choice as a side note.
- Related Work: The authors might be interested in the recent work of Igl et al. (ICML 2018, ""Deep Variational RL for POMDPs""), who approximate the belief in a POMDP using variational inference and a particle filter.

Significance for ICLR:
- In the light-dark experiment, the authors visualise the belief that the agent has at every time step. It would have been nice to see an analysis of how exactly the belief looks also for maybe 1-2 other experiments, and how (when) the agent makes a decision based on this. This could replace Table 2 (which I guess should be called Figure 2?), which I did not find very insightful.","The sentiment of the review is positive, as indicated by phrases like 'This is a solid paper,' 'the idea is clear,' 'well communicated,' and 'the results are promising.' The reviewer also appreciates the well-chosen experiments and the clear explanation of the connection between BAMDPs and POMDPs. However, the reviewer does provide constructive criticism and suggestions for improvement, which is typical in a peer review. Therefore, the sentiment score is 80. The politeness of the language is very high, as the reviewer uses polite and respectful language throughout the review, such as 'I would argue,' 'it would be nice to see,' and 'I would like to see.' The reviewer also provides specific and actionable feedback without being harsh or dismissive. Therefore, the politeness score is 90.",80,90
"

[clarity]
This paper is basically well written. 
The motivation is clear and reasonable.
However, I have some points that I need to confirm for review (Please see the significance part).


[originality]
The idea of taking advantage of von Mises-Fisher distributions is not novel in the context of DL/DNN research community.
E.g.,
von Mises-Fisher Mixture Model-based Deep learning: Application to Face Verification.

However, as described in the paper, the incorporation of von Mises-Fisher for calculating loss function seems to be novel, to the best of my knowledge.


[significance]
Unfortunately, the experiments in this paper do not fully support the effectiveness of the proposed method. 
See below for more detailed comments.


*weak baseline (comparison)
As an anonymous reviewer pointed out, the author should run baseline method with beam search if the authors aim to convince readers (including reviewers) for the effectiveness of the proposed method.
I understand that it is important to investigate the effectiveness of the proposed method in the identical settings. However, it is also important to compare the proposed method with strong baseline to reveal the relative effectiveness of the proposed method comparing with the current state-of-the-art methods. 


* open vocabulary setting
I am confused whether the experimental setting for the proposed method is really in an open vocabulary setting or not.
If my understanding is correct, the vocabulary sizes used for the proposed method were 50,000 (iwslt2016) and 300,000 (wmt16), which cannot be an open vocabulary setting. 
If this is correct, the applicability of the proposed method is potentially limited comparing with the subword-based approach.
Is there any comment for this question?


* convergence speed
I think the claim of faster convergence of the proposed method in terms of iteration may be misleading. This might be true, but it is empirically proven only by single dataset and single run. The authors should show more empirical results on several datasets or provide a theoretical justification for this claim.


Overall, basically I like the idea of the proposed method. 
I also aim to remove the large computational cost of softmax in neural encoder-decoder approach.
In my feeling, the proposed method should be a bit more improved for a recommendation of clear acceptance.
","The sentiment of the review is generally positive, as indicated by phrases like 'basically well written,' 'motivation is clear and reasonable,' and 'I like the idea of the proposed method.' However, the reviewer also points out several significant issues that need to be addressed, which tempers the overall positivity. Therefore, the sentiment score is 30. The politeness of the language is high, as the reviewer uses polite phrases such as 'I understand,' 'I think,' and 'Is there any comment for this question?' The reviewer also provides constructive feedback without being harsh or dismissive. Therefore, the politeness score is 80.",30,80
"This paper investigates sophistical exploration approaches for reinforcement learning. Motivated by the fact that most of bandit algorithms do not handle heteroscedasticity of noise, the authors built on Information Direct Sampling and on Distributional Reinforcement Learning to propose a new exploration algorithm family. Two versions of the exploration strategy are evaluated against the state-of-the-art on Atari games: DQN-IDS for homoscedatic noise and C51-IDS for heteroscedastic noise. 

The paper is well-written. The background section provides the clues to understand the approach. In IDS, the selected action is the one that minimizes the ratio between a squared conservative estimate of the regret and the information gain. Following (Ktischner and Krause 2018), the authors propose to use \log(1+\sigma^2_t(a)/\rho^2(a)) as the information gain function, which corresponds to a Gaussian prior, where \sigma^2_t is the variance of the parametric estimate of E[R(a)] and \rho^2(a) is the variance of R(a). \sigma^2_t is evaluated by bootstrap (Boostrapped DQN). Where the paper becomes very interesting is that recent works on distributional RL allow to evaluate \rho^2(a). This is the main input of this paper: combining two recent approaches for handling heteroscedasticity of noise in Reinforcement Learning.

Major concern:
While the approach is appealing for handling heteroscedastic noise, the use of a normalized variance (eq 9) and a lower bound of variance (page 7) reveal that the approach needs some tuning which is not theoretically founded. 
This is problematic since in reinforcement learning, the environment is usually assumed to be unknown. What are the results when the lower bound of the variance is not used? When the variance of Z(a) is low, the variance of the parametric estimate should be low also. It is not the case?


Minor concerns:

The color codes of Figure 1 are unclear. The color of curves in subfigures (b) (c) (d) corresponds to the color code of IDS.

The way in which \rho^2(s,a) is computed in algorithm 1 is not precisely described. In particular page 6, the equation \rho^2(s,a)=Var(Z_k(s,a)) raises some questions: Is \rho evaluated for a particular bootstrap k or is \rho is averaged over the K bootstraps ?
_____________________________________________________________________________________________________________________________________________

I read the answers of authors. I increased my rating.
","The sentiment of the review is generally positive. The reviewer acknowledges that the paper is well-written and highlights the interesting combination of recent approaches for handling heteroscedasticity of noise in reinforcement learning. However, the reviewer also points out a major concern regarding the theoretical foundation of the approach, which slightly tempers the overall positive sentiment. The politeness of the language is high, as the reviewer provides constructive feedback and specific recommendations without using harsh or dismissive language. The reviewer also acknowledges the authors' responses and indicates an increased rating, which further reflects a polite and respectful tone.",70,90
"The authors extend the theoretical results of a paper previously presented in the last edition of ICLR (2018), where it was demonstrated that Recurrent Neural Network can be interpreted as a tensor network decomposition based on the Tensor-Train (TT, Oseledets et al, 2011).
While previous results covered the multiplicative nonlinearity only, the contribution of the current paper is the extension of the analysis of universality and depth efficiency (Cohen et al, 2016) to different nonlinearities, for example ReLU (Rectified Linear Unit), which is very important from the practical point of view.
The paper is well written and have a good structure. However, I found that some deep concepts are not well introduced, and maybe other more trivial results are discussed with unnecessary details. The following comments could help authors to improve the quality of presentation of their paper:
-	Section 3.1 (Score Functions and Feature Tensor) is a bit short and difficult to read. 
o	Maybe, a more motivating introduction could be included in order to justify the definition of score functions (eq. 2). 
o	It would be also nice to state that, according to eq. (3), the feature tensor is a rank-1 tensor. 
o	I would suggest moving the definition of outer product to the Appendix, since most readers know it very well.
o	It is said that eq. 2 possesses the universal approximation property (it can approximate any function with any prescribed precision given sufficiently large M). It is not clear which is the approximation function.
-	A Connection with Tensor-Ring (TR) format, if possible, could be helpful: It is known that TR format (Zhao et al, 2016, arXiv:1606.05535), which is obtained by connecting the first and last units in a TT model, helps to alleviate the requirement of large ranks in the first and last the core tensors of a TT model reaching to a decomposition with an evenly distributed rank bounds. I think, it would be interesting to make a connection of RNN to TR because the assumption of R_i < R for all i becomes more natural. I would like to see at least some comment from the authors about the applicability of TR in the context of analysis of RNN, if possible. Maybe, also the initial hidden state defined in page 5 can be avoided if TR is used instead of TT.
-	Fig 2 shows that Test accuracy of a shallow network (CP based) is lower and increases with the number of parameters approaching to the one for RNN (TT based). It would be necessary to show the results for an extended range in the number of parameters, for example, by plotting the results up to 10^6. It is expected that, at some point, the effect of overfitting start decreasing the test accuracy.
-       When scores functions are presented (eq. 2) it is written the term ""logits"" between brackets. Could you please clarify why this term is introduced here? Usually, logit of a probability p is defined as L(p)=p/(1-p). What is the usage of this term in this work? 
-      I think the theory is presented for a model with the two-classes only but used for multiple classes in the experimental sections. It should be necessary to make some comment about this in the paper.
-      Details about how the RNN based on TT is applied must be added. More specifically, the authors should provide answers to clarify the following questions: 
(i) Are patches overlapped or non-overlapped? 
(ii) What value of M is used? and is there any general rule for this choice? 
(iii) How the classification in the 10-classes is obtained? Are you using a softmax function in the last layer? Are you using one weight tensor W_c per class (c=1,2,...,10). Please provide these technical details. 
(iv) Please, specify which nonlinear activation sigma is used in the feature map f_\theta(x).
(v) How many feature maps are used? and, Are the matrix A and vector b learned from training dataset or only the TT-cores need to be learned? ","The sentiment of the review is generally positive, as the reviewer acknowledges the extension of theoretical results and the practical importance of the work. However, the reviewer also points out areas for improvement, such as the introduction of deep concepts and the need for additional details. The politeness of the language is high, as the reviewer uses polite suggestions and constructive feedback to help the authors improve their paper.",70,90
"This paper introduces a new dataset and method for chatbots. In contrast to previous work, this paper specifically probes how well a dialogue system can use external unstructured knowledge. 

Quality:
Overall, this is a very high-quality paper. The dataset is developed well, the experimental setup is well thought-through and the authors perform many ablation studies to test different model variants. The main criticism I have would be that the human evaluation is rather simple (rating 1-5), I would have expected more fine-grained categories, especially ones that relate to how much knowledge the system uses (I appreciate the ""Wiki F1"" metric, but that is an automatic metric). As it is, the human evaluation shows that most of their contributions are not appreciated by human annotators. Further, the paper ends a bit abruptly, I would have expected a more in-depth discussion of next steps.

Clarity:
The description of the work is clear in most places. I particularly like the abstract and introduction, which set up the rest of the paper nicely. In some places, perhaps due to space restrictions, method descriptions are a bit too short.

Originality:
The paper is fairly original, especially the aspect about specifically using external knowledge. The authors could have been more clear on how the work differs from other work on non-goal directed dialogue work though (last paragraph of related work section).

Significance:
The dataset is really well-developed, hence I believe many working in the dialogue systems community will re-use the developed benchmark and build on this paper.

More detailed comments:
- Missing reference for goal-oriented dialogue datasets: Wen et al. 2017, A Network-based End-to-End Trainable Task-oriented Dialogue System, https://arxiv.org/abs/1604.04562
- How does the proposed dataset differ from the Reddit and Wikipedia datasets discussed in the last paragraph of the related work section? This should be explained.
- Page 3, paragraph ""Conversational Flow"": what is the maximum number of turns, if the minimum is 5?
- Page 3, paragraph ""Knowledge Retrieval"": how were the top 7 articles and first 10 sentences choices made? This seems arbitrary. Also, why wasn't the whole text used?
- Page 3, paragraph ""Knowledge Selection and Response Generation"": how do you deal with co-reference problems if you only ever select one sentence at a time? The same goes for the ""Knowledge Attention"" model described in Section 4.
- Page 3, paragraph ""Knowledge Selection and Response Generation"": how often do annotators choose ""no sentence selected""? It would be interesting to see more such statistics about the dataset
- Section 4.2: did you run experiments for BPE encoding? Would be good to see as this is a bit of a non-standard choice.
- Section 4.2: it would be good to explain the Cer et al. 2018 method directly in the paper
- Section 4.2: is there a reference for knowledge dropout? Also, it would be good to show ablation results for this.
- Section 5.1: why did you choose to pre-train on the Reddit data? There should be some more in-depth description of the Reddit dataset to motivate this choice.
- Section 5.1: what is the setup you use for multi-task learning on SQuAD? Is it just a hard parameter sharing model, or?
- Section 5.3: as stated above, the human evaluation is a little bit underwhelming, both in terms of setup and results. I'd expect a more fine-grained way of assessing conversations by humans, and also an explanation of why the retrieval performer without knowledge was assessed as being on par with the retrieval transformer memnet.
- Section 5.3: I assume higher=better for the human scores? This should be made explicit.
- Section 5.3: Have others used the ""F1 overlap score""? If so, cite.
- Section 5.3: I don't understand the argument that the human evaluation shows that humans prefer more natural responses. How does it show that?
- Section 5.3: The Wiki F1 score is kind of interesting because it shows to what degree the model uses knowledge. But the side-by-side comparison with the human scores shows that humans don't necessarily prefer chatbot models that use a lot of knowledge. I'd expect this to be discussed, and suggestions for future work to be made accordingly.
- Section 6: The paper ends a bit abruptly. It's be nice to suggest future areas of improvement.","The sentiment of the review is generally positive, as indicated by phrases like 'very high-quality paper,' 'well-developed dataset,' and 'well thought-through experimental setup.' However, there are some criticisms, particularly regarding the human evaluation and the abrupt ending, which slightly temper the overall positivity. Therefore, the sentiment score is 70. The politeness of the language is high, as the reviewer uses polite and constructive language throughout, such as 'I would have expected,' 'it would be good to see,' and 'I'd expect,' which indicates a respectful tone. Thus, the politeness score is 90.",70,90
"- Summary
This paper proposes a residual non-local attention network for image restoration. Specifically, the proposed method has local and non-local attention blocks to extract features which capture long-range dependencies. The local and non-local blocks consist of trunk branch and (non-) local mask branch. The proposed method is evaluated on image denoising, demosaicing, compression artifacts reduction, and super-resolution.

- Pros
  - The proposed method shows better performance than existing image restoration methods.
  - The effect of each proposed technique such as the mask branch and the non-local block is appropriately evaluated.

- Cons
  - It would be better to provide the state-of-the-art method[1] in the super-resolution task. 
    [1] Y. Zhang et al., Image Super-Resolution Using Very Deep Residual Channel Attention Networks, ECCV, 2018.
  - The technical contribution of the proposed method is not high, because the proposed method seems to be just using existing methods.
  - The contribution of the non-local operation is not clear to me. For example, how does the global information (i.e., long-range dependencies between pixels) help to solve image denoising tasks such as image denoising?

Overall, the technical contribution of the proposed method is not so high, but the proposed method is valuable and promising if we focus on the performance.
","The sentiment of the review is mixed but leans slightly positive. The reviewer acknowledges the value and promise of the proposed method, particularly in terms of performance, but also points out significant limitations in terms of technical contribution and clarity. Therefore, the sentiment score is around 20. The language used is polite and constructive, offering specific suggestions for improvement without being harsh or dismissive. Thus, the politeness score is 80.",20,80
"The proposed kernel recurrent learning (KeRL) provides an alternative way to train recurrent neural network with backpropagation through time (BPTT) where the propagation of gradients can be skipped over different layers. The authors directly assume the sensitivity function between two layers with a distance of tau in a form of Eq. (7). The algorithm of BPTT is then approximated due to this assumption. The model parameters are changed to learn the network dynamics. The optimization problem turns out to estimate beta and gamma of the kernel function. The learned parameters are intuitive. There are a set of timescales to describe the memory of each neuron and a set of sensitivity weights to describe how strongly the neurons interact on average. The purpose of this study is to save the memory cost and to reduce the time complexity for online learning with comparable performance. 

Pros:
1. KeRL only needs to compute a few tensor operations at each time step, so online KeRL learns faster than online BPTT for the case with a reasonably long truncation length.
2. Biologically plausible statements are addressed.
3. A prior is imposed for the temporal sensitivity kernel. The issue of gradient vanishing is mitigated.
4. Theoretical illustration for KeRL in Sections 3 and 4 is clear and interesting.

Cons:
1. The proposed method is an approximation to BPTT training. Suppose the system performance is constrained. Some guesses are made. The system performance can be further improved.
2. The experiment on time cost due to online learning is required so that the reduction of time complexity can be illustrated.
3. The format of tables 1 and 2 can be improved. Caption is required in Table 1. Overlarge size of Table 2 can be fixed.
4.  A number of assumptions in Sections 3 and 4 are assumed.  When addressing Section 3, some assumptions in Section 4 are used. The organization of Sections 3 and 4 can be improved.","The review starts with a clear and concise summary of the proposed method, highlighting its purpose and benefits. The sentiment is generally positive, as the reviewer acknowledges the strengths of the method, such as faster learning and biological plausibility. However, the reviewer also points out several areas for improvement, including the need for additional experiments and better organization of certain sections. The language used is polite and constructive, offering specific suggestions for improvement without being overly critical or harsh.",60,80
"The paper presents a very important problem of utilizing a model on different platforms with own numerical round-offs. As a result, a model run on a different hardware or software than the one on which it was trained could completely fail due to numerical rounding-off issues. This problem has been considered in various papers, however, the classification task was mainly discussed. In this paper, on the other hand, the authors present how the numerical rounding-off issue could be solved in Latent-Variable Models (LVM).

In order to cope with the numerical rouding-off issue, the authors propose to use integer networks. They consider either quantized ReLU (QReLU) or quantized Tanh (Qtanh). Further, in order to properly train the integer NN, they utilize a bunch of techniques proposed in the past, mainly (Balle, 2018) and (Balle et al., 2018). However, as pointed out in the paper, some methods prevent training instabilities (e.g., Eqs. 18 and 19). All together, the paper tackles very important problem and proposes very interesting solution by bringing different techniques proposed for quantized NNs together .

Pros:
+ The paper is well-written.
+ The considered problem is of great importance and it is rather neglected in the literature.
+ The experiments are properly carried out.
+ The obtained results are impressive.

Cons:
- A natural question is whether the problem could be prevented by post-factum quantization of a neural network. As pointed out in the Discussion section, such procedure failed. However, it would be beneficiary to see an empirical evidence for that.
- It would be also interesting to see how a training process of an integer NN looks like. Since the NN is quantized, instabilities during training might occur. Additionally, its training process may take longer (more epochs) than a training of a standard (float) NN. An exemplary plot presenting a comparison between an integer NN training process and a standard NN training process would be highly appreciated.
- (Minor remark). The paper is well-written, however, it would be helpful to set the final learning algorithm. This would drastically help in reproducibility of the paper.

--REVISION--
After reading the authors response and looking at the new version of the paper I decided to increase my score. The paper tackles very important problem and I strongly believe it should be presented during the conference.","The sentiment of the review is positive, as indicated by phrases such as 'very important problem,' 'very interesting solution,' 'well-written,' 'properly carried out,' and 'impressive results.' The reviewer also increased their score after reading the authors' response, which further indicates a positive sentiment. Therefore, the sentiment score is 80. The politeness of the language is also high, with the reviewer using polite and constructive language throughout the review. They provide specific recommendations in a respectful manner and acknowledge the authors' efforts and improvements. Therefore, the politeness score is 90.",80,90
"The paper proposes a method to find adversarial examples in which the changes are localized to small regions of the image. A group-sparsity objective is introduced for this purpose and it is combined with an l_p objective that was used in prior work to define proximity to the original example. ADMM is applied to maximize the defined objective. It is shown that adversarial examples in which all changes are concentrated in just few regions can be found with the proposed method.

The paper is clearly written and results are convincing. But what I am not sure I understand is what is the purpose of this research. Among the 4 contributions listed in the end of the intro only the last one, Interpretability, seems to have a potential in terms on the impact. Yet am not quite sure how “obtained group-sparse adversarial patterns better shed light on the mechanisms of adversarial perturbations”. I think the mechanisms of adversarial perturbations remain as unclear as they were before this paper.

I am not ready to recommend acceptance of this paper, because I think the due effort to explain the motivation for research and its potential impacts has not been done in this case. 

UPD: the discussion and the edits with the authors convinced me that I may have been a bit too strict. I have changed my score from 5 to 6.
","The sentiment of the review is mixed but leans slightly positive. The reviewer acknowledges that the paper is clearly written and the results are convincing, which suggests a positive sentiment. However, the reviewer also expresses significant concerns about the purpose and impact of the research, which tempers the overall sentiment. The final score change from 5 to 6 indicates a slight improvement in the reviewer's perception after further discussion. Therefore, the sentiment score is 20. The politeness of the language is generally high. The reviewer provides constructive criticism without being rude and acknowledges the authors' efforts. The language used is respectful and professional, leading to a politeness score of 80.",20,80
"The paper proposes a method to solve end-to-end learning tasks using a combination of deep networks and domain specific black-box functions. In many machine learning tasks there may be a sub-part of the task can be easily solved with a black-box function (e.g a hard coded logic).  The paper proposes to use this knowledge in order to design a deep net that mimics the black-box function. This deep net being differentiable can be utilized while training in order to perform back-propagation for the deep nets that are employed to solve the remaining parts of the task. 

The paper is well written and in my opinion the experiments are solid. They show significant gains over well-designed baselines. (It should be noted that I am not super familiar with prior work in this area and may not be aware of some related baselines that can be compared with.)

In Section 3.1.2 the authors discuss offline and online methods to train the mimicking deep network of a black-box function. The offline version suffers from wasting samples on unwanted regions while the online version will have a cold-start problem. However, I believe there can be better solution than the hybrid strategy. In fact there is a clear explore/exploit trade-off  here. Therefore, one may start with a prior over the input domain of the black-box function and then as the argument extractor learns well the posterior can be updated. Then we can Thompson sample the inputs from this posterior in order to train the mimicking network.  I think such a bandit inspired approach will be interesting to try out. ","The sentiment of the review is positive, as the reviewer praises the paper for being well-written and the experiments for being solid, with significant gains over well-designed baselines. This indicates a positive sentiment towards the paper's contributions and quality. The sentiment score is therefore 80. The politeness of the language is also high, as the reviewer uses polite and constructive language throughout the review, even when suggesting improvements. The reviewer acknowledges their own limitations in familiarity with prior work, which adds to the politeness. The politeness score is therefore 90.",80,90
"This paper presents an analysis of popularly-use RNN model for structure modeling abilities by designing Tensor Product Decomposition Networks to approximate the encoder. The results show that the representations exhibit interpretable compositional structure. To provide better understanding, the paper evaluates the performance on synthesized digit sequence data as well as several sentence-encoding tasks.

Pros:
1. The paper is well-written and easy to follow. The design of the TPDN and corresponding settings (including what an filler is and what roles are included) for experiments are understandable. It makes good point at the end of the paper (section 4) on how these analysis contribute to further design of RNN models, which seems useful.
2. The experiments are extensive to support their claims. Not only synthetic data but also several popularly-used data and models are being conducted and compared. An addition of analogy dataset further demonstrate the effect of TPDN on modeling structural regularities.

Cons:
1. More detailed and extensive discussion on the contribution of the paper should be included in the introduction part to help readers understand what's the point of investigating TPDN on RNN models.
2. Some details are missing to better understand the construction. For example, on page 4, Evaluation, it is unclear of how TPDN encoder is trained, specifically, which parameters are updated? What's the objective for training? It is also unclear of whether the models in Figure 3(c) use bidirectional or unidirectional or tree decoder? In Section 3, it could be better to roughly introduce each of the existing 4 models. How do TPDN trained for these 4 sentence encoding models need to be further illustrated. More reasons should be discussed for the results in Table 2 (why bag-of-words role seem to be ok, why skip-thought cannot be approximated well).
3. It could be better to provide the actual performance (accuracy) given by TPDN on the 4 existing tasks.
4. Further thoughts: have you considered applying these analysis on other models besides RNN?","The sentiment of the review is generally positive, as indicated by the praise for the paper being well-written, easy to follow, and the extensive experiments supporting the claims. The reviewer also appreciates the contribution to the further design of RNN models. However, there are some critical points raised, such as the need for more detailed discussion in the introduction, missing details in the construction, and suggestions for additional performance metrics. These points are presented constructively and politely, aiming to improve the paper rather than dismiss it. The language used is respectful and professional, with no signs of rudeness.",70,90
"This paper describes a model for vision-and-language navigation. The proposed
model adds two components to the baseline model proposed by Fried et al. (2018):

- a panoramic visual attention (referred to in this paper as ""visual--textual
  co-grounding""), in which the full scene around the agent's current position is
  attended to prior to selecting a direction to follow

- an auxiliary ""progress monitoring"" loss which encourages the agent to to
  produce textual attentions from which the distance to the goal can be directly
  inferred

The two components combine to give state-of-the-art results on the Room2Room
dataset: small improvements over existing approaches on the ""-seen"" evaluation
set and larger improvements on the ""-unseen"" evaluation sets. These improvements
also stack with the data-augmentation approach of Fried et al.

I think this is a reasonable submission and should probably be accepted. However, I
have some concerns about presentation and a number of specific questions about
model implementation and evaluation.

PRESENTATION AND NAMING

First off: I implore the authors to find some descriptor other than ""self-aware""
for the proposed model. ""Self-aware"" is an imprecise description of the agent in
this paper---the agent is specifically ""aware"" of its visual surroundings and
its distance from the goal, neither of which is meaningfully an aspect of
""self"". Moreover, self-awareness means something quite different in adjacent
areas of cognitive science and philosophy; overloading the term in the specific
(and comparatively mundane) way used here creates confusion. See section 3.4 of
https://arxiv.org/abs/1807.03341 for broader discussion. Perhaps something
like ""visual / temporal context-sensitivity"" to describe what's new here? A bit
clunky, but I think it makes the contributions of this work much clearer.

As suggested in the summary above, I also think ""visual--textual co-attention""
is also an unhelpfully vague description of this aspect of the contribution. The
textual attention mechanism used in this paper is the same as in all previous
work on the task. Representations of language don't even interact with the
visual attention mechanism except by way of the hidden state, and the salient
new feature of the visual attention is the fact that it considers the full
panoramic context before choosing a direction.

MODELING QUESTIONS

- p4: $y_t^{pm}$ is defined as the ""normalized distance from the current
  viewpoint to the goal"". Is this distance in units of length (as defined by the
  simulator) or units of time (i.e. the number of discrete ""steps"" needed to
  reach the goal)?

  The authors have already clarified on OpenReview that the progress monitor
  objective uses an MSE loss rather than a likelihood loss. Do I understand
  correctly that ground-truth distances are in [0, 1] but model predictions are
  in [-1, 1]? Why not use a sigmoid? Also, how does scoring beam-search
  candidates as $p_t^{pm} \times p_{k,t}$ work if $p_t^{pm}$ can flip the sign?

- The input to the progress monitor is formed by concatenating the attention
  vector $\alpha_t$ to a vector of state features, and then multiplying by a
  fixed weight matrix. How is this possible? The size of $\alpha_t$ varies
  depending on the length of the instruction sequence. Are attentions padded out
  to the length of the longest instruction in the training set? If so, how can
  the model learn when it's reached the end of a short instruction sequence?
  What would happen if the agent encountered a sequence that was too long?

EVALUATION QUESTIONS

- The progress monitor is used both as an auxiliary training objective and as a
  beam search heuristic. Is it possible to disentangle these two contributions?
  (E.g. by ignoring the scores during beam search, or by doing augmented beam
  search in a model that was trained without the auxiliary objective.)

- Not critical, but it would be nice to know if the contributions here stack
  with the pragmatic inference procedure in Fried et al.

- While, as pointed out on OpenReview, it is not required to include SPL
  evaluations, I think it would be informative to do so---the preliminary
  results with no beam search look good!

MISCELLANEOUS

p1: ""without a map"" If you can do beam search, you effectively have a map.

p1: ""...smoothly"" What does ""smoothly"" mean in this context?

p2: ""the position of grounded instruction can follow past and future
    instructions"". Is the claim here that if instructions are of the form ""ACB""
    and the agent is supposed to do ""ABC"", that the proposed model will execute
    these instructions successfully and the baseline will not? This claim does
    not appear to be evaluated anywhere in the body of the paper.

p4: ""intelligently prunes"" ""Intelligently"" is unnecessary.

p4: ""for empirical reasons"" What does this mean?

p5: ""Intuitively, an instruction-following agent is required..."" The existence
    of non-attentive models that do reasonably well at these
    instruction-following tasks suggest that this is not actually a requirement.","The sentiment of the review is generally positive, as indicated by the statement 'I think this is a reasonable submission and should probably be accepted.' However, the reviewer also expresses several concerns and provides detailed feedback, which tempers the overall positivity. Therefore, the sentiment score is 50. The politeness of the language is high, as the reviewer uses polite phrases such as 'I implore the authors' and 'it would be nice to know,' and avoids any harsh or rude language. Therefore, the politeness score is 80.",50,80
"The authors propose a policy transfer scheme which in the source domain simultaneously learns a family of policies parameterised by dynamics parameters and then employs an optimisation framework to select appropriate dynamics parameters based on samples from the target domain. The approach is evaluated on a number of simulated transfer tasks (either transferring from DART to MuJoCo or by introducing deliberate model inaccuracies).

This is interesting work in the context of system identification for policy transfer with an elaborate experimental evaluation. The policy learning part seems largely similar to that employed by Yu et al. 2017 (as acknowledged by the authors). This makes the principal contribution, in the eyes of this reviewer, the optimisation step conducted based on rollouts in the target domain. While the notion of optimising over the space of dynamics parameters is intuitive the question arises whether this optimisation step makes for a substantive contribution over the original work. This point is not really addressed in the experimental evaluation as benchmarking is performed against a robust and an adaptive policy but not explicitly against the (arguably) most closely related work in Yu et al. It could be argued, of course, that Yu et al. essentially use adaptive policy generation but they do explicitly learn dynamics parameters based on recent history of actions and observations. An explicit comparison therefore seems appropriate (or alternatively a discussion of why it is not required).

Another point which would, in my view, add significant value is explicit discussion of the baseline performances observed in the various experiments. For example, in the hopper experiment (Sec 5.2) the authors state that the baseline methods were not able to adapt to the new environment. Real value could be derived here if the authors could elaborate on why this is the case. The same applies in Sec 5.3-5.6. 

(I would add here, as an aside, that I thought the notion in Sec 5.6 of framing the learning of policies for handling deformable objects as a transfer task based on rigid objects to be a nice idea. And not one this reviewer has come across before - though this could merely be a reflection of limited familiarity with the literature).

The experimental evaluation seems thorough with the above caveat of a seemingly missing benchmark in Yu et al. I would also encourage the authors to add more detail in the experimental section in the main text specifically with regards to number of trials run to arrive at variances in the figures as well as what metric these shaded areas actually signify. 

A minor point: the J in equ 1 seems (to me at least) undefined. I suspect that it signifies the expected cumulative reward and was meant to be introduced in Sec 3 where the J may have been dropped from the latex?

If the above points were addressed I think this would make a valuable and interesting contribution to the ICLR community. As it stands I believe it is marginally below the acceptance threshold.

[ADDENDUM: given the author feedback and addition of the benchmark experiments requested I have updated my score.]


Pros:
———
- interesting work
- accessible
- effective
- thorough evaluation (though potentially missing a key benchmark)

Cons:
———
- potentially missing a key benchmark (and therefore seems somewhat incremental)
- only limited insight offered by the authors in the discussion of the experimental results
- some more details needed with regards to the experimental setup
","The sentiment of the review is generally positive, as the reviewer acknowledges the interesting nature of the work, its accessibility, and the thoroughness of the evaluation. However, the sentiment is tempered by concerns about the missing benchmark and limited insights in the discussion, leading to a sentiment score of 40. The language used in the review is polite and constructive, with suggestions for improvement and acknowledgment of the authors' efforts, resulting in a politeness score of 80.",40,80
"The authors propose a benchmark for optimization algorithms specific to deep learning called DeepOBS. They provide code to evaluate an optimizer against a suite of standard tasks in deep learning, and provide well tuned baselines for a comparison. The authors discuss important considerations when comparing optimizers, including how to measure speed and tunability of an optimizer, what metric(s) to compare against, and how to deal with stochasticity.

A clear, standardized optimization benchmark suite would be very valuable for the field. As the others clearly state in the introduction, there have been many proposed optimization algorithms, but it is hard to compare many of these due to differences in how the optimizers were evaluated in the original papers. In general, people have different requirements for what the expect from an optimizer. However, this paper does a good job of discussing most of the factors that people should consider when choosing or comparing optimizers. Providing a set of well tuned baselines would save people a lot of time in making comparisons with a new optimizer, as well as providing a canonical set of tasks to evaluate against. I particularly appreciated the breadth and diversity of the included tasks.

I am a little worried that people will still find minor quibbles with particular choices or tasks in this suite, and therefore continue to use bespoke comparisons, but I think this benchmark would be a valuable resource for the community.

Some minor comments:
- In section 2.3, there is a recommendation for how to estimate per-iteration cost. I would mention in this section that this procedure is automated and part of the benchmark suite.
- I wanted to see how the baselines performed on all of the tasks in the suite (not just on the 8 tasks in the benchmark sets). Perhaps those figures could be included in an appendix.
- The authors might want to consider including an automated way of generating performance profiles (https://arxiv.org/abs/cs/0102001) across tasks as part of DeepOBS, as a way of getting a sense of how optimizers performed generally across all tasks.","The sentiment of the review is generally positive, as the reviewer appreciates the value of the proposed benchmark suite and acknowledges its potential benefits for the community. The reviewer also highlights specific strengths, such as the breadth and diversity of the included tasks. However, there is a minor concern mentioned about potential quibbles with particular choices or tasks, which slightly tempers the overall positivity. Therefore, the sentiment score is 80. The politeness of the language is very high, as the reviewer uses polite and constructive language throughout the review, offering suggestions in a respectful manner. Thus, the politeness score is 100.",80,100
"The paper proposes a distributed optimization method based on signSGD. Majority vote is used when aggregating the updates from different workers.
 The method itself is naturally communication efficient. Convergence analysis is provided under certain assumptions on the gradient. It also theoretically shows that it is robust up to half of the workers behave independently adversarially. Experiments are carried out on parameter server environment and are shown to be effective in speeding up training. 

I find the paper to be solid and interesting. The idea of using signSGD for distributed optimization make it attractive as it is naturally communication efficient. The work provides theoretical convergence analysis under the small batch setting by further assuming the gradient is unimodal and symmetric, which is the main theoretical contribution. Another main theoretical contribution is showing it is Byzantine fault tolerant. The experiments are extensive, demonstrating running time speed-up comparison to normal SGD.  

It is interesting to see a test set gap in the experiments. It remains to be further experimented to see if the method itself inherently suffer from generalization problems or it is a result of imperfect parameter tuning. 

One thing that would be interesting to explore further is to see how asynchronous updates of signSGD affect the convergence both in theory and practice. For example, some workers might be lost during one iteration, how will this affect the overall convergence.
Also, it would be interesting to see the comparison of the proposed method with SGD + batch normalization, especially on their generalization performance. It might be interesting to explore what kind of regularization technique would be suitable for signed update kind of method.   

Overall, I think the paper proposes a novel distributed optimization algorithm that has both theoretical and experimental contribution. The presentation of the paper is clear and easy to follow. 

Suggestions: I feel the experiments part could still be improved as also mentioned in the paper to achieve competitive results. More experiments on different tasks and DNN architectures could be performed. 
","The review expresses a positive sentiment towards the paper, highlighting its solid and interesting nature, as well as its theoretical and experimental contributions. The reviewer appreciates the novel approach and clear presentation. However, there are also constructive suggestions for improvement, particularly regarding the experiments. The language used is polite and constructive, aiming to help the authors improve their work.",80,90
"In this paper the authors focus on the problem of weakly-supervised action localization. The authors state that a problem with weakly-supervised attention based methods is that they tend to focus on only the most salient regions and propose a solution to this which reduces the difference between the responses for the most salient regions and other regions. They do this by employing marginalized average aggregation to averaging a sample a subset of features in relation to their latent discriminative probability then calculating the expectation over all possible subsets to produce a final aggregation.

The problem is interesting, especially noting that current attention methods suffer from paying attention to the most salient regions therefore missing many action segments in action localization. The authors build upon an existing weakly-supervised action localization framework, having identified a weakness of it and propose a solution. The work also pays attention to the algorithm's speed which is practically useful. The experiments also compare to several other potential feature aggregators.

However, there are several weakness of the current version of the paper:

- In parts the paper feels overly complicated, particularly in the method (section 2). It would be good to see more intuitive explanations of the concepts introduce here. For instance, the author's state that c_i captures the contextual information from other video snippets, it would be good to see a figure with an example video and the behaviour of p_i and c_i as opposed to lamba_i. I found it difficult to map p_i, c_i to z and lambda used elsewhere.

- The experimental evidence does not show where the improvement comes from. The authors manage to acheieve a 4-5% improvement over STPN through their re-implemenation of the algorithm, however only have a ~2% improve with their marginalized average attention on THUMOS. I would like to know the cause in the increase over the original STPN results: is it a case of not being able to replicate the results of STPN or do the different parameter choices, such as use of leakly RELU, 20 snippets instead of 400 and only rejecting classes whose video-level probabilities are below 0.01 instead of 0.1, cause this big of an increase in results? There is also little evidence that the actual proposal (contextual information) is the reason for the reported improvement.

- There seems to be several gaps in the review of current literature. Firstly, the authors refer to Wei et al. 2017 and Zhang et al. 2018b as works which erase the most salient regions to be able to explore regions other than the most salient. The authors state that the problem with these methods is that they are not end-to-end trainable, however Li et al. 2018 'Tell Me Where to Look': Guided Attention Inference Network' proposes a method which erases regions which is trainable end-to-end. Secondly, the authors do not mention the recent work W-TALC which performs weakly-supervised action localization and outperforms STPN. It would be good to have a baseline against this method.

- The qualitative results in this paper are confusing and not convincing. It is true that the MAAN's activation sequence shows peaks which correspond to groundtruth and are not present in other methods. However, the MAAN activation sequence also shows several extra peaks not present in other methods and also not present in the groundtruth, therefore it looks like it is keener to predict the presence of the action causing more true positives, but also more false positives. It would be good to see some discussion of these failure cases and/or more qualitative results. The current figure could be easily compressed by only showing one instance of the ground-truth instead of one next to each method.

I like the idea of the paper however I am currently unconvinced by the results that this is the correct method to solve the problem.
","The sentiment of the review is mixed but leans towards the positive side. The reviewer acknowledges the interesting problem tackled by the paper and appreciates the attention to algorithm speed and comparison with other feature aggregators. However, the reviewer also points out several weaknesses, including the complexity of the method, unclear experimental evidence, gaps in the literature review, and confusing qualitative results. Therefore, the sentiment score is slightly positive. The politeness of the language is high; the reviewer uses polite language throughout, providing constructive criticism and suggestions for improvement without being rude or dismissive.",20,80
"This paper introduces a new algorithm for differential game, where the goal is to find a optimize several objective functions simultaneously in a game of n players. The proposed algorithm is an interpolation between LOLA and LookAhead, and it perserves both the stability from LOLA and the ""convergence to fixed point"" property of LookAhead. The interpolation parameter is chosen in Section 3.2.

The paper looks novel, though some notations are not completely clear to me. For example, the defintions of the ""current parameters"" \hat{\theta}_1 and \hat{\theta}_2 in Section 3.1, and the stop-gradient operator. Also, how is the diag operator in Propostion 1 is defined? Normally it only represents the diagonal entries but here it might represent the diagonal blocks.


","The sentiment of the review is generally positive, as indicated by the statement 'The paper looks novel,' which suggests that the reviewer finds the work to be original and valuable. However, the reviewer also points out several areas of confusion and requests clarification on specific notations and definitions. This indicates that while the reviewer sees merit in the paper, they also believe it requires some improvements. The language used is polite and constructive, as the reviewer asks questions and seeks clarification without being dismissive or harsh.",60,80
"Paper summary:
The paper proposes to predict bouncing behavior from visual data. The model has two main components: (1) Physics Interface Module, which predicts the output trajectory from a given incoming trajectory and the physical properties of the contact surface. (2) Visual Interface Module, which predicts the surface properties from a single image and the impact location. A new dataset called Bounce Dataset is proposed for this task.

Paper strengths:
- The paper tackles an interesting and important problem.
- The data has been collected in various real scenes.
- The idea of training the physics part of the network with synthetic data and later fine-tuning it with real images is interesting.
- The experiments are thorough and well-thought-out.

Paper weaknesses:
- It would be more interesting if the dataset was created using multiple types of probe objects. Currently, it is only a ball.

- It is not clear how the evaluation is performed. For instance, the length of the groundtruth and predicted trajectories might be different. How is the difference computed?

- The impact location (x,y) corresponds to multiple locations in 3D. Why not using a 3D point as input? It seems the 3D information is available for both the real and synthetic cases.

- Why is it non-trivial to use a deconvolution network for predicting the output point cloud trajectory?

- The length of the input trajectory can vary, but it seems the proposed architecture assumes a fixed-length trajectory. I am wondering how it handles a variable-length input.

- How is the bounce location encoded in VIM?

- I don't see any statistics about the objects being used for data collection. That should be added to the paper.

>>>>> Final score: The authors have addressed my concerns in the rebuttal. I believe this paper tackles an interesting problem, and the experiments are good enough since this is one of the first papers that tackle this problem. So I keep the initial score. 
","The sentiment of the review is generally positive, as indicated by the final score and the acknowledgment that the authors have addressed the reviewer's concerns. The reviewer appreciates the interesting problem tackled by the paper, the thorough experiments, and the innovative approach of using synthetic data for training. However, the review also lists several weaknesses and areas for improvement, which slightly tempers the overall positive sentiment. Therefore, the sentiment score is 60. The politeness of the language used in the review is high. The reviewer provides constructive feedback and poses questions in a respectful manner, without any harsh or rude language. Thus, the politeness score is 80.",60,80
"

The authors suggest a method to create combined low-dimensional representations for combinations of pairs of words which have a specific syntactic relationship (e.g. adjective - noun). Building on the generative word embedding model provided by Arora et al. (2015), their solution uses the core tensor from the Tucker decomposition of a 3-way PMI tensor to generate an additive term, used in the composition of two word embedding vectors.

Although the method the authors suggest is a plausible way to explicitly model the relationship between syntactic pairs and to create a combined embedding for them, their presentation does not make this obvious and it takes effort to reach the conclusion above. Unlike Arora's original work, the assumptions they make on their subject material are not supported enough, as in their lack of explanation of why linear addition of two word embeddings should be a bad idea for composition of the embedding vectors of two syntactically related words, and why the corrective term produced by their method makes this a good idea. Though the title promises a contribution to an understanding of word embedding compositions in general, they barely expound on the broader implications of their idea in representing elements of language through vectors.

Their lack of willingness to ground their claims or decisions is even more apparent in two other cases. The authors claim that the Arora's RAND-WALK model does not capture any syntactic information. This is not true. The results presented by Arora et al. indeed show that RAND-WALK captures syntactic information, albeit to a lesser extent than other popular methods for word embedding (Table 1, Arora et al. 2015). Another unjustified choice by the authors is their choice of weighing the Tensor term (when it is being added to two base embedding vectors) in the phrase similarity experiment. The reason the authors provide for weighing the composition Tensor is the fact that in the unweighted version their model produced a worse performance than the additive composition. One would at least expect an after-the-fact interpretation for the weighted tensor term and what this implies with regard to their method and syntactic embedding compositions in general.

Arora's generative model for word embeddings, on which the current paper is largely based upon, not only make the mathematical relationship among different popular word embedding methods explicit, but also by making and verifying explicit assumptions with regard to properties of the word embeddings created by their model, they are able to explain why low-dimensional embeddings provide superior performance in tasks that implicate semantic relationships as linear algebraic relations. Present work, however interesting with regard to its potential implications, strays away from providing such theoretical insights and suffices with demonstrating limited improvements in empirical tasks.","The sentiment of the review is moderately negative. The reviewer acknowledges the plausibility of the method but criticizes the presentation, lack of support for assumptions, and unjustified choices. The sentiment score is -40. The politeness of the language is relatively high, as the reviewer uses formal and respectful language despite the criticism. The politeness score is 60.",-40,60
"This work refines the NAS method for efficient neural architecture search. The paper brings new methods for gradient/reward updates and credit assignment. 

pros: 
1. An improvement on gradient calculation and reward back-propagation mechanism
2. Good experiment results and fair comparisons

cons:
1. Missing details on how to use the gradient information to generate child network structures. In eq.2, multiplying each one-hot random variable Zij to each edge (i, j) in the DAG can obtain a child graph whose intermediate nodes are xj. However, it is still unclear how to generate the child graph. More details on generating child network based on gradient information is expected. 
2. In SNAS, P(z) is assumed fully factorizable. Factors are parameterized with alpha and learnt along with operation parameters theta. The factorization of p(Z) is based on the observation that NAS is a task with fully delayed rewards in a deterministic environment. That is, the feedback signal is only ready after the whole episode is done and all state transitions distributions are delta functions. In eq. 3, the authors use the training/testing loss directly as reward, while the previous method uses a constant reward from validation accuracy. It is unclear why using the training/testing loss can improve the results? 
","The sentiment of the review is generally positive, as it acknowledges the improvements and good experimental results of the paper. However, it also points out specific areas where more details are needed, which slightly tempers the overall positivity. The politeness of the language is high, as the reviewer uses neutral and constructive language to provide feedback and suggestions for improvement.",60,80
"[Summary]
- This work proposes a new complex latent space described by convolutional manifold, and this manifold can map the image in a more robust manner (when some part of the image are to be restored).

[Pros]
- The results show that the latent variable mapped to the image well represents the image, and it will be helpful for the image restoration problem.
- it seems novel to adapt the idea of DIP for defining complex latent space.

[Cons]
- The main concern is that there is no guarantee that the defined latent space is continuous. 
It means that it is difficult to judge whether the interpolated point (phi_in, s_in) between two points: (phi_1, s_1) and (\phi_2, s_2), will be matched to the image distribution. 
Equation 2 in the paper seems that it just fit the generator parameter theta to map the phi_i and x_i and memorize the mapping between the training images and the given latent convolutional variables. 
If the proposed algorithm just memorizes the training image and map them into given the latent convolution, the result cannot justify the proposal that the author proposes a new latent space.

[Summary]
- This work proposes an interesting idea of defining complex latent space, but It is doubtful that this work just memorized the mapping between the training images and the latent convolutional parameters.
- I want to see the (latent space) interpolation test for the proposed latent convolutional space. If the author provides a profound explanation of the problem, I would consider changing the rating.

--------------------------
See the additional comment for the changed rating
","The sentiment of the review is mixed. The reviewer acknowledges the novelty and potential usefulness of the proposed method, which is positive, but also expresses significant concerns about the continuity of the latent space and the possibility that the method may just memorize the training data, which is negative. Therefore, the sentiment score is balanced between positive and negative aspects. The language used in the review is polite and constructive, as the reviewer provides specific feedback and suggests a way for the authors to address the concerns, indicating a willingness to reconsider the rating if improvements are made.",0,80
"This paper proposed to use dropout to randomly choose only a subset of neural network as a potential way to perform exploration. The dropout happens at the beginning of each episode, and thus leads to a temporally consistent exploration. The paper shows that with small amount of Gaussian multiplicative dropout, the algorithm can achieve the state-of-the-art results on benchmark environments. And it can significantly outperform vanilla PPO for environments with sparse rewards.

The paper is clearly written. The introduced technique is interesting. I wonder except for the difference of memory consumption, how different it is compared to parameter space exploration. I feel that it is a straightforward extension/generalization of the parameter space exploration. But the stochastic alignment and policy space constraint seem novel and important.

The motivation of this paper is mostly about learning with sparse reward. I am curious whether the paper has other good side effects. For example, will the dropout cause the policy to be more robust? Furthermore, If I deploy the learning algorithm on a physical robot, will the temporally consistent exploration cause less wear and tear to the actuators when the robot explores. In addition, I would like to see some discussions whether this technique could be applied to off-policy learning as well.

Overall, I like this paper. It is well written. The method seems technically sound and achieves good results. For this reason, I would recommend accepting this paper.","The sentiment of the review is highly positive, as the reviewer praises the clarity, novelty, and technical soundness of the paper, and ultimately recommends its acceptance. Therefore, the sentiment score is 90. The politeness of the language is also very high, as the reviewer uses polite and constructive language throughout the review, asking questions and making suggestions in a respectful manner. Thus, the politeness score is 100.",90,100
"The paper presents and discusses a new phenomenon that infrequent words tend to learn degenerate embeddings. A cosine regularization term is proposed to address this issue.

Pros
1. The degenerate embedding problem is novel and interesting.
2. Some positive empirical results.

Cons and questions
1. The theory in Section 4 suggests that the degeneration problem originates from underfitting; i.e., there's not enough data to fit the embeddings of the infrequent words, when epsilon is small. However, the solution in Section 5 is based on a regularization term. This seems contradictory to me because adding regularization to an underfit model would not make it better. In other words, if there's not enough data to fit the word embeddings, one should feed more data. It seems that a cosine regularization term could only make the embeddings different from each other, but not better.
2. Since this is an underfitting problem (as described in Section 4), I'm wondering what would happen on larger datasets. The claims in the paper could be better substantiated if there are results on larger datasets like WT103 for LM and en-fr for MT. Intuitively, by increasing the amount of total data, the same word gets more data to fit, and thus epsilon gets large enough so that degeneration might not happen.
3. ""Discussion on whether the condition happens in real practice"" below Theorem 2 seems not correct to me. Even when layer normalization is employed and bias is not zero, the convex hull can still contain the origin as long as the length of the bias vector is less than 1. In fact, this condition seems fairly strong, and surely it will not hold ""almost for sure in practice"".
4. The cosine regularization term seems expensive, especially when the vocab size is large. Any results in terms of computational costs? Did you employ tricks to speed it up?
5. What would happen if we only apply the cosine term on infrequent words? An ablation study might make it clear why it improves performance.

UPDATE:
I think the rebuttal addresses some of my concerns. I am especially glad to see improvement on en-fr, too. Thus I raised my score from 5 to 7.","The sentiment of the review is moderately positive. The reviewer acknowledges the novelty and interest of the problem and notes some positive empirical results. However, they also raise several critical points and questions about the methodology and theoretical consistency. The update indicates that the rebuttal addressed some concerns, leading to an increased score, which further suggests a positive shift in sentiment. Therefore, the sentiment score is 40. The politeness of the language is high. The reviewer uses polite and constructive language throughout, even when pointing out issues. They phrase their criticisms as questions and suggestions rather than direct negative statements, and they express appreciation for the authors' rebuttal. Thus, the politeness score is 80.",40,80
"The authors take the control-as-inference viewpoint and learn a state-independent prior (which is typically held fixed). They claim that this leads to better exploration when actions have different importance. They relate this objective to a mutual information constrained RL objective in a limiting case. They then propose a practical algorithm, MIRL and compare their algorithm against DQN and Soft Q-learning (SQL) on 19 Atari games and demonstrate improvements over both.

Generally I found the idea interesting and at a high level the deficiency of entropy regularization makes sense. However, I had great trouble understanding the reasoning behind their method and did not find the connection to mutual information helpful. Furthermore, I had a number of questions about the experiments. If the authors can clarify their motivation and reasoning and strengthen the experiments, I'd be happy to raise my score.

In Sec 3.1, why is it sensible to optimize the prior? Can the authors give intuition for maximizing \log p(R = 1) wrt to the prior? This is critical for justifying their approach. Currently, the authors provide a connection to MI, but don't explain why this matters. Does it justify the method? What insight are we supposed to take away from that? 

The experiments could be strengthened by addressing the following:
* What was epsilon during training? Why was epsilon = 0.05 in evaluation? This is quite high compared to previous work, and it makes sense that this would degrade MIRLs performance less than DQN and SQL.
* What is the performance of SQL if we use \rho as the action selector in \epsilon-greedy. This would help understand if the performance gains are due to the impact on the policy or due to the changes in the behavior policy.
* Plotting beta over time
* Comparing the action distributions for SQL and MIRL to understand the impact of the penalty. In general, a deeper analysis of the impact on the policy is important. 
* Are their environments we would expect MIRL to outperform SQL based on your theoretical understanding? Does it?
* How many seeds were run per game?
* How and why were the 19 games selected from the full set?

Comments:

The abstract claims state-of-the-art performance, however, what is actually shown is that MIRL outperforms DQN and SQL.

With a fixed prior, the action prior can be absorbed into the reward (e.g., Levine 2018), so it is of no loss of generality to assume a uniform prior.

Could state that the stationary distribution is assumed to exist and be unique.

In Sec 3.1, why is the prior state independent?

In Sec 3.1, p(R = 1|\tau) is defined to be proportional to exp(\beta \sum_t r_t). Is this well-specified? How would we compute the normalizing constant since p(R = 0 | \tau) is not defined?

Throughout, I suggest that the authors not use the phrases ""closed form"" and ""analytic"" for expressions that are in terms of intractable quantities. 

It should be noted that Sec 3.2 Optimal policy for a fixed prior \rho follows from Levine 2018 and others by transforming the fixed prior into a reward bonus.

In Sec 3.2, the last statement does not appear to be necessary for the next subsection. Remove or clarify?

I believe that the connection to MI can be simplified. Plugging in the optimal \rho into Eq 3, we can see that Eq 3 simplifies to \max_\pi E_q[ \sum_t \gamma^t r_t] - (1 - gamma)/\beta MI_p(s, a) where p(s, a) = d^\pi(s) * \pi(a | s) and d^\pi is the discounted state visitation distribution. Thus Eq 3 can be thought of as a lower bound on the MI regularized objective.

In Sec 4, the authors state the main difference between their soft operator and the typical soft operator. What other differences are there? Is that the only one?

Sec 5 references the wrong Haarnoja reference in the first paragraph.

In Sec 5, alpha_beta = 3 * 10^5. Is that correct?

=====
11/26
At this time, the authors have not responded to the reviews. I have read the other reviews and comments, and I'm not inclined to change my score.

====
12/7
The authors have addressed most of my concerns, so I have raised my score. I'm still concerned that the exploration epsilon is quite different than existing work (e.g., https://github.com/google/dopamine/tree/master/baselines).","The sentiment of the review is mixed but leans towards positive. The reviewer finds the idea interesting and acknowledges improvements over existing methods, but also expresses significant confusion and requests clarifications and additional experiments. Therefore, the sentiment score is 20. The language used is generally polite and constructive, with the reviewer expressing willingness to raise their score if concerns are addressed and providing detailed feedback. Thus, the politeness score is 80.",20,80
"The authors introduce an algorithm that addresses the problem of online policy adaptation for model-based RL. The main novelty of the proposed approach is that it defines an effective algorithm that can easily and quickly adapt to the changing context/environments. It borrows the ideas from model-free RL (MAML) to define the gradient/recursive updates of their approach, and it incorporates it efficiently into their model-based RL framework. The paper is well written and the experimental results on synthetic and real world data show that the algorithm can quickly adapt its policy and achieve good results in the tasks, when compared to related approaches. 

While applying the gradient based adaptation to the model-free RL is trivial and has  previously been proposed, in this work the authors do so by also focusing on the ""local"" context (M steps within a K-long horizon, allowing the method to  recover quickly if learning from contaminated data, and/or its global policy cannot generalize well to the local contexts. Although this extension is trivial it seems that it has not been applied and measured in terms of the adaptation ""speed"" in previous works. Theoretically, I see more value in their second approach where they investigate the application of fast parameter updates within model-based RL, showing that it does improve over the MAML-RL and non-adaptive model-based RL approaches. This is expected but  to my knowledge has not been investigated to this extent before. 

What I find is lacking in this paper is insight into how sensitive the algorithm is in terms of the K/M ratio, and also how it affects the adaptation speed vs performance (tables 3-5 show an analysis but those are for different tasks); no theoretical analysis was performed to provide deeper understanding of it. The model does solve a practical problem (reducing the learning time and having more robust model), however, it would add more value to the current state of the art in RL if the authors proposed a method for optimal selection of the recovery points and also window ratio R/L depending on the target task. This would make a significant theoretical contribution and the method could be easily applicable to a variety of tasks. where the gains in the adaptation speed are important.","The sentiment of the review is generally positive, as the reviewer acknowledges the novelty and effectiveness of the proposed algorithm, praises the writing quality, and highlights the successful experimental results. However, the reviewer also points out some limitations and areas for improvement, which slightly tempers the overall positive sentiment. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, providing specific recommendations without being harsh or dismissive. Thus, the politeness score is 90.",60,90
"The idea is nice. It is well aligned with tools that are needed to understand neural networks. However, the experiments feel like they are missing motivation as to why this method is being used. The paper does not provide very significant evidence that this method is useful. The negation example is nice but this doesn't seem to display the potential power of the method to understand a neural network.

More motivation for experimental section is needed. If the authors don't discuss a motivation then how will a reader know how to apply the tool? It seems there is no conclusion to take away from the experiments in section 5 (convolutions). 

The authors should rethink the structure of the experimental section from the standpoint of convincing someone to use this method. In section 4.1 the authors have a good discussion on what is wrong with other methods in order to motivate their approach but then they don't deliver significant evidence in the later part of the section.

The paper needs more discussion and experiments to explain how and why to use this approach. 

While the authors say ""attributing a deep network’s prediction to its input is well-studied"" they don't compare directly against these methods. 

There are many typos and grammar errors

While I think the paper could be much more impactful if the experimental section was greatly reworked; I believe the first 5 pages of the paper are a very good contribution and it should be accepted.
","The sentiment of the review is mixed but leans towards positive. The reviewer acknowledges the value and alignment of the idea with current needs in neural network research, which is a positive sentiment. However, they also express significant concerns about the lack of motivation and evidence in the experimental section, which brings the sentiment down. Therefore, the sentiment score is 20. The politeness of the language is generally high. The reviewer provides constructive criticism and offers specific recommendations without using harsh or rude language. They also end on a positive note, suggesting that the paper should be accepted if improvements are made. Therefore, the politeness score is 80.",20,80
"This paper presents a method for distilling multiple teacher networks into a student, by linearly combining feature representations from all networks at multiple intermediate layers, and gradually forcing the student to ""take over"" the learned combination.  Networks to be used as teachers are first pretrained on various initial tasks.  A student network is then trained on a target task (possibly different from any teacher task), by combining corresponding hidden layers from each teacher using learned linear remappings and weighted combinations.  Learning this combination allows the system to find appropriate teachers for the target task; eventually, a penalty on the combination weights forces all weight onto the student network, resulting in the distillation.

Applications to both reinforcement learning (atari game) and supervised image classification (cifar, svhn) are evaluated.  The reinforcement learning application is particularly fitting, since combining tasks together is less straightforward in this domain.

I wonder whether any experiments were performed where the layers correspondence between teacher models was less clear --- say, using teachers with different architectures.  Figure 1(a) (different teacher archs) as well as the text (""candidate set"" on p.4) indicate this is possible, but experiment details describe combinations of same-architecture teachers only.

In addition, I would have liked to see some further exploration of the KL term and use of ""theta_old"".  This seems potentially important, and also has ties to self-ensembling through teachers with exponential weight averaging.  Could an average network also be used here?  And how important is this term in linking student to teachers as the weights change?

Overall I find this a very interesting approach.  Rather than training a large joint model on multiple tasks simultaneously as a transfer initialization, this approach uses models already fully trained for different tasks.  This results in a potentially advantageous trade-off:  One no longer needs to carefully calibrate the different tasks and common task components in a joint model, but at the cost of requiring inference through multiple teachers when training the student.
","The sentiment of the review is generally positive, as indicated by phrases like 'Overall I find this a very interesting approach' and the detailed explanation of the method's advantages. The reviewer also provides constructive feedback and suggestions for improvement, which shows engagement and interest in the work. Therefore, the sentiment score is 80. The politeness of the language is high, as the reviewer uses polite and respectful language throughout, such as 'I wonder whether' and 'I would have liked to see,' which indicates a polite tone. Thus, the politeness score is 90.",80,90
"Prons: 
This paper provides an optimistic mirror descent algorithm to solving minmax optimization problem. Its global convergence is guaranteed under the coherence property. The experimental results are promising.

Cons: 
1.	The coherence property is still a strong assumption. The sufficient conditions provided in Corollary 3.2 and 3.3 to guarantee coherence property are too specific to cover existing GAN models.         

2.	The current theoretical contribution seems incrementally. From the perspective of operator theory, the coherence property is highly related to the pseudo-monotone property. Extragradient method to solve the pseudo-monotone VIP has already existed in the literature [1]. The proposed OMD can be simply regarded a stochastic extension of [1] and simultaneously generalize the European distance in [1] to Bregman distance. 

3.	The integrating of Adam and OMD in the experiments is very interesting. To match the experiments, we highly recommend the authors to show the convergence of OMD + Adam with or without coherence condition, rather than requiring a diminishing learning rate.

[1] Noor, Muhammad Aslam, et al. ""Extragradient methods for solving nonconvex variational inequalities."" Journal of Computational and Applied Mathematics 235.9 (2011): 3104-3108.
","The sentiment of the review is mixed but leans towards positive. The reviewer acknowledges the promising aspects of the paper, such as the optimistic mirror descent algorithm and its global convergence, as well as the interesting integration of Adam and OMD in the experiments. However, the reviewer also points out significant limitations, such as the strong assumption of the coherence property and the incremental nature of the theoretical contribution. Therefore, the sentiment score is 20. The language used in the review is polite and constructive, offering specific recommendations and citing relevant literature to support the critique. Thus, the politeness score is 80.",20,80
"... I would have liked to see some more insights.

The authors present a method for distilling knowledge from individual models to train a multilingual model. The motivation stems from the fact that while most s-o-t-a multilingual models are compact (as compared to k individual models) they fall short of the performance of the individual models. The authors demonstrate that using knowledge distillation, the performance of the multilingual model can actually be better than the individual models. 

Please find below my comments and questions.

1) The authors have done a commendable job of validating their hypothesis on multiple datasets. Solid experimentation is definitely the main strength of this paper.

2) However, this strength also makes way for a weakness. The entire experimental section is just filled with tables and numbers. The same message is repeated across these multiple tables (multi+distill > single > multi). Beyond this message there are no other insights. For example, 

- How does the performance depend on the divergence between source and target language?
- Why is there more important on some languages and less on others ?
- Why are the improvements on the TED dataset so much higher as compared to the other 2 datasets.
- What happens when the target language is something other than English? All the experiments report results from X-->English, why not in the other direction? The model then is not really ""completely"" multilingual. It is multi-source-->single target. 
- Can you comment on the total training time ?
- What happens when you do not stop the distillation even when the accuracy of the student crosses that of the teachers ? What do you mean by accuracy here? Only later when you mention that \threshold = 1 BLEU it became clear that accuracy means BLEU in this context ?

3) Is it all worth it? One disappointing factor is that end of all this effort where you train K individual models and one monolithic model with distillation, the performance gain for most language pairs is really marginal (except on the TED dataset). I wonder if the same improvements could have been obtained by even more carefully fine tuning the baseline models itself.

4) On a positive note, I like the back-distillation idea and the experiments on top-K distillation

+++++++++++++++++++
I have updated my rating after reading author's responses

","The sentiment of the review is generally positive, as the reviewer acknowledges the strengths of the paper, such as solid experimentation and the validation of the hypothesis on multiple datasets. However, the reviewer also points out several areas for improvement and raises questions about the insights and overall worth of the method. Therefore, the sentiment score is 30. The politeness of the language is quite high, as the reviewer uses phrases like 'commendable job' and 'on a positive note,' and provides constructive criticism in a respectful manner. Therefore, the politeness score is 80.",30,80
"
The authors proposed to exploit hyperbolic geometry in computing the attention mechanisms for neural networks. Specifically, they break the attention read operation into two parts: matching and aggregation. In matching step, they use the hyperbolic distance to quantify the macthing between a query and a key; in the aggregation step, they use the Einstein midpoint. Their experiments results based on synthetic and real-world data shows the new method outperforms the traditional method based on Euclidean distance. This paper is acceptable.


Question: In Figure 3(Center), the number of nodes 1000 and 1200 are pretty close. How about the results on 500 nodes and 2000 nodes? It seems the accuracy difference increases as the number of nodes increases. Is this true? 

","The sentiment of the review is positive as the reviewer acknowledges the novelty and effectiveness of the proposed method, stating that the paper is acceptable. The politeness of the language is also high, as the reviewer provides constructive feedback and poses a question in a respectful manner without any negative or harsh language.",80,90
"# Summary
This paper proposes a new kind of spherical convolution for use in spherical CNNs, and evaluates it on rigid and non-rigid 3D shape recognition and retrieval problems. Previous work has either used general anisotropic convolution or azimuthally isotropic convolution. The former produces feature maps on SO(3), which is deemed undesirable because processing 3-dimensional feature maps is costly. The latter produces feature maps on the sphere, but requires that filters be circularly symmetric / azimuthally isotropic, which limits modeling capacity. This paper proposes an anisotropic spherical convolution that produces 2D spherical feature maps. The paper also introduces an efficient way of processing geodesic / icosahedral spherical grids, avoiding complicated spectral algorithms.


# Strengths
The paper has several strong points. It is well written, clearly structured, and the mathematics is clear and precise while avoiding unnecessary complexity. Much of the relevant related work is discussed, and this is done in a balanced way. Although it is not directly measured, it does seem highly likely that the alt-az convolution is more computationally efficient than SO(3) convolution, and more expressive than isotropic S2 convolution. The most important contribution in my opinion is the efficient data structure presented in section 4, which allows the spherical convolution to be computed efficiently on GPUs for a grid that is much more homogeneous than the lat/lon grids used in previous works (which have very high resolution near the poles, and low resolution at the equator). The idea of carving up the icosahedral grid in just the right way, so that the spherical convolution can be computed as a planar convolution with funny boundary conditions, is very clever, elegant, and practical.


# Weaknesses
There is however a misunderstanding about the properties of the alt-az convolution that must be cleared up before this paper can be published. To start with, the set of rotations R(phi, nu, 0) called the alt-az group in this paper is not a group in the mathematical sense. This easy to see, because a composition of rotations of the form Rz(phi) Ry(nu) is not generally of that form. For instance we can multiply Rz(phi) Ry(nu) by the element Rz(omega)Ry(0) = Rz(omega), which gives the element Rz(phi) Ry(nu) Rz(omega). As noted in the paper, this is a general element of SO(3) (and hence not in the set of alt-az rotations). So the closure axiom of a group is violated.

This matters, because the notion of equivariance really only makes sense for a group. If a layer l satisfies l R = R l  (for R a alt-az rotation), then it automatically satisfies l RR' = RR' l, which means l is equivariant to the whole group generated by the set of alt-az rotations. As we saw before, this is the whole rotation group. This would mean that the layer is actually SO(3)-equivariant, but it has been proven [1], that any rotation equivariant layer between scalar spherical feature maps can be expressed as an azimuthally isotropic convolution. Since the alt-az convolution is not isotropic and maps between scalars on S2, it cannot be equivariant. This also becomes apparent in the experiments section, where rotational data augmentation is found to be necessary. The paper does not contain an attempted proof of equivariance, and if one tries to give one, the impossibility of doing so will become apparent.

I note that the alt-az convolution *is* equivariant to rotations in the subgroup SO(2) of rotations around the Z-axis.

Another somewhat jarring fact about the alt-az convolution is that it is not well defined on the south pole. The south pole can be represented by any pair of coordinates of the form phi in [0, 2pi], nu = +/- pi. But it is easy to see that eq. 10 will give different results for each of these coordinates, because they correspond to different rotations of the filter about the Z-axis. This is ultimately due to the fact that the set of alt-az rotations is not the same as the set of points on the sphere, topologically speaking. The set of points on the sphere can only be viewed as the quotient SO(3)/S(2).

The paragraph motivating the alt-az convolution on page 4 is not very clear, and some claims are questionable. I agree that local SO(2) invariance is too limiting. But it is not true that rotating filters is not effective in planar/volumetric CNNs, as shown by many recent papers on equivariant networks. I would suggest rewriting this paragraph to make it clearer and less speculative, and acknowledge that although rotating filters might increase computational complexity, it has often been shown very effective.


# Other comments

The experiments show that the method is quite effective. For instance, the SHREC17 results are on par with Cohen et al. and Esteves et al., presumably at a significantly reduced computational cost. That they do not substantially outperform these and other methods is likely due to the input representation, which is lossy, leading to a maximal performance shared by all three methods. An application to omnidirectional vision might more clearly show the strength of the method, but this would be a lot of work so I do not expect the authors to do that for this paper.

It would be nice to see a more direct comparison between the three definitions of spherical convolution (general SO3, isotropic S2, and anisotropic S2). Right now, the numbers reported in Cohen et al. and Esteves et al. are copied over, but there are probably many differences between the precise setup and architectures used in these papers. It would be interesting to see what happens if one uses the same architecture on a number of problems, changing only the convolution in each case.

Initially, I was a bit puzzled about why SO(3) augmentation seems to reduce accuracy in table 1. I think this is because SO(3) augmentation actually makes the classification problem harder if the input is initially aligned. Some more explanation / discussion would be good. 

It would be nice to explain the spherical parameterization in more detail. Is this operation itself rotation equivariant? 


Typos & minor issues

- Abstract: ""to extract non-trivial features"". The word non-trivial really doesn't add anything here. Similarly ""offers multi-level feature extraction capabilities"" is almost meaningless since all DL methods can be said to do so.
- Below eq. 5, D_R^{-1} should equal D_R(-omega, -nu, -phi). The order is reversed when inverting.
- ""Different notations of convolutions"" -> notions
- ""For spherical functions there is no consistent and well defined convolution operators."" As discussed above, the issue is quite a bit more subtle. There are exactly two well-defined convolution operators, but they have some characteristics deemed undesirable by the authors.
- ""rationally symmetric"" -> rotationally
- ""exact hierarchical spherical patterns"" -> extract
- It seems quite likely that the unpacking of the icosahedral/hexagonal grid as done in this paper has been studied before in other fields. References would be in order. Similarly, hexagonal convolution has a history in DL and outside.
- Bottom of page 7, capitalize ""for"".
- ""principle curvatures"" -> principal.
- ""deferent augmentation modes"" -> different
- ""inspite"" -> in spite
- ""reprort"" -> report
- ""utlize"" -> utilize
- ""computer the convolution"" -> compute


# Conclusion

Although the alt-az convolution lacks the mathematical elegance of the general anisotropic and azimuthally isotropic spherical convolutions, it still seems like a practically useful operation for some kinds of data, particularly when implemented using the homogeneous icosahedral/hexagonal grid and fast algorithm presented in this paper. Hence, I would wholeheartedly recommend acceptance of this paper if the authors correct the factual errors (e.g. the claim of SO(3)-equivariance) and provide a clear discussion of the issues. For now I will give an intermediate rating to the paper.


[1] Kondor, Trivedi, ""On the Generalization of Equivariance and Convolution in Neural Networks to the Action of Compact Groups""","The sentiment of the review is generally positive, as the reviewer acknowledges the strengths of the paper, such as its clear structure, precise mathematics, and practical contributions. However, the reviewer also points out significant weaknesses that need to be addressed before publication, which tempers the overall sentiment. Therefore, the sentiment score is 50. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, even when pointing out weaknesses and suggesting improvements. Therefore, the politeness score is 80.",50,80
"The submission builds up on recent advances in neural density estimation to develop a new algorithm for imitation learning based on a probabilistic model for predecessor state dynamics. In particular, the method trains masked autoregressive flows as a probabilistic model for state action pairs conditioned on future states. This model is used to estimate the gradient of the stationary distribution of a policies visited states. Finally, the proposed objective uses this estimate and the gradient of the log likelihood of expert actions under the policy to maximise the similarity of the expert’s and agent’s stationary state-action distributions. 

The proposed method outperforms existing imitation learning approach (GAIL & BC) on 2 simulation-based manipulation tasks. It performs particularly well in terms of sample efficiency. 
The magnitude of difference between the sample efficiencies of GAIL and the proposed approach seems quite surprising and it would be beneficial if the authors could explicitly state if the measured number of samples include the ones used for training of the probabilistic model as well as the policy (apologies if I have missed a section fulfilling this purpose).

While the improvements on the presented experiments are clear, the experimental section represents a small shortcoming of the submitted paper. The 2 experiments (clip and peg insertion) are quite similar in type and to not take into account other common domains e.g. locomotion tasks from the original GAIL paper. Furthermore, an additional comparison to SAIL would be recommended since the approaches are closely related as the authors acknowledge. The provided comparison with different types of available expert data is quite interesting and could possibly be extended to test other state-of-the-art methods (action-free versions of GAIL, AIRL,etc.).

Nonetheless, the paper overall presents a strong submission based on novelty & relevance of the proposed method and is recommended for publication. 

Minor issues:
- Related work: improve transitions between the section about trajectory tracking and BC.
- Ablation studies with less flexible probabilistic models would strengthen the experiment section further. 
- Add derivation from Eq. 3 to 4 and 5 to appendix to render the paper more self-contained and easier to access.
- A release of the code base would further strengthen the contributions of the submission.

General recommendation:
- The authors are encouraged to further investigate off-policy corrections for improved convergence.
","The sentiment of the review is generally positive, as indicated by phrases such as 'the paper overall presents a strong submission' and 'recommended for publication.' The reviewer acknowledges the novelty and relevance of the proposed method and its performance improvements. However, there are some criticisms and suggestions for improvement, particularly regarding the experimental section and additional comparisons. The politeness of the language is high, with the reviewer using polite phrases like 'it would be beneficial,' 'recommended,' and 'the authors are encouraged.' The reviewer also apologizes for potentially missing information, which adds to the politeness.",80,90
"RELATIONAL FORWARD MODELS FOR MULTI-AGENT LEARNING

Summary: Model free learning is hard, especially in multi-agent systems. The authors consider a way of reducing variance which is to have an explicit model of actions that other agents will take. The model uses a graphical structure and the authors argue it is a) interpretable, b) predicts actions better and further forward than competing models, c) can increase learning speed.

Strong Points:
-	The main innovation here is that the model uses a graph conv net-like architecture which also allows for interpretable outputs of “what is going on” in a game.
-	The authors show that the RFM increases learning speed in several games
-	The authors show that the RFM does somewhat better at forward action prediction than a naïve LSTM+MLP setup and other competing models

Weak Point
-	The RFM is compared to other models in predicting forwards actions but is not compared to other models in Figure 5, so it is not clear that the graphical structure is actually required to speed up learning. I would like to see these experiments added before we can say that the RFM is adding to performance.
-	Related: The authors argue that an advantage of the RFM is that it is interpretable, but I thought a main argument of Rabinowitz et. al. was that simple forward models similar to the LSTM+MLP here were also interpretable? If the RFM does not improve learning above and beyond the LSTM+MLP then the argument comes down to more accurate action prediction (ok) and more interpretability (maybe) which is less compelling.

Clarifying Questions
-	How does the 4 player Stag Hunt work? Do all 4 agents have to step on the Stag together or just 2 of them? How are rewards distributed? Is there a negative payoff for Hunting the stag alone as in the Peysakhovich & Lerer paper?
-	Related: In the Stag Hunt there are multiple equilibria, either agents learn to get plants (which is safe but low payoff) or they learn to Hunt (which is risky but high payoff). Is the RFM leading to more convergence to the Hunting state or is it simple leading agents to learn the safe but low payoff strategies faster?
- The choice of metric in Figure 2 (# exactly correct prediction) is non-standard (not saying it is wrong). I think it would be good to also see a plot of a more standard metric such as loglikelihood of the model's for each of X possible steps ahead. It would help to clarify where the RFM is doing better (is it better at any horizon or is it just able to look further forward more accurately than the competitors?)


","The sentiment of the review is generally positive, as indicated by the acknowledgment of the strong points such as the innovation of the model, its ability to increase learning speed, and its superior action prediction compared to other models. However, there are some critical points raised, particularly regarding the need for additional experiments to confirm the model's advantages and the comparison with simpler models. The politeness of the language is high, as the reviewer uses polite and constructive language, asking clarifying questions and making suggestions for improvement without being dismissive or harsh.",60,80
"The paper extends the PGD adversarial training method (Madry et al., 2017) to Bayesian Neural Nets (BNNs). 
The proposed method defines a generative process that ties the prediction output and the adversarial input 
pattern via a set of shared neural net weights. These weights are then assinged a prior and 
the resultant posterior is approximated by variational inference.

Strength:
  * The proposed approach is incremental, but anyway novel.
  * The results are groundbreaking.
  * There are some technical flaws in the way the method has been presented, 
but the rest of the paper is very well-written.

Major Weaknesses:

  * Equation 7 does not seem to be precise. First, the notation p(x_adv, y | w) is severely misleading. If x_adv is also an input, no matter if stochastic or deterministic, the likelihood should read p(y | w, x_adv). Furthermore, if the resultant method is a BNN with an additional expectation on x_adv, the distribution employed on x_adv resulting from the attack generation process should also be written in the form of the related probability distribution (e.g. N(x_adv|x,\sigma)).

  * Second, the constraint that x_adv should lie within the \gamma-ball of x has some implications on the validity of
the Jensen's inequality, which relates Equation 7 to proper posterior inference.

  * Blundell et al.'s algorithm should be renamed to ""Bayes-by-BACKprop"". This is also an outdated inference technique for quite many scenarios including the one presented in this paper. Why did not the authors benefit from the local reparametrization trick that enjoy much lower estimator variance? There even emerge sampling-free techniques that nullify this variance altogether and provide much more stable training experience.

And Some Minor Issues:

  * The introduction part of paper is unnecessarily long and the method part is in turn too thin. As a reader, I would prefer getting deeper into the proposed method instead of reading side material which I can also find in the cited articles.

  * I do symphathize and agree that Python is a dominant language in the ML community. Yet, it is better scientific writing practice to provide language-independent algorithmic findings as pseudo-code instead of native Python.

Overall, this is a solid work with a novel method and very strong experimental findings. Having my grade discounted due to the technical issues I listed above and the limitedness of the algorithmic novelty, I still view it as an accept case.","The sentiment of the review is generally positive, as indicated by phrases like 'novel method,' 'very strong experimental findings,' and 'solid work.' However, the reviewer also points out several major and minor weaknesses, which slightly temper the overall positive sentiment. Therefore, the sentiment score is 60. The politeness of the language is quite high, as the reviewer uses polite phrases such as 'I would prefer,' 'It is better scientific writing practice,' and 'I do sympathize and agree.' The criticisms are framed constructively, and the overall tone is respectful. Therefore, the politeness score is 80.",60,80
"The results are intriguing. However, similar methods like BN-LSTM [3] and Variational RNNs [4] achieve arguably the same with very similar mechanisms. We do not think they can be considered as orthogonal. This should be addressed by the authors. Also, hard long-term experiments like sequentially predicting pixels (like through MDLSTM-based PixelRNN) or language modelling should be favoured over short sentence image captions. 

It is possible that we will improve our ratings once our concerns are addressed.

Paper Summary:

The authors claim that the gradient along the computational path that goes through the cell state (the linear temporal path or A gradient) of an LSTM carries information about long-term dependencies. Those gradients can be corrupted by the gradient of all other computational paths (i.e. the B gradient). They claim that this makes it hard to learn long-term dependencies and has, therefore, significant negative effects on the convergence speed, training stability, and generalisation performance. They propose a method called h-detach and run experiments on the delayed copy task, sequential MNIST, permuted sequential MNIST (pMNIST), and caption generation on the MS COCO dataset. All show either somewhat improved performance or much more stable learning curves. At every step, h-detach randomly drops all gradients that flow through the h of the standard LSTM, the B gradients, and only keeps the ones from the linear temporal path, the A gradients. Experiments also suggest that the A gradients carry more long-term information than B gradients and that LSTMs with h-detach do not need gradient clipping for successful training.

Positive:

The paper is written clearly. It is well structured and well motivated. H-detach is simple, effective, and somewhat novel (see below). Experiments indicate that its main benefit is training stability as well as minor performance improvements.

Negative:

We are not sure how significant these results are for the following reasons:

- MS COCO image caption generation is the only more challenging dataset, but it seems a bit misplaced as it has very short sentences, while the authors motivate their work through a focus on long-term dependencies. Why not apply h-detach to a language model such as [1] with official online implementations, e.g., [2]. A setting with PixelRNN [6] based on MD-LSTM [7] would also be a great testbed for h-detach.

- The purpose of h-detach is to scale down the B gradients. However, methods which apply e.g. BatchNorm to the hidden state learn a scale parameter which could be learned by the network explicitly. For the backward pass, this has the effect of scaling down the B gradient. Consider e.g. [3] which also achieves similar training stability on sequential MNIST and pMNIST with little overhead. 

- Another very related method is [4] which properly applies a random dropout mask over the recurrent inputs that is shared across timesteps of an RNN. We think that h-detach is essentially achieving the same in a similar way.

Problems with Introduction and Related Work Section:

- The vanishing gradient problem was first described by Hochreiter in 1991 [5] (not by Bengio in 1994). 

- Intro mentions GRU as if it was separate from LSTM. Clarify that GRU is essentially a variant of vanilla LSTM with forget gates [8]. Since one gate is missing, GRU is less powerful than the original LSTM [9]. 

[1] Zaremba et al. ""Recurrent neural network regularization."" arXiv:1409.2329 (2014).
[2] https://www.tensorflow.org/tutorials/sequences/recurrent
[3] Cooijmans et al. ""Recurrent batch normalization."" arXiv:1603.09025 (2016).
[4] Gal et al. ""A theoretically grounded application of dropout in recurrent neural networks."" NIPS 2016.
[5] Hochreiter, Sepp. ""Untersuchungen zu dynamischen neuronalen Netzen."" Diploma thesis, TUM (1991)
[6] Oord et al. ""Pixel recurrent neural networks."" arXiv preprint arXiv:1601.06759 (2016).
[7] Graves et al. ""Multi-Dimensional Recurrent Neural Networks"" arXiv preprint arXiv:0705.2011 (2011).
[8] Gers et al. “Learning to Forget: Continual Prediction with LSTM.“ Neural Computation, 12(10):2451-2471, 2000. 
[9] Weiss et al. On the Practical Computational Power of Finite Precision RNNs for Language Recognition. arXiv:1805.04908.


Comments after rebuttal:

The  paper has clearly improved. 

It leaves a few questions open though. For example, it is surprising that h-detach doesn't work on language modelling since Dropout-LSTM and BN-LSTM clearly improve over vanilla LSTM in this case (if not every case). In the new version, the authors only reference it in one or two sentences but don't discuss this in detail. 

When dropout is mentioned, one should also mention that dropout is a variant of the old stochastic delta rule:

Hanson, S. J. (1990). A Stochastic Version of the Delta Rule, PHYSICA D,42, 265-272.  See also arXiv:1808.03578 

Nevertheless, we now think that this is a very interesting LSTM regularization paper that people who study this field should probably know. We are increasing the score by 2 points!

","The sentiment of the review is mixed but leans towards positive. The reviewer acknowledges the intriguing results and the clear, well-structured, and motivated writing of the paper. However, they also express significant concerns about the novelty and significance of the results, suggesting that similar methods achieve comparable outcomes. The sentiment score is therefore 20, indicating a slightly positive but cautious stance. The politeness of the language is generally high, as the reviewer provides constructive feedback and acknowledges the improvements made after the rebuttal. The language is respectful and professional, leading to a politeness score of 80.",20,80
"This paper presents a class of neural networks that does not have bad local valleys. The “no bad local valleys” implies that for any point on the loss surface there exists a continuous path starting from it, on which the loss doesn’t increase and gets arbitrarily smaller and close to zero. The key idea is to add direct skip connections from hidden nodes (from any hidden layer) to the output.

The good property of loss surface for networks with skip connections is impressive and the authors present interesting experimental results pointing out that
* adding skip connections doesn’t harm the generalization.
* adding skip connections sometimes enables training for networks with sigmoid activation functions, while the networks without skip connections fail to achieve reasonable performance.
* comparison of the generalization performance for the random sampling algorithm vs SGD and its connection to implicit bias is interesting.

However, from a theoretical point of view, I would say the contribution of this work doesn’t seem to be very significant, for the following reasons:
* In the first place, figuring out “why existing models work” would be more meaningful than suggesting a new architecture which is on par with existing ones, unless one can show a significant performance improvement over the other ones.
* The proof of the main theorem (Thm 3.3) is not very interesting, nor develops novel proof techniques. It heavily relies on Lemma 3.2, which I think is the main technical contribution of this paper. Apart from its technicality in the proof, the statement of Lemma 3.2 is just as expected and gives me little surprise, because having more than N hidden nodes connected directly to the output looks morally “equivalent” to having a layer as wide as N, and it is known that in such settings (e.g. Nguyen & Hein 17’) it is easy to attain global minima.
* I also think that having more than N skip connections can be problematic if N is very large, for example N>10^6. Then the network requires at least 1M nodes to fall in this class of networks without bad local valleys. If it is possible to remove this N-hidden-node requirement, it will be much more impressive.

Below, I’ll list specific comments/questions about the paper.
* Assumption 3.1.2 doesn’t make sense. Assumption 3.1.2 says “there exists N neurons satisfying…” and then the first bullet point says “for all j = 1, …, M”. Also, the statement “one of the following conditions” is unclear. Does it mean that we must have either “N satisfying the first bullet” or “N satisfying the second bullet”, or does it mean we can have N/2 satisfying the first and N/2 satisfying the second?
* The paper does not describe where the assumptions are used. They are never used in the proof of Theorem 3.3, are they? I believe that they are used in the proof of Lemma 3.2 in the appendix, but if you can sketch/mention how the assumptions come into play in the proofs, that will be more helpful in understanding the meaning of the assumptions.
* Are there any specific reasons for considering cross-entropy loss only? Lemma 3.2 looks general, so this result seems to be applicable to other losses. I wonder if there is any difficulty with different losses.
* Are hidden nodes with skip connections connected to ALL m output nodes or just some of the output nodes? I think it’s implicitly assumed in the proof that they are connected to all output nodes, but in this case Figure 2 is a bit misleading because there are hidden nodes with skip connections to only one of the output nodes.
* For the experiments, how did you deal with pooling layers in the VGG and DenseNet architectures? Does max-pooling satisfy the assumptions? Or the experimental setting doesn’t necessarily satisfy the assumptions?
* Can you show the “improvement” of loss surface by adding skip connections? Maybe coming up with a toy dataset and network WITH bad local valleys will be sufficient, because after adding N skip connections the network will be free of bad local valleys.

Minor points
* In the Assumption 3.1.3, the $N$ in $r \neq s \in N$ means $[N]$?
* In the introduction, there is a sentence “potentially has many local minima, even for simple models like deep linear networks (Kawaguchi, 2016),” which is not true. Deep linear networks have only global minima and saddle points, even for general differentiable convex losses (Laurent & von Brecht 18’ and Yun et al. 18’).
* Assumption 3.1.3 looked a bit confusing to me at first glance. You might want to add some clarification such as “for example, in the fully connected network case, this means that all data points are distinct.”","The sentiment of the review can be considered mixed. The reviewer acknowledges the impressive experimental results and the interesting aspects of the work, but also points out significant theoretical limitations and areas for improvement. Therefore, the sentiment score is 0, indicating a neutral stance overall. The politeness of the language is quite high; the reviewer provides constructive criticism and suggestions for improvement without being rude or dismissive. Thus, the politeness score is 80.",0,80
"There has been a lot of work on limited precision training and inference for deep learning hardware, but in most of this work, the accumulators for the multiply-and-add (FMA) operations that occur for inner products are chosen conservatively or treated as having unlimited precision. The authors address this with  an analytical method to predict the number of mantissa bits needed for partial summations during the forward, delta and gradient computation ops for convolutional and fully connected layers. They propose an information theoretic approach to argue that by using fewer bits of mantissa in the accumulator than necessary, the variance of the resulting sum is less than what it would have been if sufficient bits of mantissa were used. This is surprising to me, as quantization is usually modeled as _adding_ noise, leading to an _increase_ in variance (Mc Kinstry et al. 2018), so this is a nice counterexample to that intuition. Unfortunately the result is presented in a way that implies the variance reduction is what causes the degradation in performance, while obviously (?) it's just a symptom of a deeper problem. E.g., adding noise or multiplying by a constant to get the variance to where it should be, will not help the network converge. The variance is just a proxy for lost information. The authors should make this more clear.

Loss of variance is regarded as a proxy to the error induced/loss of information due to reduced mantissa prevision. The authors present their metric called Variance Retention Ratio (VRR) as a function of the mantissa length of product terms, partial sum (accumulator) terms, and the length of the accumulation. Thereafter, the mantissa precision of the accumulator is predicted to maintain the error of accumulation within bounds by keeping the VRR as close to 1 as possible. The authors use their derived formula for VRR to predict the minimum mantissa precision needed for accumulators for three well known networks: AlexNet, ResNet 32 and ResNet 18. For tightness analysis they present convergence results while perturbing the mantissa bits to less than those predicted by their formula, and show that it leads to more than 0.5% loss in the final test error of the network.

Some questions that the manuscript leaves open in it's current form:

0. Does this analysis only apply to ReLu networks where all the accumulated terms are positive? Would a tanh nonlinearity, e.g. in an RNN, result in a different kind of swamping behavior? I don't expect the authors to add a full analysis for the RNN case if it's indeed different, but it would be nice to comment on it. 
1. Do the authors assume that the gradients and deltas will always be within the exponent range of representation? I do not find a mention of this in the paper. In other words, are techniques like loss scaling, etc. needed in addition? Other studies in literature analyzing IEEE fp16 seem to suggest so.
2. The authors do not provide details on how they actually performed the experiments when running convergence experiments. It is not straightforward to change the bit width of the accumulator mantissa in CPU or GPU kernel libraries such as CUDNN or Intel MKL. So how do they model this?
3. On page 7, the authors point out that they provide a theoretical justification of why the chunk size should neither be too small or too large - but I do not see such a justification in the paper. More detailed explanation is needed.

There are a few minor typos at a few places, e.g.
 
1. Page 4: “… , there is a an accumulation length….”
2. Page 6: “…floaintg-point format…""

Some figures, notably 2 and 5, use text that is unreadably small in the captions. I know this is becoming somewhat common practice in conference submissions with strict pages limits, but I implore the authors to consider shaving off space somewhere else. Some of us still read on paper, or don't have the best eyes!","The sentiment of the review is generally positive, as the reviewer acknowledges the novelty and significance of the authors' approach, particularly highlighting the surprising and counterintuitive results. However, the review also points out several areas where the manuscript could be improved, indicating a balanced critique. Therefore, the sentiment score is 40. The politeness of the language used is quite high, as the reviewer uses polite phrases such as 'it would be nice to comment on it,' 'I implore the authors to consider,' and 'The authors should make this more clear.' The reviewer provides constructive feedback without being harsh or dismissive, so the politeness score is 80.",40,80
"This paper shows that deep convolutional networks (CNNs, without pooling) with a suitable prior over weights can be seen as shallow Gaussian processes (GPs) with a specific covariance function. It shows that this covariance function can be computed efficiently (when compared to previous attempts at resembling convolutional networks with GPs), with a cost that only depends linearly on the number of layers and the input dimensionality, i.e.~O(N^2 L D). 

To show the equivalence between deep CNNs and shallow GPs, the paper uses similar ideas to those proposed by Matthews et al (2018a) and Lee et al (2017), i.e. using the multivariate central limit theorem in very large networks, where in the case of this paper the limit is taken as the number of channels at each layer goes to infinity. Therefore, from a theoretical perspective, these ideas have been proposed before. However, the paper presents a novel efficient way to compute the convolutional kernel, which I believe has merits on its own. 

However, the model setting for classification (where deep CNNs have been successful) and consequent evaluation on the MNIST dataset is less than convincing. One of the main motivations for Bayesian CNNs and GPs (and the paper argue for this in the intro) is to be able to provide good uncertainty estimates. However, the classification problem is framed in a regression setting, where neither probabilistic estimates are evaluated or even provided. Indeed, only the error rate is given on Table 1. To me, this is certainly not enough for a Bayesian/GP method and it is a critical deficiency of the paper in its current form. While I understand having a non-Gaussian likelihood will complicate things and conflate the kernel contribution with the approximations, I believe it is necessary to provide and evaluate such probabilistic estimates and compare them to other GP approaches (even using other less than satisfying methods such as calibration/scaling). Along a similar vein, it is unclear what objective function was used for hyper-parameter learning but, given that the authors actually “sample hyper-parameters”, I am guessing a proper probabilistic objective such as the marginal likelihood is out of the question.

Other (perhaps minor) deficiencies is that the method is not scalable to large datasets (I am even surprised the authors managed to run this on full MNIST) and that no theoretical analysis is done (e.g. as in Mattews et al, 2018a). 

Minor comments:

* In the intro, “Other methods such as Gaussian Processes”: GPs are not a method and I believe the authors really mean here Gaussian process regression. 
* The prior variance over filters in Eq (3) divides over the number of channels.  Why does a Gaussian prior with infinite precision make sense here?
* The authors should report the state of the art of using GPs for MNIST classification using non-convolutional kernels).
","The sentiment of the review is mixed. The reviewer acknowledges the novelty and efficiency of the method proposed in the paper, which is a positive aspect. However, the reviewer also points out significant deficiencies, particularly in the model setting for classification and the lack of probabilistic estimates, which are critical for Bayesian methods. This results in a sentiment score of around 0, indicating a neutral stance overall. The politeness of the language is quite high. The reviewer uses polite and constructive language, even when pointing out deficiencies. Phrases like 'I believe it is necessary' and 'I am guessing' show a considerate tone, leading to a politeness score of 80.",0,80
"Paper’s contributions:
This paper considers the challenging problem of generalizing well to new RL tasks, based on having learned on a set of previous related RL tasks.  It considers tasks that differ only in their reward function (assume the dynamics are identical), and where the reward functions are constrained to be linear combinations over a set of given features.  The main approach, Universal Successor Features Approximators (USFAs) is a combination of two recent approaches:  Universal Value Function Approximators (UVFAs) and Generalized Policy Improvement (GPI).  The main claim is that while each of these methods leverages different types of regularity when generalizing to new tasks, USFAs are able to jointly leverage both types (and elegantly have both other methods as special cases).

Summary of evaluation:
Overall the paper tackles an important problem, and provides careful explanation and reasonably extensive results showing the ability of USFA to leverage structure.  I’m on the fence because I really wish the combination of generalization properties could be understood in a more intuitive way.  There are some more minor issues, such as lack of complexity analysis and a few notation details, that can be easily fixed.

Pros:
-	The problem of generalizing to new tasks in RL is an important open problem.
-	The paper is carefully written and provides clear explanation of most of the methods & results.

Cons:
-	The authors are diligent about trying to explain what type of regularities are exploited by each of UVFAs and GPI, and how this can be combined in USFAs.  However despite reading these parts carefully, I could not get a really good intuition, either in the methods or in the results, for the nature of the regularities exploited, and how it really differs.  Top of p.4 says that GPI generalizes well when the policy \pi(s) does well on task w’.  Can you give a specific MDP where Q is not smooth, but the policy does well?
-	There is no complexity analysis.  I would like to know the computational complexity of each of the key steps in Algorithm 1 (with comparison to simple UVFA and GPI).
-	It would be useful to see the empirical comparison with the approach of Ma et al. (2018), which also combines SFs and UFVAs. I understand there are differences in the details, but I would like to see confirmation of whether the claims about USFA’s superior ability to exploit structure is supported by results.

Minor comments:
-	The limitation to linear rewards is a reasonably strong assumption.  It would be good to support this, e.g. by references to domain that meet this assumption.
-	It seems the mathematical properties in Sec.3.1 could be further developed.
-	P.4: “Given a deterministic policy \pi, one can easily define a reward function r_\pi”.  I did not think this mapping was unique (see the literature on IRL, e.g. Ross et al.).  Can you provide a proof or reference to support this statement?
-	The definition of Q(s,a,w,z) is interesting. Can this be seen as a kernel between w and z?
-	\theta suddenly shows up in Algorithm 1. I presume these are the parameters of Q?  Should be defined.
-	The distribution used to sample policies seems to be a key step of this approach, yet not much guidance is given on how to do this in general.
","The sentiment of the review is generally positive, as the reviewer acknowledges the importance of the problem tackled by the paper and appreciates the careful explanation and extensive results provided. However, the reviewer expresses some reservations about the intuitive understanding of the combination of generalization properties and points out minor issues that need to be addressed. Therefore, the sentiment score is 50. The politeness of the language used in the review is high, as the reviewer provides constructive feedback and suggestions in a respectful and considerate manner. The reviewer uses phrases like 'I would like to know' and 'It would be useful to see,' which indicate a polite tone. Therefore, the politeness score is 80.",50,80
"This paper has 3 principal contributions: it proposes a different way of measuring mutual information in a neural network, proposes a compression score to compare different models, then empirically analyses different activation functions and L2 weights.

This work seems like a welcome addition to the IB thread. To me the most interesting result is simply that activation functions aren't simply about gradient flow, and that they may each have properties that are more or less desirable depending on the domain they might be used on. The authors are careful in the wording of their conclusions, I think with reason; while these results are useful in that there seem to be consistently different behaviors coming from different hyperparameters, information planes show a relatively qualitative part of the picture.

Quantitatively, the proposed compression score is interesting, but as the authors say, simplistic. It seems to me that we care more about the converged models than the whole training trajectory; how does this score evolve with time?

I think an important part of discussion that lacks in this paper is a more in-depth take as to how these findings relate to the Zhang et al [1] memorization vs generalization paper and its follow ups. There seem to be many links to be drawn.

This work is overall a good contribution, but I'll have to agree with the authors' conclusion that more principled analysis methods are required to have a solid grasp of the training dynamics of DNNs. The writing of the paper is good, but the writing of the captions could be improved. (the hard page limit of ICLR is 10 pages and your paper has a lot of captions, so I think investing into a bit more text would be good)


Comments:
- It might be worth to re-explain what the information plane plots are in a figure caption, not just in the text (the text also doesn't really explain that each point is a moment in training, and each thread a different layer, this paper should be readable by someone who has never seen these plots before). 
- It's not clear what is going on in figure 5, I can guess but, again, this paper should be readable by anyone in the field. You mention different initializations, but which exactly? What makes you say that 5c has no compression but that 5a does compression first? It should be explained explicitly.
- I believe what you say about Figure 8, but the plots are so similar that it is hard to compare them visually. Maybe a different kind of superposition into a single plot would better illustrate the compression effect of L2?
- Typo in the x axis caption of figures 9.
- Figure 9a is not readable in greyscale (or by a colorblind person), consider using a different symbol for the softmax scatter (and adding this symbol to the legend).
- The first Schmidhuber citation of the paper seems a bit out of place. I think he himself would say that deep learning has been going on for much longer than since 2015. (in fact I think you could just remove the entire first paragraph, it is just unnecessary boilerplate)
- Why should there be a direct correlation between compression and generalization? For example, it is known that training DNNs with soft targets improves test accuracy in classification, or even forcing softness in both targets and representations [2] also improves test accuracy.
- I'm still personally not sold on binning as a strategy to evaluate MI. Did you perform experiments that show that the observed difference is consistent if more computation is done to approximate MI, and not just an artefact of max-entropy binning?

[1] Zhang et al (2016) https://arxiv.org/abs/1611.03530
[2] Verma et al (2018) https://arxiv.org/abs/1806.05236
","The sentiment of the review is generally positive, as indicated by phrases like 'welcome addition,' 'interesting result,' and 'overall a good contribution.' However, the reviewer also points out several areas for improvement, which tempers the overall positivity. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer uses polite and constructive language throughout, such as 'It might be worth,' 'consider using,' and 'I think investing into a bit more text would be good.' Therefore, the politeness score is 80.",60,80
"The main contribution of this paper in practice seems to be a way to initialize the Continuous Matrix Space Model so that training actually converges, followed by a slightly different contrastive loss function used to train these models. The paper explores the pure matrix model and a mixed matrix / vector model, showing that both together improve on simpler methods on many benchmark tasks.

My main concern is that the chained matrix multiplication involved in this method is not substantially simpler than an RNN or LSTM sentence encoding model, and there are no comparisons of training and inference cost between the models proposed in this paper and conceptually simpler RNNs and LSTMs. The FastSent paper, used here as a baseline, does compare against some deep models, but they choose far more complex baselines such as the NMT encoding, which is trained on a very different loss function. Indeed the models proposed here do not seem to outperform fasttext and fastsent despite having fairly similar computational costs.

I think this paper could use a little more justification for when it's appropriate to use the method proposed here versus more straightforward baselines.","The sentiment of the review is moderately positive as it acknowledges the contributions of the paper and suggests improvements rather than dismissing the work entirely. The sentiment score is 20. The language used is polite and constructive, offering specific recommendations for improvement without being harsh or dismissive. The politeness score is 80.",20,80
"Understanding the stationary equilibrium helps to understand the practical performance of stochastic gradient descent. In this paper, the authors propose two fluctuation-dissipation relation to link measurable quantities and hyperparameters in the stochastic gradient descent algorithm. An advantage over the existing study is that the results here hold for any stationary state and do not need the analogy with continuous-time differential equations. Empirical results are also reported to verify these fluctuation relation.

Comments:

(1) I do not quite understand the second identity of (16). In particular, it seems that the authors replace the first two $\theta(t+1)$ with $\theta(t)$, and do not use this replacement for the third $\theta(t+1)$ (this was addressed by (1)).

(2) It would be helpful to the readers if the authors can give the deduction process of (12). It is not easy for me to understand how it holds.

(3) It is not clear to me how the second fluctuation-dissipation relation helps to determine the properties of loss function landscape.

(4) In Section 2.3.1, can you give some explanation for the harmonic approximation. Also the notation $\theta^*$ seems not to be defined.","The sentiment of the review is generally positive, as the reviewer acknowledges the advantages of the proposed method over existing studies and mentions the empirical results that verify the fluctuation relation. However, the reviewer also points out several areas of confusion and requests further clarification, which slightly tempers the overall positivity. Therefore, the sentiment score is 50. The politeness of the language is high, as the reviewer uses polite phrases such as 'I do not quite understand,' 'It would be helpful,' and 'can you give some explanation,' which indicates a respectful and constructive tone. Thus, the politeness score is 90.",50,90
"This paper talks about music translation using a WaveNet-based autoencoder architecture.  The models are trained on diverse training sets and evaluated under multiple settings.  What reported in this paper seems to be interesting and the performance sounds good. However, I have following comments/concerns. 

1. The paper is not clearly written. Its exposition needs significant improvement.  There are numerous inconsistent definitions and vague descriptions that make the reading sort of difficult. 
    a)  It would be very helpful if the authors can put up a figure for the description of  the WaveNet  autoencoder instead of just using words in Section 3.1
    b) The paper itself should be self-contained instead of referring readers to other references for the details of model architectures.
    c) The math symbols are poorly defined.  What is the definition of C in Section 3.3?   It is defined or referred to as ""domain classification network"" and also ""domain confusion network"" but nowhere to find in Fig. 1.
   d) ""C is minimizes"" -> ""minimizes""
   e)  In Section 4,  it says that ""Each batch is first used to train the adversarial discriminator"".  Which adversarial discriminator? Where to find in Fig. 1 as it is the only description of the network architecture?  

2.  The authors mentioned a couple of observations that left unanswered.  
    a)   I am surprised to see that without data augmentation, the training does not even converge. 
    b)  The conversion from unseen domains is more successful than the learned domains.
    c)  The decoder starts to be creative when the size of the latent space is reduced. 
   I sense that these observations seem to point to some (serious) generalization issues of the proposed model.  I would like to hear explanations from the authors. 


After reading the rebuttal:
The authors have addressed my major concerns with regard to this paper.   I have lifted my score.  Thanks for the nice response.","The sentiment of the review is mixed but leans towards positive. The reviewer acknowledges that the paper is interesting and the performance is good, but also points out several significant issues that need to be addressed. Therefore, the sentiment score is 20. The politeness of the language is generally high. The reviewer uses polite language and constructive criticism, even when pointing out major flaws. The reviewer also thanks the authors for their response, which further indicates politeness. Therefore, the politeness score is 80.",20,80
"The authors, motivated by work in topological graph analysis, introduce a new broadly applicable complexity measure they call  neural persistence--essentially a sum over norms of persistence diagrams (objects from the study of persistent homology).  The also provide experiments testing their parameter, primarily on MNIST with some work on CIFAR-10.

I'd like to preface my criticism with the following: this work is extremely compelling, and the results and experiments are sound.  I'm very interested to see where this goes.  Figure 2 is particularly compelling!

That said, I am extremely suspicious of proposals for measures of generalization which (1) do not make contact with the data distribution being studied, and (2) which are only tested on MNIST and CIFAR-10.  Additionally, (3) it is not clear what a ""good"" neural persistence is, a priori, and (4) I'm not entirely sure I agree with the author's assessment of their numerical data.

In more detail below:

1. At this point, there's a tremendous number of different suggested ways to measure ""generalization"" by applying different norms and bounds and measures from all of the far reaches of mathematics.  A new proposed measure **really needs** to demonstrate a clear competitive measure against other candidates.  The authors make a strong case that this measure is better than competitors from TGA, but I'm not yet convinced this measure is doing enough legwork.  For example, is it possible that a network has high neural persistence, but still has terrible test or validation error?  Why or why not?  Are there obvious counterexamples?  Are there reasons to think those obvious counterexamples aren't like trained neural networks?  These are all crucial questions to ask and answer if you want this sort of measure to be taken seriously.

2.  Most of your numerical experiments were on MNIST, and MNIST is weird.  It's getting to be a joke now in the community that your idea works on MNIST, but breaks once you try to push it to something harder.  Even Cifar-10 has its quirks, and observations that are true of some networks absolutely do not generalize to others.

3. While I'm convinced that neural persistence allows you to distinguish between networks trained in different ways, it isn't clear why I should expect a particular neural persistence to mean anything at all w.r.t. validation loss.  Are there situations in which the neural persistence has stopped changing, but validation loss is still changing appreciably?  Why or why not?

4. I'm concerned that the early stopping procedure used as a benchmark wasn't tuned as carefully as neural persistence was.  I also honestly cannot determine anything from Figure 4 except that your ""Fixed"" baseline is bad, and that persistence seems to do about the same as validation loss.  It even seems that Training loss is a better early stopping criteria (better than both validation and persistence!) from this plot, because it seems to perform just as well, and systematically stop earlier.  Am I reading this plot right (particularly for 1.0 fraction MNIST)?


This work currently seems like a strong candidate for the workshop track.  I would have difficulty raising my score above much more than a 6 without much more numerical data, and analysis of when the measure fails.

Edit: The authors have made a significant effort to address my concerns, and I'm updating my score to 7 from 5 in response.","The sentiment of the review is generally positive, as indicated by phrases like 'this work is extremely compelling' and 'the results and experiments are sound.' However, the reviewer also expresses significant concerns and provides critical feedback, which tempers the overall positivity. Therefore, the sentiment score is 40. The politeness of the language is high, as the reviewer uses polite phrases such as 'I'd like to preface my criticism' and 'I'm very interested to see where this goes,' and provides constructive feedback without being rude. Thus, the politeness score is 80.",40,80
"# Positive aspects of this submission

- This submission explores a very interesting problem that is often overlooked in sequence-to-sequence models research.

- The methodology in Sections 4 and 5 is very thorough and useful.

- Good comparison of last-h with attention representations, which gives good insight about the robustness of each architecture against adversarial attacks.

# Criticism

- In Section 3, even if the ""l1 + projection"" experiments seem to show that generating egregious outputs with greedy decoding is very unlikely, it doesn't definitely prove so. It could be that your discrete optimization algorithm is suboptimal, especially given that other works on adversarial attacks for seq2seq models use different methods such as gradient regularization (Cheng et al. 2018).
Similarly, the brute-force results on a simplified task in Appendix B are useful, but it's hard to tell whether the conclusions of this experiment can be extrapolated to the original dialog task.
Given that you also study ""o-greedy-hit"" in more detail with a different algorithm in Sections 4 and 5, I would consider removing Section 3 or moving it to the Appendix for consistency.","The review starts with positive aspects, highlighting the interesting problem addressed by the submission and the thorough methodology. This indicates a generally positive sentiment. The criticism section is constructive and offers specific suggestions for improvement without being harsh or dismissive, indicating a polite tone. Therefore, the sentiment score is high, and the politeness score is also high.",80,90
"The paper tackles a very interesting problem about representations, especially of the connectionist kind -- how do we know if the learned representations capture the compositional structure present in the inputs, and tries to come up with a systematic framework to answer that question. The framework assumes the presence of an oracle that can give us the true compositional structure. Then the author try to answer some refreshing questions about the dynamics of learning and compositionality while citing some interesting background reading.

However, I’m a bit torn about the experiments. On the one hand, I like the pedagogical nature of the experiments. They are small and should be easy to reproduce. On the other hand, all of them seem to be fairly similar kinds of composition with very few attributes (mostly bigrams). So whether the intuitions hold for more complex compositional structures is hard to say.

Nevertheless, it’s a well written paper and is a helpful first step towards studying the problem of compositionality in vector representations.


Minor points
Pg 3 “_grammar_ for composing meanings *where* licensed by derivations” seems incorrect. 
Figure 5: seems quite noisy to make the linear relationship claim

EDIT: I still think the compositions under consideration are the simpler ones. Still with the new experiments the coverage seems nicer. Given the authors plan to release their source code, I expect there will be an opportunity for the rest of the community to build on these, to test TRE's efficacy on more complex compositions. I updated my scores to reflect the change.","The sentiment of the review is generally positive, as the reviewer appreciates the interesting problem tackled by the paper and acknowledges the well-written nature of the work. However, there are some reservations about the experiments, which slightly temper the overall positive sentiment. The politeness of the language is high, as the reviewer uses respectful and constructive language throughout the review, even when pointing out areas for improvement.",70,90
"This paper proposed an interesting idea of learning representations of sets by permutation optimizations. Through learning a permutation of the elements of a set, the proposed algorithm can learn a permutation-invariant representation of that set. To deal with the underlying difficult combinatorial optimization problem, the authors proposed to relax the optimization constraints and instead optimize over the set of doubly-stochastic matrices with reparameterization using the Sinkhorn operator. The cost function of this optimization is related to a pairwise ordering cost, which compares the order for each pair of the elements.

The idea of using pairwise comparison information to learn permutations is interesting. The total cost function utilizes the comparison information and optimization over this cost function can lead to a permutation-invariant representation of the set. The idea of using the Sinkhorn operator to reparameterize the doubly-stochastic matrices makes the optimization objective differentiable. Also, the experiment results compared with some baseline algorithms showed the success of the proposed methods in many different tasks.

My major concern of the proposed method is on whether this method can be applied to large sets. Since the algorithm compares all pairs of elements in the set, we need O(N^2) comparisons for a set of size N and hence the proposed method might be slow if N is large. Is it possible to improve the efficiency for large sets?

Questions and Suggestions:

1. Since the authors wants to approximately solve the objective function in Equation (2), it is better if we can see a proof showing why this optimization problem is difficult.

2. For the experiment in Section 4.2, it seems that all methods (including the proposed methods and the baseline methods) are not performing well if the images are split to at least 4 * 4 equal-size tiles. I understand that currently the authors applied their method to the case of grid permutation by simply adding all cost functions of all rows and columns. Is it possible to extend the proposed method to the grid case in another way so that the results under this setting is better? 

3. It will be better if the authors can propose some more insights (probably with some theoretical analysis) when can the PO-U method performs better and when can the PO-LA method performs better.

4. The authors mentioned that, the proposed method can get good permutations even for only T=4 steps. What if we continue running the algorithm? Will the permutation converges stably?

5. The authors proposed to update the permutation matrix parameters in an alternative way (Equation (7)) and mentioned that this update works significantly better in the experiments. It will be great if the authors can have a theoretical analysis on why this is true since P and \tilde P can be quite different from each other for an arbitrary \tilde P matrix.


Minor comment:

I think there is a typo in Equation (5). The entry \tilde P_{pq} is related to not only the entry P_{pq}, but also the other entries of the matrix P. Hence, I think Equation (5) should be modified as a matrix multiplication.","The sentiment of the review is generally positive, as the reviewer acknowledges the interesting idea and the success of the proposed methods in various tasks. However, there are some concerns and suggestions for improvement, which slightly temper the overall positivity. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, making suggestions and asking questions in a respectful manner. Thus, the politeness score is 90.",60,90
"This paper develops a framework for evaluating the ability of neural models on answering free-form mathematical problems. The contributions are i) a publicly available dataset, and ii) an evaluation of two existing model families, recurrent networks and the Transformer. 

I think that this paper makes a good contribution by establishing a benchmark and providing some preliminary results. I am biased because I once did exactly the same thing as this paper, although at a much smaller scale; I am thus happy to see such a public dataset. The paper is a reasonable dataset/analysis paper. Whether to accept it or not depends on what standard ICLR has towards such papers (ones that do not propose a new model/new theory).

I think that the dataset generation process is well-thought-out. There are a large variety of modules, and trying to not generate either trivial or impossible problems is a plus in my opinion. The results and discussions in the main part of the paper are too light in my opinion; the average model accuracy across modules is not an interesting metric at all, although it does show that the Transformer performs better than recurrent networks. I think the authors should move a portion of the big bar plot (too low resolution, btw) into the main text and discuss it thoroughly. Details on how to generate the dataset, however, can be moved into the appendix. I am also not entirely satisfied by using accuracy as the only metric; how about using something like beam search to build a ""soft"", secondary metric?

One other thing I want to see is a test set with multiple different difficulty levels. The authors try to do this with composition, which is good, but I am not sure whether that captures the real important thing - the ability to generalize, say learning to factorise single-variable polynomials and test it on factorising polynomials with multiple variables? And what about the transfer between these tasks (e.g., if a network learns to solve equations with both x and y and also factorise a polynomial with x, can it generalize to the unseen case of factorising a polynomial with both x and y)? Also, is there an option for ""unsolvable""? For example, the answer being a special ""this is impossible"" character for ""factorise x^2 - 5"" (if your training set does not use \sqrt, of course).","The sentiment of the review is generally positive, as the reviewer acknowledges the paper's contribution to the field by establishing a benchmark and providing a public dataset. However, the reviewer also points out several areas for improvement, such as the need for more detailed discussions and additional metrics. The politeness of the language is high, as the reviewer uses phrases like 'I think' and 'in my opinion,' which soften the critique and make it more constructive.",60,80
"-- Paper Summary --

The primary contribution of this paper is the presentation of a novel ELBO objective for training BNNs which allows for more meaningful priors to be encoded in the model rather than the less informative weight priors featured in the literature. This is achieved by way of introducing a KL measure over stochastic processes which allows for priors to take the form of GP priors and other custom variations. Two approaches are given for training the model, one inspired by GANs, and a more practical sampling-based scheme. The performance of this training scheme is validated on a variety of synthetic and real examples, choosing Bayes by Backprop as the primary competitor. An experiment on contextual bandit exploration, and an illustrative Bayesian optimisation example  provided in the supplementary material showcase the effectiveness of this method in applications where well-calibrated uncertainty is particularly pertinent.

-- Critique --

This paper makes important strides towards giving more meaningful interpretations to priors in BNNs. To the best of my knowledge, the KL divergence between stochastic processes that gives rise to an alternate ELBO has not been featured elsewhere, making this a rather interesting contribution that is supplemented by suitable theorems both in the main text and supplementary material. The introductory commentary regarding issues faced with increasing the model capacity of BNNs is particularly interesting, and the associated motivating example showing how degeneracy is countered by fBNN is clear and effective.

The GAN-inspired optimisation scheme is also well-motivated. Although the authors understandably do not pursue that scheme due to the longer computation time incurred (rendering its use impractical), it would have been interesting to see whether the optimum found using this technique is superior to the sampling based scheme used throughout the remainder of the paper. The experimental evaluation is also very solid, striking an adequate balance between synthetic and real-world examples, while also showcasing fBNNs’ effectiveness in scenarios relying on good uncertainty quantification.

In spite of the paper’s indisputable selling points, I have several issues with some aspects of this submission. For clarity, I shall distinguish my concerns between points that I believe to be particularly important, and others which are less significant:

- Monte Carlo dropout (Gal & Ghahramani, 2016), and its extensions (such as concrete dropout), are widely-regarded as being one of the most effective approaches for interpreting BNNs. Consequently, I would have expected this method to feature as a competitor in your evaluation, yet this method does not even get a cursory mention in the text.

 - The commentary on GPs in the related work paints a dour picture of their scalability by mostly listing older papers. However, flexible models such as AutoGP (Krauth et al, 2017) have been shown to obtain very good results on large datasets without imposing restrictions on the choice of kernels.

 - The regression experiments all deal with a one-layer architecture, for which the proposed method is shown to consistently obtain better results. In order to properly assess the effectiveness of the method, I would also be interested in seeing how it compares against BBB for deeper architectures on this problem. Although the authors cite the results in Figure 1 as an indicator that BBB with more layers isn’t particularly effective, it would be nice to also see this illustrated in the cross-dataset comparison presented in Section 5.2.

 - Furthermore, given that all methods are run for a fixed number of iterations, it might be sensible  to additionally report training time along with the results in the table. This should reflect the pre-processing time required to optimise GP hyperparameters when a GP prior is used. Carrying out Cholesky decompositions for 1000x1000 matrices 10k times (as described in Section 5.2.2) does not sound insignificant.

- The observation regarding the potential instability of GP priors without introducing function noise should be moved to the main text; while those who have previously worked with GPs will be familiar with such issues, this paper is directed towards a wider audience and such clarifications would be helpful for those seeking to replicate the paper’s results. On a related note, I would be keen on learning more about other potential issues with the stability of the optimisation procedure, which does not seem to be discussed upfront in the paper but is key for encouraging the widespread use of such methods.

- The paper contains more than just a handful of silly typos and grammatical errors - too many to list here. This single-handedly detracts from the overall quality of the work, and I highly advise the authors to diligently read through the paper in order to identify all such issues.

 - The references are in an absolute shambles, having inconsistent citation styles, arXiv papers cited instead of conference proceedings, etc. While this is obviously straightforward to set right, I’m nonetheless disappointed that this exercise was not carried out prior to the paper’s submission.

 - The theory presented in Appendix A of the supplementary material appears to be somewhat ‘dumped’ there. Given that this content is crucial for establishing the correctness of the proposed method, linking them more clearly to the main text would improve its readability and give it a greater sense of purpose. I found it hard to follow in its current state.

** Minor **

 - In the introduction there should some mention of deep Gaussian processes which are implicitly a direct competitor to BNNs, and can now also be scaled to millions and billions of observations (Cutajar et al. 2017; Salimbeni et al. 2017). The former is particularly relevant to this work since the architecture can be assimilated to a BNN with special structure for emulating certain kernels.

 - Experiment 5.1.1 is interesting, and the results in Figure 2 are convincing. I would also be interested in seeing how fBNN performs when the prior is misspecified however, which may be induced by using a less appropriate GP kernel. This would complement the already provided insight on using tanh vs ReLU activations.

 - The performance improvement for the experiment on large regression datasets is quite subdued, so it might be interesting to see how both methods compare against each other when deeper BNN architectures are considered. 

- With regards to Appendix C.2, which order arccosine kernel is being used here? One can easily draw similarities between the first order arccosine kernel and NN layers with ReLUs, so perhaps it would be useful to specify which order is being used in the experiment.  

- Given that the data used for experiments in Appendix C.3 effectively has grid structure, I would be interested in seeing how KISS-GP performs on this task. There should be easily accessible implementations in GPyTorch for testing this out. Given how GPs tend to not work very well on image completion tasks due to smoothness in the kernel, this comparison may also be in fBNNs favour.

- Restating the basic architecture of the BNN being used for the contextual bandits experiment in the paper itself would be helpful in order to avoid having to separately check out Riquieme et al (2018) to find such details.

- I wonder if the authors have already thought about the extendability of their proposal to more complex BNN architectures such as Bayesian ConvNets?


-- Recommendation --

Whereas several ICLR submissions tend heavily towards validation by way of empirical evaluation, I find that the theoretic contributions presented in this paper are by themselves interesting and well-developed, which is very commendable. However, there are multiple telling signs of this being a rushed submission, and I am less inclined to argue ardently for such a paper’s acceptance. Although the paper indeed has its strong points, both in terms of novelty and varied experimental evaluation, in view of this overall lack of finesse and other concerns listed above, I think that the paper is in dire need of a thorough clean-up before being published.

Pros/Cons summary:

+   Interesting concepts that extend beyond empirical fixes.
+   Defining more interpretable priors is a very pertinent topic in the study of BNNs.
+   The presented ideas could potentially have notable impact.
+   Illustrative experiments and benchmark tests are convincing.
-   Not enough connection to MC dropout.
-   Choice of experiments and description of stochastic processes overly similar to other recent widely-publicised papers. It feels on trend, but consequently also somewhat reductive.
-   More than a few typos and grammatical errors.
-   Presentation is quite rough around the edges. The references are in a particularly dire state.","The sentiment of the review is generally positive, as the reviewer acknowledges the paper's significant contributions, interesting concepts, and potential impact. However, the reviewer also highlights several critical issues, such as the lack of connection to MC dropout, choice of experiments, numerous typos, and poor presentation, which detract from the overall quality. Therefore, the sentiment score is not entirely positive but leans towards the positive side. The politeness of the language is high, as the reviewer provides constructive feedback and suggestions for improvement without being rude or dismissive.",40,80
"The paper proposes a new discriminator loss for MMDGAN which encourages repulsion between points from the target distribution. The discriminator can then learn finer details of the target distribution unlike previous versions of MMDGAN. The paper also proposes an alternative to the RBF kernel to stabilize training and use spectral normalization to regularize the discriminator. The paper is clear and well written overall and the experiments show that the proposed method leads to improvements. The proposed idea is promising and a better theoretical understanding would make this work more significant. Indeed, it seems that MMD-rep can lead to instabilities during training while this is not the case for MMD-rep as shown in Appendix A. It would be good to better understand under which conditions MMD-rep leads to stable training. Figure 3 suggests that lambda should not be too big, but more theoretical evidence would be appreciated.
Regarding the experiments: 
- The proposed repulsive loss seems to improve over the classical attractive loss according to table 1, however, some ablation studies might be needed: how much improvement is attributed to the use of SN alone? The Hinge loss uses 1 output dimension for the critic and still leads to good results, while MMD variants use 16 output dimensions. Have you tried to compare the methods using the same dimension?
-The generalized spectral normalization proposed in this work seems to depend on the dimensionality of the input which can be problematic for high dimensional inputs. On the other hand, Myato’s algorithm only depends on the dimensions of the filter. Moreover, I would expect the two spectral norms to be mathematically related [1]. It is unclear what advantages the proposed algorithm for computing SN has.
- Regarding the choice of the kernel, it doesn’t seem that the choice defined in eq 6 and 7 defines a positive semi-definite kernel because of the truncation and the fact that it depends on whether the input comes from the true or the fake distribution. In that case, the mmd loss loses all its interpretation as a distance. Besides, the issue of saturation of the Gaussian kernel was already addressed in a more general case in [2]. Is there any reason to think the proposed kernel has any particular advantage?

Revision:

After reading the author's response, I think most of the points were well addressed and that the repulsive loss has interesting properties that should be further investigated. Also, the authors show experimentally the benefit of using PICO ver PIM which is also an interesting finding.
I'm less convinced by the bounded RBF kernel, which seems a little hacky although it works well in practice. I think the saturation issues with RBF kernel is mainly due to discontinuity under the weak topology of the optimized MMD [2] and can be fixed by controlling the Lipschitz constant of the critic.
Overall I feel that this paper has two interesting contributions (Repulsive loss + highlighting the difference between PICO and PIM) and I would recommend acceptance.






[1]: Sedghi, Hanie, Vineet Gupta, and Philip M. Long. “The Singular Values of Convolutional Layers.” CoRR 
[2]: M. Arbel, D. J. Sutherland, M. Binkowski, and A. Gretton. On gradient regularizers for MMD GANs.



","The sentiment of the review is generally positive, as the reviewer acknowledges the clarity and promise of the proposed method, and ultimately recommends acceptance of the paper. However, there are some critical points and suggestions for improvement, which slightly temper the overall positivity. Therefore, the sentiment score is 70. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, even when pointing out areas for improvement. The reviewer also acknowledges the authors' responses positively. Therefore, the politeness score is 90.",70,90
"This work addresses the problem of learning latent embeddings of high-dimensional time series data. The paper emphasises the need of interpretable representations accounting for the correlated nature of temporal data. To this scope, the study proposes to cluster the data in a latent space estimated through an auto-encoder. The clustering is obtained by leveraging on the idea of self-organising maps (SOM). Within this setting, the data is mapped into a 2D lattice where each coordinate point represents the center of an inner cluster. 
This construction motivates the formulation of the auto encoder through the definition of several cost terms promoting reconstruction, clustering, and consistency across latent mappings. 
This definition of the problem allows an heuristic for circumventing the non-differentiability of the discrete mapping. The enhance consistency over time, the model is further equipped with an additional cost term enforcing transition smoothness across data points and latent embeddings. 

The experiments are carried out with respect to synthetic 2D time-series, chaotic time-series from dynamical systems, and clinical data. In each case the proposed method shows promising results with respect to the proposed benchmark. 

The study presents some interesting methodological and technical ideas. On the other hand the manuscript presentation is quite convoluted, at the expense of a lacks of clarity in the details about the implementation of the methodology. Moreover, motivated by practical aspects, the model optimisation relies on computational strategies not completely supported from the theoretical point of view (such as the zeroing of the gradient in backpropagation, or the approximation of the clustering function to overcome non-differentiability). The impact of these modeling choices would deserve more investigation and discussion. 

Detailed comments:

- As also stated by the authors, the use of a 2D latent representation is completely arbitrary. It may be true that a 2D embedding provides a simple visualisation, however interpretability can be obtained also with much richer representations in a number of different ways (e.g. sparsity, parametric representations, …). Therefore the feeling is that the proposed structure may be quite ad-hoc, and one may wonder whether the algorithm would still generalise to more complex latent representations.
- Related to the previous comment, the number of latent points seems to be crucial to the performance of the method. However this aspect is not discussed in detail, while it would be beneficial to provide experiment about the sensitivity and accuracy with respect to the choice if this parameters.
- The method relies on several cost terms plugged together. While each of them takes care of specific consistency aspects of the model, their mutual relation and balance may be very critical. This is governed by a series of trade-off parameters whose effect is not discussed  nor explored throughout the study. I guess that the optimisation stability may be also quite sensitive to this trade-off, and it would be important to provide more details about this aspect. 
- Surprisingly, k-means seems to perform quite well in spite of its simplicity. Also, there is no mention about initialisation and choice of the parameter “k”. The authors may want to better discuss the performance of this algorithm, especially compared to its much lower modeling complexity with respect to the proposed method. 
- Still related to the comparison with respect to the state-of-art, interpretability in time series analysis can be achieved with much lesser assumptions and parameters by using standard approaches such as independent component analysis. I would expect this sort of comparison, especially in case of long-term data such as the one provided in the Lorenz system. 
- Clustering of short-term time series, such as the clinical ones, is a challenging task. The feeling is that a highly parametrised model, such as the proposed one,  may still not be superior with respect to classical methods, such as the mixture of linear regressions. This sort of comparison would be quite informative to appreciate the real value of the proposed methodology.","The sentiment of the review is mixed. The reviewer acknowledges the interesting methodological and technical ideas presented in the study, which is a positive aspect. However, they also point out several significant issues, such as the convoluted presentation, lack of clarity in implementation details, and reliance on computational strategies not fully supported theoretically. These criticisms bring the overall sentiment down. Therefore, the sentiment score is 10. The politeness of the language is quite high. The reviewer uses polite and constructive language throughout the review, even when pointing out flaws and suggesting improvements. There are no rude or harsh comments, and the tone remains professional and respectful. Therefore, the politeness score is 80.",10,80
"This paper presents Riemannian versions of adaptive optimization methods, including ADAGRAD, ADAM, AMSGRAD and ADAMNC. There are no natural coordinates on a manifold. Therefore, the authors resort to product of manifolds and view each manifold component as a coordinate. Convergence analyses for those methods are given. The the theoretical results and their Euclidean versions coincide. An experiment of embedding a tree-like graph into a Poincare model is used to show the performance of the Riemannian versions of the four methods.

This paper is well-written except a few flaws (see below). I do not have time to read the proofs carefully. The proposed methods are potentially important in some applications. Therefore, I suggest publish this paper after addressing the comments below.

Remarks:
*) P1, line 2: it particular -> in particular.
*) P3, line 9: Is R_x(v) = x + v most often chosen? A manifold is generally nonlinear. A simple addition would not give a point in the manifold.
*) P5, in Assumptions and notations paragraph: what are T and [T]? Is T the number of total iterations or the number of functions in the function family. The subscript of the function f_t seems to be an index of the functions. But its notation is also related to the number of iterations, see (8) and the algorithms in Figure 1.
*) P5, Figure 1: does a loop for the index $i$ missing?
*) Section 5: it would be clearer if the objective function is written as L:(D^n)^m \to R: \theta-> , where m is the number of nodes. Otherwise, it is not obvious to see the domain. 
*) P7, last paragraph: Tables 2 and 3 -> Figures 2 and 3.
*) Besides the application in the experiments, it would be nice if more applications, at least references, are added.

","The sentiment of the review is generally positive, as the reviewer acknowledges that the paper is well-written and the proposed methods are potentially important. The reviewer suggests publication after addressing some comments, which indicates a positive sentiment. Therefore, the sentiment score is 80. The politeness of the language is also high, as the reviewer uses polite phrases such as 'I suggest' and 'it would be nice if,' and provides constructive feedback without any rude or harsh language. Therefore, the politeness score is 90.",80,90
"This paper gives the first nonvacuous generalization bounds for
meaningful Imagenet models.  These bounds are given in terms of the
bit length of compressions of learned models together with a method
for taking into account symmetries of the uncompressed parameters.

These bounds are nonvacuous only when the compressed models are small
--- on the order of 500 Kilobytes.  State of the art compressed models
of this size achieve Imagenet accuracies slightly better than Alexnet,
16% error for top 5, and this paper reports a nonvacuous
generalization guarantees of 89% error for top 5.  While there is
still a large gap between the actual generalization and the guarantee,
this would still be a significant accomplishment.

I have one major concern.  The generalization bound involves adding an
empirical loss and a regularization term computed from a KL
divergence.  I am convinced that the authors have correctly handles
the KL divergence term.  But the paper does not contain sufficient
detail to determine if the authors correctly handle the empirical loss
term.  It is NOT correct to use the training loss of the
(deterministic) compressed model.  The generalization bound requires
that the training loss be measured under the parameter noise of the
posterior distribution.  The paper needs to be clear that this has
been done. The comments in Appendix B on noise robustness are
disturbing in this regard.

If the training loss  has been calculated correctly in the bound,
the results are significant.

Assuming correctness, I would comment that the Catoni bound, while sqeaking
out all available tightness, is very opaque.  I might be good to
consider the more transparent bounds, claimed to be essentially the
same, given in McAllester's tutorial.  If the more transparent bounds
achieve equivalent numerical results, they would make the nature of
the bounds clearer.

Another comment involves a largely ignored detail in (Dzuigaite and
Roy 17). Their bounds become vacuous if they center their Gaussian
prior at zero.  Instead they center the prior on the initial value of
the parameters.  This yields a dramatic improvement in the bound.  In
the context of the present paper, this suggests a modification of the
prior distribution on the compressed model.  We represent the model by
first selecting the r code values.  I think a distribution could be
defined on the code book that would improve its log probability, but I
will ignore that.  Given the r code values we can define a
distribution over the possible compressed representations of a weight
w_i in terms of a prior on w_i defined in terms of its initial value.
This gives a probability distribution over the compressed
representation.  Using log probability of the compressed
representation should then be a significant improvement on the first
term in (8).  This shift in the prior on compressed models has no
effect on the second term of (8) so things should only get better.
","The sentiment of the review is generally positive, as the reviewer acknowledges the significance of the results and the accomplishment of providing nonvacuous generalization bounds. However, there is a major concern regarding the handling of the empirical loss term, which slightly tempers the overall positivity. Therefore, the sentiment score is 50. The language used in the review is polite and constructive, offering specific suggestions for improvement and acknowledging the potential significance of the results if the concerns are addressed. Thus, the politeness score is 80.",50,80
"The authors splitted the features of multimodal representations to ""common"" (multimodal discriminative) and ""specific"" (modality-specific generative) factors. In this framework, their MFM can capture more detailed features. 

Pros:
(*) Learning the feature representations from two perspectives. 

(*) Even missing one modality, MFM can still achieve acceptable performance. 

(*) Using mutual information and gradient-based method to interpret their method. 

Cons:
(*) The work has some similarity to Hsu & Glass (2018), but the comparison between this work is only on CMU-MOSI.

(*) In Table. 3, it shows that language is the most informative feature for prediction. However, in Table. 2, it can be seen that if audio is missing, the result it the worse compared to the other two cases. It seems the interpretation is not convincing to me. Can you give us more explanation about this phenomenon? 

Comments:
(*) The details of SVHN-MNIST experiment are missing. Appendix B gave some information about models but specified the targeted datasets.

(*) The appendix is not clear, e.g. In Appendix B, it is said ""subsection 3.3"" but there is no section 3.3.  


","The sentiment of the review appears to be generally positive, as the reviewer highlights several pros of the work before listing the cons and comments. The pros are clearly stated and appreciated, which suggests a positive sentiment. However, the cons and comments indicate areas for improvement, which slightly tempers the overall positivity. Therefore, the sentiment score is 60. The politeness of the language is quite high. The reviewer uses polite language, such as 'Can you give us more explanation about this phenomenon?' and 'The details of SVHN-MNIST experiment are missing,' which are constructive and respectful. Therefore, the politeness score is 80.",60,80
"The authors combined several update steps together to achieve aggregated momentum. They showed that  it is more stable than the other momentum methods. Also, in Auto-encoder and image classification, AggMo outperforms than the other methods. 

Pros:
(+) Theoretical result is shown on the quadratic problem.

(+) Extensive numerical experiments are shown to illustrate the stability of AggMo.

Cons:
(+) The results are not convincing. For example, it said in the default setting (CM \beta=0.9), ResNet34 on CIFAR-10 has accuracy 90.22\%. However, it should be around 93\%.

(+)  This method is similar to multi-step gradient methods.



Comments:
(+) This is no “introduction” in the paper. 

(+) There should be “,” after mathematical equations. 
","The sentiment of the review appears to be slightly positive, as the reviewer acknowledges the theoretical results and extensive numerical experiments. However, the reviewer also points out significant issues with the results and the similarity to existing methods, which tempers the overall positivity. Therefore, the sentiment score is 20. The politeness of the language is generally neutral to slightly polite. The reviewer uses formal language and provides constructive feedback without being overly harsh or rude. Therefore, the politeness score is 10.",20,10
"# overview
This paper focuses on multi-agent reinforcement learning tasks that require communication between the agents, and further presupposes that the communication protocol is bandwidth constrained and contentious so that a scheduling mechanism is necessary.  To address this they introduce a new learned weighting scheme based scheduler and distributed actor, centralized critic based architecture which is evaluated on a couple of communication driven multi-agent tasks.

The two evaluation tasks had their bandwidth artificially constrained, and SchedNet time to convergence was shown to fall somewhere between having no communication and full communication, and somewhat better than a purely round-robin based scheduling scheme, which doesn't seem particularly informative.  From this it is difficult to assess the significance of the contributions.

# pros
* communication in multi-agent scenarios is an important aspect to consider, and this work shines a spotlight on scenarios in which bandwidth is constrained.
* general presentation fairly clear and easy to read

# cons
* Would have been more impactful to focus experiments on real-world scenarios in which bandwidth is constrained and naturally contentious

# other comments
* pg. 2 related work, suspect you meant to call out Foerster et al 2017b in second reference not Foerster et al 2017a twice.","The sentiment of the review appears to be slightly negative. The reviewer acknowledges the importance of the topic and the clarity of the presentation, which are positive points. However, the reviewer also expresses concerns about the significance of the contributions and the choice of evaluation tasks, which are negative points. Therefore, the sentiment score is slightly negative. The politeness of the language is quite high. The reviewer uses polite language throughout, providing constructive feedback without being harsh or rude.",-20,80
"In this paper the authors distinguish between two families of training objectives for seq2seq models, namely, divergence minimization objectives and max-margin objectives. They primarily focus on the divergence minimization family, and show that the MRT and RAML objectives can be related to minimizing the KL divergence between the model's distribution over outputs and the ""exponentiated payoff distribution,"" with the two objectives differing in terms of the direction of the KL. In addition, the authors propose an objective using the Hellinger distance rather than the KL divergence, and they conduct experiments on machine translation and summarization comparing all the considered objectives.

The paper is written extremely clearly, and is a pleasure to read. While the discussion of the relationship between RAML and MRT (and MRT and REINFORCE) is interesting and illuminating, many of these insights appear to have been discussed in earlier papers, and the RAML paper itself notes that it differs from REINFORCE style training in terms of the KL direction.

On the other hand, the idea of minimizing Hellinger distance is I believe novel (though related to the alpha-divergence work cited by the authors in the related work section), and it's nice that training with this loss improves over the other losses. Since the authors' results, however, appear to be somewhat below the state of the art, I think the main question left open by the experimental section is whether training with the Hellinger loss would further improve state of the art models. Even if it would not, it would still be interesting to understand why, and so I think the paper could be strengthened either by outperforming state of the art results or, perhaps through an ablation analysis, showing what aspects of current state of the art models make minimizing the Hellinger loss unnecessary.

In summary,

Pros:
- well written and interesting
- a new loss with potential for improvement over other losses
- fairly thorough experiments

Cons:
- much of the analysis is not new
- unclear if the proposed loss will improve the state of the art, and if not why 

Update after author response: thanks for your response. I think the latest revision of the paper is improved, and even though state of the art BLEU scores on IWSLT appear to be in the mid 33s, I think the improvement over the Convolutional Seq2seq model is encouraging, and so I'm increasing my score to 7. I hope you'll include these newer results in the paper.","The sentiment of the review is generally positive, as indicated by phrases like 'the paper is written extremely clearly, and is a pleasure to read' and 'the idea of minimizing Hellinger distance is I believe novel.' However, there are some critical points, such as 'many of these insights appear to have been discussed in earlier papers' and 'unclear if the proposed loss will improve the state of the art.' The politeness of the language is very high, with the reviewer using polite and constructive language throughout, such as 'thanks for your response' and 'I hope you'll include these newer results in the paper.'",60,90
"This paper proposes a Dynamic Parameter Generator (DPG) that given a test input modifies the parameters of a classification model. They also propose to regularize the training using a Data Generator (DG) to slow down catastrophic forgetting. DG is used to constrain the training that the internal representations of data generated by DG does not rapidly change. DG removes the need for storage of data or labels.

Positives:
- Both ideas of DPG and DG are novel in preventing catastrophic forgetting.
- DG is novel because it does not require storage of data and does not depend on labels.
- Experimental results are significantly better than the previous state-of-the-art.

Suggestions and clarification requests:
- Figures are very small and equations are cramped because of reduced spacing.
- There are some vague explanations in the intro that could be reduced. It would be nice to first introduce concrete math then give the intuitions. That saves some space.
- It would nice to compare to the recent Progress & compress [1]. Unfortunately, they have not provided results on benchmark MNIST tasks.
- This work is related to a recently proposed idea in architecture search [2] that learns to predict the weights of a network given its architecture.
- Can you clarify whether you have used DG at test time?
- Can you report results without using DG? It is not clear whether DPG is accountable for preventing the catastrophic forgetting or the sluggishness enforced by DG.
- Questions 1 and 2 need more formalization if the authors want to clearly prove a statement.
- As the answer to Question 1 suggests, have you explored enforcing a Lipschitz constraint?
- The answer to Question 2 is interesting. Could you rewrite it more formally? It seems like you can argue that DG’s objective encourages the employment of unused parameters which is important in tackling catastrophic forgetting.
- Can you elaborate on how much forgetting happens for DG?
- It seems that in figure 3.f and 3.c the MA method is unable to reach the best possible performance on the last task. Can you also report the table of accuracies on the last task?

[1] Schwarz, Jonathan, et al. ""Progress & Compress: A scalable framework for continual learning."" arXiv preprint arXiv:1805.06370 (2018).
[2] Brock, Andrew, et al. ""SMASH: one-shot model architecture search through hypernetworks."" arXiv preprint arXiv:1708.05344 (2017).","The sentiment of the review is generally positive, as indicated by the praise for the novelty of the DPG and DG ideas and the superior experimental results. However, there are several suggestions for improvement, which slightly temper the overall positivity. Therefore, the sentiment score is 70. The language used in the review is polite and constructive, with phrases like 'It would be nice' and 'Could you clarify,' indicating a respectful tone. Thus, the politeness score is 90.",70,90
"The authors consider the problem of learning from positive and unlabeled data in which only a subset of the true positives is labeled. While the common assumption (eg Elkan & Noto, du Plessis et al.) prescribes that the labeled set is picked independently at random from the positive set, this paper assumes that a (positive) example x is more likely to be labeled the more it exhibits positive features: formally, the higher Pr(y=1 | x), the higher Pr(o=1 | x). For instance, in the case of anomaly detection, the more likely an example is anomalous, the more likely it would get manually flagged (labeled) as positive. The authors refer to this assumption as Invariance of Order.

The proposed method requires the knowledge of the positive class prior Pr(y=1), and can be summarized in the following three steps: (i) estimate r(x)=Pr(x | y=1, o=1`)/Pr(x); (ii) find the threshold \theta such that the number of datapoints x with r(x) > \theta is a fraction Pr(y=1); (iii) train a classifier on sign(r(x) - \theta). Conceptually, the Invariance of Order assumption allows to use the order on r(x) as a proxy for an order on Pr(y=1|x), so then the knowledge of Pr(y=1) is enough to find \theta, and to port the original problem to a vanilla binary classification problem.

Concerns:
- He et al. 2018 use a very similar assumption and no comparison with that work is provided. The authors briefly mention that work in the introduction but don't perform due diligence in assessing differences/novelties with respect to that work, neither as a discussion or in the experiments.
- The requirement of knowing the fraction of positive examples is hard to justify in practice. Have you tried using the estimate obtained by Elkan et al, or other related work?
- Experiments are confusing and not convincing: apart from the very last experiment, all datasets are synthetic. No comparison with previous work is presented, except for ""unbiased PU learning (PU)"", which I assume is Elkan et al ? If that is indeed the case, which one of their methods are you comparing against? Even more troublesome is the fact that in all experiments you're providing your algorithm with the correct class-prior Pr(y=1), but it's not clear if this is provided to PU as well. You may want to consider estimating Pr(y=1) using methods from related work to see how it affects the accuracy.
- Related work discussion is completely missing apart from one paragraph in the introduction.

Minor:
- The acronym SCR is not very conventional; I would suggest IID which is often used as shorthand for independently identically distributed.
- Invariance of Order: when introducing it, you may want to add a sentence providing the intuition behind the assumption.
- Example 2 (Face recognition) is not very convincing and not very clear. Please rephrase.
- Pseudo-classification risk: why was the log-loss used? Can other losses be used as well?
- Theorem 3: add some intuition and explain tradeoff on \epsilon
- Experiments section: help the reader by adding a reminder on equations, as it's difficult to flip back and forth to their definitions. Eg, ""we trained a classifier minimizing (4) and (7) with the model (10)"" is difficult to digest and follow.
- Experiments: confusing commas in {800,1,600,3,200} => {800, 1600, 3200}
- Too many acronyms and abbreviations.

","The sentiment of the review is moderately negative, as the reviewer raises several significant concerns about the paper, including lack of comparison with related work, practical applicability of the method, and the clarity and comprehensiveness of the experiments. The reviewer does acknowledge the conceptual framework but focuses more on the shortcomings. Therefore, the sentiment score is -40. The politeness of the language is relatively high; the reviewer uses polite language and constructive criticism, offering suggestions for improvement without being rude or dismissive. Thus, the politeness score is 80.",-40,80
"The paper presents a combination of evolutionary search methods (CEM) and deep reinforcement learning methods (TD3). The CEM algorithm is used to learn a Diagional Gaussian distribution over the parametes of the policy. The population is sampled from the distribution. Half of the population is updated by the TD3 gradient before evaluating the samples. For filling the replay buffer of TD3, all state action samples from all members of the population are used. The algorithm is compared against the plane variants of CEM and TD3 as well as against the evoluationary RL (ERL) algorithm. Results are promising with a negative result on the swimmer_v2 task.

The paper is well written and easy to understand. While the presented ideas are well motivated and it is certainly a good idea to combine deep RL and evoluationary search, novelty of the approach is limited as the setup is quite similar to the ERL algorithm (which is still on archive and not published, but still...). See below for more comments:
- While there seems to be a consistent improvement over TD3, this improvement is in some cases small (e,g. ants). 
- We are learning a value function for each of the first half of the population. However, the value function from the previous individual is used to initialize the learning of the current value function. Does this cause some issues, e.g., do we need to set the number of steps so high that the initialization does not matter so much any more? Or would it make more sense to reset the value function to some ""mean value function"" after every individual?
- The importance mixing does not seem to provide a better performance and could therefore be shortened in the paper



","The sentiment of the review is generally positive, as indicated by phrases like 'The paper is well written and easy to understand' and 'Results are promising.' However, there are some criticisms regarding the novelty of the approach and specific aspects of the methodology, which slightly temper the overall positive sentiment. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, such as 'See below for more comments' and 'could therefore be shortened in the paper.' Therefore, the politeness score is 80.",60,80
"This paper analyzed the global convergence property of SGD in deep learning based on the star-convexity assumption. The claims seem correct and validated empirically with some observations in deep learning. The writing is good and easy to follow.

My understanding of the analysis is that all the claims seem to be valid when the solution is in a wide valley of the loss surface where the star-convexity holds, in general. This has been observed empirically in previous work, and the experiments on cifar10 in Fig. 2 support my hypothesis. My questions are:

1. How to guarantee the star-convexity will be valid in deep learning?
2. What network or data properties can lead to such assumption?

Also, this is a missing related work from the algorithmic perspective to explore the global optimization in deep learning: 

Zhang et. al. CVPR'18. ""BPGrad: Towards Global Optimality in Deep Learning via Branch and Pruning"".
","The sentiment of the review is generally positive. The reviewer acknowledges that the claims seem correct and validated empirically, and they find the writing to be good and easy to follow. This suggests a positive sentiment towards the paper. The politeness of the language is also high; the reviewer uses polite language, asks questions in a respectful manner, and provides constructive feedback without any negative or rude remarks.",80,90
"
The authors present a GAN based framework for Graphic Layouts. Instead of considering a graphic layout as a collection of pixels, they treat it as a collection of primitive objects like polygons. The objective is to create an alignment of these objects that mimics some real data distribution.

The novelty is a differentiable wireframe rendering layer allowing the discriminator to judge alignment. They compare this with a relation based discriminator based on the point net architecture by Qi et al. The experimentation is thorough and demonstrates the importance of their model architecture compared to baseline methods. 

Overall, this is a well written paper that proposes and solves a novel problem. My only complaint is that the most important use case of their GAN (Document Semantic Layout Generation) is tested on a synthetic dataset. It would have been nice to test it on a real life dataset.","The review is generally positive, highlighting the novelty and thorough experimentation of the paper. The sentiment score is high because the reviewer appreciates the well-written nature of the paper and the novel problem it addresses. The only complaint is minor and presented in a constructive manner. The politeness score is also high as the language used is respectful and constructive, with phrases like 'well written paper' and 'it would have been nice' indicating a polite tone.",80,90
"The authors propose a new weight re-parameterization technique called Equi-normalization (ENorm) inspired by the Sinkhorn-Knopp algorithm. The authors show that the proposed method preserve functionally equivalent property in respect of the output of the functions (Linear, Conv, and Max-Pool) and show also that ENorm converges to the global optimum through the optimization. The experimental results show that ENorm performs better than baseline methods on CIFAR-10 and ImageNet datasets.

pros)
(+) The authors provide a theoretical ground.
(+) The theoretical analysis of the convergence of the proposed algorithm is well provided.
(+) The computational overhead reduced by the proposed method compared with BN and GN looks good.

cons)
(-) There is no comparison with other weight reparameterization methods such as Weight Normalization, Normalization propagation, Instance Normalization, or Layer Normalization. 
(-) The evidence why functionally equivalence is connected to the performance or generalization ability is not clarified.  
(-) The experimental results cannot consistently show the effectiveness of the proposed method in test accuracy. In Table 4, the proposed method outperforms BN, but In Table 2 and 3, BN is mostly better than the proposed method.
(-)  The batch size shown in Table 2 and 3 may be intended to show the batch-independent property of the proposed method, but BN is also doing well in those tables. Therefore, Table 2 and 3 are not adequate to show the batch-independent property.
(-) The proposed method should evaluate with deeper networks (e.g., ResNet-50, ResNet101, or DenseNet-169) to support the superiority over BN and GN.  
(-) Adjusting c does not seem to be promising. In Table 2 and 3, ENorm-1 is better than ENorm-1.2, and also in Table 4, only the result of ENorm-1 is provided. The authors should do a parameter study with c to make all the experiments more convincing.

Comments) 
- The experimental settings are not consistent. The authors should provide the reason why they set those settings or should include some studies about the parameters (for example about the paramter c). 
- Section 3.7 is not clear to me.  How's the performance going on when adjusting c < 1?
- It is better for the authors to provide the Sinkhorn-Knopp algorithm (SK algorithm), which gave them an inspiration for this work, for better readability. 
- Why eq.(4) is necessary? For iterative optimization? If so, the authors should incorporate a detailed explanation about this in the corresponding section.
- The authors should provide a detailed description of the parameter c. It is not clear why c is necessary, and please make sure the overall derivation does not need to be modified due to the emergence of c.
- It seems that the authors could compact the paper by highlighting key ideas. 
- Typo: Annex A (on p.5).

The paper is written well and provides a sound theoretical analysis to show the main idea, but unfortunately, the experimental results do not seem to support the effectiveness of the proposed method.","The sentiment of the review is mixed. The reviewer acknowledges the strengths of the paper, such as providing a theoretical ground and reducing computational overhead, but also points out several significant weaknesses, including lack of comparison with other methods, unclear connection between functional equivalence and performance, inconsistent experimental results, and insufficient evaluation with deeper networks. Therefore, the sentiment score is around -20. The politeness of the language is quite high. The reviewer uses polite language, provides constructive feedback, and suggests improvements without being rude or dismissive. Thus, the politeness score is 80.",-20,80
"This is an interesting paper that studies the latent variable modeling from an information theoretic perspective. Specifically, the authors argue that the rate-distortion theory for lossy compression provides a natural toolkit for studying latent variable models, and they propose a lower bound (also a gap function) that could be used to assess the goodness of data fitting given a pair of prior distribution over latent factor and a likelihood function. Overall the paper is very well-written, clear to follow, and the authors did a great job in not overclaiming their results. 

Several questions follow: 
1.  In Eq. (3), why the R.H.S. is an upper bound of the L.H.S.? Under the assumption of (1) should this be equal? 
2.  In section 2, ""must use at use"" -> ""must use at least"". 
3.  Since the mutual information is convex in the conditional distribution Q(Z|X), when considering the Lagrangian, since \alpha is constrained to be positive, should the sign before \alpha be positive instead of negative? 
4.  In section 3.3, ""An very common"" -> ""A very common"". 

To me the most interesting result in this paper is in Thm. 1, Eq. (9), where the authors show that the optimization over the prior in latent variable modeling is exactly equivalent to the optimization of the channel in rate-distortion theory. Following this line the authors propose a gap function that could be used to assess the goodness of a model. One drawback of the current framework is that it only links the optimization of the prior, rather than the likelihood function, to rate-distortion theory, while in practice it is usually the other way around. Although the authors argue in section 3.3 that similar conclusion could be achieved for a family of likelihood functions, the analysis is only possible under the very restrictive (in my personal view) assumption that relies on the existence of a smooth and invertible mapping. This assumption usually does not hold in practice, e.g., the ReLU network, and as a result the analysis here is only of theoretical interest. 

The experimental validation basically shows the usefulness of the proposed gap function in assessing the goodness of model fitting in latent variable models. It would be great if there are more direct use of the proposed lower bound, but I appreciate the novelty in this paper on bridging the two subfields. 
","The sentiment of the review is generally positive, as indicated by phrases like 'interesting paper,' 'very well-written,' and 'great job.' The reviewer appreciates the novelty and clarity of the paper, although they do point out some limitations and areas for improvement. Therefore, the sentiment score is 80. The politeness of the language is also high, as the reviewer uses polite and constructive language throughout, such as 'it would be great if' and 'I appreciate the novelty.' Thus, the politeness score is 90.",80,90
"The paper presents a novel model for neural speed reading. In this new model, the authors combined several existing ideas in a nice way, namely, the new reader has the ability to skip a word or to jump a sequence of words at once. The reward of the reader is mixed of the final prediction correctness and the amount of text been skipped. The problem is formulated as a reinforcement learning problem. The results compared with the existing techniques on several benchmark datasets show consistently good improvements.

In my view, one important (also a little surprising) finding of the paper is that the reader can make jump choices successfully with the help of punctuations. And, blindly jumping a sequence of words without even lightly read them can still make very good predictions.

The basic idea of the paper, the concepts of skip and jump, and the reinforcement learning formulation are not completely new, but the paper combined them in an effective way. The results show good improvements majorly in FLOPS.

The way of defining state, rewards and value function are not very clear to me. Two value estimates are defined separately for the skip agent and the jump agent. Why not define a common value function for a shared state? Two values will double count the rewards from reading. Also, the state of the jump agent may not capture all available information. For example, how many words until the end of the sentence if you make a jump. Will this make the problem not a MDP? 

Overall, this is a good paper.

I read the authors' response. The paper should in its final version add the precise explanation of how the two states interact and how a joint state definition differs from the current one.","The sentiment of the review is generally positive, as indicated by phrases like 'novel model,' 'nice way,' 'consistently good improvements,' and 'Overall, this is a good paper.' However, there are some critical points raised, which slightly temper the overall positivity. Therefore, the sentiment score is 70. The politeness of the language is high, as the reviewer uses polite and constructive language throughout, such as 'In my view,' 'One important finding,' and 'The paper should in its final version add.' There are no rude or harsh comments, so the politeness score is 90.",70,90
"Summary
The paper focuses on pruning neural networks. They propose to identify the nodes to be pruned even before training the whole network (conventionally, it is done as a separate step after the nn was trained and involves a number of iterations of retraining pruned nn). This initial step that identifies the connections to be pruned works off a mini-batch of data.

Authors introduce  a criterion to be used for identifying important parts of the network (connection sensitivity), that does not depend on the magnitude of the weights for neurons: they start by introducing a set of binary weights (one per a weight from a neuron) that indicate whether the connection is on or off and can be removed. Reformulating the optimization problem and relaxing the constraints on the binary weights, they approximate the sensitivity of the loss with respect to these indicator variables via the gradient. Then the normalized magnitude of these gradients is used to chose the connections to keep (keeping top k connections)

Clarity:
Well written, easy to follow

Detailed comments
Overall, very interesting. Seemingly very simple idea that seem to work well. 
Table 2 does look impressive and it seems that it also reduces the overfiting, and the experiment with random labels on mnist seem to demonstrate that the method indeed preserves only connections relevant to the real labels, simplifying the architecture to a point when it cant just memorize the data

Several questions/critiques:
- When you relax the binary constraints, it becomes an approximation to an optimization problem, any indication of how far you are off solving it this way? 
- For the initialization method of the weights, you seem to state that VS-H is the one to use. I wonder if it actually task dependent and architecture dependent. If yes, then the propose method still has a hyperparameter - how to initialize the weights initially
- How does it compare with just randomly dropping the connections or dropping them based on the magnitude of the initial weights.  It seems that the meat comes from the fact that you are able to use the label and good initial values, i wonder if just doing a couple of iterations of forward-backprop and then dropping the weights based on their magnitude can give you comparable results 
- How does it compare to a distillation - it does not involve many cycles of retraining and can speed up inference time too
-Can it replace the architecture search - initialize a large architecture, use the method to prune the connections and here you go. Did you try that instead of using already pre-tuned architectures like AlexNet.

","The review starts with a summary of the paper's contributions and methods, followed by positive remarks about the clarity and interest of the work. The reviewer describes the idea as 'very interesting' and 'seemingly very simple,' which indicates a positive sentiment. The detailed comments section includes constructive questions and critiques, but they are framed in a polite and inquisitive manner, aiming to improve the work rather than criticize it harshly. The language used is respectful and professional, with no negative or rude remarks.",80,90
"In this papers, the authors introduce a new technique to output uncertainty estimates from any family of neural nets. The key insight in this paper is that when considering existing SGD methods the following behavior occurs: if we think of ""easy"" and ""hard"" to classify datapoints, a NN trained with SGD will output good uncertainty estimates early on in training, but once the network focusses on tuning the parameters for the hard cases, the uncertainty estimates for the easy datapoints deteriorates. The algorithms proposed by the authors takes an existing uncertainty method (or confidence score function) and uses intermediate snapshots of SGD training to improve the final uncertainty estimates. Note that the focus in this work is on ranking uncertainties (and the authors suggest to leave calibrating uncertainties to existing methods).

The paper generally is well written (e.g. section 5) although I found section 3 to be a bit hard to follow. I'm not very familiar with the area itself but I was surprised to see in Section 7 that the results are not compared to full Bayesian methods (possibly on a dataset that lends itself well to that).

Notes:
- Section 3, ""A selective classifier ..."" -> I think this section could use some additional untuition to make the explanation more understandable.
- Section 3, ""defined to be the selective risk as a function of coverage."" -> do you mean as a sequence of functions g?
- ","The sentiment of the review is generally positive, as the reviewer acknowledges the novelty and potential of the proposed technique. However, there are some criticisms and suggestions for improvement, particularly regarding the clarity of Section 3 and the lack of comparison with full Bayesian methods. The sentiment score is therefore 60. The politeness of the language is quite high, as the reviewer uses polite language and constructive feedback, without any rude or harsh comments. The politeness score is 80.",60,80
"This paper analyzes that the Integral Probability Metric (IPM) can be a good approximation of Wasserstein distance under some mild assumptions. They first showed two theorems based on simple cases (Gaussian Distribution and Exponential Families). Then, they proved that, for an invertible generator, a special designed neural network can approximate Wasserstein distance with IPM. The main contribution is that, for a stable generator (i.e., invertible generator), a discriminator can reversely “re-visit” inner status of the generator, then use this information to make a decision. 

In the appendix, several numerical examples are presented to support their theoretical bound. 

Q: Assumption 1, \sigma(t) is twice differentiable. However, Leaky ReLU is not twice differentiable at t=0. Do I misunderstand some part?

Q: The invertible generator assumption is not held in practice. Is that possible to extend the theorem to this case, even with a shallow network (e.g. 2 layers)?

Q: The numerical examples are all based on synthetic data. Did you have any results based on the real dataset?
","The sentiment of the review appears to be neutral to slightly positive. The reviewer acknowledges the main contributions of the paper and provides constructive questions rather than outright criticism. The politeness of the language is high, as the reviewer uses polite and inquisitive language to ask questions and seek clarifications without being confrontational or dismissive.",20,80
"This paper studies the weak supervision setting of learning a general binary classifier from two unlabeled (U) datasets with known class balances. The authors establish that this is possible by constructing an unbiased estimator, analyze its convergence theoretically, and then run experiments using modern image classification models.

Pros:
- This work demonstrates, theoretically and empirically, a simple way to train generic models using only the known class balances of several sets of unlabeled data (having the same conditional distributions p(x|y))---a very interesting configuration of weak supervision, an increasingly popular and important area

- The treatment is thorough, proceeding from establishing the minimum number of U datasets, constructing the estimator, analyzing convergence, and implementing thorough experiments

Cons:
- This is a crowded area (as covered in their related work section). As they cite, (Quadrianto et al., 2009) proposed this setting and considered linear models for k-wise classification.  Moreover, the two U datasets with known class balances can equivalently be viewed as two weak / noisy label sources with known accuracies.  Thus this work connects to many areas- both in noisy learning, as they cite heavily, but also in methods (in e.g. crowdsourcing and multi-source weak supervision) where several sources label unlabeled datasets with unknown accuracies (which are often estimated in an unsupervised fashion).

- The overall clarity of the paper's writing could be improved. For example, the introduction and related work sections take up a large portion of the paper, but are very dense and heavy with jargon that is not internally defined upfront; for example ""risk rewrite"" is introduced in paragraph 2 with no internal definition and then used subsequently throughout the paper (this defn would be simple enough to give: in the context of this paper, ""risk rewrite"" means a linear combination of the class-conditional losses; or more generally, the expected loss w.r.t. distribution over classes...).  Also intuition could be briefly given about the theorem proof strategies.

- The difference between the two class distributions over the U datasets seems like an important quantity (akin, in e.g. weak supervision / crowd source modeling papers, to quantity of how bounded away from random noise the labelers are). This is treated empirically, but would be stronger to have this show up in the theory somewhere.

- Other prior work here has handled k classes with k U sets; could have extended to cover this setting too, since seems natural

Overall take: This learning from label proportions setting has been covered before, but this paper presents it in an overall clean and general way, testing it empirically on modern models and datasets, which is an interesting contribution.

Other minor points:
- The argument for / distinction between using eqns. (3) and (4) seems a bit ad hoc / informal (""we argue that..."").  This is an important point...
- Theorem 1 proof seems fine, but some intuition in the main body would be nice.
- What does ""classification calibrated"" mean?
- Saying that three U sets are needed, where this includes the test set, seems a bit non-standard?  Also I'm confused- isn't a labeled test set used?  So what is this third U set for?
- The labels l_+ and l_- in Defn. 3 seem to imply that the two U sets are positive vs. negative; but this is not the case, correct…?
- Stating both Lemma 5 and Thm 6 seems unnecessary
- In Fig. 2, seems like could have trained for longer and perhaps some of the losses would have continued decreasing?  In particular, small PN?  Also, a table of the final test set accuracies would have been very helpful.
- More detail on experimental protocol would be helpful: what kind of hyperparameter tuning was done? repeated runs averaging?  It seems odd, for example in Fig. 3, that the green lines are so different in (a) vs. (c), and not in the way that one would expect given the decrease in theta
","The sentiment of the review is generally positive, as indicated by the praise for the theoretical and empirical contributions of the paper. However, there are several critical points raised, particularly regarding the crowded nature of the research area and the clarity of the writing. Therefore, the sentiment score is not extremely high but still positive. The politeness of the language is quite high, as the reviewer uses polite and constructive language throughout, even when pointing out areas for improvement.",60,90
"This paper proposes an auxiliary variable MCMC scheme involving variational inference for efficient MCMC. Given a target distribution p(x), the authors introduce an auxiliary variable a, and learn conditional distributions p(a|x) and q(a|x) by minimizing the KL divergence between p(x)p(a|x) and q(a)q(x|a), with q(a) something simple (the authors use Gaussian). A MH proposal step involves simulating x givea the current MCMC sample x (from p(a|x), taking a step in A-space, and then returning back to the X space (using q(x|a)).  The authors show how to calculate the acceptance probability. 

I think the idea is nice and useful (I'm surprised people haven't thought of this before), though I think the paper presents this in a less clear way (as an extension of ideas from Agakov and Barber's ""Auxiliary variational method""). While this is correct and perhaps more general, in my mind it slightly obscures the main idea, as well as the strong ties with variational autoencoders: express a complex distribution as a (learnt) transformation of a simple distribution (this is the actual approach taken in the experiments). 

The motivation of the approach is that the nonlinear encoding network can transform the complex p(x) into a simpler q(a). 
For this reason, I think an important baseline is the independent MH sampler from equation 8 (I think this essentially uses a trained VAE generative model as a proposal distribution). The authors talk about how producing independent proposals can be sub-optimal, yet it seems to me that if the encoder and decoder neural networks are powerful enough, this should do a good job. I think excluding this baseline hurts the paper a bit.

The proof of correctness while correct is a bit unclear, can perhaps be simplified if you view the MCMC algorithm as operating on an augmented space (x,a,x') with stationary distribution p(x)q(a|x)q(x'|a) (writing writing q for \tilde(q)). This clearly has the right distribution over x. Each MCMC iteration starts with x and proceeds as follow:
  1) Given x, sample a and x' from q(a|x) and q(x'|a)
  2) Make a deterministic proposal on the augmented space to swap (x,x'). The acceptance probability is now equation 2.
  3) Discard a,x'.

In figure 4, the authors use HMC as an ""improved MCMC algorithm"", yet this is not an algorithm that deals with multimodality well. More useful would be to include some tempering algorithm like serial or parallel tempering.

While I like the idea, I unfortunately don't think the experiments are very convincing (and the authors barely discuss their results). Other than mixture of Gaussians, HMC (which involves no training) appears to be superior. With some tempering, I expect it to outperform the proposed method for the MoG case

Table 2 left: since HMC involves no training, does this mean that, taking training time into account, HMC is 5-6 orders of magnitude more efficient. L?ke I mentioned earlier, these results need more discussion. 

It would also help to provide absolute training and run times, so the reader can better understand whether the proposed method of ANICE is better.

Figure 3: why don't the authors also plot the histogram of values in the auxiliary space, p(a). It would be interesting to see how Gaussian this is (this is what variational inference is trying to achieve). Also, does Figure 3(a) mean that conditioned on x, p(a|x) is basically a delta function? This would suggest that the encoder is basically learning a deterministic transformation to a simpler low-dimensional space? There is some work in this direction in the statistics literature, e.g. 
""Variable transformation to obtain geometric ergodicity in the random-walk Metropolis algorithm""

The authors some refers to the distribution of a|x as q(a|x) sometimes (in section 2.1) and sometimes as p(a|x) which is a bit confusing.

Figure 2: the labels are wrong.","The sentiment of the review is mixed but leans towards positive. The reviewer appreciates the novelty and potential usefulness of the idea, giving it a positive sentiment score of 30. However, the reviewer also points out several areas where the paper could be improved, such as clarity, inclusion of important baselines, and more convincing experiments. The politeness of the language is quite high, as the reviewer uses phrases like 'I think,' 'it would help,' and 'I unfortunately don't think,' which soften the critique and make it constructive. Therefore, the politeness score is 80.",30,80
"The paper studies the problem of generating synthetic datasets (while ensuring differential privacy) via training a GAN. One natural approach is the teacher-student framework considered in the PATE framework.  In the original PATE framework, while the teachers are ensured to preserve differential privacy, the student model (typically a GAN) requires the presence of publicly data samples. The main contribution of this paper is to get around the requirement of public data via using uniformly random samples in [0,1]^d.

Differentially private synthetic data generation is clearly an important and a long-standing open problem. Recently, there has been some work on exploiting differentially private variants of GANs to generate synthetic data. However, the scale of these results is far from satisfactory. The current paper claims to bypass this issue by using the PATE-GAN approach.

I am not an expert on deep learning. The idea of bypassing the use of public data by taking uniformly random samples seems interesting. In my view, these random vectors are used in the GAN as some sort of a basis. It is interesting to see if this result extends to high-dimensional settings (i.e., where d  is very large).","The sentiment of the review is moderately positive. The reviewer acknowledges the importance of the problem and finds the approach interesting, although they express some reservations about their expertise in deep learning and the scalability of the results. Therefore, the sentiment score is 50. The language used in the review is polite and respectful, with no negative or harsh comments. The reviewer maintains a professional tone throughout, so the politeness score is 100.",50,100
"This paper considers the compression of the model parameters in deep neural networks. The authors propose minimal random code learning (MIRACLE), which uses a random sample of weights and the variational framework interpreted by the bits-back argument. The authors introduce two theorems characterizing the properties of MIRACLE, and demonstrate its compression performance through the experiments.

The proposed approach is interesting and the performance on the benchmarks is good enough to demonstrate its effectiveness. However, since the two main theorems are based on the existing results by Harsha et al. (2010) and Chatterjee & Diaconis (2018), the main technical contribution of this paper is the sampling scheme in Algorithm 1.

Although the authors compare the performance trade-offs of MIRACLE with that of the baseline methods quoted from source materials, isn't it possible or desirable to include other competitors or other results for the baseline methods? Are there any other methods, in particular, achieving low error rate (with high compression size)? Little is discussed on why the baseline results are only a few. 

minor comment: 
- Eq.(4) lacks p(D) in front of dD.    

Pros:
- Interesting approach based-on the bits back argument
- Good performance trade off demonstrated through experiments
Cons:
- Only a few baseline results, in particular, at high compression size
","The sentiment of the review is generally positive, as indicated by phrases like 'interesting approach' and 'good performance trade off demonstrated through experiments.' However, there are some criticisms, particularly regarding the limited baseline results, which slightly temper the overall positivity. Therefore, the sentiment score is 50. The politeness of the language is high, as the reviewer uses polite language and constructive criticism, such as 'isn't it possible or desirable' and 'little is discussed,' without being harsh or dismissive. Thus, the politeness score is 80.",50,80
"The paper design a low variance gradient for distributions associated with continuous or discrete random variables. The gradient is designed in the way to approximate the  property of reparameterization gradient.  The paper is comprehensive and includes mathematical details. 

I have following comments/questions

1. What is the \kappa in “variable-nabla” stands for? What is the gradient w.r.t. \kappa?

2. In Eq(8), does the outer expectation w.r.t . y_{-v} be approximated by one sample? If so, it is using the local expectation method. How does that differs from Titsias & Lazaro-Gredilla(2015) both mathematically and experimentally? 

3. Assume y_v is M-way categorical distribution, Eq(8) evaluates f by 2*V*M times which can be computationally expensive. What is the computation complexity of GO? How to explain the fast speed shown in the experiments?

4. A most simple way to reduce the variance of REINFORCE gradient is to take multiple Monte-Carlo samples at the cost of more computation with multiple function f evaluations. Assume GO gradient needs to evaluate f N times, how does the performance compared with the REINFORCE gradient with N Monte-Carlo samples? 

5. In the discrete VAE experiment, upon brief checking the results in Grathwohl(2017), it shows validation ELBO for MNIST as (114.32,111.12), OMNIGLOT as (122.11,128.20) from which two cases are better than GO. Does the hyper parameter setting favor the GO gradient in the reported experiments? Error bar may also be needed for comparison. What about the performance of GO gradient in the 2 stochastic layer setting in Grathwohl(2017)?

6. The paper claims GO has less parameters than REBAR/RELAX. But in Figure 9, GO has more severe overfitting. How to explain this contradicts between the model complexity and overfitting?

","The sentiment of the review is generally positive, as the reviewer acknowledges that the paper is comprehensive and includes mathematical details. However, the reviewer also raises several questions and concerns, indicating that there are areas that need clarification or improvement. Therefore, the sentiment score is moderately positive. The politeness of the language is high, as the reviewer uses polite phrases such as 'I have following comments/questions' and does not use any harsh or rude language. The questions are posed in a constructive manner, aiming to improve the paper.",60,80
"Summary: This paper observes that a major flaw in common image-classification networks is their lack of robustness to common corruptions and perturbations. The authors develop and publish two variants of the ImageNet validation dataset, one for corruptions and one for perturbations. They then propose metrics for evaluating several common networks on their new datasets and find that robustness has not improved much from AlexNet to ResNet. They do, however, find several ways to improve performance including using larger networks, using ResNeXt, and using adversarial logit pairing.

Quality: The datasets and metrics are very thoroughly treated, and are the key contribution of the paper. Some questions: What happens if you combine ResNeXt with ALP or histogram equalization? Or any other combinations? Is ALP equally beneficial across all networks? Are there other useful adversarial defenses?

Clarity: The novel validation sets and reasoning for them are well-explained, as are the evaluation metrics. Some explanation of adversarial logit pairing would be welcome, and some intuition (or speculation) as to why it is so effective at improving robustness.

Originality: Although adversarial robustness is a relatively popular subject, I am not aware of any other work presenting datasets of corrupted/perturbed images.

Significance: The paper highlights a significant weakness in many image-classification networks, provides a benchmark, and identifies ways to improve robustness. It would be improved by more thorough testing, but that is less important than the dataset, metrics and basic benchmarking provided.

Question: Why do authors do not recommend training on the new datasets? ","The sentiment of the review is generally positive, as it acknowledges the thorough treatment of datasets and metrics, and highlights the significance of the paper's contributions. However, it also points out areas for improvement and poses several questions, which slightly tempers the overall positivity. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, making suggestions and asking questions in a respectful manner. Thus, the politeness score is 90.",60,90
"The authors present a deep reinforcement learning approach that uses a “self-attention”/“transformer”-style model to incorporate a strong relational inductive bias. Experiments are performed on a synthetic “BoxWorld” environment, which is specifically designed (in a compelling way) to emphasize the need for relational reasoning. The experiments on the BoxWorld environment clearly demonstrate the improvement gained by incorporating a relational inductive bias, including compelling results on generalization. Further experimental results are provided on the StarCraft minigames domain. While the results on StarCraft are more equivocal regarding the importance of the relational module—the authors do set a new state of the art and the results are suggestive of the potential utility of relational inductive biases in more general RL settings.

Overall, this is a well-written and compelling paper. The model is well-described, the BoxWorld results are compelling, and the performance on the StarCraft domain is also quite strong. The paper clearly demonstrates the utility of relational inductive biases in reinforcement learning.

In terms of areas for potential improvement:

1) With regards to framing, a naive reader would probably get the impression that this is the first-ever work to consider a relational inductive bias in deep RL, which is not the case, as the NerveNet paper (Wang et al., 2018) also considers using a graph neural network for deep RL. There are clear differences between this work and NerveNet—most prominently, NerveNet only uses a relational inductive bias for the policy network by assuming that a graph-structured representation is known a priori for the agent. Nonetheless, NerveNet does also incorporate a relational inductive bias for deep RL and shows how this can lead to better generalization. Thus, this paper would be improved by properly positioning itself w.r.t. NerveNet and highlighting how it is different. 

2) As with other work using non-local neural networks (or fully-connected GNNs), there is the potential issue of scalability due to the need to consider all input pairs. A discussion of this issue would be very useful, as it is not clear how this approach could scale to domains with very large input spaces. 

3) Some details on the StarCraft experiments could be made more rigorous and quantitative. In particular, the following instances could benefit from more experimental details and/or clarifications: 

Figure 6: The performance of the control model and relational model seem very close. Any quantitative insight on this performance gap would improve the paper. For instance, is the gap between these two models significantly larger than the average gap between runs over two different random seeds? It would greatly strengthen the paper to clarify that quantitive aspect. 

Page 8: ”We observed that—at least for medium sized networks—some interesting generalization capabilities emerge, with the best seeds of the relational agent achieving better generalization scores in the test scenario” — While there is additional info in the appendix, without quantitative framing this statement is hard to appreciate. I would suggest more quantitive detail and rigorous statistical tests, e.g.,  something like “When examining the best 10 out of ??? seeds, the relational model achieved an average performance increase of ???% compared to the control model (p=???, Wilcoxon signed-rank test). However, when examining all seeds ???? was the case.” 

Page 8: “while the former adopted a ""land sweep strategy"", controlling many units as a group to cover the space, the latter managed to independently control several units simultaneously, suggesting a finer grained understanding of the game dynamics.” This is a great insight, and the paper would be greatly strengthened by some quantitive evidence to back it up (if possible). For instance, you could compute the average percentage of agents that are doing the same action at any point in time or within some distance from each other, etc. Adding these kinds of quantitative statistics to back up these qualitative insights would both strengthen the argument, while also making it more explicit how you are coming to these qualitative judgements. 

Figure 8 caption: “Colored bars indicate mean score of the ten best seeds” — how bad is the drop to the n-10 non-best seeds? And how many seeds where used in total?

Page 13: “following Table 4 hyperparameter settings and 3 seeds” — if three seeds are used in these experiments, how are 10+?? seeds used for the generalization experiments? The main text implies that the same models for the “Collect Mineral Shards” were re-used, but it appears that many more models with different seeds were trained specifically for the generalization experiment. This should be clarified. Alternatively, it is possible that “seeds” refers to both random seeds and hyperparameter combinations, and it would improve the paper to clarify this. It is possible that I missed something here, but I think it highlights the need for further clarification. ","The sentiment of the review is quite positive, as the reviewer describes the paper as 'well-written and compelling' and highlights the strong results and clear demonstration of the utility of relational inductive biases. This suggests a sentiment score of 80. The politeness of the language is also high, as the reviewer uses polite and constructive language throughout, providing specific recommendations for improvement without being harsh or dismissive. This suggests a politeness score of 90.",80,90
"The paper proposes two approximations to the Shapley value used for generating feature scores for interpretability. Both exploit a graph structure over the features by considering only subsets of neighborhoods of features (rather than all subsets). The authors give some approximation guarantees under certain Markovian assumptions on the graph. The paper concludes with experiments on text and images.

The paper is generally well written, albeit somewhat lengthy and at times repetitive (I would also swap 2.1 and 2.2 for better early motivation). The problem is important, and exploiting graphical structure is only natural. The authors might benefit from relating to other fields where similar problems are solved (e.g., inference in graphical models). The approximation guarantees are nice, but the assumptions may be too strict. The experimental evaluation seems valid but could be easily strengthened (see comments).

Comments:

1. The coefficients in Eq. (6) could be better explained.

2. The theorems seem sound, but the Markovian assumption is rather strict, as it requires that a feature i has an S that ""separates"" over *all* x (in expectation). This goes against the original motivation that different examples are likely to have different explanations. When would this hold in practice?

3. While considering chains for text is valid, the authors should consider exploring other graph structures (e.g., parsing trees).

4. For Eqs. (8) and (9), I could not find the definition of Y. Is this also a random variable representing examples?

5. The authors postulate that sampling-based methods are susceptible to high variance. Showing this empirically would have strengthened their claim.

6. Can the authors empirically quantify Eqs. (8) and (9)? This might shed light as to how realistic the assumptions are.

7. In the experiments, it would have been nice to see how performance and runtime vary with increased neighborhood sizes. This would have quantified the importance of neighborhood size and robustness to hyper-parameters.

8. For the image experiments, since C-Shapley considers connected subsets, it is perhaps not surprising that Fig. 4 shows clusters for this method (and not others). Why did the authors not use superpixels as features? This would have also let them compare to LIME and L-Shapley.

","The sentiment of the review is generally positive, as the reviewer acknowledges that the paper is well-written, addresses an important problem, and provides valid experimental evaluations. However, there are some criticisms regarding the length, repetitiveness, and strictness of assumptions, which slightly temper the overall positive sentiment. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, offering suggestions and asking questions in a respectful manner. Therefore, the politeness score is 90.",60,90
"This paper proposed Whitening and Coloring (WC) transform to replace batch normalization (BN) in generators for GAN. WC generalize BN by normalizing features with decorrelating (whitening) matrix, and then denormalizing (coloring) features by learnable weights. The main advantage of WC is that it exploits the full correlation matrix of features, while BN only considers the diagonal. WC is differentiable and is only 1.32x slower than BN. The authors also apply conditional WC, which learn the parameters of coloring conditioned on labels, to conditional image generation.  Experimental results show WC achieves better inception score and FI distance comparing to BN on CIFAR-10, CIFAR-100, STL-10 and Tiny Imagenet. Furthermore, the conditional image generation results by WC are better than all previous methods.

I have some detailed comments below.

+ The paper is well written, and I generally enjoyed reading the paper.
+ The experimental results look sufficient, and I appreciate the ablation study sections. 
+ The score on supervised CIFAR-10 is better than previous methods. 

- The main text is longer than expectation. I would suggest shorten section 3.1 Cholesky decomposition, section 4 conditional color transformation and the text in section 5 experiments.
- The proposed WC transform is general. It is a bit unclear why it is particularly effective for generator in GAN. Exploiting the full correlation matrix sounds reasonable, but it may also introduce unstability. It would help if the authors have an intuitive way to show that whitening is better than normalization.
- It is unclear why conditional WC can be used for generation conditioned on class labels. In Dumoulin 2016, conditional instance normalization is used for generating images conditioned on styles. As image styles are described by Gram matrix (correlation) of features, changing first order and second order statistics of features is reasonable for image generation conditioned on styles. I cannot understand why conditional WC can be used for generation conditioned on class labels. I would like the authors to carefully explain the motivation, and also provide visual results like using the same random noise as input, but only changing the class conditions. 
- It is unclear to me why the proposed whitening based on Cholesky decomposition is better than ZCA-based in Huang 2018. Specifically, could the authors explain why WC is better than W_{aca}C in Table 3? 
- The authors claim progressive GAN used a larger generator to achieve a better performance than WC. The WC layer is generally larger than BN layer and has more learnable parameters. Could the authors compare  the number of parameter of generator in BN-ResNet, WC-ResNet, and progressive GAN?
- In Table 3, std-C is better than WC-diag, which indicates coloring is more important. In Table 6, cWC-diag is better than c-std-C, which indicates whitening is more important. Why? 
- What is the batch size used for training? For conditional WC, do the samples in each minibatch have same label?
- Having ImageNet results will be a big support for the paper.


===========  comments after reading rebuttal ===========

I appreciate the authors' feedback. I raised my score for Fig 7 showing the conditional images, and for experiments on ImageNet. 

I think WC is a reasonable extension to BN, and I generally like the extensive experiments. However, the paper is still borderline to me for the following concerns.

- I strongly encourage the authors to shorten the paper to the recommended 8-page. 

- The motivation of WC for GAN is still unclear. WC is general extension of BN, and a simplified version has been shown to be effective for discrimination in Huang 2018. I understand the empirically good performance for GAN. But I am not convinced why WC is particularly effective for GAN, comparing to discrimination. The smoothness explanation of BN applies to both GAN and discrimination. I actually think it may be nontrivial to extend the smoothness argument from BN to WC.

- The motivation of cWC is still unclear. I did not find the details of cBN for class-label conditions, and how they motivated it in (Gulrajani et al. (2017) and (Miyato et al. 2018). Even if it has been used before, I would encourage the authors to restate the motivation in the paper. Saying it has been used before is an unsatisfactory answer for an unintuitive setting.

- Another less important comment is that it is still hard to say how much benefits we get from the more learnable parameters in WC than BN. It is probably not so important because it can be a good trade-off for state-of-the-art results. In table 3 for unconditioned generation, it looks like the benefits come a lot from the larger parameter space. For conditioned generation in table 6, I am not sure if whitening is conditioned or not, which makes it less reliable to me. If whitening is conditioned, then the samples in each minibatches may not be enough to get a stable whitening. If whitening is unconditioned, then there seems to be a mismatch between whitening and coloring. 

====== second round after rebuttal =============
I raise the score again for the commitment of shortening the paper and the detailed response from the authors. That being said, I am not fully convinced about motivations for WC and cWC. 

- GAN training is more difficult and unstable, but that does not explain why WC is particularly effective for GAN training. 

- I have never seen papers saying cBN/cWC is better than other conditional generator conditioned on class labels. I think the capacity argument is interesting, but I am not sure if it applies to convolutional net (where the mean and variance of a channel is used), or how well it can explain the performance because neural nets are overparameterized in general. I would encourage authors to include these discussions in the paper. 
","The sentiment of the review is generally positive, as indicated by phrases like 'I generally enjoyed reading the paper' and 'I appreciate the ablation study sections.' However, the reviewer also has several concerns and suggestions for improvement, which slightly temper the overall positivity. Therefore, the sentiment score is 40. The politeness of the language is very high, with the reviewer using polite phrases such as 'I appreciate,' 'I would suggest,' and 'I strongly encourage.' The reviewer provides constructive feedback without being harsh or rude, so the politeness score is 90.",40,90
"After reading the authors' response, I'm revising my score upwards from 5 to 6.

The authors propose a defense against adversarial examples, that is inspired by ""non local means filtering"". The underlying assumption seems to be that, at feature level, adversarial examples manifest as IID noise in feature maps, which can be ""filtered away"" by using features from other images. While this assumption seems plausible,  no analysis has been done to verify it in a systematic way. Some examples of verifying this are:

1. How does varying the number of nearest neighbors change the network behavior?
2. At test time, a fixed number of images are used for denoising - how does the choice of these images change accuracy or adversarial robustness?
3. Does just simple filtering of the feature map, say, by local averaging, perform equally well? 
4. When do things start to break down? I imagine randomly replacing feature map values (i.e. with very poor nearest neighbors) will cause robustness and accuracy to go down - was this tested?

Based on the paper of Athalye et. al., really the only method worth comparing to for adversarial defense, is adversarial training. It is hard to judge absolute adversarial robustness performance without a baseline of adversarial training.","The sentiment of the review is moderately positive, as indicated by the upward revision of the score from 5 to 6 and the acknowledgment of the plausibility of the authors' assumption. However, the review also points out significant areas that need further analysis and verification. Therefore, the sentiment score is 20. The politeness of the language is quite high, as the reviewer provides constructive feedback and specific recommendations without using harsh or dismissive language. The reviewer also acknowledges the authors' efforts and provides a rationale for the suggestions. Therefore, the politeness score is 80.",20,80
"This paper proposes a dynamical neural network for sparse coding where all the interactions terms are learned.  In previous approaches (Rozell et al.) some weights were tied to the others.  Here the network consists of feedforward, lateral, and feedback weights, all of which have their own learning rule.  The authors show that the learned weights converge to the desired solution for solving the sparse coding objective.  This seems like a nice piece of work, an original approach that solves a problem that was never really fully resolved in previous work, and it brings things one step closer to both neurobiological plausibility and hardware implementation.

Other comments:

What exactly is being shown in Figure 2 is still not clear to me.

 It would be nice to see some other evaluations, for example sparsity vs. MSE tradeoff (this is reflected in the objective function in part but it would be nice to see the tradeoff).  

There is recent work from Mitya Chklovskii's group on ""similarity matching"" that also addresses the problem of developing a fully local learning rule.  The authors should incorporate a discussion of this in their final paper.
","The sentiment of the review is positive, as indicated by phrases like 'nice piece of work,' 'original approach,' and 'brings things one step closer to both neurobiological plausibility and hardware implementation.' These phrases suggest that the reviewer appreciates the novelty and significance of the work. Therefore, the sentiment score is 80. The politeness of the language is also high. The reviewer uses polite language such as 'It would be nice to see' and 'The authors should incorporate,' which are constructive and respectful suggestions rather than demands. Therefore, the politeness score is 90.",80,90
"Summary
=======
This paper introduces a method for learning neural networks with quantized weights and activations. The main idea is to stochastically – rather than deterministically – quantize values, and to replace the resulting categorical distribution over quantized values with a continuous relaxation (the ""concrete distribution"" or ""Gumbel-Softax distribution""; Maddison et al., 2016; Jang et al., 2016). Good empirical performance is demonstrated for LeNet-5 applied to MNIST, VGG applied to CIFAR-10, and MobileNet and ResNet-18 applied to ImageNet.

Review
======
Relevance:
Training non-differentiable neural networks is a challenging and important problem for several applications and a frequent topic at ICLR.

Novelty:
Conceptually, the proposed approach seems like a straight-forward application/extension of existing methods, but I'm unaware of any paper which uses the concrete distribution for the express purpose of improved efficiency as in this paper. There is a thorough discussion of related work, although I was missing Williams (1992), who used stochastic rounding before Gupta et al. (2015), and Soudry et al. (2014), who introduced a Bayesian approach to deal with discrete weights and activations.

Results:
The empirical work is thorough, achieving state-of-the-art results in several classification benchmarks. It would be interesting to see how well these methods perform in other tasks (e.g., compression or even regression), even though the literature on quantization seems to focus on classification.

Clarity:
The paper is well written and clear.","The sentiment of the review is generally positive. The reviewer acknowledges the relevance and importance of the problem addressed by the paper, notes the novelty of the approach despite it being a straightforward extension of existing methods, and praises the thoroughness and clarity of the empirical work and writing. However, the reviewer does point out a couple of missing references, which slightly tempers the overall positivity. Therefore, the sentiment score is 80. The politeness of the language is very high, as the reviewer uses respectful and constructive language throughout the review, making suggestions in a considerate manner. Thus, the politeness score is 100.",80,100
"Summary

This paper derives a new policy gradient method for when continuous actions are transformed by a
normalization step, a process called angular policy gradients (APG). A generalization based on
a certain class of transformations is presented. The method is an instance of a 
Rao-Blackwellization process and hence reduces variance.


Detailed comments

I enjoyed the concept and, while relatively niche, appreciated the work done here and do believe it has clear applications. I am not convinced that the measure theoretic perspective is always
necessary to convey the insights, although I appreciate the desire for technical correctness. Still,
appealing to measure theory does reduces readership, and I encourage the authors to keep this in
mind as they revise the text.

Generally speaking it seems like a lot of technicalities for a relatively simple result:
marginalizing a distribution onto a lower-dimensional surface.

The paper positions itself generally as dealing with arbitrary transformations T, but really is 
about angular transformations (e.g. Definition 3.1). The generalization is relatively 
straightforward and was not too surprising given the APG theory. The paper would gain in clarity
if its scope was narrowed. 

It's hard for me to judge of the experimental results of section 5.3, given that there are no other 
benchmarks or provided reference paper. As a whole, I see APG as providing a minor benefit over PG.

Def 4.4: ""a notion of Fisher information"" -- maybe ""variant"" is better than ""notion"", which implies there are different kinds of Fisher information 
Def 3.1 mu is overloaded: parameter or measure?
4.4, law of total variation -- define 


Overall

This was a fun, albeit incremental paper. The method is unlikely to set new SOTA, but I appreciated
the appeal to measure theory to formalize some of the concepts.


Questions

What does E_{pi|s} refer to in Eqn 4.1?
Can you clarify what it means for the map T to be a sufficient statistic for theta? (Theorem 4.6)
Experiment 5.1: Why would we expect APG with a 2d Gaussian to perform better than a 1d Gaussian
on the angle?


Suggestions

Paragraph 2 of section 3 seems like the key to the whole paper -- I would make it more prominent.
I would include a short 'measure theory' appendix or equivalent reference for the lay reader.

I wonder if the paper's main aim is not actually to bring measure theory to the study of policy
gradients, which would be a laudable goal in and of itself. ICLR may not in this case be the right
venue (nor are the current results substantial enough to justify this) but I do encourage authors to
consider this avenue, e.g. in a journal paper.

= Revised after rebuttal =

I thank the authors for their response. I think this work deserves to be published, in particular because it presents a reasonably straightforward result that others will benefit from. However, I do encourage further work to
1) Provide stronger empirical results (these are not too convincing).
2) Beware of overstating: the argument that the framework is broadly applicable is not that useful, given that it's a lot of work to derive closed-form marginalized estimators.
","The sentiment of the review is generally positive, as the reviewer expresses appreciation for the concept and acknowledges the work done, even though they note some limitations and areas for improvement. This is reflected in statements like 'I enjoyed the concept' and 'I think this work deserves to be published.' Therefore, the sentiment score is 60. The politeness of the language is also high, as the reviewer uses polite and constructive language throughout the review, offering suggestions and encouragement rather than harsh criticism. Phrases like 'I encourage the authors' and 'I appreciated the appeal to measure theory' indicate a polite tone. Thus, the politeness score is 80.",60,80
"This paper presents a new code-to-sequence model called code2seq that leverages the syntactic structure of programming languages to encode source code snippets, which is then decoded to natural language using a sequence decoder. The key idea of the approach is to represent a program using a set of randomly sample k paths in its abstract syntax tree. For each path, the path is encoded using a recurrent network and concatenated with the embeddings of the two leaf terminal values of the path. The path encodings are then averaged to obtain the program embedding, which is then used to initialize a sequence decoder that also attends over the path embeddings. The code2vec model is evaluated over two tasks: 1) Code summarization: predicting a method’s name from its body, and 2) Code captioning: generating a natural language sentence from method’s body depicting its functionality. The code2seq model significantly outperforms the other baseline methods, and the ablation study shows the importance of various design choices.

This paper presents an elegant way to represent programs using a set of paths in the AST, which are then weighted using an attention mechanism to attend over relevant path components. The code2seq model is extensively evaluated over two domains of code summarization and code captioning, and results show significant improvements.

The novelty of the code2seq model is somewhat limited compared to the model presented in code2vec (Alon et al. 2018a) paper. In code2vec, a program is encoded as a set of paths, where each path comes from a fixed vocabulary. The code2seq model instead uses an LSTM to encode individual paths, which allows it to generalize to new paths. This is a more natural choice for embedding paths, but it doesn’t appear to be a big conceptual advance in the model architecture. The use of subtoken embeddings for encoding/decoding identifier names is different in code2seq, but it has been proposed earlier in other code embedding models.

For the code summarization evaluation, would it be possible to evaluate the code2seq model on the dataset used by the code2vec paper? On that dataset, the code2vec approach gets a precision score of 63.1, recall of 54.4, and F1 score of 58.4, [Table 3 on page 18] which are comparable to overall scores of the code2seq model.

One of the key findings of the paper is that syntactic structure of programs is important to encode. Similar observations have been made in other program embedding papers that use for example Tree-RNN [1] or graph neural networks (GNN) [Allamanis et al. 2018]. It would be quite valuable to compare the current results with the Tree-RNN or GNN models (without performing additional dataflow and control-flow post processing) to see how well the paths-based embeddings work in comparison to these models.

The value of k=200 seems a bit large for the examples presented in the paper. What happens when smaller values of k are used (e.g. k=10, 20?) What are the average number of paths in the java programs in the dataset?

1. Chris Piech, Jonathan Huang, Andy Nguyen, Mike Phulsuksombati, Mehran Sahami, Leonidas Guibas. Learning Program Embeddings to Propagate Feedback on Student Code
ICML 2015
","The sentiment of the review is generally positive, as the reviewer acknowledges the elegance of the approach and the significant improvements shown by the code2seq model. However, there are some critical points raised regarding the novelty of the model and suggestions for further evaluation, which slightly temper the overall positivity. Therefore, the sentiment score is 60. The language used in the review is polite and constructive, offering specific recommendations and comparisons without any negative or rude remarks. Thus, the politeness score is 90.",60,90
"The paper introduces an iterative method to generate deformed images for adversarial attack. The core idea is to perturb the correctly classified image by iteratively applying small deformations, which are estimated based on a first-order approximation step, until the image is misclassified. Experimental results on several benchmark datasets (MNIST, ImageNet) and commonly used deep nets (CNN, ResNet, Inception) are reported to show the power of adversarial deformations. 

The idea of gradually adding deformations based on gradient information is somewhat interesting, and novel as far as the reviewer knows about. The method is clearly presented and the results are mostly easy to access.  However, the intuition behind the proposal does not make strong sense to the reviewer: since the main focus of this work is on model attack, why not directly (iteratively or not) adding random image deformations to fool the system? Particularly, the first-order approximation strategy (as shown in Eq.4 and Eq.5) is quite confusing. On one side (see Eq.4), the deformation \tau should be small enough in scale to make an accurate approximation. On the other side (see Eq. 5), \tau is required to be sufficiently large in order to generate misclassification. Such seemingly conflicting rules for estimating the deformation makes the proposed method less rigorous in math. 
As another downside, the related adversarial training procedure is not fully addressed. The authors briefly discussed this point in the experiment section and provided a few numerical results in Table 2. These results, as acknowledged by the authors, do not well support the effectiveness of deformation adversarial attack and defense. In the meanwhile, the mentioned adversarial training framework follows straightforwardly from PGD (Madry et al. 2018), and thus the novelty of this contribution is also weak. More importantly, it is not clear at all, both in theory and algorithm, whether the advocated gradual deformation attack and defense can be unified inside a joint min-max/max-min learning formulation, as what PGD is rooted from.

Pros: 

- The way of constructing deformation adversarial is interesting and novel
- The paper is mostly clearly organized and presented.

Cons:

- The motivation of approach is questionable. 
- The related adversarial training problem remains largely unaddressed.
- Numerical study shows some promise in adversarial attack, but is not supportive to the related defense capability. ","The sentiment of the review is mixed. The reviewer acknowledges the novelty and clear presentation of the method, which contributes positively to the sentiment. However, significant concerns are raised about the intuition behind the approach, the mathematical rigor, and the effectiveness of the adversarial training, which contribute negatively. Therefore, the sentiment score is slightly negative. The language used in the review is polite and constructive, as the reviewer provides specific feedback and suggestions without using harsh or rude language.",-20,80
"
The authors provide a clean and easily understood sufficient
condition for spurious local minima to exist in networks with
a hidden layer using ReLUs or leaky ReLUs.  This condition,
that there is not linear transformation with zero loss,
is satisfied for almost all inputs with more examples than
input variables.

The construction is elegant.  The mathematical writing in the paper,
especially describing the proof of Theorem 1, is very nice -- they
expose the main ideas effectively.

I do not know of another paper using a similar proof, but I have not
studied the proofs of the most closely related papers prior to doing
this review, so I have limited ability to vouch for this paper's
technical novelty.

The authors also show that networks using many other popular
activation functions have spurious local minima for a very
simple dataset.  All of these analysis are unified using a
simple, if technical, set of conditions on activation function.

Finally, the authors prove a somewhat technical theorem about
optima in deep linear networks, which generalizes some
earlier treatments of this topic, providing an checkable
condition for global minimality.

There is extensive discussion of related work.  I am not aware of
related work not covered by the authors.

In some cases, when the authors discuss previous work, they write as
if restriction to the realizable case is an assumption, when it seems
to me to be more of a constraint.  In other words, it seems harder to
prove the existence of spurious minima in the realizable case.
They seem to acknowledge this after their statement of their Theorem 2,
which also uses a realizable dataset.

Also, a few papers, including the Venturi, et al paper cited by
the authors, have analyzed whether spurious local minima exist
in subsets of the parameter space, including those likely to
be reached during training with different sorts of initializations.
In light of this work, the authors might want to tone down claims
about how their work shows that results about linear networks do
not generalize to the non-linear case.  In particular, to make
their construction work in the case of wide networks, they
need an overwhelming majority of the hidden units to be ""dead"",
which seems as it is unlikely to arise from training with
commonly used initializations.

Overall, I think that this paper makes an interesting and
non-obvious contribution on a hot topic.","The review starts with positive remarks about the clarity and elegance of the paper's construction and mathematical writing. The reviewer acknowledges the paper's interesting and non-obvious contribution to a hot topic. However, the review also includes some critical points, such as the limited ability to vouch for the paper's technical novelty and suggestions for toning down certain claims. Overall, the sentiment is positive but with constructive criticism. The language used is polite and respectful, even when pointing out areas for improvement.",70,90
"The method proposes to use physiological signals to improve performance of reinforcement learning algorithms. By measuring heart pulse amplitude the authors build an intrinsic reward function that is less sparse that the extrinsic one. It helps to be risk averse and allows getting better performances than the vanilla RL algorithm on a car-driving task. 

I found the paper well written and the idea is quite nice. I like the idea that risk aversion is processed as a data-driven problem and not as an optimisation problem or using heuristics. I think this general idea could be pushed further in other cases (like encourage fun, surprise, happiness etc. ). 

There are some issues with this paper yet. First, modifying the reward function also modifies the optimal policy. In the specific case of car driving, it may not be bad to modify the policy so that it makes passenger less stressed but in general, it is not good. This is why most of works based on intrinsic motivation also schedule the lambda parameter to decrease with time. This is not something explored in this paper. Also, this work is well suited to the car-driving scenario because stress is closely related to risk and accident. But it may not work with other applications. I would thus suggest that the title of the paper reflects the specific case of risk aversion. ","The review starts with a positive sentiment, appreciating the well-written paper and the innovative idea of using physiological signals to improve reinforcement learning algorithms. The reviewer expresses enthusiasm for the approach and suggests its potential for broader applications. However, the review also points out some significant issues, such as the potential drawbacks of modifying the reward function and the lack of exploration of certain parameters. Despite these criticisms, the language remains constructive and polite, offering suggestions for improvement rather than dismissing the work outright.",60,80
"PROS:
- The text is very well written, with a good balance between mathematical details and intuitions.
- I really like the high-level description of the algorithms and proof techniques

CONS:
to be completely honest, I am not sure I have learnt anything new from the paper. 
1) the proof techniques are very standard
2) although there must be some small innovations, I thought that all the results had more or less been proven by Dupuis and co-authors:
a. large deviation principles
b. the larger the swapping rate, the better (which motivated Dupuis & al to consider the infinite swapping limit.)

and
c. Bakri & al methodology to prove convergence relying on the carre du champ is by now very standard and the proofs of the paper are only minor adaptations.

I must probably be missing something, and I encourage the authors to clarify what the main novelties are when compared to the several papers by Dupuis & al. 

REMARKS:
1) I do not really understand the emphasis on optimisation while all the proofs are related to the convergence to the stationary distributions.

","The sentiment score is derived from the initial positive remarks about the writing quality and high-level descriptions, balanced by the critical feedback on the novelty and standard nature of the proof techniques. This results in a moderately positive sentiment. The politeness score is high because the reviewer uses polite language, acknowledges their own potential misunderstanding, and encourages the authors to clarify their contributions.",30,80
"In this paper, the authors derive exact formulas for computing singular values of convolutional layers of deep neural networks. By appealing to fast FFT transformations, they show that computing the singular values can be done much faster than computing the full SVD of the convolution matrix. This obviates the needs to approximate the singular values. They use these results to then devise regularization schemes for DNN layers, and show that employing this regularization helps with model performance. 

They show that the algorithm with the operator norm regularization can be solved via an alternating projection scheme. They also postulate that since this might be expensive and unnecessary, one can also perform just 2 projections after every few SG iterations, and claim that this acts as a 'warm start' for subsequent iterations. Experiments reveal that this does not degrade the performance too much. 


The paper is well written and easy to understand. The proofs follow from standard linear algebra methods, and are easy to follow. ","The review is generally positive, highlighting the novelty and efficiency of the proposed method for computing singular values of convolutional layers. The reviewer appreciates the clarity and comprehensibility of the paper, as well as the soundness of the proofs. The language used is polite and constructive, with no negative or rude remarks.",80,90
"Summary: 
The paper proposes to add to the original GAN (2014) loss a zero-centered gradient penalty as the one defined in the WGAN-GP paper. It also provides an analysis on the mode collapse and lack of stability of classical GANs. The authors compare results using their penalty on a few synthetic examples and on image net dogs generations to results using the classical GAN loss with or without gradient penalties. 

Positive points:
The paper is interesting to read and well illustrated. 
An experiment on imagenet illustrates the progress that can be achieved by the proposed penalty.

Points to improve: 

If I understood correctly, the main contribution resides in the application of the GP proposed by WGAN-GP to the original setting. Why not compare results to WGAN-GP in this case? Since the proposal of GANs, many papers addressed the mode collapse problem. WGAN-GP, VEEGAN, or Lucas et al arXiv:1806.07185, ICML 2018 to name only a few. 
The related work section looks incomplete with some missing related references as mentioned above, and copy of a segment that appears in the introduction. 
The submission could maybe improved by segmenting the work into intro / related / background (with clear equations presenting the existing GP) / analysis / approach / experiments
The experiments on synthetic data could be improved: for reproducibility, many works on GANs used the same synthetic data as VEEGAN. 
The imagenet experiment lacks details.   ","The sentiment of the review is generally positive, as indicated by phrases like 'The paper is interesting to read and well illustrated' and 'An experiment on imagenet illustrates the progress that can be achieved by the proposed penalty.' However, the review also includes several critical points and suggestions for improvement, which slightly temper the overall positivity. Therefore, the sentiment score is 40. The politeness of the language is high, as the reviewer uses polite and constructive language throughout, such as 'If I understood correctly,' 'Why not compare results to WGAN-GP in this case?' and 'The submission could maybe improved by segmenting the work.' Thus, the politeness score is 80.",40,80
"Summary: This paper mixes automated theorem proving with machine learning models. The final goal, of course, is to be able to train a model that works in conjunction with an automated theorem proving system to efficiently prove theorems, and, ideally, in a way that resembles the way humans prove theorems. This is a distant goal, and the authors instead focus on several tractable tasks that are required for future progress in this direction. They start by integrating the Coq theorem proving environment with ML frameworks, allowing for the creation of models that perform various tasks related to theorem proving. In particular, they focus on two tasks. One is to estimate how many steps are left to complete the proof given a current proof state. The other is to determine what is a good choice of next step. Finally, they also consider issues surrounding representations of the various data structures involved in proofs (i.e., the proof tree, variables, etc.). They test various models on a synthetic nearly trivial logical expression proof, along with a more complicated (and meaningful real world) group theory result.

Strengths: This is a very important area. Automated theorem proving has a potentially very significant impact, and being able to take advantage of some of the recent successes in ML would be excellent. The main environment proposed here, integrating PyTorch with Coq could potentially be a very useful platform for future research in this area. The paper exposes many interesting questions, and I generally think we need more exploratory papers that open up an area (as opposed to seeking to finalize existing areas) 

Weaknesses: The paper is pretty tough to understand without a lot of background in all of the existing theorem proving work (which might be fine for a conference in this area, but for this venue it would be nice to be more self-contained). The organization could also use some work, since it's often tough to figure out what the authors actually did. The experimental results seem very preliminary---although it's hard to say, as there is no easy way to compare the results to anything else out there. In general a lot of details seem missing.

Verdict: The authors admit this is a preliminary work, and I agree with that. The paper certainly introduces many more questions than it answers. However, I think that in this case it's a good thing, and this type of paper has the potential to inspire a lot of new and exciting research, so I voted for acceptance.

Comments and questions:

- As mentioned, a lot of the terminology is introduced very quickly and could stand to be more self-contained, i.e., ""tactics"" could be defined as being simple transformations that are applied to a current proof state to obtain another proof state, and each language has a library of tactics available.

- Probably the major contribution of the work is the integration of the CoQ and Pytorch, so a bit more content describing how the Python data structures that wrap around Coq structures would be interesting here.

- I didn't really understand one of the major contributions: the embedding function for the M_i conditioned on the environment. How does the sampled Gaussian vector work here? In general this section is pretty confusing, it would be great to include a schematic to show how the different levels of embeddings for different structures work here.

- How does the real-world dataset work? Does the dataset contain one automated proof of the entire theorem, or several different proofs (ultimately produced by different user choices)? Are you measuring accuracy on the proofs of individual lemmas?","The sentiment of the review is generally positive. The reviewer acknowledges the importance of the research area and the potential impact of integrating automated theorem proving with machine learning. They also appreciate the exploratory nature of the paper and its potential to inspire future research. However, the reviewer points out several weaknesses, such as the paper being tough to understand without extensive background knowledge, organizational issues, and preliminary experimental results. Despite these criticisms, the overall sentiment leans towards acceptance, indicating a positive score. The politeness of the language is high, as the reviewer provides constructive feedback and suggestions for improvement without being harsh or dismissive. They use polite language and acknowledge the efforts of the authors.",70,90
"This paper investigates batch normalization from three points of view. i) Loss decomposition, ii) learning rate selection, iii) generalization. If carefully read, I believe authors have interesting results and insightful messages. However, as a whole, I found the paper difficult to follow. Too much content is packed into too little space and they are not necessarily coherent with each other. Many of the technical terms are not motivated and even not defined. Overall, cleaning up the exposition would help a lot for readability. 

I have a few other technical comments.
1) Theorem 1 is not acceptable for publication. It is not a rigorous statement. This should be fixed.
2) Effective and maximum learning rate is not clear from the main body of the paper. I can intuitively guess what they are but they lack motivation and definition (as far as I see).
3) In Section 3 I believe random data is being assumed (there is expectation over x in some notation). This should be stated upfront. Authors should broadly comment on the applicability of the learning rates calculated as N->\infty in the finite N,P regime?","The sentiment score is determined based on the overall tone of the review. The reviewer acknowledges that the paper has interesting results and insightful messages, which is positive. However, the reviewer also mentions that the paper is difficult to follow, lacks coherence, and needs significant improvements in exposition. Therefore, the sentiment score is slightly positive but tempered by the criticisms. The politeness score is based on the language used by the reviewer. The reviewer uses polite language, such as 'I believe,' 'should be fixed,' and 'cleaning up the exposition would help,' which indicates a respectful and constructive tone.",20,80
"Cons

1.	It’s unclear why LABC produces lower scores than ‘normal’ training on ‘normal’ testing.
2.	The text says nothing I can find to explain why in Fig 5 the ‘entity’ vectors have all 0s except in one dimension, which seems to make the problem considerably easier.
3.	In a sense, there is no cross-domain adaptation required in the symbolic task: min is min, whether it operates on dimension k of the source vectors or dimension j of the target vectors. On the other hand, dimensions are processed independently in the model, as far as I can tell, so there’s no free transfer of learning min on dimension k to knowing min on dimension j. It would be good to comment on this issue.
4.	There seem to be obvious analogies (so to speak) to GANs, and it is very curious that this is not mentioned anywhere that I can see. This is particularly glaring in Sec. 5.3.
5.	The quantitative results are scattered throughout the prose; it would be challenging, but worthwhile, to gather them into an actual table.

Pros

6.	The basic idea (“We should aspire to select as negative examples those examples that are plausible considering the most abstract principles that describe the data”, p. 14) is very intuitive, common-sensical, bordering on obvious. But it is not at all obvious that the idea has as much power as is demonstrated in the experiments. The transfer to novel domain combinations, novel domains, and novel values of dimensions is impressive and surprising.
7.	The result that the proposed training, designed to promote generalization on analogy tasks, also seems to promote improved sensory processing is interesting. Whether it really instantiates the parallel connection argued for by the High-Level Perception view from psychology/philosophy is debatable, but that is itself an interesting connection that the authors should be praised for identifying.
8.	In general, the connection to the cognitive literature is creative and tantalizing and provides good scientific grounding for the work.
9.	The linking to the flexibility of word meanings in the final paragraph pushes the limit of the plausibility of connection to broader cognitive issues, but I’m inclined to indulge the authors for at least bringing up this important and relevant issue.  
","The review contains both positive and negative feedback. The negative points are presented first and are quite detailed, indicating areas where the paper could be improved. However, the positive points are also substantial and highlight the strengths of the paper. The sentiment score is balanced by the fact that the reviewer acknowledges the impressive and surprising results, as well as the creative and tantalizing connections to cognitive literature. The politeness score is high because the reviewer uses polite language, even when pointing out flaws, and praises the authors for their interesting connections and grounding.",50,80
"This paper is one of a sequence of works trying to learn heuristics for solving combinatorial optimisation problems. Compared to its predecessors, its contributions are three-fold. First, it introduces a tweak on the REINFORCE learning algorithm, outperforming more complicated methods. Second, it introduces a new model for combinatorial tasks which delivers interesting results on several tasks which are varied though related. Finally, it evaluates this model on many tasks.

****Quality and clarity****
This is a very high-quality paper. 
The writing is clear and sharp, and the reading experience is quite enjoyable (the witty first paragraph sets the tone for what is to follow), even if the text is at times a bit verbose. 
Another point to commend is the honesty of the paper (see e.g. the comment on the performance of the model on TSP vs specialised solvers such as Concord).
The related work section is complete and well documented.
Finally, the experimental results are clearly presented and well-illustrated.

****Originality and significance****
On the theoretical side, the contributions of this paper are interesting but not ground-breaking. The REINFORCE tweak is close to other algorithms that have been tried in the last few years (such as indeed the one presented in Rennie et al, 2016). The model architecture, while successful, is not a large departure from the Transformer presented in Vaswani et al, 2017.

More significant is the complete set of experiments on a varied subset of combinatorial tasks, which showcases one of the promises of using machine learning for combinatorial optimisation: reusability of a single model for many tasks.

****Conclusion****
Overall, this is a nice, very well-written paper. Its contributions, though not ground-breaking, are significant to the field, and constitute another step in the right direction.

Pros
- high-quality writing
- very clear
- complete experiments on a variety of tasks, some of which do not have optimal solvers
- honest assessment of the model

Cons
- the theoretical contributions are not ground-breaking (either the the tweak on REINFORCE or the model architecture)
- the model is still far from obtaining meaningful results on TSP (although it's interesting to compare to previous learned models, only solving problems with 100 nodes also illustrates how far we have to go...)

Details
- Dai et al has been published at NIPS and is no longer an arxiv preprint
- the comparison to AlphaGo should either be expanded upon or scratched. Although it could be quite interesting, as it is it's not very well motivated.","The sentiment of the review is generally positive, as indicated by phrases such as 'very high-quality paper,' 'clear and sharp writing,' and 'significant to the field.' However, it also includes some constructive criticism, noting that the theoretical contributions are not ground-breaking. Therefore, the sentiment score is 80. The politeness of the language is very high, with the reviewer using polite and respectful language throughout, even when providing criticism. Phrases like 'Another point to commend,' 'honest assessment,' and 'nice, very well-written paper' indicate a high level of politeness. Therefore, the politeness score is 90.",80,90
"The paper is interesting and I like it. I draws parallels from biological learning and the well known critical learning phases in biological systems to artificial neural network learning. 
A series of empirical simulation experiments that all aim to disturb the learning process of the DNN and to artificially create criticality are presented. They are providing food for thought, in order to introduce some quantitative results, the authors use well known Fisher Information to measure the changes. So far so good and interesting.
I was disappointed to see Tishby's result (2017) only remotely discussed, an earlier work than the one by Tishby is by Montavon et al 2011 in JMLR. Also in this work properties of successive compression and dimensionality reduction are discussed, perhaps the starting point of quantitative analysis of various DNNs. 

To this point the paper presents no theoretical contribution, rather empirical findings only, that may or may not be ubiquitous in DNN learning systems. The latter point may be worthwhile to discuss and analyse. 
Overall, the paper is interesting with its nice empirical studies but stays somewhat superficial. To learn more a simpler toy model may be worthwhile to study. 

","The sentiment of the review is generally positive, as indicated by phrases like 'The paper is interesting and I like it' and 'So far so good and interesting.' However, the reviewer also expresses some disappointment and criticism, particularly regarding the lack of theoretical contribution and the superficial nature of the empirical findings. Therefore, the sentiment score is 40. The politeness of the language is quite high, as the reviewer uses polite phrases and constructive criticism, such as 'I was disappointed to see' and 'may be worthwhile to discuss and analyse.' Therefore, the politeness score is 80.",40,80
"Summary:
This work tackles few-shot (or meta) learning from a probabilistic inference viewpoint. Compared to previous work, it uses a simpler setup, performing task-specific inference only for single-layer head models, and employs an objective based on predictive distributions on train/test splits for each task (rather than an approximation to log marginal likelihood). Inference is done amortized by a network, whose input is the task training split. The same network is used for parameters of each class (only feeding training points of that class), which allows an arbitrary number of classes per task. At test time, inference just requires forward passes through this network, attractive compared to non-amortized approaches which need optimization or gradients here.

It provides a clean, decision-theoretic derivation, and clarifies relationships to previous work. The experimental results are encouraging: the method achieves a new best on 5-way, 5-shot miniImageNet, despite the simple setup. In general, explanations in the main text could be more complete (see questions). I'd recommend shortening Section 4, which is pretty obvious.

- Quality: Several interesting differences to prior work. Well-done experiments
- Clarity: Clean derivation, easy to understand. Some details could be spelled out better
- Originality: Several important novelties (predictive criterion, simple model setup, amortized inference network). Closely related to ""neural processes"" work, but this happened roughly at the same time
- Significance: The few-shot learning results are competitive, in particular given they use a simpler model setup than most previous work. I am not an expert on these kind of experiments, but I found the comparisons fair and rather extensive

Interesting about this work:
- Clean Bayesian decision-theoretic viewpoint. Key question is of course whether
   an inference network of this simple structure (no correlations, sum combination
   of datapoints, same network for each class) can deliver a good approximation to
   the true posterior.
- Different to previous work, task-specific inference is done only on the weights of
   single-layer head models (logistic regression models, with shared features).
   Highly encouraging that this is sufficient for state-of-the-art few-shot classification
   performance. The authors could be more clear about this point.
- Simple and efficient amortized inference model, which along with the neural
   network features, is learned on all data jointly
- Optimization criterion is based on predictive distributions on train/test splits, not
   on the log marginal likelihood. Has some odd consequences (question below),
   but clearly works better for few-shot classification

Experiments:
- 5.1: Convincing results, in particular given the simplicity of the model setup and
   the inference network. But some important points are not explained:
   - Which of the competitors (if any) use the same restricted model setup (inference
      only on the top-layer weights)? Clearly, MAML does not, right? Please state this
      explicitly.
   - For Versa, you use k_c training and 15 test points per task update during
      training. Do competitors without train/test split also get k_c + 15 points, or
      only k_c points? The former would be fair, the latter not so much.
- 5.2: This seems a challenging problem, and both your numbers and reconstructions
   look better than the competitor. I cannot say more, based on the very brief
   explanations provided here.
   The main paper does not really state what the model or the likelihood is. From
   F.4 in the Appendix, this model does not have the form of your classification
   models, but psi is input at the bottom of the network. Also, the final layer has
   sigmoid activation. What likelihood do you use?
   One observation: If you used the same ""inference on final layer weights"" setup
   here, and Gaussian likelihood, you could compute the posterior over psi in closed
   form, no amortization needed. Would this setup apply to your problem?

Further questions:
- Confused about the input to the inference network. Real Bayesian inference would
   just see features h_theta(x) as inputs, not the x's. Why not simply feed features in
   then?
   Please do improve the description of the inference network, this is a major
   novelty of this paper, and even the appendix is only understandable by reading
   other work as well. Be clear how it depends on theta (I think nothing is lost by
   feeding in the h_theta(x)).
- The learning criterion based on predictive distributions on train/test splits seem
   to work better than ELBO-like criteria, for few-shot classification.
   But there are some worrying aspects. The marginal likelihood has an Occam's
   razor argument to prevent overfitting. Why would your criterion prevent overfitting?
   And it is quite worrying that the prior p(psi | theta) drops out of the method
   entirely. Can you comment more on that?

Small:
- p(psi_t | tilde{x}_t, D_t, theta) should be p(psi_t | D_t, theta). Please avoid a more
   general notation early on, if you do not do it later on. This is confusing
","The sentiment of the review is generally positive. The reviewer appreciates the clean, decision-theoretic derivation, the simplicity of the model setup, and the competitive results achieved by the method. However, there are some critical points and questions raised, particularly regarding the clarity of explanations and some experimental details. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, even when pointing out areas for improvement.",70,90
"This paper introduces a novel architecture for sequence modeling, called the trellis network. The trellis network is in a sense a combination of RNNs and CNNs. The authors give a constructive proof that the trellis network is a special case of a truncated RNN. It also resembles CNNs since the neurons at higher levels have bigger receptive fields. As a result, techniques from RNN and CNN literature can be conveniently brought in and adapted to trellis network. The proposed method is evaluated on benchmark tasks and shows performance gain over existing methods.

The paper is well-written and easy to follow. The experimental study is extensive. The reviewer believes that this paper will potentially inspire future research along this direction. However, the novelty of the proposed method compared to the TCN seems limited: only weight sharing and input injection. It would be great to include the performance of the TCN on the PTB dataset, on both word and character levels in Table 1 and 2.

According to Theorem 1, to model an M-truncated L-layer RNN, a trellis network needs M + L − 1 layers. When M is large, it seems that a trellis network needs to be deep. Although this does not increase to model size due to weight sharing, does it significantly increase computation time, both during training and inference?

The review might have missed it, but what is the rationale behind the dotted link in Figure 1a, or the dependence of the activation function $f$ on $z_t^{(i)}$? It seems that it is neither motivated by RNNs nor CNNs. From RNN's point of view, as shown in the proof of Theorem 1, $f$ only depends on its first argument. From CNN's point of view, the model still gets the same reception field without using $z_t^{(i)}$.

Minor comments:
The authors might want to give the full name of TCN (temporal convolutional networks) and a short introduction in Section 2 or at the beginning of Section 4.","The sentiment of the review is generally positive, as the reviewer acknowledges the novelty of the proposed architecture, the clarity of the writing, and the extensive experimental study. However, there are some critical points regarding the novelty compared to TCN and questions about the computational efficiency, which slightly temper the overall positivity. Therefore, the sentiment score is 60. The politeness of the language is very high; the reviewer uses polite and constructive language throughout the review, making suggestions and asking questions in a respectful manner. Thus, the politeness score is 90.",60,90
"The paper proposes a code completion task that given the rest of a program, predicts the content of an expression. This task has similarity to code completion tasks in the code editor of an IDE. The paper proposes an interesting problem, but the paper would benefit if writing and evaluation are significantly improved.

The work builds on prior research by Allamanis et al. 2018b that performs such completions of single variables by picking from the variables in the scopes. The difference here is that portions of parse trees are predicted as opposed to a single variables, where the algorithm from the prior research is used to predict single variables.

Writing-wise the paper is hard to read on the technical part with many unclear details and this portion needs a good amount of extra explanations. The Epsilon set includes triples which are not described and need understanding equation (2). The first element of this triple is an edge label <edge>($a$, $v$) where $a$ is an AST and $v$ is a node. Thus, edges of the graph end up between entire ASTs and nodes? While I can see how could this make sense, there is certainly lack of explanation going on here. Overall, this part is hard to parse and time-consuming to understand except at high level. Furthermore, the text has many functions without signatures and they seem to be used before they are defined (e.g. getRepresentation).

Technically, the approach also seems very similar to N3NN by Parisotto et al, ICLR 2017. There should be more elaboration on what is new here. Otherwise, the novelty of the paper really is just combining this work with Allamanis et al. 2018b.

In terms of evaluation, the task seems to be on a different set of expressions than the one explained in the exposition. How many expressions where there in the evaluation programs and how many were chosen to evaluate on and based on what criteria. It seems from the exposition that expressions with field accessed and function calls are not possible to be generated, but then some completions show method calls. How much of the full task is actually solved? In particular, several of the cited prior works solve specific problems like constants that are ignored here.

The evaluation is mostly an ablation studies of the proposed approach by removing edges from the final idea. 
Besides this, the paper also introduces a new dataset for showcasing the technique and does not report sizes and running times, essentially not answering basic questions like what is the trade-off between the different techniques. Comparison to actual prior works on similar tasks is also lacking (some TODO is left in the paper), but there is the claim that existing neural techniques such as seq2seq perform ""substantially worse"". I guess the authors have extra experiments not included for lack of space or that the evaluation was not ready at submission time.
","The sentiment of the review is mixed but leans towards the negative side. The reviewer acknowledges that the paper proposes an interesting problem, which is a positive point, but the majority of the review focuses on significant issues with the writing, technical clarity, and evaluation. The reviewer points out that the paper is hard to read, lacks clear explanations, and has many technical ambiguities. Additionally, the evaluation is criticized for being incomplete and lacking comparison to prior works. Therefore, the sentiment score is -40. The politeness of the language is relatively high. The reviewer uses polite language and constructive criticism, avoiding any rude or harsh terms. They provide specific feedback and suggestions for improvement, which indicates a polite tone. Therefore, the politeness score is 80.",-40,80
"* Summary
This paper proposes batch normalization for learning RNNs with binary or ternary weights instead of full-precision weights. Experiments are carried out on character-level and word-level language modeling, as well as sequential MNIST and question answering.


* Strengths
- I liked the variety of tasks used evaluations (sequential MNIST, language modeling, question answering).
- Encouraging results on specialized hardware implementation.


* Weaknesses
- Using batch normalization on existing binarization/ternarization techniques is a bit of an incremental contribution.
- All test perplexities for word-level language models in table 3 underperform compared to current vanilla LSTMs for that task (see Table 4 in https://arxiv.org/pdf/1707.05589.pdf), suggesting that the baseline LSTM used in this paper is not strong enough.
- Results on question answering are not convincing -- BinaryConnect has the same size while achieving substantially higher accuracy (94.66% vs 40.78%). This is nowhere discussed and the paper's major claims ""binaryconnect method fails"" and ""our method [...] outperforms all the existing quantization methods"" seem unfounded (Section 5.5).
- In the introduction, I am lacking a distinction between improvements w.r.t. training vs inference time. As far as I understand, quantization methods only help at reducing memory footprint or computation time during inference/test but not during training. This should be clarified.
- In the introduction on page 2 is argued that the proposed method ""eliminates the need for multiplications"" -- I do not see how this is possible. Maybe what you meant is that it eliminates the need for full-precision multiplications by replacing them with multiplications with binary/ternary matrices? 
- The notation is quite confusing. For starters, in Section 2 you mention ""a fixed scaling factor A"" and I would encourage you to indicate scalars by lower-case letters, vectors by boldface lower-case letters and matrices by boldface upper-case letters. Moreover, it is unclear when calculations are approximate. For instance, in Eq. 1 I believe you need to replace ""="" with ""\approx"". Likewise for the equation in the next to last line on page 2. Lastly, while Eq. 2 seems to be a common way to write down LSTM equations, it is abusive notation.


* Minor Comments
- Abstract: What is ASIC? It is not referenced in Section 6.
- Introduction: What is the justification for calling RNNs over-parameterized? This seems to depend on the task. 
- Introduction; contributions: Here, I would like to see a distinction between gains during training vs test time.
- Section 3.2 comes out of nowhere. You might want to already mention why are introducing batch normalization at this point.
- The boldfacing in Table 1, 2 and 3 is misleading. I understand this is done to highlight the proposed method, but I think commonly boldfacing is used to highlight the best results.
- Figure 2b. What is your hypothesis why BPC actually goes down the longer the sequence is?
- Algorithm 1, line 14: Using the cross-entropy is a specific choice dependent on the task. My understanding is your approach can work with any differentiable downstream loss?","The sentiment of the review is mixed. The reviewer acknowledges some strengths of the paper, such as the variety of tasks used for evaluation and the encouraging results on specialized hardware implementation. However, the review also highlights several weaknesses, including the incremental nature of the contribution, underperformance in certain tasks, and unclear claims. The sentiment score is therefore slightly negative. The politeness of the language is generally high; the reviewer uses polite and constructive language, offering specific suggestions for improvement without being dismissive or rude.",-20,80
"Summary

This paper decomposes the image restoration task in two part: the restoration part handled by a restoration RNN, and the number of steps to apply the RNN is determined using a policy unit. 
State of the art results are achieved on blind grey level Gaussian noise denoising on the BSD68 dataset.

The approach is novel to my knowledge, the paper is well written, the results are good and well illustrated.

Questions:
-It would be nice to present results on color images, and on datasets that contains natural noises.  
-Lowering the learning rate on plateaus during training is done by hand or is there an automatic criterion to define the plateaus?

Minor:
page 1: extra "")"" after ref to Bredies et al 2010
could cite Chen, Zu, Koltun ICCV17 in deep models for restoration
Several ""L"" have been replaced by ""_' e.g. under review at IC_R, R_-based, etc in the whole paper
p.4: rain-> train
greatly influence -> greatly influences
p5: typo performace
make a uniform bib: whole first name or abbr. , no URL, etc.
p6: the weight -> the set of weights 
add the specification that the noise is Gaussian
the sentence ""the training set and testing set of ..."" is used twice, remove one.
p7 Table 1: the perf of DnCNN-B is 29.16 and not 29.15 for sigma 25, right?","The sentiment of the review is positive, as indicated by phrases such as 'the approach is novel,' 'the paper is well written,' and 'the results are good and well illustrated.' The reviewer also provides constructive feedback and minor corrections, which suggests a positive overall impression. Therefore, the sentiment score is 80. The politeness of the language is high, as the reviewer uses polite phrases like 'It would be nice to' and 'could cite,' and provides feedback in a respectful manner. Thus, the politeness score is 90.",80,90
"This paper trains a neural network to solve the satisfiability problems. Based on the message passing neural network, it presents NeuroSAT and trains it as a classifier to predict satisfiability under a single bit of supervision. After training, NeuroSAT can solve problems that are larger and more difficult than it ever saw during training. Furthermore, the authors present a way to decode the solutions from the network's activations. Besides, for unsatisfiable problems, the paper also presents NeuroUNSAT, which learns to detect the contradictions in the form of UNSAT cores.

Relevance: this paper is likely to be of interest to a large proportion of the community for several reasons. Firstly, satisfiability problems arise from a variety of domains. This paper starts with a new angle to solve the SAT problem. Secondly, it uses neural networks in the SAT problem and establishes that neural networks can learn to perform a discrete search. Thirdly, the system used in this paper may also help improve existing SAT solvers.

Significance: I think the results are significant. For the decoding satisfying assignments section, the two-dimensional PCA embeddings are very clear. And the NeuroSAT's success rate for more significant problems and different problems has shown it's generalization ability. Finally, the sequences of literal votes in NeuroUNSAT have proved its ability to detect unsatisfied cores.

Novelty: NeuroSAT’s approach is novel. Based on message passing neural networks, it trains a neural network to learn to solve the SAT problem. 

Soundness: This paper is technically sound. 

Evaluation: The experimental section is comprehensive. There are a variety of graphs showing the performance and ability of your architecture. However, the theoretical analysis isn't very sufficient. For instance, why does the change of the dataset from the original SR(n) to SRC(n,u) lead to the change of the behavior of the network from searching for a satisfying assignment indefinitely to detecting the unsatisfiable cores?

Clarity: As a whole, the paper is clear. The definition of the problem, the model structure, the data generation, the training procedure, and the evaluation are all well organized. However, there is still a few points requiring more explanation. For instance, in figure 3, I am not sure whether darker value means larger value or smaller value because the authors only mentioned that white represents zero, blue negative and red positive. Also, in figure 7, I am not sure whether those black grids represent higher positive values or lower negative values.

A few questions:

What's the initialization of the two vectors the authors use for tiling operation? Does the initialization differ for different types of SAT problems?

How do the authors decide the number of iterations necessary for solving a particular SAT problem?

","The sentiment of the review is highly positive, as the reviewer praises the relevance, significance, novelty, soundness, and clarity of the paper. The reviewer highlights the paper's potential interest to the community, the significance of the results, the novelty of the approach, and the technical soundness. The only minor criticisms are related to the theoretical analysis and some clarity issues in the figures, which are presented constructively. Therefore, the sentiment score is 90. The politeness of the language is very high, as the reviewer uses polite and respectful language throughout the review, even when pointing out areas for improvement. Therefore, the politeness score is 100.",90,100
"The paper addresses the latent space distribution mismatch in VAEs and GANs. The authors try to solve the issue by optimal transport theory and the proposed method on the latent space yields better quality in the generated samples.

To me, the motivation is not very strong. In DCGAN, amazingly, latent space linear operations can carry over to the generated images. But it’s not something people are usually concerned with in GANs.  I understand that latent space operations can provide insights into how the trained generator works. But how can it improve the actual GAN training? Choosing Gaussian or uniform distribution for the latent variable is mainly for ease of computation and I am not sure if the motivation to match the distributions is very strong in GAN applications. Perhaps it more important in the context of VAEs.

At the first glance, the proposed form of transformation is not surprising. Though optimal transport is a very powerful theoretical tool, it serves more like an explanation or validation, rather than the motivation. I felt the theory part could be simper. 

In the quantitative comparisons with other methods, all simulations seem to be in the context of GAN. The difference in 2-point cases (table 2) is not significant and the author only compares with linear interpolation but not SLERP. I would like to see more quantitative comparisons with other methods and also some empirical studies in the context of VAEs. ","The sentiment of the review is moderately negative. The reviewer expresses skepticism about the motivation behind the work and questions its significance in the context of GANs. The reviewer also finds the proposed transformation unsurprising and suggests that the theory part could be simpler. However, the review is not entirely dismissive and acknowledges the use of optimal transport as a powerful theoretical tool. The politeness of the language is neutral to slightly polite. The reviewer uses phrases like 'I understand' and 'I would like to see,' which soften the critique and make it more constructive.",-40,20
"This paper proposes a variant of GEM called A-GEM that substantially improves the computational characteristics of GEM while achieving quite similar performance. To me the most interesting insight of this work is the proof that an inner product between gradients can suffice instead of needing to solve the quadratic program in GEM – which I have found to be a major limitation of the original algorithm.  The additional experiments using task descriptors to enable zero shot learning are also interesting.  Moreover, the discussion of the new evaluation protocol and metrics make sense with further clarification from the authors. Overall, I agree with the other reviewers that this paper makes a clear and practical contribution worthy of acceptance. 
","The sentiment of the review is highly positive, as indicated by phrases like 'substantially improves,' 'most interesting insight,' 'interesting,' and 'clear and practical contribution worthy of acceptance.' These expressions suggest a strong approval of the paper's contributions. Therefore, the sentiment score is 90. The politeness of the language is also very high, with the reviewer using respectful and appreciative language throughout the review. Phrases like 'I have found,' 'make sense with further clarification,' and 'I agree with the other reviewers' indicate a considerate and professional tone. Thus, the politeness score is 100.",90,100
"This paper proposes a novel method to perform meta-learning for stochastic gradient MCMC. They utilize a general family of SDEs that guarantees preservation of the target density with somewhat loose constraint on the drift and diffusion functions (from Ma et al. (2015)). Then, they propose learning these functions on a set of training tasks and evaluating on unseen, different tasks, in a meta-learning fashion.

This paper is well written and easy to follow. They do a very good job presenting the motivation for their work as well as seminal work in SG-MCMC. The idea is fairly natural, especially in light of recent success of meta-learning and learning optimizers. They do a thorough survey of related work and also do a good job presenting their method in context of very modern work on MCMC and SG-MCMC.

I am not completely convinced by the meta-training objective; both losses seem natural but quite intractable to compute in practice. The use of Stein indicates that the kernel must probably be *very* carefully crafted and given that the whole method relies on this objective, it seems like this could be a breaking point. I am also curious to know how you diagnostic/evaluate the choice of these kernels.

In terms of evaluation, the experimental results are not the most convincing given that across the board, they are (except in one case) in 4 case within 0.2% of SGHMC and in the two others, within 0.5% and 0.8% respectively. This seems a bit weak, especially considering the compute invested both at training time and for each SG-MCMC step (i.e. getting the outputs from the neural networks vs simply doing HMC). Is there really a case for using the method over SG-HMC? I would have also very much liked to see a run-time evaluation.","The sentiment of the review is generally positive, as the reviewer praises the paper for being well-written, easy to follow, and for presenting a thorough survey of related work. However, the reviewer also expresses some concerns about the meta-training objective and the experimental results, which slightly temper the overall positive sentiment. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, even when pointing out potential weaknesses. Therefore, the politeness score is 90.",60,90
"This paper introduces a domain adaptation approach based on the idea of Cyclic GAN. Two different algorithms are proposed. The first one incorporates a semantic consistency loss based on domain-specific classifiers acting on full cycles of the of the generators. The second one also makes use of domain-specific classifiers, but acting either directly on the training samples or on the data mapped from one domain to the other.

Strengths:
- The different terms in the proposed loss functions are well justified.
- The results on low-resources supervised domain adaptation indicate that the method works better than the that of Motiian et al. 2017.

Weaknesses:
- Novelty is limited: The two algorithms are essentially small modification of the semantic consistency term used in Hoffman et al. 2018. They involve making use of both the source and target classifiers, instead of only the source one, and, for the relaxed version, making use of complete cycles instead of just one mapping from one domain to the other. While the modifications are justified, I find this a bit weak for ICLR.

- It is not clear to me why it is worth presenting the relaxed cycle-consistency object, since it always yields worse results than the augmented one. In fact, at first, I though both objectives would be combined in a single loss, and was thus surprised not to see Eq. 5 appear in Algorithm 1. It only became clear when reading the experiments that the authors were treating the two objectives as two different algorithms. Note that, in addition to not performing as well as the augmented version, it is also unclear how the relaxed one could work in the unsupervised scenario.

- Experiments:
* In 4.1, the authors mention that 10 samples per class are available in the target domain. Are they labeled or unlabeled? If labeled, are additional unlabeled samples also used?
* In Table 1, and in Table 3, is there a method that corresponds to CyCADA? I feel that this comparison would be useful considering the similarity. That said, I also understand that CyCADA uses both a reconstruction term (as in Eq. 4) and a semantic consistency one, whereas here only a semantic reconstruction term is used. I therefore suggest the authors to also compare with a baseline that replaces their objective with the semantic consistency one of CyCADA, i.e., CyCADA without reconstruction term.
* In 4.2, it is again not entirely clear if the authors use only the few labeled samples, or if this is complemented with additional unlabeled samples. In any event, does this reproduce the setting used by Motiian et al. 2017?
* As the argument is that the proposed loss is better than the reconstruction one and that of Hoffman et al. 2018 for low-resource supervised adaptation, it would be worth demonstrating this empirically in Table 2.

Summary:
The proposed objective functions are well motivated, but I feel that novelty is too limited and the current set of experiments not sufficient to warrant publication at ICLR.

After Response:
After the authors' response/discussion, while I appreciate the additional results provided by the authors, I still feel that the contribution is a bit weak for ICLR.
","The sentiment of the review is mixed but leans towards the negative side. The reviewer acknowledges some strengths of the paper, such as the well-justified loss functions and better performance compared to a previous method. However, the reviewer also points out significant weaknesses, including limited novelty and unclear experimental details. The overall sentiment score is -30, reflecting a slightly negative but not entirely dismissive tone. The politeness of the language is quite high; the reviewer uses polite and constructive language throughout, even when pointing out weaknesses. Therefore, the politeness score is 80.",-30,80
"This work is concerned with the problem of batch contextual bandits, in which a target contextual bandit policy is optimized on the data generated by a different logging policy. The main problem is to come up with a low-variance low-bias estimator for the value of the target policy. Many of the known techniques are based on an unbiased estimator known as inverse propensity scoring (IPS), which uses the distribution over actions of the logging policy, conditioned on the observed contexts. However, IPS suffers from large variance. The paper's idea is to do a maximum likelihood fit of a simple surrogate policy to the logged data, and then use the conditional distribution over actions of the surrogate policy to compute inverse propensity scores.
The theoretical results show that the bias of this estimator vanishes asymptotically, whereas the variance is smaller than IPS. Experiments using known/unknown logging policies on artificial/real-world bandit data show that the IPS scores computed with the proposed technique are empirically better than those computed directly using the logging policy. Moreover, the advantage increases when the distribution extracted from the surrogate policy is used to compute more sophisticated estimators than IPS.

The off-policy evaluation in contextual bandits is an important problem, and this paper appears to make some progress. However, the theoretical analysis is a bit disappointing, as it does not shed much light on the reasons why using a surrogate policy should help. Some additional discussion would add value to the paper.

The result about the decrease in variance depends on assumptions that are not clearly justified, and is expressed in terms of abstract quantities that hard to connect to concrete scenarios. In the end, one does not get many new insights from the theory.

In Assumptions 3.3-3-4, what is the variable w.r.t the asymptotic notations are understood? By that I mean, the variable n such that f(n) = O(g(n)).

The experiments are competent and quite elaborated. However, the statistical significance of the improvements in Table 1 is unclear.

The evaluation criterion for the Criteo experiment is unclear. As a consequence it is hard to appreciate the significance of the improvements in this case.","The sentiment of the review is mixed. The reviewer acknowledges the importance of the problem and the progress made by the paper, which is positive. However, they also express disappointment with the theoretical analysis and raise several concerns about the assumptions and clarity of the results. Therefore, the sentiment score is slightly positive at 20. The language used in the review is polite and constructive, with phrases like 'appears to make some progress,' 'some additional discussion would add value,' and 'the experiments are competent and quite elaborated.' The reviewer provides specific feedback without being rude or dismissive, so the politeness score is high at 80.",20,80
"Summary:

This paper addresses the computational aspects of Viterbi-based encoding for neural networks. 

In usual Viterbi codes, input messages are encoded via a convolution with a codeword, and then decoded using a trellis. Now consider a codebook with n convolutional codes, of rate 1/k. Then a vector of length n is represented by inputing a message of length k and receiving n encoded bits. Then the memory footprint (in terms of messages) is reduced by rate k/n. This is the format that will be used to encode the row indices in a matrix, with n columns.  (The value of each nonzero is stored separately.)  However, it is clear that not all messages are possible, only those in the ""range space"" of my codes. (This part is previous work Lee 2018.) 

The ""Double Viterbi"" (new contribution) refers to the storage of the nonzero values themselves. A weakness of CSR and CSC (carried over to the previous work) is that since each row may have a different number of nonzeros, then finding the value of any particular nonzero requires going through the list to find the right corresponding nonzero, a sequential task. Instead, m new Viterbi decompressers are included, where each row becomes (s_1*codeword_1 + s_2*codeword2 + ...) cdot mask, and the new scalar are the results of the linear combinations of the codewords. 

Pros:
 - I think the work addressed here is important, and though the details are hard to parse and the new contributions seemingly small, it is important enough for practical performance. 
 - The idea is theoretically sound and interesting.

Cons: 
 - My biggest issue is that there is no clear evaluation of the runtime benefit of the second Viterbi decompressor. Compressability is evaluated, but that was already present in the previous work. Therefore the novel contribution of this paper over Lee 2018 is not clearly outlined.
 - It is extremely hard to follow what exactly is going on; I believe a few illustrative examples would help make the paper much clearer; in fact the idea is not that abstract. 
 - Minor grammatical mistakes (missing ""a"" or ""the"" in front of some terms, suggest proofread.)

","The sentiment score is determined by the overall tone and content of the review. The reviewer acknowledges the importance and theoretical soundness of the work, which is positive, but also points out significant issues such as the lack of clear evaluation and difficulty in understanding the paper. This mixed feedback results in a sentiment score of 20. The politeness score is based on the language used in the review. The reviewer uses polite language, providing constructive criticism and suggestions for improvement without being rude or dismissive. Therefore, the politeness score is 80.",20,80
"The submission analyzes parameter averaging in GAN training, positing that using the exponential moving average (EMA) leads to more well-behaved solutions than using moving averages (MA) or no averaging (None). 

While reading the submission, the intuitively given explanations for using EMA (cycling, mainly) seem reasonable. However, I do not think there is sufficient understanding of the (non-)convergence behavior in real-world GAN settings, and this submission does not contribute much to it.
The theoretical underpinnings in Section 3.1 are quite thin, and focus on describing one particular example of a bilinear saddle problem, which is quite far from a typical GAN, as used e.g. in computer vision problems. Although interesting to read, I would not draw any wider-reaching conclusions from this carefully constructed example.

Instead, the submission serves mainly as an experimental study on why EMA works better in some of the tested cases than MA/None. Main quantitative measures are the often-used IS and FID. It is clear from both the provided quantitative values as well as the provided qualitative images that either averaging method is likely better then no averaging.

Unfortunately, IS and FID contradict each other somewhat for EMA vs. MA in Table 2, which is attributed to IS being [more] flawed [than FID]. Neither measure is flawless, however, which diminshes the usefulness of the numeric results somewhat. Well designed human studies may be complicated to set up and costly to conduct, but these could demonstrate additional confirmation of the usefulness of the proposed method.

EMA introduces an additional hyperparameter, beta, which is only discussed very briefly, and only in the context of qualitative results. I missed a more thorough discussion of the impact of beta.

Overall, the submission makes an interesting proposition (usage of EMA during GAN training), but falls short in convincing me that this is a useful thing to do in broader contexts. Overall originality is minor; projected significance is minor to medium.

EDIT: After the rebuttal, resulting in several changes and additions to the paper, I am changing my rating from 5 -> 6.","The sentiment of the review is mixed but leans slightly positive. The reviewer acknowledges the interesting proposition of using EMA during GAN training and notes some positive aspects, such as the reasonable explanations and the clear quantitative and qualitative results. However, the reviewer also points out several significant shortcomings, such as the lack of sufficient theoretical underpinning, the contradictory results between IS and FID, and the brief discussion of the hyperparameter beta. The sentiment score is therefore set at 10. The politeness of the language is quite high, as the reviewer provides constructive criticism without using harsh or dismissive language. The reviewer also acknowledges the changes made after the rebuttal and adjusts their rating accordingly. The politeness score is set at 80.",10,80
"This paper presents a mixed integer programming technique for verification of piecewise linear neural networks. This work uses progressive bounds tightening approach to determine bounds for inputs to units. The authors also show that this technique speeds up the bound determination by orders of magnitude as compared to other complete and incomplete verifiers. They also compare the advercerial accuracies on MNIST and CIFAR and improve on the lower bounds as compared to PGD and upper bounds as compared to SOA. The paper is well written and presents a valuable technique for evaluating robustness of classifiers to adversarial attacks. 
","The review is highly positive, highlighting the valuable contribution of the paper and its effectiveness in improving bounds determination and adversarial accuracies. The language used is polite and respectful, acknowledging the authors' efforts and the significance of their work.",90,90
"Paper summary: The paper presents a robust Analysis by Synthesis classification model that uses the input distribution within each class to achieve high accuracy and robustness against adversarial perturbations. The architecture involves training VAEs for each class to learn p(x|y) and performing exact inference during evaluation. The authors show that ABS and binary ABS outperform other models in terms of robustness for L2, Linf and L0 attacks respectively. 

The paper in general is well written and clear, and the approach of using generative methods such as VAE for better robustness is good. 

Pros: 
Using VAEs for modeling class conditional distributions for data is an exhaustive approach. The authors show in Fig 4 that ABS generates adversarials that are semantically meaningful for humans, which is not achieved by Madry et al and other models. 

Cons: 
1) The main concern with this work is that it is heavily tailored towards MNIST and the authors do mention this. Scaling this to other datasets does not seem easy. 
2) Using VAEs to model the conditional class distributions is a nice idea, but how does this scale for datasets with large number of classes like imagenet? This would result in having 1000s of VAEs. 
3) It would be nice to see this model behaves for skewed datasets. 

","The sentiment of the review is generally positive, as indicated by phrases like 'well written and clear' and 'good approach.' However, the reviewer also raises significant concerns about the scalability of the model to other datasets and its performance on skewed datasets. Therefore, the sentiment score is not fully positive but leans towards the positive side. The language used in the review is polite and constructive, with no rude or harsh language, and the reviewer provides specific, actionable feedback.",60,90
"This paper seeks to answer the question of whether models which process sequences, but are not strictly classical RNNs, are Turing complete.

The authors present proofs that both the Transformer and Neural GPU are turing complete, under certain conditions. I do not consider myself qualified to properly verify the proof but it seems to be presented clearly. The authors note that the conditions involved are not how these models are used in the real world. Given the complex construction required for this more theoretically based proof, it seems reasonable that this should be published now, rather than waiting until the further work discussed in the final section is completed.

I have a number of questions where if a brief answer is possible, this would enhance the manuscript. The main question is, of the simplifications and approximations used for the proof, how much does that take the model away from what is used in practice? For example, the assumption of the piecewise linear sigmoid seems like a quite big change, as there are large regions of the space which now have zero gradients. If you run a real implementation of these models, with the normal sigmoid replaced by this one, does training still work? If not, what are the implications for the proof?

The rational numbers assumption is interesting - again I wonder how this would affect the model in reality, obviously all floating points on a computer represent rationals, but it would be interesting to get a better understanding on how the lack of infinite precision rationals on real hardware affects the main results.

Does the proof rely on the input and output dimensionality being the same? Eg in the preliminaries, x_i and y_i are both d-dimensional - could this be changed?

Overall this paper is novel and interesting, I have to give a slightly low confidence score because I'm unfamiliar with a lot of the background here (eg the Siegelamnn & Sontag work). The paper does seem concise and well written.

typos and minor points:

Circular convolution definition only appears to define the values directly adjacent to the border, would it be more appropriate to define S_{h+n, :, :} = S{n, :, :}?

paragraph above equation 5, 'vectores' -> 'vectors'","The sentiment of the review is generally positive. The reviewer acknowledges the novelty and interest of the paper, and although they express some reservations due to their unfamiliarity with certain background work, they still recommend publication. This suggests a sentiment score of around 70. The language used is polite and constructive, with the reviewer asking questions and making suggestions in a respectful manner, which indicates a politeness score of around 90.",70,90
"Summary: the authors propose a new algorithm, APL, for a few-shot and a life-long learning based on an external memory module. APL uses a surprise-based signal to determine which data points to store in memory and an attention mechanism to the most relevant points for prediction. The authors evaluate APL on a few-shot classification task on Omniglot dataset and on a number analogy task.

Quality: the authors consider interesting approach to life-long learning and I really liked the idea of a surprise-based signal to choose the data to store. However, I am not convinced by the learning setting that authors study. While a digit-symbol task from the introduction is interesting to study the properties of APL, I fail to see any real world analogy where it is useful. The same happens in a few-shot omniglot classification. The authors decided to shuffle the labels within episodes that, I guess, is supposed to represent different tasks in a typical life-long learning scenario. Again, it maybe interesting to study the behaviour of the algorithm, but I don't see any practical relevance here. It would make more sense to study the algorithm in a life-long learning setting, for example, considered in [1] and [2].

Clarity: the paper is well-written in general. I failed to decode the meaning behind the paragraph under Figure 3 on page 4 and would advise the authors to re-write it. The same goes to the first paragraph on page 3.

Originality: the paper builds on the prior work of Kaiser et al., 2017 and Santoro et al., 2016, but the proposed modifications are novel to my best knowledge.

Significance: below average: the paper combines interesting ideas that potentially can be used in different learning contexts and with other algorithms, however, the evaluation does not show the benefit in an obvious way.

Other comments: 
* throughout the whole paper it is not clear if the embeddings are learned or not. I suppose they are, but what then happens to the ones in memory? If they are not, like in ImageNet example, where do they come from?
* the hyperparameter \sigma: the authors claim ""the value of \sigma seems to not matter too much"". Matter for what? It's great if the performance is stable for a wide range of \sigma, but it seems like it should have a great influence over the memory footprint of APL. I feel this is an important point that needs more attention.
* it would be interesting to see how APL performs with a simple majority vote instead the decoder layer. This would count for an ablation study and could emphasize the role of the decoder.
* Figure 4, b) plots are completely unreadable on black-and-white print, the authors might like to address that
* In conclusion, the first claim about state-of-the-art accuracy with smaller memory footprint: I don't think that the results of the paper justify this claim.

[1] Yoon et al, Lifelong Learning with Dynamically Expandable Networks, ICLR 2017
[2] Rebuffi et al,  iCaRL: Incremental Classifier and Representation Learning, CVPR 2017

********************
After authors response:

Thanks to the authors for a detailed response. The introduction led me to believe that the paper solves a different task from what it actually does. I still like the algorithm and, given that the scope of the paper is limited to a few-shot learning, I tend to change my evaluation and recommend to accept the paper. It was a good idea to change the title to avoid possible confusion by other readers. The introduction is still misleading though. It creates the impression that APL solves a more general problem where it would be good enough to limit the discussion to a few-shot learning setting and explain it in greater detail for an unfamiliar reader. Some details also seem to be missing, e.g. I didn't get that the memory is flushed after each episode and could not find where this is mentioned in the paper.","The sentiment of the review is mixed but leans towards positive. The reviewer appreciates the interesting approach and novel ideas but has concerns about the practical relevance and clarity of certain sections. The initial sentiment score would be around 20, but it improves after the authors' response, leading to a final sentiment score of 60. The politeness of the language is consistently high, with constructive feedback and suggestions provided in a respectful manner, resulting in a politeness score of 80.",60,80
"This paper empirically explores heuristics commonly used in deep learning: learning rate restarts, warmup and distillation. The authors utilize two recently proposed tools for neural network analysis: mode connectivity (MC) finding a low loss pathway between two given points in the space of DNN parameters and CCA measuring the correlation of  DNN layer activations. Conducting a set of experiments and analyzing the results the authors refine the intuition behind the considered heuristics and dynamics of corresponding training procedures. 

Strengths:

+ The authors conduct experiments ensuring robustness of MC framework.
+ In the chosen settings the experimental methodology of the paper sounds reasonable. I find the idea of DNN analysis from both perspectives of weight space and activations important.
+ Paper is well-written and organized clearly. All the used methods and experiments are adequately described.
+ The authors draw connections between obtained results and hypotheses introduced in prior work.

Weaknesses:

- There is a possible flaw in the choice of experimental settings. Authors mention Batch Normalization (BN) among heuristics widely used in deep learning. It is known that properties of both loss surface and activations are different between DNN architectures which include BN layers and those which do not. To emphasize generality of obtained results, it would be beneficial to conduct experiments for both types of DNN architectures as at the moment the majority of the results are presented for VGG architecture which typically does not include BN. Impact of other architecture modifications (e.g. skip connections) might be considered as well.

- I find the significance of the results unclear. Although the particular insights of the learning procedures are revealed there is not enough attention paid to their value for possible improvements of the procedures and their applications. There is only one idea proposed by the authors based on the experimental results – fixing the deeper layers during the warmup phase, but the practical implications of this idea are not discussed.

Other comments:

* The scale used in Figure 3 and similar figures in the appendix is not easily comprehensible. I recommend to comment further on the scale or possibly adjust it.
","The sentiment of the review is generally positive, as indicated by the strengths listed and the constructive nature of the weaknesses. The reviewer acknowledges the robustness of the experimental methodology, the clarity of the paper, and the importance of the analysis. However, they also point out specific areas for improvement, such as the choice of experimental settings and the significance of the results. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, offering suggestions for improvement rather than harsh criticism.",70,90
"This paper focuses on multi-choice QA and proposes a coarse-to-fine scoring framework. Where the coarse-grained answer scoring model computes the scores with the attention over the whole passages, and the fine-grained one only uses local contexts for each answer option (candidate).

The proposed approach was evaluated on the only dataset of WikiHop, and achieved large improvement over the other methods on the leaderboard. However, I found the paper lack of motivation about the designs of the coarse and fine scoring models. For example, why using self-attention after GRU and co-attention in the two answer scoring models?

Another concern I have is about the novelty. Besides the complicated model designs, the coarse and fine scoring models are both following some common ideas in previous work. And each model could achieve on-par results compared to previous baselines. This makes me feel that the whole approach looks more like model combination of two not-so-novel (and not very well-motivated) models.

Thirdly, the only evaluation on WikiHop brings more problems to the above two points. Since the motivation of the architecture design is not very clear, I am not sure whether the architectures could generalize to other benchmarks. Similar concern for the model combination approach.

Moreover, the proposed approach is a general architecture for multiple-choice datasets requiring multiple evidence. To verify its generalizability, I suggest the authors add further experiments on one dataset from the following ones: either multi-choice QA datasets like ARC and RACE/RACE-open, or other open-domain QA datasets like TriviaQA, by treating the re-ranking of answer predictions as multi-choice QA problems (like the approach in Evidence Aggregation for Open-Domain QA from ICLR2018).

A minor question: why the CFC w/o encoder could still work so well? At least the fine-grained scoring model should heavily rely on encoders. Otherwise, according to Eq (17), the fine-grained model cannot use any contextual information.","The sentiment of the review is mixed but leans towards negative due to the concerns raised about the lack of motivation, novelty, and generalizability of the proposed approach. The reviewer acknowledges the improvement in performance but expresses significant doubts about the design and applicability of the models. Therefore, the sentiment score is -40. The politeness of the language is relatively high; the reviewer uses polite language and constructive criticism, suggesting further experiments and providing specific questions for clarification. Thus, the politeness score is 80.",-40,80
"The paper studies the problem of representation learning in the context of hierarchical reinforcement learning by building on the framework of  HIRO (Nachum et al. (2018)). The papers propose a way to handle sub-optimality in the context of learning representations which basically refers to the overall sub-optimality of the entire hierarchical polity with respect to the task reward. And hence, the only practical different from the HIRO paper is that the proposed method considers representation learning for the goals, while HIRO was directly using the state space.


I enjoyed reading the paper. The paper is very *well* written. 

Experimental results:  The authors perform the series of experiments on various high dimensional mujoco env, and  show that the representations learned using the proposed method outperforms other methods (like VAEs, E2C etc), and can recover the controllable aspect of the agent i.e the x, y co-ordinate. This is pretty impressive result.

Some questions:

[1] Even though the results are very interesting, I'm curious as to how hard authors try to fit the VAE baseline. Did authors try using beta VAEs (or variants like InfoVAE) ?  Since the focus of the entire paper is about representation learning (as well as the focus of the conference), it is essential to make sure that baselines are strong. I would have suspected that it would have been possible to learn x,y co-ordinate in some cases while using improved version of VAE like beta VAE etc.

[2] One of the major intuitions behind sub-optimality is to learn representations that can generalize, as well as can be used for continual learning (or some variant of it!). This aspect is totally missing from the current paper. It would be interesting to show that the representations learned using the proposed method can transfer well to other scenarios or can generalize in the presence of new goals, or can be sample efficient in case of continual learning. 

I think, including these results would make the paper very strong.  (and I would be happy to increase my score!).

","The sentiment of the review is positive, as indicated by phrases like 'I enjoyed reading the paper' and 'This is pretty impressive result.' The reviewer appreciates the quality of writing and the experimental results. The sentiment score is therefore 80. The politeness of the language is also high, as the reviewer uses polite language throughout, such as 'I'm curious,' 'It would be interesting,' and 'I would be happy to increase my score.' The politeness score is 90.",80,90
"This paper presents two new ideas on leveraging program semantics to improve the current neural program synthesis approaches. The first idea uses execution based semantic information of a partial program to guide the future decoding of the remaining program. The second idea proposes using an ensembling approach to train multiple synthesizers and then select a program based on a majority vote or shortest length criterion. The ideas are evaluated in the context of the Karel synthesis domain, and the evaluation shows a significant improvement of over 13% (from 77% to 90%).

The idea of using program execution information to guide the program decoding process is quite natural and useful. There has been some recent work on using dynamic program execution in improving neural program repair approaches, but using such information for synthesis is highly non-trivial because of unknown programs and when the DSL has complex control-flow constructs such as if conditionals and while loops. This paper presents an elegant approach to handle conditionals and loops by building up custom decoding algorithms for first partially synthesizing the conditionals and then synthesizing appropriate statement bodies.

The idea of using ensembles looks relatively straightforward, but it hasn’t been used much in synthesis approaches. The evaluation shows some interesting characteristics of using different selection criterion such as shortest program or majority choice can have some impact on the final synthesized program.

The evaluation results are quite impressive on the challenging Karel domain. It’s great to see that execution and ensembling ideas lead to practical gains.

There were a few points that weren’t clear in the paper:

1. Are the synthesis models still trained on original input-output examples like Bunel et al. 2018? Or are the models now trained on new dataset comprising of (partial-inputs-->final-output) pairs obtained from the partial execution algorithm?

2. In algorithm 2, the algorithm generates bodies for if and else branches until generating the else and fi tokens respectively. It seems the two bodies are being generated independently of each other using the standard synthesizer \Tau. Is there some additional context information provided to the two synthesis calls in lines 8 and 9 so that they know to produce else and fi tokens?

3. Is there any change to the beam search? One can imagine a more sophisticated beam search with semantic information can help as well (e.g. all partial programs that lead to the same intermediate state can be grouped into 1).
","The sentiment of the review is quite positive, as the reviewer praises the novel ideas and their practical gains, particularly highlighting the significant improvement in the Karel synthesis domain. The sentiment score is therefore 80. The politeness of the language is also high, as the reviewer uses polite and constructive language throughout the review, even when pointing out areas that need clarification. The politeness score is 90.",80,90
"This paper proposes N-ball embedding for taxonomic data. An N-ball is a pair of a centroid vector and the radius from the center, which represents a word.

Major comments:

- The weakness of this paper is lack of experimental comparisons with other prominent studies. The Poincare embedding and the Lorentz model are recently proposed and show a good predictive performance in hypernymy embedding.
- WordNet concepts are actually structed in DAG. Recent studies on structure embedding can hadle DAG data. It is not clear how to extend N-ball embedding for handling DAT structures. 

- Related work is not sufficiently described.

- It is not clear why N-ball embedding is suitable for hierarchical structures.
","The sentiment of the review is slightly negative as it highlights several major weaknesses of the paper without mentioning any strengths. The reviewer points out the lack of experimental comparisons, insufficient description of related work, and unclear suitability of the proposed method for hierarchical structures. The politeness of the language is neutral; the reviewer provides constructive criticism without using rude or overly harsh language.",-40,0
"Summary:
Proposes Counterfactual Guided Policy Search (CF-GPS), which uses counterfactual inference from sampled trajectories to improve an approximate simulator that is used for policy evaluation. Counterfactual inference is formalized with structural causal models of the POMDP. The method is evaluated in partially-observed Sokoban problems. The dynamics model is assumed known, and a learned model maps observation histories to a conditional distribution on the starting state. CF-GPS outperforms model-based policy search and a ""GPS-like"" algorithm in these domains. GPS in MDPs is shown to be a particular case of CF-GPS, and a connection is also suggested between stochastic value gradient and CF-GPS.

Review:
The work is an interesting approach to a relevant problem. Related literature is covered well, and the paper is well-written in an approachable, conversational style. 

The approach is technically sound and generally presented clearly, with a few missing details. It is mainly a combination of existing tools, but the combination seems to be novel. 

The experiments show that the method is effective for these Sokoban problems. A weakness is that the setting is very ""clean"" in several ways. The dynamics and rewards are assumed known and the problem itself is deterministic, so the only thing being inferred in hindsight is the initial state. This could be done without all of the machinery of CF-GPS. I realize that the CF-GPS approach is domain-agnostic, but it would be useful to see it applied in a more general setting to get an idea of the practical difficulties. The issue of inaccurate dynamics models seems especially relevant, and is not addressed by the Sokoban experiment. It's also notable that the agent cannot affect any of the random outcomes in this problem, which I would think would make counterfactual reasoning more difficult.

Comments / Questions:
* Please expand on what ""auto-regressive uniformization"" is and how it ensures that every POMDP can be expressed as an SCM
* What is the prior p(U) for the experiments? 
* ""lotion-scale"" -> ""location-scale""

Pros:
* An interesting and well-motivated approach to an important problem
* Interesting connections to GPS in MDPs

Cons:
* Experimental domain does not ""exercise"" the approach fully; the counterfactual inference task is limited in scope and the dynamics and rewards are deterministic and assumed known
* Work may not be easily reproducible due to the large number of pieces and incomplete specification of (hyper-)parameter settings ","The sentiment of the review is generally positive, as indicated by phrases like 'interesting approach,' 'well-written,' and 'technically sound.' However, there are some criticisms regarding the experimental domain and the reproducibility of the work, which slightly temper the overall positivity. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer uses polite and constructive language throughout, such as 'please expand' and 'it would be useful to see,' without any rude or harsh comments. Thus, the politeness score is 90.",60,90
"Summary:
The authors propose a network for VQA incorporating hand-crafted modules and their hierarchy, each of which is a network for a high-level vision task. Some modules may share the same sub-modules at a different level in the module hierarchy. Each module is individually (not end-to-end) trained with a dataset containing a dedicated annotation for their high-level tasks. The proposed model shows comparable scores to the existing models.

Presentation and clarity:
The paper is well written and easy to follow and contains reasonable experiments for understanding the proposed method.

Originality and significance:
I mainly do not agree that this work generalizes NMN. Instead, I believe that this work is a special case of NMN where the modules and their hierarchy are manually defined based on the authors' intuition. Meanwhile, the proposed network architecture is static, and thus the main idea of having multiple modules in a network is not novel as other approaches using static network architectures such as [A] also facilitate multiple modules for different sub-procedures (e.g., RNN for questions and CNN for image) and sometimes share modules in multiple stages too. The main difference between this and previous works is that the modules in this work deal with high-level tasks chosen by the authors. I am not convinced that designing the modules with high-level tasks is a better choice over designing modules that are less task-specific. Rather, I see more drawbacks as the proposed method requires multiple datasets with diverse task-specific annotation. Also, the modules and their connectivity are less scalable and extendable as they are not learned.

Considering all the model and dataset complexities, the improvements over black-box models are mostly marginal. The main benefits we get from all these complexities are the interpretability. However, for many modules, the interpretability comes from indirect signals that are often not clear how to interpret for the question answering. On the other hand, the manually designed sub-tasks may cause error propagation in the network as these modules are not directly optimized for the final objective.

Some questions and comments:
I do not understand why it is necessary to have the image captioning module as it does not directly relate to the question answering. Moreover, the caption itself is generated without conditioning on the question.

[A] Yang, Zichao, et al. ""Stacked attention networks for image question answering."" CVPR 2016.


== After discussion phase
Based on the rebuttal and additional experiments that clarified and resolved my questions, I change my initial rating.","The sentiment of the review appears to be mixed but leans towards the negative side. The reviewer acknowledges that the paper is well-written and easy to follow, which is positive. However, the reviewer expresses significant concerns about the originality and significance of the work, stating that it is not a generalization of NMN and that the proposed method has several drawbacks. The reviewer also questions the necessity of certain modules and the overall benefits of the proposed approach. Therefore, the sentiment score is -30. The politeness of the language used is quite high. The reviewer provides constructive criticism and raises questions in a respectful manner, without using any harsh or rude language. Therefore, the politeness score is 80.",-30,80
"Following recent work on Hindsight Experience Replay (Andrychowicz et al. 2017), the authors extend the idea to policy gradient methods. They formally describe the goal-conditioned policy gradient setup and derive the extensions of the classical policy gradient estimators. Their key insight to deriving a computationally efficient estimator is that for many situations, only a small number of goals will be ""active"" in a single trajectory. Then, they conduct extensive experiments on a range of problems and show that their approach leads to improvements in sample efficiency for goal-conditioned tasks.

Although the technical novelty of the paper is not high (many of the estimators follow straightforwardly from previous results, however, the goal subsampling idea is a nice contribution), the paper is well written, the topic is of great interest, and the experiments are extensive and insightful. I expect that this will serve as a nice reference paper in the future, and launching point for future work. 

The only major issue I have is that there is no comparison to HER. I think it would greatly strengthen the paper to have a comparison with HER. I don't think it diminishes their contributions if HER outperforms HPG, so I hope the authors can add that.

Comments:

In Sec 6.1, it seems surprising that GCPG+B underperforms GCPG. I understand that HPG+B may underperform HPG, but usually for PG methods a baseline helps. Do you understand what's going on here?

In Sec 6.2, it would be helpful to plot the average return of the optimal policy for comparison (otherwise, it's hard to know if the performance is good or bad). Also, do you have any explanations for why HPG does poorly on the four rooms?

====

Raising my score after the authors responded to my questions and added the HER results.","The sentiment of the review is generally positive, as the reviewer acknowledges the well-written nature of the paper, the interest of the topic, and the extensive and insightful experiments. The reviewer also expects the paper to serve as a good reference in the future. However, there is a minor critique regarding the lack of comparison to HER, which is addressed in a constructive manner. The politeness of the language is high, as the reviewer provides feedback in a respectful and encouraging tone, even when pointing out areas for improvement. The reviewer also raises their score after the authors respond to their questions and add the HER results, indicating a fair and appreciative attitude.",80,90
"last time i had two comments:
1. the real data motifs did not look like what i'd expect motifs to look like. now that the authors have thresholded the real data motifs, they do look as i'd expect.
2. i'm not a fan of VAE, and believe that simpler optimization algorithms might be profitable.  i acknowledge that SCC requires additional steps; i am not comparing to SCC. rather, i'm saying given your generative model, there are many strategies one could employ to estimate the motifs.  i realize that VAE is all the rage, and is probably fine.  in my own experiments, simpler methods often work as well or better for these types of problems.  i therefore believe this would be an interesting avenue to explore in future work.","The reviewer's sentiment appears to be generally positive, as they acknowledge improvements made by the authors and suggest future avenues for exploration rather than criticizing the current work. The sentiment score is therefore 50. The language used is polite and constructive, with phrases like 'I acknowledge,' 'I realize,' and 'I therefore believe,' indicating a respectful tone. The politeness score is 80.",50,80
"Summary: the paper proposes a method for Deep Neural Networks (DNN) that identifies automatically relevant features of the set of the classes, enriching the predictions made with the visual features that contributed to that class, supporting, thus, interpretation (understanding what the model has learned) and explanation (justification of the predictions/classifications made by the model). This scheme does not rely on additional annotations, like earlier techniques do.

The contributions of this paper are relevant to, I would say, a large segment of the AI community, since interpretability and explainability of AI (XAI) is the focus of many current works in the area, and there are still many unresolved issues. I consider this paper suitable for ICLR 2019, in particular, it fits the call for papers topic “visualization or interpretation of learned representations”.

The authors also present a new dataset (am8Flower) that can be used by the community for future evaluations of explanation methods for DNN. From my point of view, this is a significant contribution, since there is a lack of datasets that can be used for evaluation.

The authors motivate properly the need for this research/study, addressing the main weakness of the two more common strategies for interpreting DNN, (1) manually inspecting visualizations of every single filter or (2) comparing the internal activations produced by a given model w.r.t. a dataset with pixel-wise annotations of possibly relevant concepts.

I would encourage the authors to write the limitations and weakness of their proposal w.r.t. similar approaches they reviewed. I am aware that the space is limited, but in p.8, section 4.3, when Table 1 is introduced and the authors confirm that their proposal has higher IoU than other methods, the authors could explain, in brief, what are the weaknesses of their method w.r.t. the other approaches analyzed.

Another clarification concerns the initialization of input parameters, such as sparsity; e.g., p.6 sparsity is initialized with 10 for all datasets, why? How has this value been selected and how sensitive is the performance regarding variations of this value?

Once again, I know that the space is limited, but I would like to be able to see some of the figures better (since this is an essential part of the paper). The additional material complements very well the paper and shows larger figures, but I think that the paper itself should be self-sufficient, and figures like Fig. 5 should be enlarged so it is easier to see some details.

Just a concern or something that I quite did not understand about one of the arguments the authors use to justify the evaluation carried out: the authors claim that they want to avoid the subjectivity introduced by humans (citing Gonzalez-Garcia et al. 2017), and prefer to avoid user studies, presenting a more objective approach in their evaluation. Ok, but then, the analysis presented in, for example, page 7, is based mainly in their interpretation of the results, a qualitative analysis of the images (we can see fur patterns, this and that, etc.). So aren’t they interpreting the results obtained as users? So after all, aren’t the visual explanations and feedback intended for users? Why should we claim that we want to avoid the subjectivity introduced by humans in the evaluation when the method proposed here is actually going to be used by users –with their inherent subjectivity? I do not mean that the evaluation carried out is not interesting per se, but it could be motivated differently, or it could be complemented later on with future user studies (that would make an interesting addition to the paper). Moreover, I also wonder whom the authors see as intended users for the proposed scheme.

Small comments:
P.1 “useful insights on the internal representations”  insights into the internal representations.
P. 2: space needed in “back-propagation methods.Third,”
P. 3: Remove “s” in verb (plural authors): “Similarly, Bach et al. (2015) decomposes the classification”  decompose or decomposed
P.3: n needed “Chattopadhyay et al. (2018) exteded”  extended
P.3: “This saliency-based protocol assume that”  protocol assumes
P.3: “highlighted by the the explanations”  remove one “the”
P. 5: “space. As as result we get”  remove one “as”
P. 5: “and compensate this change”  compensate for this change
P. 6: “In this experiment we verify”  In this experiment, we verify
P. 6: “To this end, given a set of identified features we”  To this end, given a set of identified features, we
P. 6: “Note that the OnlyConv method, makes the assumption”  remove “,” after method
P. 7: “In order to get a qualitative insight of the type of”  insight into the
P. 7: I would write siamese and persian cat with capital “S” and “P” (Siamese, Persian)
P. 7: others/ upper “Some focus on legs, covered and uncovered, while other focus on the upped body part.”  while others focus on the upper body part
P. 7: “These visualizations answers the question”  answer
P. 7:  “In this section we assess”  In this section, we
P. 7: Plural “We show these visualization for different”  these visualizations
P. 7: In “Here our method reaches a mean difference on prediction confidence”  difference in prediction …
P. 7: “This suggest that our method is able”  This suggests that
P. 8: state-of-the-art
P. 8: “has higher mean IoU”  has a higher mean IoU
Whole document: when using “i.e.” add “,” after: i.e.,

References: Some of the references in the list have very little information to be able to find it/proper academic citation, e.g. , Yosinski et al. 2015; Vedaldi and Lenc, 2015:

Jason Yosinski, Jeff Clune, Anh Mai Nguyen, Thomas J. Fuchs, and Hod Lipson. Understanding neural networks through deep visualization. 2015.

A. Vedaldi and K. Lenc. Matconvnet: Convolutional neural networks for matlab. In MM, 2015.

Ref Doersch et al.: What makes paris look like paris?  Paris
","The review is generally positive, highlighting the relevance and significance of the paper's contributions to the AI community, particularly in the area of interpretability and explainability of AI. The reviewer considers the paper suitable for ICLR 2019 and appreciates the introduction of a new dataset. However, the reviewer also provides constructive criticism and suggestions for improvement, such as discussing the limitations of the proposed method, clarifying parameter initialization, and improving figure visibility. The language used is polite and professional, with the reviewer acknowledging space limitations and expressing their concerns and suggestions in a respectful manner.",80,90
"EDIT: I thank the authors for providing all clarifications. I think this paper is a useful contribution. It will be of interest to the audience in the conference.

Summary:
This paper provides a method to jointly learn from crowdsourced worker labels and the actual data. The key claimed difference is that previous works on crowdsourced worker labels ignored the data. At a higher level, the algorithm comprises maximizing the mutual information gain between the worker labels and the output of a neural network (or more generally any ML model) on the data. 

Evaluation:
I like the idea behind the algorithm. However there are several issues on which I ask the authors to provide some clarity. I will provide a formal ""evaluation"" after that. (For the moment, please ignore the ""rating"". I will provide one after the rebuttal.) 

(1) As the authors clarified, one key aspect of the ""information intersection"" assumption is that the crowdsourced labels are statistically independent from the data when conditioned on the ground truth. How strongly does this coincide with reality? Since the work is primary empirical, is there any evidence on this front?

(2) In the abstract, introduction etc., what does it mean to say that the algorithm is an ""early algorithm""?
-- Thanks for the clarification. I would suggest using the term ""first algorithm"" in such cases. However, is this the first algorithm towards this goal? See point (3).

(3) The submitted paper misses an extremely relevant piece of literature: ""Learning From Noisy Singly-labeled Data"" (arXiv:1712.04577). This paper also aims to solve the label + features problem together. How do the results of this paper compare to that of this submission?

(4) ""Model and assumptions"" Is the i.i.d. assumption across the values of ""i""? Then does that not violate the earlier claim of accommodating correlated mistakes?

(5) Recent papers on crowdsourcing (such as Achieving budget-optimality with adaptive schemes in crowdsourcing arXiv:1602.03481 and  A Permutation-based Model for Crowd Labeling: Optimal Estimation and Robustness arXiv:1606.09632) go beyond restricting workers to have a common confusion matrix for all questions. In this respect, these are better aligned with the realistic scenario where the error in labeling may depend on the closeness to the decision boundary. How do these settings and algorithms relate to the submission?

(6) Page 5: ""Later we will show....""   Later where? Please provide a reference.

(7) Theorem 3.4, The assumption of existence of experts such that Y^S is a sufficient statistic for Y: For instance, suppose there are 10 experts who all have a 0.999 probability of correctness (assume symmetric confusion matrices) and there are 5 non-experts who have a 0.001 probability of correctness and even if we suppose all are mutually independent given the true label, then does this satisfy this sufficient statistic assumption? This appears to be a very strong assumption, but perhaps the authors have better intuition?

(8) The experiments comprise only some simulations. The main point of experiments (particularly in the absence of any theoretical results) towards bolstering the paper is to ensure that the assumptions are at least somewhat reasonable. I believe there are several datasets collected from Amazon Mechanical Turk available online? Otherwise, would it be possible to run realistic experiments on some crowdsourcing platforms?
","The sentiment of the review is generally positive, as indicated by phrases like 'I think this paper is a useful contribution' and 'I like the idea behind the algorithm.' However, the reviewer also points out several issues that need clarification, which tempers the overall positivity. Therefore, the sentiment score is 50. The politeness of the language is high, as the reviewer uses polite phrases such as 'I thank the authors,' 'Thanks for the clarification,' and 'I would suggest.' The reviewer also avoids any harsh or rude language, making the politeness score 90.",50,90
"1) Summary
This paper proposes a hierarchical reinforcement learning (HRL) method for visual motor control of humanoid agents. The method is decomposed into a high-level controller that takes in visual input and proprioceptive information, and a low-level controller (they compare may ways of doing this) that takes care of the agent’s motor control. In experiments, the proposed method is tested on a variety of RL tasks where the many low-level controllers presented in the paper are compared against each other.

2) Pros:
+ Novel high-level controller that takes in front-view visual information
+ Novel multi-policy low level controller
+ Interesting experimental section

3) Cons:
Numerical comparison to previous methods:
- The only issue I found with this paper is that there is no comparison with other methods. Even if the other methods do not take in front-view visual input, it would be nice to compare with them. Maybe visual inputs results in better high-level controller? Or even show that performance is similar would be an interesting result.

4) Comments:
Jerky transitions in switching controller:
- Due to the fact that one policy takes over after each other based on the high-level controller choice, there is a jerk artifact that shows when the policies are being changed/executed. Did you guys try to add a connection in feature space between policies rather than only passing the state of the agent? This may be able to help with that artifact that sampling noise adds to the actions. Can the authors comment on this?

Steerable controller limited rotation:
- From observing the steerable controller policy in action, it seems the policy learned a steering that is somewhat independent of what the limbs are doing. Maybe adding a mechanism where the leg motion intensity depends more on the direction of movement could be a way to fix the issue where this policy moves to fast for the turning it tries to do. Maybe an energy based objective to minimize the torques or something in that line.

4) Conclusion:
To the best of my knowledge, this paper proposes a novel interesting method for modeling humanoid motor skills with front-view visual input. However, as mentioned above, the paper lacks of numerical comparisons with other methods, and only compares against its own variations which is more of an ablation study. I am willing to increase my review score if the authors successfully address the concerns mentioned above","The sentiment of the review is generally positive, as indicated by the pros listed and the overall tone of the review. The reviewer acknowledges the novelty and interesting aspects of the paper, although they do point out some areas for improvement, particularly the lack of numerical comparisons with other methods. This constructive criticism is framed in a way that suggests the reviewer is open to increasing their score if the issues are addressed. The politeness of the language is high; the reviewer uses polite phrases such as 'Did you guys try,' 'Can the authors comment on this,' and 'I am willing to increase my review score,' which indicate a respectful and constructive approach.",70,90
"PAPER SUMMARY:

This paper proposes a new POVI method for posterior inference in BNN. Unlike existing POVI techniques that optimize particles in the weight space which often yields sub-optimal results on BNN due to its over-parameterized nature, the new POVI method aims to maintain and update particles  directly on the space of regression functions to overcome this sub-optimal issue.

NOVELTY & SIGNIFICANCE:

In general, I am inclined to think that this paper has made an important contribution with very promising results but I still have doubts in the proposed solution technique (as detailed below) and am not able to converge to a final rating at this point.

TECHNICAL SOUNDNESS:

The authors claim that the new POVI technique operates directly on the function-space posterior to sidestep the over-parameterized issue of BNN but ultimately each function particle is still identified by a weight particle (as detailed in Eq. (2)). In terms of high-level ideas, I am not sure I understand the implied fundamental differences between this work and SVGD and how significant is it.

On the technical level, the key difference between the proposed work and SVGD seems to be the particle update equation in (2): The gradient flow is multiplied with the derivative of the BNN evaluated at the corresponding weight particle (in SVGD, the gradient flow was used alone). The authors then mentioned that this update rule results from minimizing the difference between f(X, theta) and f(X, theta) + \epsilon * v(f(., theta))(X). I do not follow this step -- please elaborate.

The theoretical justification that follows Eq. (3) is somewhat incoherent: What is \Epsilon(q(f(x)))? This has not been defined before or anywhere in the main text. Furthermore, the paragraph that follows the theoretical justification implies the computation of the gradient flow in (3) involves the likelihood term -- why is that?

In Algorithm 1, why do we sample from both the training set and some measure \mu? I am sure there must be a reason for this but I could not find it anywhere except for a short statement that ""for convenience, we choose \mu in such a way that samples from \mu always consists a mini-batch from X"". Please elaborate.

Will the proposed POVI converge?

CLARITY:

I think this paper has clarity issue with the technical exposition. The explanation tends to be very limited and even appear coherent at important points. For example, see
my 3rd point above. 
","The sentiment score is derived from the initial positive remarks about the paper's contribution and promising results, but it is tempered by the reviewer's doubts and inability to converge to a final rating. This mixed sentiment results in a score of 20. The politeness score is high because the reviewer uses polite language throughout, such as 'please elaborate' and 'I am inclined to think,' which indicates a respectful and constructive tone. Therefore, the politeness score is 80.",20,80
"The main contribution of the paper are methods for propagating approximate uncertainty in neural networks through max and argmax layers. The proposed methods are explained well. The paper is clearly written. The methods are validated in small scale experiments and seem to work well.

The proposed approach is not much more accurate than Monte Carlo dropout, but is more computationally efficient. The standard way of efficiently predicting at test time with a dropout-trained network is to simply scale the weights. Could the authors try calibration on networks of this type and compare against the proposed method with calibration? (i.e. scale the predicted logits of the standard test-time network to be on the same scale as the logits under your approach)","The review starts with positive remarks about the paper's contributions, clarity, and validation. It acknowledges that the proposed methods are well-explained and work well in small-scale experiments. However, it also points out that the proposed approach is not significantly more accurate than Monte Carlo dropout, although it is more computationally efficient. The reviewer suggests an additional experiment for comparison, which is framed as a constructive recommendation rather than a criticism. The language used is polite and professional throughout.",70,90
"Revision: The authors addressed most of my concerns and clearly put in effort to improve the paper. The paper explains the central idea better, is more precise in terminology in general, and the additional ablation gives more insight into the relative importance of the advantage weighting. I still think that the results are a bit limited in scope but the idea is interesting and seems to work for the tasks in the paper. I adjusted my score to reflect this.

Summary:
The paper proposes an HRL system in which the mutual information of the latent (option) variable and the state-action pairs is approximately maximized. To approximate the mutual information term, samples are reweighted based on their estimated advantage. TD3 is used to optimize the modules of the system. The system is evaluated on continuous control task from OpenAI gym and rllab.

For the most part, the paper is well-written and it provides a good overview of related work and relevant terminology. The experiments seem sound even though the results are not that impressive. The extra analysis of the option space and temporal distribution is interesting. 

Some parts of the theoretical justification for the method are not entirely clear to me and would benefit from some clarification. Most importantly, it is not clear to me why the policy in Equation 7 is considered to be optimal. Given some value or advantage function, the optimal policy would be the one that picks the action that maximizes it. The authors refer to earlier work in which similar equations are used, but in those papers this is typically in the context of some entropy maximizing penalty or KL constraint. A temperature parameter would also influence the exploration-exploitation trade-off in this ‘optimal’ policy. I understand that the rough intuition is to take actions with higher advantage more often while still being stochastic and exploring but the motivation could be more precise given that most of the subsequent arguments are built on top of it. However, this is not the policy that is used to generate behavior. In short, the paper is clear enough about how the method is constructed but it is not very clear to me *why* the mutual information should be optimized with respect to this 'optimal' policy instead of the actual policy one is generating trajectories from.

HRL is an interesting area of research with the potential to learn complicated behaviors. However, it is currently not clear how to evaluate the importance/usefulness of hierarchical RL systems directly and the tasks in the paper are still solvable by standard systems. That said, the occasional increase in sample efficiency over plain TD3 looks promising. It is somewhat disappointing that the number of beneficial option is generally so low. To get more insight in the methods it would have been nice to see a more systematic ablation of related methods with different mutual information pairings (action or state only) and without the advantage weighting. Could it be that the number of options has to remain limited because there is no parameter sharing between them? It would be interesting to see results on more challenging control problems where the hypothesized multi-modal advantage structure is more likely to be present.

All in all I think that this is an interesting paper but the foundations of the theoretical motivation need a bit more clarification. In addition, experiments on more challenging problems and a more systematic comparison with similar models would make this a much stronger paper.

Minor issues/typos:
- Contributions 2 and 3 have a lot of overlap.
- The ‘o’ in Equation 2 should not be bold font. 
- Appendix A. Shouldn’t there be summations over ‘o’ in the entropy definitions?


","The sentiment of the review is generally positive, as the reviewer acknowledges the effort put into addressing concerns and improving the paper. The reviewer appreciates the improvements in clarity and precision, and finds the idea interesting, although they note some limitations in the results and theoretical justification. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, offering specific suggestions for improvement without being harsh or dismissive. Thus, the politeness score is 90.",60,90
"In this paper, the authors presented a large experimental study of curiosity-driven reinforcement learning on various tasks. In the experimental studies, the authors also compared several feature space embedding methods, including identical mapping (pixels), random embedding, variational autoencoders and inverse dynamics features. The authors found that in many of the tasks, learning based on intrinsic rewards could generate good performance on extrinsic rewards, when the intrinsic rewards and extrinsic rewards are correlated. The authors also found that random features embedding, somewhat surprisingly, performs well in the tasks.

Overall, the paper is well written with clarity. Experimental setup is easy to understand. The authors provided code, which could help other researchers reproduce their result.

Weaknesses: 

1) as an experimental study, it would be valuable to compare the performance of curiosity-based learning versus learning based on well-defined extrinsic rewards. The author is correct that in many tasks, well-behaved extrinsic rewards are hard to find. But for problems with well-defined extrinsic rewards, such a comparison could help readers understand the relative performance of curiosity-based learning and/or how much headroom there exists to improve the current methods.

2) it is surprising that random features perform so well in the experiments. The authors did provide literature in classification that had similar findings, but it would be beneficial for the authors to explore reasons that random features perform well in reinforcement learning.","The sentiment of the review is generally positive, as indicated by phrases such as 'the paper is well written with clarity' and 'experimental setup is easy to understand.' The reviewer also appreciates the provision of code for reproducibility. However, the reviewer does point out some weaknesses, which are framed as constructive suggestions rather than criticisms. The politeness of the language is high, as the reviewer uses polite and respectful language throughout, such as 'it would be valuable' and 'it would be beneficial.' The reviewer avoids any harsh or negative language.",80,90
"The paper proposes to use a differentiable drawing environment to synthesize images and provides information about some initial experiments. 

Not yet great about this paper: 
 - the paper feels premature: There is a nice idea, but restricting the drawing environment to be 
 - Some of the choices in the paper are a bit surprising, e.g. the lines in the drawing method are restricted to be at most 16 points long. If you look at real drawing data (e.g. the quickdraw dataset: https://quickdraw.withgoogle.com/data) you will find that users draw much longer lines typically. 
EDIT: the new version of the paper is much better but still feels like a bit incomplete. I personally would prefer a more complete evaluation and discussion of the proposed method. 
 - the entire evaluation of this paper is purely qualitative (and that is not quite very convincing either). I feel it would be important for this paper to add some quantitative measure of quality. E.g. train an MNIST recognizer synthesized data and compare that to a recognizer trained on the original MNIST data. 
 - a proper discussion of how the proposed environment is different from the environment proposed by Ganin et al (Deepmind's SPIRAL) 

Minor comments: 
 - abstract: why is it like ""dreaming"" -> I do agree with the rest of that statement, but I don't see the connection to dreaming
 - abstract: ""upper agent"" -> is entirely unclear here. 
 - abstract: the footnote at the end of the abstract is at a strange location
 - introduction: and could thus -> and can thus 
 - introduction: second paragraph - it would be good to add some citations to this paragraph. 
 - resulted image-> resulting image
 - the sentence: ""We can generate....data is cheap"" - is quite unclear to me at this time. Most of it becomes clearer later in the paper - but I feel it would be good to put this into proper context here (or not mention it)
 - we obtained -> we obtain
 - called a generator -> call a generator 
 - the entire last paragraph on the first page is completely unclear to me when reading it here. 
 - equations 1, 2: it's unclear whether coordinates are absolute or relative coordinates. 
- fig 1: it's very confusing that the generator, that is described first is represented at the right. 
 - sec 3.2 - first line: wrong figure reference - you refer to fig 2 - but probably mean fig 1
 - page 3 bottom: by appending the encoded color and radius data we have a feature with shape 64x64xn -> I don't quite see how this is true. The image was 64x64 -> and I don't quite understand why you have a color/radius for each pixel. 
 - sec 3.3 - it seem sthat there is a partial sentence missing 
 - sec 3.4 - is it relevant to the rest of the paper that the web application exists (and how it was implemented). 
 - fig 2 / fig 3: these figures are very hard to read. Maybe inverting the images would help. Also fig 3 has very little value.  ","The sentiment of the review is mixed but leans towards the negative side. The reviewer acknowledges that there is a 'nice idea' but repeatedly mentions that the paper feels 'premature,' 'incomplete,' and 'not quite very convincing.' The sentiment score is therefore set at -40. The politeness of the language is generally neutral to slightly polite. The reviewer uses phrases like 'I personally would prefer' and 'it would be important,' which are polite, but also includes more direct and critical comments. The politeness score is set at 20.",-40,20
"This paper presents a dialogue response generation model based on the framework of adversarial autoencoder. Specifically, the proposed model uses an autoencoder to encode and decode a response in a dialogue, conditioning on the context of the dialogue. The RNN encoded context is used as the prior of the latent variable in the autoencoder, and the whole dialogue (context + response) is used to infer the posterior of the latent variable. The inference is done by the adversarial training to match the prior and the posterior of the latent variable. Besides constructing the prior with a single Gaussian, the variant of the proposed model is also proposed where the prior is constructed with a Gaussian mixture model.

My comments are as follows:

1. The paper is well-written and easy to follow.

2. The experiments seem quite strong and the compared models are properly selected. I'm not an expert in the specific area of the dialogue generation. But to me, the results seem convincing to me. 

3. The usage of the Wasserstein distance in the proposed model does not make sense to me. Both the adversarial training in AAE and minimising the Wasserstein distance are able to match the prior and posterior of the latent variable. If the former is used in the proposed model, then how is the Wasserstein distance used at the same time? I also checked Algorithm 1 and did not find how the Wasserstein distance comes in. This is the first question that needs the authors to clarify.

4. To me, the significance of this paper mainly goes to combining several existing frameworks and tricks into the specific area of dialogue generation. Although the empirical results show the proposed model outperforms several existing models, my concern is still on the originality of the paper. Specifically, one of the main contributions goes to using the Gaussian mixture to construct the prior, but this is not a whole new idea in VAE or GAN, nor using the Gumbel trick. 

5. It is good to see that the authors showed some comparisons between DialogWAE and DialogWAE-GMP, letting us see GMP does help the performance. But a minor concern is that it seems hard to identify which part makes DialogWAE get superior performance than others. Are all the models running with the same experiment settings including the implementation of the RNNs?","The sentiment of the review is generally positive, as indicated by the comments that the paper is well-written, easy to follow, and the experiments are strong and convincing. However, there are some concerns about the originality and clarity of certain aspects, which slightly temper the overall positive sentiment. Therefore, the sentiment score is 60. The politeness of the language is very high, as the reviewer uses polite phrases such as 'needs the authors to clarify' and 'a minor concern,' and provides constructive feedback without being harsh or rude. Therefore, the politeness score is 90.",60,90
"The paper introduces RCPO, a model-free deep RL algorithm for learning optimal policies that satisfy some per-state constraint on expectation. The derivation of the algorithm is quite straightforward, starts from the definition of constrained optimization problem, and proceed by forming and optimizing the Lagrangian. Additionally, a value function for the constraint is learned. The algorithm is only compared to a baseline optimizing the Lagrangian directly using Monte-Carlo sampling.

The paper has two major problems. First, while the derivation of the method makes intuitively sense, it is supported by vaguely stated theorems, which mixes rigorous guarantees with practical approximations. For example, Equation 4 assumes strong duality. How would the result change if weak duality was used instead? The main result in Theorem 1 makes the assumption that dual variable is constant with respect the policy, which might be true in practice, but it is not obvious how the approximation affects the theory. Further, instead of simply referring to prior convergence results, I would strongly suggest including the exact prior theorems and assumptions in the appendix.

The second problem is the empirical validation, which is incomplete and misleading. Constrained policy optimization is not a new topic (e.g. work by Achiam et al.), so it is important to compare to the prior works. It is stated in the paper that the prior methods cannot be used to handle mean value constraints. However, it would be important to include experiments that can be solved with prior methods too, for example the experiments in Achiam at al. for proper comparison. The results in Table 2 are confusing: what makes the bolded results better than the others? If the criterion is highest return and torque < 25%, then \lambda=0.1 should be chosen for Hopper-v2. Also, The results seem to have high variance, and judging based on Table 2 and Figure 3, it is not obvious how well RCPO actually works.

To summarize, while the topic is undoubtedly important, the paper would need be improved in terms of better differentiating the theory from practice, and by including a rigorous comparison to prior work.

Minor points:
- What is exactly the difference between discounted sum constraint and mean value constraint?
- Could consider use colors in Table 1.
- Section 4.1.: What does “... enables training using a finite number of samples” exactly mean in this case?
- Table 2: The grid for \lambda is too sparse. 
- Proof of Theorem 1: What does it mean \theta to be stable?
- Proof of Theorem 2: “Theorem C” -> “Theorem 1”
","The sentiment of the review is mixed, leaning towards negative. The reviewer acknowledges the importance of the topic but highlights two major problems with the paper: the theoretical derivation and the empirical validation. The sentiment score is -40 because the review points out significant flaws that need to be addressed, although it does not dismiss the paper entirely. The politeness score is 20 because the language used is generally professional and constructive, but it is also direct and critical, which slightly reduces the politeness score.",-40,20
"Revision post-discussion: The paper's notation and model has been clarified, and my concerns about the paper have been addressed. Proposing a latent tree structure on the latent space of generative models is a strong contribution, the model performs well and seems to find meaningful and interpretable structure in the latent space.


The paper proposes a latent tree superstructure for the latent space of VAE’s. The idea itself is novel and interesting, and could have major impact in learning structured manifolds.

The overall presentation of the method is direct but slightly confusing. It seems that the zb grouping corresponds to different dimensions of the full z_i-vector of a single data point x_i. This should be made more explicit. 

The method itself has three levels of groupings: the zb’s, the conditioned variables Yb, and the connections between the Y’s. The method is also called a  Bayesian Network, but the paper seems to avoid defining it as a BN. I wonder if the method could be presented in a simpler form, if all the structure is necessary, and if the method could be defined directly as a BN. For instance, why do the Y’s have to have a hierarchical tree structure, wouldn’t a “flat” grouping into zb's be sufficient? 

In eq 2 the p(z) is defined as a mixture of Y-conditioned Gaussians, while in eq 4 its defined in the conventional encoder form N(z ; mu_x, sigma_x). These forms don’t seem to be compatible with each other. The term H seems to be entropy, but its not explained. It can’t be computed if we use the eq 2 definition of p(z). The interplay between these two structures is unclear. Furthermore, in fig 1 the tree is showed as a network (no arrows), while in fig 2 its a tree. I can’t find the definition for the dependencies P(Y | Y’), are these simply conditional density tables, or are they implicit? I also can’t see how are the \Sigma_{yb} defined. Are they of full rank? What is their dimension?

The inference sections are well motivated and efficient techniques are used. 

The synthetic experiment has 4 dimensional “z”, but the “W” matrix is 10x2, these do not match. What is the connection between Y_1 and Y_2 (in fig4 there is a dependency between)? Why is the dependency undirected if the model is a tree? The fig4b does not show ground truth to assess how well the model fits. The experiment should also include comparisons to the mentioned earlier works, and show how they perform. Why is there an arrow from the green scatter to the z3/z4? The main problem of the synthetic example is that it does not demonstrate why the tree structure learning is useful. The experiment should highlight a case where there is a natural latent tree structure corresponding to some realistic phenomena in real datasets.

The section 4.3. shows that the proposed method does find better representations of the MNIST than VAE, but does not mention that there are numerous extended VAE methods (and others) that would perform better than the LTVAE here. Those should be at least acknowledged, and preferably compared to.

The main results of the paper are very good with great performance in clustering, and the facets and clusters look great. The system has clearly learnt meaningful latent structures.

There are no learning curves or running time analyses. One would expect the proposed method to be slow with multiple levels of inference (tree structure, tree parameters, AE networks), and this should be discussed. How large datasets can it handle?

Overall the paper proposes a BN-style structure on VAE latent space with great performance, but somewhat incomplete experimental section, and some presentation issues.","The sentiment of the review is generally positive, as the reviewer acknowledges the strong contribution of the paper and its potential impact. However, the reviewer also points out several areas of confusion and suggests improvements, which slightly tempers the overall positivity. Therefore, the sentiment score is 60. The language used in the review is polite and constructive, with the reviewer providing specific feedback and suggestions without being harsh or dismissive. Thus, the politeness score is 80.",60,80
"In this submission, the authors present a variational smoothing interpretation of the data noising approach presented in (Xie et al., 2017). Although the theoretical coverage of the problem gives interesting insights. However, a comparison to related work w.r.t. alternative regularization approaches is missing. Similarly, the perplexity values reported in the experimental results on Penn Treebank are far away from state-of-the-art results published by many competitors on this task, e.g. see the current state-of-the-art results on Penn Treebank by (Yang et al., 2017, https://arxiv.org/pdf/1703.02573.pdf and references therein). It is bad practice to ignore existing work completely like this. The interesting question here would be, inhowfar the presented smoothing/regularization methods are complementary to existing approaches, and if the presented methods do provide improvements on top of these.

Finally, the mode of language modeling evaluation presented here, without considering an actual language or speech processing task, provides limited insight w.r.t. its utility in actual applications. Moreover, the very limited size of the language modeling tasks chosen here is highly advantageous for smoothing/regularization approaches. It remains totally unclear, how the presented approaches would perform on more realistically sized tasks and within actual applications.
","The sentiment of the review is generally negative, as it highlights several significant shortcomings in the submission, such as the lack of comparison to related work, subpar experimental results, and limited applicability of the evaluation. The reviewer uses phrases like 'far away from state-of-the-art results,' 'bad practice to ignore existing work,' and 'limited insight,' which indicate a critical stance. Therefore, the sentiment score is -70. The politeness of the language is relatively neutral to slightly negative. While the reviewer does not use overtly rude language, the tone is quite direct and critical, especially with phrases like 'It is bad practice to ignore existing work completely like this.' Thus, the politeness score is -20.",-70,-20
"This paper presents an improvement on the local/derivative-free learning algorithm equilibrium propagation. Specifically, it trains a feedforward network to initialize the iterative optimization process in equilibrium prop, leading to greater stability and computational efficiency, and providing a network that can later be used for fast feedforward predictions on test data. Non-local gradient terms are dropped when training the feedforward network, so that the entire system still doesn't require backprop. There is a neat theoretical result showing that, in the neighborhood of the optimum, the dropped non-local gradient terms will be correlated with the retained gradient terms.

My biggest concern with this paper is the lack of significant literature review, and that it is not placed in the context of previous work. There are only 12 references, 5 of which come from a single lab, and almost all of which are to extremely recent papers. Before acceptance, I would ask the authors to perform a literature search, update their paper to include citations to and discussion of previous work, and better motivate the novelty of their paper relative to previous work. Luckily, this is a concern that is addressable during the rebuttal process! If the authors perform a literature search, and update their paper appropriately, I will raise my score as high as 7.

Here are a few related topic areas which are currently not discussed in the paper. *I am including these as a starting point only! It is your job to do a careful literature search. I am completely sure there are obvious connections I'm missing, but these should provide some entry points into the citation web.*
- The ""method of auxiliary coordinates"" introduces soft (often quadratic) couplings between post- and pre- activations in adjacent layers which, like your distributed quadratic penalty, eliminate backprop across the couplings. I believe researchers have also done similar things with augmented Lagrangian methods. A similar layer-local quadratic penalty also appears in ladder networks.
- Positive/negative phase (clamped / unclamped phase) training is ubiquitous in energy based models. Note though that it isn't used in classical Hopfield networks. You might want to include references to other work in energy based models for both this and other reasons. e.g., there may be some similarities between this approach and continuous-valued Boltzmann machines?
- In addition to feedback alignment, there are other approaches to training deep neural networks without standard backprop. examples include: synthetic gradients, meta-learned local update rules, direct feedback alignment, deep Boltzmann machines, ...
- There is extensive literature on biologically plausible learning rules -- it is a field of study in its own right. As the paper is motivated in terms of biological plausibility, it would be good to include more general context on the different approaches taken to biological plausibility.

More detailed comments follow:

Thank you for including the glossary of symbols!

""Continuous Hopfield Network"" use lowercase for this (unless introducing acronym)

""is the set non-input"" -> ""is the set of non-input""

""$\alpha = ...$ ... $\alpha_j \subset ...$"" I could not make sense of the set notation here.

would recommend using something other than rho for nonlinearity. rho is rarely used as a function, so the prior of many readers will be to interpret this as a scalar. phi( ) or f( ) or h( ) are often used as NN nonlinearities.

inline equation after ""clamping factor"" -- believe this should just be C, rather than \partial C / \partial s.
Move definition of \mathcal O up to where the symbol is first used.

text before eq. 7 -- why train to approximate s- rather than s+? It seems like s+ would lead to higher accuracy when this is eventually used for inference.

eq. 10 -- doesn't the regularization term also decrease the expressivity of the Hopfield network? e.g. it can no longer engage in ""explaining away"" or enforce top-down consistency, both of which are powerful positive attributes of iterative estimation procedures.

notation nit: it's confusing to use a dot to indicate matrix multiplication. It is commonly used in ML to indicate an inner product between two vectors of the same shape/orientation. Typically matrix multiplication is implied whenever an operator isn't specified (eg x w_1 is matrix multiplication).

eq. 12 -- is f' supposed to be h'? And wasn't the nonlinearity earlier introduced as rho? Should settle on one symbol for the nonlinearity.

This result is very cool. It only holds in the neighborhood of the optimum though. At initialization, I believe the expected correlation is zero by symmetry arguments (eg, d L_2 / d s_2 is equally likely to have either sign). Should include an explicit discussion of when this relationship is expected to hold.

""proportional to"" -> ""correlated with"" (it's not proportional to)

sec. 3 -- describe nonlinearity as ""hard sigmoid""

beta is drawn from uniform distribution including negative numbers? beta was earlier defined to be positive only.

Figure 2 -- how does the final achieved test error change with the number of negative-phase steps? ie, is the final classification test error better even for init eq prop in the bottom row than it is in the top?

The idea of initializing an iterative settling process with a forward pass goes back much farther than this. A couple contexts being deep Boltzmann machines, and the use of variational inference to initialize Monte Carlo chains

sect 4.3 -- ""the the"" -> ""to the""","The sentiment of the review is generally positive, as the reviewer acknowledges the improvements and theoretical contributions of the paper. However, the reviewer also expresses significant concerns about the lack of a comprehensive literature review, which is a critical aspect of the paper. The reviewer provides constructive feedback and specific recommendations for improvement, indicating a willingness to raise their score if the authors address these concerns. The politeness of the language is high, as the reviewer uses polite and encouraging language, offers detailed suggestions, and acknowledges the potential for the authors to improve their work during the rebuttal process.",50,90
"The authors propose a number of methods to identify individual important neurons in a machine translation system. The crucial assumption, drawn from the computer vision literature, is that important neurons are going to be correlated across related models (e.g. models that are trained on different subsets of the data). This hypothesis is validated to some extent: erasing the neurons that scored highly on these measures reduced BLEU score substantially. However, it turns out that most of the activation of the important neurons can be explained using sentence position. Supervised classification experiments on the important neurons revealed neurons that tracked properties such as the span of parentheses or word classes (e.g., auxiliary verbs, plural nouns, etc).

Strengths:
* The paper is very well written and provides solid intuitions for the methods proposed.
* The methods seem promising, and the degree of localist representation is striking.
* The methods may be able to address the question of *how* localist the representations are (though no numerical measure of localism is proposed).
* There is a correlation between the neuron importance metrics proposed in the paper and the effect on BLEU score of erasing those neurons from the network (of course, it’s not clear what particular linguistic properties are affected by this erasure - the decrease BLEU may reflect inability to track specific word tokens more than any higher-level linguistic property).

Weaknesses:
* It wasn't clear to me why the neurons that track particular properties (e.g., being inside a parentheses) couldn't be identified using a supervised classifier to begin with, without first identifying ""important"" neurons using the unsupervised methods proposed in the paper. The unsupervised methods do show their strength in the more exploratory visualization-based analyses -- as the authors point out (bottom of p. 6), the neuron that activates on numbers but only at the beginning of the sentence does not correspond to a plausible a-priori hypothesis. Still, most of the insight in the paper seems to be derived from the supervised experiments.
* The particular linguistic properties that are being investigated in the classification experiments are fairly limited. Are there neurons that track syntactic dependencies, for example?
* I wasn't sure how the GMMs (Gaussian mixture models) for predicting linguistic properties from neuron activations were set up.
* It's nice to see that individual neurons function as knobs that can change the gender or tense of the output (with varying accuracy). At the same time, I was unable to follow the authors' argument that this technique could be used to reduce gender bias in MT.
* I wasn't sure what insight was gained from the SVCCA analyses -- this method seems to be a bit of a distraction given the general focus on localist vs. distributed representation. In general, I didn’t come away with an understanding of the pros and cons of each of the methods.","The sentiment of the review is generally positive, as evidenced by the praise for the paper's writing quality, the promise of the methods, and the striking degree of localist representation. However, there are also several critical points raised, which slightly temper the overall positivity. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer uses polite and constructive language throughout, even when pointing out weaknesses. The reviewer provides specific feedback and suggestions without being harsh or dismissive, leading to a politeness score of 90.",60,90
"In this paper, the authors studied zeroth order sign SGD. Sign SGD is commonly used in adversarial example generation. Compared to sign SGD, zeroth-order sign SGD does not require the knowledge of the magnitude of the gradient, which makes it suitable to optimize black-box systems. The authors studied the convergence rate of zeroth-order sign SGD, and showed that under common assumptions, zero-order sign SGD achieves O(sqrt(d/T)) convergence rate, which is slower than sign SGD by a factor of sqrt(d). However, sign SGD requires an unrealisitcally large mini-batch size, which zeroth-order sign SGD does not. The authors demonstrated the performance of zeroth-order sign SGD in numerical experiments.

Overall, this is a well written paper. The convergence property of the zeroth-order sign SGD is sufficiently studied. The proposal seems to be useful in real world tasks.

Weaknesses: 
1) out of curiosity, can we improve the convergence rate of the zeroth-order sign SGD if we assume the mini-batch size is of order O(T)? This could help us better compare zeroth-order sign SGD and sign SGD.
2) Figure 2 is too small to be legible. Also, it seems that the adversarial examples generated by zeroth-order sign SGD have higher distortion than those found by zeroth-order SGD on CIFAR-10 dataset. Is it true? If so, it would be beneficial to have a qualitative explanation of such behavior.","The sentiment of the review is positive, as indicated by phrases like 'well written paper' and 'sufficiently studied.' The reviewer acknowledges the usefulness of the proposal in real-world tasks. The politeness of the language is also high, as the reviewer uses polite phrases such as 'out of curiosity' and 'it would be beneficial,' which suggest a respectful and constructive tone. The reviewer provides specific recommendations without being overly critical or harsh.",80,90
"This is a reasonable paper based on a simple intuition. The authors have noticed that some of the state of the art methods (they use Li et al - ICML18 as the main reference) are using only some simple normalization for improving the transfer learning and as such they propose preserving the outer layer output of the target network and aligning it with the one of the source network. On top of that they also propose modeling the difference of feature maps considering an attention mechanism obtain through supervised learning. 

The idea in itself is interesting and valuable. However, I have had some difficulty in understanding precisely how the ""behavior"" is really regularized. While I understand what is depicted in Figure 1 I'm not completely sure this really means that the network behavior is regularized rather than simply correlating the two outputs. In the evaluation, the authors present in Figure 4 some qualitative examples but I would have expected to see some quantitative evaluation of this. I would have liked to see experiments on some larger datasets that are commonly used in computer vision (e.g., Caltech 256 is rather old even if it has been used in Li et al.). The quantitative results in Table 1 and 2 indicate some slight improvement but I'm not completely convinced that this is really significant in the end. The results in Figure 4 tend to show that with the attention mechanism there is a central bias and most of the results tend to be concentrated on the center of the image (in this case the result might also be correct but the examples presented are not too eloquent). 
","The sentiment of the review is moderately positive. The reviewer acknowledges that the paper is reasonable and the idea is interesting and valuable, which indicates a positive sentiment. However, the reviewer also points out several areas of concern and suggests improvements, which tempers the overall positivity. Therefore, the sentiment score is 30. The politeness of the language is high. The reviewer uses polite language throughout, such as 'I would have liked to see' and 'I'm not completely convinced,' which indicates a respectful and constructive tone. Therefore, the politeness score is 80.",30,80
"This paper introduced a new stochastic layer termed variance layer for Bayesian deep learning, where the posterior on weight is a zero-mean symmetric distribution (e.g., Gaussian, Bernoulli, Uniform). The paper showed that under 3 different prior distributions, the Gaussian Dropout layer can converge to variance layer. Experiments verified that it can achieve similar accuracies as conventional binary dropout in image classification and reinforcement learning tasks, is more robust to adversarial attacks, and can be used to sparsify deep models.

Pros:
(1)	Proposed a new type of stochastic layer (variance layer)
(2)	Competitive performance on a variety of tasks: image classification, robustness to adversarial attacks, reinforcement learning, model compression
(3)	Theoretically grounded algorithm

Cons:
(1)	My main concern is verification. Most of the comparisons are between variance layer (zero-mean) and conventional binary dropout, while the main argument of the paper is that it’s better to set Gaussian posterior’s mean to zero. So in all the experiments the paper should compare zero-mean variance layer against variational dropout (neuron-wise Eq. 14) and sparse variational dropout (additive Eq. 14), where the mean isn’t zero.
(2)	The paper applies variance layers to some specific layers. Are there any guidelines to select which layers should be variance layers?

Some minor issues:
(1)	Page 4, equations of Gaussian/Bernoulli/Uniform variance layer, they should be w_ij=…, instead of q(w_ij)= …
(2)	What’s the prior distribution used in the experiment of Table 1?

","The sentiment of the review is generally positive, as it highlights several strengths of the paper, such as the introduction of a new stochastic layer, competitive performance on various tasks, and a theoretically grounded algorithm. However, it also points out some significant concerns regarding verification and guidelines for applying the variance layers. Therefore, the sentiment score is 60. The language used in the review is polite and constructive, offering specific recommendations for improvement without being harsh or dismissive. Thus, the politeness score is 80.",60,80
"This paper studied learning unsupervised node embeddings by considering the structural properties of networks. Experimental results on a few data sets prove the effective of the proposed approaches over existing state-of-the-art approaches for unsupervised node embeddings. 

Strength:
- important problem and interesting idea
- the proposed approach seems to be effective according to the experiments
Weakness:
- some parts of the paper are quite unclear
- the complexity of the proposed algorithm seems to be very high
- the data sets used in the experiments are very small

Details:
-In the introduction, ""it is in general impossible to find an embedding in R^d such that ..."", why do we have to make v and v'(and u, and u') far from each other?
- In Equation (2), How is P_ij defined exactly, are they parameters? I am quite confused about this part
- In Equation (6), the posterior distribution should be P(X|G) since X is the latent variable to be inferred, right？
- In Table 2 and 3, how are the degree and block information leveraged into the model?
","The sentiment of the review is moderately positive. The reviewer acknowledges the importance of the problem and the effectiveness of the proposed approach, which indicates a positive sentiment. However, the reviewer also points out several weaknesses and areas of confusion, which tempers the overall positivity. Therefore, the sentiment score is 30. The politeness of the language is quite high. The reviewer uses polite language, such as 'seems to be effective' and 'I am quite confused,' which indicates a respectful tone. Therefore, the politeness score is 80.",30,80
"The authors study the problem of when the linear interpolant between two random variables follows the same distribution. This is related to the prior distribution of an implicit generative model. In the paper, the authors show that the Cauchy distribution has such a property, however due to the heavy-tails is not particularly useful. In addition, they propose a non-linear interpolation that naturally has this property.

Technically the paper in my opinion is solid. Also, the paper is ok-written, but I think it needs improvements (see comments).

Comments:

#1) In my opinion the motivation is not very clear and should be improved. In the paper is mentioned that the goal of shortest path interpolation is to get smooth transformations. So, in principle, I am really skeptical when the linear interpolant is utilized as the shortest path. Even then, what is the actual benefit of having the property that the linear interpolants follow the same distribution as the prior? How this is related to smoother transformations? What I understand is that, if we interpolate between several random samples, we will get less samples near the origin, and additionally, these samples will follow the prior? But how this induces smoothness in the overall transformation? I think this should be explained properly in the text i.e. why is it interesting to solve the proposed problem.

#2) From Observation 2.2. we should realize that the distribution matching property holds if the distribution has infinite mean? I think that this is implicitly mentioned in Section 2.2. paragraph 1, but I believe that it should be explicitly stated.

#3) Fig.1 does not show something interesting, and if it does it is not explained. In Fig. 2 I think that interpolations between the same images should be provided such that to have a direct comparison. Also, in Fig. 3 the norm of Z can be shown in order to be clear that the Cauchy distribution has the desired property. 

#4) Section 2.2. paragraph 6, first sentence. Here it is stated that the distribution ""must be trivial or heavy-tailed"". This refers only to the Cauchy distribution? Since earlier the condition was the infinite mean. How these are related? Needs clarification in the text.

#4) In Figure 4, I believe that the norms of the interpolants should be presented as well, such that to show if the desired property is true. Also in Figure 5, what we should see? What are the improvements when using the proposed non-linear interpolation?


Minor comments:

#1) Section 1.2. paragraph 2. For each trained model the latent space usually has different structure e.g. different untrained regions. So I believe that interpolations is not the proper way to compare different models.

#2) Section 1.3 paragraph 1, in my opinion the term ""pathological"" should be explained precisely here. So it makes clear to the reader what he should expect.

#3) Section 2.2. paragraph 2. The coordinate-wise implies that some Z_i are near zero and some others significantly larger? 

In generally, I like the presented analysis. However, I do not fully understand the motivation. I think that choosing the shortest path guarantees smooth transformations. I do not see why the distribution matching property provides smoother transformations. To my understanding, this is simply a way to generate less samples near the origin, but this does not directly means smoother transformations of the generated images. I believe that the motivation and the actual implications of the discussed property have to be explained better.","The sentiment of the review is generally positive, as the reviewer acknowledges that the paper is technically solid and well-written, although it needs some improvements. The sentiment score is therefore 50. The politeness of the language is high, as the reviewer uses phrases like 'in my opinion,' 'I believe,' and 'I think,' which are polite and considerate. The politeness score is 80.",50,80
"This paper investigates speaker adaption with a few samples based on an existing (pre-trained) multi-speaker TTS system. The three approaches in this paper are almost the same as the voice cloning work in Arik et al. (2018). However, it is still very beneficial to demonstrate these approaches for linguistic feature conditioned WaveNet.

Detailed comments:

1) This manuscript is not self-contained, as it omits the important details for acquiring linguistic features (e.g., phoneme duration model) and fundamental frequency (F0) at training and test time. The only information is that it uses existing model (Zen et al., 2016) to predict linguistic features and F0. What type of linguistic features are used in this work? Is the existing model (Zen et al., 2016) trained on the same training set as WaveNet model?

2) It seems the only speaker-dependent part of the system is the embedding table for WaveNet. Actually, both linguistic features (e.g., phoneme duration) and fundamental frequency sequence are highly speaker-dependent. The authors normalize F0 to make it as speaker-independent as possible. What about the speaker-dependent linguistic features? Why not keep them as speaker dependent, and do speaker-adaption for the new speaker at inference?

3) In my opinion, it’s a bit superfluous to name fine tuning as non-parametric few-shot adaption, and auxiliary network (speaker encoding) as parametric few-short adaption. Both ideas are quite natural as in Arik et al. (2018).

4) The abbreviations SEA-ALL, SEA-EMB and SEA-ENC are appeared without explanation. 

5) It would be better to provide more details about early termination criterion in Section 3.1. Is it simply the validation loss?

6) In Table 1, the MOS from Arik et al. (2018) and Jia et al. (2018) are not comparable. The experimental settings are different. Perhaps more importantly, these MOS evaluations are done by different group of people.

7) In Section 5.3, Nachmani et al. (2018) and Arik et al. (2018) have also used speaker verification model as an objective evaluation.

Overall, this is a good work with limited novelty but solid results. However, it can be improved in many ways as detailed  in previous comments. I would like to raise my rating if these comments can be addressed properly.","The sentiment of the review is moderately positive. The reviewer acknowledges the solid results and the beneficial demonstration of the approaches, despite noting the limited novelty and several areas for improvement. Therefore, the sentiment score is 40. The politeness of the language is quite high. The reviewer uses polite language, such as 'it would be better' and 'I would like to raise my rating,' and provides constructive feedback without being harsh or dismissive. Therefore, the politeness score is 80.",40,80
"The paper proposes a method to classify vulnerable and non-vulnerable binary codes where each data instance is a binary code corresponding to a sequence of machine instructions. The contributions include the creation of a new dataset for binary code vulnerability detection and the proposition of an architecture based on a supervised adaptation of variational auto-encoder, built upon the result of a sequential information,  
and using a regularization term to better discriminate positive from negative data. An experimental evaluation on the data proposed is presented, including several baselines, the results show the good behavior of the method.

Pros:
-Presentation of new application of representation learning models
-Construction of a new dataset to the community for binary software vulnerability detection
-The proposed model shows a good performance
Cons:
-The presentation of the dataset is for me rather limited while it is a significant contribution for the authors, it seems to be an extension of an existing dataset for source code vulnerability detection.
-From the last remark, it is unclear for me if the dataset is representative of binary code vulnerability problem
-The proposed architecture is reasonable and maybe new, but I find it natural with respect to existing work in the literature.

Comments:

-If providing a new dataset is a key contribution, the authors should spend more time to present the dataset. What makes it interesting/novel/challenging must be clarified. 
This dataset seems actually built from the existing NDSS18 dataset for source code vulnerability detection. If I understood correctly, the authors have compiled (and sometimes corrected) the source to create binaries, then they use the labels in NDSS18 to label the binary codes obtained. 
This a good start and can be useful for the community.
However the notion of vulnerability is not defined and it is difficult for me to evaluate the interest of the dataset.
I am not an expert in the field, but I am not that convinced that vulnerability for binary codes is necessary related to vulnerability that can be detected from source codes.
Indeed, one can think that some vulnerability may appear in binary codes that cannot be detected from source codes: e.g. use of unstable libraries, problems with specific CPU architectures, problems du to different interpretation of standard.

The current version of dataset seems to be a data where one tries to find the vulnerability that can be detected from code. It would be interesting here to know if detecting the vulnerabilities are easier from source code or from binary code.

It could be good if the authors could discuss more this point.

-The architecture proposed by the authors seems to use a sequential model (RNN or other) as indicated in Fig.2, the authors should precise this point.
The architecture is general enough to work on other problems/tasks - which is good - but the authors focus on the binary vulnerability code dataset in the experiments.

If the authors think that their contribution is to propose a general method for sequence classification, it could be good to apply it on other datasets.
Otherwise, something maybe more specific to the task would be useful.
In particular, there is no clear discussion to justify that variational autoencoders are better models for the task selected, it coud be good to argue more about it.

That being said, having non fixed priors and trying to maximize the divergence between positive and negative distributions are good ideas, but finally rather natural.

","The sentiment of the review is generally positive, as the reviewer acknowledges the contributions of the paper, such as the presentation of a new application of representation learning models, the construction of a new dataset, and the good performance of the proposed model. However, the reviewer also points out several areas for improvement, such as the limited presentation of the dataset and the need for more discussion on certain points. The politeness of the language is high, as the reviewer provides constructive feedback and suggestions for improvement in a respectful and professional manner.",60,80
"This paper proposes a new method to imitate expert efficiently. The paper first proposes a way to compute reward function from expert demonstration and uses the log probability to represent this reward function.  Then they find a form of bellman equation that can optimize the reward stably. After the 'Q learning without IRL', an off-policy RL off-pac is applied. So this paper achieves comparable results to GAIL but uses much less data amount. 

clarity:
This paper is clearly written.

originality:
This paper is original.

pros:
Comparable performance with GAIL.
Better performance than Behavioral Cloning
New way of using demonstrations

cons:
Although both the method and the experiments look promising, there is a very simple yet competitive baseline missing. This baseline is also mentioned in the original GAIL paper: you initialize GAIL with BC, and then train GAIL. That's the baseline for a set of fair comparison.
","The sentiment of the review is generally positive, as indicated by the acknowledgment of the paper's clarity, originality, and the advantages of the proposed method. The reviewer highlights the comparable performance with GAIL, better performance than Behavioral Cloning, and the new way of using demonstrations. However, the review also points out a missing competitive baseline, which is a constructive criticism rather than a negative sentiment. The politeness of the language is high, as the reviewer uses respectful and professional language throughout the review, even when pointing out the missing baseline.",80,90
"This paper is built on a simple but profound observation: Frey's bits-back coding algorithm can be implemented much more elegantly when replacing arithmetic coding (AC) with asymmetric numerical systems (ANS), a much more recent development not known at the time, simply due to the fact that it encodes symbols in a stack-like fashion rather than queue-like.

This simple observation makes for an elegantly written paper, with promising results on MNIST. I truly enjoyed reading it, and I'm convinced that it will spark some very interesting further work in the field of compression with latent-variable models.

Having said that, I would like to point out some possible limitations of the proposed approach, which I hope the authors will be able to address/clarify:

1. At the beginning of section 2.1, the authors define the symbols as chained conditionals prod_n p(s_n | s_1 ... s_n-1), which is generally permissible in AC as well as ANS, as long as the decoding order is taken into account. That is, in AC, the symbols need to be encoded starting with the first symbol in the chain (s_1), while in ANS, the symbols must be encoded starting with the last symbol in the chain, because the decoding order is inverted.

In their description of BB-ANS, the authors omit the discussion of conditional chains. It is unclear to me if a conditioning of the symbols is feasible in BB-ANS due to the necessity to maintain a strict decoding order. It would be very helpful if the authors could clarify this, and update the paper accordingly, because this could present a serious limitation. For instance, the authors simply extrapolate the performance of their method to PixelVAE; however, this model is autoregressive, so a conditioning of symbols seems necessary. Similarly, in appendix A, the authors mention the work of Minnen et al. (2018), where the same situation would apply, albeit one probabilistic level higher (on encoding/decoding the latents with an autoregressive prior).

2. Furthermore, in both cases (PixelVAE and Minnen et al.), the symbols (s) and latents (y) are defined as jointly conditioned on each other (i.e., computing the posterior on one element of y requires knowledge of all elements of s, and computing the likelihood on one element of s requires knowledge of all elements of y). This seems to imply that all operations pertaining to one data vector (i.e. to one image) would have to be done in a monolithic fashion, i.e.: first sample all elements of y from the stack, then encode all elements of s, and then encode all elements of y. Hence, if the goal is to compress only one image, the algorithm would never get to the point of reusing the ""bits back"", and the overhead of BB-ANS would be prohibitive. It seems that in the MNIST experiments, the authors avoid this problem by always encoding a large number of images at a time, such that the overhead is amortized.

3. Similarly, although the compression of continuous-valued variables up to arbitrary precision is an exciting development and I do not wish to undermine the importance of this finding, it should be noted that the finer the quantization gets, the larger the potential overhead of the coding scheme will grow. In practice, this would make it necessary to encode more and more images together, in order to still benefit from the method. This would be a good point to make in the discussion.

4. The authors state in the appendix that learned compression methods like Ballé et al. (2018) and Minnen et al. (2018) could be improved by using BB-ANS. The potential gain of BB-ANS for these models seems rather small, though, as the entropy of y must be larger or equal to the entropy of y conditioned on s: H[y] >= H[y|s], the latter of which should represent the potential coding gain. Ballé et al. (2018), however, found that the bits used to encode the hierarchical prior (i.e. H[y]) is only a small fraction of the total bitrate, thus upper bounding the potential gains for this type of model.

Overall, I think this is a well-written, important and elegant paper, and I would like to see it accepted at this conference. If the authors can satisfactorily address some of the above potential limitations, it might turn out to be even better.
","The sentiment of the review is highly positive, as evidenced by phrases like 'elegantly written paper,' 'promising results,' and 'I truly enjoyed reading it.' The reviewer expresses a strong belief in the paper's potential to spark further work in the field. However, the review also includes constructive criticism and suggestions for improvement, which are presented in a polite and respectful manner. The reviewer uses phrases like 'I would like to point out,' 'It would be very helpful,' and 'I do not wish to undermine,' which indicate a high level of politeness and a constructive approach to feedback.",90,95
"Summary. The paper is an improvement over (Balle et al 2018) for end-to-end image compression using deep neural networks. It relies on a generalized entropy model and some modifications in the training algorithm. Experimentals results on the Kodak PhotoCD dataset show improvements over the BPG format in terms of the peak signal-to-noise ratio (PSNR). It is not said whether the code will be made available.

Pros. 
* Deep image compression is an active field of research of interest for ICLR. The paper is a step forward w.r.t. (Balle et al 2018). 
* The paper is well written. 
* Experimental results are promising.

Cons.
* Differences with (Balle et al 2018) should be emphasized. It is not easy to see where the improvements come from: from the new entropy model or from modifications in the training phase (using discrete representations on the conditions).
* I am surprised that there is no discussion on the choice of the hyperparameter \lambda: what are the optimal values in the experiments? Are the results varying a lot depending on the choice? Is there a strategy for an a priori choice? 
* Also is one dataset enough to draw conclusions on the proposed method?

Evaluation.
As a non expert in deep learning compression, I have a positive opinion on the paper but the paper seems more a fine tuning of the method of (Balle et al 2018). Therefore I am not convinced that the improvements are sufficiently innovative for publication at ICLR despite the promising experimental results.

Some details.
Typos: the p1, the their p2 and p10, while whereas p3, and and figure 2 
p8: lower configurations, higher configurations, R-D configurations
","The sentiment of the review is generally positive, as indicated by phrases like 'The paper is an improvement' and 'Experimental results are promising.' However, the reviewer also expresses some reservations about the novelty and sufficiency of the improvements, which tempers the overall positivity. Therefore, the sentiment score is 40. The politeness of the language is high, as the reviewer uses polite and constructive language throughout, such as 'I am surprised' and 'I have a positive opinion,' without any rude or harsh comments. Therefore, the politeness score is 80.",40,80
"In a distributed learning system where a parameter server maintains a full resolution copy of the parameters, communication costs can be reduced by (a) discretizing the weights that the server broadcasts to the workers, and (b) discretizing the gradients that the workers return to the parameter server. Following existing literature, the authors propose to discretize the parameters in a manner that limits its impact on the loss function by means of a diagonal approximation of the Hessian. This also means that one can bound the difference between the gradient for the full precision parameter and the gradient for the discretized parameter.  In contrast, they discretize the gradients stochastically so that the discretized version is an unbiased estimator of the full precision stochastic gradient. Since the stochastic gradient is itself an unbiased estimator of the gradient, this means we are dealing with an estimator whose variance has increased in a manner we can bound as well. The theoretical analysis consists in pushing these two bounds through classical analyses of the stochastic gradient algorithm, in this case, a regret-based version in the style of Zinkevich or Duchi.  Although i did not check the minute details of the proof, the argument feels correct and familiar.  They also give an interesting result in favor of clipping gradients, worth developing.

Although the title promises an analysis that holds for deep networks, this analysis strictly applies only to convex models. The author argue that the predictions made by this analysis also apply to deep networks, and support this argument with extensive experiments (which certainly represent a fair amount of work).  This result is believable but should not be construed as an analysis. Nevertheless, both results (the theoretical result for convex model and the empirical result for deep networks) are interesting and worth sharing.

The main caveat comes from the style the parallel learning algorithm they are considering.  In the data-parallel case (which they consider), parameter servers approaches have been displaced by setups where all workers update their copy of the weights using the allReduced gradients.  One could also use discretized gradients to speedup the allReduce operation (this is less of a win because latencies dominate) but this would only result in an increased variance and a much simpler analysis.

Finally I am not completely up-to-date with this line of work and cannot evaluate the novelty with confidence. This was not known to me, which is only a piece of evidence.

-- bumping down my score because the misleading title was not addressed by the author response.
-- bumping it up again because the authors have reacted.
","The sentiment score is derived from the overall tone of the review, which is generally positive but with some critical points. The reviewer acknowledges the correctness and familiarity of the argument, the interesting results, and the extensive experiments, which are positive aspects. However, the reviewer also points out the misleading title and the caveat regarding the style of the parallel learning algorithm, which are negative aspects. Therefore, the sentiment score is moderately positive. The politeness score is high because the reviewer uses polite language throughout the review, even when pointing out issues. Phrases like 'the argument feels correct and familiar,' 'interesting result,' and 'worth sharing' indicate a respectful and constructive tone.",40,80
"The authors are proposing a method for allowing the generation of multiple objects in generated images given simple supervision such as bounding boxes and their associated labels. They control the spatial location of generated objects by the mean of an object pathway added to the architecture of both Generator and Discriminator within a GAN framework. They show generated results on Multi-MNIST, CLEVR with discussions of their model's abilities and properties. they also provide quantitative results on MSCOCO (IS and FID) using StackGAN and AttGAN models with the object pathway modifications and show some improvements compared to the original models. However it must be noted (as commented by the authors) that these models are using image captions only and do not have explicit supervision of bounding box and object labels.

This paper proposes a simple approach to generating requested objects in GAN-based image generation task, The method is supervised and requests (in its current form) the Bounding Boxes and Labels of the objects to integrate into the image generation. This task of controlling the nature (identity) and size of objects to integrate in a generated image is an important one and is significant to the GAN-base image generation community. In terms of originality, the approach is a nice simple architecture that takes care of the spatial location problem head-on. It seems like an obvious step but this does not take away from the merits of the proposed method.

The generator Global path is given a noise component. From the text, it does not seem that the Object path is given a noise component. Do you generate always the same object given the same label and Bounding Box then? Why not integrate some noise in this pipeline too?


Multi-MNIST:
The authors present results on Multi-MNIST 50K customed data to present the ability of the model to accurately put request images in the correct bounding box (BB) and do some ablation study. This is an interesting test as it shows that indeed the method proposed generates digits where it is expected to. Could you provide the ground truth labels for each/some image/s? For the failure cases it is often not clear what digit is what. For the Row E and F, 1s could be 7s and vice versa. Since it is a qualitative study, it would be nice to have the Ground Truth (GT) (which you provide to G at for generation). For the failure case of Row D (right) an interesting results would have been to have example of a digit bounding box from top to bottom with few pixel vertical shift to visualize when the model starts to mess-up the generation. This seems to point that your model (exposed to the location from BB for the object paths) is sensitive to what locations it has seen in training. How would you make the object path more robust to unseen location (overall you need to design an object of a given size, then locate it in your empty canvas prior to the CNN for generation)?

CLEVR:
The images resolution make it hard to really see the shape of the images (here too, the GT would be great). The bounding boxes make the images even harder to parse. I know the colors change but ""We can confirm that the model can control both location and object's shape"". For the location, it is true, for the shape is hard to completely tell at this resolution without GT. 

MS-COCO:
 Just a comment in passing on the fact that resizing images from COCO to 256x256 will inherently distort quite a bit of images, the median size (for each dim) for COCO is 640x480, if I am not mistaken. Most, if not all images in COCO are not 1.0 size ratio.
The quantitative results on COCO seem to confirm that the proposed method is generating ""better"" images according to IS and FID. This is a good thing, however the technique is strongly supervised (Bounding Box and Object Labels, caption compared to solely captions for StackGAN and AttGAN) so this result should be expected and really put into perspective as your are not comparing models w/ the same supervision (which you mention in the Discussion).

Discussion: 
I appreciate that the authors addressed the limitations of their approach in this section. The overlapping BBs seems to be an interesting challenge. Did you try to normalize the embeddings in overlapping area? A simple sum does not seem to be a good solution. In Figure 7 w/ overlapping zebras, the generation seems completely lost. 

In terms of clarity, the paper is well-written but would benefit *greatly* from using variables names when discussing 'layout embedding', 'generated local labels', etc. Variable names and equations, while not necessary, can go a long way to clearly express a model's internal blocks (most of the papers you referenced are using this approach). The paper employs none of this commonly used standard and suffers from it. I myself had to write down on the margin the different variables used at each step described in text to have an understanding of what was done (with help of Figure 1). You should reference Figure 1 in the Introduction, as you cover your approach there and the Figure is useful to grasp your contributions.

Another comment concerning clarity is, while it is fine to rely on previously published papers for description of our own work, you should not assume full knowledge from the reader and your paper should stand on its own without having the reader lookup for several papers to have an understanding of your training procedures. If one uses GAN training, it should be expected to cover/formulate quickly the min max game and the various losses you are trying to minimize. I am afraid that ""using common GAN procedures"" is not enough. When describing your experimental setup, pointing to another paper as ""hyperparameters remain the same as in the original training procedure"" should not be a substitution for covering it too, even if lightly in the Appendix. For instance: in the Appendix, it is mentioned that training was stopped at 20 epochs for Multi-MNIST, 40 for CLEVR... How did you decide on the epoch (early stopping, stopped before instabillity of GAN training, etc.) Did you use SGD? ADAM? Did you adjust the learning rate, which schedule? etc. for your GAN training. This information in the Appendix would make the paper overall stronger. 

Last comment: In terms of generation multiple objects. Have you had the chance to run an object detector on your generated image (you can build one on MSCOCO given the bounding box and label, finetune an ImageNet pretrained model). It would be interesting to see if the generated images are good enough for object detection.

Post-Rebuttal: Given the work from the authors on improving the clarity of the paper as well as investigating the use of object detection metrics to compare their methods, I decided to move my rating upward to 7  ","The sentiment of the review is generally positive, as the reviewer acknowledges the significance and originality of the proposed method, and appreciates the authors' efforts to address limitations. However, there are several critical points and suggestions for improvement, which slightly temper the overall positivity. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer uses constructive language, offers detailed suggestions, and acknowledges the authors' efforts and the importance of their work. Thus, the politeness score is 80.",60,80
"Summary:
The paper considers the problem of online stochastic convex optimization in a fully distributed topology. In particular, the authors focus on the synchronous setting and to avoid the slow progress that can be obtained by slow nodes, called stragglers, they propose an online distributed optimization method called Anytime Minibatch (AMB). In the update of AMB rather than fixing the minibatch size, they fix the computation time in each epoch. This characteristic prevents the stragglers from holding up the entire network, while allowing nodes to benefit from the partial work carried out by the slower nodes. 

A convergence analysis of AMB is provided showing that the online regret achieves the optimum performance. Numerical evaluations where a comparison of AMB and the ""Fixed MiniBatch"" method (FMB)are also presented.

Comments:
I believe that the idea of the paper is interesting and the convergence analysis seems correct, however i have some concerns regarding  the presentation and the numerical evaluation. 

1) In the title the word ""online"" is mentioned but never explained  in the main text. What is this mean? What are the differences compare to the ""static"" setting? See for example the work of [Tsianos, Rabbat (2016)] for more details on that. What are the related literature on this setting?

2) In the last paragraph of Introduction is highlighted that the algorithm AMB has the optimum performance?  The authors should add an appropriate reference there and explain why this is optimum for their setting. I believe that for the convenience of the reader current Section 5 called ""previous work"" can move immediately after introduction and more details of AMB with the existing literature should be provided. Probably rename the section ""Closely relate work"".

3) Section 2 is devoted mostly on the formal presentation of algorithm AMB. I strongly suggest the addition of a pseudocode of the algorithm in the appendix (or even in the main text if there is a space) where the reader can easily understand how the algorithm works.

4) On the Algorithm:  if some nodes are very slow and they do not make any update during the given time T what will happen? How this will affect the performance of the method? In this case does it make sense to increase the value of T.

5) On numerical evaluation:  A comparison of AMB and FMB  is presented both in synthetic and real data showing that AMB can be faster than FMB in terms of wall clock time. 
I am not sure if the performance of the AMB is as good as one should expect especially for the case of synthetic data. Will it be possible to construct a synthetic example with extremely slow nodes where the improvement of the performance is much better than 50%?

In general i find the paper interesting, with nice ideas and I believe that will be appreciated from researchers that are interested on control theory/signal processing and information theory.  Since the paper is focused on convex optimization I am not sure if it will be particularly interesting for a substantial fraction of the ICLR attendees.
","The sentiment of the review is generally positive, as the reviewer finds the paper interesting and acknowledges the correctness of the convergence analysis. However, the reviewer also expresses concerns about the presentation and numerical evaluation, which slightly tempers the overall positive sentiment. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer uses polite phrases such as 'I believe,' 'I strongly suggest,' and 'Will it be possible,' indicating a respectful and constructive tone. Thus, the politeness score is 80.",60,80
"The paper analyses the data collected from 6005 neurons in a mouse brain. Visual stimuli are presented and the responses of the neurons recorded. In the next step, a rotational equivariant neural network architecture together with a sparse coding read-out layer is trained to predict the neuron responses from the stimuli. Results show a decent correlation between neuron responses and trained network. Moreover, the rotational equivariant architecture beats a standard CNN with similar number of feature maps. The analysis and discussion of the results is interesting. Overall, the methodological approach is good.

I have trouble understanding the plot in Figure 4, it also does not print well and is barely readable on paper.

I have a small problem Figure 6 where ""optimal"" response-maps are presented. From my understanding, many of those feature maps are not looking similar to feature maps that are usually considered. Given the limited data available and the non-perfect modeling of neurons, the computed optimal response-map might include features that are not present in the dataset. Therefore, it would be interesting to compare those results with the stimuli used to gather the data. E.g. for a subset of neurons, one could pick the stimulus that created the maximum response and compare that to what the stimulus with the maximum response of the trained neuron was. It might be useful to include the average correlation of the neurons belong to each of the 16 groups(if there are any meaningful differences), especially as the cut-off of ""correlation 0.2 on the validation set"" is rather low.

Note: I am not an expert in the neural-computation literature, I am adapting the confidence rating accordingly.","The sentiment of the review is generally positive, as the reviewer appreciates the methodological approach and finds the analysis and discussion interesting. However, there are some criticisms regarding specific figures and suggestions for improvement. Therefore, the sentiment score is 60. The language used is polite, with phrases like 'I have trouble understanding' and 'it might be useful,' indicating a constructive and respectful tone. Thus, the politeness score is 80.",60,80
"This paper builds on the long recent tradition of analyzing deep linear neural networks. In addition to an ample appendix bringing the page total to 20, the authors went over the recommended eight pages, hitting the hard limit of 10 and thus per reviewing directions will be held to a higher standard than the other (mostly 8-page) papers. 

The recent literature on deep linear networks has explored many paths with the hope of producing insights that might help explain the performance of deep neural networks. A recent line of papers by Soudry and Srebro among others focuses on the behavior of stochastic gradient descent. This paper’s analysis comes from a different angle, following the work by Saxe et al (2013) whose analysis considers a (classic one hidden layer) linear teacher network that generates labels and a corresponding student trained to match those labels. The analysis hinges on the singular value decomposition of the composite weight matrix USV^T = W = W^{32} W^{21}.

One aim of the present work, that appears to be a unique contribution above the prior work is to focus on the role played by task structure, suggesting that certain notions of task structure may play a more significant role than architecture and that any bounds which consider architecture but not task structure are doomed to be excessively loose. 

To facilitate their analysis, the authors consider an artificial setup that requires some specific properties. For example, the number of inputs are equal to the input dimension of the network, with the inputs themselves being orthonormal. The labeling function includes a noise term and the singular values of the teacher model admit an interpretation as signal to noise ratios. Given their setup, the authors can express the train and test errors analytically in terms of the weight matrices of the teacher and student and the input-output covariance matrix. The authors then analyze the gradient descent dynamics in what appears to follow the work of Saxe 2013 although I am not an expert on that paper. The analysis focuses on the time dependent evolution of the singular values of the student model, characterized via a set of differential equations.

The next analysis explores a condition that the authors dub “training aligned” initial conditions. This involves initializing the student weights to have the same singular vectors as the training data input-output covariance but with all singular values equal to some amount epsilon. The authors show that the learning dynamics give rise to what they characterize as a singular value “detection wave”. Detecting the modes in descending order by their corresponding singular values.

A set of synthetic experiments show close alignment between theory and experiment.

Section 3.5 offers just one paragraph on a “qualitative comparison to nonlinear networks”. A few issues here are that aesthetically, one-paragraph subsections are not ideal. More problematic is that this theory presumably is building towards insights that might actually be useful towards understanding deep non-linear networks. Since the present material is only interesting as an analytic instrument, I would have hoped for greater emphasis on these connections, with perhaps some hypotheses about the behavior of nonlinear nets driven by this analysis that might subsequently be confirmed or refuted. 

The paper concludes with two sections discussing what happens when nets are trained on randomly labeled data and knowledge transfer across related tasks respectively. 

Overall I think the paper is well-written and interesting, and while I haven’t independently verified every proof, the technical analysis appears to be interesting and sound. The biggest weaknesses of this paper---for this audience, which skews empirical---concern the extent to which the work addresses or provides insight about real neural networks. One potential weakness in this line of work may be that it appears to rely heavily on the linearity of the deep net. While some other recent theories seem more plausibly generalized to more general architectures, it’s not clear to me how this analysis, which hinges so crucially on the entire mapping being expressible as a linear operator, can generalize. 

On the other hand, I am personally of the opinion that the field is in the unusual position of possessing too many tools that “work” and too few new ideas. So I’m inclined to give the authors some license, even if I’m unsure of the eventual utility of the work. 

One challenge in reviewing this paper is that it builds tightly on a number of recent papers and without being an authority on the other works, while it’s possible to assess the insights in this paper, it’s difficult to say conclusively which among them can rightly be considered the present paper’s contributions (vs those of the prior work).
","The sentiment of the review is generally positive, as the reviewer describes the paper as 'well-written and interesting' and acknowledges the technical analysis as 'interesting and sound.' However, there are some criticisms regarding the paper's relevance to real neural networks and its reliance on linearity, which slightly temper the overall positive sentiment. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer provides constructive feedback and acknowledges the authors' efforts and contributions. The reviewer also expresses their personal opinion in a respectful manner, giving the authors 'some license' despite uncertainties. Thus, the politeness score is 80.",60,80
"This paper explores the inevitability of adversarial examples with concentration inequalities. It is motivated by the difficulties of achieving adversarial robustness in literature. It derives isoperimetric inequalities on a cube, and then discuss the adversarial robustness of data distributed inside the cube, with the assumption that the data has bounded density. These inequalities are established on different norms. The authors then discuss limitation of the proposed bounds when analyzing practical data distribution and discussed the influence of dimensionality on adversarial robustness.


Novelty of the idea:
The idea of using concentration inequalities to explain vulnerability is novel in the field of adversarial examples and is a relevant/meaningful angle on understanding this phenomenon. (Although there are concurrent works also relating concentration inequalities to adversarial robustness, they don't diminish the novelty of this work.)



On technical contributions:
In summary, this paper applies / adapts previous results in concentration inequalities to develop bounds related to adversarial examples. The bounds in Lemma 3 are on any p>0, this seems to be new to my knowledge, but the technical contribution in the proof is limited.

Here are some detailed comments.

The authors claim that
""This question is complicated by the fact that simple, geometric isoperimetric inequalities fail to exist for the cube, and the shapes that achieve minimal \eps-expansion (if they exist) depend on the volume they enclose and the choice of \eps.""
This statement is at least misleading, if not wrong. It is well known that geometric isoperimetric inequality does exist for cube for the L2 case (see Ledoux, M., 2001. Proposition 2.8.), and the proof procedure the author used is also very similar to the proofs in Ledoux, M., 2001.

Theorem 5's proof is confusing, if not wrong. 
This is my brief recap on the first part of Thm 5, 
If there exists eps and p such that, for all classifiers on MNIST, a random image has eps-adv with probability at least p, then for all classifiers on b-MNIST, a random image has b*eps-adv with probability at least p.
The proof in Appendix E says b-MNIST images can be classified by first downsampling. These downsampled classifiers do not cover ""all classifiers on b-MNIST"", so I don't see how the proof stands.
Likewise, the proof of the second part has the similar problem.
Therefore, I'm not yet convinced that Thm 5 is correct.
Also I suggest the authors use more rigorous language to present Theorem 5, in a similar fashion to previous theorems.

Re: Lemma 4, my understanding is that it is from previous literature. The authors should point out exactly where is it from (with section# and theorem#), so that readers and reviewers can more easily check the correctness of it.

The authors mention that ""Intuitively, the concentration limit Uc can be interpreted as a measure of image complexity.""
I think this statement is problematic. It is, at best, oversimplifying the the problem. If we assume the data lies in low-dimensional space, the volume of the support will be 0, no matter how complex the shape of the manifold is. This lead to unbounded density in the ambient dimension.
Even when considering ""expanded dataset"" like the authors discussed in Section 7, it is not obvious that Uc can be interpreted as image complexity. To make such a claim, more assumptions need to made and more analyses need to be done.
Similar comments applies to the ""correlations between pixels"" and concentration.



On the significance:
As the author themselves have already mentioned, the bounds described in the paper all depends on the bounded density of the data distribution. In practice, the density of data distribution is difficult to understand, if not impossible. Therefore it is still inconclusive whether the ""inevitability"" exists. But to be fair, I believe this is mostly due to the difficulty of the problem being studied.



Clarity and writing:
The skeleton of the paper is well written and easy to follow. I've pointed out some problems in my previous comments.
I also appreciate that the authors made efforts to not overclaim.

here are a few more comments:
- I personally feel Section 3 as an ""warm-up"" section is redundant, and the authors can consider move them to the appendix.
- In Section 6 and 7, the authors talk about when is the bound ""meaningful"" and ""active"". This part is confusing/misleading. eps=sqrt(n) is actually the maximum possible perturbation and not falls into the common ""adversarial perturbation"" where the perturbation does not change the semantic meaning of the image. There should be a least an additional numerical examples on small eps, so the readers have better ideas on the tightness/looseness of the bound.



References:
Ledoux, M., 2001. The concentration of measure phenomenon (No. 89). American Mathematical Soc..

==========================
I change my rating on this paper to be 6, after the authors' response. 
","The sentiment of the review is mixed. The reviewer acknowledges the novelty and relevance of the idea, but also points out several technical issues and limitations. The sentiment score is therefore slightly positive. The language used is generally polite, with constructive criticism and suggestions for improvement, though there are a few instances where the language could be perceived as slightly blunt.",20,60
"The paper describes a new learning framework, based on generative
adversarial imitation learning (GAIL), that is able to learn sub-tasks
policies from unsegmented demonstrations. In particular, it follows
the ideas presented in InfoGAIL, that depends on a latent variable,
and extend them to include a sequence of latent variables representing
the sequence of different subtasks. The proposed approach uses a
pre-training step, based on a variational auto-encoder (VAE), to
estimate latent variable sequences. The paper is well written and
relates the approach with the Options framework. It also shows,
experimentally, its performance against current state-of-the-art
algorithms.  

Although the authors claim in the appendix that the approach is
relatively independent on the dimensionality of the context variable,
this statement needs further evidence. The approach is similar to HMMs
where the number f hidden states or latent variables can make a
difference in the performance of the system.

Also, it seems that the learned contexts do not necessarily correspond
to meaningful sub-tasks, as shown in the circle-world. In this sense,
it is not only relevant to determine the ""right"" size of the context
variable, but also how to ensure a meaningful sub-task segmentation. 
","The sentiment of the review is generally positive, as the reviewer acknowledges that the paper is well-written, relates the approach to the Options framework, and demonstrates its performance against state-of-the-art algorithms. However, the reviewer also points out some areas that need further evidence and clarification, which slightly tempers the overall positive sentiment. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer uses polite and constructive language to provide feedback and suggestions for improvement. There are no rude or harsh comments, and the tone is respectful throughout. Therefore, the politeness score is 90.",60,90
"In this work, the authors propose Switchable Normalization (SN), which *learns* to switch / select different normalization algorithms (including batch normalization (BN), Instance Normalization (IN), Layer Normalization (LN)) in layers of the networks and in different applications. The idea is motivated by observations (shown in Fig 1) that, 1) different tasks tend to have applied different normalization methods; 2) some normalization methods (e.g. BN) are fragile to very small batch size.

The authors propose a general form for different normalization methods, which is a Gaussian normalization and then scale and shift by scalars. Different normalization methods utilize different statistics as the mean and the variance of the Gaussian normalization. The authors further propose to learn the combination weights on mean and variance, which is w_k and w'_k in Eqn (3). To avoid duplicate computation, the authors also do some careful simplification on computing mean and variance with all of the three normalization methods.

In the experiment part, the authors demonstrate the effectiveness of the proposed SN method on various kinds of tasks, including ImageNet classification, object detection and instance segmentation in COCO, semantic image parsing and video recognition. In all of the tasks tested, which also cover the common application in computer vision, SN shows superior and robust performance.

Pros:
+ Neat motivation;
+ Extensive experiments;
+ Clear illustration;

Cons
- There are still some experiment results missing, as the authors themselves mentioned in the Kinetics section (but the reviewer thinks it would be ready); 
- In Page 3 the training section and Page 4, the first paragraph, it mentioned Θ and Φ (which are the weights for different normalization methods) are jointly trained and different from the previous iterative meta-learning style method. The authors attribute ""In contrast, SN essentially prevents overfitting by choosing normalizers to improve both learning and generalization ability as discussed below"". The reviewer does not see it is well justified and the reviewer thinks optimizing them jointly could lead to instability in the training (but it did not happen in the experiments). The authors should justify the jointly training part better.
- Page 5 the final paragraph, the reviewer does not see the point there. ""We would also like to acknowledge the contributions of previous work that explored spatial region (Ren et al., 2016) and conditional normalization (Perez et al., 2017). ""  Please make it a bit more clear how these works are related. ","The sentiment of the review is generally positive, as indicated by the praise for the neat motivation, extensive experiments, and clear illustration. However, there are some criticisms regarding missing experimental results, the justification of jointly training weights, and the clarity of certain points. Therefore, the sentiment score is 60. The language used in the review is polite, with constructive feedback and suggestions for improvement rather than harsh criticism. Thus, the politeness score is 80.",60,80
"Overall this paper is ok. The algorithm seems novel, but is clearly very closely related to other things in the literature. The paper is also let down by poor exposition in several areas. The numerical results seem reasonably strong, at least against relatively old baselines.

Equation 8 is crucial to the final algorithm, but is presented with no proof or explanation.

Just above theorem 1 the sentence does not parse ""Further, for each s, let λs be the solution to "", firstly there is no 'solution' to an equation, secondly should it be λs or pi?

The discussion following theorem 1 is very messy and hard to follow and the notation is horrendous. I'm confused as to why the indicator function in the 'disaggregated' update only includes states for which the constraint is already satisfied, what about the states where it is not? I presume this is because you initialize from the previous policy, but this seems very approximate and even worse updating the parameters for one state might significantly move the policy in some other state meaning large violations are possible and not dealt with.

The connections to the papers 'MAXIMUM A POSTERIORI POLICY OPTIMISATION' and 'Relative Entropy Policy Search' should be mentioned, as another commenter said previously.

I don't think TRPO/PPO is SOTA anymore, so maybe these baselines aren't particularly interesting.

Figure 2 is incomprehensible.

Two of the references are repeated (Schulman et al, Wang et al).

The appendices include long lists of equalities with no explanation (e.g. appendix B), how is a reader meant to reasonably follow those steps? Each non-trivial equality needs a sentence explaining what was used to get it.","The sentiment of the review is moderately negative. The reviewer acknowledges that the algorithm seems novel and the numerical results are reasonably strong, but they also point out several significant issues such as poor exposition, lack of proof for crucial equations, messy discussions, and incomprehensible figures. The politeness of the language is neutral to slightly negative. The reviewer uses direct and critical language without much cushioning, which can come across as blunt or harsh.",-40,-20
"The paper considers invasive BMIs and studies various ways to avoid daily recalibration due to changes in the brain signals. 
While I like the paper and studied methods -- using adverserial domain adaptation is interesting to use in this context --, I think that the authors oversell a bit. 
The problem of nonstationarity rsp. stability is an old one in non-invasive BCIs (shenoy et al JNE 2006 was among the first) and a large number of prior methods have been defined to robustify feature spaces, to project to stable subspaces etc. Clearly no Gans at that time. The least the authors could do is to make reference to this literature, some methods may even apply also for the invasive data of the paper.
While the authors did not clearly say that they present an offline analysis; one method, the GAN, gets 6% better results then the competitors. I am not sure whether this is practically relevant in an online setting. But this needs to be clearly discussed in the paper and put into perspective  to avoid wrong impression. Only an online study would be convincing. 

Overall, I think the paper could be accepted, the experiments are nice, the data is interesting, if it is appropriately toned down (avoiding statements about having done something for the first time) and properly references to prior work are given. It is an interesting application domain. I additionally recommend releasing the data upon acceptance. 

","The sentiment of the review is generally positive, as the reviewer expresses appreciation for the paper and its methods, and suggests that the paper could be accepted with some revisions. However, the reviewer also points out areas where the paper oversells its contributions and lacks proper references to prior work, which slightly tempers the overall positive sentiment. Therefore, the sentiment score is 60. The politeness of the language is quite high, as the reviewer uses polite phrases such as 'I like the paper,' 'I think,' and 'I recommend,' and provides constructive feedback without being harsh or rude. Thus, the politeness score is 80.",60,80
"Pros:
- Paper proposes a somewhat complicated but easy to understand idea for open set classification. Formulation is quite intriguing.
- Outperforming recent baselines on most scenarios, despite being a linear classifier on fixed CNN features.

Cons:
- Experiment setup somewhat flawed (but the same flaw is in prior work too)
    To elaborate: DeCAF7 is trained on ImageNet, which gives the underlying network extra categorical information of the 1000 classes. Some of these clases are arguably in the ""unknown classes"" in the open set setting. This may jeopardize the premise since the feature knows those classes are semantically different from known classes. Unfortunately (Busto & Gall, 2017) and (Saito et al., 2018) do this too.
    This is especially problematic since DeCAF7 has a near-linear relationship to the final sigmoid logits, which are the 1000-way ImageNet class scores. This makes the authors formulation (separate subspaces for known and unknown classes) more easily exploit this leaked information. This is because the 1000-way scores obviously have subspaces for all 1000 ImageNet classes, and by extension, the ""known"" and ""unknown"" classes in the open set setting. 
    If this is true and is the main reason that the proposed method outperforms, I would not consider the conclusion of the paper very informative. Instead, its signifies the need of a better experiment setup for the problem.
    A way to strengthen the paper is to use a network pre-trained on other datasets (e.g. Places, or a subset of ImageNet) to verify the findings of the paper.
- Lacks clarity for what is being done at test time. 
    I cannot find whether the final SVM is trained on original DeCAF features, or S and T. If it is the latter, how are the representations of target domain data obtained at test time? Are they d dimentional or 2d dimentional?
    Can you clarify that the test samples are not used for unsupervised training?
- Experiment elaborate but feels incomplete.
    It feels like the authors are proposing 3 variations of the method, and there is not one of them that consistently outperform the others. If so, the paper would lack some ablation analysis that provides insights of what makes the FRODA-SVM outperform prior art. For example, how much do the hyperparameters matter? What happens if e.g. d or lambda1 is very large/small?

Clarity:
- Abstract spends too much time on defining problem setup
- ""Faster than prior work"" refers to the training time, and excludes the DeCAF feature extraction.

Originality:
I am not familiar with the related work.

Significance:
It is quite impressive that a linear model on fixed CNN activations outperforms prior art. However, see the first point in the cons.


-----------
Edit: most of the issues listed in ""cons"" are addressed. Although the additional experiments are not very comprehensive, they can better support the claims. I am bumping up the rating to 7.
","The sentiment of the review is generally positive, as indicated by the pros listed and the final bump in rating to 7 after addressing most of the issues. The reviewer acknowledges the intriguing formulation and the impressive performance of the proposed method. However, the review also points out several significant flaws and areas for improvement, which tempers the overall positivity. Therefore, the sentiment score is 40. The politeness of the language is quite high. The reviewer uses polite and constructive language throughout, even when pointing out flaws. They provide detailed explanations and suggestions for improvement without being dismissive or rude. Therefore, the politeness score is 80.",40,80
"The paper presents an approach for simultaneously learning policies and reward functions for reaching goals that are described by an instruction providing spatial relations among objects. The proposed platform, called Adversarial Goal-Induced Learning from Examples (AGILE), is composed of an off-the-shelf RL module like A3C and a separate module for learning a reward function, implemented using the NMN paradigm. The RL module is trained using the reward function learned by the reward module. The reward module is trained to map a given <instruction, state> into a score between 0 and 1 depending on how well the provided state satisfies the instructions provided in the instruction. The returned score is used as a reward function. The training of the reward function is performed by using a dataset of positive examples, and using the states visited by the agent while it's learning as negative examples. To account for the fact that the agent becomes better over time and its visited states can no longer be used as negative examples, the authors proposed a heuristic where the states visited by the agent are not all used as negative examples, but only those that have the lowest scores.
The paper also presents an empirical evaluation of the proposed approach on a synthetic task where the agent is tasked with move bocks of different shapes and colors to a desired final configuration. The AGILE approach was compared to the baseline A3C algorithm where a sparse binary reward signal was used only whenever the agent reaches the goal state. AGILE is also compared to A3C with an auxiliary task of reward prediction. 
The paper is clearly written and technically strong. However, I have two issues with this paper: 1) the proposed approach is a simple combination of A3C and the NMN architecture, 2) the experiments are performed on simple synthetic tasks that make learning spatial relations fairly easy, I would love to see more real images as it has been demonstrated in prior works on learning  spatial relations. It is not clear from these experiments if the proposed approach will scale up to higher-dimensional inputs. Moreover, there are several stability issues that can be caused by the proposed approach. For instance, the reward function is changing over time, how does that affect the learning rate? Also, instead of using the learned policy itself to generate negative examples and run into non IID data, instabilities, and increasingly good negative examples, why not use a fixed dataset of negative examples generated with a random policy? It would be interesting to do perform an experiment where you compare to the classical reward learning setup where you simply provided labeled positive and negative examples and classify them offline, then use the learned reward function online for RL. 
How did you tune the hyper-parameter \rho (percentage of negative examples to discard) for specific tasks? Do you have any guarantees for this approach?
In the generalization experiments, it is mentioned that 10% of the instructions are held out. Are these 10% randomized?","The sentiment of the review is mixed. The reviewer acknowledges that the paper is clearly written and technically strong, which is positive. However, they also express significant concerns about the simplicity of the proposed approach and the limitations of the experiments, which are negative points. Therefore, the sentiment score is slightly positive but not overwhelmingly so. The politeness of the language is high; the reviewer uses polite and constructive language throughout, even when pointing out issues and making suggestions for improvement.",20,80
"This manuscript introduces a computational method to speed up training and inference in deep neural networks: the method is based on dynamic pruning of the compute graph at each iteration of the SGD to approximate computations with a sparse graph. To select which neurons can be zeros and ignored at a given iteration, the approach computes approximate activations using random projections. The approach gives an overall decrease in run-time of 0.8 to 0.6. I believe that its largest drawback is that it does not lead to the same sparsity pattern in a full minibatch, and hence cannot be implemented using matrix-matrix multiplications (GEMM). As a result, the compute-time speed ups are not huge, though the decrease in memory is important. In my eyes, this is the largest drawback of the manuscript: the total computational speed-up demonstrated is not fully convincing.

The manuscript is overall well written and easy to understand, though I wish that the authors employed less acronyms which forced me to scan back as I kept forgetting what they mean.

The strength of the paper are that the solution proposed (dynamic approximation) is original and sensible. The limitations are that I am not sure that it can give significant speedups because I it is probably hard to implement to use well the hardware.

Questions and comments:

1. Can the strategy contributed be implemented efficiently on GPUs? It would have been nice to have access to some code.

2. Fig 8(b) is the most important figure, as it gives the overall convergence time. Is the ""dense baseline"" using matrix-vector operations (VMM) or mini-batched matrix-matrix operation (GEMM)?

3. Can the method be adapted to chose a joint sparsity across a mini-batch? This would probably mean worst approximation properties but would enable the use of matrix-matrix operations.

4. It is disappointing that figure 8 is only on VGG8, rather than across multiple architectures.

5. The strategy of zeroing inputs of layers can easily create variance that slows down overall convergence (see Mensh TSP 2018 for an analysis of such scenario). In stochastic optimization, there a various techniques to recover fast convergence. Do the authors think that such scenario is at play here, and that similar variance-reduction methods could bring benefits?

6. I could not find what results backed the numbers in the conclusion: 2.3 speed up for training. Is this compared to VMM implementations? In which case it is not a good baseline. Is this for one iteration? In which case, it is not what matters at the end.

7. Is there a link between drop-out and the contributed method, for instance if the sparsity was chosen fully random? Can the contributed method have a regularizing effect?

","The sentiment score is determined by the overall tone and content of the review. The reviewer acknowledges the originality and sensibility of the proposed solution, which is positive, but also points out significant drawbacks and limitations, which are negative. The sentiment is mixed but leans slightly towards the positive due to the acknowledgment of the paper's strengths. Therefore, the sentiment score is 20. The politeness score is determined by the language used in the review. The reviewer uses polite and constructive language throughout, even when pointing out drawbacks and limitations. There are no rude or harsh comments. Therefore, the politeness score is 80.",20,80
"This paper proposes a novel approach to explain neural network predictions by learning hierarchical representations of groups of input features and their contribution to the final prediction. The proposed method is a straightforward extension of the contextual decomposition work by (Murdoch et. al. 2018) which estimates feature interpretability for LSTMs. This work extends (Murdoch et. al. '18) to more general NN architectures and further employs agglomerative clustering to identify groups of features-- as opposed to individual features--that are predictive of the output. 

Results are shown using a LSTM trained on the standard Stanford sentiment task and a VCG DNN trained on ImageNet which show the superior performance of the proposed approach. In addition, the paper also provides some survey results where ""humans"" were asked to pick more interpretable models. 

The paper is nicely written and puts itself nicely in context of the previous work. Though, I have several concerns:

1). Biggest concern: Conditioning on the (Murdoch et. al. 18) paper, the methodological novelty of the proposed approach is minimal. Though, the experimental gains on the vision and NLP tasks are nice.

2). It was unclear to me how the agglomerative algorithm (Algorithm 1) was run. That is, was it run as part of the LSTM estimation for instance for the sentiment task OR was it run post-hoc after getting the model estimates from LSTM? If it was run post-hoc then I am unsure if we can assume that the ""agglomeratively grouped CD scores of individual features"" are the same as the ""CD scores for the groups/interactions of features"" in terms of their contribution to the final prediction.

3). Though, the paper mentions several times regarding generalizing (Murdoch et. al. 18) to architectures other than LSTMs but still the experimental results on the sentiment task uses an LSTM as the model. It would have been nice to show the comparative strength of the proposed approach on a different architecture even for the sentiment task. (I understand that the paper uses a different DNN architecture for the vision task).

4). The paper talks several times about diagnosing why a model went wrong e.g. the ""negation"" in the case of the LSTM model in Figure 2, but never discusses the bigger and more interesting problem. How can we build an improved LSTM model for the sentiment task which classifies that incorrect prediction correctly? 

","The sentiment of the review is moderately positive. The reviewer acknowledges the novelty and superior performance of the proposed approach, as well as the quality of the writing. However, they also express several concerns about the methodological novelty, clarity of the algorithm, and the scope of the experiments. Therefore, the sentiment score is 40. The politeness of the language is high. The reviewer uses polite language, such as 'nicely written' and 'it would have been nice,' and provides constructive feedback without being rude or dismissive. Therefore, the politeness score is 80.",40,80
"The authors focus on the selection problem of k statistically significant features discriminating 2 probability distributions accessible via samples. They propose a non-parametric approach under the PSI (post selection inference) umbrella using MMD (maximum mean discrepancy) as a discrepancy measure between probability distributions. The idea is to apply (asymptotically) normal MMD estimators, rephrase the top-k selection problem as a linear constraint, and reduce the problem to Lee et al., 2016. The efficiency of the approach is illustrated on toy examples and in GAN (generative adversarial network) context. The technique complements the PSI-based independence testing approach recently proposed by Yamada et al., 2018. 

The submission is a well-organized, clearly written, nice contribution; it can be relevant to the machine learning community.

Below I enlist a few suggestions to improve the manuscript:
-Section 1: The notion of characteristic kernel (kernel when MMD is metric) has not been defined, but it was referred to. 'Due to the mean embeddings in RKHS, all moment information is stored.': This sentence is somewhat vague.
-Section 1: 'MMD can be computed in closed form'. This is rarely the case (except for e.g. Gaussian distributions with Gaussian or polynomial kernels). I assume that the authors wanted refer to the estimation of MMD.
-Section 1: 'K nearest neighbor approaches (Poczos & Schneider, 2011)'. The citation to this specific estimator can go under alpha-divergences. The Wasserstein metric could also be mentioned.
-Section 3.1: k is used to denote the number of selected features and also the kernel used in MMD. I suggest using different notations.
-Theorem 1: '\Phi is the CDF...'. There is no \Phi in the theorem.
-Section 3.2: The existence of MMD (mean embedding) requires certain assumptions: E_{x\sim p}\sqrt{k(x,x)} < \infty, E_{x\sim q}\sqrt{k(x,x)} < \infty.
-Section 3.2.: block estimator: 'B_1 and B_2 are finite'. 'fixed'?
-Section 3.2.: MMD_{inc}: 
   i) 'S_{n,k}': k looks superfluous.
   ii) 'l': it has not been introduced (cardinality of D).
-Section 3.3: typo: 'covraiance' (2x)
-Section 3.3: Fan et al. 2013: The citation can go to \citep{}.  
-Theorem 2: 
   i)'c' is left undefined.
   ii)Comma is missing before 'where'.
   iii)\xrightarrow{d} (Theorem 2, Corollary 3-4): Given that 'd' also denotes dimension in the submission, I suggest using a different notation for convergence in distribution.
-At the introduction of block-MMD the block size (B) was fixed, while in the experiments (e.g. Figure 3) it is growing with the sample size (B=\sqrt{n}). The assumption on B should be clearly stated.
-Section 5.1: (b) mean shift: comma is missing before 'where'.
-References: 
   i) Abbreviations and names in the titles should be capitalized (such as cramer, wasserstein, hilbert-schmidt, gan, nash). 
   ii) Scholkopf should be Sch\{""o}lkopf (in the ALT 2005 work).
   iii) 'Exact post-selection inference, with application to the lasso': All the authors are listed; 'et al.' is not needed.","The review starts with a positive sentiment, highlighting that the submission is well-organized, clearly written, and a nice contribution relevant to the machine learning community. This indicates a positive sentiment. The language used throughout the review is polite and constructive, offering specific suggestions for improvement without any negative or rude remarks. The reviewer provides detailed feedback in a respectful manner, which suggests a high level of politeness.",80,90
"The paper introduces an adaptation of the Scattering transform to signals defined on graphs
by relying on multi-scale diffusion wavelets, and studies a notion of stability of this representation
with respect to changes in the graph structure with an appropriate diffusion metric.

The notion of stability in convolutional networks is an important one, and the proposed notion of stability
with respect to diffusion distances seems like an interesting and relevant way to extend this to signals on graphs.
With this goal in mind, the authors introduce a scattering transform on graphs by relying on diffusion wavelets,
and provide an appropriate study of stability, which seems to highlight relevant properties of the graphs.
The proposed representation seems to provide benefits compared to the previous work of Zou & Lerman,
particularly regarding computational efficiency, as well as stability with respect to a metric that is perhaps more
useful, though there is a dependence on the graph topology through the spectral gaps.
In addition, the experiments on author attribution and source localization suggest that the
resulting representation remains discriminative, in addition to providing stability to changes in graph structure.

I find that these contributions provide an interesting advance in theoretical understanding of graph convolutional networks
from a stability perspective, in addition to introducing a useful non-learned representation,
and am thus in favor of acceptance.

Nevertheless, some parts of paper would benefit from further discussions and more clarity:

- other than empirically, one aspect that's missing compared to the original study of the scattering transform is energy preservation. The authors could at least provide a discussion of whether such a property can be obtained here as well (does it depend on the spectral gap through C(beta)?)

- what is the role of the spectral gap in the stability bounds? is this a drawback of the diffusion metric / choice of wavelets?

- Section 3.2 suggests that metric stability is a good way to characterize stability, by seeing deformations in Euclidian domains as a change to the ground metric. Yet, in Euclidian scattering, the same representation is applied to a deformed signal and the original signal, and stability is measured with the Euclidian metric.
Can the link be made more precise, by explaining what a deformation of a signal would be on a graph, or by applying arguments from the proposed construction to the Euclidian case?

- the paper is heavy on terminology from wavelets and harmonic analysis, a more detailed presentation of diffusion wavelets and related concepts such as localization would be beneficial. Also, it seems like the chosen wavelets in the construction favor spatial over frequential localization - is this due to a trade-off? if so, can it be avoided?


Some more detailed comments:
- Section 2, 'generally far weaker': what is meant by 'weaker'?
- Section 3.3:
  * 'calculus on T': T is used before being defined
  * clarify what norm is used (I assume operator norm?)
  * 'defines a distance', 'stronger than .. GH': this should probably be justified
- Section 4:
  * 'optimal spatial localization', 'temporal difference', 'favoring spatial over frequential localization': these could be clarified
  * 'amplify the signal': what does this mean?
  * the sentence about the choice of the appropriate J is not clear and should be further clarified
- Section 5.1:
  * the sentence about the choice pi/pi* = 1 should be clarified. Also, where is this assumption used?
  * epsilon_psi, epsilon_U should be defined
  * 'given that [..] by definition': this doesn't seem to be defined elsewhere
  * (16): isn't a factor m missing in the first term?","The review starts with a positive sentiment, appreciating the novelty and relevance of the proposed method. The reviewer acknowledges the contributions and expresses favor towards acceptance, which indicates a positive sentiment. However, the review also includes several constructive criticisms and suggestions for improvement, which are presented in a polite and professional manner. The language used is respectful and aims to help the authors improve their work, without being harsh or dismissive.",80,90
"The authors suggest and analyse two types of preconditioners for optimization, a Newton type and a Fisher type preconditioner.

The paper is well written, the analysis is clear and the significance is arguably given. The authors run their optimizers on a synthetic benchmark data set and on imagenet.
The originality is not so high as the this line of research exists for long. 
The ""Lie"" in the title is (technically correct, but) a bit misleading, as only matrix groups were used.
","The sentiment of the review is generally positive, as the reviewer acknowledges that the paper is well-written, the analysis is clear, and the significance is arguably given. However, the reviewer also notes that the originality is not high and finds the title slightly misleading. Therefore, the sentiment score is not fully positive but leans towards the positive side. The language used is polite, as the reviewer provides constructive feedback without being harsh or rude.",60,80
"This paper proposes a method for stereo reconstruction using Deep Learning. Like some previous methods, a 'cost volume' is first computed by plane sweeping, in other words the cost volume is indexed by the 2D locations in the image plane, and the disparities for 3D planes parallel to the image plane. A network then predicts the disparities for each image location from this cost volume.

The contributions with respect to the state-of-the-art are:

- the cost volume is computed using differential warps, thus the network can be trained end-to-end;

- a better cost volume is computed from the original cost volume and the reference image.

The results look good, both quantitatively and qualitatively. The paper reads well, and related work is correctly referenced.

There is nothing wrong with the proposed method, it makes sense and I am convinced it works well. However, I found the contributions quite straightforward, and it is difficult to get excited about the paper.

More details would be welcome for Section 3.2","The sentiment of the review is generally positive, as the reviewer acknowledges that the method works well and the results are good both quantitatively and qualitatively. However, the reviewer also mentions that the contributions are straightforward and not particularly exciting, which tempers the overall enthusiasm. Therefore, the sentiment score is 50. The language used in the review is polite and constructive, with phrases like 'nothing wrong with the proposed method' and 'more details would be welcome,' indicating a polite tone. Therefore, the politeness score is 80.",50,80
"The authors introduce a continuous relaxation for categorical variables so as to utilize the gradient descent to optimize the connection weights and the network architecture. It is a cool idea and I enjoyed the paper. 

One question, which I think is relevant in practice, is the initialization of the architecture parameters. I might be just missing, but I couldn't find description of the initial parameter values. As it is gradient based, it might be sensitive to the initial value of alpha. 

In (5), the subscript for alpha should be removed as it defines a function of alpha. I think (5) is misleading as it is because of k-1. (and remove one ""the"" in ""minimize the the validation"" in the sentence above (5))","The sentiment of the review is positive, as indicated by phrases like 'It is a cool idea and I enjoyed the paper.' This suggests a favorable view of the work. The sentiment score is therefore 80. The politeness of the language is also high, as the reviewer uses polite phrases such as 'I might be just missing' and 'I think,' which soften the critique and make it more constructive. The politeness score is 90.",80,90
"Summary:
The authors present a novel adversarial attack scheme where a neural net is repurposed or ""reprogrammed"" to accomplish a different task than it the one it was originally trained on. This reprogramming from task1 to task2  is done through a given image from task2 additively enhanced with an adversarial program which is trained given the knowledge of the models parameters. A mapping from the repurposed output from task1 to relevant output for taks2 is also necessary (h_g function).

Review:
This approach seems quite novel as it enables the repurposing of ImageNet classifiers to be used for counting dots in images, MNIST and CIFAR10 classifications. This new type of ""adversarial attack"" by repurposing a model shows surprising efficacy at allowing an attacked models to change its task at hand. Some tasks being more difficult (CIFAR10) than MNIST or counting dots.

The paper is well-written and explains clearly the proposed technique. The proposed technique is simple in its formulation.
The assumption it is based on (access to model parameters) is acceptable for the sake of proof of concept.
Overall it is an interesting paper to read and seems of significance for the community working on adversarial attacks.

Few comments/questions come to mind though:
- The adversarial images are quite different from a common image as they embed the program around the new task images. This makes the technique itself quite susceptible to detection (just look at the statistics of the input images).
- How do you handle front end processing? Usually for ImageNet classification, a system will (for instance) resize its input to 256x256, center crop to 224x224 and renormalize the RGB features to match the statistics from the training data. It looks like the images generated are passed as inputs to the system. Do you assume that the front-end steps are not applied or do you assume it is (by including them in the network while training your program W).  My assumption is that you include those steps in the training network for W.
- The size of the program is disproportionately big compare to the task2 embedded image. This begs the question: what happens when you limit the size of the program to a smaller percentage of the whole image? When do you see a break in the reprogramming? Do you need that much extra programming W in your adversarial images?
- As the adversarial images seem to be quite easy to detect, would it be easy to integrate it into some task1 images? The equation (2) gives X_{adv} = \tilda{X} + P, could you use X_{adv} + w * X_{task1}, basically finding a way to hide the program and task2 image within a task1 image. This seems difficult, but have you thought of such approach?

Overall this is a paper that is a pleasant read and should be considered for publication.

Post Rebuttal: The draft paper improves on the original paper and demonstrates possible concealment of the program. I adjusted my rating upward to 8.  ","The sentiment of the review is positive, as indicated by phrases like 'novel adversarial attack scheme,' 'well-written,' 'interesting paper,' and 'pleasant read.' The reviewer also adjusted their rating upward after the rebuttal, further indicating a positive sentiment. Therefore, the sentiment score is 80. The politeness of the language is also high, as the reviewer uses polite and constructive language throughout the review, such as 'Few comments/questions come to mind though' and 'Overall this is a paper that is a pleasant read.' Therefore, the politeness score is 90.",80,90
"This paper presents a novel method for budgeted cost sensitive learning from Data Streams.
This paper seems very similar to the work of Contrado’s RADIN algorithm which similarly evaluates sequential datapoints with a recurrent neural network by adaptively “purchasing” the most valuable features for the current datapoint under evaluation according to a budget. 

In this process, a sample (S_i) with up to “d” features arrives for evaluation.  A partially revealed feature vector x_i arrives at time “t” for consideration.  There seems to exist a set of “known features” that that are revealed “for free” before the budget is considered (Algorithm 1).  Then while either the budget is not exhausted or some other stopping condition is met features are sequentially revealed either randomly (an explore option with a decaying rate of probability) or according to their cost sensitive utility.  When the stopping condition is reached, a prediction is made.  After a prediction is made, a random mini-batch of the partially revealed features is pushed into replay memory along with the correct class label and the P. Q, and target Q networks are updated.

The ideas of using a sequentially revealed vector of features and sequentially training a network are in Contrado’s RADIN paper.   The main novelty of the paper seems to be the use of MC dropout as an estimate of certainty in place of the softmax output layer and the methods of updating the P and Q networks.
The value of this paper is in the idea that we can learn online and in a cost sensitive way.  The most compelling example of this is the idea that a patient shows up at time “t” and we would like to make a prediction of disease in a cost sensitive way.  To this end I would have liked to have seen a chart on how well this algorithm performs across time/history.  How well does the algorithm perform on the first 100 patients vs the last 91,962-91,062 patients at what point would it make sense to start to use the algorithm (how much history is needed).

Am I correct in assuming there are some base features that are revealed “for free” for all samples?  If so how are these chosen?  If so how does the number of these impact the results?  

In Contrado’s RADIN paper the authors explore both the MNIST dataset and others, including a medical dataset “cardio.”  Why did you only use RADIN as a comparison for the MNIST dataset and not the LTRC or diabetes dataset?  Did you actually re-implement RADIN or just take the numbers from their paper?  In which case, are you certain which MNIST set was used in this paper? (it was not as well specified as in your paper).

With respect to the real world validity of the paper, given that the primary value of the paper has to do with cost sensitive online learning, it would have been better to talk more about the various cost structure and how those impact the value of your algorithm.  For the first example, MNIST, the assumed uniform cost structure is a toy example that equates feature acquisition with cost.  The second example uses computational cost vs relevance gain.  This would just me a measure of computational efficiency, in which case all of the computational cost of running the updates to your networks should also be considered as cost.  With respect to the third proprietary diabetes dataset, the costs are real and relevant, however there discussion of these are given except to say that you had a single person familiar with medical billing create them for you (also the web address you cite is a general address and does not go to the dataset you are using). 

 In reality, these costs would be bundled.  You say you estimate the cost in terms of overall financial burden, patient privacy and patient inconvenience.  Usually if you ask the patient to fill out a survey it has multiple questions, so for the same cost you get all the answers.  Similarly if you do a blood draw and test for multiple factors the cost to the patient and the hospital are paid for the most part upfront.  It is not realistic to say that the cost of asking a patient a questions is 1/20th of the cost of the survey.  The first survey question asked would be more likely 90-95% of the cost with each additional question some incremental percentage.  To show the value of your work, a better discussion of the cost savings would be appreciated.             
","The sentiment of the review is mixed but leans towards the negative side. The reviewer acknowledges the novelty of the method and its potential value but raises several critical points about the similarity to existing work, lack of comprehensive evaluation, and issues with cost structures. Therefore, the sentiment score is -30. The politeness of the language is generally neutral to polite. The reviewer asks questions and makes suggestions without using harsh or rude language, so the politeness score is 20.",-30,20
"In the paper, the authors proposed a new algorithm for instance-wise feature selection. In the proposed algorithm, we prepare three DNNs, which are predictor network, baseline network, and selector network. The predictor network and the baseline networks are trained so that it fits the data well, where the predictor network uses only selected features sampled from the selector network. The selector network is trained to minimize the KL-divergence between the predictor network and the baseline network. In this way, one can train the selector network that select different feature sets for each of given instances.

I think the idea is quite simple: the use of three DNNs and the proposed loss functions seem to be reasonable. The experimental results also look promising.

I have a concern on the scheduling of training. Too fast training of the predictor network can lead to the subotpimal selection network. I have checked the implementations in github, and found that all the networks used Adam with the same learning rates. Is there any issue of training instability? And, if so, how we can confirm that good selector network has trained?

My another concern is on the implementations in github. The repository originally had INVASE.py. In the middle of the reviewing period, I found that INVASE+.py has added. I am not sure which implementations is used for this manuscript. It seems that INVASE.py contains only two networks, while INVASE+.py contains three networks. I therefore think the latter is the implementation used for this manuscript. If this is the case, what INVASE.py is for?
I am also not sure if it is appropriate to ""communicate"" through external repositories during the reviewing period.","The sentiment of the review is generally positive, as the reviewer acknowledges the reasonableness of the proposed algorithm and the promising experimental results. However, the reviewer also raises some concerns about the training schedule and the use of external repositories during the review period. Therefore, the sentiment score is not fully positive but leans towards the positive side. The politeness of the language is high, as the reviewer uses polite phrases such as 'I think,' 'I have a concern,' and 'I am not sure,' which indicate a respectful tone while pointing out issues.",60,80
"
In this work, the authors considers a variation of GAN by consider simultaneously decrease the probability that real data is real for the generator. To include such a property, the authors propose a relativistic discriminator which estimate the probability that the given real data is more realistic than the fake data. Numerical results are performed to show that the proposed methods are effective, and the resulting GANs are relatively more stable and generate higher quality data samples than their non-relativistic counterparts.

Overall the paper is well written and the rationale behind the proposed modification is clear. In particular, the authors use three different perspective, (the prior knowledge, the divergence minimization, and the gradient expressions), to explain what they thought is missing in the state-of-the-art. By proposing to utilize the information about both real and fake data in the discriminator definition, the authors’ have (to some extent) alleviated the above shortcoming of the state-of-the-art.  Unfortunately, like almost all papers related to the field,  there has been no rigorously justification behind the proposed methods. 

The English of the paper has to be significantly improved. For example, grammar errors like “this mean….”, “didn’t converge, …”

Unfortunately, the codes of the paper is not released, I will encourage the authors to do so. 
","The sentiment of the review is generally positive, as the reviewer acknowledges that the paper is well-written and the rationale behind the proposed modification is clear. However, the sentiment is slightly tempered by the mention of the lack of rigorous justification and the need for significant improvement in English. Therefore, the sentiment score is 50. The politeness score is 50 as well, as the reviewer uses polite language but also includes some direct and critical feedback, such as the need for significant improvement in English and the encouragement to release the code.",50,50
"This paper proposes an algorithm to approximate kernel matrix based on the Taylor expansion of the element-wise functions. The authors provide a spectral norm based error bound for their method and the corresponding results for the special case, \epsilon-sparse matrix.

I have some comment as follow.

1. Can you provide some comparison with Nystrom methods? It is very popular for kernel approximation and looks more efficient than the proposed algorithm. 

2. The analysis relies on the Gaussian assumption on the input matrix. Can we extend it to more general case?

3. In section 5, the paper said “as our method consists in computing the sparse eigenvectors of a p \times p matrix which can be done by power method, the complexity of estimating the principal component is about O(ps) where s is the sparsity level”. The time complexity of the proposed algorithm is not clearly.
a) Is there any bound for the sparsity level s? Why the eigenvectors of p \times p matrix is sparse?
b) The convergence of power method is heavily affected by the eigen-gap of the matrix. Is there any theoretical or empirical result for the convergence behavior of power method on approximate matrix and original matrix?

","The sentiment of the review appears to be neutral to slightly positive. The reviewer acknowledges the proposed algorithm and its contributions but raises several critical questions and requests for additional information. This suggests a sentiment score of around 10. The language used in the review is polite and professional, with phrases like 'Can you provide' and 'Is there any,' indicating a politeness score of 80.",10,80
"This work analyses the information bottleneck (IB) method applied to the supervised learning of a deterministic rule Y=f(X).

The idea as I understood it is as follows:
1) In a first section the authors discuss the relationship between supervised learning through minimization of the empirical cross entropy and the maximization of the empirical mutual information with an intermediate latent variable T. 
2) They show that in the case of a deterministic rule, the information bottleneck curve has a simple shape, piecewise linear, and is not strictly concave. 
3) They show that the optimization of the IB Lagrangian for different \beta does not lead to a point by point exploration of the IB curve.
4) They propose a cure to the previous issue by introducing the squared IB Lagrangian. 
5) They exhibit uninteresting representations (noisy versions of the output Y) that are on the IB curve.
6) They show that multiple successive representations (like in DNNs), have identical predicting power (mutual information with output Y) when they allow for perfect prediction. 
7) They use the IB method to train a neural net on MNIST, using the Kolchinsky estimate of the mutual informations. 
	- they show that the optimization of the squared IB reaches more different points on the IB curve,
	- but that these representations are possibly uninteresting (hard clustering of uneven numbers of grouped classes) 
	- they show that for large enough value of beta, zero error is reached. 

The necessity of noise in the IB theory has been already pointed out by (Gilad-Bachrach et al., 2003; Shwartz-Ziv et al.  2017), although the more thorough analysis proposed here is novel. In practice, besides a few recent propositions (Kolchinsky et al., 2017; Alemi et al., 2016; Chalk et al., 2016) the IB Lagrangian is not a usual objective function for supervised learning. The motivation and impact of this work studying deterministic rules is therefore not completely convincing. 

Further pros and cons:

Pros:
- The discussion is generally well written. 
- This work provides in depth clarification of the counter-intuitive behaviors of the IB method in the case to the learning of a deterministic rule. 
- These are demonstrated with experiments conducted on the MNIST dataset for concreteness.

Cons:
- The fact that multiple successive representations have identical predicting power when the prediction error is zero, was already observed for example in Shwartz-Ziv et al.  2017. It is not clear why this should be considered as an issue. It also seems to be a straightforward observation when restricting to the empirical measure on the training set. 
- The fact that the entire IB curve is not explored point by point by the IB Lagrangian is not necessarily an issue for learning. In the experiments of the present paper, the results seem to suggest that the interesting intermediate representations (separation in 10 compact clusters of the MNIST classes) is actually easier to obtain (large range of \beta) optimizing the IB Lagrangian rather than the proposed squared IB Lagrangian. 

Questions:
- Do the authors know of an application where the full probing of the IB curve would be necessary?
- In Section 2, when injecting the decomposition of the prediction density q(y|x) over the intermediate variable t in eq (3) was a Jensen inequality replaced by an equality?
","The sentiment of the review appears to be slightly positive, as the reviewer acknowledges the novelty and thoroughness of the analysis, as well as the clarity of the discussion. However, the reviewer also expresses some reservations about the motivation and impact of the work, and points out that some observations are not new or particularly problematic. Therefore, the sentiment score is 20. The politeness of the language is quite high, as the reviewer uses polite and constructive language throughout the review, even when pointing out cons and asking questions. Therefore, the politeness score is 80.",20,80
"The authors present a method for generating points clouds with the help of graph convolution and a novel upsampling scheme. The proposed method exploits the pairwise distances between node features to build a NN-graph. The upsampling scheme generates new points via a slimmed down graph convolution, which are then concatenated to the initial node features. The proposed method is evaluated on four categories of the ShapeNet dataset. Resulting point clouds are evaluated via a qualitative and quantitative comparison to r-GAN.

As far as I know, the paper introduces an overall novel and interesting idea to generate point clouds with localized operations.


The following questions could be addressed by the authors in a revised manuscript:

* The upsampling operation is not well motivated, e.g., neighboring node features are weighted independently, but root node features are not. What is the intuition besides reducing the number of parameters? Are there significant differences when not using diagonal weight matrices?
* As computation of pairwise node feature distances and graph generation based on nearest neighbors are expensive tasks, more details on the practical running time and theoretical complexity should be provided. Can the complexity be reduced by rebuilding graphs only after upsampling layers? How would this impact the performance of the proposed model?
* Although the evaluation on four categories is reported, Table 2 only gives results for two categories.
* How is the method related to GANs which generates graphs, such as GraphGAN or NetGAN?","The sentiment of the review is generally positive, as indicated by the statement 'the paper introduces an overall novel and interesting idea to generate point clouds with localized operations.' This suggests that the reviewer finds the work innovative and valuable. However, the review also includes several critical questions and suggestions for improvement, which slightly temper the overall positivity. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, such as 'could be addressed' and 'more details should be provided.' There are no rude or harsh comments, so the politeness score is 90.",60,90
"This paper proposed an adversarially regularized AE algorithm that improve interpolation in latent space. Specifically, a critic is used to predict the interpolation weight \alpha and encourage the interpolated images to be more realistic. The paper verified the method on a newly proposed synthetic line benchmark and on downstream classification and clustering tasks.

Pros:
1.	A novel algorithm that promotes the interpolation ability of AE
2.	A new synthesized line benchmark to verify the interpolation ability of different AE variants
3.	Strong results on downstream classification and clustering tasks

Cons: 
1.	The interplay of the adversarial network (between AE and critic) isn’t very clear and can be improved
2.	Eq. 1, should x be x_1 or a new data other than x1 and x2?
3.	The paper states that the 2nd term of Eq. 1 isn’t crucial. If x is a new data (other than x1 or x2), how can the critic infer \alpha without a reference to x1 or x2?
4.	The paper states that “encouraging this behavior also produce semantically smooth interpolation …”. Besides the empirical evidences from data, it would be better to any some theoretical justifications.
","The sentiment of the review is generally positive, as indicated by the pros listed and the constructive nature of the cons. The reviewer acknowledges the novelty and strong results of the proposed algorithm, which suggests a positive sentiment. However, there are some critical points that need clarification, which slightly tempers the overall positivity. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, providing specific feedback without being harsh or dismissive. Thus, the politeness score is 90.",60,90
"The paper proposes a method for inverse reinforcement learning based on AIRL. It's main contribution is that the shaping function is not learned while training the discriminator, but separately as an approximation of the empowerment (maximum mutual information). This shaping term aims to learn disentangled rewards without being restricted to learning state-only reward functions, which is a major restriction of AIRL.

The main weakness of the paper is, that it does not justify or motivate the main deviations compared to AIRL. The new objective for updating the policy is especially problematic because it does no longer correspond to the RL objective but includes an additional term that biases the policy towards actions that increase its empowerment. Although both terms of the update can be derived independently from an IRL and Empowerment perspective respectively, optimizing the sum was not derived from a common problem formulation. By combining these objectives, the learned reward function may lead to policies that fail to match the expert demonstration without such bias. This does not imply that the approach is not sound per se, however, simply presenting such update without any discussion is insufficient--especially given that it constitutes the main novelty of the approach. I think the paper would be much stronger if the update was derived from an empowerment-regularized IRL formulation. And even then, the implications of such bias/regularization would need to be properly discussed and evaluated, in particular with respect to the trade-off lambda, which--again--is hardly mentioned in the submission. I'm also not sure if the story of the paper works out; when we simply want to use empowerment as shaping term, why not use two separate policies for computing the empowerment and reward function respectively. Is the bias in the policy update maybe more important than the shaping term in the discriminator update for learning disentangled rewards?

Keeping these issues aside, I actually like the paper. It tackles the main drawback of AIRL and the idea seems quite nice. Having a reward function that does not actively induce actions that can be explained by empowerment, may not always be appropriate, but often enough it may be a sensible approach to get more generalizable reward functions. The paper is also well written with few typos. The parts that are discussed are clear and the experimental results seem fine as well (although more experiments on the reward transfer would be nice).

Minor notes:
I think there is a sign error in the policy update
Typo in the theorem, grantee should be guarantee

Question:
Please confirm that the reward transfer was learned with a standard RL formulation. Does the learned policy change, when we use the empowerment objective as well?



Update (22.11)
I think that the revised version is much better than the original submission because it now correctly attributes the improved generalization to an inductive bias in the policy update.  However, the submission still seems borderline to me. 

- The proposed method uses the empowerment both for regularization as well as for reward shaping, but it is not clear whether the latter improves generalization. If the reward shaping was not necessary, it would be cleaner to use empowerment only for regularization. If the reward shaping is beneficial, this should be shown in an ablative experiment.

- The benefit of using empowerment (whether for reward shaping or for regularization) should be discussed. Empowerment for generalization is currently hardly motivated.

- The derivation could be a bit more rigorous.

As the presentation is now much more sound, I slightly increased my rating.","The sentiment of the review is mixed but leans towards positive. The reviewer acknowledges the strengths of the paper, such as tackling the main drawback of AIRL and presenting a nice idea, but also points out significant weaknesses, particularly the lack of justification for deviations from AIRL and the need for more rigorous derivation and discussion. The sentiment score is therefore 20. The politeness of the language is high; the reviewer uses polite and constructive language, offering suggestions for improvement and acknowledging the positive aspects of the paper. The politeness score is 80.",20,80
"The paper presents a coupled deep learning approach for generating realistic liquid simulation data that can be useful for real-time decision support applications. While this is a good applied paper with a large variety of experimental results, there is a significant lack of novelty from a machine learning perspective. 

1. The primary novelty here is in the problem formulation (e.g., defining cost function etc.) where two networks are used, one for learning appropriate deformation parameters and the other to generate the actual liquid shapes. This is an interesting idea to generate the required training data and build a generalizable model. 

2. But based on my understanding, this does not really explicitly incorporate the physical laws within the learning model and can't guarantee that the generated data would obey the physical laws and invariances. So, this is closer to a graphics approach and deep learning has been used before extensively in a similar manner for shape generation, shape transformation etc.    

3. In terms of practical applications, to the best of my knowledge there are sophisticated physics-based and graphics based approaches that perform very fast fluid simulations. So, the authors need to provide accuracy and computation cost/time comparisons with such methods to establish the benefits of using a deep learning based surrogate model.   

xxxxxxxxxxxxxxxxxxx

I appreciate the rebuttals from the authors, updated my score, but I still believe (just like another reviewer) that this is better suited for a workshop or a conference like SIGGRAPH. ","The sentiment of the review is mixed. The reviewer acknowledges the practical applicability and the variety of experimental results, which is positive. However, they also point out a significant lack of novelty from a machine learning perspective and express concerns about the incorporation of physical laws and the need for comparisons with existing methods. This results in a sentiment score of around -20. The politeness of the language is quite high; the reviewer uses polite phrases such as 'I appreciate the rebuttals' and 'the authors need to provide,' which indicates a respectful tone. Therefore, the politeness score is around 80.",-20,80
"Summary:
The paper presents techniques for training a non expansive network, which keeps the Lipchitz constant of all layers lower than 1. While being non-expansive, means are taken to preserve distance information better than standard networks. The architectural changes required w.r.t standard networks are minor, and the most interesting changes are made to the loss minimized. The main claim of the paper is that the method is robust against adversarial attacks of a certain kind. However, the results presented show that a) such robustness comes at a high cost of accuracy for standard examples, and b) even though the network is preferable to a previous alternative in combating adversarial examples, the accuracy obtained in the face of adversarial attacks is too low to be of practical value. Other properties of the networks, explored empirically, are that the confidence of the prediction is indicative of robustness (to adversarial attacks) and that the networks learn better in the presence of high label noise. 
In short, this paper may be of interest to a sub-community interested in defense against certain types of adversarial attacks, even when the defense level is much too low to be practical. I am not part of this community, hence did not find this part very interesting. I believe the regularization results are of wider interest. However, to present this as the main contribution of L2NNN more work is required to find configuration which are resilient to overfit yet enable high training accuracy, and more diverse experiments are required.
Pros:
+ the idea of non expansive network is interesting and important
+ results indicate some advantages in fighting adversarial examples and label noise
Cons:
- the results for fighting adversarial examples are not significant from a practical perspective
- the results for copying with label noise are preliminary and require expansion with more experiments.
- the method has costs in accuracy, which is lower than standard networks and this issue is not faced with enough attention
- presentation clarity is medium: proofs for claims are missing, as well as relevant background on the relevant adversarial attacks. The choice to place the related work at the end also reduces presentation clarity.

More detailed comments:
Pages 1-3: In many places, small proofs are left to the reader as ‘straightforward’. Examples are: the claim in the introduction, in eq. 2, in section 2.2, section 2.3’ last line of page 3, etc.. While the claim are true (in the cases I tried to verify them long enough), this makes reading difficult and not fluent. For some of these claims I do not see the argument behind them. In general, I think proofs should be brought for claims, and short proofs (preferably) should be brought for small claims. Leaving every proof to the reader as an exercise is not a convenient strategy. 
Page 4: The loss is complex and its terms utility require empirical evidence. The third term is shown to be clearly useful, enabling a trade off between train accuracy and margin. However, the utility of terms 4) and 5) is not verified. Do we really need both these terms? Cannot we just stay with one?
The main claim is robustness w.r.t “white-box non targeted L2-bounded attacks”. This seems to be a very specific attack type, and it is not explained at all in the text. Hence it is hard to judge the value of this robustness. Explanation of adversarial attack kinds, and specifically of “white-box non targeted
L2-bounded attacks” is required for this paper to be a stand alone readable paper. Similarly ‘L_\infty’-bounded attacks, for which results are shown, should be explained.
Table 1,2: First, the model architecture used in these experiments is not stated. Second, the accuracy of the ‘natural’ baseline classifier, at least in the MNist case, is somewhat low – much better results can be obtained with CNN on MNist. Third, the accuracies of the suggested robust models are very low compared to what can be obtained on these datatsets. Forth, while the accuracies under attack of the proposed method are better than those of Madri et al., both are quite poor and indicate that the classifier is not useful under attack (from a practical perspective).
Page 6: The classifiers which share the work between an L2NNN network and a regular more accurate network may be interesting, as the accuracies reported for them are significantly higher than the L2NNN networks. However, the robustness scores are not reported for these classifiers, so it is not possible to judge if they lead to a practical and effective strategy.
Page 7: For me, the results with partially random labels are the most interesting in the paper. The resistance of L2NNN to overfit and its ability to learn with very noisy data are considerably better than the suggested alternatives.
Relevant work not mentioned “Spectral Norm Regularization for Improving the Generalizability of Deep Learning” - Yuichi Yoshida and Takeru Miyato, Arxiv, 2017.

I have read the rebuttal.
The discussion was interesting, but I do not see a need to change my assessment.
The example of ad-blocking in indeed a case (the first I encounter) where l2- perturbated adversarial examples can be useful for cyber attack. The other ones are less relevant (the attacks are not based on adversarial attacks in the sense used in the paper: images crated with small gradient-direction perturbations). Anyway talking about 'attacks on a self-driving car' are still not neaningful to me: I do not understand what adversarial examples have to do with this.
I do not find the analogy of 'rocket improvements and moon landing' convincing: in 69 rocket improvements were of high interest in multiple applications, and moon landing was visible over the corner. 

","The sentiment of the review is mixed but leans towards the negative side. The reviewer acknowledges some positive aspects of the paper, such as the interesting idea of a non-expansive network and some advantages in fighting adversarial examples and label noise. However, the reviewer also points out several significant drawbacks, including the impracticality of the results, the preliminary nature of the findings, and issues with presentation clarity. Therefore, the sentiment score is -30. The politeness of the language is generally respectful and constructive, even when pointing out flaws. The reviewer provides detailed feedback and suggestions for improvement without using harsh or dismissive language. Thus, the politeness score is 70.",-30,70
"This paper shows that deep ""narrow"" neural networks (i.e. all hidden layers have maximum width at most the input dimension) with a variety of activation functions, including ReLU and sigmoid, can only learn functions with unbounded level set components, and thus cannot be a universal approximator. This complements previous work, such as Nguyen et. al 2018 which study connectivity of decision regions and Lu et. al 2017 on ReLU networks in different ways.

Overall the paper is clearly written and technically sound. The result itself may not be super novel as noted in the related work but it's still a strict improvement over previous results which is often constrained to ReLU activation function. Moreover, the proofs of this paper are really nice and elegant. Compared to other work on approximation capability of neural networks, it can tell us in a more intuitive way and explicitly which class of functions/problems cannot be learned by neural networks if none of their layers have more neurons than the input dimension, which might be helpful in practice. Given the fact that there are not many previous work that take a similar approach in this direction, I'm happy to vote for accepting this paper.  

Minor comments:
The proof of Lemma 3 should be given for completeness. I guess this can be done more easily by setting delta=epsilon, A_0=A and A_{i+1}=epsilon-neighborhood of f_i(A_i)?
page7: the square brackets in ""...g(x'')=[y-epsilon,y+epsilon]..."" should be open brackets.
page7:""By Lemma 4, every function in N_n has bounded level components..."" -> ""..unbounded...""","The review expresses a positive sentiment towards the paper, highlighting that it is clearly written, technically sound, and provides a strict improvement over previous results. The reviewer appreciates the elegance of the proofs and the practical implications of the findings. The minor comments are constructive and polite, suggesting improvements without any negative tone.",80,90
"This paper proposed an approximation technique to learn the large-scale graph with the desired edge density. It was well-written and contains thorough experimental results and analysis.

A minor drawback is that while this work was motivated by the use of k-NN graph in graph convolution network (GCN), there was no evidence on how well A-NN performs in compare to k-NN with GCN.","The sentiment of the review is positive as it praises the paper for being well-written and containing thorough experimental results and analysis. This is reflected in the first sentence. The minor drawback mentioned does not significantly detract from the overall positive sentiment. Therefore, the sentiment score is 80. The politeness of the language is high, as the reviewer uses polite and constructive language, even when pointing out the minor drawback. The use of phrases like 'A minor drawback' and 'while this work was motivated by' indicates a polite tone. Therefore, the politeness score is 90.",80,90
"This work extends on [1] by constructing CNN filters using Fourier-Bessel (FB) bases for rotation equivariant networks. Additionally to [1] it extends the process with using SO(2) bases which allow to learn combination of rotated FB bases and ultimately achieve good performance with less parameters than standard CNN networks thanks to filter truncation.

In general, this work is well written and shows interesting results. However it lacks context with regards to other existing works. For example [2] also uses steerable filters for achieving rotation equivariance, however with different steerable bases (rotation harmonics instead of FB). It would be useful to clarify why FB bases are more appropriate for truncation, eventually providing empirical evidence (even though rotation harmonics would probably need more parameters). Authors mention [2], however disregard it due to computational complexity, which would be the same if the rotation harmonics bases were truncated as well.

Similarly, this work is not strong in evaluating against existing methods. It provides evaluation of the vanilla group equivariant networks in a similar configuration, but due to design choices in the training and test set, it is not possible to compare it against other algorithms and other steerable bases such as those from [2]. This degrades the results slightly as it does not allow to verify the baseline results from other works.

Additionally, it would be useful to provide an ablation study which would show how important the bases in SO(2) are important for the model accuracy. This would allow to compare the results against the [1] as the FB filters are steerable as well (Equation 4).

It is hard to reach a final rating for this submission. On one hand, it can be seen as an incremental improvement of [1] for a new domain of tasks, without a thorough comparison against existing methods. On the other hand, the paper is well written and the results look promising - evaluation verifies that the algorithm performs well in multiple tasks with a fraction of parameters.

Considering that authors plan to release the source code and that this conference aims for publishing novel ideas (and the goal of this work is to achieve rotation equivariance with less parameters, which hasn't been tackled before), I am inclined towards acceptance of this paper, even though the experiments can be significantly improved.

Unfortunately, I was not able to verify correctness of the provided proofs.

Additional minor issues:
* The paper does not specify what FB bases exactly are being used (such as in [table 1;1]), mainly it does not seem to specify the SO(2) bases.
* It would be useful to visualise K and K_\alpha in Figure 1.
* Citations, if not part of the sentence, should be in parentheses to improve readability (\citep for natbib).
* On page 8, end of first paragraph - wrong reference (see S.M.)
* L, in section 2.3 is not defined.

[1] Qiu, Qiang, et al. ""DCFNet: Deep Neural Network with Decomposed Convolutional Filters."", ICML 2018
[2] Weiler, Maurice, et al. “Learning Steerable Filters for Rotation Equivariant CNNs.” CVPR 2018
","The sentiment of the review is generally positive, as the reviewer acknowledges that the work is well-written and shows promising results, despite noting some areas for improvement. This is reflected in the inclination towards acceptance of the paper. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer provides constructive feedback in a respectful and professional manner, using phrases like 'it would be useful' and 'I am inclined towards acceptance.' Thus, the politeness score is 80.",60,80
"Summarization:
This paper presents a framework (called FX Network) of quantizing the weights and gradients of neural networks, based on five quantization criteria proposed in literature. The proposed framework can quantize the neural network obtaining a minimal or close-to-minimal error for a pre-specified precision level.


Pros:
- The proposed FX network can quantize all variables including both network weights and back-propagated gradients.
- Promising results have been obtained. Experimental results on CIFAR have shown that the proposed quantization framework had reduced the representational cost, computational cost, and the communication  by up to 6x, 8x, and 4x, respectively, compared to the 32-b FL baseline and related works.
- The paper is well written.




Cons:
- The experiment results showed in Figure 3 are quite confusing: why do the curves of the test error and loss suddenly drop at epoch 100? Explanation is needed. 

- This proposed quantization method require to pre-train a network with high precision in advance, similarly as the student-teacher framework or knowledge distillation. Different from BN and TG, FX network requires to pre-train a 32-b floating-point network, which requires more extra computational costs. 

- How does the quantization method compare with strategies like parameter pruning and sharing? It is better to see a discussion with them. It is also suggested to show the improvement of the proposed framework in terms of inference time during test. 

","The sentiment of the review is generally positive, as indicated by the praise for the framework's ability to quantize all variables, the promising experimental results, and the well-written nature of the paper. However, there are some critical points raised, such as the confusing experimental results in Figure 3, the additional computational costs required for pre-training, and the lack of comparison with other strategies like parameter pruning and sharing. These criticisms are constructive and aimed at improving the paper. The language used is polite and professional, with suggestions for improvement rather than harsh criticism.",70,80
"This paper provides theoretical analysis for two kinds of straight-through estimation (STE) for activation bianrized neural networks. It is theoretically shown that the ReLU STE has better convergence properties the identity STE,  by studying the properties of the orientation and norm of the course gradients for STE.

While the paper presents many theoretical results which might be useful for the community, they are not organized very well.  It is a bit hard for readers to quickly find the most important theoretical results.  Moreover, some symbols are used without definition, e.g. g_{relu} is used before being defined in sec 3.1. The discussions for most theoretical results are very short or not organized well, making the whole paper hard to follow,  e.g., ""the key observation ..."" after Lemma 4 is actually not about the Lemma 4 above, but Lemma 5 in the next Lemma.  Another major concern is that activation quantization is usually used in combination with weight quantization.  It would be more useful if weight and activation quantizations can be analyzed together.

Clarity in the experiment part can also be further improved. From Table 1, the clipped ReLU STE has the best performance, however, there is no theoretical analysis for it. For ResNet-20 with 2-bit activation, the training loss/accuracy results of vanilla ReLU is much worse than clipped ReLU, is there any explanation for this?  For the discussion in sec 4.2, what information does it want to convey?  What is the ""normal schedule of learning rate""? What if the small learning rate 1e-5 is kept after 20 epochs?

Typo: The last sentence on page 3, the definition of y*.

------------------------

The author response have addressed most of my concerns. Thus I have increased my score. ","The sentiment of the review is mixed but leans towards positive, especially after the author response addressed most concerns. Initially, the reviewer acknowledges the theoretical contributions of the paper but points out several organizational and clarity issues, which suggests a sentiment score of around 20. The politeness of the language is quite high, as the reviewer provides constructive feedback without being harsh or dismissive, and even acknowledges improvements made after the author response, suggesting a politeness score of 80.",20,80
"This paper introduces a method for handing input data that is defined on irregular mesh-type domains. If I understand it correctly, the core technique is  to perform Fourier analysis to transform the input data to the frequency domain, which is then transformed back to a regular domain before applying a standard neural network. The claimed result is that this is better than standard linear interpolations. The key technical contribution is to define FT on points, edges, and meshes (This reviewer appreciates these efforts). Explicit formula are given. However, the paper does not perform convolutions on the input irregular domain directly, which is quite disappointing. The experimental results are preliminary. It is expected to perform evaluation on more applications such as semantic segmentation. 


The major issue of the paper is that the goal was not stated clearly. Does it target for a neural network that is defined on irregular domains or simply a technique to handling irregular domains? Given the Fourier transform, it is possible to define convolutions directly as multiplications in the Fourier domain....the paper can be more interesting, if it follows this line.

Overall, it is hard to champion the paper based on the current technical approach and the experimental results. 
","The sentiment of the review is mixed but leans towards the negative side. The reviewer acknowledges the efforts and technical contributions of the paper but expresses disappointment with the lack of direct convolutions on the irregular domain and the preliminary nature of the experimental results. The sentiment score is therefore -40. The language used in the review is generally polite, with phrases like 'This reviewer appreciates these efforts' and 'it is expected to perform evaluation on more applications,' indicating a constructive tone. The politeness score is 60.",-40,60
"The paper presents several ways to regularize plain ReLU networks to optimize 3 things

- the adversarial robustness, defined as the fraction of examples for which adversarial perturbation exists
- the provable adversarial robustness, defined as the fraction of examples for which some method can show that there exists no adversarial example within a certain time budget
- the verification speed, i.e. the amount of time it takes some method to verify whether there is an adversarial example or not
 
Overall, the ideas are sound and the analysis is solid. My main concern is the comparison between the authors method and the 'certification' methods, both conceptually and regarding performance.

The authors note that their method falls under 'verification', whereas many competing methods fall under 'certification'. They point to two advantages of verification over certification: (1) the ability to provide true negatives, i.e. prove that an adversarial example exists when it does, and (2) certification requires that 'models must be trained and optimized for a specific certification method'. However, neither argument convinces me regarding the utility of the authors method. 

Regarding (2): The authors method also requires training the network in a specific way (with RS loss), and it is only compatible with verifiers that care about ReLU stability. 

Regarding (1): It is not clear that this would be helpful at all. Is it really that much better if method A has 80% proven robustness and 20% proven non-robustness versus method B that has 80% proven robustness and 20% unknown? One could make the case that method B is actually even better.

So overall, I think one has to compare the authors method and the certification methods head-to-head. And in table 3, where this is done, Dvijotham comes out on top 2 out of 2 times and Wong comes out on top 2 out of 4 times. That does not seem convincing. Also, what about the performance numbers form other papers discussed in section 2?

-------

Other issues:

At first glance, the fact that the paper only deals with (small) plain ReLU networks seems to be a huge downside. While I'm not familiar with the verification / certification literature, from reading the paper, I suspect that all the other verification / certification methods also only deal with that or highly similar architectures. However, I will defer to the other reviewers if this is not the case.

To expand upon my comment above, I think the paper should discuss true adversarial accuracy on top of provable adversarial robustness. Looking at table 1, for instance, for rows 2, 3 and 4, it seems that the verifier used much less than 120 seconds on average. Does that mean the verifier finished for all test examples? And wouldn't that mean that the verifier determined for each test example exactly whether an adversarial example existed or not? In that case, I would write ""true adversarial accuracy"" instead of ""provable adversarial accuracy"" as column header. If the verifiers did not finish, I would include in the paper for how many examples the result was ""adverarial example exists"" and for how many the result was ""timeout"". I would also include that information in table 3, and I would also include proving / certification times there. 

Based on the paper, I'm not quite sure whether the idea of training with L1 regularization and/or small weight pruning and/or ReLU pruning for the purpose of improving robustness / verifiability was an original idea of this paper. In either case, this should be made clear. Also, the paper seems to use networks with adversarial training, small weight pruning, L1 and ReLU pruning as its baseline in most cases (all figures except table 1). If some of these techniques are original contributions, this might not be an appropriate baseline to use, even if it is a strong baselines.

Why are most experiments presented outside of the ""experiments"" section? This seems to be bad presentation.

I would include all test set accuracy values instead of writing ""its almost as high"". Also, in table 3, it appears as if using RS loss DOES in fact reduce test error significantly, at least for CIFAR. Why is that?

While, again, I'm not familiar with the background work on verification / certification, it appears to me from reading this paper that all known verification algorithms perform terribly and are restricted to a narrow range of network architectures. If that is the case, one has to wonder whether that line of research should be encouraged to continue.

--------

Minor issues:

- ""our focus will be on the most common architecture for state-of-the-art models: k-layer fully-connected feed-forward DNN classifiers"" Citation needed. Otherwise, I would suggest removing this statement.
- ""such models can be viewed as a function f(.,W)"" - you also need to include the bias in the formula I think
- ""convolutional layers can be represented as fully-connected layers"". I think what you mean is ""convolutional layers can be represented as matrix multiplication""
- could you make the difference between co-design and co-training more clear?
- The paper could include in the appendix a section outlining the verification method of Tjeng","The sentiment of the review is mixed. The reviewer acknowledges that the ideas are sound and the analysis is solid, which is positive. However, they express significant concerns about the comparison between the authors' method and certification methods, which is negative. Therefore, the sentiment score is slightly positive. The language used in the review is generally polite and constructive, with the reviewer providing detailed feedback and suggestions for improvement without being rude or dismissive.",20,80
"This paper presents an incremental extension to the Self-imitation paper by Oh, Junhyuk, et al. The previous paper combined self-imitation learning with actor-critic methods, and this paper directly integrates the idea into the generative adversarial imitation learning framework.

I think the idea is interesting, but there remains some issues very unclear to me. In the algorithms, when updating the good trajectory buffer, it is said ""We define ‘good trajectories’ as any trajectories whose the discounted sum of rewards are higher than that of the policy"". What does ""that of the policy"" mean? How do you know the reward of the policy?

Second, without defining good trajectories, I don't think Algorithm 1 would work. Algorithm ` 1 misses the part of how to update buffer B. After introducing their own algorithm, the author did not provide much solid proof or analysis for why this self-imitation learning works.

In the experiment section, the author implemented GASIL for various applications and presented reasonable results and compared them with other methods. Nevertheless, without theoretical proof, it is hardly convincing that the results could be consistently reproduced instead of being merely accidental for some applications.

Update:
The rebuttal resolves some of my concerns. However, I still think the contribution is incremental. The current version looks too heuristic, more theoretical analysis or inspirations need to be added.
","The sentiment of the review is mixed but leans slightly negative. The reviewer acknowledges that the idea is interesting and that the rebuttal resolves some concerns, but they also point out several significant issues and express doubts about the contribution's significance and theoretical grounding. Therefore, the sentiment score is -20. The politeness of the language is generally respectful and constructive, with the reviewer providing specific feedback and suggestions without using harsh or dismissive language. Thus, the politeness score is 80.",-20,80
"In this paper the authors propose DL2 a system for training and querying neural networks with logical constraints

The proposed approach is intriguing but in my humble opinion the presentation of the paper could be improved. Indeed I think that the paper is bit too hard to follow. 
The example at page 2 is not clearly explained.

In Equation 1 the relationship between constants S_i and the variables z is not clear. Is each S_i an assignment to z?

I do not understand the step from Eq. 4 to Eq. 6. Why does arg min become min?

At page 4 the authors state ""we sometimes write a predicate \phi to denote its indicator function 1_\phi"". I’m a bit confused here, when is the indicator function used in equations 1-6?

What kind of architecture is used for implementing DL2? Is a feedforward network used? How many layers does it have? How many neurons for each layer? No information about it is provided by authors.

It is not clear to me why DL2/training is implemented in PyTorch and DL2/querying in TensorFlow. Are those two separate systems? And why implementing them using different frameworks?

In conclusion, I’m a bit insecure about the rating to give to this paper, the system seems interesting, but several part are not clear to me.

[Minor comments]
It seems strange to me to use the notation L_inf instead of B_\epsilon to denote a ball.

In theorem 1. \delta is a constant, right? It seems strange to me to have a limit over a constant.","The sentiment of the review is mixed but leans slightly positive. The reviewer finds the proposed approach intriguing and interesting, but also points out several areas where the paper is unclear or lacking in detail. This results in a sentiment score of around 10. The language used in the review is polite and constructive, with phrases like 'in my humble opinion' and 'I’m a bit confused here,' which indicate a respectful tone. Therefore, the politeness score is 80.",10,80
"The authors present an architecture search method where connections are removed with sparse regularization. It produces good network blocks relatively quickly that perform well on CIFAR/ImageNet.

There are a few grammatical/spelling errors that need ironing out.

e.g. ""In specific"" --> ""Specifically"" in the abstract, ""computational budge"" -> ""budget"" (page 6) etc.

A few (roughly chronological comments).

- Pioneering work is not necessarily equivalent to ""using all the GPUs""

- There are better words than ""decent"" to describe the performance of DARTS, as it's very similar to the results in this work!

- From figure 2 it's not clear why all non-zero connections in (b) are then equally weighted in (c). Would keeping the non-zero weightings be at all helpful?

-  Why have you chosen the 4 operations at the bottom of page 4? It appears to be a subset of those used in DARTS.

- How do you specifically encode the number of surviving connections? Is it entirely dependent on budget?

- You should add DARTS 1st order to table 1. 

- Measuring in GPU days is only meaningful if you use the same GPU make for every experiment. Which did you use?

- The ablation study is good, and the results are impressive.

I propose a marginal acceptance for this paper as it produces impressive results in what appears to be a short amount of search time. However, the implementation details are hazy, and some design choices (which operations, hyperparameters etc.) aren't well justified.

------------
UPDATE: Score changed based on author resposne
------------
","The sentiment of the review is generally positive, as the reviewer acknowledges the impressive results and the short search time of the proposed method. However, the sentiment is tempered by the mention of several issues that need addressing, such as grammatical errors, unclear implementation details, and some unjustified design choices. Therefore, the sentiment score is not fully positive but leans towards the positive side. The politeness of the language is quite high; the reviewer uses polite language and constructive criticism, offering specific suggestions for improvement without being rude or dismissive.",60,80
"This paper proposes to use GAN to address the image compression problem. It is shown to achieve superior results over the past work in two different settings (GC and SC). 

Novelty:

It has been well discovered in the literature of GANs that they can resolve the problem of blurriness in generation, compared to the traditional MSE loss. This paper proposes to combine a GAN loss with MSE, together with an entropy loss. However similar approaches were used such as video prediction [1] from 2016. The paper lacks a few references like this.

Major questions:

- How do the different loss terms play against each other? The entropy term and the MSE apparently conflict with each other. And how would this affect L_gan? I would like to request some more analysis of this or ablation study on different terms.

- How well does the GAN converge? A plot of G and D loss is often presented with GAN approaches.

- Discrete latent variable is in itself an interesting problem [2]. I see the image compression as a task to discover a discrete latent variable with minimal storage. Perhaps one most important problem is how to estimate the gradient through the discrete bottleneck. But the paper doesn't provide much insights or experiments on this. 

- I'm not fully convinced by the claim of the noise that this paper uses to combine the code can act as a regularizer. Adding the noise makes the decoder output stochastic, but the compression problem seems to be deterministic by nature, unlike many other generation problems.

[1] https://arxiv.org/abs/1511.05440
[2] https://arxiv.org/abs/1711.00937","The sentiment of the review is moderately positive as the reviewer acknowledges the superior results achieved by the proposed method and recognizes the novelty of combining GAN loss with MSE and entropy loss. However, the reviewer also points out several major questions and areas for improvement, indicating a balanced view. Therefore, the sentiment score is 30. The politeness of the language is quite high as the reviewer uses polite phrases such as 'I would like to request,' 'I'm not fully convinced,' and provides constructive feedback without being harsh or dismissive. Thus, the politeness score is 80.",30,80
"Update: Lower the confidence and score after reading other comments. 
===

In this paper, the authors benchmark several RL algorithms on their abilities of generalization. The experiments show interpolation is somehow manageable but extrapolation is difficult to achieve. 

The writing quality is rather good. The authors make it very clear on how their experiments run and how to interpret their results. The experiments are also solid. It's interesting to see that both EPOpt and RL^2, which claim to generalize better, generalize worse than the vanilla counterparts. Since the success rates are sometimes higher with more exploration, could it be possible that the hyperparameters of EPOpt and RL^2 are non-optimal? 

For interpolation/extrapolation tasks, all 5 numbers (RR, EE, DR, DE, RE) are expected since the geometric mean is always 0 once any of the numbers is 0. 

What does ``""KL divergence coefficient"" in RL^2-PPO mean? OpenAI's Baselines' implementation includes an entropy term as in A2C. 
","The sentiment of the review is generally positive. The reviewer appreciates the writing quality, clarity, and solidity of the experiments. They find the results interesting and provide constructive feedback on potential hyperparameter optimization. The politeness of the language is high, as the reviewer uses respectful and considerate language throughout the review.",80,90
"This paper proposes an encoder-decoder model based on the graph representation of inputs and outputs to solve the multi-label classification problem. The proposed model considers the output labels as a fully connected graph where the pair-wise interaction between labels can be modelled.

Overall, although the proposed approach seems interesting, the representation of the paper needs to be improved. Below I listed some comments and suggestions about the paper.

- The proposed model did not actually use any graph structure of input and output, which can potentially mislead the readers of the paper. For instance, the encoder is just a fully connected feed-forward network with an additional attention mechanism. In the same sense, the decoder is also just a fully connected feed-forward network. Furthermore, the inputs and outputs used throughout the paper do not have any graph structure or did not use any inferred graph structure from data. I recommend using any graph-structured data to show that the proposed model can actually work with the graph-structured data (with proper graph notations) or revise the manuscript without graph2graph representation.

- I personally do not agree with the statement that the proposed model is interpretable because it can visualise the relation between labels through the attention. NN is hard to interpret because the weight structure cannot be intuitively interpretable. In the same sense, the proposed model cannot avoid the problem with the nature of black-box mechanism. Especially, multiple weight matrices are shared across the different layers, which makes it more difficult to interpret. Although the attention weights can be visualised, how can we visualise the decision process of the model from end-to-end? The question should be answered to claim that the model is interpretable.

- 2.2.1, 2.2.2, 2.3 shares the similar network layer construction, which can be represented as a new layer of NN with different inputs (or at least 2.2.2 and 2.3 have the same layer structure). It would be better to encapsulate these explanations into a new NN module which can be reused multiple parts of the manuscript for a concise explanation.

- Although the network claims to model the interactions between labels, the final prediction of labels are conditionally independent to each other, whereas the energy based models such as SPEN models the structure of output directly. In that sense, the model does not take into account the structure of output when the prediction is made although the underlying structure seems to model the 'pair-wise' interaction between labels.

- In Table1, if the bold-face is used to emphasise the best outcome, I found it is inconsistent with the result (see the output of delicious and tfbs datasets).

- Is it more natural to explain the encoder first followed by the decoder?","The sentiment of the review is mixed but leans towards the negative side. The reviewer acknowledges that the proposed approach is interesting but points out several significant issues with the representation and claims made in the paper. The sentiment score is therefore -30. The language used in the review is polite and constructive, offering specific recommendations and avoiding any rude or dismissive language. The politeness score is 80.",-30,80
"Overview:

This paper proposes an approach to document classification in a low-resource language using transfer learning from a related higher-resource language. For the case where limited resources are available in the target low-resource language (e.g. a dictionary, pretrained embeddings, parallel text), multi-task learning is incorporated into the model. The approach is evaluated in terms of document classification performance using several combinations of source and target language.

Main strengths:

1. The paper is well written. The model description in Section 2 is very clear and precise.
2. The proposed approach is simple but still shows good performance compared to models trained on corpora and dictionaries in the target language.
3. A large number of empirical experiments are performed to analyse different aspects and the benefits of different target-language resources for multi-task learning.

Main weaknesses:

1. The application of this model to document classification seems to be new (I am not a direct expert in document classification), but the model itself and the components are not (sequence models, transfer learning and multitask learning are well-established). So this raises a concern about novelty (although the experimental results are new).

2. With regards to the experiments, it is stated repeatedly that the DAN model which are compared to uses ""far more resources."" The best ALL-CACO model also relies on several annotated but ""smaller"" resources (dictionaries, parallel text, embeddings). Would it be possible to have a baseline where a target-language model is trained on only a small amount of annotated in-domain document classification data in the target language? I am proposing this baseline in order to answer two questions. (i) Given a small amount of in-domain data for the task at hand, how much benefit do we get from additionally using data from a related language? (ii) How much benefit do we get from using target-language resources that do not address the task directly (dictionaries, embeddings) compared with using a ""similar"" amount of data from the specific task?

Overall feedback:

This is a well-written paper, but I think since the core of the paper lies in its empirical evaluation, the above experiments (or something similar) would greatly strengthen the work.

Edit: I am changing my rating from 5 to 6 based on the authors' response.","The sentiment of the review is generally positive, as indicated by the praise for the clarity of the writing, the simplicity and performance of the proposed approach, and the extensive empirical experiments. However, there are some concerns about the novelty of the model and suggestions for additional experiments to strengthen the work. The sentiment score is therefore 60, reflecting a positive but not overwhelmingly enthusiastic sentiment. The politeness of the language is very high, as the reviewer provides constructive feedback in a respectful and considerate manner, even changing their rating based on the authors' response. The politeness score is 90.",60,90
"The biggest contribution is the setting part, where one seeks to adapt one source to multiple, but somewhat similar, target domains.  It is interesting to explore such direction since in many real-world applications, applying the model to many different target domains are required.   

It is also noted that there is one very related work ""Multi-target Unsupervised Domain Adaptation without Exactly Shared Categories"" available online (https://arxiv.org/pdf/1809.00852.pdf).  It is desirable to have a discussion and comparison with them since they are doing Multi-target Unsupervised Domain Adaptation. In their method, the exact shared category is even not required. 

For the algorithm part, authors basically adopt the information-theoretic approach to handle the proposed method. This part contribution is limited since the techniques involved are very common in the domain adaptation. 

","The review starts with a positive note, appreciating the contribution of the setting part and its relevance to real-world applications. However, it quickly shifts to a more critical tone, pointing out the need for a discussion and comparison with a related work and mentioning that the algorithm part's contribution is limited due to the commonality of the techniques used. The sentiment score is therefore mixed, leaning slightly positive due to the initial appreciation but tempered by the subsequent criticisms. The language used is neutral to polite, as the reviewer provides constructive feedback without using harsh or rude language.",20,50
"Strengths:
- clear explanation of the problem
- clear explanation of the model and its application (pseudocode)
- clear explanation of training and resulting hyperparameters

Weaknesses:
- weak experimental settings: 
-- (a) comparison against 'easy to beat' baselines. The comparison should also include as baselines the very relevant methods listed in the last paragraph of the related work section (Snow et a.l 2005, Sun and Grishman 2010, Liao et al. 2017, Cambria et al. 2018). 
-- (b) unclear dataset selection: it is not clear which datasets are collected by the authors and which are pre-existing datasets that have been used in other work too. It is not clear if the datasets that are indeed collected by the authors are publicly available. Furthermore, no justification is given as to why well-known publicly available datasets for this task are not used (such as CoNLL-YAGO (Hoffart et al. 2011), ACE 2004 (NIST, 2004; Ratinov et al. 2011), ACE 2005 (NIST, 2005; Bentivogli et al. 2010), and Wikipedia (Ratinov et al. 2011)).
- the coverage of prior work ignores the relevant work of Gupta et al. 2017 EMNLP. This should also be included as a baseline.
- Section 2 criticises Mikolov et al.'s skip-gram model on the grounds that it introduces noisy entities because it ignores context structure. Yet, the skip-gram model is used in the preprocessing step (Section 3.1). This is contradictory and should be discussed.
- the definition of synonyms as entities that are interchangeable under certain contexts is well known and well understood and does not require a reference. If a reference is given, it should not be a generic Wikipedia URL.
- the first and second bulletpoint of contributions should be merged into one. They refer to the same thing. 
- the paper is full of English mistakes. A proficient English speaker should correct them.
","The review starts by listing the strengths of the paper, which indicates a balanced and constructive approach. However, the weaknesses section is quite detailed and critical, pointing out several significant issues with the experimental settings, dataset selection, coverage of prior work, and other aspects. The language used is direct but not overly harsh, maintaining a professional tone throughout. The sentiment score is slightly negative due to the number of critical points raised, but the politeness score remains high as the reviewer provides specific recommendations without being rude.",-20,80
"The paper proposes to plan by taking an initial plan and improving it. The authors claim that 1) this will achieve results faster than planning from scratch and 2) will lead to better results than using quick, local heuristics. However, when starting with an initial solution there is always the danger of the final solution being overly biased by the initial solution. The authors do not address this adequately. They show how to apply tree and DAG-based LSTMs to job scheduling and shortening expressions. Since they are simply using previously proposed LSTM variants, I do not see much contribution here. The experiments show some gains on randomly generated datasets. More importantly, details are missing such as the definitions of SP and RS from section 4.4.","The sentiment of the review is slightly negative. The reviewer acknowledges the proposed approach but raises concerns about potential biases and the lack of novelty in using previously proposed LSTM variants. Additionally, the reviewer points out missing details in the experiments section. Therefore, the sentiment score is -40. The politeness of the language is neutral to slightly polite. The reviewer does not use harsh or rude language and provides constructive criticism, but the tone is straightforward and lacks any particularly polite phrasing. Therefore, the politeness score is 20.",-40,20
"I appreciate the work that went into creating this paper, but I'm afraid I see little justification for accepting it.  I have three major complaints with this paper:                                                                         
                                                                                                     
1. I think the framing of decaNLP presented in this paper does more harm than good, because it perpetuates a misguided view of question answering.
                                                                                                     
Question answering is not a unified phenomenon.  There is no such thing as ""general question answering"", not even for humans.  Consider ""What is 2 + 3?"", ""What's the terminal velocity of a rain drop?"", and ""What is the meaning of life?""  All of these questions require very different systems to answer, and trying to pretend they are the same doesn't help anyone solve any problems.
                                                                                                     
Question answering is a _format_ for studying particular phenomena.  Sometimes it is useful to pose a task as QA, and sometimes it is not.  QA is not a useful format for studying problems when you only have a single question (like ""what is the sentiment?"" or ""what is the translation?""), and there is no hope of transfer from a related task.  Posing translation or classification as QA serves no useful purpose and gives people the wrong impression about question answering as a format for studying problems.

We have plenty of work that studies multiple datasets at a time (including in the context of semi-supervised / transfer learning), without doing this misguided framing of all of them as QA (see, e.g., the ELMo and BERT papers, which evaluated on many separate tasks).  I don't see any compelling justification for setting things up this way.
                                                                                                     
2. One of the main claims of this paper is transfer from one task to another by posing them all as question answering.  There is nothing new in the transfer results that were presented here, however.  For QA-SRL / QA-ZRE, transfer from SQuAD / other QA tasks has already been shown by Luheng He (http://aclweb.org/anthology/N18-2089) and Omer Levy (that was the whole point of the QA-ZRE paper), so this is merely reproducing that result (without mentioning that they did it first).  For all other tasks, performance drops when you try to train all tasks together, sometimes significantly (as in translation, unsurprisingly).  For the Czech task, fine tuning a pre-trained model has already been shown to help.  Transfer from MNLI to SNLI is known already and not surprising - one of the main points of MNLI was domain transfer, so obviously this has been studied before.  The claims about transfer to new classification tasks are misleading, as you really have the _same_ classification task, you've just arbitrarily changed how you're encoding the class label.  It _might_ be the case that you still get transfer if you actually switch to a related classification task, but you haven't examined that case.
                                                                                                     
3. This paper tries to put three separate ideas into a single conference paper, and all three ideas suffer as a result, because there is not enough space to do any of them justice.  Giving 15 pages of appendix for an 8 page paper, where some of the main content of the paper is pushed to the appendix, is egregious.  Putting your work in the context of related work is not something that should be pushed into an appendix, and we should not encourage this behavior.
                                                                                                     
The three ideas here seem to me to be (1) decaNLP, (2) the model architecture of MQAN, (3) transfer results.  Any of these three could have been a single conference paper, had it been done well.  As it stands, decaNLP isn't described or motivated well enough, and there isn't any space left in the paper to address my severe criticisms of it in my first point.  Perhaps if you had dedicated the paper to decaNLP, you could have given arguments that the framing is worthwhile, and described the tasks and their setup as QA sufficiently (as it is, I don't see any description anywhere of how the context is constructed for WikiSQL; did I miss it somewhere?).  For MQAN, there's more than a page of the core new architecture that's pushed into the appendix.  And for the transfer results, there is very little comparison to other transfer methods (e.g., ELMo, CoVe), or any deep analysis of what's going on - as I mentioned above, basically all of the results presented are just confirming what has already been done elsewhere.","The sentiment of the review is quite negative, as the reviewer expresses significant dissatisfaction with the paper's framing, originality, and structure. The reviewer uses phrases like 'does more harm than good,' 'misguided view,' and 'egregious,' which indicate strong disapproval. Therefore, the sentiment score is -80. Despite the negative sentiment, the language used is relatively polite. The reviewer acknowledges the effort put into the paper and provides detailed, constructive criticism without resorting to personal attacks or rude language. Thus, the politeness score is 40.",-80,40
"The submission proposes to combine a tree2tree autoencoder with a sequence encoder for natural language. It uses the autoencoding objective to appropriately shape the latent space and train the decoder, and then uses a second training step to align the output of a sequence encoder with the input for the tree decoder. Experiments on a recent dataset for the natural language-to-code task show that the proposed model is able to beat simple baselines.

There's much to like about this paper, but also many aspects that are confusing and make it hard to tease out the core contribution. I'm trying to reflect my understanding here, but the authors could improve their paper by providing an explicit contribution list. Overall, there seem to be three novel things presented in the paper:
(1) (Pre)training the (program) tree decoder using an autoencoder objective
(2) The doubly-recurrent tree decoder, which follows a different signal propagation strategy from most other approaches.
(3) An ""attention"" mechanism over the point in latent space (that essentially rescales parts of the decoder input)

However, the experiments do not evaluate these contributions separately; and so their relative merits remain unclear. Primarily, I have the following questions (for the rebuttal, and to improve the paper):

Re (1):
 (a) Does the pre-training procedure help? Did you evaluate joint end-to-end training of the NL spec encoder and the tree decoder? 
 (b) The auto-encoder objective would allow you to train on a larger corpus of programs without natural language specifications. Arguably, the size of the dataset is insufficient for most high-capacity deep learning models, and as you use word embeddings trained on a much larger corpus...), you could imagine training the autoencoder on an additional corpus of programs without NL specs. Did you attempt this?

Re (2): 
 (a) The tree decoder is unusual in that (one) part of the recurrence essentially enforces a breadth-first expansion order, whereas almost all other approaches use a depth-first technique (with the only exception of R3NN, as far as I remember). You cite the works of Yin & Neubig and Rabinovich et al.; did you evaluate how your decoder compares to their techniques? (or alternatively, you could compare to the absurdly complex graph approach of Brockschmidt et al. (arxiv 1805.08490)))
 (b) Ablations on this model would be nice: How does the model perform if you set the horizontal (resp. the vertical) input to 0 at each step? (i.e., ablations to standard tree decoder / to pure BFS)

Re (3): This is an unusual interpretation of the attention mechanism, and somewhat enforced by your choice (1). If you run an experiment on end-to-training (without the autoencoder objective), you could use a standard attention mechanism that attends over the memories of the NL encoder. I would be interested to see how this would change performance.

As the experimental evaluation seems to be insufficient for other researchers to judge the individual value of the paper's contribution, I feel that the paper is currently not in a state that should be accepted for publication at ICLR. However, I would be happy to raise my score if (some) of the questions above are answered; primarily, I just want to know if all of the contributions are equally important, or if some boost results more than others.


Minor notes:
- There are many spelling mistakes (""snipped"" for ""snippet"", ""isomorhpic"", ...) -- running a spell checker and doing a calm read-through would help with these details.
- page1par2: Writing specifications for programs is never harder than writing the program -- a program is a specification, after all. What you mean is the hardness of writing a /correct/ and exact spec, which can be substantially harder. However, it remains unclear how natural language would improve things here. Verification engineers will laugh at you if you propose to ""ease"" their life by using of non-formal language...
- page1par3: This (and the rest of the paper) is completely ignoring the old and active field of semantic parsing. Extending the related work section to compare to some of these works, and maybe even the experiments, would be very helpful.
- page2par3 / page6par4 contradict each other. First, you claim that mostly normal english vocabulary is used, with only occasional programming-specific terms; later you state that ""NL vocabulary used in specifications is strongly related to programming"". The fact that there are only 281 (!!!) unique tokens makes it very doubtful that you gain anything from using the 1.9million element vocab of GLoVe instead of direct end-to-end training...
- page4par3: You state ""a reference to a previously used variable may require 'climbing up' the tree and then descending"" - something that your model, unlike e.g. the work of Yin & Neubig, does not support. How important is this really? Can you support your statement by data?
- page5, (14) (and (13), probably): To avoid infinite recursion, the $h_i^{(pred)}$ on the right-hand-side should probably be $h_{i-1}^{(pred)}$

","The sentiment of the review is mixed. The reviewer acknowledges positive aspects of the paper, such as the novelty of the proposed model and its ability to outperform simple baselines. However, the reviewer also points out several confusing aspects and significant shortcomings in the experimental evaluation, which prevent a clear understanding of the core contributions. Therefore, the sentiment score is 0 (neutral). The politeness of the language is generally high. The reviewer uses polite phrases like 'could improve,' 'would be happy to raise my score,' and 'primarily, I just want to know,' which indicate a constructive and respectful tone. Therefore, the politeness score is 80 (polite).",0,80
"Summary:
--------------
The paper considers the problem of constructing compositional robotic morphologies that can solve different continuous control tasks in a (multi-agent) reinforcement learning setting. The authors created an environment where the actor consists of a number of primitive components which interface with each other via ""linking"" and construct a morphology of a robot. To learn in such an environment, the authors proposed a graph neural network policy architecture and showed that it is better than the baselines on the proposed tasks.

I find the idea of learning in environments with modular morphologies as well as the proposed tasks interesting. However, the major drawback of the paper is the lack of any reasonable details on the methods and experiments. It's hard to comment on the novelty of the architecture or the soundness of the method when such details are simply unavailable.

More comments and questions are below. I would not recommend publishing the paper in the current form.


Comments:
----------------
- If I understand it correctly, each component (""limb"") represents an agent. Can you define precisely (ie mathematically) what the observations and actions of each agent are?

- Page 4, paragraph 2: in the inline equation, you write that a sum over actions equals policy applied to a sum over states. What does it mean? My understanding of monolithic agents is that observations and actions must be stacked together. Otherwise, the information would be lost.

- Page 4, paragraphs 3-(end of section): if I understand it correctly, the proposed method looks similar to the problem of ""learning to communicate"" in a cooperative multi-agent setting. This raises the question, how exactly the proposed architecture is trained? Is it joint learning and joint execution (ie there's a shared policy network, observation and action spaces are shared, etc), or not? All the details on how to apply RL to the proposed setup are completely omitted.

- Is the topology of the sub-agents restricted to a tree? Why so? How is it selected (in cases when it is not hand-specified)?

- From the videos, it looks like certain behaviors are very unphysical or unrealistic (eg parts jumping around and linking to each other). I'm wondering which kind of simulator was used? How was linking defined (on the simulator level)? It would be nice if such environments with modular morphologies were built using the standard simulators, such as MuJoCo, Bullet, etc.


All in all, despite potentially interesting ideas and setup, the paper is sloppily written, has mistakes, and lacks crucial details.","The sentiment of the review is mixed but leans towards negative. The reviewer acknowledges the interesting idea and setup, which gives a slight positive sentiment, but the overall tone is critical due to the lack of details and the presence of mistakes. Therefore, the sentiment score is -40. The politeness of the language is relatively high; the reviewer uses polite language and constructive criticism without being rude or dismissive. Thus, the politeness score is 60.",-40,60
"The paper proposes to use a reward function to guide the learning of energy-based models for structured prediction. The idea is to update the energy function based on a random search algorithm guided by a reward function. At each iteration, the SPEN proposes a solution, then a better one is found by the search algorithm, and the energy function is updated accordingly.  Experiments are made on three use-cases and show that this method is able to outperform other training algorithms for SPENs. 

In term of model, the proposed algorithm is interesting since it can allow us to learn from weakly supervised datasets (i.e a reward function is enough). Note that in Section 3, the reward function R is never properly defined which would be nice. The algorithm is quite simple and well presented in the paper. The fact that it is based on a margin could be discussed a little bit more since the effect of the margin is not clear in the paper (the value of alpha). Moreover, the structured prediction problem has already been handled as the maximization of a reward function using RL techniques (see works by H. Daume, and works by F. Maes) and the interest of this approach w.r.t these papers is not clear to me. A clear discussion on that point (and experimental comparison) would be nice. 

The experimental section could be improved. First, the experiments on multi-label classification do not provide any comparison with SoTA methods while the two other use-cases provide some comparisons. Moreover, as far as I understand, the different use-cases could be fully supervised, and different reward functions could be defined. So investigating more deeply the consequences of the nature of the supervision/reward on these use-cases could be interesting and strengthen the paper.  Moreover, training sets are very small and it is difficult to know if this method can work on large-scale problems. 

Pro:
* interesting algorithm for structured prediction (base on reward)
* interesting results on some (toy) use-cases

Cons:
* Lack of discussion on the positive/negative point of the approach w.r.t SoTA, and on the influence of the reward function
* Lack of experimental comparisons 
* Only toy (but complicated) problems with limited training sets
","The review starts with a summary of the paper's contributions and acknowledges the interesting aspects of the proposed algorithm and its results. This indicates a generally positive sentiment towards the work. However, the review also points out several areas for improvement, such as the lack of discussion on the reward function, the need for more experimental comparisons, and the limited scope of the experiments. The language used is constructive and polite, offering specific suggestions for improvement without being harsh or dismissive.",50,80
"In this paper the authors introduce a new technique for softmax inference. In a multiclass setting, the idea is to take the output of a NN and turn it into a gating function to choose one expert. Then, given the expert, output a particular category. The first level of sparsity comes from the first expert. The second level of sparsity comes from every expert only outputting a limited set of output categories.

The paper is easy to understand but several sections (starting from section 2) could use an english language review (e.g. ""search right"" -> ""search for the right"", ""predict next word"" -> ""predict the next word"", ...) In section 3, can you be more specific about the gains in training versus inference time? I believe the results all relate to inference but it would be good to get an overview of the impact of training time as well. You motivate some of the work by the fact that the experts have overlapping outputs. Maybe in section 3.7 you can address how often that occurs as well?

Nits:
- it wasn't clear how the sparsity percentage on page 3 was defined?
- can you motivate why you are not using perplexity in section 3.2?
","The sentiment of the review is generally positive, as the reviewer acknowledges the ease of understanding the paper and provides constructive feedback for improvement. The sentiment score is set at 50 because the review is more positive than neutral but not overwhelmingly enthusiastic. The politeness score is set at 75 because the language used is polite and constructive, offering specific suggestions for improvement without being overly critical or harsh.",50,75
"This paper introduces deficiency bottleneck for learning a data representation and represent  complicated channels using simpler ones. This problem has a natural variational form that can be easily implemented from VIB. Experiments show good performance comparing to VIB. 

This paper is well-written and easy to read. The idea using KL divergence creating a deficiency channel to learn data representation is very natural. It is interesting that this formulation could be understood as minimizing a regularized risk gap of statistical decision problems, which justifies the usage of deficiency bottleneck (eq.9). 

My biggest concern is the lack of comparison with other representation learning methods, which is a very well studied problem. However, it looks like authors only compared with VIB which is similar to the proposed method in terms of the objective function. For example, how does the method compare with (variants of) Variational Autoencoder? A discussion on this or some empirical evaluations would be nice. ","The sentiment of the review is positive, as indicated by phrases like 'well-written and easy to read,' 'very natural,' and 'interesting.' The reviewer appreciates the paper's approach and its justification. However, there is a concern about the lack of comparison with other methods, which is presented as a constructive suggestion rather than a harsh criticism. The politeness of the language is high, as the reviewer uses polite phrases like 'My biggest concern' and 'would be nice,' which soften the critique and make it more constructive.",80,90
"This paper presents a network compression method based on block-diagonal sparse structure for RNN. Two kinds of group mixing methods are discussed. Experiments on PTB and SQUAD have shown its superiority over ISS.
The idea present is interesting, and this paper is easy to follow. However, this paper can be improved from the following perspectives.
1.	The method of balancing the quantity of different parts in knowledge distillation is trivial. It is quite general trick.
2.	Details of experimental setup were unclear. For example, the optimization method used, the block size, and the hyper-parameters were unclear. In addition, it is also unclear how the block diagonal structure was used for the input-to-hidden weight matrix only or all weights. 
3.	In addition, the proposed method was compared with ISS only. Since there are many methods of compressing RNNs, comparison with other competitors (e.g., those presented in Related work) are necessary.  Moreover, more experiments with other tasks in addition to NLP will be better.  
4.	In Table 2, the comparison with ISS seems be unfair. The proposed methods, i.e., LGP-shuffle was obtained based on the distillation. However, ISS was trained without distillation. From Table 3, when Cmse and Ckl were set to zero, the result was much worse. The reviewer was wondering that how does ISS with distillation perform. 
","The sentiment of the review is moderately positive. The reviewer acknowledges that the idea presented is interesting and the paper is easy to follow, which indicates a positive sentiment. However, the reviewer also points out several areas for improvement, which tempers the overall positivity. Therefore, the sentiment score is 40. The politeness of the language used is quite high. The reviewer uses polite language such as 'can be improved,' 'unclear,' and 'necessary,' and avoids any harsh or rude language. Therefore, the politeness score is 80.",40,80
"This paper proposes a method for learning sentences encoders using artificially generated (fake) sentences. While the idea is interesting, the paper has the following issues:

- There are other methods that aim at generating artificial training data, e.g.:  Z. Zhao, D. Dua, S. Singh. Generating Natural Adversarial Examples. International Conference on Learning Representations (ICLR). 2018,  but no direct comparison is made. Also InferSent  (which is cited as related work) trains sentence encoders on SNLI: https://arxiv.org/pdf/1705.02364.pdf. Again a comparison is needed as the encoders learned perform very well on a variety of tasks. Finally, the proposed idea is very similar to ULMfit (https://arxiv.org/pdf/1801.06146.pdf) which trains a language model on a lot of unlabeled data and then finetunes it discriminatively. Finally, there should be a comparison against a langauge model without any extra training in order to assess the benefits of the fake sentence classification part of the model.

- It is unclear why the fake sentence construction method proposed by either swapping words or just removing them produces sentences that are fake and/or useful to train on. Sure it is simple, but not necessarily fake. A language model would be able to discriminate between them anyway, by assigning high probability to the original ones, and low probability to the manipulated ones. Not sure we need to train a classifier on top of that.

- I found the notation in section 2 confusing. What kind of distribution is P(enc(x,theta1)|theta2, theta3)? I understand that P(x|theta) is the probability of the sentence given a model, but what is the probability of the encoding? It would also be good to see the full derivation to arrive at the expression in the beginning of page 3. 

- An argument in favour of the proposed method is training speed; however, given that less data is used to train it, it should be faster indeed. In fact, if we consider the amount of time per million sentences, the previous method considered in comparison could be faster (20 hours of 1M sentences is 1280 hours for 64M sentences, more than 6 weeks). More importantly, it is unclear from the description if the same data is used in training both systems or not.

- It is unclear how one can estimate the normalization factor in equation 2; it seems that one needs to enumerate over all fake sentences, which is a rather large number due to the number of possible word swaps in the sentence,

- I am not sure the generator proposed generates realistic sentences only, ""Chicago landed in John on Friday"" is rather implausible. Also there is no generation method trained here, it is rule-based as far as I can tell. There is no way to tell the model trained to generate a fake sentence as far as I can tell.

- It is a bit odd to criticise other methods ofr using LSTMs with ""millions of parameters"" while the proposed approach also uses them. A comparison should calculate the number of parameters used in either case.

- what is the motivation for having multiple layers without non-linearity instead of a single layer?","The sentiment of the review is slightly negative. The reviewer acknowledges that the idea is interesting but lists several significant issues with the paper, indicating a sentiment score of -40. The politeness of the language used is relatively neutral to slightly polite. The reviewer uses phrases like 'it is unclear' and 'I found' which are not overly harsh but still critical, indicating a politeness score of 20.",-40,20
"This submission sets out to taxonomize evasion-time attacks against deep RL and introduce several new attacks, including two heuristics for efficient evasion-time attacks and attacks that target the environment dynamics and RL system’s actions. The main limitation of this paper is probably its broad scope, which unfortunately prevents it in its current form from addressing each of the goals stated in the introduction systematically to draw conclusive takeaways. 

Taxonomizing the space of adversaries targeting deep RL at test time is a valuable contribution. While the existing taxonomy is a good start, it would be useful if you can clarify the following points in your rebuttal. Why were the “further categorization” items separated from adversarial capabilities? Being constrained to real-time or physical perturbations appears to be another way to describe the adversary’s capabilities. In addition, is there a finer-grained way to characterize the adversary’s knowledge beyond white-box vs. black-box? This binary perspective is common but not very informative. One way to move forward would be for instance to think about the different components of a RL system, and identify those that are relevant to have knowledge of when adversaries are mounting attacks. It would also be helpful to position prior work in the taxonomy. Finally, the taxonomy currently stated in the submissions is more a taxonomy of attacks (or adversaries) than a taxonomy of vulnerabilities, so the title of Section 3 could perhaps be updated accordingly. 

Section 4.1 gives a good overview of different attack strategies against RL based on modifying the observations analyzed by the agent. Many of these attacks are applications of known attack strategies and will be familiar to readers with adversarial ML background (albeit some of these strategies were previously introduced and evaluated against “supervised” classifiers only). One point was unclear however: why is the imitation learning based black-box attack not a transferability-based attack? As far as I could understand, the strategy described corresponds exactly to the commonly adopted strategy of transferring adversarial examples found on a substitute model (see for instance “Intriguing properties of neural networks” by Szegedy et al. and “Practical Black-Box Attacks against Machine Learning” by Papernot et al.). In other words, Section 4.1 could be rescoped to put emphasis on the attack strategies that have not been explored previously in the context of reinforcement learning: e.g., the finite difference approach with adaptive sampling or the universal attack with optimal selection of initial frames. It is unfortunate that the treatment of these two attacks is currently deferred to the appendix as they make the paper more informative. Similarly, Sections 4.2 and 4.3 would benefit from being extended to put forward the new attack threat model considered in these two sections. 

While the introduction claimed to make a systematic evaluation of attacks against RL, the presentation of the experimental section can be improved to ensure the analysis points out the relevant takeaways. For instance, it is unclear what the differences are between results on TORCS and other tasks included in the Appendix. Specifically, results on Enduro do not seem as conclusive as those presented on TORCS. Do you have some intuition as to why that is the case? In Figure 7, it appears that a large number of frames need to be manipulated before a drop on cumulative reward is noticeable. Previous efforts manipulated single frames only, could you stress why the setting is different here? Throughout the section, many Figures are small and it is difficult to infer whether the difference between the white-box and black-box variants of an attack is significant or not. Could you analyze this in more details in the text? In Table 2, how should the L2 distance be interpreted? In other words, when is the adversary successful? 

If you can clarify any of the points made above in your rebuttal, I am of course open to revise my review. 

Editorial details: 
Figures are not readable when printed. 
Figure 5 is improperly referenced in the main body of the paper. 
Figure 7: label is incorrect for Torcs and Hopper (top of figure)
","The review starts with a balanced overview of the paper's contributions and limitations, indicating a neutral to slightly positive sentiment. The reviewer acknowledges the value of taxonomizing adversaries targeting deep RL and provides constructive feedback on how to improve the paper. The language used is polite and professional, with suggestions framed as questions or recommendations rather than criticisms. The reviewer also offers to revise their review if the authors address the points raised, further indicating a positive and collaborative tone.",20,80
"Paper Summary: 
The idea of the paper is to improve Hindsight Experience Replay by providing natural language instructions as intermediate goals. 

Paper Strengths:
Unfortunately, there is not many positive points about the paper except that it explores an interesting direction. 

Paper Weaknesses: 

I vote for rejection of the paper due to the following issues:

- It is not clear how a description for a point along the way is provided (when the agent is not at a target). It is not clear how those feedback sentences are generated. That is the main claim of the paper and it is not clear at all.

- The result of DQN is surprising (it is always zero). DQN is not that bad. Probably, there is a bug in the implementation. There should be comments on this in the rebuttal.

- According to several recent works, algorithms like A3C work much better than DQN. Does the proposed method provide improvements over A3C as well?

- The only measure that is reported is success rate. The episode length should be reported as well. I suggest using the SPL metric proposed by Anderson et al. in ""On Evaluation of Embodied Navigation Agents"".

- Replacing one word with its synonym is considered as zero-shot. That is not really a zero-shot setting. Please refer to the following paper, which is missing in the related work:
Interactive Grounded Language Acquisition and Generalization in a 2D World, ICLR 2018

- The environments are toy environments. The experiments should be carried out in more complex environments such as THOR or House3D that include more semantics.

- What is the difference between this method and providing a large negative reward at a non-target object?

- The paper discusses the advantages of word embeddings over one-hot vectors. That is obvious and not the goal of this paper. 

- It seems the same environment is used for train and test.

------------------------
Post rebuttal comments:

Most of my concerns have been addressed. My new rating is 5. I like the idea of having a compact representation for the hindsight experience replay, but there are still a few issues:

- I expected more complexity in vision and language. I do not agree with the rebuttal that AI2-THOR or House3D are not suitable. This level of complexity would be ok if this paper was among the first ones to explore this domain, but there are already several works. The zero-shot setting (changing the word with its synonym) is also so simplistic.

- The proposed method uses much more annotations than the baselines so the comparisons are not really fair. This information should have been added to the baseline to see how this additional information changes the performance. Basically, it is not clear if the improvement should be attributed to the extra annotation or the way the advice is given.

- The writing is still confusing. For instance, it is mentioned that ""Concretely, for each state s ∈ S, we define T as a teacher that gives an advice T(s)"", while that is not true since later it is mentioned that ""the teacher give advice based solely on the terminal state"". These statements are contradictory, and it is not trivial at all to provide an advice for each state.
","The sentiment score is derived from the overall tone and content of the review. The initial sentiment is quite negative, as the reviewer lists numerous weaknesses and votes for rejection. However, the sentiment improves slightly in the post-rebuttal comments, where the reviewer acknowledges that most concerns have been addressed and provides a new rating of 5. Therefore, the sentiment score is -40, reflecting a generally negative but slightly improved sentiment. The politeness score is based on the language used throughout the review. The reviewer uses polite language, even when pointing out significant flaws. Phrases like 'I suggest,' 'please refer to,' and 'it is not clear' indicate a respectful tone. Thus, the politeness score is 60.",-40,60
"This work shows that adding a simple blurring into max pooling layers can address issues of image classification instability under small image shifts. In general this work presents a simple and easy to implement solution to a common problem of CNNs and even though it lacks more thorough theoretical analysis of this problem from the signal processing perspective (such as minimal size of the blurring kernel for fulfilling the Nyquist-Shannon sampling theorem), it seems to provide ample empirical evidence.

Pros:
+ The introduction and motivation is really well written and Figure 3 provides a clear visualisation main max pooling operator issues.
+ The proposed method is really simple and shows promising results on the CIFAR dataset. With random shifts, authors had to tackle cropping with circular shifts. As it can cause artifacts in the data, authors also provide baseline performances on the original data (used for both training and testing).
+ Authors provide a thorough evaluation, ranging from comparing hidden representations to defining consistency metrics of the classified classes.

This work is lacking in the experimental section due to some missing details and few inconsistencies. I believe the most of my concerns can be relatively easily fixed/clarified in an update of this submission.

Major issues, which if fixed would improve the rating:
- It is not correct to average test accuracy and test consistency as both measures are different quantities, especially when using them for ranking. The difference between accuracy of different methods are considerably smaller than differences in the classification consistency. 
- It is not clear how many shifts are used for computing the ""Random Test Accuracy"" and the ""Classification Accuracy"". Also whether the random shifts are kept constant between evaluated networks and evaluation metrics.
- Authors do not address the question what is the correct order of operations for the blurring. E.g. would the method empirically work if blurring was applied before max pooling? Do the operations commute?
- The selection of the filters is rather arbitrary, especially regarding the 1D FIR filters. The separability of these filters should be discussed.
- I believe authors should address how this work differs to [1], as it also tests different windowing functions for pooling operators, even though in different tasks.

Minor issues, which would be nice to fix however which do not influence my rating:
* Section 3.1 - And L-Layer deep *CNN*, H_l x W x C_l -> H_l x W_l x C_l
* Section 3.1. Last paragraph - I would not agree with the statement that in CNNs the shift invariance must necessarily emerge upon shift equivariance. If anything, this may hold only for the last layer of a network without fully connected layers and with average pooling of the classifier output (ResNet/GoogleNet like networks).
* Explicitly provide the network architecture as [Simonyan14] does not test on CIFAR and cannot use Batch normalisation.
* It would be useful to add citation for the selected FIR filters.
* The flow of section 4.2. can be improved to help readability. The three metrics should be first motivated before their introduction. Metric 2. paragraph - the metric is defined below, not above. 
* It would be interesting to see what would be the performance if the blurring filters were trained as well (given some sensible initialisation).
* One future direction would be to verify that this approach generalises to larger networks as well. It might be worth to discuss this in the conclusions.

[1] Scherer, Dominik, Andreas Müller, and Sven Behnke. ""Evaluation of pooling operations in convolutional architectures for object recognition."" Artificial Neural Networks–ICANN 2010. Springer, Berlin, Heidelberg, 2010. 92-101.","The review starts with a positive sentiment, highlighting the simplicity and effectiveness of the proposed method. The reviewer appreciates the well-written introduction and motivation, as well as the thorough evaluation provided by the authors. However, the review also points out several major and minor issues that need to be addressed. The language used is polite and constructive, offering specific recommendations for improvement without being overly critical or harsh.",70,90
"The paper improves upon the Real NVP/Glow design by proposing better dequantization schemes and more expressive forms of coupling layers. I really like Real NVP models, which I think are a bit underappreciated. Thus, I’m happy that there are papers trying to improve their performance.  However, I wish this was done with more rigour.

The paper makes 3 claims about the current flow models: (1) it is suboptimal to use additive uniform noise when dequantizing images, (2) affine coupling layers are not expressive enough, and (3) the architectures fail to capture global image context. I’ll comment on these claims and proposed solutions below.

(1) I agree with the reasoning behind the need for a better dequantization distribution. However, I think the authors should provide an evidence that the lower bound is indeed loose when q is uniform. For example, for the CIFAR-10 model, the authors calculated a gap of 0.025 bpd when using variational dequantization. What would this gap be when using uniform q?  Maybe, a clear illustration of the dequantization effect on a simpler dataset or a toy example would be more useful.

(2) My main concern about the mixture CDFs coupling layer is how much bigger the model becomes and how much slower it trains. I find this analysis crucial when deciding whether 0.05 bpd improvement as reported in Table 1 is worth the hassle.

(3) As a person not familiar with the Transformer, I couldn’t understand how exactly self-attention works and how much it helps the model to capture the global image context. Also, I think this problem needs a separate illustration on a dataset of larger images.  
  
The experiments section is very weak in backing up the identified problems and proposed solutions. Firstly, I think it is more clear if the ablation study is done in reverse: instead of making Flow++ and removing components, start with the vanilla model and then add stuff.  Secondly, it’s not clear if these improvements generalize across datasets, e.g. when images are larger than 32x32. Though, larger inputs may lead to huge models which are impossible to train when the resources are quite limited. That’s why I find it important to report how much complexity is added compared to the initial Real NVP. Also, I think it’s a well-known fact that sampling from PixelCNN models is slow unlike for Real NVPs, so I don’t find the results in Table 3 surprising or even useful. 

To conclude, I find this paper unfinished and wouldn’t recommend its acceptance until the analysis of the problems and their solutions becomes better thought out.  ","The sentiment of the review is mixed. The reviewer expresses appreciation for the effort to improve Real NVP models and acknowledges the potential of the proposed improvements. However, the reviewer also points out significant shortcomings in the paper, such as the lack of rigorous analysis and weak experimental validation. Therefore, the sentiment score is moderately positive but tempered by substantial criticism. The language used is generally polite, with constructive feedback and suggestions for improvement, although there are some direct criticisms. Thus, the politeness score is high but not perfect.",30,80
"This paper proposes an approach to model social influence in a scenario-independent manner by instantiating the concept of intrinsic motivation and combine various human abilities as part of a reinforcement learning function in order to improve the agent's operation in social dilemma scenarios. 

Agents are operationalised as convolutional neural network, linear layers and LSTM. Using these base mechanisms, different abilities (communication, models of other agents (MOA)), their causal influence is inferred based on counterfactual actions. The architecture is explored across two different sequential social dilemmas. 

The architecture is described in sufficient detail, with particular focus on the isolation of causal influence for communication and MOA influence. The experimental evaluation is described in sufficient detail, given the low complexity of the scenarios. While the agents with communicative ability and MOA show superior performance, a few results warrant clarification.

Figure 6a) highlights the performance of influencers in contrast to a visible actions baseline. This specific scenarios shows the necessity to run experiments for larger number of runs, since it appears that action observations may actually outperform influencer performance beyond 3 steps. Please clarify what is happening in this specific case, and secondly, justify your choice of steps used in the experimental evaluation. 

Another results that requires clarification is Figure 6f), which is not sufficiently discussed in the text, yet provides interesting patterns between the MOA baseline performance decaying abruptly at around 3 steps, with the influence MOA variant only peaking after that. Please clarify the observation. Also, could you draw conclusions or directions for a combination of the different approaches to maximise the performance (more generally, beyond this specific observation)? 

A valuable discussion is the exemplification of specific agent behaviour on Page 7. While it clarifies the signalling of resources in this specific case, it also shows shortcomings of the model's realism. How would the model perform if agents had limited resources and would die upon depletion (e.g. the de facto altruistic influencer in this scenario - since it only performs two distinct actions)? The extent of generalisability should be considered in the discussion. 

In general, the paper motivates and discusses the underlying work in great detail and is written in an accessible manner (minor comment: the acronym LSTM is not explicitly introduced). The quality of presentation is good. ","The sentiment of the review is generally positive, as the reviewer acknowledges the detailed description of the architecture and the experimental evaluation. The reviewer also appreciates the accessible writing and good quality of presentation. However, there are some points that require clarification, which slightly tempers the overall positive sentiment. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, making suggestions and asking for clarifications in a respectful manner. Thus, the politeness score is 90.",60,90
"I like this paper. What the authors have done is of high quality. It is well written and clear. However, quite a lot of experiments are necessary to make this paper publishable in my opinion.

Strenghts:
- The idea to use a GAN for model compression is something that many must have considered. It is good to see that someone has actually tried it and it works well.
- I think the compression score is definitely an interesting idea on how to compare GANs that can be of practical use in the future.
- The experimental results, which are currently in the paper, largely support what the authors are saying.

Weaknesses:
- The authors don't compare how good this technique is in comparison to simple data augmentation. My suspicion is that the difference will be small. I realise, however, that the advantage of this method over data augmentation is that it is harder to do it for tabular data, for which the proposed method works well. Having said that, models for tabular data are usually quite simple in comparison to convnets, so compressing them would have less impact.
- The experiments on image data are done with CIFAR-10, which as of 2018 is kind of a toy data set. Moreover, I think the authors should try to push both the baselines and their technique much harder with hyperparameter tuning to understand what is the real benefit of what they are proposing. I suspect there is a lot of slack there. For comparison, Urban et al. [1] trained a two-layer fully connected network to 74% accuracy on CIFAR-10 using model compression.

[1] Urban et al. Do Deep Convolutional Nets Really Need to be Deep (Or Even Convolutional)? 2016.","The sentiment of the review is generally positive, as indicated by phrases like 'I like this paper' and 'What the authors have done is of high quality.' However, the reviewer also points out several weaknesses and necessary improvements, which tempers the overall positivity. Therefore, the sentiment score is 60. The politeness of the language is high, with the reviewer using phrases like 'I realise, however,' and 'Having said that,' which show a respectful and considerate tone. Thus, the politeness score is 80.",60,80
"This paper proposes an approximate second-order method with low computational cost. A common pitfall of second-order methods is the computation (and perhaps inversion) of the Hessian matrix. While this can be avoided by instead relying on Hessian-vector products as done in CG, it typically still requires several iterations. Instead, the authors suggest a simpler approach that relies on one single gradient step and a warm start strategy. The authors points out that the resulting algorithm resembles a momentum method. They also provide some simple convergence proofs on quadratics and benchmark their method to train deep neural networks.

While I find the research direction interesting, the execution is rather clumsy and many details are not sufficiently motivated. Finally, there is a lot of relevant work in the optimization community that is not discussed in this paper, see detailed comments and references below.

1) Method
The derivation of the method is very much driven on a set of heuristics without theoretical guarantees. In order to derive the update of the proposed method, the authors rely on three heuristics:
a) The first is to reuse the previous search direction z as a warm-start. The authors argue that this might be beneficial if If z does not change abruptly. In the early phase, the gradient norm is likely to be large and thus z will change significantly. One might also encounter regions of high curvature where the direction of z might change quickly from one iteration to the next.
The ""warm start"" at s_{t-1} is also what yields the momentum term, what interpretation can you give to this choice?

b) The second step interleaves the updates of z and w instead of first finding the optimum z. This amounts to just running one iteration of CG but it is rather unclear why one iteration is an appropriate number. It seems one could instead some adaptive strategy where CG with a fixed accuracy. One could potentially see if allowing larger errors at the beginning of the optimization process might still allow for the method to converge. This is for instance commonly done with the batch-size of first-order method. Gradually increasing the batch-size and therefore reducing the error as one gets close to the optimum can still yield to a converging algorithm, see e.g. 
Friedlander, M. P., & Schmidt, M. (2012). Hybrid deterministic-stochastic methods for data fitting. SIAM Journal on Scientific Computing, 34(3), A1380-A1405.

c) The third step consists in replacing CG with gradient descent.
""If CG takes N steps on average, then Algorithm 2 will be slower than SGD by a factor of at least N, which can easily be an order of magnitude"".
First, the number of outer iterations may be a lot less for the Hessian-free method than for SGD so this does not seem to be a valid argument. Please comment.
Second, I would like to see a discussion of the convergence rate of solving (12) inexactly with krylov subspace methods. Note that Lanczos yields an accelerated rate while GD does not. So the motivation for switching to GD should be made clearer.

d) The fourth step introduces a factor rho that decays z at each step. I’m not really sure this makes sense even heuristically. The full update of the algorithm developed by the author is:
w_{t+1} = w_t - beta nabla f + (rho I - beta H) (w_t - w_{t-1}).
The momentum term therefore gets weighted by (rho I - beta H). What is the meaning of this term? The -beta H term weights the momentum according to the curvature of the objective function. Given the lack of theoretical support for this idea, I would at least expect a practical reason back up by some empirical evidence that this is a sensible thing to do.
This is especially important given that you claim to decay rho therefore giving more importance to the curvature term.
Finally, why would this be better than simply using CG on a trust-region model? (Recall that Lanczos yields an accelerated linear rate while GD does not).

2) Convergence analysis
a) The analysis is only performed on a quadratic while the author clearly target non-convex functions, this should be made clear in the main text. Also see references below (comment #3) regarding a possible extension to non-convex functions.
b) The authors should check the range of allowed values for alpha and beta. It appears the rate would scale with the square root of the condition number, please confirm, this is an important detail. I also think that the constant is not as good as Heavy-ball on a quadratic (see e.g. http://pages.cs.wisc.edu/~brecht/cs726docs/HeavyBallLinear.pdf), please comment.
c) Sub-sampling of the Hessian and gradients is not discussed at all (but used in the experiments). Please add a discussion and consider extending the proof (again, see references given below).

3) Convergence Heavy-ball
The authors emphasize the similarity of their approach to Heavy-ball. They cite the results of Loizou & Richtarik 2017. Note that they are earlier results for quadratic functions such as 
Lessard, L., Recht, B., & Packard, A. (2016). Analysis and design of optimization algorithms via integral quadratic constraints. SIAM Journal on Optimization, 26(1), 57-95.
Flammarion, N., & Bach, F. (2015, June). From averaging to acceleration, there is only a step-size. In Conference on Learning Theory (pp. 658-695).
The novelty of the bounds derived in Loizou & Richtarik 2017 is that they apply in stochastic settings.
Finally, there are results for non-convex functions such convergence to a stationary point, see
Zavriev, S. K., & Kostyuk, F. V. (1993). Heavy-ball method in nonconvex optimization problems. Computational Mathematics and Modeling, 4(4), 336-341.
Also on page 2, ""Momentum GD ... can be shown to have faster convergence than GD"". It should be mentioned that this only hold for (strongly) convex functions!

4) Experiments
a) Consider showing the gradient norms. 
b) it looks like the methods have not yet converged in Fig 2 and 3.
c) Second order benchmark:
It would be nice to compare to a method that does not use the GN matrix but the true or subsampled Hessian (like Trust Region/Cubic Regularization) methods given below.
Why is BFGS in Rosenbrock but not in NN plots?
d) ""Batch normalization (which is known to improve optimization)"" 
This statement requires a reference such as
Towards a Theoretical Understanding of Batch Normalization
Kohler et al… - arXiv preprint arXiv:1805.10694, 2018

5) Related Work
The related work should include Cubic Regularization and Trust Region methods since they are among the most prominent second order algorithms. Consider citing Conn et al. 2000 Trust Region,  Nesterov 2006 Cubic regularization, Cartis et al. 2011 ARC.
Regarding sub-sampling: Kohler&Lucchi 2017: Stochastic Cubic Regularization for non-convex optimization and Xu et al.: Newton-type methods for non-convex optimization under inexact hessian information.

6) More comments

Page 2
Polyak 1964 should be cited  where momentum is discussed.
""Perhaps the simplest algorithm to optimize Eq. 1 is Gradient Descent"". This is technically not correct since GD is not a global optimization algorithm. Maybe mention that you try to find a stationary point
rho (Eq. 2) and lambda (Eq. 4) are not defined

Page 4: 
Algorithm 1 and 2 and related equations in the main text: it should be H_hat instead of H.

Background
“Momemtum GD exhibits somewhat better resistance to poor scaling of the objective function”
To be precise the improvement is quadratic for convex functions. Note that Goh might not be the best reference to cite as the article focuses on quadratic function. Consider citing the lecture notes from Nesterov.

Section 2.2
This section is perhaps a bit confusing at first as the authors discuss the general case of a multivalue loss function. Consider moving your last comment to the beginning of the section.

Section 2.3
As a side remark, the work of Dauphin does not rely on the Gauss-Newton approximation but a different PSD matrix, this is probably worth mentioning.

Minor comment: The title is rather bold and not necessarily precise since the stepsize of curveball is not particularly small e.g. in Fig 1.
","The sentiment of the review is mixed, leaning towards negative. The reviewer acknowledges the interesting research direction but criticizes the execution and lack of sufficient motivation for many details. The review contains several critical comments and suggestions for improvement, indicating a sentiment score of -40. The politeness of the language is generally maintained, with the reviewer providing constructive feedback and references to support their points. The language is formal and respectful, resulting in a politeness score of 60.",-40,60
"The paper claims to propose a novel generative probabilistic neural network model such that its encoder (classifying an image) can be approximated by a convolutional neural network with ReLU activations and MaxPooling layers. Besides the standard parameters of the units (weights and biases), the model has two additional latent variables per unit, which decide whether and where to put the template (represented by the weights of the neuron) in the subsequent layer, when generating an image from the class. Furthermore, the authors claim to derive new learning criteria for semi-supervised learning of the model including a novel regulariser and claim to prove its consistency. 

Unfortunately, the paper is written in a way that is completely incomprehensible (for me). The accumulating ambiguities, together with its sheer length (44 pages with all supplementary appendices!), make it impossible for me to verify the model and the proofs of the claimed theorems. This begins already with definition of the model. The authors consider the latent variables as dependent and model them by a joint distribution. Its definition remains obscure, let alone the question how to marginalise over these variables when making inference. Without a thorough understanding of the model definition, it becomes impossible (for me) to follow the presentation of the learning approaches and the proofs for the theorem claims.

In my view, the presented material exceeds the limits of a single conference paper. A clear and concise definition of the proposed model accompanied by a concise derivation of the basic inference and learning algorithm would already make a highly interesting paper.

Considering the present state of the paper, I can't, unfortunately, recommend to accept it for ICLR.

","The sentiment score is determined based on the overall tone and content of the review. The reviewer expresses significant frustration and disappointment with the paper, describing it as 'completely incomprehensible' and stating that they cannot recommend it for acceptance. This indicates a highly negative sentiment, which I would rate at -80. The politeness score is assessed by examining the language used. Despite the negative sentiment, the reviewer maintains a professional tone, using phrases like 'unfortunately' and 'in my view,' which soften the critique. Therefore, the politeness score is relatively high, at 60.",-80,60
"The authors present a method for fine-tuning neural networks so inference can be performed in a quantized low bit data format down to 3 bits. The authors achieve this through a combination of three techniques:
1. Noise injection to fine-tune the weights before quantization. The effect of noise injection can model that of quantization, but rather than being stuck in a quantization bin, fine grained weight updates are still possible
2. A schedule that quantizes layer by layer, rather than all layers at the same time
3. Clipping weights and activations within a learned range to obtain finer grained bins within that range. 


The main contribution is a novel combination of mostly existing techniques. Clipping (or clamping as the authors call it) has been proposed by Zhang et al. 2018, but it's an interesting contribution to have the clipping learned directly via backpropagation with a straight-through estimator. Treating the quantization as noise has been proposed in a different form in McKinstry et al. 2018. Gradual quantization appears novel, but is also the least interesting of the techniques. Therefore, novelty on ideas/methods is somewhat limited, and the contribution is mostly the in the impressive experimental results, which appear to be outperforming previous methods. The main weaknesses are poor writing, and that some details of the implementation required to reproduce the results are missing. For example, the training schedule is not given, e.g. how many epochs to train the clean model, how many with noise, how many quantized. Details on the gradual quantization are also missing. Block based quantization is completely heuristic and not well motivated. If this is the main novel ingredient, more details on the mechanics would be needed. Is both the noise injection and the quantization done in blocks? If the motivation is in ""the opportunity to adapt”, then what does the adaptation look like? 

As above, my other main issue is with the writing, there are many examples where I would suggest improvements:

This work could be improved greatly by copy editing for English grammar. There are many typos (including ones that can be caught by autocorrect, missing punctuation, or using similar but unrelated words, e.g. ""token"" instead of ""taken""). The manuscript appears hastily put together and not ready for publication. 

The acronym NICE already has a meaning in the DL literature: Dinh, L., Krueger, D., & Bengio, Y. (2014). NICE: Non-linear Independent Components Estimation. It confusing to reuse it. 

The term clamping is only explained on page 4 but used since the abstract. It’s used in a nonstandard way to mean “constrained to lie within a range” which should be explained earlier. I think “clipped” would be a better term, following the related Choi et al. 2018. Clamping usually means ""constrained to a fixed value"" (not a range), so it is not a good term to use in this context. 

Are the results shown in table 2 and table 3 from a single trial or averaged across reruns? If single trial, it's misleading to have 2 figures after the decimal. Even non-quantized ResNet tends to have 0.5% or so run to run variability, which is much larger than the differences between some of the methods shown here. In fact, a lot of the results could just be due to picking a lucky random seed. 

Comparisons are shown against methods JOINT (Jung et al), LQ-Nets (Zhang et al), FAQ (McKinstry et al). It would be helpful to present them with the same names in the ""related work"" section, and explain why they were picked out for the comparison. For someone not familiar with the literature it's hard to see why these 3 would be the obvious picks. 

Readability would increase if table 2 and 3 were moved to section 4 where they are referenced, rather than after the discussion. Fig 2 font size too small and hard to read. 
","The sentiment score is derived from the overall tone of the review, which acknowledges the novelty and impressive experimental results but also highlights significant weaknesses in writing and missing implementation details. This mixed feedback results in a sentiment score of -20. The politeness score is based on the language used, which, while critical, remains professional and constructive, suggesting improvements rather than dismissing the work outright. Therefore, the politeness score is 50.",-20,50
"PROs
-seemingly reasonable approach to polyphonic music generation: figuring out a way to splitting the parts, share parameters appropriately, measuring entropy per time, all make sense
-the resulting outputs tend to have very short-term harmonic coherence (e.g. often a ‘standard chord’ with some resolving suspensions, etc), with individual parts often making very small stepwise motion (i.e. reasonable local voice leading)
-extensive comparison of architectural variations
-positive results from listening experiments

CONs
-musical outputs are *not* clearly better than some of the polyphonic systems described; despite the often small melodic steps, the individual lines are quite random sounding; this is perhaps a direct result of the short history
-I do not hear the rhythmic complexity that is described in the introduction
-the work by Johnson (2015) (ref. provided below) should be looked at and listened to; it too uses coupled networks, albeit in a different way but with a related motivation, and has rhythmic and polyphonic complexity and sounds quite good (better, in my opinion) 
-some unclear sections (fixable, especially with an appendix; more detail below)
-despite the extensive architectural comparisons, I was not always clear about rationale behind certain choices, eg. if using recurrent nets, why not try LSTM or GRU? (more questions below)
-would like to have heard the listening tests; or at least read more about how samples were selected (again, perhaps in an appendix and additional sample files)

 quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).

Quality -- In this work, various good/reasonable choices are made. The quality of the actual output is fine. It is comparable to-- and to my ears not better than-- existing polyphonic systems such as the ones below (links to sample audio are provided here):

-Bachbot - https://soundcloud.com/bachbot (Liang et al 2017)
- tied parallel nets - http://www.hexahedria.com/2015/08/03/composing-music-with-recurrent-neural-networks/ (Johnson 2015, ref below)
-performanceRNN - https://magenta.tensorflow.org/performance-rnn - (Simon & Oore 2017)
..others as well..


Clarity -- Some of the writing is ""locally"" clear, but one large, poorly-organized section makes the whole thing confusing (details below). It is very helpful that the authors subsequently added a comment with a link to some sample scores; without that, it had been utterly impossible to evaluate the quality. There are a few points that could be better clarified:
	-p5”a multi-hot vector of notes N”. It sounds like N will be used to denote note-numbers, but in fact it seems like N is the total number of notes, i.e. the length of the vector, right? What value of N is used?
-p5 “a one-hot vector of durations D”. It sounds like D will be used to denote durations, but actually I think D is the length of the 1-hot vector encoding durations right? What value of D is used, and what durations do the elements of this vector represent?
-similarly, does T represent the size of the history? This should really be clarified.
	-p5 Polyphonic models.
		-Eq (2), (3), (4): Presumably the h’s are the hidden activations layers?
		-the networks here correspond to the blue circles in Fig 1, right? If so, make the relationship clear and explicit 
		-Note that most variables in most equations are left undefined       
		-actually defining the W’s in Eq(2-4)  would allow the authors to refer to the W’s later (e.g. in Section 5.2) when describing weight-sharing ideas. Otherwise, it’s all rather confusing. For example, the authors could write, “Thus, we can set W_p1 = W_p2 = W_p3 = W_p4” (or whatever is appropriate). 
	-Generally, I found that pages 5-7 describe many ideas, and some of them are individually fairly clearly described, but it is not always clear when one idea is beginning, and one idea is ending, and which ideas can be combined or not. On my first readings, I thought that I was basically following it, until I got to Table 5, which then convinced me that I was in fact *not* quite following it. For example, I had been certain that all the networks described are recurrent (perhaps due to Fig1?), but then it turned out that many are in fact *not* recurrent, which made a lot more sense given the continual reference to the history and the length of the model’s Markov window etc. But the reader should not have had to deduce this. For example, one could write, 
	“We will consider 3 types of architectures: convolutional, recurrent, .... In each architecture, we will have [...] modules, and we will try a variety of combinations of these modules. The modules/components are as follows:”. It’s a bit prosaic, but it can really help the reader. 
-Appendices, presented well, could be immensely helpful in clarifying the exact architectures; obviously not all 22 architectures from Table 5 need to be shown, but at least a few of them shown explicitly would help clarify. For example, in Fig1, the purple boxes seem to represent notes (according to the caption), but do they actually represent networks? If they really do represent notes, then how can “notes” receive inputs from both the part-networks and the global network? Also, I was not entirely clear on the relationship of the architecture of the individual nets (for the parts) to that of the global integrating network. E.g. for experiment #20, the part-net is an RNN (with how many layers?? with regular or LSTM cells?) followed by a log-linear predictor (with one hidden layer of 300 units right? or are there multiple layers sometimes?), but then what is the global network? Why does the longest part-history vector appear to have length 10 based on Table 5, but according to Table 3 the best-performing history length was 20? Though, I am not sure the meaning of the “bottom/top” column was explained anywhere, so maybe I am completely misunderstanding that aspect of the table? Etc.
-Many piano scores do not easily deconstruct into clean 4-part polyphony; the example in Appendix A is an exception. It was not clear to me how piano scores were handled during training. 
-Terminology: it is not entirely clear to me why one section is entitled “homophonic models”, instead of just “monophonic models”. Homophonic music usually involves a melody line that is supported by other voices, i.e. a sort of asymmetry in the part-wise structure. Here, the outputs are quite the opposite of that: the voices are independent, they generally function well together harmonically, and there is usually no sense of one voice containing a melody. If there’s some reason to call it homophonic, that would be fine, but otherwise it doesn’t really serve to clarify anything. However, the authors do say that the homophonic composition tasks are a “minor generalization of classic monophonic composition tasks”, so this suggests to me that there is something here that I am not quite understanding.

The last sentence of Section 5.3 is very confusing-- I don’t understand what lin_n is, or 1_n is, or how to read the corresponding entries of the table. The first part of the paragraph is fairly clear. 

Table 4: “The first row” actually seems like it is referring to the second row. I know what the authors mean, but it is unnecessarily confusing to refer to it in this way. One might as well refer to “the zeroth row” as listing the duration of the clip :)

The experimental evaluation: I would like to hear some of the paired samples that were played for subjects. Were classical score excerpts chosen starting at random locations in the score, or at the beginning of the score? It is known that listening to a 10-second excerpt without context can sometimes not make sense. I would be curious to see the false positives versus the false negatives. Nevertheless, I certainly appreciate the authors’ warning to interpret the listening results with caution.




Originality & Significance -- So far, based both on the techniques and the output, I am not entirely convinced of the originality or significance of this particular system. The authors refer to “rhythmically simple polyphonic scores” such as Bachbot, but I cannot see what is rhythmically fundamentally more sophisticated about the scores being generated by the present system. One nice characteristic of the present system is the true and audible independence of the voices.

One of the contributions appears to be the construction of models that explicitly leverage with shared weights some of the patterns that occur in different “places” (pitch-wise and temporally) in music. This is both very reasonable, and also not an entirely novel idea; see for example the excellent work by Daniel Johnson, “Generating Polyphonic Music Using Tied Parallel Networks” (paper published 2017, first shared online, as far as I know, in 2015: links to all materials available at http://www.hexahedria.com/2015/08/03/composing-music-with-recurrent-neural-networks/  )
Another now common (and non exclusive) way to handle some of this is by augmenting the data with transposition. It seems that the authors are not doing this here. Why not? It usually helps. 

Another contribution appears to be the use of a per-time measure of loss. This is reasonable, and I believe others have done this as well. I certainly appreciated the explicit justification for it, however.

Note that the idea of using a vector to indicate metric subdivision was also used in (Johnson 2015).

Playing through some of the scores, it is clear that melodies themselves are often quite unusual (check user studies), but the voices do stay closely connected harmonically, which is what gives the system a certain aural coherence. I would be interested to hear (and look at) what is generated in two-part harmony, and even what is generated-- as a sort of baseline-- with just a single part. 

I encourage the authors to look at and listen to the work by Johnson:
-listening samples: http://www.hexahedria.com/2015/08/03/composing-music-with-recurrent-neural-networks/
-associated publication: http://www.hexahedria.com/files/2017generatingpolyphonic.pdf

Overall, I think that the problem of generating rhythmically and polyphonically complex music is a good one, the approaches seem to generally be reasonable, although they do not appear to be particularly novel, and the musical results are not particularly impressive. The architectural choices are not always clearly presented.
			
		
","The sentiment of the review is mixed, with both positive and negative aspects highlighted. The reviewer acknowledges the reasonable approach and positive results from listening experiments but also points out several shortcomings, such as unclear sections and lack of rhythmic complexity. Therefore, the sentiment score is slightly negative. The language used is generally polite and constructive, offering specific recommendations and comparisons to other works without being dismissive or rude.",-20,80
"Summary: This paper proposes an integration of active learning for multi-task learning with policy search. This integration is built on an existing framework, EPOpt, which each time samples a set of models and a set of trajectories for each model. Only trajectories with the bottom \epsilon percentile returns will be used to update the multi-task policy. This paper proposes a way to improve the sample-efficiency so that fewer trajectories will be sampled and fewer trajectories will be loss. 

In general, the paper presentation is easy to follow. The idea is well motivated of why an active learning integration is needed. The related work is a bit too narrow, e.g. work [1] on the same approach like EPOpt or meta-learning (for model adaptation) [2] (and others more on this topic)

[1] T. Kurutach, I. Clavera, Y. Duan, A. Tamar, and P. Abbeel. Model-Ensemble Trust-Region Policy Opti
mization. In ICLR, 2018.

[2] C. Finn, P. Abbeel, and S. Levine. Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks. In ICML, 2017.

In overall, I have major concerns regarding to the proposed framework.
- Active learning is a method that is in general known to be an optimal trade-off between exploration vs. exploitation in finding a global optimal solution. That means, the proposed use of linear stochastic bandits is trying to find an optimal arm \theta^* (the worst trajectory) that gives the highest reward (the lowest return). In my opinion, integrating this idea naively into EPOpt to sample a set of trajectories would only aim to find the worst trajectory among all trajectories from all models. This is clearly not enough to say ""finding ALL the WORSE regions among trajectory space"" to improve the policy. Therefore, a new way of integration or a new objective should be used in order to make a principled framework. 

- The statement over sample-efficiency gain vs. EPOpt in Section 4 is too loose which is not based on any detailed analysis or further theoretical results.

- The experiment results are not well presented: there is no results for EPOpt in Fig. 1; 


Minor comments:

- Algorithm 1: argument of GetTrajectory (in LEARN) should be \theta_i, instead of \pi_\theta_i?.


In conclusion, the proposed framework is not yet principled. Experiment results are too preliminary and not well presented. ","The sentiment of the review is mixed but leans towards negative. The reviewer acknowledges that the paper is easy to follow and the idea is well-motivated, which is positive. However, the reviewer expresses major concerns about the proposed framework, its integration, and the presentation of experimental results. This results in a sentiment score of -40. The language used in the review is generally polite, with constructive criticism and specific recommendations for improvement, leading to a politeness score of 60.",-40,60
"The paper addresses the incremental few-shot learning problem where a model starts with base network and then introduces the novel classes, building a connection between novel and base classes via an attention module.

Strengths:
+ clear writing. 
+ the experiments are compared with related work and the ablation studies can verify the effectiveness of the proposed (or ""introduced"" would be a precise term) recurrent BP.

Weakness:

- [Novelty]
The paper title is called attention attractor network, which shares very relevance to previous CVPR work (Gidaris & Komodakis, 2018). So the first thing I was looking for is the clear description of the difference between these two. Unfortunately, in related work, authors mention the CVPR work without stating the difference (last few lines in Section 2). As such, I don't see much novelty in the paper compared with previous work. Eqn. (7)-(10) explicitly describes the attention formula. What's the distinction from the CVPR work?

- [Motivation of the regularizer using Recurrent BP is not clear]
The use of recurrent BP is probably the most distinction from previous work. However, I don't see a clear description on why such a technique is necessary.

Starting from the first line in Section 3.3, ""since there is no closed-form of the regularizer in Eqn (13)"", E needs BPTT or the introduced recurrent BP. This part is simply a re-adaption of other algorithms. A very simple question is, how about use other regularizers to replace Eqn (13)? 

- [Some experiments missing]
The experiments section 4.6 uses a case of None and ""best WD"" to address some of my concerns. This is good. Does the ""gamma random"" indicates only E is used without the ||W||^2? why the best WD for one-shot is zero? This implies the model is best for applying no weight decay?

What's the effect of using the recurrent BP technique to the CVPR work? Is there some similar improvement? If yes, then the paper makes some contribution by the regularization. If not, what's the reason?

How about using the truncated BPTT with a larger T?

In general, I think the recurrent BP part should be the highlight of the paper and yet authors fail to spread such a spirit in the abstract or title. And there are some experiments missed as I mentioned above.
","The sentiment of the review is mixed but leans towards the negative side. The reviewer acknowledges some strengths of the paper, such as clear writing and effective experiments, but raises significant concerns about the novelty, motivation, and completeness of the experiments. Therefore, the sentiment score is -40. The politeness of the language is generally respectful and constructive, even when pointing out weaknesses. The reviewer uses polite language and phrases like 'unfortunately' and 'I don't see,' which indicate a polite but critical tone. Therefore, the politeness score is 60.",-40,60
"To the best of my understanding the paper proposes some methodological ideas for visualizing and analyzing representations. 
The paper is unclear mainly because it is a bit difficult to pinpoint the contribution and its audience. What would help me better understand and potentially raise my rating is an analysis of a classical model on a known dataset as a case study and some interesting findings would help make it more exciting and give the readers more incentives to try this out. Like train an AlexNet and VGG imagenet model and show that the embeddings are better aligned with the wordnet taxonomy in one of the other. This should be possible with their approach if i understand it correctly. 

pros:
- visualization and analysis is a very exciting and important topic in machine learning
- this is clearly useful if it worked
cons:
- not sure what the contribution claim for the paper is since these types of plots existed already in the literature (is it a popularization claim ?)","The sentiment of the review can be considered slightly positive. The reviewer acknowledges the importance and excitement of the topic and mentions that the proposed ideas could be useful if they worked. However, the review also points out significant issues, such as the lack of clarity in the paper and the difficulty in identifying the contribution and its audience. Therefore, the sentiment score is 20. The politeness of the language is quite high. The reviewer uses polite language, such as 'To the best of my understanding' and 'What would help me better understand,' and provides constructive feedback without being harsh or dismissive. Therefore, the politeness score is 80.",20,80
"General:
In general, this is a well-written paper and I feel pleasant to read the paper. The paper proposed a model named Pseudo Invertible Autoencoder(PIE) which combines invertible architecture and inference model.

Strength:
1. The explanation of the paper is very clear and consistent.
2. The idea is interesting. A lot of papers related to the inverse problem focus on perfect invertibility, but the author(s) emphasize the importance of invertible compression and relate PIE to the inference model.

Possible Improvements:
1. The experiments could have been more convincing: 1) The only competitors are VAE and WAE. 2)The only data set has been tested was MNIST data set. There are many great works mentioned in the paper and those works should also be compared in a way.
2. The content could be more compact so that more experiments can be shown to support the paper. It seems to me there is too much explanation to previous works in the paper. 
3. The paper has 9 pages which exceed the suggestion a little bit.
4. I am not sure if the author(s) checked the grammar of the paper carefully. I found quite few typos in the paper. Page 3: 'Rather then' should be 'Rather than' and 'As we are interested' should be as 'As we are interested in'; Page 4: 'Can me' should be 'Can be'; Page 6: 'Better then' should be 'Better than'; Fig.6 (b): Should it be '0' or 'g(z)'?

Conclusion:
This is a good and clean paper in general. It explains the related work and presents PIE with necessary details. My biggest concern is that empirical validation(experiment) is poor. As a conclusion, I tend to vote for weak rejection.

Minor Suggestion:
Refer to the conference instead of arXiv if the paper was already published.","The sentiment of the review is generally positive, as indicated by phrases like 'well-written paper' and 'pleasant to read.' However, the reviewer also points out significant areas for improvement, such as the need for more convincing experiments and better grammar. Therefore, the sentiment score is 50. The politeness of the language is high, as the reviewer uses polite phrases and constructive criticism, such as 'possible improvements' and 'minor suggestion,' leading to a politeness score of 80.",50,80
"This paper proposes a Bayesian extension to knowledge base embedding methods, which can be used for hyperparameter learning. My rating is based on following aspects.

Novelty. 
Applying Bayesian treatment to embedding methods for uncertainty modelling and hyperparameter tuning is not new (examples include PMF [1] and Bayesian PMF [2]), and Sec 3 can be regarded as a knowledge base extension of them with a different likelihood (MF considers user-item pairs while knowledge base considers head-edge-tile triplets). However, it seems that there is little work considering the hyperparameter tuning problems for knowledge base embeddings.

Quality & Clarity.
This paper makes two arguments. 1. Small data problems exist, and needs parameter uncertainty; 2. Bayesian treatment allows efficient optimization over hyperparameters. However, as mentioned in Sec 4 and Sec 5, they still use MAP estimations with tuned hyperparameters instead of variational distribution directly. This does not support the parameter uncertainty argument (since there is no uncertainty in parameters of the final model, i.e., those re-trained in line 10 of algorithm 1). More analysis, both theoretically and experimentally, is needed to address this argument. The hyperparameter tuning argument is well-supported by both theoretical analysis and experiments. 

My questions are mainly about experiments. Overally, I think current experiments cannot support the claims well and further experiments are needed.
1.	As mentioned above, the parameter uncertainty issue hasn’t been well verified (Figure 3 demonstrates the advantages of hyperparameter tunning instead of uncertainty in parameters).
2.	Table 1 & 2 demonstrates that hyperparameter tunning using algorithm 1 introduces performance improvement on ComplEx and DistMult. Since the Bayesian treatment is general, such an improvement should be found for other knowledge base embedding methods. 
3.	Time complexity is not analyzed (since Algorithm 1 requires re-train the models).
4.	Algorithm 1 is a one-EM-step approximation for optimizing the ELBO. How well such a algorithm approximates the optimal solution of ELBO. For example, what will happens if running line 4-10 for multiple times? Does the performance increase or decrease?

[1] Salakhutdinov and Minh, Probabilistic Matrix Factorization, NIPS 2007.
[2] Salakhutdinov and Minh, Bayesian Probabilistic Matrix Factorization using Markov Chain Monte Carlo, ICML 2008.
","The sentiment of the review is moderately positive. The reviewer acknowledges the novelty in addressing hyperparameter tuning for knowledge base embeddings, although they note that Bayesian treatment for uncertainty modeling is not new. The reviewer appreciates the theoretical and experimental support for hyperparameter tuning but points out significant gaps in the verification of parameter uncertainty and the need for further experiments. Therefore, the sentiment score is 30. The politeness of the language is high; the reviewer uses formal and respectful language throughout the review, providing constructive feedback without being dismissive or rude. Hence, the politeness score is 80.",30,80
"The paper tackles the problem of semi-supervised classification using GAN-based models. They proposed a manifold regularization by approximating the Laplacian norm using the stochastic finite difference. The motivation is that making the classifier invariant to perturbations along the manifold is more reasonable than random perturbations. The idea is to use GAN to learn the manifold. The difficulty is that (the gradient of) Laplacian norm is impractical to compute for DNNs. They stated that another approximation of the manifold gradient, i.e. adding Gaussian noise \delta to z directly (||f(z) - f(g(z+\delta))||_F) has some drawbacks when the magnitude of noise is too large or too small. The authors proposed another improved gradient approximation by first computing the normalized manifold gradient \bar r(z) and then adding a tunable magnitude of \bar r(z) to g(z), i.e., ||f(z) - f(g(z) +\epsilon \bar r(z) )||_F. Since several previous works Kumar et al. (2017) and Qi et al. (2018) also applied the idea of manifold regularization into GAN, the authors pointed out several advantages of their new regularization.

Pros:
- The paper is clearly written and easy to follow. It gives some intuitive explanations of why their method works.
- The idea is simple and easy to implement based on a standard GAN.
- The authors conduct various experiments to show the interaction of the regularization and the generator.

Cons:
- For semi-supervised classification, the paper did not report the best results in other baselines. E.g., in Table 1 and 2,  the best result of VAT (Miyato et al., 2017) is VAT+Ent, 13.15 for CIFAR-10 (4000 labels) and 4.28 for SVHN (1000 labels). The performance of the proposed method is worse than the previous work but they claimed ""state-of-the-art"" results. The paper also misses several powerful baselines of semi-supervised learning, e.g. [1,2]. The experimental results are not very convincing because many importance baselines are neglected.
- The paper does not have a significant novel contribution, but rather extends GANs (improved-GAN mostly) with a manifold regularization, which has been explored in many other works Kumar et al. (2017) and Qi et al. (2018). 

I'm wondering whether other smoothness regularizations can achieve the same effect when applied to semi-supervised learning, e.g. spectral normalization[3]. It would be better to compare with them.

References:
[1] Adversarial Dropout for Supervised and Semi-Supervised Learning, AAAI 2018
[2] Smooth Neighbors on Teacher Graphs for Semi-supervised Learning, CVPR 2018
[3] Spectral Normalization for Generative Adversarial Networks, ICLR 2018","The sentiment of the review is mixed. The reviewer acknowledges the clarity and simplicity of the paper, as well as the intuitive explanations and various experiments conducted. However, they also point out significant drawbacks, such as the lack of best results in comparison to other baselines, the absence of several powerful baselines, and the limited novelty of the contribution. Therefore, the sentiment score is slightly positive. The language used in the review is polite and constructive, providing specific feedback and suggestions for improvement without being harsh or dismissive.",20,80
"The authors present a study on multi-agent communication.
Specifically, they adapt communication to be targeted and multi-staged.
Experiments on  2 synthetic datasets and 1 3D visual dataset confirm that both additions are beneficial

Overall, this paper was somewhat clear and more importantly includes experiments on House3D, a more realistic dataset.

My main concern is the following: the method is not about targeting, but about selectively hearing.
If agents are sharing the reward then why should targeted communication be beneficial at all? Isn't the optimal strategy to just communicate everything to everyone? I understand that they should be selective at the listening side to properly integrate only the relevant information (so, attend over all received messages), but why should we expect the speaker to apriori know who this message should go to? Moreover, I don't really understand how targeted communication can even work (in the way the authors explain it) since the agents have partial information (e.g., in shapes they only see 5x5 around them), so they don't really know who is where --  but I could potentially see this working should the agents put information about their own identity and location.  So, given the positive results that the authors get, my understanding is that the signature doesn't have information about who should the recipient of the information be but more about what where the properties of the sender of this information.  So, based on my understanding, I don't feel that the flow of the story quite matches what is really happening and this might be very confusing for prospective readers. Can the authors elaborate on this, aim i getting things wrong?

There is literally no information about model size (or at least I wasn't able to find any). Is there any weight-sharing across agents? Do you obtain CommNets by using the implementations of the authors or by ablating the signature-part of your model? Moreover, why do agents have a limited view window on the SHAPES -- is (targeted) communication redundant when agents have full observability? The part about how multi-staged communication is implemented is quite cryptic at the moment -- is multi-staged the fact that the message is out-putted by processing with a recurrent unit? The messages is factorized into two parts k and u leading to a vector of size D -- what happens should we have one message of size D (rather than factorizing into 2), something like this would control for any improvements obtained from increases the parameters of the model.

Finally,  if the premises of the paper is to define more effective communication protocols, evident in the use of continuous communication, (rather than studying what form can multi-agent communication etc etc), a necessary baseline  (especially in cases where agents share reward), is to communicate the full observation (rather than a function of it).  This baseline is not presented here and it's absolutely necessary.
","The sentiment of the review is mixed. The reviewer acknowledges the clarity of the paper and the inclusion of experiments on a realistic dataset, which is positive. However, the review also contains significant concerns and criticisms about the methodology and the clarity of the explanations, which are negative. Therefore, the sentiment score is slightly negative. The politeness of the language is generally respectful and constructive, with the reviewer asking questions and suggesting improvements rather than dismissing the work outright. Thus, the politeness score is positive.",-20,60
"The paper proposes a new ""sorting"" layer in neural networks that offers
some theoretical properties to be able to learn network which are 1-Lipschitz
functions.

The paper contains what seems to be a nice contribution but the manuscript
seems to have been written in a rush which makes it full of typos
and very hard to read. This unfortunately really feels like unfinished work.

Just to name a few:

- Please check the use of \citep and \citet. See eg Szegedy ref on page 3.

- Unfinished sentence ""In this work ..."" page 3.

- ""]"" somewhere at the bottom of page 4.

- ""Hence, neural network has cannot to lose Jacobian norm... "" ???

etc...

Although I would like to offer here a comprehensive review I consider
that the authors have not done their job with this submission. ","The sentiment score is determined by the overall tone and content of the review. The reviewer acknowledges that the paper contains a 'nice contribution,' which is a positive remark. However, the majority of the review is critical, pointing out numerous typos and readability issues, and ultimately stating that the work feels 'unfinished.' This mix of a positive initial comment followed by extensive criticism results in a sentiment score of -40. The politeness score is assessed based on the language used. While the reviewer is critical, they do not use rude or offensive language. Phrases like 'please check' and 'I would like to offer' indicate a polite tone, despite the critical content. Therefore, the politeness score is 20.",-40,20
"This submission is an great ablation study on the capabilities of modern reinforcement learning to discover the causal structure of a synthetic environment. The study separates cases where the agents can only observe or they can also act, showing the expected gains of active intervention.

The experiments are so far synthetic, but it would be really interesting to see how the lessons learned extend to more realistic environments. It would also be very nice to have a sequence of increasingly complex synthetic environments where causal inference is the task of interest, such that we can compare the performance of different RL algorithms in this task (the authors only used one).

I would change the title to ""Causal Reasoning from Reinforcement Learning"", since ""meta-learning"" is an over-loaded term and I do not clearly see its prevalence on this submission.","The sentiment of the review is positive, as indicated by phrases like 'great ablation study' and 'really interesting to see'. The reviewer appreciates the work done and provides constructive suggestions for improvement. The politeness of the language is high, as the reviewer uses phrases like 'it would be really interesting' and 'I would change the title', which are polite and considerate ways to offer feedback.",80,90
"Update: see comments ""On revisions"" below.

This paper essentially introduces a label-dependent regularization to the VIB framework, matching the encoder distribution of one computed from labels. The authors show good performance in generalization, such that their approach is relatively robust in a number of tasks, such as adversarial defense.

The idea I think is generally good, but there are several problems with this work.

First, there has been recent advances in mutual information estimation, first found in [1]. This is an important departure from the usual variational approximations used in VIB. You need to compare to this baseline, as it was shown that it outperforms VIB in a similar classification task as presented in your work.

Second, far too much space is used to lay out some fairly basic formalism with respect to mutual information, conditional entropy, etc. It would be nice, for example, to have an algorithm to make the learning objective more clear. Overall, I don't feel the content justifies the length.

Third, I have some concerns about the significance of this work. They introduce essentially a label-dependent “backwards encoder” to provide samples for the KL term normally found in VIB. The justification is that we need the bottleneck term to improve generalization and the backwards encoder term is supposed to keep the representation relevant to labels. One could have used an approach like MINE, doing min information for the bottleneck and max info for the labels. In addition, much work has been done on learning representations that generalize using mutual information (maximizing instead of minimizing) [2, 3, 4, 5] along with some sort of term to improve ""relevance"", and this work seems to ignore / not be aware of this work.

Overall I could see some potential in this paper being published, as I think the approach is sensible, but it's not presented in the proper context of past work.

[1] Belghazi, I., Baratin, A., Rajeswar, S., Courville, A., Bengio, Y., & Hjelm, R. D. (2018). MINE: mutual information neural estimation. International Conference for Machine Learning, 2018.
[2] Gomes, R., Krause, A., and Perona, P. Discriminative clustering by regularized information maximization. In NIPS, 2010.
[3] Hu, W., Miyato, T., Tokui, S., Matsumoto, E., and Sugiyama, M. Learning discrete representations via information maximizing self-augmented training. In ICML, 2017.
[4] Hjelm, R. D., Fedorov, A., Lavoie-Marchildon, S., Grewal, K., Trischler, A., & Bengio, Y. (2018). Learning deep representations by mutual information estimation and maximization. arXiv preprint arXiv:1808.06670.
[5] Oord, Aaron van den, Yazhe Li, and Oriol Vinyals. ""Representation learning with contrastive predictive coding."" arXiv preprint arXiv:1807.03748 (2018).","The sentiment of the review is mixed. The reviewer acknowledges the potential of the paper and the general sensibility of the approach, which is positive. However, the review also highlights several significant issues, such as the lack of comparison to recent advances, excessive space used for basic formalism, and concerns about the significance of the work. Therefore, the sentiment score is slightly positive. The politeness of the language is generally high. The reviewer uses polite language, such as 'it would be nice' and 'I think the approach is sensible,' and provides constructive criticism without being rude or dismissive.",20,80
"Summary:
The role of auxiliary tasks is to improve the generalization performance of the principal task of interest. So far, hand-crafted auxiliary tasks are generated, tailored for a problem of interest. The current work addresses a meta-learning approach to automatically generate auxiliary tasks suited to the principal task, without human knowledge.  The key components of the method are: (1) meta-generator; (2) multi-task evaluator. These two models are trained using the gradient-based meta-learning technique (for instance, MAML).  The problem of image classification is considered only, while authors claimed the method can be easily applied to other problems as well. 

Strengths:
- To my best knowledge, the idea of applying the meta-learning to the automatic generation of auxiliary tasks is novel. 
- The paper is well written and easy to read.
- The method nicely blends a few components such as self-supervised learning, meta-learning, auxiliary tasks into a single model to tackle the meta auxiliary learning. 

Weakness:
- The performance gain is not substantial in experiments. I would like to suggest to use the state-of-the-arts classifier for the principal task and to evaluate how much gain your method can get with the help of auxiliary tasks. You can refer to the state-of-the-arts performance on CIFAR.
- If the information on the hierarchy of sub-categories is not available, it will be an annoying hyperparameters that should be well tuned.
","The sentiment of the review is generally positive, as evidenced by the acknowledgment of the novelty of the idea, the clarity of the writing, and the effective integration of various components into a single model. However, there are some criticisms regarding the performance gain and potential hyperparameter tuning issues, which slightly temper the overall positivity. Therefore, the sentiment score is 70. The politeness of the language is high, as the reviewer uses polite suggestions and constructive feedback without any harsh or rude language. Thus, the politeness score is 90.",70,90
"The paper proposes a novel adversarial attack on deep neural networks. It departs from the mainstream literature in two points: 
1. A 'federated' learning setting is considered, meaning that we optimize a DNN in parallel (imagine a map-reduce approach, where each node performs SGD and then a central server (synchronously) updates the global parameters by averaging over the results of the nodes) and an attacker has control over one of the nodes.
2. The treat model is not the common data poisoning setting, but 'model poisoning' (the attacker can send an arbitrary parameter vector back to the server).

The paper, which is well written, starts with proposing a couple of straightforward (naive) attacks, which are subsequently used as a baseline. Since there (apparently) is no direct related work, these baselines are used in the experimental comparisons. Then the authors propose a more sophisticated attacks, based on alternatingly taking a step into the attack direction (to get an effective attack) and minimizing the loss (to Camouflage the attack), respectively. They add also the feature of restricting the solution being not to far away from the usual benign SGD step.

All in all, I am acknowledging that his paper introduces the federated learning paradigm to 'adversarial examples' subcommunity of ICLR and would make for good discussions at a potential poster. I find the used method slightly oversimplistic, but this is maybe fine for a proof of concept paper. 

Final judgement: For me this paper is a 6-7 rating paper; a nice addition to the program, but not a must-have.

A have a question to the authors that is important to me: it seems that the baseline attack could be very very simply detected by checking on the server the norm of the update vector of the attacked node. Since the vector has been boosted, the norm will be large. While your distance-based regularization somewhat takes that effect away, it remains unclear to what amount. Can you give me some (empirical) details on this issue? / or clarify if I am completely off here?  thank you","The sentiment of the review is moderately positive, as the reviewer acknowledges the novelty and potential of the paper, giving it a rating of 6-7 and suggesting it would be a good addition to the program. However, the reviewer also notes that the method is slightly oversimplistic, which tempers the overall enthusiasm. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer uses polite phrases such as 'I am acknowledging,' 'would make for good discussions,' and 'thank you,' and poses questions in a respectful manner. Thus, the politeness score is 90.",60,90
"The paper proposes a framework for training generative models that work on composed data. The models are trained in an adversarial fashion. The authors apply it to decompose foreground/background parts on MNIST images, and to perform sentence composition/decomposition.

High level comments:
* Clarity: In terms of language and writing style, the paper is written very clearly and easy to follow. In terms of presentation, there are some details that are omitted which would have made understanding easier and the work more reproducible.
* Quality: The idea that is introduced seems intuitive and reasonable, but the experiments does not have enough details to prove that this method works (i.e. no quantitative results presented).  Moreover, the presentation of the method is not very well done (missing details), especially since the authors used the upper limit of 10 pages.
* Originality: I am not familiar with the literature of generative models to judge this precisely, but according to the related work section it sounds like an original idea that is worth sharing.
* Significance: I believe the idea of modeling data composition explicitly sounds intuitive and interesting, and it is worth sharing. However, the experimental section does not have enough evidence that it is actually possible to learn this, so it is not clear whether the contribution is significant.

Pros:
-	interesting new problem formulation 
-	simple and clear language
-	the theoretical analysis in the last section could be interesting more generally in the context of GANs
-	the framework is applied on 2 different modalities: images and text.

Cons:
-	hard to tell whether this approach works since the metrics for evaluation are not specified and there are no quantitative results in the experimental section (only 1 qualitative example per task)
-	the work is not reproducible due to the lack of details (see more explanations below)
-	the theoretical analysis is a standalone piece of the paper, without any discussion about the implications, or making connections to the previous sections.

Detailed comments:
1.	I believe the weakest part of this paper is the evaluation section. The authors run their framework on 4 tasks of increasing difficulty. While the MNIST examples make for a nice and intuitive qualitative analysis, the are no quantitative results at all. The only result that is reported for each task is one qualitative picture. The authors make statements such as “The decomposition network learns to decompose the digits and backgrounds correctly” , “Given one component, decomposition function and the other component can be learned.” but there is not mention for how these conclusion are made (no metrics, no numbers). Indeed, it is difficult in general to quantify the results of generative models, but most other GAN papers introduce some sort metric that can be used to aggregate the evaluation on an entire dataset. If the authors manually inspected the results, they should at least report how many images they inspected and how many looked correct. 
2.	Aside from evaluation, there are some other details missing from the presentation. The individual details may not be major, but because all of these are missing together, it really affects the overall quality of the paper. For example:
    	 the authors state: “To train discriminator(s), a regularization is applied. For brevity, we do not show the regularization term (see Petzka et al. (2017)) used in our experiments.”. For reproducibility purposes, I believe it is important to at least mention the type of regularization, at least in the appendix. 
    	There is a parameter alpha used to balance the losses. What values was used in the experiments?
    	Choices of models are often not explained. Why did you choose that form for c(o1, o2) in section 3.3? Why DCGAN for component generators, and U-net for decomposition?
    	It is not explained in detail how the Yelp-reviews dataset is altered to achieve coherence. The authors mention that “As we sample a pair independently, the input sentences are not generally coherent but the coherence can be achieved with a small number of changes.”. However, the specific algorithm by which these changes are made is not specified, and thus it can’t be reproduced.
3.	The theoretical section is an interesting contribution, but the paper just states a list of theorems without making any connections to the applications used before, or a broader discussion about how these fit in the context of GANs more generally.
4.	My understanding is that both datasets used are created by the authors by making alterations to MNIST and Yelp-reviews dataset, thus making them to some extent synthetic datasets suited to fit this problem formulation. I would have like to see how this composition/decomposition works on existing datasets with no alterations. Does it still work? 
5.	In section 2.3, in the coherent sentence experimental setting, I don’t fully understand the design of the task. Figure 2 shows an example where composition and decomposition are not symmetric (i.e. composing then decomposing does not go back to the input sentences), although one of your losses is supposed to ensure exactly this cyclic consistency. Why not choose another problem that doesn’t directly violate your assumptions?

Minor issues: 
6.	From the related work section, it is not clear how your approach is different from Azadi et al. (2018). Please include more details.
7.	In section 2.4, you mention using Wasserstein GANs, with no further details about this model (not even a one line description). Without reading their paper, the readers of your paper could not easily follow through this section. The losses further introduced are also not explained intuitively (e.g. what do the two expectation terms in l_g_i represent?).
8.	I believe there are some errors in which tasks reference which figures in section 3.3. Should Task 2 refers to Figure 6, and Task 3 to Figure 7?
9.	What exactly is range(.) in section 4? If this refers to the interval of values that a variable can take, the saying “is a matrix of size |range(Z)| × |range(Y )|” doesn’t exactly make sense. Please define formally. 

Final remarks and advice: 
Overall, I believe the paper introduces some interesting ideas. There is definitely value in the problem definition and theoretical analysis. However, I believe the paper needs more work on presentation and evaluation, especially since the authors opted for 10 pages and according to ICLR guidelines “Reviewers will be instructed to apply a higher standard to papers in excess of 8 pages.”. Hopefully the above comments will help the authors improve this work!","The sentiment of the review is mixed but leans towards positive. The reviewer acknowledges the interesting problem formulation, clear language, and theoretical analysis, but also points out significant issues with the evaluation section, lack of reproducibility, and missing details. Therefore, the sentiment score is 20. The politeness of the language is high; the reviewer uses constructive language, provides detailed feedback, and offers advice for improvement, so the politeness score is 80.",20,80
"This paper presents a thorough and systematic study of the effect of pre-training over various NLP tasks on the GLUE multi-task learning evaluation suite, including an examination of the effect of language model-based pre-training using ELMo. The main conclusion is that both single-task and LM-based pre-training helps in most situations, but the gain is often not large, and not consistent across all GLUE tasks.

This paper represents an impressive amount of experimentation. The study and the experimental results will be useful and interesting to the community. The result that some tasks' performance are negatively correlated with each other is surprising. The paper is clearly written. 

One clarification question I have is about what the ""Single-task"" pre-training means. The paper seems to suggest that it consists of pre-training a model on the same task on which it is later evaluated. I'm confused by what this means, and how this is different from just training on that task. ","The sentiment of the review is quite positive, as evidenced by phrases like 'thorough and systematic study,' 'impressive amount of experimentation,' and 'useful and interesting to the community.' The reviewer appreciates the clarity of the writing and finds the results surprising and valuable. Therefore, the sentiment score is 80. The politeness of the language is also high, as the reviewer uses polite phrases such as 'One clarification question I have' and 'I'm confused by what this means,' which indicate a respectful and constructive tone. Therefore, the politeness score is 90.",80,90
"This paper presents a new approach in network quantization. The key insights of this paper is quantizing different layers with different bit-widths, instead of using fixed 32-bit width for all layer weights and activation in previous works. At the same time, this paper adopted the idea form both DARTS and ENAS with parameter sharing, and introduces a new differentiable neural architecture search framework. As the authors proposed, this DNAS framework is able to search efficiently and effective through a large search space.  As demonstrated in the Experiment section of the paper, it achieves better validation accuracy than ResNet with much smaller model size and lower computational cost.

1. An improved gradient method in updating the network architecture and parameters compared to DARTS and ENAS. It applies the Gumbel softmax to refine the sub-graph structure without training the entire super-net through the whole process. The work is able to obtain the same level of validation accuracy on Cifar-10 as ResNet while reduce the model parameters by a large margin. 
2. The work is in the middle ground of two previous works: ENAS by Pham et al. (2018) and DARTS by Liu et al. (2018). However, there is no comparison with ENAS and DARTS in experiments. ENAS samples child networks from the super net to be trained independently while DARTS trains the entire super net together without decoupling child networks from the super net. By using Gumbel Softmax with an annealing temperature, The proposed DNAS pipeline behaves more like DARTS at the beginning of the search and behaves more like ENAS at the end. 
","The sentiment of the review is generally positive, as it highlights the novelty and efficiency of the proposed approach, as well as its superior performance in terms of validation accuracy and model size compared to ResNet. However, it also points out a significant omission in the experimental comparison with ENAS and DARTS, which slightly tempers the overall positivity. Therefore, the sentiment score is 70. The language used in the review is polite and constructive, providing specific feedback and suggestions without any negative or rude remarks. Thus, the politeness score is 90.",70,90
"The paper presents a method for training a probabilistic model for Multitask Transfer Learning. The key idea is to introduce a latent variable ""z"" per task which to capture the commonality in the task instances. Since this leads to an intractable likelihood the authors use the standard ELBO with a Variational Distribution over ""z"" defined as a Gaussian + Inverse Autoregressive Flow. For classification, the authors also show that they can combine the model with the main idea in Prototypical Networks. 

The experiments evaluate on three different task, the comparison against MAML on the toy problem is quite interesting. However, the results on the Mini-Imagenet suggest that the main contributors to the better performance are the Prototypical Networks idea and the improved ResNet. Additionally, the authors compare against MAML only on the toy task and not on their synthetic dataset. I think that the experiments need better comparisons (there have been published an improved version of MAML, or even just add results from your own implementation of MAML with the same ResNet on the 3rd task as well). 

A major issue is that the model presented is not really a Hierarchical Bayesian model as being strongly presented. It is much more a practical variational algorithm, which is not bad by no means, but I find its ""interpretation"" as a Hierarchical Bayesian method as totally unnecessary and making the paper significantly harder to read and follow than it needs to be. This is true for both the base model and the model + ProtoNet. I think that the manuscript itself requires more work as well as a better comparison of the method to baseline algorithms.


Section 2.2:

The authors start by introducing a ""Hierarchical Bayes"" model over the parameters of a Neural Network for multi-task learning. By defining the model parameters to be an implicit function of some low-dimensional noise and the hyper-parameter they shift the inference to the noise variable ""z"". One issue, which I won't discuss further, is that this defines a degenerate distribution over the parameters (a fact well known in the GAN literature), which seem counter-intuitive to call ""Bayesian"". Later, since the parameters ""w"" has vanished from the equation the authors conclude that now they can change the whole graphical models such that there is actually no distribution over the parameters of a Neural Network, while the hyper-parameter IS now the parameters of a Neural Network and the latent variable is an input to it. Mathematically, the transformation is valid, however, this no longer corresponds to the original graphical model that was described earlier. The procedure described here is essentially a Variational Model with latent variable ""z"" for each task and the method performs a MAP estimation of the parameters of the Generative Model by doing Variational Inference (VAE to be exact) on the latent ""z"". There is nothing bad about this model, however, the whole point of using a ""Hierarchical Bayes"" for the parameters of the Network serves no purpose and is significantly different to the actual model that is proposed. 

In section 2, the prior term p(a) in equation 7 and Algorithm 1 is missing.

Section 3:

The authors argue that they add yet another level of hierarchy in the Graphical Model with a further latent variable ""v"", which is unclear fundamentally why you need it as it can be subsumed inside ""z"" (from a probabilistic modelling perspective they play similar roles). Additionally, they either do not include a prior or on ""v"" or there is a mistake in the equation for p(S|z) at the bottom of page 4. The main motivation for this comes from the literature where for instance if we have a linear regression and ""v"" represents the weights of the last linear layer with a Gaussian Prior than the posterior over ""v"" has an analytical form. After this whole introduction into the special latent variable ""v"", the authors actually use the idea from Prototypical Networks. They introduce a valid leave-one producer for training. However, the connection to the latent variable ""v"" which was argued to be the third level of a Hierarchical Bayes model is now lost, as the context c_k is no longer a separate latent variable (it has no prior and in the original Prototypical Network although the idea can be interpreted in a probabilistic framework it is never presented as a Hierarchical Bayes).  

","The review starts by acknowledging the method presented in the paper and its key ideas, which indicates a neutral to slightly positive sentiment. However, the reviewer quickly moves on to point out several significant issues with the paper, such as the need for better comparisons in the experiments and the misrepresentation of the model as a Hierarchical Bayesian model. The language used is critical but not impolite; the reviewer provides constructive feedback and specific recommendations for improvement. Therefore, the sentiment score is slightly negative due to the critical nature of the feedback, and the politeness score is positive as the language remains respectful and constructive.",-20,60
"Focus on navigation problems, this paper proposes Q-map, a neural network that estimates the number of steps (in terms of the discount factor gamma) required to reach any position on the observable screen/window. Moreover, it is shown that Q-map can be applied for exploration, by trying to reach randomly selected goal.

Pros
1. Novel goal-based exploration scheme

Cons
1. Similar idea has been proposed before
For example, Dayan (1993) estimates the number of steps to reach any position on the map using successor representations. Discussion about this field (successor representations/features) is completely missing in the paper.
Ref:
- Peter Dayan. Improving generalization for temporal difference learning: The successor representation. Neural Computation, 5(4):613–624, 1993.
- Andre Barreto, Will Dabney, Remi Munos, Jonathan J Hunt, Tom Schaul, David Silver, and Hado van Hasselt. Successor features for transfer in reinforcement learning. In Advances in Neural Information Processing Systems, pp. 4058–4068, 2017.
- Andre Barreto, Diana Borsa, John Quan, Tom Schaul, David Silver, Matteo Hessel, Daniel Mankowitz, Augustin Zidek, and Remi Munos. Transfer in deep reinforcement learning using successor features and generalised policy improvement. In International Conference on Machine Learning, pp. 510–519, 2018.

2. Comparison to existing methods is only vaguely discussed
For example, it is claimed multiple times that UVFA requires the goal coordinates, but Q-map also requires coordinates when doing the exploration.

3. The network architecture is not clearly presented
For example, the output of the network needs to be clipped, which suggests that there is no output transform. Since the predicted output is in [0,1], it would make sense to use Sigmoid transform for each pixel and use logistic loss.

4. The proposed exploration scheme could be unnecessarily complicated
Sec.3.1 provides lengthy discussion about the drawback of eps-greedy exploration. Then in Sec.3.2, \epsilon_r is basically the same as the eps-greedy algorithm, using to randomly select an action. Isn't this a ""bad"" thing as suggested in Sec.3.1? Moreover, the new exploration scheme requires two more hyper-parameters (min/max distance threshold), which will add more complication to the already very complicated deep RL learning procedure.

5. Experiment results are limited
For the toy experiment in Sec.2.3, the map are relatively simple. The example of Dayan (1993) with an agent surrounded by walls is an interesting scenario and should be included. The proposed Q-map (ConvNet) could fail because it is hard to learn geodesic distance with only local information. More importantly, there is no comparison to similar methods in Sec.3. UVFA can replace Q-map to do similar exploration.

6. Writing can be greatly improved
There are many grammar errors. To name a few, ""agent capable to produce"", ""the gridworld consist of"", ""in the thrist level"".

Minors
- UFV should be UVF in the introduction
- Citation in Sec.3 is not consistent with the rest of the paper. Use \citep or \citet properly.","The sentiment of the review is slightly negative, as it highlights several significant issues with the paper, such as the lack of discussion on related work, vague comparisons to existing methods, unclear network architecture, unnecessarily complicated exploration scheme, limited experimental results, and poor writing quality. However, it does acknowledge a novel goal-based exploration scheme, which adds a positive note. The sentiment score is therefore -40. The politeness of the language is neutral to slightly polite. The reviewer provides constructive criticism and specific examples to help the authors improve their work, without using harsh or rude language. The politeness score is 20.",-40,20
"
Pros:
I also study some related tasks and suspect that Wasserstein is helpful for measuring co-occurrence-based similarity. It is nice to see the effort in this direction. 

Cons:
The methods are either not very novel or not very well-motivated. The experiment results are interesting but mixed. If the doubts about the experiments are clarified and the methods are motivated better (or the strengths/weaknesses are better analyzed), I will vote for acceptance.

Related work:
In addition to the work in the related work section, some other work also studied the NLP applications of Wasserstein, especially the ones (such as [1,2,3]) which are related to similarity measurement. The authors should include them in the related work section. 

Question about experiments:
1. Why are the SIF scores reported in Table 1 much lower than the results reported in Arora et al., 2017 and in [4]?
2. If we compare CMD with DIVE + C * delta S, the proposed method wins in EVALution and Weeds, loses in Baroni, Kotlerman, BLESS, and Levy. If you compare DIVE + delta S (Chang et al. 2017) with DIVE + C * delta S, delta S also wins in EVALution and Weeds, loses in Baroni, BLESS, Kotlerman, and Levy (although CMD seems to be better than DIVE + delta S). 
Based on the fact that your method has a high correlation with DIVE + delta S (Chang et al. 2017), I guess that CMD does not work very well when the dataset contains random negative samples, but work well when all the negative samples are similar to the target words. If my guess is right, the performance should be improved on average if you multiply the scores from CMD with the word similarity measurement.
3. To make it efficient, CMD seems to sacrifice some resolutions by using the K representative context. Does this step hurt the performance? Could you provide some performance comparison with different numbers of K to let readers know whether there is a tradeoff between accuracy and efficiency?
4. Since the results are mixed, I suppose readers would like to know when this method will perform better and the reasons for having worse results sometimes.

Writing and presentation suggestions/questions:
1. If the proposed method is a breakthrough, I am fine with the title but I think the experiment results tell us that Wasserstein is not all you need. I understand that everyone wants to have an eye-catching title for their paper. The title of this paper indeed serves this purpose. Since the strategy is effective, more and more people might start to write papers with a title like this. However, having lots of paper called ""XXX is all you need?"" or ""Is XXX all you need?"" is definitely not good for the whole community. Please use a more specific title such as Context Mover Distance or something like that.
2. The last point in the contribution is not supported by experiments. I suggest that the authors move this point to the future work section.
3. It is good to see some negative results like Baroni in Table 2. Results on other datasets should not be put into Table 4 in Appendix.
4. Using Wasserstein barycenter to measure sentence similarity seems to be novel, but the motivation is not very clear. Based on A.6, we could see that for each sentence, authors basically find the representative word which is most likely to co-occur with every word in the sentence (has the highest average relatedness rather than similarity) and measure the Wasserstein distance between the co-occurrence probability distribution. I suppose sometimes relatedness is a better metric when measuring sentence similarity, but I think authors should provide some motivative sentence pairs to explain when that is the case.
Using Wasserstein to detect hypernym seems to also be novel, but the motivation is also not clear. Again, a good example would be very helpful.
This point is also related to the last question for experiments.

Minor writing suggestions:
1. In section 3, present the full name of CITE
2. If you put some important equations to the appendix (e.g., the definition of SPPMI_{alpha,gamma}), remember to point readers to the appendix. 
3. In the second paragraph of section 7, Nickel & Kiela, 2017 is a method supervised by a hierarchical structure like WordNet rather than a count-based or word embedding based methods. 
4. In Chang et al., the training dataset is not Wikipedia dump from 2015. This difference of evaluation setup should be mentioned somewhere (e.g., in the caption of Table 2).
5. The reference section is not very organized. For example, the first name of Benotto is missing for the PhD thesis ""Distributional Models for Semantic Relations: A Study on Hyponymy and Antonymy"". The arXiv papers are cited using different formats. Only some papers have URL. The venue's names are sometimes not capitalized. Gaussian embedding is cited twice, etc.


[1] Kusner, M., Sun, Y., Kolkin, N., & Weinberger, K. (2015). From word embeddings to document distances. In International Conference on Machine Learning (pp. 957-966).
[2] Xu, H., Wang, W., Liu, W., & Carin, L. (2018). Distilled Wasserstein Learning for Word Embedding and Topic Modeling. NIPS 
[3] Rolet, A., Cuturi, M., & Peyré, G. (2016, May). Fast dictionary learning with a smoothed wasserstein loss. In Artificial Intelligence and Statistics (pp. 630-638).
[4] Perone, C. S., Silveira, R., & Paula, T. S. (2018). Evaluation of sentence embeddings in downstream and linguistic probing tasks. arXiv preprint arXiv:1806.06259.
","The sentiment of the review is mixed. The reviewer acknowledges the effort and potential of the work but also points out significant shortcomings in the novelty and motivation of the methods, as well as mixed experimental results. This results in a sentiment score of -20. The language used is generally polite and constructive, offering specific suggestions for improvement without being harsh or dismissive, resulting in a politeness score of 80.",-20,80
"This paper presents a laudable attempt to generalize the learning of active learning strategies to learn general strategies that apply across many different datasets that have variables of different, not pre-determined, types, and apply the learned active learning strategies to datasets that are different from what they have been learned with. The paper is written quite clearly and is clear in its discussion of what its advance is beyond the current state of the art.

Unfortunately, the motivation of the details of the algorithm and the experiment analysis leave the paper short of what is needed to truly assess the value of this area of work and; therefore, short of what is needed for publication in ICLR. The most notable shortcoming is on page 4, at the bottom, where the actions are described. Among the components of the actions are statistics related to the dataset---the average distance from the chosen point to all the labeled data, and the average distance from the chosen point to all the unlabeled data. The authors do not provide a motivation for the use of these particular statistics. Additionally, the authors did not explore any other statistics. I should think that statistics relevant to the sparsity of the data (e.g., how well they cluster). Additionally, what distance measure is being used? A variety of distance metrics should be explored, such as d-separation for continuous variables and Hamming distance for discrete variables, should be tested, as they intuitively seem likely to affect the results. Additionally, many values are chosen for the experiments without motivation and without testing a variety of values (e.g., 30 for the size of the dataset used to calculate the reward, 1000 RL iterations, and others).

In the experiments, there needs to be discussion of how much variety there is in the different datasets in terms of their statistical properties that are relevant to active learning, such as how well the data cluster? That would help in understanding why the new algorithm performs as it does relative to the baseline.

One relatively minor point: The authors state on page 3, ""For example, the probability that the classifier assigns to a datapoint suits this purpose because most classifiers estimate this value."" This is a bit misleading---only generative classifiers would do this, not discriminative classifiers.

Pros:
1. Very clear writing.
2. Good motivation for the general problem.
3. Precise description of algorithm.

Cons:
1. Poor motivation for the particular algorithm implementation---features used in the actions, parameter values chosen.
2. Lack of experiments with different choices for features and parameter values.
3. Lack of assessment of the dataset characteristics and how they relate to algorithm performance.","The sentiment of the review is mixed. The reviewer acknowledges the clear writing, good motivation for the general problem, and precise description of the algorithm, which are positive aspects. However, the reviewer also points out significant shortcomings in the motivation of the algorithm details, lack of exploration of different statistics and parameter values, and insufficient assessment of dataset characteristics. Therefore, the sentiment score is slightly negative. The politeness of the language is high. The reviewer uses polite and constructive language throughout the review, providing specific recommendations for improvement without being rude or dismissive.",-20,80
"
-- Contribution, Originality, and Quality --

This paper has presented two approaches for transfer learning in the reinforcement learning (RL) setting: max-ent GPI (Section 3.1) and DC (Section 3.2). The authors have also established some theoretical results for these two approaches (Theorem 3.1 and 3.2), and also demonstrated some experiment results (Section 5).

These two developed approaches are interesting. However, based on existing literature (Barreto et al. 2017; 2018, Haarnoja et al. 2018a), neither of them seems to contain *significant* novelty. The derivations of the theoretical results (Theorem 3.1 and 3.2) are also relatively straightforward. The experiment results in Section 5 are interesting.

-- Clarity --

I have two major complaints about the clarity of this paper. 

1) Section 4 of the paper is not well written and is hard to follow.

2) Some notations in the paper are not well defined. For instance

2a) In page 3, the notation \delta has not been defined.
2b) In page 6, both notation V_{\theta'_V} and V'_{\theta_V} have been used. I do not think either of them has been defined. 

-- Pros and Cons --

Pros:

1) The proposed approaches and the experiment results are interesting.

Cons:

1) Neither the algorithm design nor the analysis has sufficient novelty, compared to the typical standard of a top-tier conference.

2) The paper is not very well written, especially Section 4.

3) For Theorem 3.2, why not prove a variant of it for the general multi-task case?

4) It would be better to provide the pseudocode of the proposed algorithm in the main body of the paper.","The sentiment of the review is mixed. The reviewer acknowledges that the proposed approaches and experimental results are interesting, but they also point out that the novelty of the work is limited and the paper is not well written, especially Section 4. Therefore, the sentiment score is slightly negative. The language used in the review is polite, as the reviewer provides constructive criticism and specific examples of issues without using harsh or disrespectful language.",-20,80
"The paper introduces RL based approach to prune layers in a DenseNet. This work extends BlockDrop to DenseNet architecture making the controller independent form the input image. The approach is evaluated on CIFAR10 and CIFAR100 datasets as well as on ImageNet showing promising results.

In order to improve the paper, the authors could take into consideration the following points:

1. Given the similarity of the approach with BlockDrop, I would suggest to discuss it in the introduction section clearly stating the similarities and the differences with the proposed approach. 
2. BlockDrop seems to introduce a general framework of policy network to prune neural networks. However, the authors claim that BlockDrop ""can only be applied to ResNets or its variants"". Could the authors comment on this? 
3. In the abstract, the authors claim: ""Our experiments show that DenseNet with LWP is more compact and efficient than existing alternatives"". It is hard to asses if the statement is correct given the evidence presented in the experimental section. It is not clear if the method is more efficient and compact than others, e. g.  CondenseNet. 
4. In the experimental section, addressing the following questions would make the section stronger: What is more important FLOPs or number of parameters? What is the accuracy drop we should allow to pay for reduction in number of parameters or FLOPs?
5. For the evaluation, I would suggest to show that the learned policy is better than a random one: e. g. not using the controller to define policy (in line 20 of the algorithm) and using a random random policy instead.
6. In Table 1, some entries for DenseNet LWP are missing. Is the network converging for this setups? 
7. \sigma is not explained in section 3.3. What is the intuition behind this hyper parameter?
8. I'd suggest moving related work section to background section and expanding it a bit.
9. In the introduction: ""... it achieved state-of-the-art results across several highly competitive datasets"". Please add citations accordingly.

Additional comments:
1. It might be interesting to compare the method introduced in the paper to a scenario where the controller is conditioned on an input image and adaptively selects the connections/layers in DenseNet at inference time.
2. It might be interesting to report the number of connections in Table 1 for all the models.

Overall, I liked the ideas presented in the paper. However, I think that the high degree of overlap with BlockDrop should be addressed by clearly stating the differences in the introduction section. Moreover, I encourage the authors to include missing results in Table 1 and run a comparison to random policy. In the current version of the manuscript, it is hard to compare among different methods, thus, finding a metric or a visualization that would clearly outline the ""efficiency and compactness"" of the method would make the paper stronger.","The sentiment of the review is generally positive, as the reviewer acknowledges the promising results and expresses appreciation for the ideas presented in the paper. However, the review also includes several critical points and suggestions for improvement, which slightly temper the overall positivity. Therefore, the sentiment score is 50. The language used in the review is polite and constructive, with suggestions framed in a helpful manner rather than as harsh criticisms. The reviewer uses phrases like 'I would suggest,' 'could the authors comment,' and 'I'd suggest,' which indicate a polite tone. Therefore, the politeness score is 80.",50,80
"Unfortunately, while this is interesting work, the authors emails are listed on the first page and the acknowledgments are very revealing. I am a big fan of Google, UCL, and the Royal Society, and this strongly biases my view of the work. 

My biased review:

- the paper is interesting, and should go to another venue. I do not think the authors will get benefit from presenting this work at ICLR (there is a tiny quantum focus).

- how is the cost function justified? I'd be curious to see how the authors derived it. Right now above Eq 2.4 it seems like it is heuristic to balance successful/erroneous/inconclusive rates. If it is a heuristic, the paper should clearly state this. 

- using simple examples of quantum data and quantum states would go a long way towards helping me understand the problem setup (Eq 2.1). It took me a while to grok this.

- The acronym POVM is never defined. ","The sentiment score is derived from the overall tone and content of the review. The reviewer acknowledges the work as interesting but suggests it is not suitable for the current venue, which indicates a mixed sentiment. The sentiment score is therefore set at -20, reflecting a slightly negative sentiment due to the recommendation to submit elsewhere. The politeness score is based on the language used in the review. The reviewer uses polite language, such as 'I'd be curious to see' and 'should clearly state this,' which indicates a respectful and constructive tone. Therefore, the politeness score is set at 80.",-20,80
"The paper proposes a method for using auxiliary tasks to support the optimization with respect to a main task. In particular, the method assumes the existence of a loss function for the main task that we are interested in, and a loss function for an auxiliary task that shares at least some of the parameters with the main loss function. When optimizing for the main loss function, the gradient of the auxiliary loss function is also used to update the shared parameters in cases of high cosine similarity with the main task. The method is demonstrated on image classification and a few reinforcement learning settings.

The idea of the paper is simple, and the method has a nice property of (if ignoring some caveats) guaranteeing steps that are directionally correct with respect to the main task. In that sense it is useful in practice, as it limits the potential damage the auxiliary task does to the optimization of the main task.

As the authors also note, the method suffers from some drawbacks. Although the method limits the negative effect of the auxiliary task on the optimization of the main loss function, it can still slow down optimization if the auxiliary task is not well chosen. In that sense, the method is no silver bullet. In addition, the method seems fairly computationally expensive (it would be interesting to understand how much it slows down an update, I would assume the added complexity is roughly a constant multiplier). However, as an alternative to naively adding an auxiliary task, the proposed method is a welcome addition to the tool box of practitioners.

Although the experiments presented in the paper are quite different from each other, I would have wished for even more experiments. The reason is that as the method does not guarantee faster convergence, its applicability is mainly an empirical question. Especially experiments where auxiliary tasks have been used before would be interesting to test with the only addition being introducing the method proposed.

The paper is generally well written and the results are fairly clearly presented. As a minor comment, the authors might want to check that articles (such as ""the"") are not missing in the text.

All in all, the main merit of the proposed method is its conceptual simplicity and easy to understand value in practical applications where an auxiliary loss function is available. The method also seems to work well enough in the experiments presented.","The sentiment of the review is generally positive, as the reviewer acknowledges the conceptual simplicity and practical value of the proposed method, despite noting some drawbacks and areas for improvement. The sentiment score is therefore 60. The language used in the review is polite and constructive, offering suggestions and minor comments without being harsh or dismissive. The politeness score is 80.",60,80
"The paper is well written and addresses an interesting problem. Overall, I do find the federated dropout idea quite interesting. As for the lossy compression part, I am a bit skeptical on its application for this problem. In general, I believe that the manuscript could greatly benefit from answering the questions that I am raising below. It would certainly help me better appreciate the contributions of this work. 

The lossy aspect of the compression inevitably introduces performance downgrades. However, compression/communication systems are designed to make sure that the information dropped is not important for the task at hand (e.g., high frequencies that are not perceived by our eyes in the spatial domain are typically dropped when compressing images through zig zag scanning after transformation). Randomly dropping coefficients as suggested in this paper seems odd to me (the subsampling technique that is used). Can you justify this approach? The manuscript does hint that this approach provides lukewarm results. Could there be a better approach that focuses on parts of the model that deemed “less” important if a notion of coefficient importance can be derived? 

Can you emphasize more the benefits of compression and federated drop out, versus training a low capacity model with less parameters? The introduction refers to the low capacity approach as a naive model. Could this be compared experimentally? This would help better appreciate the benefits of the federated dropout strategies that are proposed here. In the experiments, could you explain why increases in q (quantization steps) seems to lead to limited or marginal accuracy improvements? 

For the results shown in Figure 4, did you also use any form of subsampling and quantization? Also, do you have a justification for why with some amounts of dropout, the accuracy may improve but at a slower pace (pretty much the punch line of these experiments)? It is an interesting finding but it is counter intuitive and requires explanations in my view. 

On the communication cost experiments, can you explain precisely how did you compute these reduction factors? Did you tolerate some form of accuracy degradation? Also, did you consider the fact that more ""rounds"" are needed to get to a target accuracy level? Is there a cost associated with these additional rounds and was that cost taken into consideration? Adding clarity on this would certainly help. ","The review starts with a positive note, acknowledging that the paper is well-written and addresses an interesting problem. The reviewer finds the federated dropout idea interesting but expresses skepticism about the lossy compression part. The review is constructive, providing detailed questions and suggestions for improvement. The language used is polite and professional, aiming to help the authors improve their manuscript.",50,80
"pros:

- Clearly written and sound paper.
- Addresses interesting problem. 
- Improves existing methods used for this learning scenario.  

cons:

- The core contribution is a special case of previously published more general framework which is not cited in the paper.

It is clearly written paper with a good motivation. The major problem is that the core contribution, namely, the risk reformulation in Theorem 1 and the derived loss (6), are special cases of more general framework published in 
   Jesus Cid-Sueiro et al. Consistency of Losses for Learning from Weak Labels. ECML 2014.

The work of [Cid-Sueiro2014] proposes a general way how to construct losses for learning from weak labels. They require that the distribution of weak labels is a linear transformation of the true label distribution, i.e. the assumption (3) of the paper under review. According to [Cid-Sueiro2014], the loss on weak labels is constructed by $weak_loss = L*original_loss$, where $L$ is the left inversion of the ""mixing matrix"" $T$ in (3). [Cid-Sueiro2014] also shows that such weak loss is classification calibrated which implies statistical consistency of the method. 

Learning from complementary labels is a special case when the mixing matrix is $T=(E-I)/(K-1)$ (E is unitary matrix, I is matrix of ones, K is number of labels). In this case, the left inversion of $T$ is simply $L=- E*(K-1) + I$ and so the weak loss is $weak_loss=L*loss$ which corresponds to the loss (5) proposed in the paper under review (in fact, the loss (5) also adds a constant term (Y-2)/(Y-1) which however has no effect on the minimizer). 

The novel part of the paper is the non-negative risk estimator proposed in sec 3.3 and the online optimization methods addressed in sec 3.4. These extensions, although relatively straightforward, are empirically shown to significantly improve the results.","The review starts with positive remarks about the clarity and soundness of the paper, as well as its interesting problem and improvements over existing methods. However, it also points out a significant issue regarding the core contribution being a special case of a previously published framework that is not cited. The language used is polite and constructive, providing specific references and explanations to support the critique. The sentiment score is positive due to the initial praise and acknowledgment of the novel parts, but it is tempered by the significant critique. The politeness score is high because the reviewer maintains a respectful and constructive tone throughout.",50,90
"In this work, the authors explore different ways to pre-train contextualized word and sentence representations for use in other tasks. They propose two main methods: a straight-forward extension of the ElMO model for hierarchical uni-directional language models, and a de-noising auto-encoder type method which allows to train bi-directional representations. The learned contextual representations are evaluated on three downstream tasks, demonstrating the superiority of the bi-directional training setting, and beating strong baselines on extractive summarization.

The method is clearly presented and easy to follow, and the experiments do seem to support the author's claims, but their exposition misses several important details (or could be presented more clearly). For the document segmentation task, are the articles taken from a held-out set, or are they contained in the pre-training set? For passage retrieval, is the representation the same or are the representations re-trained from scratch using paragraph blocks? What exactly are the other features (those can go in the appendix)? And for the extractive summarization task, how many sentences are selected? Is pre-training also done on Wikipedia, or are those representations trained on news text?

A comparison to non-contextualized sentence representations would also be welcome (SkipThought, InferSent, ElMO-pool for settings other than passage retrieval). Note also that the local pre-training is not equivalent to ElMO, as the later sees context form the whole document rather than just the current sentence.

It is interesting to see that contextualized sentence representations can be used and that the Mask-LM objective yields better results than L+R-LM, but these points would be better made if the above questions were answered.","The review starts with a positive sentiment, acknowledging the clear presentation and the support of the authors' claims by their experiments. However, it quickly transitions to a more critical tone, pointing out several missing details and suggesting additional comparisons and clarifications. The sentiment score is therefore moderately positive, as the reviewer recognizes the value of the work but also highlights significant areas for improvement. The language used is polite and constructive, with the reviewer providing specific questions and suggestions without being dismissive or rude.",40,80
"Major Contribution:
The paper introduces a method that combines the advantage and of model-based RL and imitation learning and offset their weakness. The method proposes a probabilistic inference approach to analyze the action of the model.

Organization/Style:
The paper is well written, organized, and clear on most points. 

Technical Accuracy:
I'm not an expert in RL. The method is obscure to me, but from my point of view, the experiments are done quite thoroughly and the results look good.

Presentation:
Good. 

Adequacy of Citations: 
The author should consider adding the related works include:
Bojarski, Mariusz, et al. ""End to end learning for self-driving cars."": using CNNs to implement imitation learning for self-driving cars

Multimedia:
Videos are helpful to understand the method and are well composed.","The review is generally positive, highlighting the major contributions of the paper and noting that it is well-written, organized, and clear. The reviewer also acknowledges the thoroughness of the experiments and the helpfulness of the multimedia. However, there is a minor suggestion to add a citation, which is presented in a constructive manner. The language used is polite and respectful throughout.",80,90
"This paper gives a model for understanding locally connected neural networks. The main idea seems to be that the network is sparsely connected, so each neuron is not going to have access to the entire input. One can then think about the gradient of this neuron locally while average out over all the randomness in the input locations that are not relevant to this neuron. Using this framework the paper tried to explain several phenomena in neural networks, including batch normalization, overfitting, disentangling, etc.

I feel the paper is poorly written which made it very hard to understand. For example, as the paper states, the model gives a generative model for input (x,y) pairs. However, I could not find a self-contained description of how this generative model works. Some things are described in Section 3.1 about the discrete summarization variables, but the short paragraph did not describe: (a) What is the ""multi-layer"" deterministic function? (b) How are these z_\alpha's chosen? (c) Given z's how do we generate x? (d) What happens if we have z_\alpha and z_\beta and the regions \alpha and \beta are not disjoint? What x do we use in the intersection?

In trying to understand the paper, I was thinking that (a)(b) The multilayer deterministic function is a function which gives a tree structure over the z_\alpha's, where y is the root. (I have no idea why this should be a deterministic function, intuitively shouldn't y be chosen randomly, and each z_\alpha chosen randomly conditioned on its parent?)  (c) there is a fixed conditional distribution of P(x_\alpha|z_\alpha), and I really could not figure out (d). The paper definitely seems to allow two receptive fields to intersect as in Figure 1(b).

Without understanding the generative model, it is impossible for me to evaluate the later results. My general comments there is that there are no clear Theorems that summarizes the results (the Theorems in the paper are all just Lemmas that are trying to work towards the final goal of giving some explanations, but the explanations and assumptions are not formally written down). Looking at things separately (as again I couldn't understand the single paragraph describing the generative model), the Assumption in Theorem 3 seems extremely limiting as it is saying that x_j is a discrete distribution (which is probably never true in practice). I wouldn't say ""the model does not impose unrealistic assumptions"" in abstract if you are going to assume this, rather the model just makes a different kind of unrealistic assumptions (Assumptions in Theorem 2 might be much weaker, but it's hard to judge that).

==== After reading the revision

The revised version is indeed more clear about how the teacher network works, and I have tried to understand the later parts of the paper again. The result of the paper really relies on the two assumptions in Theorem 2. Of the two assumptions, the first one seems to be intuitive (and it is OK although exact conditional independence might be slightly strong). The second assumption is very unclear though as it is not an assumption that is purely about the model/teacher network (which are the x and z variables), it also has to do with the learning algorithm/student network (f's and g's). It is much harder to reason about the behavior of an algorithm on a particular model and directly making an assumption about that in some sense hides the problem. The paper mentioned that the condition is true if z is fine-grained, but this is very vague - it is definitely true if z is super fine-grained to satisfy the assumption in Theorem 3, but that is too extreme.

Overall I still feel the paper is a bit confusing and it would benefit from having a more concrete example. I like the direction of the work but I can't recommend for recommendation at this stage.","The sentiment score is derived from the overall tone and content of the review. The reviewer acknowledges the direction of the work and the improvements made in the revision, but ultimately finds the paper confusing and not ready for recommendation. This mixed feedback results in a sentiment score of -20. The politeness score is based on the language used throughout the review. The reviewer provides constructive criticism and specific suggestions without using harsh or rude language, resulting in a politeness score of 50.",-20,50
"This paper formulates a new method called human-guided column networks to handle sparse and noisy samples. Their main idea is to introduce human knowledge to guide the previous column network for robust training.

Pros:

1. The authors find a fresh direction for learning with noisy samples. The human advice can be viewed as previledged information.

2. The authors perform numerical experiments to demonstrate the efficacy of their framework. And their experimental result support their previous claims.

Cons:

We have three questions in the following.

1. Motivation: The authors are encouraged to re-write their paper with more motivated storyline. The current version is okay but not very exciting for idea selling. For example, human guidance should be your selling point, and you may not restrict your general method into ColumnNet, which will limit the practical usage.

2. Related works: In deep learning with noisy labels, there are three main directions, including small-loss trick [1], estimating noise transition matrix [2,3], and explicit and implicit regularization [4]. I would appreciate if the authors can survey and compare more baselines in their paper instead of listing some basic ones.

3. Experiment: 
3.1 Baselines: For noisy labels, the authors should add MentorNet [1] as a baseline https://github.com/google/mentornet From my own experience, this baseline is very strong.
3.2 Datasets: For datasets, I think the author should first compare their methods on symmetric and aysmmetric noisy data. Besides, the authors are encouraged to conduct 1 NLP dataset.

By the way, if your human guidance is totally wrong, how your model handle such extreme cases? Could you please discuss this important point in your paper?

References:

[1] L. Jiang, Z. Zhou, T. Leung, L. Li, and L. Fei-Fei. Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels. In ICML, 2018.

[2] G. Patrini, A. Rozza, A. Menon, R. Nock, and L. Qu. Making deep neural networks robust to label noise: A loss correction approach. In CVPR, 2017.

[3] J. Goldberger and E. Ben-Reuven. Training deep neural-networks using a noise adaptation layer. In ICLR, 2017.

[4] T. Miyato, S. Maeda, M. Koyama, and S. Ishii. Virtual adversarial training: A regularization method for supervised and semi-supervised learning. ICLR, 2016.","The sentiment of the review is generally positive, as the reviewer acknowledges the novelty and efficacy of the proposed method. However, there are several constructive criticisms and suggestions for improvement, which slightly temper the overall positivity. Therefore, the sentiment score is 50. The politeness of the language is high, as the reviewer uses phrases like 'the authors are encouraged' and 'I would appreciate,' which are polite ways to offer criticism and suggestions. Thus, the politeness score is 80.",50,80
"The paper presents and evaluates different common inductive biases in Deep RL. These are systematically evaluated on different experimental settings.

The paper is easy to read and the authors explain well the setting and their findings. The comparison and evaluations is well conducted and valuable contribution to the literature.  I would have liked some more details on the motivating example in section 3.1, maybe with a figure supporting the explanation of the example. ","The review starts with a positive sentiment, highlighting that the paper is easy to read and well-explained. It also mentions that the comparison and evaluations are valuable contributions to the literature. The only critique is a minor suggestion for additional details in one section, which is phrased politely. Therefore, the sentiment score is high, and the politeness score is also high.",90,100
"After rebuttal, I adapted the score. See below for original review.
--------------------------------------------


The authors implement a two-stage multi-objective optimization scheme to optimize neural network architectures with several conflicting goals.
I can not accept the paper in its current form.

In short, I have the following main criticisms:
1. use of crowding distance(CD) instead of hypervolume-contribution.
CD is not consistent with the HV estimator, especially CD might remove solutions that have a large HV-contribution and thus HV will not increase monotonically. The effect is even visible in Figure 8c) as in iteration 22, HV is decreasing as crowding distance removes a good offspring. In short: Crowding distance should not be used as long as the number of objectives does not prohibit computing the HV-contribution.

2. No good justification of BN. It is unclear to me why BN should be used instead of more iterations at stage 1. In 4.4 BN is only compared to the uniform initialization, but this comparison has no meaning given that we already have an optimized front that improved on the uniformly sampled distribution. To be honest, the samples shown from BN do not look very convincing as a lot of very poor architectures are created.

A proper comparison would be comparing the 2-step approach with only the first step and the same budget. Then we could compare samples from both distributions (either sampling from the front using mutation/crossover or sampling from BN). Also we would have a fair comparison of the obtained fronts and HV-values.

3. Ablation study cross-over
I am not convinced by the results presented. The paper says this is a ""small scale"" study but does not give the number of iterations/samples. It is clear that in the setup of the mutation operator cross-over might help, simply because it can change many more connections in a single iteration than mutation alone, which is limited to max 1 change. Allowing up to two mutations and no crossover could already proof to be better (orsmaller size of offspring population, see below)


Smaller concerns:

1. The results suggest that the uniform distribution might not be tuned well, as it only covers the ""expensive"" networks but not the ""cheap"" networks. A better initialization scheme that covers the x-axis better might already show vastly different results. As the Flop-objective is cheap to compute and does not require simulation, one could expect to tune this offline before initialization.

2. No handling of Noise.
During optimization, the chosen starting point and SGD algorithm will introduce noise into the process. Thus, the final test accuracy will be noisy. As an elitist dominance scheme is used, one might easily end up with an architecture that has a large variance when trained, i.e. when performing a final training pass on the full dataset, the performance might be very different. Moreover, the algorithm might stop convergence towards the true pareto front as it is held back by noisy ""good"" results. This should be discussed in the paper

3. A single-offspring approach might be better than sampling a full population (or offspring size in the order of parallel instances one can expend to run). 40 sounds excessive given that the sampling distribution is only improved through selection and given that the pareto front approximation appears to include less than 40 elements. This might also affect the results in the ablation study for cross-over: more iterations with reduced offspring size allows for more mutations of successful offspring.

4. Some unclear or wrong wordings:
page 4: ""As a consequence[...] the best solution encountered [...] will always be present in the final population. "" What do you consider ""best"" in a 2-objective problem? Do you mean: the best in each objective?
page 6, footnote1: this is not true. even without crossover the selection operator ties the solutions together, an offspring has to beat any point in the population, not necessarily its direct parent.

5. Figure 8a) does not include the state of the art result for CIFAR10, see for example 
http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130","The sentiment of the review is negative, as indicated by the statement 'I can not accept the paper in its current form' and the detailed criticisms provided. The sentiment score is -80 because the reviewer highlights several significant issues without acknowledging any strengths of the paper. The politeness score is 20 because, although the review is critical, it is not rude. The reviewer uses formal language and provides constructive feedback, but the tone is firm and lacks any positive reinforcement.",-80,20
"The paper proposes a class of Evolutionary-Neural hybrid agents (Evo-NAS) to take advantage of both evolutionary algorithms and reinforcement learning algorithms for efficient neural architecture search. 

1. Doesn't explain how exactly the mutation action is learned, and missing the explanation of how RL acts on its modification on NAS (Evo-NAS). 
2. Very poor explanation on LEARN TO COUNT experiment. The experiment contains difficult setups on a toy data, which makes it difficult to repeat. In figure 3, the paper says that the sample efficiency of the Evo-NAS strongly outperforms both the evolutionary and the neural agent. However, where the strength comes from is not discussed in detail. In figure 2, the paper claims that PQT outperforms Reinforce for both the Neural and the Evo-NAS agent. For the Evo-NAS agent, the gain is especially pronounced at the beginning of the experiment. Thus, the paper concludes that PQT can provide a stronger training signal than Reinforce. However, how much stronger training signal can obtain of the proposed method is not discussed. Because the experiments of 5.1 is setup on a toy data with complicated parameters. The conclusions based on this data set is not convincing. It would be better to add comparative results on the CIFAR and Imagenet data for convenient comparisons with state-of-the-art. 
3. Confusing notation and experimental setup. In 5.1, the sequence a is first defined as <a1, a2, .., an>. Then, after eq.2, the sequence a is given as a=<1, 2, ..., n>. It would be better to use different symbols here. ","The sentiment of the review is quite negative, as it highlights several significant issues with the paper, such as poor explanations, confusing notation, and unconvincing experimental setups. The reviewer does not provide any positive feedback or acknowledgment of the paper's strengths. Therefore, the sentiment score is -80. The politeness of the language is neutral to slightly negative. The reviewer points out flaws directly without using overly harsh or rude language, but the lack of any positive or encouraging remarks makes the tone somewhat blunt. Therefore, the politeness score is -20.",-80,-20
"CAML seems an interesting meta-learning algorithm. I like the idea that the context parameters are used to modulate the whole network during the inner loop of meta-learning, while the rest of the network parameters are adaped in the outer loopand shared across tasks. Also, it is good to see that CAML is competitive with on few shot CNNs.

The paper is very well presented. Experiments are reasonably solid.

If I understood correctly, although CAML has achieved better accuracy it seems CAML still requires a decent amount of parameter/network structure optimisation. Would be good if the paper has a section talking about practical tricks of how to find the best CAML hyperparameter quickly.","The review starts with positive remarks about the CAML algorithm, highlighting its interesting approach and competitive performance. The reviewer appreciates the presentation and the solidity of the experiments. However, the review also includes a constructive suggestion for improvement regarding the optimization of parameters and network structure. The language used is polite and constructive, aiming to help the authors improve their work.",80,90
"This paper suggests to rethink about the bias-variance tradeoff from statistical machine learning in the context of neural networks. Based on some empirical observations, the main claims in this work are that (1) it is not always the case that the variance will increase when we use bigger neural network models (particularly, by increasing the network width); (2) the variance should be decomposed into two parts: one part accounts for the variance caused by random initialization of network parameters/optimization and the other part is caused by ""sampling of the training set"".

For the first claim is based the empirical observation that increasing the number of hidden units did not cause the incrase of variance (as in figure 1). However, to my understanding, it only means increasing the number of hidden units is probably not a good way to increase the network capacity. In other words, this cannot be used as an evidence that the bias-variance tradeoff is not valid in neural network learning.

For the second claim, I don't like the way that they decompose the variance into two parts. To be clear, the classical bias-variance tradeoff doesn't consider the optimization error as an issue. For a more generic view of machine learning errors, please refer to ""The Tradeoffs of Large Scale Learning"" (Bottou and Bousquet, 2008). In addition, if the proposed framework wants to include the optimization error, it should also cover some other errors caused by optimization, for example, early stopping and the choice of a optimization algorithm.

Besides these high-level issues, I also found the technical parts of this paper is really hard to understand. For example,

- what is exactly the definition of $p(\theta|S)$? The closely related case I can think about is in the Baysian setting, where we want to give a prior distribution of model (parameter). But, clearly, this is not the case here. 
- similar question to the ""frequentist risk"", in the definition of frequentist risk, model parameter $\theta$ should be fixed and the only expectation we need to compute is over data $S$
- in Eq. (5), I think I need more technical detail to understand this decomposition.","The sentiment of the review is moderately negative. The reviewer expresses disagreement with the main claims of the paper and points out several high-level and technical issues. The sentiment score is -60 because the reviewer does not find the claims convincing and highlights significant problems. The politeness score is 20 because, while the reviewer is critical, they do not use rude language and maintain a professional tone throughout the review.",-60,20
"Authors establish a connection between communication reduction in distributed optimization and dithered quantization. This allows us to understand prior approaches in a new perspective, and also motivates authors to develop two new distributed training algorithms which communication overhead is significantly reduced. The first algorithm, DQSG, uses dithered quantization to reduce the communication bits. The second algorithm, NDQSG, uses nested dithered quantization to further reduce the amount of needed communication. The usefulness of these algorithms are empirically validated by computing the raw communication bits and average entropy of them. Therefore, dithered communication seems to provide both theory and algorithm which are useful.

The paper is clearly written. It provides a succinct review of dithered quantization and previous works, and figures provide a good insight into why the algorithm works, especially Figure 3.

Theorems in this paper are mostly about plugging in properties of dithered quantization into standard results in stochastic optimization, but they are still useful. The analysis of NDQSG does not seem to be as complete as that of DQSG, however. With NQSG, now workers are divided into two groups, and there would be an interesting tradeoff between assignments to these two: how should we balance two groups? This might be tricky to analyze, but it is still useful to clarify limitations and provide conjectures. At least, this could be analyzed empirically.

pros:
* establishing a connection to other topic of research often facilitates productive collaboration between two fields
* provides a new perspective to understand prior work
* provides new useful algorithms

cons:
* experiments were conducted on small models and small datasets
* unclear models are large enough to demonstrate the need for communication reduction; in other words, it is unclear wall-time would actually be reduced with these algorithms.","The review starts with a positive sentiment, highlighting the novel connection made by the authors and the development of new algorithms. The reviewer appreciates the clarity of the writing and the usefulness of the algorithms. However, the review also points out some limitations, such as the incomplete analysis of NDQSG and the small scale of the experiments. Despite these criticisms, the overall tone remains constructive and polite, aiming to provide helpful feedback for improvement.",70,80
"This paper presents an approach to evaluate the quality of segmentations. To achieve this, a variational auto-encoder (VAE) is trained on the ground truth masks to extract shape-relevant information in the feature space, assuming that incorrect segmentations will be far from the normal distribution. Then, a regression model is trained to predict the quality of the segmentation based on the shape-learned features. The authors use several datasets focusing on pancreas segmentation to evaluate their quality-assessment approach, showing competitive performance with respect to other approaches.

The paper is well written, easy to follow in general, and the methodology is sound. Nevertheless, I have some concerns related to the applicability of this approach.

- Closely related works in the literature are missing:

There is a closely related recent work that used auto-encoders on the sets of ground-truth masks to build representations of shape and constrain the outputs of deep networks: Otkay et al., Anatomically Constrained Neural Networks (ACNN): Application to Cardiac Image Enhancement and Segmentation, IEEE TMI 2017

This work does not focus directly on quality assessment. However, I believe the loss in this work, which evaluates the difference between the obtained segmentation (characterized by the outputs of a deep network) and an auto-encoder description of shape, can be used directly as a criterion for evaluating the quality of segmentation (on validation data) in term of consistency with the shape prior. I think this work is very closely related and should be discussed. 

Also, a quick google search provided some missing references related to this work. I think including comparisons to the recent work in [1], for example, would be appropriate. As the focus is on quality assessment of medical image segmentation, I would suggest a deeper review of the literature.

[1] Vanya V Valindria, Ioannis Lavdas, Wenjia Bai, Konstantinos Kamnitsas, Eric O Aboagye, Andrea G Rockall, Daniel Rueckert, and Ben Glocker. Reverse classification accuracy: Predict- ing segmentation performance in the absence of ground truth. IEEE Transactions on Medical Imaging, 2017. 
[2] S. Chabrier, B. Emile, C. Rosenberger, and H. Laurent, “Unsupervised performance evaluation of image segmentation,” EURASIP Journal on Applied Signal Processing, vol. 2006, pp. 217–217, 2006. 
[3] Gao H, Tang Y, Jing L, Li H, Ding H. A Novel Unsupervised Segmentation Quality Evaluation Method for Remote Sensing Images. Sensors. 2017 Oct 24;17(10):2427.

- The proposed quality assessment uses the learned shape features.  Even though it is strong prior information, there  might be situations where the predicted segmentation might be plausible in terms of shape, but is not a good segmentation. 

-  I wonder how this approach works in problems with a high size/shape variation. For example, in the case of tumors, where their shape is unpredictable and each unknown case can be seen as a ‘rare’ example.

- To better capture the shape in the proposed approach, images need to be aligned, which limits the applicability of this approach to aligned volumes only. 

- This approach gives a global hint about a given segmentation result, as a whole. I think it would be more interesting to provide local information on a segmentation, as it may happen that a predicted contour is generally correct, but there are some crispy borders in some points due to low contrast, for example. Even though the quality assessment would say that the prediction is correct, the contour may be unusable for certain applications, where a minimal surface distance is required (e.g., radiotherapy).

- As the quality assessment is based on shape and not in image information, it would be interesting to see how accurately it predicts the performance on different image modalities (for example, the method is trained on ground truth masks corresponding to CT images and quality is assessed in segmentations performed in MRI).

If I understood correctly, comparison with other methods is done with the same dataset under the same conditions (i.e., all the images are pre-aligned). As the other methods might not have the limitation of requiring aligned images, it would be interesting to compare also the performances in this situation.

How the training (or the VAE) is adapted for DeepLab-3, as it is based on 2D convolutions?

Minor: The paper needs a proof-read to fix some issues (e.g. ‘the properties of F is encoded’)
","The sentiment of the review is generally positive, as the reviewer acknowledges that the paper is well-written, easy to follow, and the methodology is sound. However, the reviewer also expresses several concerns and provides constructive criticism, which slightly tempers the overall positivity. Therefore, the sentiment score is 50. The politeness of the language is high, as the reviewer uses polite phrases such as 'I believe,' 'I think,' and 'I would suggest,' and provides detailed feedback in a respectful manner. Thus, the politeness score is 90.",50,90
"The paper describes a cache side-channel attack on a deep learning model. In a cache side-channel attack, the attacker sets up a process on the same machine where the victim process (that is running the training or evaluation job for the DNN model) is running. It is assumed that the victim process uses a common shared library for DNN computations as the attacking process. The attacking process flushes the cache, then observes access times for key functions. The paper shows that, based on the speed of accessing previously flushed functions, the attacker can discover the high-level network architecture, namely the types of layers and their sequence. The paper shows that, by spying on such cache access patterns in the Tensorflow library, this method can reliably extract the above high-level information for 11 different network architectures. It also describes a few counterattack alternatives whereby the victim can obfuscate its cache access patterns for self-protection.

The significance of the results is not clear to me. The extracted information is very high level. What realistic attacks can be constructed from such a coarse-grained fingerprinting? The experimental results show that the fingerprint can be used to map the architecture to one of the 13 well-known architectures (VCC16, ResNet, DenseNet, Inception, etc.). But so what? What does the victim lose by revealing that it's using one of a few very well known types of DNNs (the ones tested in this paper). There may very well be a good reason why this is very dangerous, but that is not explained in the paper. Not being familiar with this line of research and its significance, I looked up several of the related papers (Suciu et al., 2018, Tramer et al., 2017, Papernot et al., 2017, Yan et al., 2018). None of them could explain why this particular type of fingerprinting is dangerous.

Of the cited previous work, Yan et al., 2018 seems to present the most closely related approach. The method described in that paper is very similar: cache side attack on a shared library through a co-located attacker process. They monitor at a finer grain -- Generalized Matrix Multiplications -- and are thus able to infer more details such as the size of the layers. This also makes the inference problem harder -- they were able to narrow down the search space of networks from >4x10^35 to 16 (on VGG16). On the surface, the results presented in this paper seem stronger. But they are actually solving a much easier problem -- their search space is one of 13 well-known networks. To me, Yan et al.'s approach is a much more powerful and promising setup.

Overall, while the paper is clearly written and presents the idea succinctly, it is derivative of previous research, and the results are not stronger. I'm not an expert in this area, so it's possible that I missed something. Based on my current understanding, however, I recommend reject.","The sentiment score is derived from the overall tone and content of the review. The reviewer acknowledges that the paper is clearly written and presents the idea succinctly, which is positive. However, the reviewer also expresses significant doubts about the significance and originality of the results, ultimately recommending rejection. This mixed but predominantly critical view results in a sentiment score of -40. The politeness score is based on the language used throughout the review. The reviewer uses polite and respectful language, even when expressing criticism, and acknowledges their own potential lack of expertise in the area. This results in a politeness score of 80.",-40,80
"This interesting paper tackles the problem of joint source-channel coding, by means of learning.

From 100kft heights, especially given the choice of VIMCO gradient estimates, this is effectively a ""let's embed a source-channel-decoder simulator and differentiate through it"", and find a solution that is better than source|channel factorized classic methods, or hand-tuned approaches.

The method and results are good. The authors also show some interesting results about the representations learned, about how decoded samples (images) change smoothly when the (discrete) embedding (the-codes) changes over deltas of hamming_d()=1bit. This is very good results IHMO. One limitation of this method is the fixed-code-length.

Jumping straight to my main main issue with this paper: no code was made available, at least not at this time.

While the authors do provide an extensive appendix with hyper-parameter specs, usually in my experience when dealing with discrete / monte-carlo methods, it's usually rather hard to reproduce results. I really strongly advise the authors to provide fully reproducible code for this paper, to help further research on this topic.

Besides that I have three technical comments / request regarding this paper:

1// the choice of BSC channel - while this is the easiest most natural choice, and we should certainly have results on BSC, I am left wondering why the authors didn't try other more complex / more realistic channels? The authors only mention this as potential area of future research in the last sentence of the conclusions. 

There are several reasons for this comment: first of all, it is well known that even classic joint source-channel coding methods do shine on complex channels, such fading/erasure channels and/or in general channels with correlated error sequences. Such channels are indeed key in modern wireless communications, and are easy to simulate. Given that more-complex channels could be introduced in the channel model p(y_hat|y) -  it would not change the rest of the method - it would be particularly interesting to see what results this method achieve in these more complex environments.

2// I would like to hear more about the choice of VIMCO. Understood the authors statement to ""preserve the hard discreteness"" ~ that said methods like Gumbel-SM and several others also referenced in the paper ~ have been used  successfully to solve for propagating gradients through discrete units. This is where, in my opinion, experiments comparing VIMCO approximation results to at least one other method could allow to decide / validate the best architecture. 

This is also because, in my previous experience, this type of networks with discrete units may be hard to train. I would like to hear from the authors about how stable the training was under different hyper-parameters, and perhaps see some convergence curves for the loss function(s).

3// it's not 100% clear to me where the limitation of fixed code-length come into play from the architecture. Could the authors please point this out clearly?

Thank you!
","The sentiment of the review is generally positive, as indicated by phrases like 'interesting paper,' 'method and results are good,' and 'very good results IHMO.' However, the reviewer does express a significant concern about the lack of available code, which slightly tempers the overall positivity. Therefore, the sentiment score is 70. The politeness of the language is high, as the reviewer uses polite phrases such as 'I really strongly advise,' 'I would like to hear more,' and 'Could the authors please point this out clearly?' This indicates a respectful and constructive tone, leading to a politeness score of 90.",70,90
"In this paper, the authors consider CNN models from the lens of kernel methods. They build upon past work that showed that such models can be seen to lie in appropriate RKHS, and derive upper and lower bounds for the kernel norm. These bounds can be used as regularizers that help train more robust neural networks, especially in the context of euclidean perturbations of the inputs, and training GANs. They show that the bounds can also be used to recover existing special cases such as spectral norm penalizations and gradient regularization. They derive generalization bounds from the point of view of adversarial learning, and report experiments to buttress their claims.

Overall, the paper is a little confusing. A lot of the times, the result seem to be a derivative of the work by Bietti and Mairal, and looks like the main results in this paper are intertwined with stuff B+M already showed in their paper. It's hard to ascertain what exactly the contributions are, and how they might not be a straightforward consequence of prior work (for example, combining results from Bietti and Mairal; and generalization bounds for linear models). It might be nice to carefully delineate the authors' work from the former, and present their contributions. 

Page 4: Other Connections with Lower bounds: The first line "" ""we may also consider ... "". This line is vague. How will you ensure the amount of deformation is such that the set \bar{U} is contained in U ?

Page 4 last paragraph: ""One advantage ... complex architectures in practice"" : True, but the tightness of the bounds *do* depend on ""f"" (specifically the RKHS norm). It needs to be ascertained when equality holds in the bounds you propose, so that we know how tight they are. What if the bounds are too loose to be practical?

eqn (8): use something else to denote the function 'U'. You used 'U' before to denote the set. 

eqn (12): does \tilde{O} hide polylog factors? please clarify. 



","The sentiment of the review is slightly negative, as the reviewer finds the paper confusing and questions the originality of the contributions. The reviewer suggests that the results may be derivative of previous work and requests clarification on the authors' unique contributions. The politeness of the language is relatively high, as the reviewer uses polite phrases such as 'It might be nice to' and 'please clarify,' and provides constructive feedback without being rude or dismissive.",-30,70
"The paper proposes a learnable bloom filter architecture. While the details of the architecture seemed a bit too complicated for me to grasp (see more on this later), via experiments the authors show that the learned bloom filters are more compact that regular bloom filters and can outperform other neural architectures when it comes to retrieving seen items.

A bloom filter is fairly simple, K hash functions hash seen items into K bit vectors. During retrieval, if all of the bits hashed to are 1 then we say we've seen the query. I think there's simpler ways to derive a continuous, differentiable version of this which begs the question why the authors chose a relatively more elaborate architecture involving ZCA transform and first/second moments. Perhaps the authors need to motivate their architecture a bit better.

In their experiments, a simple LSTM seems to perform remarkably well (it is close to the best in 2 (a), (b); and crashes in (c) but the proposed technique is also outperformed by vanilla bloom filters in (c)). This is not surprising to me since LSTMs are remarkably good at remembering patterns. Perhaps the authors would like to comment on why they did not develop the LSTM further to remedy it of its shortcomings. Some of the positive results attained using neural bloom filters is a bit tempered by the fact that the experiments were using a back up bloom filter. Also, the neural bloom filters do well only when there is some sort of querying pattern. All of these details would seem to reduce the applicability of the proposed approach.

The authors have addressed most (if not all) of my comments in their revised version. I applaud the authors for being particularly responsive. Their explanations and additional experiments go a long way towards lending the insights that were missing from the original draft of the paper. I have upped my rating to a 7.","The sentiment of the review is generally positive, as indicated by the final statement where the reviewer mentions they have increased their rating to a 7 and applauds the authors for being responsive. However, there are some critical points and suggestions for improvement, which slightly temper the overall positivity. Therefore, the sentiment score is 60. The politeness of the language is quite high, as the reviewer uses polite phrases such as 'I applaud the authors' and 'Perhaps the authors need to,' indicating a respectful and constructive tone. Thus, the politeness score is 80.",60,80
"Quality: The overall quality of this paper is good. It adopts a simple but novel idea on SISR and shows clear improvement against existing method (e.g., SRGAN). 

Clarify: This paper is well written and easy to follow. It shows a clear motivation for adopting the implicit probabilistic model.

Originality: To the best of my knowledge, this paper is the first work to learn multi-modal probabilistic model for SISR.

Significance: While the results can be further improved (still look a bit blurred), this paper shows an interesting and important direction to learn better mappings for SISR.

Pros:
+ The writing is clear.
+ The proposed method is well motivated and easy to understand.
+ The experimental results include both objective and subjective evaluations.

Cons:
- The two-stage architecture is similar to the following generative models and SR methods. It’s suggested to discuss them as well.
[a] Denton, E. L., Chintala, S., & Fergus, R. “Deep generative image models using a￼ laplacian pyramid of adversarial networks”. NIPS, 2015.
[b] Karras, T., Aila, T., Laine, S., & Lehtinen, J. “Progressive growing of gans for improved quality, stability, and variation”. ICLR 2018.
[c] Lai, W. S., Huang, J. B., Ahuja, N., & Yang, M. H. “Deep laplacian pyramid networks for fast and accurate super-resolution.” CVPR 2017.
[d] Wang, Y., Perazzi, F., McWilliams, B., Sorkine-Hornung, A., Sorkine-Hornung, O., & Schroers, C. “A Fully Progressive Approach to Single-Image Super-Resolution.”. CVPR Workshops 2018.

- In the hierarchical sampling (section 2.4), it’s not clear how to generate the upper noise vector “conditioned on the lower noise vector”. 

- The hierarchical sampling seems to improve the efficiency of training. I wonder does it affect the results of testing?

- In the implementation details (section 2.5), I don’t understand why you need to transfer the the feature activations from GPU to CPU? I think all the computation can be done on GPU for most common toolboxes. Projecting the activations to a lower dimension with a “random Gaussian matrix” sounds harmful to the results.

- How do you generate the low-resolution images? Are you using bicubic downsampling or other approaches? This detail should be clarified.

- While the evaluation with PSNR and SSIM is a reference to show the quality, many literatures already show that PSNR and SSIM are not correlated well with human perception. It is suggested to also evaluate with some perceptual metrics, e.g., LPIPS [e].
[e] Zhang, R., Isola, P., Efros, A. A., Shechtman, E., & Wang, O. “The unreasonable effectiveness of deep features as a perceptual metric.” CVPR 2018.

- In Figure 7, how do you generate different results from the same input image for SRGAN? From my understanding, SRGAN doesn’t take any noise vector as input and cannot generate multi-modal results.

- I feel that the comparison with only SRGAN is not enough. There are some GAN-based SR methods [f][g]. It’s also suggested to compare with MSE-based state-of-the-art SR algorithms [h][i].

[f] Sajjadi, M. S., Schölkopf, B., & Hirsch, M. “Enhancenet: Single image super-resolution through automated texture synthesis.“ ICCV 2017.
[g] Wang, X., Yu, K., Dong, C., & Loy, C. C. “Recovering realistic texture in image super-resolution by deep spatial feature transform.” CVPR 2018.
[h] Lim, B., Son, S., Kim, H., Nah, S., & Lee, K. M. “Enhanced deep residual networks for single image super-resolution.” CVPR Workshops 2017.
[i] Zhang, Y., Tian, Y., Kong, Y., Zhong, B., & Fu, Y. “Residual dense network for image super-resolution.” CVPR 2018.

","The review starts with a positive sentiment, highlighting the overall quality of the paper, its clear writing, and its novel approach. The reviewer acknowledges the originality and significance of the work, although they note that the results could be further improved. The pros listed are all positive, while the cons are constructive and aimed at improving the paper. The language used is polite and professional, with suggestions and questions framed in a respectful manner.",80,90
"This  seems like a very interesting concept, creating adversarial agents for each class that essentially compete with each other.  It seems like this might be a very promising method for arguing for even more abstract classes like ""circus"" vs ""zoo"" 

I wise more had been said about why the Honest Advocate outperformed the standard Advocate on the MIMIC dataset.  

The authors state:

""Advocates can effectively compete to generate higher quality evidence, though this effect was
largely localized to a few class-pairs (e.g. shirts v.s. pullovers). ""

Does it do this on things that are essentially very similar?  

Overall, I think this is a great idea. I have been looking for some similar work and consider this work to be similar in the multi-generative aspect: ""MEGAN: Mixture of Experts of Generative Adversarial Networks for Multi-modal Image Generation"" - Park, Yoo, Bahng, Choo and Park, IJCAI 2018, but I cannot find similar work using the generative experts as collective adversaries for discrimination.

The paper is clear and well written.  Improvements for the paper would be going into more detail about why the method works.  It would have been great to have seen a data set on which the method performs poorly - that would give additional insight into its strengths and weaknesses.

","The sentiment of the review is generally positive, as indicated by phrases like 'very interesting concept,' 'promising method,' and 'great idea.' The reviewer also mentions that the paper is 'clear and well written.' However, there are some critical points, such as the need for more detail on why the method works and a suggestion to include a dataset where the method performs poorly. These criticisms are constructive and aimed at improving the paper. The politeness of the language is high, as the reviewer uses polite phrases and constructive feedback without any negative or rude language.",80,90
"Summary:
This paper proposes a novel optimization strategy regarding softmax cross-entropy loss, to extract the effective features of well generalization in the framework of metric learning.
The authors focus on the ""temperature"" parameter in the softmax and through analyzing the role of the temperature in terms of gradient, propose the approach of heating-up softmax in which the temperature is varied from low to high in training.
And, the effects of normalization such as by l2 and BatchNorm are discussed in the framework of heated-up softmax.
The experimental results on metric learning tasks demonstrate the effectiveness of the proposed method in comparison with the other methods.

Comments:
Pros:
+ The idea of heating up the temperature in softmax is interesting, and seems novel in the literature of metric learning.
+ The performance improvement, especially produced by batchNorm-based normalization, is shown.

Cons:
- The formulation of tempered softmax with normalization is already presented in [Wang et al., 2017].
- The reason why the heating-up approach contributes to better metric learning is not clearly provided in a well convincing way.
- It lacks an important ablation study to fairly validate the method.
- The discussion/comparison is limited to the simple softmax function.

Although the reviewer likes the idea of heating up softmax, this paper can be judged as a borderline slightly leaning toward reject, due to the above weak points, the details of which are explained as follows.

- Formulation
The softmax equipped with temperature for the normalized features and weights are shown in [Wang et al., 2017]. The only difference from that work is the way to deal with temperature; in [Wang et al., 2017], the temperature is ""optimized"" as a trainable parameter, while it is dealt with in a hand-crafted way of heating up in this work. Honestly speaking, it is unclear which approach is better, though the optimization in [Wang et al., 2017] seems elegant as stated in that paper. The only way to validate this work compared to [Wang et al., 2017] is to empirically evaluate those two methods in the experiments. Such a comparison experiment is not found and it is a main flaw of this paper.

- Justification of the method
The gradients of the softmax cross-entropy loss parameterized with a temperature T are well analyzed in Sections 3.1&3.2. But, in Section 3.3, the reviewer cannot find the clear and convincing explanation for why the temperature T should be increased in the training. My question is: why don't you use alpha=4 consistently throughout the training?
 It might be related to the process of simulated annealing (though ""temperature"" is usually cooled down in SA), and more interestingly, it would also be possible to find connection with the work of [Guo et al., 2017]. In [Guo et al., 2017], the temperature in the softmax is optimized as a post processing for calibrating the classifier outputs. Though the calibration task itself is a little bit apart from the metric learning of the authors' interest, we can find in that paper an interesting result that the temperature is heated up to increase the confidence of the classifier outputs, which is quite similar to the process of fine-tuning by heating up softmax as done in this work. Therefore, the reviewer guesses that the effectiveness of heating up softmax can also be interpreted from the viewpoint of [Guo et al., 2017].

There is also less description about Figure 1; in particular, the reviewer cannot understand what Figure 1(d) means.

- Ablation study
To empirically resolve the above concerns, it is necessary to present the empirical comparison with the ""static"" softmax.
Namely, the methods of HLN/HBN should be carefully compared to LN/BN of ""alpha=4"", not only those of alpha=16 shown in Table 1&2; the comparison in Table 3 seems unfair since the authors apply the static softmax without normalization.
And, it would be better to show the performance of heated-up softmax ""without"" normalization to show the important role of the normalization, as done in [Wang et al., 2017].
In summary, since the proposed method is composed of a heating-up approach and feature normalization, the authors are required to validate the method from those two aspects, respectively, for increasing the significance of this paper.

- Other loss function
For achieving a compactness in feature representation, the simple softmax requires both temperature and normalization. It, however, is also conceivable to employ the other types of loss function for that purpose, such as [a] which is based on the (Mahalanobis) distance with taking into account the margin between categories. The distance based loss also embeds features into localized clusters, which satisfies the authors' objective in this work. To validate the proposed method, it is required to compare the method with such a different types of loss function.

[a] Wan, W., Zhong, Y., Li, T., & Chen, J. (2018). Rethinking Feature Distribution for Loss Functions in Image Classification, In CVPR2018, pp. 9117–9126.","The sentiment of the review is slightly negative, as indicated by the statement that the paper is 'borderline slightly leaning toward reject' due to several weak points. The reviewer acknowledges some positive aspects, such as the novelty of the idea and performance improvement, but these are outweighed by significant criticisms regarding the formulation, justification, lack of ablation study, and limited discussion/comparison. Therefore, the sentiment score is -20. The politeness of the language is quite high; the reviewer uses phrases like 'honestly speaking,' 'the reviewer likes the idea,' and 'it would be better to show,' which indicate a respectful and constructive tone. Thus, the politeness score is 80.",-20,80
"
Summary:
This paper proposes MetaMimic, an algorithm that does the following:
(i) Learn to imitate with high-fidelity with one-shot. The setting is that we have access to several demonstrations (only states, no actions) of the same task. During training, we have pixel observations plus proprioceptive measurements). At test time, the learned policy can imitate a single new demonstration (consisting of only pixel observations) of the same task.
(ii) When given access to rewards, the policy can exceed the human demonstrator by augmenting its experience replay buffer with the experience gained while learning (i). Therefore, even in a setting with sparse rewards and no access to expert actions (only states), the policy can learn to solve the task.

Overall Evaluation:
This is a good paper. In my opinion however, it does not pass the bar for ICLR.

Pros:
- The paper is well written. The contributions are clearly listed, the methods section is easy to follow and the authors explain the choices they make. The illustrations are clear and intuitive.
- The overview of hyperparameter choice and tuning / importance factor in the Appendix is useful.
- Interesting pipeline of learning policies that can use demonstrations without actions.
- The results on the simulated robot arm (block stacking task with two blocks) are good.

Cons:
- The abstracts oversells the contribution a bit when saying that MetaMimic can learn ""policies for high-fidelity one-shot imitation of diverse novel skills"". The setting that's considered in the paper is that of a single task, but different demonstrations (different humans from different starting points). This seems restrictive, and could have been motivated better.
- Experimental results are shown only for one task; block stacking with a robot arm in simulation.
- Might not be a good topical fit for ICLR, but more suited for a conference like CoRL or a workshop. The paper is very specific to imitation learning for a manipulation / control tasks, where we can (1) reset the environment to the exact starting position of the demonstrations, (2) the eucledian distance between states in the demonstration and visited by the policy is meaningful (3) we have access to both pixel observations and proprioceptive measurements. The proposed method is an elegant way to solve this, but it's unclear how well it would perform on different types of control problems, or when we want to transfer policies between different (but related) tasks.

Questions:
- Where does the ""task stochasticity"" come from? Only from the starting state, and from having different demonstrations? Or is the transition function also stochastic?
- The learned policy is able to do one-shot imitation, i.e., given a new demonstration (of the same task) the policy can follow this demonstration. Do I understand correct that this mean that there is *no* additional learning required at test time?
- It is not immediately clear to me why the setting of a single task but new demonstrations is interesting. Could the authors comment on this? One setting I could imagine is that the policy is trained in simulation, but then executed in the real-world, given a new demonstration. (If that's the main motivation though, then the experiments might have to support that this is possible - if no real-world robot is available, maybe the same simulator with a slightly different camera angle / light conditons or so.)
- The x-axis in the figures says ""time (hours)"" - is that computation time or simulated time?

Other Comments:
- In 3.2, I would be interested in seeing the following baseline comparison: Learn the test task from scratch using the one available demonstration, with the RL procedure (Equation 2, but possibly without the second term to make it fair). In Figure 5, we can see that the performance on the training tasks is much better when training on only 10 tasks, compared to 500. Then why not overfit to a single task, if that's what we're interested in? 
- An interesting baseline for 3.3 might be an RL algorithm with shaped rewards: using an additional reward term that is the eucledian distance to the *closest* datapoint from the demonstration. Compared to the baselines shown in the results section, this would be a fairer comparison because (1) unlike D4PG we also have access to information from the demonstrations and (2) no additional information is needed like the action information in D4PGfD and (3) we don't have the need for a curriculum.

Nitpick (no influence on score):
[1. Introduction]
- I find the first sentence, ""One-shot imitation is a powerful way to show agents how to solve a task"" a bit confusing. I'd say one-shot imitation is a method, not a way to show how to solve a task. Maybe an introductory sentence like ""Expert demonstrations are a powerful way to show agents how to solve a task."" works better?
- Second sentence, the chosen example is ""manufacturing"" tasks - do you mean manipulation? When reading this, I had to think of car manufacturing - a task I could certainly not imitate with just a few demonstrations.
- Add note that with ""unconditional policy"" you mean not conditioned on a demonstration.
[2. MetaMimic]
- [2.1] Third paragraph: write ""Figure 2, Algorithm 1"" or split the algorithm and figure up so you can refer to them separately.
- [2.1] Last paragraph, second line: remove second ""to""","The sentiment of the review is generally positive, as indicated by phrases like 'This is a good paper' and the detailed list of pros. However, the reviewer also mentions that the paper does not meet the bar for ICLR, which tempers the overall positivity. Therefore, the sentiment score is set at 50. The politeness of the language is high, with the reviewer using polite and constructive language throughout the review, such as 'I would be interested in seeing' and 'Could the authors comment on this?' Therefore, the politeness score is set at 90.",50,90
"Overall this paper contributes many interesting insights into the specific application of empathetic dialog into chatbot responses.  The paper in particular is contributing its collected set of 25k empathetic dialogs, short semi-staged conversations around a particular seeded emotion and the results of various ways of incorporating this training set into a generative chatbot.

While the results clearly do not solve the problem of automating emapthy, the paper does give insights into which methods perform better than others (Generation vs Retrieval) and explicitly adding emotion predictions vs using an ensemble of encoders.

There is a lot in this paper, and I think it could have been better organized.
I am more familiar with emotion related research and not language to language translation, so I would have appreciated a better explanation of the rationale for using BLEU scores.  I did some online research to understand these Bilingual Evaluation Understudy Scores and while it seems like they measure sentence similarity, it is unclear how they capture ”relevance” at least according to the brief tutorial that I read (https://machinelearningmastery.com/calculate-bleu-score-for-text-python/).  I did not see the paper describing the use of this score in the references but perhaps I missed it – could you please clarify why this is a good metric for relevance?  It seems that these scores are very sensitive to sentence variation.  I am not sure if you can measure empathy or appropriateness of a response using this metric.
For your data collection you have 810 participants and 24,850 conversations.  Are the 810 participants all speakers or speakers and listeners combined?  How many conversations did each speaker/listener pair perform 32?  (one for each emotion) or 64? (two for each emotion) Was the number variable?  If so what is the distribution of the contribution – e.g. did one worker generate 10,000 while several hundred workers did only three of four?  Was it about even?  Just for clarity – how did you enroll participants?  Was it through AMT?  What were the criteria for the workers?  E.g. Native English speaker, etc.

In your supplemental material, I found the interchanging of the words “context” and “emotion” confusing.  The word context is used frequently throughout your manuscript: “dialog context,” “situational context” - emotions are different from situations, the situational utterance is the first utterance describing the emotion if I read your manuscript correctly.  Table 6 should use “Label” or “Emotion” instead of the more ambiguous “Context.”  

My understanding is that speakers were asked to write about a time when they experienced a particular feeling and they were given a choice of three feelings that they could write about.  You then say that workers are forced to select from contexts they had not chosen before to ensure that all of the categories were used.  From this I am assuming that each speaker/listener worker pair had to write about all 32 emotions – is this correct?  Another interpretation of this is that you asked new workers to describe situations involving feelings that had not been chosen by other workers as data collection progressed to ensure that you had a balanced data set.  This would imply that some emotional situations were less preferred and potentially more difficult to write about.  It would be interesting if this data was presented.  It might imply that some emotion labels are not as strong if people were forced to write about them rather than being able to choose to write about them.  
Were these dialogs ever actually annotated?  You state in section 2, Related Work “we train models for emotion detection on conversation data that has been explicitly labeled by annotators” – please describe how this was done.  Did independent third party annotators review the dialogs for label correctness?  Was a single rater or a majority vote used to decide the final label.  For example, in Table 1, the label “Afraid” is given to a conversation that could also have reasonable been generated by the label “Anxious” a word explicitly used in the dialog.  I am guessing that the dialogs are just labeled according to the label / provocation word and that they were not annotated beyond that, but please make this clear.  
In the last paragraph you state “A few works focus..” and then list 5.  This should rather be “several other works have focused on “ …  
Conversely, you later state in section 3 “Speaker and Listener”, “We include a few example conversations from the training data in Table 1,” this should more explicitly be “two.”
Also in section 3 when you describe your cross validation process, you state “We split the conversations into approximately 80/10/10 partitions.  To prevent overlap of <<discussed topics>> we split the data so that all the sets of conversations with the same speaker providing the prompt would be in the same partition.  
In your supplemental material you state that workers were paired.  Each worker is asked to write a prompt, which also seems to be the first utterance in the dialog they will start.  You state each worker selects one emotion word from a list of three which is somehow generated (randomly?) form your list of 32 .  I am assuming each worker in the pair does this, then the pair has a two “conversations” one where the first worker is the speaker and another where the second worker is the speaker – is this correct?  It is not entirely clear from the description. Given that you have 810 workers and 24,850 conversations, I am assuming that each worker had more than one conversation.  My question is  - did they generate a new prompt / first utterance for each conversations.  I am assuming yes since you say there are 24,850 prompts/conversations.  For each user are all of the situation/prompts they generate  describing the same emotion context?  E.g. would one worker write ~30 conversations on the same emotion.  This seems unlikely, and it seems more likely that given the number of conversations ~30 per participant is similar to the number of emotion words that you asked each worker to cycle through nearly all of the emotions or that given they were able to select, they might describe the same emotion, e.g. “fear” several times.  If the same worker was allowed to select the same emotion context multiple times was it found that they re-used the same prompt several times?  I am assuming that this is the case and that this is what you mean when you say that you “prevent overlap of discussed topics” between sets when you exclude particular workers.  Is this correct?  Or did you actually look and code the discussed topics to ensure no overlap even across workers (e.g. several people might have expressed fear of heights or fear of the dark).

In section 4, Empathetic dialog generator, you state that the dialog model has access to the situation description given by the speaker (also later called the situational prompt) but not the emotion word prompt.  Calling these both prompts makes the statement about 24,850 prompts/conversations a bit ambiguous.  A better statement would be 24,850 conversations based on unique situational prompts/descriptions (if they are in fact unique situational prompts.  I am assuming they are not if you are worried about overlapping “discussed topics” which I am assuming are the situational prompts since the dialogs are very short and heavily keyed off these initial situational prompts)

In your evaluation of the models with Human ratings you describe two sets of tests.  In one test you say you collect 100 annotations per model.  More explicitly, did you select 100 situational prompts and then ask workers to rate the response of each model?  Was how many responses was each worker shown?  How many workers were used?  Are the highlighted numbers the only significant findings or just the max scores?  Annotations is probably not the correct word here.

Please also describe your process for assigning workers to the second human ratings task.     

Since the two novel aspects of your paper are the new dataset and the use of this dataset to create more empathetic chatbot responses (""I know the feeling"") I have focused on these aspects of the paper in my review.

I found the inclusion of Table 7 underexplained in the text.  The emotion labels for all these datasets are not directly comparable so I would have liked to have seen more explanation around how these classifications were compared.  It would also be helpful to know how more similar emotions such as ""afraid"" and ""anxious"" were scored vs ""happy"" and ""sad"" confusions 
","The sentiment of the review is generally positive, as the reviewer acknowledges the paper's contributions and interesting insights into empathetic dialog in chatbots. However, the reviewer also points out several areas for improvement and requests clarifications, which slightly tempers the overall positivity. Therefore, the sentiment score is 50. The politeness of the language used is high; the reviewer uses polite phrases such as 'I would have appreciated,' 'could you please clarify,' and 'please describe,' indicating a respectful and constructive tone. Thus, the politeness score is 80.",50,80
"This is a paper about sentence embedding based on orthogonal decomposition of the spanned space by word embeddings. Via Gram-Schmidt process, the sequence of words in a sentence is regarded as a sequence of incoming vectors to be orthogonalized. Each word is then assigned 3 scores: novelty score, significance score, and uniqueness score. Eventually, the sentence embedding is achieved as weighted average of word embeddings based on those scores. The authors conduct extensive experiments to demonstrate the performance of the proposed embedding. I think the idea of the paper is novel and inspiring. But there are several issues and possible areas to improve:

1. What if the length of the sentence is larger than the dimension of the word embedding? Some of the 3 scores will not be well-defined.

2. Gram-Schmidt process is sensitive to the order of the incoming vectors. A well-defined sentence embedding algorithm should not. I suggest the authors to evaluate whether this is an issue. For example, if by simply removing a non-important stop word at the begging of the sentence and then the sentence embedding changes drastically, then it indicates that the embedding is problematic.

3. I’m confused by the classification between training-free sentence embedding and unsupervised sentence embedding? Don’t both of them require training word2vec-type embedding?

4. The definition of the three scores seems reasonable, but requires further evidence to justify. For example, by the definition of the scores, do we have any proof that the value of \alpha indeed demonstrated the related importance level?","The sentiment of the review is generally positive, as indicated by phrases like 'the idea of the paper is novel and inspiring' and the acknowledgment of extensive experiments conducted by the authors. However, the reviewer also points out several issues and areas for improvement, which slightly tempers the overall positivity. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer uses polite suggestions such as 'I suggest' and 'requires further evidence to justify,' and avoids any harsh or rude language. Thus, the politeness score is 90.",60,90
"The main idea of this paper is that a 'realistic' way to attack GCNs is by adding fake nodes. The authors go on to show that this is not just a realistic way of doing it but it can done in a straightforward way (both attacks to minimize classification accuracy and GAN-like attacks to make fake nodes look just like real ones). 

The idea is neat and the experiments suggests that it works, but what comes later in the paper is mostly rather straightforward so I doubt whether it is sufficient for ICLR. I write ""mostly"" because one crucial part is not straightforward but is on the contrary, incomprehensible to me.  In Eq (3) (and all later equations) , shouldn't X' rather than X be inside the formula on the right? Otherwise it seems that the right hand side doesn't even depend on X' (or X_{fake} ). 
But if I plug in X', then the dimensions for weight matrices  W^0 and W^1 (which actually are never properly introduced in the paper!) don't match any more. So what happens? To calculate J you really need some extra components in W0 and W1. Admittedly I am not an expert here, but I figure that with a bit more explanation I should have been able to understand this. Now it remains quite unclear...and I can't accept the paper like this.

Relatedly, it is then also unclear what exactly happens in the experiments: do you *retrain* the network/weights or do you re-use the weights you already had learned for the 'clean' graph? 

All in all: 
PRO:
- basic idea is neat 
CON:
- development is partially straightforward, partially incomprehensible.

(I might increase my score if you can explain how eq (3) and later really work, but the point that things remain rather straightforward remains). ","The sentiment of the review is mixed, with both positive and negative elements. The reviewer appreciates the basic idea and acknowledges that the experiments suggest it works, but expresses significant doubts about the sufficiency and clarity of the paper. The sentiment score is therefore slightly negative at -20. The politeness of the language is generally respectful and constructive, with the reviewer offering specific feedback and indicating a willingness to reconsider their score if clarifications are provided. The politeness score is 60.",-20,60
"The authors propose a method for image restoration, where the restored image is the MAP estimate. A pretrained GAN is utilized to approximate the prior distribution of the noise-free images. Then, the likelihood induces a constraint which is based on the degradation function. In particular, the method tries to find the latent point for which the GAN generates the image, which if gets degraded will match the given degraded image. Also, an optimization algorithm is presented that solves the proposed constrained optimization problem.

I find the paper very well written and easy to follow. Also, the idea is pretty clean, and the derivations are simple and clear. Additionally, the Figures 2,3 are very intuitive and nicely explain the theory. However, I think that there are some weaknesses (see comments):

Comments: 

#1) I do not understand exactly what the ""general method"" means. Does it mean that you propose a method, where you can just change the F, such that to solve a different degradation problem? So you provide the general framework where somebody has to specify only the F?

#2) Clearly, the efficiency of the method is highly based on the ability of the GAN to approximate well the prior distribution of the noise-free images.

#3) There are several Equations that can be combined, such that to save enough white space in order to discuss further some actual technical details. For instance, Eq. 2,3 can be easily combined using the proportional symbol, Eq. 8,9,10,11 show actually the same thing.

#4) I think that the function F has to be differentiable, and this should be mentioned in the text. Also, I believe that some actual (analytic) examples of F should be provided, at least  in the experiments. The same holds for the p(Omega). This parameter Omega is estimated individually for each degraded image?

#5) Before Eq. 8 the matrix V is a function of z and should be presented as such in the equations.

#6) I believe that it would be nice to include a magnified image of Fig. 3, where the gradient steps are shown. Also, my understanding is that the optimization goal is to find first a feasible solution, and then find the point that maximizes f. I think that this can be clarified in the text.

#7) The optimization steps seem to be intuitive, however, there is not any actual proof of converge. Of course, the example in the Figure 3 is very nice and intuitive, but it is also rather simple. I would suggest, at least, to include some empirical evidences in the experiments that show convergence.

#8) In the experiments I think that at least one example of F and p(Omega) should be presented. Also, what the numbers in Table 4 show? Which is the best value that can be achieved? These numbers correspond to several images, or to a unique image? 
#9) I think that MNIST is almost a toy experiment, since the crucial component of the proposed method is the prior modeling with the GAN. I believe that a more challenging experiment should be conducted e.g. using celebA dataset.

Minor comments:

#1) In the paragraph after Eq. 4 the equality p_r(x)=p_G(x) is very strong assumption. I would suggest to use the \simeq symbol instead.

#2) After Eq. 6 the ""nonnegative"" should be ""nonzero"".

#3) Additional density estimation models can be used e.g. VAEs, GMM. Especially, I believe that the VAE will provide a way to approximate the prior easier than the GAN.

#4) In Section 2 paragraph 2, the sentence ""However, they only ... and directly"" is not clear what means.

In general, I find both the proposed model and optimization algorithm interesting. Additionally, the idea is nicely presented in the paper. Most of my comments are improvements which can be easily included. The two things that make me more skeptical, is the convergence of the proposed algorithm and the experiments. The MNIST is a relatively simple experiment, and I would like to see how the method works in more challenging problems. Also, I think that additional methods to compute the image prior should be included in the experiments.","The review starts with a positive sentiment, praising the paper for being well-written, easy to follow, and having clear derivations and intuitive figures. The reviewer then provides a detailed list of constructive comments and suggestions for improvement, which are framed in a polite and professional manner. The language used is considerate and aims to help the authors improve their work. The reviewer also acknowledges the strengths of the paper while pointing out areas for enhancement, which indicates a balanced and fair assessment.",80,90
"The paper investigates the use of multi-objective optimization techniques in GAN-setups where there are multiple discriminators. Using multiple discriminators was proposed in Durugkar et al, Arora et al, Neyshabur et al and others. The twist here is to focus on the Pareto front and to import multiple gradient descent and hypervolume-maximization based methods into GANs. 

The results are decent. The authors find that optimizing with respect to multiple discriminators increases diversity of samples for a computational cost. However, just scaling up (and carefully optimizing), can yield extremely impressive samples, https://arxiv.org/abs/1809.11096. It is unclear how the tradeoffs in optimizing against multiple discriminators stack-up against bigger GANs. 

From my perspective, the paper is interesting because it introduces new methods into GANs from another community. However, the results themselves are not sufficient for publication. 
","The sentiment of the review can be considered slightly positive, as the reviewer acknowledges the paper's interesting approach and the introduction of new methods into GANs from another community. However, the sentiment is tempered by the critique that the results are not sufficient for publication. Therefore, the sentiment score is 20. The politeness of the language is quite high, as the reviewer provides constructive feedback without being harsh or dismissive. The reviewer uses phrases like 'from my perspective' and 'the results are decent,' which indicate a polite tone. Therefore, the politeness score is 80.",20,80
"In this paper the authors propose an extension to successor features (SF). Akin to UVFAs, they condition on some goal state by concatenating to the current state after some shared preprocessing. The authors claim three contributions: 1) introducing the USF, 2) proposing an appropriate deep learning architecture for it, and 3) showing experimentally that USFs improve transfer both within a goal set and to novel goals.

Claims 1) and 2) don't seem particularly noteworthy. Extending SF to be goal-conditioned is very straightforward, doesn't leverage anything unique to the SF formalism (e.g. the reward weights w already encode a goal in some sense), and doesn't attempt to extend its theoretical grounding. The architecture is likewise unsurprising, and the lack of ablations or alternatives make it seem rather unmotivated.

The usage of a Q-learning loss instead of a reward-prediction loss for updating phi is mentioned without citation. This seems quite novel, and could be a significant contribution if its advantage was demonstrated experimentally.

The experiments appear to show a significant advantage for USFs. For the training-goal-set advantage, it would be useful to know the architecture of multi-goal DQN. One hypothesis is that the extra weight-sharing is what is giving USFs an edge, and this should be ruled out. It is briefly mentioned that UVFAs weren't considered due to their stated instability, but its unclear how they differ from the multi-goal DQN.

The novel-goal results are impressive at first glance, but there is a glaring omission. Hindsight experience replay (HER) is mentioned but not evaluated, and would very likely trivialise the train/test goal-set distinction (unless the test goals were never previously visited). As these results are the primary contribution of this paper, this must be addressed prior to publication acceptance.

Edit: The addition of HER experiments push this up a bit (5-->6). I'm still concerned about how significant the contribution is (as it is a straightforward extension to SFs), but the empirical results are now quite strong.","The sentiment of the review is mixed, with both positive and negative aspects. The reviewer acknowledges the empirical strength of the results but questions the significance and novelty of the contributions. The sentiment score is therefore slightly positive. The language used is generally polite, with constructive criticism and suggestions for improvement, indicating a high politeness score.",20,80
"UPDATE:
Thanks for your response. As you mentioned, methods like [1] and [2] do perform open-ended recombination. Note that these methods perform not only texture transfer but also color transfer, while the proposed method seems to perform mostly only color transfer. As shown in Figure 6, essentially what the method does is transfer the color of the style image to the content image, sometimes with a little tweak, making the image distorted. One could say that in terms of image style transfer, the proposed method actually underperforms [1] and [2]. 

Hence I agree with R2 that comparison is still necessary for the submission to be more convincing and complete.

------------------------------

This paper proposed to use a mechanism of leakage filtering to separate styles and content in the VAE encoding, and consequently enable open-ended content-style recombination. Essentially the model tries to maximize the similarity between images in S^+ and minimize the similarity between those in S^-.

I have several questions:

One concern that I have is the relationship/difference between this work and previous work on style transfer, especially universal/zero-shot style transfer as in [1,2]. In the introduction and related work sections, the authors argue that most previous work assumes that content classes in testing are the same as those in training, and that they are not general purpose. Note that various works on style transfer already address this issue, for example in [1, 2]. For those models, content is represented by high-level feature maps in neural networks, and style is represented by the Gram matrix of the feature maps. The trained model is actually universal (invariant to content and styles). Actually these methods use even less supervision than STOC since they do not require labels (e.g., digit labels in MNIST).

This brings me to my second concern on proper baselines. Given the fact that previous universal/zero-shot style transfer models focus on similar tasks, it seems necessary to compare STOC to them and see what the advantages of STOC is. Similar experiments can be conducted for the data augmentation tasks.

In Sec. 4, the authors mentioned that U-Net skip connection is used. Does it affect the effectiveness of the content/style separation, since the LF objective function is mostly based on the encoding z, which is supposed ‘skipped’ in STOC. Will this lead to additional information leakage?

It is not clear how the last term of L_{LF} is computed. Could you provide more details?

The organization and layout of figures could be improved. The title/number for the first section is missing.

Missing references:

[1] Universal style transfer via feature transforms, 2017
[2] ZM-Net: Real-time zero-shot image manipulation network, 2017
[3] Structured GAN, 2017","The sentiment of the review is moderately negative. The reviewer points out several shortcomings of the paper, such as the method primarily performing color transfer rather than texture transfer, the need for comparison with previous methods, and concerns about the effectiveness of the content/style separation. However, the reviewer acknowledges the proposed mechanism and provides constructive feedback. The politeness of the language is high. The reviewer uses polite phrases like 'I have several questions,' 'One concern that I have,' and 'Could you provide more details?' which indicate a respectful and constructive tone.",-40,80
"This paper discusses conditions under which  the convergence of training models with low-precision weights do not rely on model dimension. Extensions to two kinds of non-linear quantization methods are also provided. The dimension-free bound of the this paper is achieved through a tighter bound on the variance of the quantized gradients.  Experiments are performed on synthetic sparse data and small-scale image classification dataset MNIST.

The paper is generally well-written and structure clearly. However, the bound for linear quantization is not fundamentally superior than previous bounds as the ""dimension-free"" bound in this paper is achieved by replacing the bound in other papers using l2 norm with l1 norm. Note that l1 norm is related to the l2 norm as: \|v\|_1 <= \sqrt{d}\|v\|_2, the bound can still be dependent on  dimension, thus the title may be misleading. Moreover, the assumptions  1 and 2 are much stronger than previous works, making the universality of the theory limited. The analysis on non-linear quantization is interesting, which can really theoretically improve the bound. It would be nice to see some more empirical results on substantial networks and  larger datasets which can better illustrate the efficacy of the proposed non-linear quantization.

Some minor issues:
1. What is HALP in the second contribution before Section 2?
2. What is LP-SVRG in Theorem 1?
3. What is \tilde{w} in Theorem 2?","The review starts with a positive sentiment, acknowledging that the paper is generally well-written and clearly structured. However, it quickly transitions to a more critical tone, pointing out significant limitations in the paper's claims and assumptions. The reviewer also provides constructive feedback and specific recommendations for improvement. The language used is polite and professional, even when pointing out the paper's shortcomings.",20,80
"In the paper, the authors try to propose an adaptive learning rate method called predictive local smoothness.  They also do some experiments to show the performance. 

The following are my concerns:

1. The definition of the L(x_t) is confusing. In (8), the authors define L(x_t), and in (10), the authors give another definition.  Does the L(x_t) in (10) always guarantee that (8) is satisfied? 

2. In theorem 1, \mu^2 = \frac{1}{n} \sum_{i=1}^n L_i^2(x_t) + \frac{2}{n^2}  \sum_{i<j}^n L_i(x_t) L_j(x_t) > v. It looks like that \mu > (1-\rho^2) v, no matter the selection of \rho.  Why?

3. How do you compute L_i(x_t)  if x is a multi-layer neural network?

4. The experimental results are too weak. In 2018, you should at least test your algorithm using a deep neural network, e.g. resnet. The results on a two-layer neural network mean nothing. 

5. sometimes, you algorithm even diverge. for example, figure 3 second column third row.  


","The sentiment of the review is moderately negative. The reviewer points out several concerns and weaknesses in the paper, such as confusing definitions, weak experimental results, and instances where the algorithm diverges. The sentiment score is -60. The politeness of the language is neutral to slightly polite. The reviewer uses formal language and does not use any derogatory terms, but the feedback is direct and critical. The politeness score is 10.",-60,10
"This paper proposes runs variational inference with discrete mean-field distributions. The paper claims the proposed method is able to give a better estimation of uncertainty from the model. 

Rating of the paper in different aspects ( out of 10)
Quality 6, clarify 5, originality 8, significance of this work 5 

Pros: 

1. The paper proposes a generic discrete distribution as the variational distribution to run inference for a wide range of models. 

Cons:

1. When the method begins to use mean-field distributions, it begins to lose fidelity in approximating the posterior distributions. Even the model is able to do a good job in approximating marginal distributions, it is hard to evaluate whether the model is gaining benefit overall. 

2. I don't see a strong reason for using discrete distributions. In one dimensional space, a distribution can be approximated in different ways. Using discrete distributions only increases the difficulty of reparameterization. 

3. In the experiment evaluation, the algorithm seems only marginally outperforms competing methods. 


Detailed comments: 

In the motivation of the paper, it cites low-precision neural networks. However, low-precision networks are for a different purpose -- small model size and saving energy. 

equation 6 is not clear to me.

In equation 10, how are these conditional probabilities parameterized? Is it like: z ~ Bernoulli( sigmoid(wz) ) ?

It is nice to have a brief introduction of the evaluation measure SGR. 

In table 3, 1st column, the third value seems to be the largest, but the fourth is bolded. 
","The sentiment of the review appears to be slightly positive but mixed, as indicated by the pros and cons listed. The reviewer acknowledges the originality and potential significance of the work but also points out several critical issues. Therefore, the sentiment score is 10. The language used in the review is generally polite and constructive, with no rude or harsh language. The reviewer provides specific feedback and suggestions for improvement, which indicates a politeness score of 80.",10,80
"# Paper summary
This paper advances a method for accelerating semantic segmentation on video content at higher resolutions. Semantic segmentation is typically performed over single images, while there is un-used redundancy between neighbouring frames. The authors propose exploiting this redundancy and leverage block motion vectors from MPEG H.264 video codec which encodes residual content between keyframes. The block motion vectors from H264 are here used to propagate feature maps from keyframes to neighbouring non-keyframe frames (in both temporal directions) avoiding thus an additional full forward pass through the network and integrate this in the training pipeline. Experimental results on CamVid and Cityscapes show that the proposed method gets competitive results while saving computational time.


# Paper strengths
- This paper addresses a problem of interest for both academic and industrial purposes.
- The paper is clearly written and the authors argument well their contributions, adding relevant plots and qualitative results where necessary.
- The two-way interpolation with block motion vectors and the fusion of interpolated features are novel and seem effective.
- The experimental results, in particular for the two-way BMV interpolation, are encouraging.


# Paper weaknesses

- The idea of using Block Motion Vectors from compressed videos (x264, xvid) to capture motion with low-cost has been previously proposed and studied by Kantorov and Laptev [i] in the context of human action recognition. Flow vectors are obtained with bilinear interpolation from motion blocks between neighbouring frames. Vectors are then encoded in Fisher vectors and not used with CNNs as done in this paper. In both works, block motion vectors are used as low-cost alternatives to dense optical flow. I would suggest to cite this work and discuss similarities and differences.


- Regarding the evaluation of the method, some recent methods dealing with video semantic segmentation, also using ResNet101 as backbone, are missing, e.g. low latency video semantic segmentation[ii]. Pioneer Clockwork convnets are also a worthy baseline in particular in terms of computational time (results and running times on CityScapes are shown in [ii]). It would be useful to include and compare against them.

- In Section 4.1.2 page 7 the authors mention a few recent single-frame models ((Yu et al. (2017); Chen et al. (2017); Lin et al. (2017); Bilinski & Prisacariu (2018)) as SOTA methods and the current method is competitive with them. However I do not see the results from the mentioned papers in the referenced Figures. Is this intended?

- On a more general note related to this family of approaches, I feel that their evaluation is usually not fully eloquent. Authors compare against similar pipelines for static processing and show gains in terms of computation time. The backbone architecture, ResNet-101 is already costly for high-resolution inputs to begin with and avoiding a full-forward pass brings quite some gains (though a part of this gain is subsequently attenuated by the latency caused by the batch processing of the videos). There are recent works in semantic segmentation that focus on architectures with less FLOPs or memory requirements than ResNet101, e.g. Dilated ResNets [iii], LinkNet[iv]. So it could be expected that image-based pipelines to be getting similar or better performance in less time. I expect the computational gain on such architectures when using the proposed video processing method to be lower than for ResNet101, and it would make the decision of switching to video processing or staying with frame-based predictions more complex. 
The advantage of static image processing is simpler processing pipelines at test time without extra parameters to tune. It would be interesting and useful to compare with such approaches on more even grounds.


# Conclusion 
This paper takes on an interesting problem and achieves interesting results. The use of Block Motion Vectors has been proposed before in [i] and the main novelty of the paper remains only the interpolation of feature maps using BMVC. The experimental section is missing some recent related methods to benchmark against.
This work has several strong and weak points. I'm currently on the fence regarding my decision. For now I'm rating this work between Weak Reject and Borderline  

# References

[i] V. Kantorov and I. Laptev, Efficient feature extraction, aggregation and classification for action recognition, CVPR 2014
[ii] Y. Li et al., Low-Latency Video Semantic Segmentation, CVPR 2018
[iii] F. Yu et al., Dilated Residual Networks, CVPR 2017
[iv] A. Chaurasia and E. Culurciello, LinkNet: Exploiting Encoder Representations for Efficient Semantic Segmentation, arXiv 2017
","The sentiment of the review is mixed. The reviewer acknowledges the strengths of the paper, such as addressing a relevant problem, clear writing, and novel contributions. However, the reviewer also points out significant weaknesses, including the lack of citation of related work, missing comparisons with recent methods, and concerns about the evaluation. The sentiment score is therefore slightly negative. The politeness of the language is high; the reviewer uses polite and constructive language throughout, providing specific recommendations and suggestions for improvement without being harsh or dismissive.",-20,80
"This paper presents a method for generating 3D objects. They train a VAE to generate voxel occupancy grids. Then, they allow a user to generate novel shapes using the learned model by combining latent codes from existing examples. 

Pros:
- The idea of linking affordances to 3D object generation is interesting, and relevant to the machine learning and computer vision communities.

- They propose to evaluate the quality of the shape based on a physical simulation (Section 4.4.3), which is an interesting idea.

Cons:
- This paper is not well written. The method is described in too much detail, and the extra length (10 pages) is unnecessary. Cross entropy, VAEs, and many of the CNN details can usually just be cited, instead of being described to the reader.

- The paper uses suggestive terminology, like ""functional essence"" and ""functional arithmetic"" for concepts that are fairly mundane (see Lipton and Steinhardt, 2018 for an extended discussion of this issue). For example, the ""functional essence"" of a class is essentially an average of the VAE latent vectors (Section 3.3.1). The paper claims, without sufficient explanation, that this is computation is motivated by the idea that ""form follows function"".

- The results are not very impressive. There is no rigorous evaluation. They propose several nice metrics to use (eg. affordance simulation), but the results they present for each metric are quite limited. The qualitative results are also not particularly compelling.

- The paper should more thoroughly evaluate the importance weighting that is described in Section 3.3.2.

 - The technical approach (combining VAE vectors to make new shapes) is not particularly novel[

Overall:

The paper should not be accepted in its current form, both due to the confusing writing, and the lack of careful evaluation.
","The sentiment of the review is generally negative, as it highlights several significant issues with the paper, including poor writing, lack of rigorous evaluation, and limited novelty. The reviewer does acknowledge some positive aspects, such as the interesting idea of linking affordances to 3D object generation and the use of physical simulation for evaluation, but these are outweighed by the criticisms. Therefore, the sentiment score is -60. The politeness of the language is relatively neutral to slightly polite. The reviewer provides constructive feedback without using harsh or rude language, but the tone is firm and critical. Thus, the politeness score is 20.",-60,20
"Based on the CapsNet concept of Sabour the authors proposed a trace-back method to perform a semantic segmentation in parallel to classification. The method is evaluate on MNIST and the Hippocampus dataset.

The paper is well-written and well-explained. Nevertheless, I think it would be useful to have some illustrations about the network architecture. Some stuff which is explained in text could be easily visualized in a flow chart. For example, the baseline architecture and your Tr-CapsNet could be easily explained via a flow chart. With the text only, it is hard to follow. Please think about some plots in the final version or in the appendix. One question which is aligned to that: How many convolutional filters are used in the baseline model?

Additionally, think about a pseudo-code for improved understandability. 

Some minor concerns/ notes to the authors:
1.	At page 5: You mentioned that the parameters lambda1 and lambda 2 are important hyper-parameters to tune. But in the results you are not explaining how the parameters were tuned. So my question is: How do you tuned the parameters? In which range do you varied the parameters?
2.	Page 6; baseline model: Why do you removed the pooling layers?
3.	I’m curious about the number of parameters in each model. To have a valid discussion about your model is better than the U-Net-6 architecture, I would take into account the number of parameters. In case that your model is noticeably greater, it could be that your increased performance is just due to more parameters. As long as your discussion is without the number of parameters I’m not convinced that your model is better. A comparison between models should be always fair if two models are architectural similar.
4.	Why is the magnitude of lambda1 so different between the two dataset that you used?
5.	Could you add the inference times to your tables and discuss that in addition?
6.	What kind of noise is added to MNIST?
7.	What is the state-of-the-art performance on the Hippocampus dataset?
8.	What would be the performance in your experiments with a MaskRCNN segmentation network?
9.	I’m not familiar with the Hippocampus dataset. I missed a reference where the data is available or some explaining illustrations. 
10.	For both datasets, more illustrations about the segmentation performance would be fine to evaluate your method. At least in the appendix…
	
My major concern is that both datasets are not dealing with real background noise. I’m concerned that the results are not transferable to other datasets and that the method shines promising just because of the simple datasets only. For example, due to the black background MNIST digits are well separated (if we skip that you added some kind of noise). So, from that point of view your results are not convincing and the discussion of your results appearing sparse and not complete.
To make your results transparent you could think about to publish the code somewhere.
","The review starts with a positive sentiment, acknowledging that the paper is well-written and well-explained. However, it quickly transitions into constructive criticism, suggesting improvements such as adding illustrations and pseudo-code for better understanding. The reviewer also raises several specific questions and concerns, particularly about the methodology and the datasets used. The language used is polite and constructive, aiming to help the authors improve their work rather than dismissing it outright. The reviewer provides detailed feedback and suggestions, which indicates a positive and helpful attitude overall.",50,80
"Partially observable Markov decision processes (POMDPs) are a widely-used framework to model decision-making with uncertainty about the environment and under stochastic outcome. In conventional POMDP models, the observations that the agent receives originate from fixed known distribution. However, in a variety of real-world scenarios the agent has an active role in its perception by selecting which observations to receive. Due to the combinatorial nature of such a selection process, it is computationally intractable to integrate the perception decision with the planning decision. 

The author proposes a new form of POMDPs called AP2-POMDP, which takes active perception into account. The AP2-POMDP problem restricts the maximum number of sensors that can be selected by an agent. The agent also faces the planning problem to select the sensors. To prevent such expansion of the action space, the authors propose a greedy strategy for observation selection and obtain a near optimal bound based on submodular optimization.

The author also proposes a greedy-based scheme for the agent to find an almost optimal active perception action by minimizing the uncertainty of beliefs. The author also uses theories to prove the near-optimal guarantees of this greedy method. The author also proposes a novel perception-aware point-based value iteration to calculate the value function and obtain the policy. The author also operates an interesting simulation experiment, which shows less uncertainty of the robot when taking planning actions when using the proposed solver.

The contribution is significant in reinforcement community. The writing is in general clear. It can be improved with minor modifications, for example, explaining math equations better in English. 

My main comment for the authors is whether they have considered the scenario where the perception and the planning actions are connected. I agree with the authors that the best strategy for perception is to reduce uncertainty (and indeed, the greedy approach yields a near-optimal performance), given the restricted situation that the perception and planning are two separated processes. Nonetheless, in most real-world applications, the two processes are coupled, and therefore, we face, immediately, the trade-off between exploration and exploitation. I wonder if the authors have considered how they can extend their approach to such scenarios. 

A few minor comments:

	
(i) The authors should add a legend and perhaps, more explanation in the captions of Figure 5. The colors of the heat-map are confusing. If dark blue and dark red represent lowest and highest frequency, what about other colors? Are there obstacles placed in the grid? If so, are they placed as shown in Figure 3(b)?
	
(ii) What is the effect of k, the maximum number of sensors to be placed? Can the authors provide a figure showing the change of performance with varying k?
	
(iii) It will be more convincing if the author deploys this algorithm to real-world robots and demonstrate its effectiveness. 	

","The sentiment of the review is generally positive, as the reviewer acknowledges the significance of the contribution to the reinforcement learning community and finds the writing clear, with only minor modifications suggested. The reviewer also provides constructive feedback and specific recommendations for improvement, which indicates a positive and supportive attitude. The politeness of the language is high, as the reviewer uses polite phrases such as 'My main comment,' 'I wonder if,' and 'It will be more convincing if,' which show respect and consideration for the authors' work.",80,90
"This paper presents a new adversarial defense based on ""cleaning"" images using a round trip through a bidirectional gan.  Specifically, an image is cleaned by mapping it to latent space and back to image space using a bidirectional gan.  To encourage the bidirectional gan to focus on the semantic properties, and ignore the noise, the gan is trained to maximize the mutual information between z and x, similar to the info gan.

Pros:
	1. The paper presents a novel (as far as I am aware) way to defend against adversarial attacks by cleaning images using a round trip in a bidirectional gan

Cons:
	1. The method performs significantly worse than existing techniques, specifically adversarial training.
		a. The authors argue ""Although better than FBGAN, adversarial training has its limitation: if the attack method is harder than the one used in training(PGD is harder than FGSM), or the perturbation is larger, then the defense may totally fail. FBGAN is effective and consistent for any given classifier, regardless of the attack method or perturbation.""
		b. I do not buy their argument, however, because one can simply apply the strongest defense (PGD 0.3 in their results) and this outperforms their method in *all* attack scenarios.  And if someone comes out with a new stronger attack there's no guarantee their method will be strong defense against that method
	2. The paper is not written that well.  Even though the technique itself is very simple, I was unable to understand it from the introduction, and didn't really understand what they were doing until I reached the 4th page of the paper. 
	

Missing citation:
PixelDefend: Leveraging Generative Models to Understand and Defend against Adversarial Examples  (ICLR 2018)
","The sentiment of the review is mixed but leans towards negative. The reviewer acknowledges the novelty of the approach but criticizes its performance compared to existing methods and the clarity of the writing. Therefore, the sentiment score is -40. The politeness of the language is generally respectful and constructive, with no rude or harsh language used, so the politeness score is 60.",-40,60
"This is a well-written paper studying the important problem of dynamic network embedding. Please find below some pros and cons of this paper.
Pros:

* Studies the important problem of network embedding under a more realistic setting (i.e., nodes & edges evolve over time).
* Introduces an interesting architecture that uses two forms of attention: structural and temporal.
* Demonstrated the effectiveness of the temporal layers through additional experiments (in appendix) and also introduced a variant of their proposed approach which can be trained incrementally using only the last snapshot.

Cons:

* The authors compared against several dynamic & static graph embedding approaches. If we disregard the proposed approach (DySAT), the static methods seem to match and even, in some cases, beat the dynamic approaches on the compared temporal graph datasets. The authors should compare against stronger baselines for static node embedding, particularly GAT which introduced the structural attention that DySAT uses to show that the modeling of temporal dependencies is necessary/useful. Please see [1] for an easy way to train GCN/GAT for link prediction.
* There are actually quite a number of work done on network embedding on dynamic graphs including [2-4]. In particular, [2-3] support node attributes as well as the addition/deletion of nodes & edges. The author should also compare against these work.
* The concept of temporal attention is quite interesting. However, the authors do not provide more analysis on this. For one, I am interested to see how the temporal attention weights are distributed. Are they focused on the more recent snapshots? If so, can we simply retain the more relevant recent information and train a static network embedding approach? Or are the attention weights distributed differently?

[1] Modeling Polypharmacy Side Effects with Graph Convolutional Networks. Zitnik et. al. BioInformatics 2018. 
[2] Attributed Network Embedding for Learning in a Dynamic Environment. Li et. al. In Proc. CIKM '17. 
[3] Streaming Link Prediction on Dynamic Attributed Networks. Li et. al. In Proc. WSDM '18. 
[4] Continuous-Time Dynamic Network Embeddings. Nguyen et. al. In Comp. Proc. WWW '18. ","The sentiment of the review is generally positive, as indicated by the initial statement 'This is a well-written paper studying the important problem of dynamic network embedding.' The reviewer acknowledges the strengths of the paper, such as addressing an important problem, introducing an interesting architecture, and demonstrating effectiveness through experiments. However, the reviewer also provides constructive criticism and suggestions for improvement, which are presented in a balanced and respectful manner. The politeness of the language is high, as the reviewer uses polite phrases like 'Please find below,' 'The authors should,' and 'I am interested to see,' which indicate a respectful and considerate tone.",70,90
"Summary: The paper proposes a new architecture to defend against adversarial examples. The authors propose a network with new type of hidden units (RBFI units). They also provide a training algorithm to train such networks and evaluate the robustness of these models against different attacks in the literature. 

Main concern: I think the idea  proposed here of using RBFI units is very interesting and intuitive. As pointed out in the paper, the RBFI units make it difficult to train networks using standard gradient descent, because the gradients can be uninformative. They propose a new training algorithm based on ""pseudogradients"" to mitigate this problem. However, while evaluating the model against attacks, only gradient based attacks are used (like PGD attack of Madry et al., or Carlini and Wagner). It's natural to expect that since the gradients are uninformative, these attacks might fail. However, what if we considered similar ""pseudogradient"" based attacks? In particular, just use the same training procedure formulation to attack (where instead of minimizing loss like in training, we maximize loss)?
I think this key experiment is missing in the paper and without this evaluation, it's hard to claim whether the models are more robust fundamentally, or it's just gradient masking. 


Revision: After the authors revision, I change my score since they addressed my main complaint about results using pseudogradient attacks 
","The sentiment score is determined by the overall tone and content of the review. The reviewer starts with a positive note, appreciating the interesting and intuitive idea of using RBFI units, which suggests a positive sentiment. However, the main concern about the missing key experiment introduces a critical but constructive tone. The sentiment score is therefore moderately positive at 50. The politeness score is high because the reviewer uses polite language throughout, acknowledges the authors' efforts, and provides constructive feedback without being dismissive or rude. The politeness score is 80.",50,80
"This paper proposes two gated deep learning architectures for sensor fusion. They are all based on the previous work 
Naman Patel et al's modality fusion with CNNs for UGV autonomous driving in indoor environments (IROS). By having the grouped features, the author demonstrated improved performance, especially in the presence of random sensor noise and failures.

#Organization/Style:
The paper is well written, organized, and clear on most points. A few minor points:
1) The total length of the paper exceeds 8 pages. Some figures and tables should be adjusted to have it fit into 8 pages.
2) The literature review is limited.
3) There are clearly some misspellings. For example, the ""netgated"" is often written as ""negated"".

#Technical Accuracy:
The two architecture that the author proposes all based on the grouped features, which to my point of view, is a very important and necessary part of the new model. However, the author failed to rigorously prove or clearly demonstrated that why this is effective to our new model.  Moreover, how to make groups or how many groups are needed are not clearly specified. The experiments used only two completely different datasets, none of them are related to the previous sensor fusion method they are trying to compete. I'm afraid this method cannot generalize to a common case.

In addition, if we look at Table 4 and Table 5, we can find the first Group-level Fusion Weight actually increases, which seems contradictory to the result shown in Table 6.

#Adequacy of Citations: 
Poor coverage of literature in sensor fusion. There are less than 10 references are related to sensor fusion.

Overall, it is not an ICLR standard paper.","The sentiment score is derived from the overall tone of the review. The reviewer acknowledges that the paper is well-written, organized, and clear on most points, which is positive. However, the review also highlights several significant issues, such as the length of the paper, limited literature review, misspellings, lack of rigorous proof for the proposed model, and poor coverage of sensor fusion literature. These criticisms bring the sentiment closer to neutral but still slightly positive due to the initial praise. Therefore, the sentiment score is 10. The politeness score is based on the language used in the review. The reviewer uses polite language, such as 'well written,' 'organized,' and 'clear,' and provides constructive feedback without being rude or harsh. Therefore, the politeness score is 80.",10,80
"Statistics based on KNN distances are ubiquitous in machine learning. In this paper the authors propose to apply the existing LID metric to GANs. The metric can be decomposed as follows: (1) Given a point x in X, compute the k-nearest neighbors KNN(x, X) and let those distances be R1, R2, …, Rk. Now, rewrite LID(x, X) = [max_over_i (log Ri) - mean_over_i (log Ri)] to uncover that the distribution of (log-)distances is summarized as a function of the max distance and the mean distance. (2) To extend the metric to two sets, A and B, define CrossLID(A; B) = E_(x in A) [LID(x, B)]. To see why CrossLID is useful, let X be the observed data and G the generated data. First consider CrossLID(A, B) where A=B=X which determines a lower-bound which is essentially the average (over elements of A) LID statistic determined by the underlying KNN graph of X. Now, keep A=X, and progressively change B to G (say by replacing some points from X with some points from G). This will induce a change of the distance statistics of some points from A, which will be detected on the individual LID scores of those points, and will hence be propagated to CrossLID. As a result, LID close to the baseline LID detects both sample quality issues as well as mode dropping/collapse issues. In practice, instead of computing this measure in the pixel space, one can compute it in the feature space of some feature extractor, or in some cases directly in the learned feature space of the generator. Finally, given some labeling of the points, one can keep track of the CrossLID statistic for each mode and use this during training to oversample modes for which the gap between the expected CrossLID and computed one is large.

Clarity: I think the clarity can be improved -- instead of stating the (rather abstract) properties of LID, the readers might benefit from the direct discussion of the LID estimator and a couple of examples, derive the max - mean relationship for the MLE estimator and provide some guiding comments. In a later section one might discuss why the estimator is so powerful and generally applicable. Secondly, the story starts with “discriminability of the distance measure” and the number of latent variables needed to do it, but I felt that this only complicated matters as many of these concepts are unclear at this point. 
Originality: Up to my knowledge, the proposed application is novel, albeit built on an existing (well-known) estimator. Nevertheless, the authors have demonstrated several desirable properties which might be proven useful in practice.
Significance of this work: The work is timely and attempts to address a critical research problem which hinders future research on deep generative models.

Pro:
- Generally well written paper, although the clarity of exposition can be improved. 
- Estimator is relatively easy to compute in practice (i.e. the bottleneck will still be in the forward and backward passes of the DNNs).
- Can be exploited further when labeled data is available
- Builds upon a strong line of research in KNN based estimators.
- Solid experimental setup with many ablation studies.

Con:
- FID vs CrossLID: I feel that many arguments against FID are too strong. In particular, in “robustness to small input noise” and “robustness to input transformation” you are changing the underlying distribution *significantly* -- why should the score be invariant to this? After all, it does try to capture the shift in distribution. In the robustness to sample size again FID is criticized to have a high-variance in low-sample size regime: This is well known, and that’s why virtually all work presenting FID results apply 10k samples and average out the results over random samples. In this regime it was observed that it has a high bias and low variance (Figure 1 in [1]). In terms of the dependency of the scores to an external model, why wouldn’t one compute FID on the discriminator feature space? Similarly, why wouldn’t one compute FID in the pixel space and get an (equally bad) score as LID in pixel space? Given these issues, in my opinion, Table 1 overstates the concerns with FID, and understates the issues with CrossLID. 
- FID vs CrossLID in practice: I argue that the usefulness comes from the fact that relative model comparison is sound. From this perspective it is critical to show that the Spearman’s rank correlation between these two competing approaches on real data sets is not very high -- hence, there are either sample quality or mode dropping/collapsing issues detected by one vs the other. Now, Figure 1 in [1] shows that this FID is sensitive to mode dropping. Furthermore, FID is also highly correlated with sample quality (page 7 of [2]).
- A critical aspect here is that in pixel space of large dimension the distances will tend to be very similar, and hence all estimators will be practically useless. As such, learning the proper features space is of paramount importance. In this work the authors suggest two remedies: (1) Compute a feature extractor by solving a surrogate task and have one extractor per data set. (2) During the training of the GAN, the discriminator is “learning” a good feature space in which the samples can be discriminated. Both of these have significant drawbacks. For (1) we need to share a dataset-specific model with the community. This is likely to depend on the preprocessing, model capacity, training issues, etc.. Then, the community has to agree to use one of these. On the other hand, (2) is only useful for biasing a specific training run. Hence, this critical aspect is not addressed and the proposed solution, while sensible, is unlikely to be adopted.
- Main contributions section is too strong -- avoiding mode collapse was not demonstrated. Arguably, given labeled data, the issue can be somewhat reduced if the modes correspond to labels. Similarly, if the data is well-clusterable one can expect a reduction of this effect. However, as both the underlying metric as well as the clustering depends on the feature space, I believe the claim to be too strong. Finally, if we indeed have labels or some assumptions on the data distribution, competing approaches might exploit it as well (as done with i.e. conditional GANs).
- In nonparametric KNN based density estimation, one often uses statistics based on KNN distances. What is the relation to LID?

With respect to the negative points above, without having a clear cut case why this measure outperforms and should replace FID in practice, I cannot recommend acceptance as introducing yet another measure might slow down the progress. To make a stronger case I suggest:
(1) Compute Spearman's rank correlation between FIDs and CrossLIDs of several trained models across these data sets.
(2) Compute the Pearson's correlation coefficient across the data sets. Given that your method has access to dataset specific feature extractors I expect it perform significantly better than FID.
 
[1] https://arxiv.org/pdf/1711.10337.pdf
[2] https://arxiv.org/pdf/1806.00035.pdf

========
Thank you for the detailed responses. I have updated my score from 5 to 6.","The sentiment of the review is generally positive, as indicated by the acknowledgment of the paper's novelty, significance, and solid experimental setup. However, there are several critical points raised, particularly regarding the comparison with FID and the practical applicability of the proposed method. Therefore, the sentiment score is not fully positive but leans towards the positive side. The politeness of the language is high, as the reviewer provides constructive feedback and suggestions for improvement without using harsh or rude language.",40,80
"This paper investigates a meta-learning approach for the contextual bandit problem. The goal is to learn a generic exploration policy from datasets, and then to apply the exploration policy to contextual bandit tasks. The authors have adapted an algorithm proposed for imitation learning (Ross & Bagnell 2014) to their setting. Some theoretical guarantees straightforwardly extracted from (Ross & Bagnell 2014) and from (Kakade et al 2008) are presented. Experiments are done on 300 supervised datasets.

Major concerns:

1 This paper investigates a problem that does not correspond to the real problem: how to take advantage of a plenty of logs generated by a known stochastic policy (or worst unknown deterministic policy) for the same (or a close) contextual bandit task? 
Most of companies have this problem. I do not know a single use case, in which we have some full information datasets, which are representative of contextual bandit tasks to be performed. If the full information datasets does not correspond to the contextual bandit tasks, it is not possible to learn something useful for the contextual bandit task. 

2 The experimental validation is not convincing.

The experiments are done on datasets, which are mostly binary classification datasets. In this case, the exploration task is easy. May be it is the reason why the exploration parameter \mu or \epsilon = 0 provides the best results for MELEE or \epsilon-greedy?

The baselines are not strong. The only tested contextual bandit algorithm is LinUCB. However a diagonal approximation of the covariance matrix is used when the dimension exceeds 150. In this case LinUCB is not efficient. There are a lot of contextual bandit algorithms that scale with the dimension.


3 The theoretical guarantees are not convincing. 

The result of Theorem 1 is a weak result. A linear regret against the expected reward of the best policy is usually considered as a loosely result. Theorem 2 shows that there is no theoretical gain of the use of the proposed algorithm: the upper bound of the expected number of mistakes obtained when Banditron is used in MELEE is upper than the one of Banditron alone.

Minor concerns:

The algorithms are not well written. POLOPT function has sometimes one parameter, sometimes two and sometimes three parameters. The algorithm 1 is described in section 2, while one of the inputs of the algorithm 1 (feature extractor function) is described in section 3.1. The algorithm 1 seems to return all the N exploration policies. The choice of the returned policy has to be described.

In contextual bandits, the exploration policy is not handcrafted. The contextual bandit algorithms are designed to be optimal or near optimal in worst case: they are generic algorithms.
","The sentiment of the review is quite negative. The reviewer expresses major concerns about the relevance of the problem investigated, the experimental validation, and the theoretical guarantees. The language used is direct and critical, but not overtly rude. The reviewer points out specific issues and provides detailed feedback, which indicates a professional tone despite the negative sentiment.",-70,-10
"This paper tried to analyze the high-dimensional geometry of adversarial examples from a geometric framework. The authors explained that there exists a tradeoff between being robust to different norms. They further proved that it is insufficient to learn robust decision boundaries by training against adversarial examples drawn from balls around the training set. Moreover, this paper showed that nearest neighbor classifiers do not suffer from this insufficiency.
 
In general, I think this paper is very interesting and enlightening. The authors analyzed the most robust boundary of norm 2 and norm infinity in different dimensions through a simple example and concluded that the single decision boundary cannot be robust in different norms. In addition, the author started from a special manifold and proposed a bound (ratio of two volumes) to prove the insufficiency of the traditional adversarial training methods and then extended to arbitrary manifold. It is good that this might provide a new way to evaluate the robustness of adversarial training method. However, I have some concerns: 1) Is it rigorous to define the bound by vol_X/vol_pi? In my opinion, the ratio of the volume of intersection (X^\del and \pi^\del) and vol \pi^\del may be more rigorous? 2) I don't know if such bound can be useful or easily applied in other work? In my opinion, it might be difficult, since the volume itself appears difficult to calculate. 
I think the paper is a bit complicated or heavy in mathematics, and not easy to follow (though I believe I have well understood it). Some typos and minor issues are also listed as below. 

Minor concerns:
1. At the end of the introduction, 3 attacking methods, FGSM, BIM, and PGD, should be given their full names and also citations are necessary.
2. Could you provide a specific example to illustrate the bound in Eq. (3), e.g. in the case of d=3, k=1.
3. In Page 7, “Figure 4 (left) shows that this expression approaches 1 as the codimension (d-k) of Pi increases.”  I think, the subfigure shows that the ratio approaches 1 when d and k are all increased.
","The sentiment of the review is generally positive, as indicated by phrases like 'very interesting and enlightening' and 'it is good that this might provide a new way to evaluate the robustness of adversarial training method.' However, the reviewer does express some concerns and suggestions for improvement, which slightly tempers the overall positivity. Therefore, the sentiment score is 70. The politeness of the language is high, as the reviewer uses polite phrases such as 'I think,' 'In my opinion,' and 'Could you provide,' and offers constructive feedback without being harsh or dismissive. Thus, the politeness score is 90.",70,90
"This work proposes a hybrid VAE-based model (combined with an adversarial or maximum mean discrepancy (MMD) based loss) to perform timbre transfer on recordings of musical instruments. Contrary to previous work, a single (conditioned) decoder is used for all instrument domains, which means a single model can be used to convert any source domain to any target domain.

Unfortunately, the results are quite disappointing in terms of sound quality, and feature many artifacts. The instruments are often unrecognisable, although with knowledge of the target domain, some of its characteristics can be identified. The many-to-many results are clearly better than the pairwise results in this regard, but in the context of musical timbre transfer, I don't feel that this model successfully achieves its goal -- the results of Mor et al. (2018), although not perfect either, were better in this regard.

I have several further concerns about this work:

* The fact that the model makes use of pitch class and octave labels also raises questions about applicability -- if I understood correctly, transfer can only be done when this information is present. I think the main point of transfer over a regular generative model that goes from labels to audio is precisely that it can be done without label information.

* The use of fully connected layers also implies that it requires fixed length input, so windowing and stitching are necessary for it to be applied to recordings of arbitrary length. Why not train a convolutional model instead?

* I think the choice of a 3-dimensional latent space is poorly justified. Why not use more dimensions and project them down to 3 for visualisation and interpetation purposes with e.g. PCA or t-SNE? This seems like an unnecessary bottleneck in the model, and could partly explain the relatively poor quality of the results.

I appreciated that the one-to-one transfer experiments are incremental comparisons, which provides valuable information about how much each idea contributes to the final performance.

Overall, I feel that this paper falls short of what it promises, so I cannot recommend acceptance at this time.



Other comments:

* In the introduction, an adversarial criterion is referred to as a ""discriminative objective"", but ""adversarial"" (i.e. featuring a discriminator) and ""discriminative"" mean different things. I don't think it is correct to refer to an adversarial criterion as discriminative.

* Also in the introduction, it is implied that style transfer constitutes an advance in generative models, but style transfer does not make use of / does not equate to any generative model.

* Some turns of phrase like ""recently gained a flourishing interest"", ""there is still a wide gap in quality of results"", ""which implies a variety of underlying factors"", ... are vague / do not make much sense and should probably be reformulated to enhance readability.

* Introduction, top of page 2: should read ""does not learn"" instead of ""do not learns"".

* Mor et al. (2018) do actually make use of an adversarial training criterion (referred to as a ""domain confusion loss""), contrary to what is claimed in the introduction.

* The claim that training a separate decoder for each domain necessarily leads to prohibitive training times is dubious -- a single conditional decoder would arguably need more capacity than each individual separate decoder model. I think all claims about running time should be corroborated by controlled experiments.

* I think Figure 1 is great and helps a lot to distinguish the different domain translation paradigms.

* I found the description in Section 3.1 a bit confusing as it initially seems that the approach requires paired data (e.g. ""matching samples"").

* Section 3.1, ""amounts to optimizing"" instead of ""amounts to optimize""

* Higgins et al. (2016) specifically discuss the case where beta in formula (1) is larger than one. As far as I can tell, beta is annealed from 0 to 1 here, which is an idea that goes back to ""Generating Sentences from a Continuous Space"" by Bowman et al. (2016). This should probably be cited instead.

* ""circle-consistency"" should read ""cycle-consistency"" everywhere.

* MMD losses in the context of GANs have also been studied in the following papers:
- ""Training generative neural networks via Maximum Mean Discrepancy optimization"", Dziugaite et al. (2015)
- ""Generative Models and Model Criticism via Optimized Maximum Mean Discrepancy"", Sutherland et al. (2016)
- ""MMD GAN: Towards Deeper Understanding of Moment Matching Network"", Li et al. (2017)

* The model name ""FILM-poi"" is only used in the ""implementation details"" section, it doesn't seem to be referred to anywhere else. Is this a typo?

* The differences between UNIT (GAN; C-po) and UNIT (MMD; C-po) in Table 1 seem very small and I'm not convinced that they are significant. Why does the MMD version constitute an improvement? Or is it simply more stable to train?

* The descriptor distributions in Figure 3 don't look like an ""almost exact match"" to me (as claimed in the text). There are some clearly visible differences. I think the wording is a bit too strong here.","The sentiment of the review is generally negative, as the reviewer expresses disappointment with the sound quality and artifacts in the results, and states that the paper falls short of its promises. This is evident from phrases like 'the results are quite disappointing' and 'I cannot recommend acceptance at this time.' However, the reviewer does acknowledge some positive aspects, such as the incremental comparisons in the one-to-one transfer experiments and the helpfulness of Figure 1. Therefore, the sentiment score is -60. The politeness of the language is relatively high, as the reviewer uses polite and constructive language throughout the review, even when pointing out flaws. Phrases like 'I appreciated' and 'I think' indicate a respectful tone. Thus, the politeness score is 70.",-60,70
"This paper proposes a new application of embedding techniques for mathematical problem retrieval in adaptive tutoring. The proposed method performs much better than baseline sentence embedding methods. Another contribution is on using negative pre-training to deal with an imbalanced training dataset. 

To me this paper is just not good enough - the method essentially i) use ""a professor and two teaching assistants"" to build a ""rule-based concept extractor"" for problems, then ii) map problems into this ""concept space"" and simply treat them as words. There are several problems with this approach. 

First, doing so does not touch the core of the proposed application. For tutoring applications, the most important thing is to select a problem that can help students improve; even if you can indeed select a problem that is the most similar to another problem, is it the best one to show a student? There are no evaluations on real students in the paper. Moreover, the main difference between math problems and other problems is that there are math expressions; I do not think that using words/concept labels only is enough without touching on the math expressions.

Second, the proposed method does not sound scalable - the use of a professor and two teaching assistants to construct the concept extractor, and the use of an expert TA to select a small set of informative words. I am not sure how this will generalize to a larger number of problem spanning many different domains.

I also had a hard time going through the paper - there aren't many details. Section 2.1 is where the method is proposed, yet most of the descriptions there are unclear. Without these details it is impossible to judge the novelty of the ""rule-based concept extractor"", which is the key technical innovation.","The sentiment of the review is negative, as the reviewer explicitly states that the paper is 'just not good enough' and lists several significant issues with the proposed method. The sentiment score is -80 because the review is highly critical but not entirely dismissive. The politeness score is -20 because the language used is somewhat blunt and lacks the courtesy typically expected in academic reviews, though it is not overtly rude.",-80,-20
"Paper summary:

Given a pre-trained VAE (e.g. over images), this paper is about inferring the distribution over missing variables (e.g. given half the pixels, what is a plausible completion?). The paper describes an approach based on variational inference with normalizing flows: given observed variables, the posterior over the VAE's latents is inferred (variationally) and plausible completions for missing variables are sampled from the VAE decoder.

Technical quality:

The presented method is technically correct. The evaluation carefully compares different types of normalizing flow and HMC, and seems to follow good practices.

I have a suggestion for improving the GVI method. The way it's described in the paper, GVI requires computing the determinant of a DxD matrix, which costs O(D^3), and there is no guarantee that the matrix is invertible. However, this approach over-parameterizes the covariance matrix of the modelled Gaussian. Without losing any flexibility, you can use a lower triangular matrix with strictly positive diagonal elements (e.g. the diagonal elements can be parameterized as the exp of unconstrained variables). That way, the determinant costs O(D) (it's just the product of diagonal elements) and you ensure that the matrix is invertible (because the determinant is strictly positive), without hurting expressivity. You can think of this as parameterizing the Cholesky decomposition of the covariance matrix.

Also, there are more flexible normalizing flows, such as Inverse Autoregressive Flow, that can be used instead of the planar flow used in the paper.

Clarity:

The paper is written clearly and in full detail, and the mathematical exposition is clear and precise.

Some typos and minor suggestions for improvement:
- It'd be good to move Alg. 1 and Fig. 1 near where they are first referenced.
- Page 2: over to \theta --> over \theta
- Eq. 3: p_\theta appears twice in the middle.
- one can use MCMC to attempt sampling --> one can use MCMC to sample
- Eq. 5: should be q_\psi as subscript of E.
- Fig. 7, caption: should be GVI vs. NF.
- In references, should be properly capitalized: Hamiltonian, Langevin, Monte Carlo, Bayes, BFGS
- Lemma 1: joint divergence is equivalent to --> joint divergence is equal to
- Lemma 1: in the chain rule for KL, the second KL term should be averaged w.r.t. its free variables.

Originality:

In my opinion, there is little original contribution in this paper. The inference method presented (variational inference with normalizing flows) is well-known and already in use. The paper applies this method to VAEs, which is a straightforward application of a well-known inference method to a relatively simple graphical model (z -> {x, y}, with x, y independent given z).

I don't see the need for introducing a new term (cross-coder). According to the paper, a cross-coder is precisely a normalizing flow (i.e. an invertible smooth transformation of a simple density). I think new terms for already existing ideas add cognitive load to the community, and are better avoided.

Significance:

In my opinion, constructing generative models that can handle arbitrary patterns of missing data is an important research direction. However, this is not exactly what the paper is about: the paper is about inference in a given generative model. Given that there is (in my opinion) no new methodology in the paper, I wouldn't consider this paper a significant contribution.

I would also suggest that in a future version of the paper there is more motivation (e.g. in the introduction) of why the problem the paper is concerned with (i.e. missing data in generative models) is significant. Is it just for image completion / data imputation, or are there other practical problems? Is it important as part of another method / solution to another problem?

Review summary:

Pros:
- Technically correct, gives full detail.
- Well and clearly written, precise with maths.
- Evaluation section interesting to read.

Cons:
- No original contribution.
- Could do a better job motivating the importance of the problem.

Minor points:
- I don't completely agree with the way VAEs are described in sec. 2.1. As written, it follows that VAEs must have a Gaussian prior and a conditionally independent decoder. Although these are common choices in practice, they are not necessary: for example, one could take the prior to be a Masked Autoregressive Flow and the decoder a PixelCNN.
- Same for observation 1. This is not an observation, but an assumption; that is, the paper assumes that the decoder is conditionally independent. This is of course an assumption that we can satisfy by design, but it's a design choice that restricts the decoder in a specific way.","The sentiment of the review is mixed. While the reviewer acknowledges the technical correctness, clarity, and detailed evaluation of the paper, they also highlight a lack of originality and significant contribution. The sentiment score is therefore slightly negative at -20. The politeness of the language is high, as the reviewer provides constructive feedback and suggestions in a respectful manner, without any rude or harsh language. The politeness score is 80.",-20,80
"In this paper, the authors study the problem if learning for observation, a reinforcement learning setting where an agent is given a data set of experiences from a potentially arbitrary number of demonstrators. The authors propose a method which deploys these experience to initialize a place. Then estimate the value of this policy in order to improve it.

The paper is well written and it is easy to follow. 

Most of the theoretical results are interesting and the derivations are kinda straightforward but not fully matching the main claim in the paper. Mainly the contribution in this paper heavily depends on an assumption that Q^D and Q^\beta are close to each other. This assumption simplifies the many things resulting in a simple algorithm. But this assumption is too strong while the main challenge in the line of learning from observation comes from the fact that this assumption does not hold. Under this assumption and the similarity in distributions mentioned in proposition 4.2 make the contribution of this paper significantly weak.

Please let me know if you do not actually use this assumption in your results and justification.","The sentiment of the review is generally positive, as indicated by phrases like 'well written' and 'easy to follow.' However, the reviewer also points out significant weaknesses in the paper's assumptions, which tempers the overall positivity. Therefore, the sentiment score is 30. The politeness of the language is quite high, as the reviewer uses polite phrases such as 'please let me know' and does not use any harsh or rude language. Therefore, the politeness score is 80.",30,80
"The submission presents an extension to the Allamanis et al ICLR'18 paper on learning from programs as graphs. The core contribution is the idea of introducing extra nodes and edges into the graph that correspond to (potentially rare) subwords used in the analyzed program code. Experiments show that this extended graph leads to better performance on two tasks, compared to a wide range of baseline methods.

Overall, this is a nice paper with a small, incremental idea and substantial experiments that show its practical value. I only have minor comments / questions on the actual core content. However, the contribution is very incremental and of interest to a specialized subsegment of the ICLR audience, so it may be appropriate to reject the paper and redirect the authors to a more specialized venue.

Minor comments:
- There's a bunch of places where \citep/\citet are mixed up (e.g., second to last paragraph of page 2). It would make sense to go through the paper one more time to clean this up.
- Sect. 4: I understand the need to introduce context, but it feels that more space should be spent on the actual contribution here (step 3). For example, it remains unclear why this extra nodes / edges are only introduced for subwords appearing in variables - why not also for field names / method names?
- Sect. 5: It would be helpful if the authors would explicitly handle the code duplication problem (Lopes et al., OOPSLA'17), or discuss how they avoided these problems. Duplicated data files occurring in several folds are a significant risk to the validity of their experimental findings, and very common in code corpora.
- Table 1: It is unclear to me what the ""Pointer Sentinel"" model can achieve. Without edges connecting the additional words to where they occur, it seems that this should not be performing different than ""Closed Vocab"", apart from noise introduced by additional nodes.
- Table 1: Do Pointer Sentinel/GSC use a CharCNN to embed node labels of nodes that are not part of the ""cache"", or a closed vocabulary? [i.e., what's the embedding of a variable ""foo""?] If not, what is the performance of the GSC model with CharCNN-embeddings everywhere? That would be architecturally simpler than the split variant, and so may be of interest.
- Page 6: When truncating to 500 nodes per graph: How many graphs in your dataset are larger than that?
- Page 7: Do you really use attention over all nodes, instead of only nodes corresponding to variables? How do you deal with results where the model picks a non-variable (e.g., a corresponding cache node)? Does this happen?
","The sentiment of the review is generally positive, as indicated by phrases like 'nice paper' and 'substantial experiments that show its practical value.' However, the reviewer also notes that the contribution is very incremental and may only be of interest to a specialized subsegment of the ICLR audience, suggesting a possible rejection. This mixed sentiment results in a score of 50. The politeness of the language is high, as the reviewer uses polite and constructive language throughout, such as 'It would make sense to go through the paper one more time to clean this up' and 'It would be helpful if the authors would explicitly handle the code duplication problem.' Therefore, the politeness score is 80.",50,80
"Summary: 
This paper presents three small improvements for training binarized neural networks: (1) a modified straight-through estimator, (2) a novel regularizer to push weights to +/- 1, and (3) the use of scaling factors for the binarized weights. Using the methods presented, the validation accuracies on ImageNet and CIFAR-10 are improved by just under 2 percentage points.

  Pros:
    - Decent improvement in the performance of the binarized network in the end
    - The presented regularizers make sense and seem effective. The modified straight-through estimator seems reasonable as well, although the authors do not compare to recent work with a similar adjustment. 

  Cons:
    - The paper is poorly written and confusing. It reads as if it was written in one pass with no editing or re-writing to clarify contributions or key points, or ensure consistency.
    - While the final numbers are acceptable, the experiments themselves could be stronger and could be presented more effectively.
   - The novelty of the scale factors is questionable.


Questions and comments:

1. How exactly is the SS_\beta activation used? It is entirely unclear from the paper, which contradicts itself in multiple ways. Is SS_\beta used in the forward pass at all for either the weight or activation binarization? Or is only its derivative used in the backward pass? If the latter, then you are not replacing the activation anywhere but are simply using a different straight-through estimator in place of the saturated straight-through estimator (e.g., see [1]).
   (a) At the beginning of Section 3.3, you say that you modify the training procedure by replacing the sign binarization with the SS_\beta activation. This sounds like it is referring to the activation function at each layer; however, the pseudocode says that you are using sign() as the per-layer activation. 
   (b) Further, Figure 4 shows that you are using the SS_\beta function to do weight binarization. However, again, the pseudocode shows that you are using sign() to do the weight binarization. 

2. In [1], the authors used a similar type of straight-through estimator (essentially, the gradient of tanh instead of hard_tanh) and found that to be quite effective. You should compare to their method. Also, it's possible that SS_\beta reduces to tanh for some choice of \beta -- is this true?

3. The use of scale factors seems to greatly increase the number of parameters in the network and thus greatly decrease the compression benefits gained by using binarization, i.e., you require essentially #scale_factors =  a constant factor times the number of actual parameters in the network (since you have a scale factor for each convolutional filter and for each column of each fully-connected layer). As a result of this, what is the actual compression multiplier that your network achieves relative to the original network?

4. For the scale factor, how does yours differ from that used in Rastegari et al. (2016)? It seems the same but you claim that it is a novel contribution of your work. Please clarify.

5. Why did learning \beta not work? What was the behavior? What values of \beta did learning settle on? 

6. I assume that for each layer output y_i = f(W_i x_i), the regularizer is applied as R(y_i) while at the same time y_i is passed to the next layer -- is this correct? The figures do not clearly show this and should be changed to more clearly show how the regularizer is computed and used, particularly in relation to the activation.

7. In the pseudocode:
   (a) What does ""mostly bitwise operations"" mean? Are some floating point?
   (b) Is this the shift-based batchnorm of Hubara et al. (2016)?

8. For Table 1:
   (a) I assume these are accuracies? The caption should say.
   (b) Why are there no comparisons to the performance of other methods on this dataset?
   (c) Any thoughts as to why your method performs better than the full-precision method on this dataset for VGG?

8. For Table 2:
   (a) Does Table 2 show accuracies on ImageNet? You need to make this clear in the caption.
   (b) What type of behavior do the runs that do not converge show? This seems like a learning rate problem that is easily fixable. Are there no hyperparameter values that allow it to converge?
   (c) What behavior do you see when you use SS_1 or SS_2, i.e., \beta = 1 or \beta = 2? Since lower \beta values seem better.
   (d) The regularization seems to be the most useful contribution -- do you agree?
   (e) Why did you not do any ablations for the scale factor? Please include these as well.

9. For Table 3, did you compute the numbers for the other approaches or did you use the numbers from their papers? Each approach has its own pros and cons. Please be clear.

10. Are there any plots of validation accuracy versus epoch/time for the different algorithms in order to ensure that the reported numbers were not simply cherry-picked from the run? I assume that you simply used the weights from the end of the 50th epoch -- correct? 

11. Is there evidence for your introductory claims that 'quantizing weights ... make neural networks harder to train due to a large number of sign fluctuations' and 'maintaining a global structure to minimize a common cost function is important' ? If so, you should cite this evidence. If not, you should make it clear that these are hypotheses. 

12. Why are there not more details about the particular architectures used? These should be included in the appendices to aid those who would like to rerun your experiments. In general, please include more experiment details in the body or appendices.


Detailed comments:
- R(l) is not defined in Figure 1 and thus is confusing. Also, its replacement of 'Error' from the original figure source makes the figure much more confusing and less clear.

- Typos:
   - 'accustomed' (p.1)
   - 'the speed by quantizing the activation layers' doesn't make sense (p.1)
   - 'obtaining' (p.4)
   - 'asymmetric' doesn't make sense because these are actually symmetric functions across the y-axis (p.4)
   - 'primary difference is that this regularization ...' --> 'primary difference is that their regularization ...' (p.4)
   - 'the scales with 75th percentile of the absolute value ... ' is very confusing and unclear (p.7)
   - 'the loss metric used was the cross-entropy loss, the order of R_1.' I do not know what you're trying to say here (p.8)

- Citations: Fix the capitalization issues, typos, and formatting inconsistencies.


[1]  Friesen and Domingos. Deep Learning as a Mixed Convex-Combinatorial Optimization Problem. ICLR 2018.


-------------------

After reading the author response, I do not think the paper does a sufficient job of evaluating the contributions or comparing to existing work. The authors should run ablation experiments, compare to existing work such as [1], and evaluate on additional datasets. These were easy tasks that could have been done during the review period but were not.

If I wanted to build on top of this paper to train higher accuracy binary networks, I would have to perform all of these tasks myself to determine which contributions to employ and which are unnecessary. As such, the paper is currently not ready for publication.
","The sentiment score is determined by the overall tone and content of the review. The reviewer acknowledges some positive aspects of the paper, such as the decent improvement in performance and the effectiveness of the regularizers. However, the review is predominantly critical, highlighting significant issues with the writing, clarity, novelty, and experimental rigor. Therefore, the sentiment score is slightly negative. The politeness score is based on the language used throughout the review. While the reviewer is critical, they maintain a professional tone and provide constructive feedback without resorting to rudeness or personal attacks. Thus, the politeness score is positive.",-30,40
"In this paper, authors propose a deep generative model and a variant for graph generation and conditional graph generation respectively. It exploits an encoder which is built based on GCN and GraphSAGE, a autoregressive LSTM decoder which generates the graph embedding, and a factorized edge based probabilistic model for generating edge and node type. For conditional generation, authors also propose a discriminating training scheme based on maximizing the mutual information. Experiments on ZINC dataset show that the proposed method is promising.

Strength:

1, The problem this paper tries to tackle is very challenging and of great significance. Especially, the conditional graph generation direction under the deep learning context is novel. 

2, The overall model is interesting although it is a bit complicated as it combines quite a few modules.

Weakness:

1, In the reconstruction experiment, comparisons with several recent competitive methods are missing. For example, the methods which have been already discussed in the related work, Li et al. (2018a), You et al. (2018a) and You et al. (2018b). Moreover, it is not explained whether the comparison setting is the same as Jin et al. (2018) and what the size of the latent code of their method is. It seems less convincing by just taking results from their paper and do the comparison.

2, Authors motive their work by saying in the abstract that “other graph generative models are either computationally expensive, limiting their use to only small graphs or are formulated as a sequence of discrete actions needed to construct a graph, making the output graph non-differentiable w.r.t the model parameters”. However, if I understood correctly, in Eq. (7), authors compute the soft adjacency tensor which is a dense tensor and of size #node by #node by #edge types. Therefore, I did not see why this method can scale to large graphs.

3, The overall model exploits a lot of design choices without doing any ablation study to justify. For example, how does the pre-trained discriminator affect the performance of the conditional graph generation? Why not fine-tune it along with the generator? The overall model has quite a few loss functions and associated weights of which the values are not explained at all.

4, Conditional generation part is not written clearly. Especially, the description of variational mutual information phase is so brief that I do not understand the motivation of designing such an objective function. What is the architecture of the discriminator?

5, How do authors get real attributes from the conditionally generated molecules? It is not explained in the paper.

Typos:

1, There are a few references missing (question mark) in the first and second paragraphs of section 2.

2, Methods in the experiment section are given without explicit reference, like GCPN.

3, Since edge type is introduced, I suggest authors explicitly mention the generated graphs are multi-graph in the beginning of model section. 

Overall, I do not think this paper is ready for publishing and it could be improved significantly.

---------------------------------------------------------------------------------------------------------------------------------------------------------------------

Update:

Thanks for the detailed explanation. The new figure 1 is indeed helpful for demonstrating the overall idea. 

However, I still found some claims made by authors problematic. 
For example, it reads in the abstract that ""...or are formulated as a sequence of discrete actions needed to construct a graph, making the output graph non-differentiable w.r.t the model parameters..."". 
Clearly, Li et al. 2018b has a differentiable formulation which falls under your description.

Besides, I suggest authors adjust the experiment such that it focuses more on comparing conditional generation. 
Also, please set up some reasonable baselines based on previous work rather than saying it is not directly comparable.
Directly taking numbers from other papers for a comparison is not a good idea given the fact that these experiments usually involve quite a few details which could potentially vary significantly.

Therefore, I would like to keep my original rating. 
","The sentiment of the review is mixed but leans towards the negative side. The reviewer acknowledges the significance and novelty of the problem tackled by the paper and finds the overall model interesting, which contributes positively to the sentiment. However, the reviewer lists several significant weaknesses and concludes that the paper is not ready for publishing, which heavily impacts the sentiment negatively. Therefore, the sentiment score is -40. The politeness of the language used is generally high. The reviewer uses polite language, provides constructive criticism, and offers suggestions for improvement. Even when pointing out weaknesses, the reviewer maintains a respectful tone. Therefore, the politeness score is 80.",-40,80
"In this paper, the effect of batch normalization to the maximum eigenvalue of the Fisher information is analyzed. The techinique is mostly developed by Karakida et al. (2018). The main result is an informal bound of the maximum eigenvalue, which is given without proof. Though, the numerical result corresponds to the derived bound.

The paper is basically well written, but the technical part has several notational problems. For example, there is no definition of ""\otimes"", ""\odot"", and ""Hess"" operators.

The use of the mean-field theory is an interesting direction to analyze batch normalization. However, in this paper, it seems failed to say some rigorous conclusion. Indeed, all of the theoretical outcomes are written as ""Claims"" and no formal proof is given. Also, there is no clear explanation of why the authors give the results in a non-rigorous way, where is the difficult part to analyze in a rigorous way, etc. 

Aside from the rigor issue, the paper heavily depends on the study of Karakida et al. (2018). The derivation of the bound (44) is directly built on Karakida's results such as Eqs. (7,8,20--22), which reduces the paper's originality.

The paper also lacks practical value. Can we improve an algorithm or something by using the bound (44) or other results?","The sentiment of the review is moderately negative, as it highlights several significant issues with the paper, such as lack of rigor, originality, and practical value. However, it does acknowledge that the paper is well-written and that the use of mean-field theory is an interesting direction. Therefore, the sentiment score is -40. The politeness of the language is relatively high, as the reviewer uses polite phrases like 'basically well written' and 'interesting direction,' and avoids harsh or rude language. Thus, the politeness score is 60.",-40,60
"Authors propose a new method called TriMap, which captures higher orders of structure with triplet information, and minimize a roust loss function for satisfying the chosen triplets.
 
The proposed method is motivated by the misleading selection approach for a dimensionality reduction method using local measurements. And then, authors resort to an evaluation based on visual clues based on a number of transformations. Authors then claim that any DR method preserving the global structure of the data should be able to handle these transformations.  An example on MNIST data illustrate these properties, but it is still not clear what are the visual clues as the criterion to select a good DR method and what are the global structures.
 
Authors discussed the results in Figure 4 for six real-world datasets, but there is no convincing evidence from the corresponding domains or reference researches for the support of the global structure in the learned embedding space.  It will be good to add some convincing evidences for the conclusion.
 
As the method highly depends on the subset of sampled triplets, it is interesting to see how the global structure changes if a different set of triplets is used.  In addition, it is unclear why sampled triplets can achieve a global structure of data instead of pairwise relations. From the experiments, triplets are also sampled according to the pairwise nearest neighbor graph.","The sentiment of the review appears to be slightly negative. The reviewer acknowledges the proposed method and its motivation but raises several concerns and points out unclear aspects and lack of convincing evidence. This indicates a sentiment score of -30. The politeness of the language is relatively high; the reviewer uses phrases like 'it will be good to add' and 'it is interesting to see,' which are constructive and polite. Therefore, the politeness score is 70.",-30,70
"This paper proposes Deep Overlapping Community detection model (DOC), a graph convolutional network (GCN) based community detection algorithm for network data. The model is a simple combination of GCN and existing framework for community detection. The proposed algorithm is compared to baselines on various datasets, and demonstrated to be accurate in many cases.

I think the paper does not deal with one of the most important aspects of network modeling - the degree heterogeneity of nodes. Many works reported that lack of degree corrections would result in bad estimates of community structures [1,2,3]. Probably including the degrees as feature of nodes would be helpful. 

Regarding the stochastic gradient descent by edge subsampling, I think the authors should mention [4], where the idea of edge subsampling in stochastic gradient descent setting was introduced before this work. Also, it is worth noting that we may lose some important distributional properties in graphs if we naively subsample from it [5]. For instance, sampling from positive and negative pairs to balance the class contribution may distort the sparsity and degree distributions of subsampled graphs. 

If we choose to use Bernoulli-Poisson link function, we can reduce the time complexity of likelihood and gradient computation to O(N + E), where N is the number of nodes and E is the number of edges, with the auxiliary variable trick introduced in [6]. In that case we don't really have to worry about subsampling. Why didn't you consider applying this to your model?

Regarding the experiments, I think some important baselines are missing [3, 6]. Also, I wonder whether the proposed algorithm would scale to the graphs with more than 100,000 nodes. 

References
[1] B. Karrer and M. E. J. Newman. Stochastic blockmodels and community structure in networks. Physical Review E, 83(1):016107, 2011.
[2] P. K. Gopalan, C. Wang, and D. Blei. Modeling overlapping communities with node popularities. NIPS 2013.
[3] A. Todeschini, X. Miscouridou and F. Caron. Exchangeable Random Measures for Sparse and Modular Graphs with Overlapping Communities. CoRR 2016.
[4] J. Lee, C. Heakulani, Z. Ghahramani, L. F. James, and S. Choi. Bayesian inference on random simple graphs with power law degree distributions. ICML 2017.
[5] P. Orbanz. Subsampling large graphs and invariance in networks. CoRR 2017.
[6] M. Zhou. Infinite edge partition models for overlapping community detection and link prediction. AISTATS 2015","The sentiment of the review is slightly negative. The reviewer acknowledges the accuracy of the proposed algorithm in many cases but highlights several significant shortcomings and missing elements in the paper, such as the lack of consideration for degree heterogeneity, missing important baselines, and potential issues with subsampling. The politeness of the language is high. The reviewer provides constructive criticism and suggestions for improvement without using harsh or rude language. They also reference relevant literature to support their points, which is a respectful and professional approach.",-30,80
"This paper presents an a particular architecture for conditional discriminators in the cGAN framework. Different to the conventional approach of concatenating the conditioning information to the input, the authors propose to process them separately with two distinct convolutional networks fusing (by element-wise addition) intermediate features of the conditioning branch into the input branch at each layer.

Pros:
+ The writing is mostly clear and easy to follow.
+ I feel that exploring better conditioning strategies is an important direction. Quite often the discriminator discards additional inputs if no special measures against this behaviour are taken.
+ The proposed method seem to outperform the baselines

Cons:
- I’m generally not excited about the architecture as it seems a slight variation of the existing methods. See, for example, the PixelCNN paper [van den Oord et al., 2016] and FiLM [Perez et al., 2017].
- Theoretical justification of the approach is quite weak. The paper shows that the proposed fusion method may result in higher activation values (in case of the ReLU non-linearity, other cases are not considered at all) but this is not linked properly to the performance of the entire system. Paragraph 3 of section 3.1 (sentence 3 and onward) seems to contain a theoretical claim which is never proved.
- It seems that the authors never compare their results with the state-of-the-art. The narrative would be much more convincing if the proposed way of conditioning yielded superior performance compared to the existing systems. From the paper it’s not clear how bad/good the baselines are.

Notes/questions:
* Section 3.1, paragraph 1: Needs to be rephrased. It’s not totally clear what the authors mean here.
* Section 3.1, paragraph 4: “We observed that the fusion …” -  Could you elaborate on this? I think you should give a more detailed explanation with examples because it’s hard to guess what those “important features” are by looking at the figure.
* Figure 4: I would really want to see the result of the projection discriminator as it seems to be quite strong according to the tables. The second row of last column (which is the result of the proposed system) suspiciously resembles the ground-truth - is it a mistake?
* Figure 5: It seems that all the experiments have not been run until convergence. I’m wondering if the difference in performance is going to be as significant when the model are trained fully.

In my opinion, the proposed method is neither sufficiently novel nor justified properly. On top of that, the experimental section is not particularly convincing. Therefore, I would not recommend the paper in its present form for acceptance.","The sentiment of the review is mixed but leans towards negative. The reviewer acknowledges some positive aspects, such as the clarity of writing and the importance of exploring better conditioning strategies. However, the cons outweigh the pros, with significant criticism regarding the novelty, theoretical justification, and experimental validation of the proposed method. Therefore, the sentiment score is -40. The politeness of the language is generally respectful and constructive, with the reviewer providing specific feedback and suggestions for improvement without being harsh or dismissive. Thus, the politeness score is 60.",-40,60
"This method deals with compressing tiny videos using an end-to-end learned approach. However, the paper has a significant number of limitations, which I will discuss below.

1. The method has only been trained on very small videos due to the fact that fully connected layers are used. I don't really understand why was this necessary, and it's not explained in the paper at all. Just this fact makes it completely infeasible for any ""real"" application.
2.  The evaluation was done on very limited domains. Of huge concern to me is the fact that very good results are presented on the sprites dataset. However, that dataset can be literally encoded by providing an index in a lookup table of sprites, so it's absolutely ludicrous to compare learned methods on that set to general video compression methods. The results look a lot less exciting when looking at the Kinetics 64x64 dataset. 
3. The evaluation (again) is problematic because the results refer to PSNR. PSNR for video is a very overloaded term. In fact, just the way to compute PSNR is not very clear for video. Video compression papers in general compute it in one of two ways: take the mean squared error over all the pixels in the video, then compute PSNR; or compute per frame PSNR then average. Additionally, none of the papers in this domain use RGB, because the human visual system is much more sensitive to detail preservation (the Y/luminance channel) than they are to chroma (color) changes. When attempting to present results for video, I would recommend to use PSNR-Y (and explain which type it is!), while also mentioning which ITU recommendation is used for defining the Y channel (there are multiple recommendations). 
4. It is not very clear how the global code is obtained. It is implied that all frames get processed in order to come up with f, but does this mean that they're processed via an LSTM model, or is there a single fully connected layer which takes as input all frames? In terms of modeling f, it sounds like the hyperprior model from Balle et al is employed, but again it's not clear to me how (is it modelling an entire video or a sequence?). I would really like to see a diagram for the network structure that computes f.

Ont he positives of the paper: I applaud the authors with respect to the fact that they made an effort to explain how the classical codecs were configured and being explicit about the chroma sampling that's employed. 

I think all the problems I mentioned above can be fixed, so I don't want to reject the paper per se. If possible, should the authors address my concerns (i.e., add more details), I think this could be an interesting ""toy"" method. ","The sentiment score is derived from the overall tone of the review, which is critical but not entirely dismissive. The reviewer acknowledges the effort made by the authors and suggests that the paper could be interesting if the issues are addressed. Therefore, the sentiment score is -40, indicating a predominantly negative but not entirely dismissive sentiment. The politeness score is based on the language used, which, while critical, remains professional and constructive. The reviewer uses phrases like 'I applaud the authors' and 'I don't want to reject the paper per se,' which indicate a polite tone. Therefore, the politeness score is 60.",-40,60
"The paper aims at studying the setting of perturbed rewards in a deep RL setting. Studying the effect of noise in the reward function is interesting. The paper is quite well-written. However the paper studies a rather simple setting, the limitations could be discussed more clearly and there are one or two elements unclear (see below).

The paper assumes first the interesting case where the generation of the perturbed reward is a function of S*R into the perturbed reward space. But then the confusion matrix does *not* take into account the state, which is justified by ""to let our presentation stay focused (...)"". I believe these elements should at least be clearly discussed. Indeed, in that setting, the theorems given seem to be variations of existing results and it is difficult to understand what is the message behind the theorems.

In addition, it is assumed that the confusion matrix C is known or estimated from data but it's not clear to me how this can be done in practice.  In equation 4, how do you have access to the predicted true rewards?

Additional comments:
- The discount factor can be 0 but can not, in general, be equal to 1. So the equation in paragraph 2.1 ""0 < γ ≤ 1"" is wrong.
- The paper mention that ""an underwhelming amount of reinforcement learning studies have focused on the settings with perturbed and noisy rewards"" but there are some works on the subject (e.g., https://arxiv.org/abs/1805.03359) and a discussion about the differences with the related work would be interesting.","The sentiment of the review is moderately positive, as the reviewer acknowledges that the paper is well-written and the topic is interesting, but also points out several areas for improvement. Therefore, the sentiment score is 40. The politeness of the language is quite high, as the reviewer uses polite phrases such as 'I believe,' 'could be discussed more clearly,' and 'it's not clear to me,' which indicate a respectful tone. Thus, the politeness score is 80.",40,80
"This work proposes to use duality gap and minimax loss as measures for monitoring the progress of training GANs. The authors first showed a relationship between duality gap(DG) and Jensen-Shannon divergence and non-negativeness on DG. Then, a comprehensive discussion was presented on how to estimate and efficiently compute DG. A series of experiments were designed on synthetic data and real-world image data to show 1) how duality gap is sensitive to capture non-convergence during training and 2) how minimax loss efficiently reflects the sample quality from generator. 


I was not very familiar with GANs, thus I'm not sure on the significance of paper and would like to see opinions from other reviews on this. For reviewing this paper, I also read the cited works such as Salimans (2016), Heusel (2017). Compared with them, the theoretical contribution of this work seems less significant. Also, I'm not quite impressed by the advantages of proposed metrics. However, this work is nicely written, the ideas are delivered clearly, experiments are nicely designed. I kind of enjoying reading this paper due to its clarity.


Other concerns:

There are two D_1 in Equation Mixed Nash equilibrium.
","The sentiment of the review appears to be mixed. The reviewer acknowledges the clarity and well-designed experiments of the paper, which is positive, but also expresses uncertainty about the significance of the theoretical contributions and the advantages of the proposed metrics. Therefore, the sentiment score is slightly positive. The politeness of the language is high, as the reviewer uses polite phrases such as 'nicely written' and 'I kind of enjoying reading this paper,' even when expressing doubts or concerns.",20,80
"The paper propose to incorporate an additional class for adversarial and out-distribution samples in CNNs. The paper propose to incorporate natural out-distribution images and interpolated images to the additional class, but the problem of selecting the out-distribution images is itself an important problem. The paper presents a very simple approaches for selecting the out-distribution images that relies on many hidden assumptions on the images source or the base classier, and the interpolation mechanism is also too simple and there is the implicit assumption of low complexity images. There exists more principled approaches for selecting out-distribution images that has not considered here like those based on uncertainty estimation or recently proposed direct out-distribution detectors.
In summary, the quality of the paper is poor and the originality of the work is low. The paper is easily readable.","The sentiment of the review is quite negative, as it criticizes the simplicity and hidden assumptions of the proposed approach, and states that the quality and originality of the paper are poor. This leads to a sentiment score of -80. The politeness of the language, however, is relatively neutral to slightly polite. The reviewer does not use harsh or rude language, but rather points out the issues in a straightforward manner. This results in a politeness score of 20.",-80,20
"The paper presents the use of an invertible transformtion layer in addition to the conventional variational autoencoder to map samples from decoder to image space, and shows it improves over both synthesis quality and diversity. 
The paper is well motivated, and the main motivation is nicely presented in Fig.1, and the main idea clearly shown in Fig.2 in an easy-to-understand manner. Existing works are properly discussed in the context before and after the main method. Convincing results are presented in the experimental section, with ablation tests in Tables 1-3, quantitative comparison in Table 4, and qualitative visual images in Figs.4-5. 

I incline to my current score after reading the response and other reviews.","The sentiment of the review is quite positive, as it praises the paper for being well-motivated, clearly presented, and providing convincing results. The reviewer appreciates the clarity of the figures and the thoroughness of the experimental section. Therefore, the sentiment score is high. The language used in the review is polite and respectful, with no negative or harsh words. The reviewer acknowledges the authors' efforts and provides constructive feedback in a courteous manner, leading to a high politeness score.",90,95
"This paper provides an algorithm that excludes the bad training data in the training process and obtain a more accurate model for both supervised and unsupervised learning problem. The paper gives the theoretical guarantee for mixed linear regression and Gaussian mixture model, and also conducts the experiments for deep image classification and deep generative models.

Major Concerns:
1, As said in related work, a soft version of this paper’s method has been proposed in the previous work, and the major seems to be that there is no initialization in the previous work which only leads to local convergence. Therefore, based on my understanding, the only innovation in this paper is that it gives the initialization process so that the algorithm can converge to the global optimal solution. But even this innovation only successes on some specific problems (Section 4-7). There are too few innovations.

2, In Section 4, for mixed linear regression, Theorem 1 and Theorem 2 together can not guarantee the global optimal solution for the algorithm. The author should demonstrate  “strict inequality” property in the 3rd line in Theorem 2, because it should correspond to the  “strict inequality” property in the 2nd line in Theorem 1.

3. Another angle to view the target problem in paper is from the outlier detection problem. The sparse learning formulation and theory can be conducted to solve this problem. Many existing theoretical analysis methods and optimization methods can be applied. For example, authors can refer to 

A Robust AUC Maximization Framework With Simultaneous Outlier Detection and Feature Selection for Positive-Unlabeled Classification, 2017

The comparison to these type of methods need to be included. 

Minor Concerns:
1, Theorem 2 does not give the probability, only mentioning “high probability”. How high? I do not find the probability in the proof as well. The same concern happens to Theorem 4. I think that

2, In Section 6 and 7, the author does not compare with other algorithms, which can not show the advantage of this algorithm.","The sentiment of the review is mixed but leans towards negative due to the major concerns raised about the lack of innovation and insufficient theoretical guarantees. The reviewer acknowledges the contributions but emphasizes significant shortcomings. Therefore, the sentiment score is -40. The politeness of the language is generally respectful and constructive, with the reviewer providing specific feedback and suggestions for improvement without using harsh or dismissive language. Thus, the politeness score is 60.",-40,60
"In this paper, the authors proposed 'defensive distinction' to address questions: Are adversarial examples distinguishable from natural examples? Are adversarial examples generated by different methods distinguishable from each other?

I have some major concerns about this submission.

1) The presentation of this work should further be improved. It contains many vague sentences. For example, ""Unfortunately, even state-of-the-art defense approaches such as adversarial training and defensive distillation still suffer from major limitations and can be circumvented."" I really hope I can see some justifications based on authors' approach for this argument. Also, the definition of 'AdvGen-Model' is not clear. Do you mean Adversarial attack generator knows the network model (i.e., white-box attack)? It is also not clear that how representative scenarios and cases in Table 1 affect the implementation of the proposed experiments (implementation details rather than results). 

2) The technical contribution of this paper is weak, and the experiments are not enough to support its main claim. MNIST is a simple dataset, please try larger and more complex datasets. The contribution of the current version is limited. 

 ","The sentiment of the review is generally negative, as indicated by phrases like 'I have some major concerns' and criticisms regarding the presentation and technical contribution of the paper. The politeness of the language is relatively neutral to slightly polite, as the reviewer uses phrases like 'I really hope' and 'please try,' which soften the critique.",-60,20
"The paper deals with a problem of expressiveness of a piecewise linear neural network, characterized by the number of linear regions of the function modeled. This is one of the widely accepted measure of expressiveness of a linear model. As such, it has been studied before. The main contributions of the paper are:
1) Different algorithms are proposed that allow to compute the bounds faster, leveraging probabilistic algorithms
2) Tighter bounds are obtained 
I find the results somewhat interesting. However, I do not think there is a lot of practical value in having faster algorithms for obtaining the bounds, as they are not used in practice anyway. I am also not convinced that the quest for tighter-and-tighter bounds in this approach is the right scientific direction. I find the paper to be an interesting contribution, but of a marginal value to the progress of the domain and for the improvement of our understanding of the models.","The sentiment of the review is mixed. While the reviewer acknowledges that the paper presents some interesting results and contributions, they also express doubts about the practical value and scientific direction of the work. This results in a sentiment score of around -20, indicating a slightly negative sentiment. The language used in the review is polite and respectful, even when expressing criticism, which results in a politeness score of 80.",-20,80
"Authors propose a supervised method for predicting adjacency matrix for a set of points. Loss function consists of 4 terms: intersection over union loss with respect to target adjacency, cross entropy loss, symmetry penalty and L2 regularization of parameters. Learning process consists of alternating node feature updates parametrized by GCN-like layers and updates of the adjacency matrix (different across layers).

My main concern is the heuristic nature of the method without any successful real data application. I do not see this work as impactful or of interest to ICLR community.

Directly regarding the content I have following comments and questions:

Word ""structure"" seems to be used in several meanings. For example ""We consider the problem of predicting the structure of a given set of points (which we assume are the nodes of a graph) and an initial structure (connections of the points)."" It only becomes somewhat clear later what is actually the learning problem studied in this paper.

""The learned convolutions"" - convolution is a particular mathematical operation. I believe authors should refer to the weights of their architecture instead.

Symmetry penalty of equation 14 seems unnecessary. When optimizing for symmetric matrix it should be recognized that corresponding symmetric entries are identical variables. Hence it is sufficient, and mathematically appropriate, to correct the gradient computed without symmetric consideration. Correction is simply sum of the gradient with itself transposed (without diagonal entries).

""We compare against traditional generative models for graphs: mixed-membership stochastic block models (MMSB) "" - could you please elaborate on how you use MMSB for graph generation. The use-case I am familiar with is community detection.
","The sentiment of the review is slightly negative, as the reviewer expresses a main concern about the heuristic nature of the method and its lack of successful real data application, which they believe makes the work not impactful or of interest to the ICLR community. The specific comments and questions further indicate areas where the reviewer finds the paper lacking or unclear. The politeness of the language is relatively high, as the reviewer uses polite phrases such as 'I believe authors should' and 'could you please elaborate,' and avoids any rude or harsh language.",-40,80
"This paper proposes a specific measure of difficulty for training examples called “easiness”. Easiness is based on training the model N times and counting the number of times an example is classified correctly. Based on this measure, they introduce “matching rate” as a measure of similarity of two architectures. Two architectures are suggested to be similar if the set of easy and hard examples is similar. The rest of the paper presents comparisons of architectures. Considering the problems below, I don’t see any reliable contribution in this paper.

- Why this specific definition of easiness? Can you compare to simply using “loss” as a measure for the difficulty of an example?
- e_t seems to be measuring the variance of training on a single example. If there is only one example that is always classified correctly, the denominator can be simplified to K. It doesn’t tell us how many training iterations it takes to fit that example.
- Why this specific formulation for “matching rate”? Why not a more common measure of similarity between sets such as intersection over union (IoU)? Can you suggest any references using a similar similarity score?
- Numbers in Table 1 do not seem particularly big to support the claim in section 4 that “...CNNs start learning from the *same* examples even if CNN architectures are different”. 0.20 is definitely bigger than random 0.1 for the matching rate but it still means only a 20% match.
- Random 0.1 is redundant in table 1.
- In section 4, define “contradicted patterns”.
- Are all images in Figure 1 for one model? How does it compare to visualizing examples according to their loss?
- The conclusion in section 5 says “... different CNNs start learning from similar patterns”. As mentioned above, “easiness” and consequently “matching rate” do not provide information about the progress of training and only final trained models. Regardless, this conclusion does not seem particularly unexpected or informative.
- Section 6 proposes to test a model on data with a different structure from data provided in training. This is a distribution mismatch and the model is not trained to handle.","The sentiment of the review is quite negative, as the reviewer explicitly states that they do not see any reliable contribution in the paper and lists several critical points. The sentiment score is -80 because the review is highly critical but not entirely dismissive. The politeness score is 10 because, while the reviewer is critical, they maintain a professional tone and do not use rude or disrespectful language.",-80,10
"The paper addresses the robustness of deep neural networks to adversarial example attacks. It uses a bilateral filtering as a preprocessing step to recover clean data from adversarial ones. It can also get combined with adversarial training and be trained end-to-end. The paper is well written, the background and introduction is clear. However I have comments about their implementation and experimental results:

1. They are claiming to have a very high distortion while their model can still perform well.  They have to make sure obfuscated gradients has not happened and they have implemented the back-propagation from attack to defense correctly. 

2. Also if they had consider black-box threats as well as white-box one, it would have been more informative of how their method actually performs. Specially, in order to check whether obfuscated gradients has happened, the black-box threats are better choice, which they have not tried.  

So considering the above, there might be an issue in what they are claiming, so they might reconsider their method. Maybe reviewing their codes by some experts in this topic can give a good evaluation. 

(I have to mention that I am not an expert in Adversarial networks)



","The sentiment of the review is generally positive, as the reviewer acknowledges that the paper is well-written and the background and introduction are clear. However, the reviewer also points out significant concerns regarding the implementation and experimental results, which slightly lowers the sentiment score. The politeness of the language is high, as the reviewer uses polite phrases such as 'I have comments,' 'they have to make sure,' and 'maybe reviewing their codes by some experts.' The reviewer also explicitly mentions that they are not an expert in Adversarial networks, which adds a humble tone to the review.",30,80
"Based on my understanding, this paper describes a novel approach for addressing the large batch training problem. The authors propose increasing the batch size based on reductions in the largest eigenvalue of the Hessian. This is combined with adversarial training using the fast gradient sign method to reduce the total number of iterations required for training and improve generalization performance. Unfortunately, although the numerical results seem quite promising, the algorithm and its explanation and details are not described clearly in the paper, which makes me lean towards rejection. I describe this more fully below:

1. Description of the Algorithm

The description of the algorithm in Section 3.1 is simply not clear, and lacks clear exposition motivating why the algorithm ought to work. To add to this confusion, there appear to be some inconsistencies between the (brief) description of the method and the description given in the Main Contributions and Limitations section in the Introduction. 

As an example, in Section 3.1, the approach for computing the eigenvalue of the Hessian is not described. Which eigenvalue is computed? How is this done? What is the batch size used in this computation? Is it computed over the full training set? The Limitations section briefly describes this (power iteration to tolerance <= 10^-2), but this should be elaborated on in Section 3.1. In fact, the limitations should not be discussed until a clear description of the algorithm is given.

The introduction makes this even more confusing by claiming the second order information is computed by “backpropagating the Hessian operator”. This seems to imply that the 3rd derivative information is computed for second-order information. Later in the Introduction, the authors claim to use Hessian matvecs to perform the power iteration. I believe that the authors mean that the Hessian-vector product is obtained by differentiating the product g’v (a scalar quantity). 

In addition, it was not described how the learning rate is changed in the algorithm. Later in the experiments, none of the additional hyperparameters in the procedure are given, such as the duration factor, kappa, the hyperparameters in the adversarial training, and more. This all ought to be included for completeness.

2. Questions about Details of the Algorithm

If it is indeed the case that the authors are using power iteration to compute the largest eigenvalue, why not use Lanczos method as it typically works better for symmetric matrices? In addition, if the intention was to compute the largest eigenvalue of the Hessian, one must be wary that the power iteration/Lanczos method computes the eigenvalue with largest magnitude (the absolute value of lambda), which may mean that it’s possible that the algorithm is utilizing negative curvature information rather than positive curvature information (particularly in the earlier epochs), which may contradict their intuition based on flat minima. This needs to be addressed.

Secondly, there is no explanation as to why increasing the batch size would lead to consistent decrease in the eigenvalues of the Hessian. This is certainly not true for all optimization problems. Even if the flat minima/sharp minima hypothesis is assumed, is it possible for the iterates after increasing the batch size to still tend towards sharper minimizers after being in a flat region? This intuition and explanation needs to be expanded on (and argued for) in order for the algorithm to make any conceptual sense.

Lastly, why is the duration factor needed to increase the batch size if the eigenvalue condition fails? if the duration factor is removed, how does the batch size evolve? Is it necessary? How is the duration factor tuned?

3. Inconsequential Theoretical Results

The authors also prove a theorem bounding the expected optimality gap with adaptive batch sizes. On closer look, this is a simple adaptation of the result by Bottou, Curtis, and Nocedal [2] and does not utilize any of the algorithmic mechanisms described in the paper. Hence, the theoretical result is not novel, does not provide any additional insight on the algorithm, and could be applied to any adaptive/changing batch size SG algorithm. In my opinion, this ought to be removed. (Assumption 2 is also mentioned in the main paper, but is only described in the Appendix.)

4. Additional Considerations

The paper is missing much work done by Nocedal’s group on increasing batch sizes (some of which utilize the L-BFGS approximation to the Hessian); see [1, 3].

Other relevant work by Sagun, Bengio, and others on large batch training, flat minima, and the Hessian in deep learning ought to be included as well; see [4-7]. 

Lastly, the algorithm demonstrates some significant improvements on the number of iterations. However, efficiency with respect to epochs is not discussed. It may make sense to plot test loss/error against epochs and batch size against iterations for clarity.

Typos/Grammatical Errors:
- Page 2: Should not state “(We refer to this method as ABS)”, easier to include by including (ABS) after Adaptive Batch Size in the beginning of the bullet point.
- Page 6: Section 4: “information” not “informatino”
- Page 6: Section 4: “the” not “teh”
- Page 7: Section 4.1: “confirms” not “confirming”
- Page 7: Section 4.1: no “a” in “a very consistent performance”

Summary:

Overall, although the paper presents some promising numerical results, it lacks a detailed description and explanation of the algorithm to be worthy of publication. It leaves many aspects of the algorithm open to the reader’s interpretation, and I do not believe I could reproduce the results with the information provided. The manuscript needs significant changes to the detail, structure, and writing before it can be considered for publication.

References:
[1] Bollapragada, Raghu, et al. ""A progressive batching L-BFGS method for machine learning."" arXiv preprint arXiv:1802.05374(2018).
[2] Bottou, Léon, Frank E. Curtis, and Jorge Nocedal. ""Optimization methods for large-scale machine learning."" SIAM Review 60.2 (2018): 223-311.
[3] Byrd, Richard H., et al. ""Sample size selection in optimization methods for machine learning."" Mathematical programming134.1 (2012): 127-155.
[4] Chaudhari, Pratik, et al. ""Entropy-sgd: Biasing gradient descent into wide valleys."" arXiv preprint arXiv:1611.01838(2016).
[5] Jastrzębski, Stanisław, et al. ""DNN's Sharpest Directions Along the SGD Trajectory."" arXiv preprint arXiv:1807.05031(2018).
[6] Sagun, Levent, et al. ""Empirical Analysis of the Hessian of Over-Parametrized Neural Networks."" arXiv preprint arXiv:1706.04454 (2017).
[7] Zhu, Zhanxing, et al. ""The Regularization Effects of Anisotropic Noise in Stochastic Gradient Descent."" arXiv preprint arXiv:1803.00195 (2018).","The sentiment of the review is moderately negative, as the reviewer acknowledges the promising numerical results but expresses significant concerns about the clarity and completeness of the algorithm's description, leading to a recommendation for rejection. The sentiment score is -60. The politeness of the language is quite high, as the reviewer provides detailed feedback and constructive criticism without using harsh or disrespectful language. The politeness score is 80.",-60,80
"Summary:

This paper proposes layer wise training of neural networks using classification auxiliary tasks for training each layer. Experiments are presented on CIFAR10 and Imagenet. Accuracies close to end to end training are obtained. 

The layer wise training is repeated for J steps, the auxiliary tasks are trained on top of the shallow one layer (of width M ) with a network  of depth k and width tilde{M}. Layerwise training is done using sgd with momentum, and the learning rate is decayed through epochs. 

Note that the layer wise training is done with large width M than typical end to end networks in use. 

The authors argue and test the hypothesis that auxiliary tasks  encourage the linear separability of CNN features. 

To reduce the size of the learned network the author propose a layer wise compression using the filter removal technique of Molchanov et al .

Reproducibility:

This empirical  work has been investigated for a while with mild success, the authors should make their code available to the community to confirm and reproduce  their findings.  I encourage the authors to make their code available during the review/discussion period. 


Significance of the work:

From reading the paper it is not clear what is the main ingredient that makes this layer wise training  successful, negative results would help in understanding what is important for the accuracy. 

Some more ablation studies and negative results will be insightful and here are few suggestions in that direction:

- Authors claim that they used invertible downsampling as max or average pooling  lead to information loss. Does the layer wise training give worst results with average or max pooling? If so please report those numbers to know what is the implications of this choice of pooling.  

- On the width of the networks, seems it is key for the success of the approach.  What if  you train wider networks with J that is small? (  J=3 for instance but much  wider networks, instead of J=8 now for imagenet.)

- To answer the same question above one needs also to see what are the accuracies for J=8 with thiner networks (smaller M )?

- Would the accuracy  with the layer wise training reach a  plateau if one uses an architecture with J higher than 8? 

- Transferability of the learned features: end to end features are know to be transferable. It would be good to see if this still holds using the network layer wise trained on imagenet for CIFAR10 or other datasets. 

Other Questions:
- Section 3.2 is vague. In Proposition 3.1 and  Proposition 3.2 can you add some text to explain what are the implicitions of the claims? “Thus our training permits to extend some results on shallow CNN to deeper CNNs …” which shallow results ?

- “For k>1 batch normalization was useful “ is this only on the auxiliary problems networks  or you used also batch norm for the layer ?

- The ensemble used is Z=\sum_{j=1}^J 2^j z_j , this uses the network of J layers ,  also the O(J) auxiliary networks  of depth k. Please report the number of parameters for all models (single and ensembles) in Table 1 and Table 2. 

- In the conclusion: “The framework can be extendable to the parallel training …” how would this possible since one needs the output of the first training to do the training of the next layer. can you elaborate on what is meant here?

Minor:
page 2 bottom have competitorsand -> have competitors and 
the non linearity rho in equation 1 and throughout the paper put a bracket for its argument \rho(x) not \rho x
Page 6 , Imagenet paragraph : W —> We
section 4.2 we define linear separability etc… a space is missing before Further 
section 4.3 we report our results .. (Imagenet) a space is missing after ImageNet)

Overall Assessment: 
This is a good paper, making the code available and adding more ablation studies and explanations of width versus depth and the choice of pooling will make the contributions easier to understand. ","The sentiment of the review is generally positive, as indicated by phrases like 'This is a good paper' and 'I encourage the authors.' The reviewer acknowledges the significance of the work and provides constructive feedback aimed at improving the paper. The politeness of the language is high, with the reviewer using polite phrases such as 'I encourage,' 'please,' and 'would be good to see.' The reviewer also provides detailed suggestions and minor corrections in a respectful manner.",80,90
"The paper proposes a hybrid model-free and model-based RL agent for the task of navigation. Reaching the target is decomposed into a set of sub-goals, and the plan is updated as the agent explores the environment. The method has been tested in the House3D environment for the task of RoomNav, where the goal is to navigate towards a certain room. 

The idea of integrating RL agents with semantic knowledge is interesting. However, the paper has several major issues that should be addressed in the rebuttal:

(1) The experiment results in Figure 3 and Figure 4 are based on groundtruth room information. The only experiment that is fully automatic is the one in Figure 5. However, there is no difference between the proposed method and the baselines in that case. So the proposed method is not effective without groundtruth information.

(2) The only evaluation metric that is used is ""Success Rate"". That metric is not sufficient for evaluation of navigation agents since it does not include episode length information. All of the results should be based on the protocol mentioned in ""On Evaluation of Embodied Navigation Agents"", arXiv 2018. 

(3) There is no termination action according to Appendix B. So the agent does not know if it is at the target or not. It seems the agent will stop if it issues ""stay still"" three times. That is different from termination action. Also, it is confusing what 450 pixels means for a scene classifier that works on the image.

(4) The paper is written in a convoluted way:
   (a) It is not clear if the semantic model is trained along with the RL model end-to-end or not.
   (b) Regarding multi-target sub-policies, is there a separate policy for each pair of intermediate targets? 
   (c) Regarding inference and planning on M, what is \tau exactly? How is the length of the plan determined? 
   (d) Why is the model updated only after a fixed number of steps? That increases the episode length. 

(5) The number of T_i's is manually set to 8. That causes serious generalization issues. How do we know how many T_i's exist in a new environment?


Minor comments:
- The paper mentions ""An example of such environments is House3D which contains 45k real-world 3D scenes"". House3D includes only synthetic scenes. They should not be called real-world scenes.
- How is the reward shaping done?

****
Final comments after reading the response and the reviews:

Regarding the fairness of the review, success rate is not sufficient to evaluate navigation agents. A random agent can achieve 100% success if it is given enough time. So it is totally fair to ask for a metric (such as SPL) that is a function of both success rate and episode length. 

I am going to increase the rating to 5 since some of my concerns have been addressed. There are still a number of issues:

- The authors did not run the experiments with the termination action. I disagree that this is orthogonal to the focus of the paper. This is not just an additional action. It indicates whether the agent has learned anything or it is just a combination of better obstacle avoidance and luck. The SPL numbers are so low (maximum SPL is 6.19%) so adding the termination action will probably make the method similar to random.

- There is a huge gap between success rate and SPL numbers. For instance, success rate is 66.4, while SPL is 5.84 (note that for some reason the SPL numbers are multiplied by 10 in the table). I doubt that the agent has learned anything meaningful in comparison to the baseline. I understand that the task is hard, but this gap is so huge.

- A separate policy is trained for each sub-target. This doesn't scale. There should be one policy for all targets. 

","The sentiment score is determined by the overall tone and content of the review. The reviewer acknowledges the interesting idea of integrating RL agents with semantic knowledge but points out several major issues that need to be addressed. The sentiment is mixed, leaning towards negative due to the significant criticisms, but not entirely dismissive. Therefore, the sentiment score is -30. The politeness score is assessed based on the language used. The reviewer uses polite language, such as 'should be addressed,' 'it is confusing,' and 'I disagree,' without resorting to rudeness or personal attacks. Therefore, the politeness score is 60.",-30,60
"In their abstract, the authors claim to provide state-of-the-art perplexity on Penn Treebank, which is not true. As the authors state, their notion of ""state-of-the-art"" excludes exactly that earlier work, which does provide state-of-the-art perplexity on Penn Treebank (Yang et al. 2017), as stated in Sec. 4.1. The question is, why one would exlude the mixture-of-softmax approach here? This is clearly misleading.

The authors introduce the idea of past decoding for the purpose of regularization. It remains somewhat unclear, why this bigram-centered regularization would strongly contribute for prediction in general.

The results obtained show moderate improvements of approx. 1 point in perplexity on top of their best current result on Penn Treebank. Considering the small size of the corpus for the evaluation of a regularization method, the results even seem optimistic - it remains unclear, if this approach would readily scale to larger datasets. The mode of language modeling evaluation presented here, without considering an actual language or speech processing task, provides limited insight w.r.t. its utility in actual applications. Moreover, the very limited size of the language modeling tasks chosen here is highly advantageous for smoothing/regularization approaches. It remains totally unclear, how the presented approaches would perform on more realistically sized tasks and within actual applications.
","The sentiment of the review is quite negative, as it points out several flaws and misleading claims in the authors' work. The reviewer questions the validity of the authors' claims about state-of-the-art performance and expresses skepticism about the generalizability and practical utility of the proposed method. The politeness of the language is neutral; while the reviewer is critical, they do not use rude or overly harsh language. They focus on the issues without resorting to personal attacks or derogatory remarks.",-70,0
"
Summary:
The paper proposes an unsupervised method to learn a vector representation for short genomic sequences, so-called kmers (like n-grams in natural language processing). The method learns a representation that will result in a good predictor of kmer counts from the kmer sequence itself. The idea is that neighbouring kmers (from the same gene) would have similar counts (same gene expression), and hence would be embedded near to each other. The paper shows some small empirical experiments for 3 tasks: showing similar genes are close, able to distinguish different genes, and able to detect genomic structural variation.

This is an interesting idea, and would have large impact if done well. However the current approach has multiple weaknesses which leave the proposal less strong that it could be. The paper is written clearly, and while the idea is motivated from word2vec and derivatives, the application to kmers is original.


Overall comments:
- There are two issues conflated in the word scalability:
  1. computational scalability, where the authors need to run the method on a more realistic dataset and show that the LSTM converges.
  2. statistical scalability, which I will expand in the next point.
- The design of finding an embedding that will identify the count given a kmer has several weaknesses, which the paper did not address:
  1. Two genes could have similar expression, hence similar kmer counts, but different kmers.
  2. A kmer can appear in multiple genes, and the total count is the sum of all of them.
  3. Copy number variation (since the paper is interested in cancer) would affect counts
  4. Two kmers with only one or two differences could be due to SNPs. Should they be near or far?
  5. Should we learn a representation for each individual, or a representation for the population? Depending on how the sample id (and hence vector v_i) is used, one can get different effects.
- It seems wasteful that there is no representation learning for each individual, but instead it is just a fixed (arbitrary?) vector in a look up table.
- The choice of embedding dimension 2 seems to be driven by the fact that the authors wanted to visualize. This is tied up with a weakness that the paper does not measure the quality of the embedding, e.g. using reconstruction error. A good approach is to show that the resulting embedding is useful for some other prediction task (which is usually the reason we want to find an embedding). Reporting mean squared error for Figure 4C would also be helpful.

Minor typos/issues:
- page 3, Section 3: does j range over k-mers in x_{ij}? You also use it r_j in the first sentence.
- page 3, Section 3: read length = 100. kmer length = 24. This should be put in the experimental section. Furthermore due to reverse complements, it would be better to have an odd number for k, e.g. 23.
- page 3,4: using angle brackets to mean a pair is uncommon. Suggest tuple (u,v).
- page 4: The notation U in the description of the LSTM can be confused with two other things:
U is the kmer embedding space, and u_{ij} is the embedding vector.
- page 6: In the text you refer to Figure 3A, I assume you mean Figure 3.
- page 6, Figure 3: Unclear what the three columns are. I assume similar to Figure 4, they are three individuals.
- page 6, task 2: It is unclear how the reader can see that the information recovered by kmer2vec is the same information recovered by standard RNA-Seq analysis.
- page 8, first word: Not sure how Figure 4B shows what the sentence is trying to say.
","The sentiment of the review is mixed. The reviewer acknowledges the interesting idea and potential large impact of the paper, which is positive. However, they also highlight multiple weaknesses and areas for improvement, which brings the overall sentiment down. Therefore, the sentiment score is 20. The politeness of the language is generally high. The reviewer uses polite language, such as 'suggest' and 'would be helpful,' and avoids harsh criticism, focusing on constructive feedback. Therefore, the politeness score is 80.",20,80
"This work tackles the problems encountered by bounded memory storage mechanisms when faced with abundant data, of which much may be irrelevant or redundant. Such a problem is faced in lifelong learning settings, where a limitless data stream must somehow be encoded and stored so as to be useful at later points in time. 

The researchers propose a solution based on “learning what to remember”. That is, rather than encode every observation (which can quickly become problematic), the model learns to replace less important memories. The importance of a memory is determined by its correlation with future reward; a “memory retention policy” is learned via reinforcement learning, wherein the model learns to retain or discard memories based on these actions’ (i.e., retentions) impact on future reward. Experiments to show the effectiveness of this mechanism include gridworld IMaze and Random Mazes, bAbI question answering (task 2), and Trivia QA. 

Altogether the work does well to clearly describe an interesting approach to an important problem. The model is motivated and explained well, and there were no issues with understanding its inner workings. 

Regarding the work’s novelty, there is a precedent for using RL-based write schemes (DNTM from Gulcehre et al, 2016), which the authors point out. I am not entirely convinced that the proposed writing scheme is a substantial addition over this past work, but I am not overly concerned about this since proper due credit is assigned in the paper. Perhaps a bit more discussion about the advantages of the proposed writing scheme could go a long way, since as it stands now, the paper simply claims that this past work “only considers the pairwise relationships between the current data instance and each individual memory”, and I’m not sure how much substance actually underlies this difference.

Unfortunately I think there is a fundamental problem with the work. The model is a proposed solution for problems with vast amounts of streaming data; problems that, presumably, current memory models would struggle with. However, the tasks in the paper do not fall in this domain. Instead, the authors chose to artificially cripple the size of their memory (using, for example, just a handful of memory “slots”) and demonstrate its performance on tasks that are otherwise completely within the realm of being solved by conventional memory models. This is fine as a jumping off point for the research, but for the model to be taken seriously as a valid solution to problems involving such a scale of data that current models cannot even cope, then it needs to show its worth on problems involving such a scale of data that current models cannot cope. 

Demonstrating success here is important for a few reasons. First, such high-data scenarios may involve situations where many, many memories need to be encoded and considered for the future, since they are all useful or necessary for future performance. The experiments do not show whether the model can scale to, say, 100 or 1000 memories, which is within the realm of being “reasonable” for current memory architectures. Second, high-data scenarios may involve an abundant amount of distracting, irrelevant data. This places particularly tough demands on the RL-based writing mechanism, which will undoubtedly face problems with temporal credit assignment if: (a) the time between encoding and retrieval is long, and (b) there is high reward noise in the intermediate time. Thus, the authors should stress-test the components of their model, since these stresses will undoubtedly exist in the problems that the model is proposed to solve.

Some other minor considerations include the following. (1) The use of a single bAbI task is questionable. Why not run the model on the full suite? (2) How do conventional memory models perform on the tasks? Why are the baselines only variants of the proposed model? 

To conclude and summarize, as a proposed solution to scenarios with streams of abundant data -- which the authors claim is a domain that current memory models may struggle -- the proposed model should tackle problems that: 1) have characteristics more reminiscent of these scenarios, and 2) are problems on which current memory models struggle, for the reasons claimed in the paper. In particular, it would be valuable to see model performance on tasks wherein very long stretches of time need to be considered. This is important because it can address questions with memory scaling (how does the model cope with more than a handful of memories?), and issues that would crop up in a reinforcement learning-based approach to memory retention over long time intervals (namely, long-term temporal credit assignment). 
","The sentiment of the review is generally positive, as the reviewer acknowledges the clear description, interesting approach, and proper credit given to previous work. However, the sentiment is tempered by significant concerns about the applicability and scalability of the proposed model, leading to a more balanced sentiment score. The politeness of the language is high, as the reviewer provides constructive criticism and suggestions for improvement without using harsh or dismissive language.",20,80
"Summary of the paper:
This paper studies using a three-layer convolutional neural network for the XOR detection problem. The first layer consists of 2k 2 dimensional filters, the second layer is ReLU + max pooling and the third layer are k 1s and k (-1)s. This paper assumes the input data is generated from {-1,+1}^{2d} and a margin loss is used for training. 
The main result is Theorem 4.1, which shows to achieve the same generalization error, defined as the difference between training and test error, the over-parameterized neural network needs significantly fewer samples than the non-over-parameterized one. 
Theorem 5.2 and 5.3 further shows randomly initialized gradient descent can find a global minimum (I assume is 0?) for both small and large networks. 


Major Comments:
1.  While this paper demonstrates some advantages of using over-parameterized neural networks, I have several concerns.
This is a very toy example, XORD problem with boolean cube input and non-overlapping filters. Furthermore, the entire analysis is highly tailored to this toy problem and it is very hard to see how it can be generalized to more practical settings like real-valued input. 
2. The statement of Theorem 4.1 is not clear. The probabilities p_+ and p_- are induced by the distribution D. However, the statement is given p_+ and p_-, there exists one D satisfies certain properties. 
3. In Theorem 5.1 and 5.2, the success probability decreases as the number of samples (m) increases. 


Minor Comments:
1. The statement of Theorem 4.1 itself does not show the advantage of over-parameterization because optimization is not discussed. I suggest also adding discussion on the optimization to Sec.4 as well.
2. Page 5, last paragraph: (p1p-1)^m -> (p_+p_-)^m.
3. There are many typos in the references, e.g. cnn -> CNN, relu -> ReLU, xor -> XOR.

","The sentiment of the review is mixed but leans towards the negative side. The reviewer acknowledges some advantages of the paper but expresses significant concerns about the generalizability and clarity of the results. Therefore, the sentiment score is -30. The language used is polite and constructive, offering specific suggestions for improvement without being rude or dismissive. Thus, the politeness score is 80.",-30,80
"The authors hypothesize that, under appropriate conditions, neural networks without specific architectural biases trained by model-free reinforcement learning algorithms are capable of learning procedures that are analogous to planning. This is certainly an important area of research in reinforcement learning.

Unfortunately, the approach employed to demonstrate this hypothesis seems flawed, which is why this submission should be rejected in its present form.

The authors suggest that the presence of planning should be accompanied by three observable characteristics: generalization of desired behavior to radically different situations, learning of desired behavior from small amounts of data, and ability to benefit from additional ""thinking"" time. Instead of trying to identify how an environmental model is represented by a network and how it is used for planning, the authors focus on checking for the aforementioned characteristics.

Even after conceding their strong claim despite weak argumentation provided by the authors, there are fundamental experimental issues that make the conclusions of this study unwarranted. Regarding the first two characteristics, the concepts of ""radically different situations"" or ""small amounts of data"" are extremely vague. Basically the authors assume that their problems are difficult enough to require planning. Having solved these problems with their proposed architecture, they conclude that planning must have occurred. Regarding the use of additional ""thinking time,"" the authors claim that the improvement in performance caused by providing additional micro-steps to a recurrent neural network is clear evidence that something analogous to planning is happening, which is obviously not the case.

While it would not be surprising if there was indeed something analogous to planning happening inside the networks under consideration, this paper presents no stronger evidence for this claim than most other deep reinforcement learning papers that tackle complex environments.

Perhaps the most important contribution of this submission is the architecture based on ConvLSTMs proposed by the authors, which apparently surpasses many alternatives, including some biased towards planning. However, surpassing planning models is not strong evidence of planning. When stripped of unwarranted claims made by the authors regarding implicit planning, the proposed architecture does not seem sufficiently novel to warrant acceptance.

The authors should be commended for what was certainly very demanding experimental work, even though it does not support their core claims. Their second most important contribution is the experimental comparison between several recent architectures in a diverse selection of environments. 

The writing is clear and accessible, except possibly for the architectural details described in Section 2.1.2, which do not seem very important. There are also several typos in Appendix D.2.

Regarding related work, the authors mention that Pang and Werbos [1] ""advanced the approach."" But they do not explain how they advanced this approach. In fact, we could not find much about this in the 1998 paper. Also, to our knowledge, ""additional thinking time"" was first proposed in the context of reinforcement learning and planning with two interacting RNNs by Schmidhuber [2, Section: ""more network ticks than environmental ticks""].

[1] Xiaozhong Pang & Paul J. Werbos (1998): Neural Network Design for J Function Approximation in Dynamic Programming
[2] J.  Schmidhuber. Making the world differentiable: On using fully recurrent self-supervised neural networks for dynamic reinforcement learning and planning in non-stationary environments. TR FKI-126-90, TU Munich, November 1990.

Perhaps a strongly revised version the paper might become more acceptable if the authors addressed the issues above and especially toned down their claims about having experimentally identified the emergence of planning. Instead they should be extremely careful here, perhaps present this as ""intriguing results,"" and address all possible counter arguments.
","The sentiment of the review is mixed but leans towards the negative side. The reviewer acknowledges the importance of the research area and commends the authors for their experimental work and clear writing. However, they also point out significant flaws in the approach and argue that the paper does not provide strong evidence for its claims. Therefore, the sentiment score is -40. The politeness of the language is generally high; the reviewer uses phrases like 'the authors should be commended' and 'perhaps a strongly revised version might become more acceptable,' which indicates a polite tone. Therefore, the politeness score is 80.",-40,80
"# overview
This paper leverages a consensus based approach for computing and communicating approximate gradient averages to each node running a decentralized version of stochastic gradient descent.

Though the PushSum idea isn't new, its application to distributed SGD and corresponding convergence analysis represents a valuable contribution, and the experimental results indicate a potentially large speedup (in highly variable or latent networks) without substantially sacrificing model accuracy.

The paper itself is reasonably comprehensive but does miss out on comparisons with more recent but equally promising approaches, namely AD-PSGD. 

# pros
* Empirically shown to be significantly faster than SGD, D-PSGD in high-latency, communication bound configurations which is a fairly common real-world setup. There is an accuracy tradeoff at work here, but performance doesn't seem to suffer too much as the node count scales.
* introduces and proves theoretical guarantees for SGP approximate distributed average convergence for smooth, non-convex case, including upper bounded convergence rates.

# cons
* biggest criticism is that AD-PSGD from Lian et al 2018 is not included in experimental comparisons even though the paper is referenced. Authors state that asynchronous methods typically generalize worse than their synchonous counterparts but that isn't what Lian et al found in their comparison with D-PSGD (see table 2 and 3 from their paper). This comparison would be particularly interesting as AD-PSGD also performs well in the high network latency regime that SGP is touted for.
* would've liked to see comparison on other tasks beyond just image classification on ResNet.

# other comments
* Didn't see mention of specific iteration count value(s) K used in experiments or hyperparameters A.3. Since it bounds the convergence rate, this would be important to include.
* Found a few small typos:
  * pg. 5: Relatively -> Relative
  * pg. 7, fig. 2: part -> par
  * pg. 8, sec. 5.3. par. 2: achieves -> achieved
  * pg. 8, sec. 5.3, par. 2: ""neighbors also to increases"" (drop ""to"")
  * pg. 12, sec. A.2: ""send-buffer to filled"" -> ""send-buffer to be filled""","The sentiment of the review is generally positive, as the reviewer acknowledges the valuable contribution of the paper and its potential for large speedups in certain network conditions. However, there are some criticisms, particularly the lack of comparison with AD-PSGD and the limited scope of tasks evaluated. Therefore, the sentiment score is 60. The politeness of the language is very high, as the reviewer provides constructive feedback and suggestions in a respectful and professional manner. The reviewer also points out minor typos without being harsh. Thus, the politeness score is 90.",60,90
"This paper is clearly written and explains everything in a good detail. I have a few questions about the design of the algorithm and experiments that I will explain next. Most importantly, I am confused why the communication actions are modeled with continuous actions. Also, the communicating agent idea is incorporated in MADDPG paper, and the contribution of the proposed network is unclear to me.  Right now, I am leaning toward weak reject now but I might update my evaluations after seeing the authors' feedback.

1) First, your construction of communication medium simply seems to be learning a method for graph sparsification and this deserves some explanation.  Also, I think that using the graph terminology for describing the communication medium structure will significantly improve the clarity of the paper. For example, I assume that by saying that m^t = ...=m^{t+C-1} you mean that you simply fix the communication graph structure for C steps, not the communicating observations. Based on your notations, it is a little bit confusing -- in your notations $m^t$ is the set of observation that flows through the graph which should be different than $m^{t+1}$.

2) Even MADDPG is very challenging to train! Now, this paper utilizes two MADDPG, and that is something that concerns me a lot. I don't think that replicating the results of this paper is possible by other people. How much was the cost of the hyper-parameters search? 

3) Why the decision of where to send the observations is modeled with a continuous control action? It can be simply modeled with discrete action in a more efficient way, right? What I mean is that $c_i$ can be a binary which tells whether send an observation or not. Am I missing anything?

4) In section 2, you argue that in the original MADDPG paper, there is no inter-agent communication. As far as I remember, they have some experiments for cooperative communication or covert communication in which the communication is allowed between the agents. I would like to know more about this statement; maybe I am missing something. Why you are not designing the communication network which is a differentiable medium such as Foerster 2015? Isn't that efficient?

5) In alternating case, I don't see (intuitively) why the communication should help to improve upon MADDPG. My intuition is that each agent will be the gifted one 1/3 on average. This means that the agents cannot perceive who gives the correct information and the policy should converge to a point where the communication does not give any new information.

6) I would like to see what will happen with C=1? I think this hyper-parameter deserves some analysis to see how it affects the performance of the proposed method.

7) In section 5, you say that in original DDPG, there is no real need for inter-agent communication"". This is a little bit strict statement, I guess. For example in the case with full observability, the agents can send messages which conveys their intention and help each other.



Minor:
* I would suggest using partially observability terminology instead of saying noisy observation because I think it includes a more general class of the problems to solve.
* ""that a coupled through a communication medium"" -> ""that are coupled through a communication medium""
* In section 4.2, it is unclear to me what is the exploration strategy. Please explain more.
* section 5.2: Using the term lower bound is not accurate. Try changing it to something else or use with quotations: ""lower bound""
* What will happen you choose the top-k rule for sending the information? For example, does top-2 (two-to-one) rule improve the results? The experiments might be added in future.
-----------------------------------
post rebuttal: the authors have responded to my main questions, and I would like to increase my score, but I cannot agree with them on possile future extensions of this work, e.g. in learning from pixels.","The sentiment of the review is mixed but leans towards the negative side initially, as the reviewer expresses confusion and concerns about the design and contributions of the paper. However, the sentiment improves slightly after the authors' rebuttal. Therefore, the sentiment score is -20. The politeness of the language is generally respectful and constructive, with the reviewer providing detailed feedback and suggestions without using harsh or rude language. Thus, the politeness score is 80.",-20,80
"If the stated revisions are incorporated into the paper, it will be a substantially stronger version. I'm leaning towards accepting the revised version -- all my concerns are addressed by the authors' comments.
---
The paper uses a Seq2Seq network to re-rank candidate items in an information retrieval task so as to account for inter-item dependencies in a weakly supervised manner. The gain from using such a re-ranker is demonstrated using synthetic experiments as well as a real-world experiment on live traffic to a recommender system.

Paragraph below eqn2: for *any* fixed permutation. Figure1 and notation indicates that, at each step of decoding, the selected input x_j is fed to the decoder. The text suggests the embedding of the input e_j is fed to the decoder (which is consistent with ""go"" being a d-dimensional vector, rather than the dimensionality of x).

Single step decoder with linear cost: Is there a missing footnote? If not, why call it p^1? Simpler notation to just call it p.
Eqn7: Role of baseline. In REINFORCE, b(x) is typically action-independent (e.g. approximating the state value function induced by the current policy). L_pi(theta) is action dependent (depends on the permutation sampled from P_theta). So, I'm unclear about the correctness of Eqn7 (does it yield an unbiased policy gradient?)

Eqn5: Expected some discussion about the mismatch between training loss (per-step cross entropy) vs. testing loss (e.g. NDCG@k). Does a suitable choice of w_j allow us to recover standard listwise metrics (that capture interactions, e.g. ERR)?

Expt implementation: Why was REINFORCE optimizing NDCG@10 not regularized?
Expt cascade click model: Expected an even simpler experiment to begin with; [Is the Seq2Slate model expressive enough to capture listwise metrics?] Since the relevances are available, we can check if Seq2Slate trained to the relevance labels yields NDCG performance comparable to LambdaMART, and whether it can optimize metrics like ERR.

Table1: On the test set, is NDCG&MAP being computed w.r.t the ground truth relevances? So, is the claim that Seq2Slate is more robust when clicks are noisy in a one-sided way (i.e. relevant items may not receive clicks)? Not clear how much of this benefit comes from a more expressive model to predict relevances (see suggested expt above) vs. Seq2Slate from clicks. NDCG & MAP definitely don't account for inter-item dependencies, so unclear what is being tested in this experiment.

For diverse-clicks, eta=0 while for similar-clicks, eta>0 (in a dataset dependent way). Why? Can expt numbers for the 3 choices of eta be added to the appendix? [Seems like cherry-picking otherwise]

Can Ai et al, 2018 be benchmarked on the current expt setup? Is it identical to the single-step decoder proposed in the paper?

Comment: Would be more accurate to call seq2slate a re-ranker throughout the text (in the abstract and intro, the claim is seq2slate is a ranker).

Expected to see training time and inference time numbers. Since Seq2Slate does extra computation on top of, e.g. LambdaMART, it is useful to know how scalable it can be during training, and when the extra perf is worth the O(n^2) or O(n) [for single-step decoding] during inference.

General comments:
Clarity: The paper is well written and easy to follow. There are a few notation choices that can be improved/clarified.
Originality: This work seems closely related to Ai et al, SIGIR 2018. Going from a single-shot decoding to sequential decoding is an incremental step; the real-world experiment seemed the most novel and compelling contribution (however, it is unclear how one can reproduce it).
Significance: The paper addresses a significant real-world problem. Many high-impact applications of ranking rely on being able to model inter-dependencies well.
Quality: The paper has interesting contributions, but can be substantially stronger (see some of the specific comments above). For instance, A careful study of the computational vs. performance trade-off, fine-grained comparison of different decoding architectures, better understanding of which architectural choices allow us to model any arbitrary ranking metric more effectively vs. which ones are more robust to click noise vs. which ones capture inter-item dependencies.
","The review starts with a positive sentiment, indicating that the reviewer is leaning towards accepting the revised version of the paper, provided the stated revisions are incorporated. This suggests a sentiment score of 80. The language used throughout the review is constructive and polite, offering specific recommendations and clarifications without being dismissive or rude. Therefore, the politeness score is 90.",80,90
"The problem addressed in this paper is worth the attention of the community. Not so much for it being of strong interest to the majority of ICLR attendees, but due to the fact that it deals with data of origin (finance) and properties (high-order Markov chain dependencies) that have never been considered in the past.

However, apart from this merit, the paper as is stands now suffers from major prohibitive shortcomings. Specifically, the authors have failed to provide a detailed account of the novel network architecture introduced in this paper. Their description is too vague, and misses crucial details. For instance, they use a convolutional structure (layer?) at some point. How is this configured? Why do they need a convolutional layer, since they present it with a vector output from a preceding dense layer? What about the CDA network (both its configuration and its training procedure)? These are crucial aspects that the authors have failed to describe at all.

In the same vein, technical details concerning the selection of the training algorithm hyper-parameters are also missing from the experimental section. Although not as glaring an omission as the previously described ones, these are also crucial for the replicability of the presented research results. 

Finally, the authors have failed to provide comparisons to alternative baselines. For instance, why not train a simple LSTM and use to generate new samples. Why not use a recurrent variational autoencoder? Eventually, since the time-series we are dealing with are governed by a high-order Markov chain, we not fit and sample from a high-order hidden Markov model? These are crucial questions that must be adequately addressed by the authors. ","The sentiment of the review is mixed but leans towards negative. The reviewer acknowledges the importance of the problem addressed by the paper, which gives a slight positive sentiment. However, the majority of the review focuses on the significant shortcomings and missing details, which are critical for the paper's acceptance. Therefore, the sentiment score is -40. The politeness of the language is relatively high. The reviewer uses formal and respectful language, even while pointing out major flaws. There are no rude or disrespectful comments, so the politeness score is 80.",-40,80
"The work releases a large-scale multimodal dataset recorded from the X-Plane simulation, as a benchmark dataset to compare various representation learning algorithms for reinforcement learning. The authors also proposed an evaluation framework based on some simple supervised learning tasks and disentanglement scores. The authors then implemented and compared several representation learning algorithms using this dataset and evaluation framework. 

pros:
1.  Releasing this dataset as a benchmark for comparing representation learning algorithms can potentially impact the community greatly;
2. The authors combined several existing work on measuring representation learning algorithms and proposed an evaluation framework to evaluate the quality of learned representation using supervised learning tasks and disentanglement scores;
3. The authors implemented an extended list of representation learning algorithms and compared them on the dataset;

cons:
1. the paper lacks clarification and guideline to convince the readers of the usefulness of the dataset and the evaluation framework. The authors spent almost half of the space explaining different existing representation learning algorithms. A more convincing story would be to find a few well-established representation learning algorithms to corroborate on the reliability of the dataset and the evaluation metrics;
2. More details should be put into describing the dataset. It is not clear why this dataset is particularly suited for evaluating representation learning in the context of reinforcement learning. Do the authors have insight on the difficulty of the task? While having multi-modality is appreciated, it might worth thinking a separate dataset focusing on a single modality, e.g., image;
3.  Given that the authors designed the dataset for evaluating representation learning for reinforcement learning, it is worth evaluating these algorithms on solving the main task using some standard RL techniques on top of the learned representations.
4. Table 4 is difficult to parse. ","The sentiment of the review appears to be generally positive, as the reviewer acknowledges the potential impact of the dataset and the comprehensive evaluation framework proposed by the authors. However, the review also includes several critical points that suggest areas for improvement, which slightly tempers the overall positive sentiment. Therefore, the sentiment score is 40. The language used in the review is polite and constructive, offering specific recommendations without being harsh or dismissive. The reviewer uses phrases like 'it might worth thinking' and 'it is worth evaluating,' which are considerate and suggestive rather than commanding. Therefore, the politeness score is 80.",40,80
"Summary
This paper proposes label propagation network (LPN), a neural network to learn label prediction and similarity measure (weights) between data points simultaneously in semi-supervised setting. The proposed method simulates label propagation steps with the forward pass of LPN, enabling backpropagation through label propagation steps.

Strong points
- Learning both weights and label predictions in SSL seems to be novel (provided that the author's claim in the related work section is right).
- Good performance.
- The paper is generally well written.

Concerns
- Replacing the label propagation by forward pass of a neural network is an attractive idea, but because of that the convergence guarantee is lost.  As Figure 4 shows, LPN without bifurcation mechanism seems to suffer from convergence issue as the number of evaluation step grows. I guess that the algorithm may go wrong even with bifurcation mechanism for some data, for example if the bifurcation rate grows too fast/slow.
- The original label propagation works with weights without entropy. Does introducing entropy term (e(h_i;theta)) is always helpful? For instance, if some data points erroneously get certain during initial iterations, the whole algorithm may fail.
- The performance reported for GCN is quite different from what is presented in the GCN paper, and authors explain that this is due to the different experimental setting. For me the performance gap is quite significant to be originated from different experimental setting. Could you elaborate on this? Also, how many GCN layers were used?
- Too many hyperparameters to tune.

Minor points
- I think the line above Eq (4) should be like \tilde w_ij = w_ij / sum_k w_ik.
- Eq (10) is quite misleading. The original weight w_ij should be symmetric (w_ij = w_ji), but this is not. Also, considering the intuition behind the label propagation, I think Eq (10) should be like alpha_ij(h_i, h_j) = exp(e(h_j) + d(h_i, h_j)), not e(h_i) as written the paper.
- In the experiments setting, the authors calling their algorithm as DeepLP_alpha and DeepLP_phi. I guess these should be LPN_alpha and LPN_phi.
","The sentiment of the review is generally positive, as indicated by the strong points listed and the overall positive tone in the summary. However, there are several concerns and minor points that suggest areas for improvement, which slightly temper the overall positivity. Therefore, the sentiment score is 40. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, even when pointing out concerns and minor issues. Therefore, the politeness score is 80.",40,80
"This paper presents a multiview framework for sentence representation in NLP tasks. Authors propose two architectures, one using a generative objective, while the other uses a discriminative objective. Both combine a recurrent based encoding function and a linear model. Large experiments have been conducted on several NLP tasks and datasets, showing improvement of the introduced frameworks compared to baselines.

The paper is globally well written and has a clear presentation. But I'm not sure to understand why authors motivate their work on the asymmetric information processing in the two hemispheres of the human brain. It sounds like a nice motivation, but the work presented here does not show any clear answer for this, except the idea of combining two different encoders for sentence representation..

My main concern is about the term multiview since the merging step is somewhat trivial (min/max/averaging vectors or concatenation). This is far from significant works on multiview learning, see: ""Multi-view learning overview: Recent progress and new challenges"".

Table 3, where G and D refer respectively to Generative and Discriminative models. But what differences between G1, G2, G3 ; D1, D2, D3 ?

Invertible constraint is a nice idea for using inverse of the decoder as the encoder. Is it really to take advantages of decoder information on the encoder/representation part? Or also to reduce the amount of parameters learnt in the model? Moreover, it is unclear on the ablation study: did you consider the original encoder ; or still the inverse of decoder but without the constraint? Unfortunately, it seems to not give significant gain, according to ablation study in table 6.

In this current form, I feel this paper does not give sufficient novelty to be accepted at ICLR.","The sentiment of the review is mixed. The reviewer acknowledges the clear presentation and the large experiments conducted, which is positive. However, they express significant concerns about the motivation behind the work, the use of the term 'multiview,' and the novelty of the paper. Therefore, the sentiment score is slightly negative. The language used is generally polite, with phrases like 'I'm not sure to understand,' 'My main concern is,' and 'In this current form, I feel,' which indicate a respectful tone despite the critical feedback.",-20,80
"The paper considers sparse kernel design in order to reduce the space complexity of  a convolutional neural network. In specifics, the proposed procedure is composed of following steps: 1) remove repeated layers, 2) remove designs with large degradation design, and 3) further remove design for better parameter efficiency.

The paper proposed the composition of group convolution, pointwise convolution, and depthwise convolution  for the sparse kernel design, which seems pretty promising. In addition, the authors discussed the efficiency of each convolution compositions.

I failed to appreciate the idea of information field, I didn't understand the claims that ""For one output tensor, sizes of information fields for all activations are usually the same"". When introducing a new concept, it's very important to make it clear and friendly. The author could consider more intuitive, high level, explanation, or some graphic demonstrations. Also, I couldn't see why this notion is important in the rest of the paper.

Personally I'm so confused by the theorem. It looks like a mathematical over-claim to me. It claims that the best efficiency is achieved when M N = C. However, is it always the case? What is M N \neq C? What does the theorem mean for real applications?

All the reasoning and derivation are assuming the 3 x 3 spatial area and 4 way tensor. I would assume these constant are not important, the paper could be much stronger if there is a clear notion of general results.","The sentiment of the review is mixed. The reviewer acknowledges the promise of the proposed method and the discussion on the efficiency of convolution compositions, which indicates a positive sentiment. However, the reviewer also expresses confusion and criticism regarding the explanation of the information field and the theorem, which introduces a negative sentiment. Overall, the sentiment can be considered slightly positive. The politeness of the language is generally high. The reviewer uses phrases like 'I failed to appreciate,' 'it's very important to make it clear and friendly,' and 'the author could consider,' which are polite ways to express criticism and suggestions.",20,80
"This paper presents an iterative approach to separate unobserved distribution signal from a mixture with observed distribution. The proposed approach looks reasonable to me, however, the experiment and analysis are insufficient.
1. At test time, does the input also go through the same number of iterations (10)? I would like to see how the separated results evolve over iterations.
2. It is not clear what is the quality of samples generated by GLO. In the image separation task, GLOM performs better than GAN, but worse in other tasks. Analysis is needed here.
3.  I noticed that only in the music separation task, finetuning is significantly better than vanilla NES. Is it because generative models can synthesize more realistic data samples? For example, would the generator learn to synthesize X+B with temporal synchronization? More analysis is also needed here.

============================

I think the reviewer addressed my questions and concerns in the rebuttal, so I raised my rating to 6.","The sentiment of the review is moderately positive. The reviewer acknowledges that the proposed approach is reasonable but points out that the experiment and analysis are insufficient. This suggests a sentiment score of around 20. The language used is polite and constructive, as the reviewer provides specific recommendations for improvement without being harsh or dismissive, leading to a politeness score of 80.",20,80
"The paper starts by establishing that biases play an important, negligible role in existing DNNs.
Specifically, they help improve classification performance, and networks trained with biases do make use of biases.

Then, the authors recognize that the state of the art DNNs use ReLU and variants, which are a piece-wise linear function.
Over the linear regions, the entire DNN can be collapsed into a single linear model f(x) = Wx + b.

Then the authors argue that the existing gradient-based attribution methods (for interpreting DNNs) often ignore the attribution of the `b` terms in the heatmap.
That is, when backpropagating the DNN outputs back to the input, the gradient of (Wx + b) wrt x is exactly W only (ignoring the contribution of b).

The paper then proposes a method for backpropagating biases.
From the presented results, I only can conclude that bias backpropagation does show a different heatmap compared to regular gradient-based methods.
However, it is unclear how much this BBp result is advancing our understanding of DNNs.
The result for this is still preliminary.

- Clarity
Research is well motivated, and paper presentation shows a nice, coherent story.

- Originality
AFAIK, the direction of looking at bias attribution is novel.

- Significance
The significance of the paper is limited because (1) the paper only considers the positive region of ReLUs; (2) the empirical results are preliminary and do not show a convincing usefulness of BBp.
Suggestions: authors may design a toy dataset or find a dataset that has some inherent biases (e.g. data imbalance) to show that DNNs do capture interesting information in the biases. From there, hopefully the impact of BBp can be clearer.

At the moment, the paper appears not ready for publication.","The sentiment of the review is mixed but leans slightly positive. The reviewer acknowledges the novelty and clarity of the research but also points out significant limitations and the preliminary nature of the results. Therefore, the sentiment score is 10. The politeness of the language is high; the reviewer provides constructive feedback and suggestions for improvement without being harsh or dismissive. Thus, the politeness score is 80.",10,80
"I am upgrading my reviews after the rebuttal, which actually has convinced me that there is something interesting going on in this paper. However, I'm not entirely convinced as the approach seems to be ad hoc. the intuitions provided are somewhat satisfactory, but it's not clear why the method works.. for example, the approach is highly sensitive to the hyperparameter ""drop rate"" and there is no way to find a good value for it. I'm inclined towards rejection as, even though results are almost satisfying, I yet don't understand what exactly is happening. Most of the arguments seems to be handwavy. I personally feel like a paper as simple as this one with not enough conceptual justifications, but good results (like this one), should go to a workshop. 

======
The authors propose to randomly drop a few parameters at the beginning and fix the resulting architecture for train and test. The claim is that the resulting network is robust to adversarial attacks.

Major concerns:
An extremely simple approach of pruning neural networks (randomly dropping weights) with no justification whatsoever. There are so many other network pruning papers available. If the point is to use pruned network then the authors must provide analysis over other pruning schemes as well.

Another major concern (technical contributions): How is the idea of randomly dropping weights different from Deep Expander Networks (Prabhu et al., ECCV 2018)? Please clarify.

Minor suggestion: Another simple approach to test the hypotheses would be to try dropout at test time and see the performance.","The sentiment score is derived from the overall tone of the review, which is mixed but leans towards negative. The reviewer acknowledges some interesting aspects of the paper but expresses significant doubts and concerns about the approach and its justifications. Therefore, the sentiment score is -40. The politeness score is based on the language used, which is generally respectful but includes some critical remarks. The reviewer uses phrases like 'not entirely convinced,' 'highly sensitive,' and 'no justification whatsoever,' which are critical but not rude. Therefore, the politeness score is 20.",-40,20
"This work introduces a novel defense method ""Neural Fingerprinting"" against adversarial examples.
In the training process, this method embeds a set of characteristic labeled samples so that responses of the model around real data show a specific pattern.  The defender can detect if a given query is adversarial or not by checking the pattern at test time.

Strong point:
The strong point is that the proposed method seems to be appropriate and technically original. The performance is well investigated and compared with several competitors.  The organization is good and the idea is clearly stated. 
  
Weak point:
One question is that why the proposed method can be protective against the adaptive CW attack. In the public discussion, the authors mention that the defense works successfully because the landscape of the fingerprinting loss is non-convex and no gradient method is guaranteed to find a suitable solution. If this is correct, did you repeatedly try the gradient-based attack with changing random seeds? By doing so, the attack might work successfully with a certain probability.

Comments:
The presented method seems to have a certain similarity with digital watermarking of deep neural networks, for example:
https://gzs715.github.io/pubs/WATERMARK_ASIACCS18.pdf
It would be interesting to mention to these methods in the related work section.

","The sentiment of the review is generally positive, as the reviewer acknowledges the novelty and technical originality of the proposed method, as well as its good performance and clear organization. However, there is a critical question raised about the method's effectiveness against adaptive CW attacks, which slightly tempers the overall positive sentiment. Therefore, the sentiment score is 70. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, offering suggestions and asking questions in a respectful manner. Thus, the politeness score is 90.",70,90
"This paper proposes a new architecture for adversarial training that is able to improve both accuracy and robustness performances using an attention-based model for feature prioritization and L2 regularization as implicit denoising. The paper is very clear and well written and the contribution is relevant to ICLR. 

Pros:

- The background, model and experiments are clearly explained. The paper provides fair comparisons with a strong baseline on standard datasets.
- Using attention mechanisms to improve the model robustness in an adversarial training setting is a strong and novel contribution 
- Both quantitative and qualitative results are interesting. 

","The sentiment of the review is highly positive, as indicated by phrases like 'very clear and well written,' 'relevant to ICLR,' and 'strong and novel contribution.' The reviewer highlights several strengths of the paper, including clear explanations, fair comparisons, and interesting results. The politeness of the language is also very high, as the reviewer uses respectful and appreciative language throughout the review.",100,100
"Summary:
The authors present an empirical analysis of how the size of SGD batches affects neural networks' training time.   

Strengths:
As mini-batches training is highly popular nowadays, the problem emphasized by the authors may have a high impact in the community. Together with recent analysis on the generalization properties of over-parametrized models, the paper may help understand more general open problems of neural networks' training. A nice contribution of the paper is the observation that different phases of scaling behaviour exist across a range of datasets and architectures.  

Weaknesses:
Based on empirical evaluation, the paper cannot make any claim about the generality of the obtained results. Even if the authors' analysis is based on a large set of benchmarks, it is hard to asses whether and how the results extend to cases that are not included in Section 4. In particular, it is not clear how the definition of different training phases can help the practitioner to tune the training parameters, as the size and range of the different regimes depend so strongly on the model's architecture and dataset at hand.  

Questions:
- have the properties of mini-batches training been explored from a formal/theoretical perspective? do those results match and confirm the proposed empirical evaluation?
- how are the empirical results obtained in the experiment section expected to depend on the specific dataset/benchmark? For example,  given a particular architecture, what are the key features that define the three training phases (shape of the nonlinearity, number of layers, underlying distribution of the dataset)?
- what is a  batch size that does not allow one to 'fully utilize our available compute'?
- does the amount of over-parameterization in the model have any effects on the definition of the training phases? How are the results obtained in the paper linked to the generalization gap phenomenon?","The sentiment of the review is generally positive, as indicated by the acknowledgment of the paper's potential high impact and its contribution to understanding neural networks' training. However, the review also points out significant weaknesses, particularly the limitations in the generality of the results. Therefore, the sentiment score is 40. The language used in the review is polite and constructive, with no rude or harsh comments. The reviewer asks questions and provides feedback in a respectful manner, so the politeness score is 80.",40,80
"
[Summary]
This paper proposed an algorithm for zero-shot translation by using both dual learning (He et al, 2016) and multi-lingual neural machine translation (Johnson et al 2016). Specially, a multilingual model is first trained following (Johnson et al 2016) and then the dual learning (He et al 2016) is applied to the pre-trained model using monolingual data only. Experiments on MultiUN and WMT are carried out to verify the proposed algorithm. 

[Details]
1.	The idea is incremental and the novelty is limited. It is a simple combination of dual learning and multilingual NMT. 

2.	Many important multilingual baselines are missing. [ref1, ref2]. At least one of the related methods should be implemented for comparison.

3.	The Pseudo NMT in Table 3 should also be implemented as a baseline for MultiUN experiments for in-domain verification.

4.	A recent paper [ref3] proves that using more monolingual data will be helpful for NMT training. What if using more monolingual data in your system? I think using $1M$ monolingual data is far from enough.

5.	What if using more bilingual sentence pairs? Will the results be boosted? What if we use more language pairs?

6.	Transformer (Vaswani et al. 2017) is the state-of-the-art NMT system. At least one of the tasks should be implemented using the strong baseline.

[Pros] (+) A first attempt of dual learning and multiple languages; (+) Easy to follow.
[Cons] (-) Limited novelty; (-) Experiments are not enough.

References
[ref1] Firat, Orhan, et al. ""Zero-resource translation with multi-lingual neural machine translation."" EMNLP (2016).
[ref2] Ren, Shuo, et al. ""Triangular Architecture for Rare Language Translation."" ACL (2018).
[ref3] Edunov, Sergey, et al. ""Understanding back-translation at scale.""EMNLP (2018). 

I am open to be convinced.

==== Post Rebuttal ===
Thanks the authors for the response. I still have concerns about this work. Please refer to my comments ""Reply to the rebuttal"". Therefore, I keep my score as 5.

","The sentiment of the review is slightly negative, as indicated by the comments on the limited novelty and insufficient experiments. However, the reviewer acknowledges the attempt and the clarity of the paper, which prevents the sentiment from being entirely negative. Therefore, the sentiment score is -20. The politeness of the language is generally respectful and constructive, with phrases like 'I am open to be convinced' and 'Thanks the authors for the response,' indicating a polite tone. Thus, the politeness score is 80.",-20,80
"This paper proposed a multi-Level framework for learning node embeddings for large-scale graphs. The author first coarsens the graphs into different levels of subgraphs. The low-level subgraphs are obtained with the node embeddings of the higher-level graphs with a graph convolutional neural network. By iteratively applying this procedure, the node embeddings of the original graphs can be obtained. Experimental results on several networks (including one network with ~10M node) prove the effective and efficiency of the proposed method over existing state-of-the-art approaches.   

Strength:
- scaling up node embedding methods is a very important and practical problem
- experiments show that the proposed methods seems to be very effective. 
Weakness:
- the proposed method seems to be very heuristic
- some claims in the papers are wrong according to existing literatures

Overall, the paper is well written and easy to follow. The proposed method is simple but heuristic.  However, the performance seems to be quite effective according to the experiments. The reasons that why the method works need to be better explained, which can significantly the quality of the paper and its impact in the future.

Details:
-- In the introduction part, ""However, such methods rarely scale to large datasets (e.g., graphs with over 1 million nodes) since they are computationally expensive and often memory intensive"". This is not TRUE! In the paper of LINE (Tang et al. 2015). It shows the LINE model can easily scale up to networks with one million nodes with a few hours. 
-- The authors use Equation (7) to learn the parameters of the graph convolutional neural network. I am really surprised that this method works. Especially the learned parameters are shared across different layers. 
-- Have you tried and compared different approaches of graph coarsening?
-- In Figure 2. (a), according to Equation (1), in the second step, the weight of the edge between A and DE should be 2/sqrt(3)*sqrt(4)?","The sentiment of the review is generally positive, as indicated by phrases like 'the paper is well written and easy to follow' and 'the proposed methods seems to be very effective.' However, there are some criticisms, such as the method being 'very heuristic' and some claims being 'wrong according to existing literatures.' Therefore, the sentiment score is 50. The politeness of the language is quite high, as the reviewer uses polite language and constructive feedback, such as 'The reasons that why the method works need to be better explained' and 'Have you tried and compared different approaches of graph coarsening?' Therefore, the politeness score is 80.",50,80
"The paper proposes a new way of defining CNNs for omnidirectional images. The method is based on graph convolutional networks, and in contrast to previous work, is applicable to other geometries than spherical ones (e.g. fisheye cameras). Since standard graph CNNs are unable to tell left from right (and up from down, etc.), a key question is how to define anisotropic filters. This is achieved by introducing several directed graphs that have orientation built into the graph structure.

The paper is fairly well written, and contains some new ideas. However, the method seems ad-hoc, somewhat difficult to implement, and numerically brittle. Moreover, the method is not equivariant to rotations, and no other justification is given for why it makes sense to stack the proposed layers to form a multi-layer network. 

The results are underwhelming. Only experiments with small networks on MNIST variants are presented. A very marginal improvement over SphericalCNNs is demonstrated on spherical MNIST. I'm confused by the dataset used: The authors write that they created their own spherical MNIST dataset, which will be made publicly available as a contribution of the paper. However, although the present paper fails to mention it, Cohen et al. also released such a dataset [1], which raises the question for why a new one is needed and whether this is really a useful contribution or only results in more difficulty comparing results. Also, it is not stated whether the 95.2 result for SphericalCNNs was obtained from the authors' dataset or from [1]. If the latter, the numbers are not comparable.

The first part of section 3.2 is not very clear. For example, L^l is not defined. L is called the Laplacian matrix, but the Laplacian is not defined. It would be better to make this section more self contained.

In the related work section, it is stated that Cohen et al. use isotropic filters, but this is not correct. In the first layer they use general oriented spherical filters, and in later layers they use SO(3) filters, which allows anisotropy in every layer. Estevez et al. [2] do use isotropic spherical filters.

In principle, the method is applicable to different geometries than the spherical one. However, this ability is only demonstrated on artificial distortions of a sphere (fig 3), not practically relevant geometries like those found fisheye lenses.

In summary, since the approach seems a bit un-principled, does not have nice theoretical properties, and the results are not convincing, I recommend against acceptance of this paper in its current form.


[1] https://github.com/jonas-koehler/s2cnn/tree/master/examples/mnist
[2] Estevez et al. Learning SO(3) Equivariant Representations with Spherical CNNs","The review starts with a neutral to slightly positive sentiment, acknowledging that the paper is fairly well written and contains some new ideas. However, it quickly shifts to a more critical tone, pointing out several significant issues with the method, its implementation, and the results. The sentiment score is therefore slightly negative overall. The language used is generally polite, with constructive criticism and specific recommendations, although it is firm in its assessment that the paper should not be accepted in its current form.",-30,70
"This paper proposes an unsupervised method for subgoal discovery and shows how to combine it with a model-free hierarchical reinforcement learning approach. The main idea behind the subgoal discovery approach is to first build up a buffer of “interesting” states using ideas from anomaly detection. The states in the buffer are then clustered and the centroids are taken to be the subgoal states.

Clarity:
I found the paper somewhat difficult to follow. The main issue is that the details of the algorithm are scattered throughout the paper with Algorithm 1 describing the method only at a very high level. For example, how does the algorithm determine that an agent has reached a goal? It’s not clear from the algorithm box. Some important details are also left out. The section on Montezuma’s Revenge mentioned that the goal set was initialized using a “custom edge detection algorithm”. What was the algorithm? Also, what exactly is being clustered (observations or network activations) and using what similarity measure? I can’t find it anywhere in the paper. Omissions like this make the method completely unreproducible. 

Novelty:
The idea of using clustering to discover goals in reinforcement learning is quite old and the paper does a poor job of citing the most relevant prior work. For example, there is no mention of “Dynamic Abstraction in Reinforcement Learning via Clustering” by Mannor et al. or of “Learning Options in Reinforcement Learning” by Stolle and Precup (which uses bottleneck states as goals). The particular instantiation of clustering interesting states used in this paper does seem to be new but it is important to do a better job of citing relevant prior work and the overall novelty is still somewhat limited.

Significance:
I was not convinced that there are significant ideas or lessons to be taken away from this paper. The main motivation was to improve scalability of RL and HRL to large state spaces, but the experiments are on the four rooms domain and the first room of Montezuma’s Revenge, which is not particularly large scale. Existing HRL approaches, e.b. Feudal Networks from Vezhnevets et al. have been shown to work on a much wider range of domains. Further, it’s not clear how this method could address scalability issues. Repeated clustering could become expensive and it’s not clear how the number of clusters affects the approach as the complexity of the task increases. I would have liked to see some experiments showing how the performance changes for different numbers of clusters because setting the number of clusters to 4 in the four rooms task is a clear use of prior knowledge about the task.

Overall quality:
The proposed approach is based on a number of heuristics and is potentially brittle. Given that there are no ablation experiments looking at how different choices (number of clusters/goals, how outliers are selected, etc) I’m not sure what to take away from this paper. There are just too many seemingly arbitrary choices and moving parts that are not evaluated separately.

Minor comments:
- Can you back up the first sentence of the abstract? AlphaGo/AlphaZero do well on the game of Go which has ~10^170 valid states.
- First sentence of introduction. How can the RL problem have a scaling problem? Some RL methods might, but I don’t understand what it means for a problem to have scaling issues.
- Please check your usage of \cite and \citep. Some citations are in the wrong format.
- The Q-learning loss in section 2 is wrong. The parameters of the target (r+\gamma max Q) are held fixed in Q-learning.","The sentiment of the review is generally negative, as the reviewer points out several significant issues with the paper, including lack of clarity, limited novelty, and questionable significance. The reviewer also mentions that the proposed approach is based on heuristics and is potentially brittle, which further contributes to the negative sentiment. Therefore, the sentiment score is -70. The politeness of the language used in the review is relatively high. Despite the critical feedback, the reviewer maintains a professional tone and provides constructive criticism without being rude. Thus, the politeness score is 70.",-70,70
"The authors present an OOD detection scheme with an ensemble of generative models. When the exact likelihood is available from the generative model, the authors approximate the WAIC score. For GAN models, the authors compute the variance over the discriminators for any given input. They show that this method outperforms ODIN and VIB on image datasets and also achieves comparable performance on Kaggle Credit Fraud dataset.

The paper is overall well-written and easy to follow. I only have a few comments about the work.

I think the authors should address the following points in the paper.
- What is the size of the ensemble for the experiments?
- How does the size of the ensemble influence the measured performance?
- It is Fast Gradient Sign Method (FGSM), not FSGM. See [1]. Citing [1] for FGSM would also be appropriate.

Quality. The submission is technically sound. The empirical results support the claims, and the authors discuss the failure cases. 
Clarity. The paper is well-written and easy to follow while providing useful insight and connecting previous work to the subject of study.
Originality. To the best my knowledge, the proposed approach is a novel combination of well-known techniques.
Significance. The presented idea improves over the state-of-the-art.


References
[1] I. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and Harnessing Adversarial Examples,” in ICLR, 2015.
-------------------
Revision. The rating revised to 6 after the discussion and rebuttal.
 ","The sentiment of the review is positive, as indicated by phrases like 'well-written and easy to follow,' 'technically sound,' and 'improves over the state-of-the-art.' The reviewer also provides constructive feedback and acknowledges the strengths of the paper. Therefore, the sentiment score is 80. The politeness of the language is high, as the reviewer uses polite and respectful language throughout the review, such as 'I think the authors should address' and 'would also be appropriate.' Therefore, the politeness score is 90.",80,90
"The paper presents an unsupervised sentence encoding method based on automatically generating inconsistent sentences by applying various transformations either to a single sentence or a pair and then training a model to classify the original sentences from the transformed ones.

Overall, I like the paper as it presents a simple method for training unsupervised sentence models which then can be used as part of further NLP tasks.

A few comments on the method and results:

- The results on Table 2 shows that supervised methods outperform unsupervised methods as well as the consistency based models with MultiTask having the largest margin. It would've been interesting to experiment with training multi-task layers on top of the sentence encoder and see how it would've performed.
- The detail of the architecture is slightly missing in a sense that it's not directly clear from the text if the output of the BiLSTMs is the final sentence encoding or the final layer before softmax?
- Also I would've thought that the output of LSTMs passed through nonlinear dense layers but the text refers to two linear layers.
- When I first read the paper, my eyes were looking for the result when you combine all of the transformations and train a single model :) - any reason why you didn't try this experiment?
- The paper is missing comparison and reference to recent works on universal language models (e.g. Radford et al 2018, Peters et al 2018, Howard et al 2018) as they rely on more elaborate model architectures and training compared to this paper but ultimately you can use them as sentence encoders. 
- One final note, which could be a subsequent paper is to treat these transformations as part of an adversarial setup to further increase the robustness of a language model such as those mentioned previously.","The sentiment of the review is positive, as indicated by the reviewer's statement 'Overall, I like the paper' and the constructive feedback provided. The sentiment score is 80 because the reviewer expresses a clear appreciation for the paper while suggesting improvements. The politeness score is 90 because the reviewer uses polite language throughout, such as 'It would've been interesting' and 'One final note,' which indicates a respectful and considerate tone.",80,90
"Title: DADAM: A consensus-based distributed adaptive gradient method for online optimization

Summary: 

The paper presented DADAM, a new consensus-based distributed adaptive moment estimation method, for online optimization. The author(s) also provide the convergence analysis and dynamic regret bound. The experiments show good performance of DADAM comparing to other methods. 

Comments: 

1) The theoretical results are nice and indeed non-trivial. However, could you please explain the implication to equation (7a)? Does it have absolute value on the LHS? 

2) Can you explain more clearly about the section 3.2.1? It is not clear to me why DADAM outperform ADAM here. 

3) Did you perform algorithms on many runs and take the average? Also, did you tune the learning rate for all other algorithms to be the best performance? I am not sure how you choose the parameter \alpha here. What if \alpha changes and do not base on that in Yuan et al. 2016? 

4) The deep learning experiments are quite simple. In order to validate the performance of the algorithm, it needs to be run on more datasets and networks architectures. MNIST and CIFAR-10 and these simple network architectures are quite standard. I would suggest to provide more if the author(s) have time. 

In general, I like this paper. I would love to have discussions with the author(s) during the rebuttal period. 
","The sentiment of the review is generally positive, as indicated by the summary and the concluding statement expressing a liking for the paper and a willingness to discuss it further. The reviewer acknowledges the non-trivial theoretical results and good performance of the proposed method. However, the review also contains several critical comments and suggestions for improvement, which slightly temper the overall positivity. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer uses polite phrases such as 'could you please,' 'I would suggest,' and 'I would love to have discussions,' indicating a respectful and constructive tone. Therefore, the politeness score is 90.",60,90
"This paper aims to distinguish between networks which memorize and those with generalize by introducing a new detection method based on NMF. They evaluate this method across a number of datasets and provide comparisons to both PCA and random ablations (as in Morcos et al., 2018), finding that NMF outperforms both. Finally, they show that NMF is well-correlated with generalization error and can be used for early stopping. 

This is an overall excellent paper. The writing is clear and and focused, and the experiments are careful and rigorous. The discussion of prior work is thorough. The question of how to detect memorization in DNNs is one of great interest, and this makes nice steps towards this goal. As such, it will likely have significant impact.  

Major comments:

1) The early stopping section could benefit from more experiments. In particular, it would be helpful to see a scatter plot of the time of peak test loss as a function of NMF/Ablation AuC local maxima and to measure the correlation between these rather than simply showing 3 examples.

Minor comments: 

1) While the comparisons to random ablations are mostly fair, it is worth noting that the variance on random ablations appears to be lower than that of NMF and PCA. 

2) The error bars on the plots are often hard to see. Increasing the transparency somewhat would be helpful.

Typos: 

1) Section 1, third paragraph: “We show that networks that networks that generalize…” should be “We show that networks that generalize...”

2) Section 3.1, third paragraph: “Because threshold is the…” should be “Because thresholding is the…”

3) Section 3.2, third paragraph: “In the most non-linear case we would…” should be “In the most non-linear case, we would…”

4) Figure 2 caption: “...with increasing level of…” should be “...with increasing levels of…”

5) Section 4.1.1, second to last line of last paragraph: missing space before final sentence

6) Figure 4a label: “Fahsion-MNIST” should be “Fashion-MNIST”
","The sentiment of the review is highly positive, as indicated by phrases such as 'overall excellent paper,' 'writing is clear and focused,' 'experiments are careful and rigorous,' and 'will likely have significant impact.' The reviewer provides constructive feedback and suggestions for improvement in a respectful and encouraging manner, which reflects a high level of politeness. The language used is supportive and aims to help the authors improve their work without being dismissive or harsh.",90,95
"###### Post-Revision ########################
Thank you for revising the paper and addressing the reviewers' concerns. The updated version reads much better and I have updated my score. 

Unfortunately, I still think that the experimental analysis is not enough to warrant acceptance. I would encourage the authors to have a more detailed set of experiments to showcase the effectiveness of their method and have ablation studies to disentangle the effects of the different moving parts. 
###### Post-Revision ########################

This paper considers arranging the examples into mini-batches so as to accelerate the training of metric embeddings. The
- The paper doesn't have sufficient experimental evidence to convince me that the proposed method is useful. There is no comparison against baselines. The paper is not clearly written or well organized. Detailed comments below:
- For example, when introducing focus and context entities, it would be helpful to give examples of this to make it clearer. 
- In section 3, please clarify that after drawing both positive and negative examples, what is the size of the minibatch for which the gradient is calculated? 
- How do you choose the size of the microbatches? If the microbatch size is too small, then the effect of associating examples is small. 
- In the line, ""Instead, we use LSH modules that are available at the start of training and are only a coarse proxy of the target similarity"" Why are you not iteratively refining the LSH modules as the training progresses? Won't this lead to an improvement in the performance? 
- In the line ""The coarse embedding can come from a weaker (and cheaper to train) model or from a partially-trained model. In our experiments we use a lower dimension SGNS model."" Could you please clarify what is the additioanal computational complexity of the method? This involves additional computational cost? It doesn't seem to me that the results justify this increased computation. Please justify this. 
- In Lemma 3.2, the term s_i is undefined
- ""In early training, basic coordinated microbatches with few or no LSH refinements may be most effective. As training progresses we may need to apply more LSH refinements. Eventually, we may hit a regime where IND arrangements dominate."" This explanation is vague and has no theoretical or empirical evidence supporting it. Please clarify this. 
- Please fix the size of the axes and the legend in all the figures. 
- For figure 1, how is the step-size chosen? What is the dimensionality of the examples?
- From figure 3, it is not clear that the proposed methods lead to significant gains over the independently sampling the examples? Are there any savings in the wall clock time for the proposed methods? Why is there no comparison against other methods that have proposed non-uniform sampling of examples for SGD (like Zhang, 2017)? Are the hyper-parameters chosen in a principled way for these experiments? ","The sentiment of the review is mixed but leans towards the negative side. The reviewer acknowledges improvements in the revised version but still finds the experimental analysis insufficient, which prevents them from recommending acceptance. The sentiment score is therefore slightly negative. The language used is polite and constructive, offering specific suggestions for improvement without being harsh or dismissive.",-30,80
"This paper proposed a new hashing algorithm with a new loss function. A multi-indexing scheme is adopted for search.  There is one key issue: in general hashing is not good at multi-indexing search for vector-based search in the Euclidean distance or Cosine similarity. The advantage of hashing is reducing the code size and thus memory cost, but it is still not as good as quantization=based approach. 

Here are comments about the experiments.
(1) Table 1: do other algorithms also use multi-indexing or simply linear scan?
(2)  Figure 4: HDT-E is better than PQ. It is not understandable. Something important is missing. How is the search conducted for PQ? Is multi-indexing used? It is also strange to compare the recall in terms of #(distance comparisons). 

","The sentiment of the review is slightly negative. The reviewer acknowledges the proposed new hashing algorithm and its components but immediately points out a key issue with the approach, suggesting that hashing is generally not effective for multi-indexing search in vector-based searches. The comments on the experiments further highlight confusion and skepticism about the results presented in the paper. The politeness of the language is neutral; the reviewer does not use any rude or overly critical language but is straightforward and direct in pointing out issues and asking questions.",-30,0
"In this paper, the authors proposed a fortified network model, which is an extension to denoising autoencoder. The extension is to perform the denoising module in the hidden layers instead of input layer. The motivation of this extension is that the denoising part is more effective in the hidden layers. Overall, this extension is quite sensible, and empirical results justify the utility of this extension. The major issue, which was left as an open question in the end of Section 3, is that when and where to use fortified layers. The authors discussed this issue, but did not solve this issue. Nevertheless, I do believe solving this issue requires a sequence of papers. Overall the paper reads very well, but there are a number of minor places to be improved. 

 
(1) a grammar error at ""provide a reliable signal of the existence of input data that do not lie on the manifold on which it the network trained.""

(2) a grammar error at ""This expectation cannot be computed, therefore a common approach is to to minimize the empirical risk""

(3) The sentence ""For a mini-batch of N clean examples, x(1), ..., x(N), each hidden layer h(1)_k, ..., h(N)_k is fed into a DAE loss"" is a little confusing to me. ""h(1)_k, ..., h(N)_k"" is only for one hidden layer, rather than ""each hidden layer"". Right?","The sentiment of the review is generally positive, as the reviewer acknowledges the sensible nature of the proposed extension and the empirical results that justify its utility. The reviewer also expresses belief in the potential of the work, despite noting an unresolved issue. Therefore, the sentiment score is 70. The politeness of the language is high, as the reviewer uses constructive language, acknowledges the strengths of the paper, and provides specific, minor corrections in a respectful manner. Thus, the politeness score is 90.",70,90
"The idea proposed in this paper is to aid in understanding networks by showing why a network chose class A over class B.  To do so, the goal is to find an example that is close to the original sample, but belongs to the other class. As is mentioned in the paper, it is crucial to stay on the data manifold for this to be meaningful. In the paper, an approach using a GAN to traverse the manifold is proposed and the experimental evaluation is done on MNIST.

If my understanding is correct the proposed approach requires:
Finding a noise code z_0 such that the GAN generates an image G(z_0) close to the original input x. As a metric L2 distance is proposed.
Find a point close to z_b that is close z_0  s.t. Class B is the most likely class and class A is the second most likely prediction. Specifically it is required that
The log likelihood of but classified as class B with the same log likelihood of class B for G(z_b) is the same as the log likelihood of class A for the input x.
Such that all other classes have a log likelihood that is at least epsilon lower than both the one of class A and class B.

The proposed approach is compared to a set of other interpretability methods, which were 
Grad-Cam, lime, PDA, xGEM on MNIST AND Fashion MNIST data. The proposed evaluation is all qualitative, i.e. subjective. It must also be noted that in the methods used for comparison are not used as originally intended.


Currently, I do not recommend this paper to be accepted for the following reasons.
The idea of using a GAN is to generate images in input space is not novel by itself. Although the application for interpretability by counterfactuals is. It is unclear to me how much of the appealing results come from the GAN model and how much come from truly interpreting the network. I have detailed this below by proposing a very simplistic baseline which could get similar results.
The experimental approach is subjective and I am not convinced by the experimental setup.
On the other hand, I do really appreciate the ideas of traversing the manifold. 

Remarks 
Related work and limitations of existing interpretability methods are discussed properly. Of course, the list of discussed methods is not exhaustive. The work on the PPGAN and the “Synthesizing the preferred inputs for neurons in neural networks via deep generator networks” is not mentioned although it seems very related to the proposed approach to traverse the manifold. What that work sets apart from the proposed approach is that is could be applied to imaganet and not just MNIST. 

Traversing the manifold to generate explanations is certainly a good idea and one that I completely support. One limitation of the proposed approach is that it is unclear to me whether a point on the decision boundary is desirable or that a point that is equally likely is desirable. My reasoning is that the point on the decision boundary is the minimal change and therefore the best explanation. In such a setup, the GAN is still crucial to make sure the sample remains on the data manifold and is not caused by adverarial effects.

The exact GAN structure and training approach should be detailed in this paper. Now only a reference is provided. 

Can you clarify how the constraints are encoded in the optimization problem?

The grad cam reference has the wrong citation

I do not understand the second paragraph of section 4.1. As mentioned in the paper, these other methods were not designed to generate this type of application. Therefore the comparison could be considered unfair. 

I would propose the following baseline. For image x from class A, find image y from class B such that x-y has minimal L2 norm and is correctly classified. Use y instead of the GAN generated image. Is the result much less compelling? Is it actually less efficient that the entire GAN optimization procedure on these relatively small datasets? 


I do have to say that I like the experiment with the square in the upper corner. It does show that the procedure does not necesarrily exploits adversarial effects. However, the baseline proposed above would also highlight that specific square?


Figure 5 shows that multiple descision boundaries are crossed. Is this behaviour desired? It seems very likely to me that it should be possible to move from 9 to 8 while staying on the manifold without passing through 5? Since the method takes a detour through 5’s is this common behaviour?


FINAL UPDATE
--------------------
Unfortunately, I am not entirely convinced by the additional experiments that we are truly looking into the classifier instead of analyzing the generative model. 
I believe this to be currently the key issue that, even after the revision, needs to be addressed more thoroughly before it can be accepted for publication. ","The sentiment of the review is mixed but leans towards the negative side. The reviewer acknowledges some positive aspects, such as the idea of traversing the manifold and the experiment with the square in the upper corner, but ultimately does not recommend the paper for acceptance due to significant concerns about the novelty and experimental setup. Therefore, the sentiment score is -40. The politeness of the language is generally high, with the reviewer providing constructive feedback and suggestions for improvement without being rude or dismissive. Thus, the politeness score is 80.",-40,80
"The paper addresses the problem of training an object detection network that can achieve good performance on both clean and noisy images.   
The proposed approach is based on a gating network that decides whether
the image is clean or  noisy. in case of  noisy image a denoising  method is applied.  The network components form a mixture of experts architecture and are  jointly trained after a component-level pretraining.
How good is the gate performance? what happen if you use only one of the trained experts for all the clean/noisy  test data? It is not clear how you combined the results of the two experts. Are you computing a weighted average of the original and the enhanced images? Did you try to use a hard decision gating at test time? 
  ","The sentiment of the review appears to be neutral to slightly positive. The reviewer acknowledges the problem addressed by the paper and describes the proposed approach without any negative language. However, there is no explicit praise or positive feedback, which keeps the sentiment closer to neutral. The politeness of the language is neutral to slightly polite. The reviewer asks questions and makes suggestions in a straightforward manner without using any harsh or overly critical language.",10,20
"Thank you for an interesting read.

The paper proposes adding an adversarial loss to improve the reconstruction quality of an auto-encoder. To do so, the authors define an auxiliary variable y, and then derive a GAN loss to discriminate between (x, y) and (x, decoder(encoder(x))). The algorithm is completed by combining this adversarial ""reconstruction"" loss with adversarial loss functions that encourages the matching of marginal distributions for both the observed variable x and the latent variable z. 

Experiments present quite a lot of comparisons to existing methods as well as an ablation study on the proposed ""reconstruction"" loss. Improvements has been shown on reconstructing input images with significant numbers.

Overall I think the idea is new and useful, but is quite straight-forward and has some theoretical issues (see below). The propositions presented in the paper are quite standard results derived from the original GAN paper, so for that part the contribution is incremental and less interesting. The paper is overall well written, although the description of the augmented distribution r(y|x) is very rush and unclear to me.

There is one theoretical issue for the defined ""reconstruction"" loss (for JS and f-divergences). Because decoder(encoder(x)) is a deterministic function of x, this means p(y|x) is a delta function. With r(y|x) another delta function (even that is not delta(y=x)), with probability 1 we will have mismatched supports between p(y|x) and r(y|x). 

This is also the problem of the original GAN, although in practice the original GAN with very careful tuning seem to be OK... Also it can be addressed by say instance noise or convolving the two distributions with a Gaussian, see [1][2].

I think another big issue for the paper is the lack of discussion on how to choose r(y|x), or equivalently, a(x). 

1. Indeed matching p_{\theta}(x) to p^*(x) and q(z) to p(z) does not necessarily returns a good auto-encoder that makes x \approx decoder(encoder(x)). Therefore the augmented distribution r(y|x) also guides the learning of p(y|x) and with appropriately chosen r(y|x) the auto-encoder can be further improved.

2. The authors mentioned that picking r(y|x) = \delta(y = x) will result in unstable training. But there's no discussion on how to choose r(y|x), apart from a short sentence in experimental section ""...we used a combination of reflecting 10% pad and the random crop to the same image size..."". Why this specific choice? Since I would imagine the distribution r(y|x) has significant impact on the results of PAGAN, I would actually prefer to see an in-depth study of the choice of this distribution, either theoretically or empirically. 

In summary, the proposed idea is new but straight-forward. The experimental section contains lots of results, but the ablation study by just removing the augmentation cannot fully justify the optimality of the chosen a(x). I would encourage the authors to consider the questions I raised and conduct extra study on them. I believe it will be a significant contribution to the community (e.g. in the sense of connecting GAN literature and denoising methods literature).

[1] Sonderby et al. Amortised MAP Inference for Image Super-resolution. ICLR 2017.
[2] Roth et al. Stabilizing Training of Generative Adversarial Networks through Regularization. NIPS 2017.","The review starts with a positive note, appreciating the interesting read and acknowledging the novelty and usefulness of the idea. However, it also points out several theoretical issues and areas for improvement, which brings the sentiment down to a more neutral level. The language used is polite and constructive, offering specific recommendations and encouraging further study, which indicates a high level of politeness.",20,80
"The contribution of the paper is to show that WGAN with entropic regularization maximize a lower bound on the likelihood of the observed data distribution. While the WGAN formulation minimizes the Wasserstein distance of the transformed latent distribution and the empirical distribution which is already a nice measure of ""progress"", having a bound on the likelihood can be interesting.

Pros:
+ I like the entropic GAN formulation and believe it is very interesting as it gives access to the joint distribution of latent and observed variables. 
+ While there are some doubtful statements, overall the paper is well written and easy to read.

Cons:
- The assumption of injectivity of the generator could be problematic, as it might not be fulfilled due to mode collapse.
- I feel the theory is not very deep. Since one has a closed form of the transportation map (Eq. 3.7), the likelihood of the data is obtained by marginalizing out the latent space. However, this assumes that the inner dual maximization problem is solved to stationarity so that Eq 3.7 holds, which is not the case in practice (5 discriminator updates).
- Thus in Sec. 4.1 for the likelihood at various points in training it is not clear what is actually happening.
- Sec 4.3 for unregularized GANs might be problematic. In general, the transportation plan is not a density function, so I'm not certain whether Theorem 1 / Corollary 2 still hold. Furthermore, the heuristic for ""inverting"" G^* is very crude. 

- There are also some minor problematic statements in the paper. While they can be easily fixed, they give me doubts:
  * The original VAE paper is not cited in the introduction for VAEs
  * The 2013 paper by Cuturi cited on page 2 has nothing to do with ""computational aspects of GANs"". It is about fast computation of approximate OT between two discrete prob. measures. 
  * First-order / second-order Wasserstein distance is I think a bit unusual name for W_1, W_2
  * On pg. 4, the point of the entropy term is to make the objective strongly convex. Strict convexity has no computational benefits.
","The review starts with a positive sentiment, appreciating the contribution of the paper and highlighting interesting aspects of the entropic GAN formulation. However, it quickly transitions to a more critical tone, pointing out several significant issues and minor problematic statements. The sentiment score is therefore mixed but leans slightly positive due to the initial appreciation. The language used is generally polite, with constructive criticism and suggestions for improvement, though it does express doubts and concerns clearly.",20,70
"
Brief summary: This work proposes a way to perform imitation learning from raw videos of behaviors, without the need for any special time-alignment or actions present. They are able to do this by using a recurrent siamese network architecture to learn a distance function, which can be used to provide rewards for learning behaviors, without the need for any explicit pose estimation. They demonstrate effectiveness on 2 different locomotion domains. 

Overall impression:
Overall, my impression from this paper is that the idea is to use a recurrent siamese network to learn distances which make sense in latent space and provide rewards for RL. This is able to learn interesting behaviors for 2 tasks. But I think the writing needs significant work for clarity and completeness, and there needs to be many more baseline comparisons. 

Abstract comments:
trail and error -> trial and error

Introduction comments:

Alternative reasons why pose estimation won’t work is because for any manipulation tasks, you can’t just detect pose of the agent, you also have to detect pose of the objects which may be novel/different

Few use image based inputs and none consider the importance of learning a distance function in time as well as space -> missed a few citations (eg imitation from observation (Liu, Gupta, et al))

Therefore we learned an RNN-based distance function that can give reward for out of sync but similar behaviour -> could be good to emphasize difference from imitation from observation (Liu, Gupta, et al) and TCN (Semanet et al), since they both assume some sort of time alignment

Missing related work section. There is a lot of related work at this point and it is crucial to add this in. Some things that come to mind beyond those already covered are:
1. Model-based Imitation Learning from State Trajectories
2. Reward Estimation via State Prediction
3. infoGAIL
4. Imitation from observation 
5. SFV: Reinforcement Learning of Physical Skills from Videos
6. Universal planning networks
7. https://arxiv.org/abs/1808.00928
8. This might also be related to VICE (Fu, Singh et al), in that they also hope to learn distances but for goal images only.
It seems like there is some discussion of this in Section 3.1, but it should be it’s own separate section.

Section 3 comments:
a new model can be learned to match this trajectory using some distance metric between the expert trajectories and trajectories produced by the policy π -> what does this mean. Can this be clarified?
 The first part of Section 3 belongs in preliminaries. It is not a part of the approach. 

Section 3.2
Equations 9 and 10 are a bit unnecessary, take away from the main point

What does distance from desired behaviour mean? This is not common terminology and should be clarified explicitly.

Equation 11 is very confusing. The loss function is double defined.  what exactly Is the margin \rho (is it learned?) The exact rationale behind this objective, the relationship to standard siamese networks/triplet losses like TCN should be discussed carefully. This is potentially the most important part of the paper, it should be discussed in detail.Also is there a typo, should it be || f(si) - f(sn)|| if we want it to be distances? Also the role of trajectories is completely not discussed in equation 11.

Section 3.3 
The recurrent siamese architecture makes sense, but what the positive and negative examples are, what exactly the loss function is, needs to be defined clearly. Also if there are multiple demonstrations of a task, which distance do we use then?

The RL simulation environment is it made in-house, based on bullet or something else?

Data augmentation - how necessary is this for method success? Can an ablation be done to show the necessity of this?

Algorithm 1 has some typos 
- > is missing in line 3
- Describe where reward r is coming from in line 10

Section 4.1
Walking gate -> walking gait

There are no comparisons with any of the prior methods for performing this kind of thing. For example, using the pose estimation baseline etc. Using the non-recurrent version. Using TCN type of things. It’s not hard to run these and might help a lot, because right now there are no baseline comparisons
","The sentiment score is determined by the overall impression and tone of the review. The reviewer acknowledges the novelty and effectiveness of the proposed method but also points out significant areas for improvement, such as writing clarity, completeness, and the need for more baseline comparisons. This mixed feedback results in a sentiment score of 10, indicating a slightly positive but critical stance. The politeness score is assessed based on the language used throughout the review. The reviewer uses polite and constructive language, providing specific suggestions and corrections without being rude or dismissive. This results in a politeness score of 80.",10,80
"In this paper the authors develop the clever idea to use attractor networks, inspired by Hopfield nets, to “denoise” a recurrent neural network.  The idea is that for every normal step of an RNN, one induces an additional ""dimension"" of recurrency in order to create attractor dynamics around that particular hidden state. The authors introduce their idea and run some basic experiments. This paper is well written and the idea is novel (to me) and worthy of exploration.  Unfortunately, the experiments are seriously lacking in my opinion, as I believe *the major focus* of those experiments should be comparisons to other denoising / regularization techniques.

MAJOR

The point is taken that RNNs are susceptible to noise due to iterated application of the function. In my experience, countering noise (in the sense of gaussian noise added) isn’t a huge problem in practice because there are many regularization methodologies to handle it. This leads me to the point that I think the experiments need to compare across a number of regularization techniques.  The paper is motivated by discussion of noise, “noise robustness is a highly desirable property in neural networks”, and the experiments show improved performance on smaller datasets, all of which speak to regularization. So I believe comparisons with regularization techniques are pretty important here. 

MODERATE

There is some motivation at the beginning of this piece, in particular about language, and does not contain citations, but should.

“Training is in complete batches to avoid the noise of mini-batch training.”  Please explain, I guess this is not a type of noise that the method handles? 

What about problems that require graded responses, which is likely anything requiring integration? For example,  what happens in the majority task if the inputs were switched to a non-discrete version, where one must hold analog numbers?


MINOR

Any discussion about the (presumably dramatic) increase in training time due to the attractor dynamics unrolling + additional batching due to noise vectors (if I understood correctly)?

What are your confidence intervals over?  Presumably, we’d like to get confidence over multiple network instantiations.

Pg 1. Articulated neural network? 


QUESTIONS

Does using a the ‘c’ variable as a bias instead of an initial condition really matter? 

How does supervised training via eqn (4) relate to the classic training of Hopfield nets? I assume not at all, but it would be useful to clarify?

What RNN architecture did you use in the Figure 5 simulations (tanh vanilla RNN or GRU?)
","The sentiment of the review is mixed. The reviewer acknowledges the novelty and potential of the idea, which is positive, but also expresses significant concerns about the lack of comprehensive experiments, which is negative. Therefore, the sentiment score is around 20. The language used is generally polite and constructive, with phrases like 'please explain' and 'I believe,' indicating a polite tone. Thus, the politeness score is 80.",20,80
"The main claim the authors make is that providing privacy in learning should go beyond just privacy for individual records to providing privacy for data contributors which could be an entire hospital. Adding privacy by design to the machine learning pipe-line is an important topic. Unfortunately, the presentation of this paper makes it hard to follow. 

Some of the issues in this paper are technical and easy to resolve, such as citation format (see below) or consistency of notation (see below). Another example is that although the method presented here is suitable only for gradient based learning this is not stated clearly. However, other issues are more fundamental:
1.	The main motivation for this work is providing privacy to a client which could be a hospital as opposed to providing privacy to a single record – why is that an important task? Moreover, there are standard ways to extend differential privacy from a single record to a set of r records (see dwork & Rote, 2014 Theorem 2.2), in what sense the method presented here different than these methods?
2.	Another issue with the hospitals motivation is that the results show that when the number of parties is 10,000 the accuracy is close to the baseline. However, there are only 5534 registered hospitals in the US in 2018 according to the American Hospital Association (AHA): https://www.aha.org/statistics/fast-facts-us-hospitals. Therefore, are the sizes used in the experiments reasonable?
3.	In the presentation of the methods, it is not clear what is novel and what was already done by Abadi et al., 2016
4.	The theoretical analysis of the algorithm is only implied and not stated clearly
5.	In reporting the experiment setup key pieces of information are missing which makes the experiment irreproducible. For example, what is the leaning algorithm used? If it is a neural network, what was its layout? What type of cross validation was used to tune parameters?
6.	In describing the experiment it says that “For K\in\{1000,10000} data points are repeated.” This could mean that a single client holds the same point multiple times or that multiple clients hold the same data point. Which one of them is correct? What are the implications of that on the results of the experiment?
7.	Since grid search is used to tune parameters, more information is leaking which is not compensated for by, for example, composition bounds
8.	The results of the experiments are not contrasted against prior art, for example the results of Abadi et al., 2016.

Additional comments
9.	The introduction is confusing since it uses the term “federated learning” as a privacy technology. However federated learning discusses the scenario where the data is distributed between several parties. It is not necessarily the case that there are also privacy concerns associated, in many cases the need for federated learning is due to performance constraints.
10.	In the abstract the term “differential attacks” is used – what does it mean?
11.	“An independent study McMahan et al. (2018), published at the same time”- since you refer to the work of McMahan et al before your paper was reviewed, it means that the work of McMahan et al came out earlier.
12.	In the section “Choosing $\sigma$ and $m$” it is stated that the higher \sigma and the lower m, the higher the privacy loss. Isn’t the privacy loss reduced when \sigma is larger? Moreover, since you divide the gradients by m_t then the sensitivity of each party is of the order of S/m and therefore it reduces as m gets larger, hence, the privacy loss is smaller when m is large. 
13.	At the bottom of page 4 and top of page 5 you introduce variance related terms that are never used in the algorithm or any analysis (they are presented in Figure 3). The variance between clients can be a function of how the data is split between them. If, for example, each client represents a different demography then the variance may be larger from the beginning.
14.	In the experiments (Table 1), what does it mean for \delta^\prime to be e-3, e-5 or e-6? Is it 10^{-3}, 10^{-5} and 10^{-6}?
15.	The methods presented here apply only for gradient descent learning algorithms, but this is not stated clearly. For example, would the methods presented here apply for learning tree based models?
16.	The citations are used incorrectly, for example “sometimes referred to as collaborative Shokri & Shmatikov (2015)” should be “sometimes referred to as collaborative (Shokri & Shmatikov, 2015)”. This can be achieved by using \citep in latex. This problem appears in many places in the paper, including, for example, “we make use of the moments accountant as proposed by Abadi et al. Abadi et al. (2016).” Which should be “we make use of the moments accountant as proposed by Abadi et al. (2016).” In which case you should use only \cite and not quote the name in the .tex file.
17.	“We use the same deﬁnition for differential privacy in randomized mechanisms as Abadi et al. (2016):” – the definition of differential privacy is due to Dwork, McSherry, Nissim & Smith, 2006
18.	Notation is followed loosely which makes it harder to follow at parts. For example, you use “m_t” for the number of participants in time t but in some cases,  you use only m as in “Choosing $\sigma$ and $m$”.
19.	In algorithm 1 the function ClientUpdate receives two parameters however the first parameter is never used in this function. 
20.	Figure 2: I think it would be easier to see the results if you use log-log plot
21.	Discussion: “For K=10000, the differrntially private model almost reaches accuracies of the non-differential private one.” – it is true that the model used in this experiment achieves an accuracy of 0.97 without DP and the reported number for K=10000 is 0.96 which is very close. However, the baseline accuracy of 0.97 is very low for MNIST.
22.	In the bibliography you have Brendan McMahan appearing both as Brendan McMahan and H. Brendan McMahan


It is possible that underneath that this work has some hidden jams, however, the presentation makes them hard to find. 

","The sentiment of the review is mixed but leans towards the negative side. The reviewer acknowledges the importance of the topic but points out numerous issues with the paper, both technical and fundamental. The sentiment score is -40 because the review highlights more problems than strengths. The politeness score is 20 because the language is generally professional and constructive, but there are instances where the critique could be perceived as blunt or overly critical.",-40,20
"In this work the authors propose an end to end approach for model based reinforcement learning from images, where the main building blocks are locally-linear dynamical systems and variational auto-encoders (VAE). Specifically, it is assumed that the input features (i.e., the images) are generated from a low dimensional latent representation mapped through parametric random functions; the latter are modeled via neural networks. A recognition model based on convolutional neural networks operates on the reverse way and is responsible for projecting the input features to the latent space, in order to proceed with the reinforcement learning task. The variational framework is employed in order to jointly learn the VAE and the linear dynamics on the latent state. As a final step, once the model is fitted a linear quadratic system (LQS) is solved in order to learn the cost function and the optimal policy. 

* The paper is well motivated and tries to solve an interesting problem, that of data-efficient reinforcement learning. The experiments are well picked and demonstrate the advantages of the proposed approach towards solving the task, however, the method is only evaluated on few environments and compared against only a couple of other methods. I would expect a broader evaluation and/or comparison against more methods. Since the model is able to reach TRPO’s performance in much less steps it would be nice to see how it performs against PPO from [Schulman et al. 2017] (at least on the simulated environments). Also, would it make sense to compare against [Levine et al. 2016] that has been evaluated on similar tasks?

[Schulman et al 2017] “Proximal Policy Optimization Algorithms”.
[Levine et al. 2016] “End-to-End Training of Deep Visuomotor Policies”.

* Methodologically, the paper is sound. The model part (as the authors point out) is based on [Johnson et al. 2016] and is well explained. On the other hand, the policy part, and in particular the policy update in Section 4.2 has some issues regarding readability. There is a strong interplay between Section 4.2, Section 2.1 and Appendix D and the authors did not manage to nicely explain what exactly is happening during the update phase. In the beginning the reader has the impression that we are finding the optimal policy via the closed-form LQS. Later on we switch to constrained optimisation for the cost by accounting for the KL divergence between the policy on two episodes. Finally, in the appendix we are back to the original quadratic cost. The authors need to clarify all the above. Also, they need to explicitly mention why they opt for stochastic optimisation (is it because of minibatching?)

* To continue with the policy, in Section 4.2 the authors argue that although the optimal policy can be found in closed form this is not desirable because the policy will overfit the model and will not generalise well in the real environment. I disagree with this statement. If this happens it effectively means that the learned model or the assumption/learning of the linear dynamics is not right. The authors seem to also agree with this since they clearly state in the the experimental section that “... our method does not heavily rely on an accurate model...”. To my understanding, this means that we need to refine the modelling strategy and not learn a sub-optimal policy. I am really interested in the authors opinion on that.

* The above argument is also directly related to the recognition model and learning of the policy in the latent state (I completely agree with that). The recognition network, which in this case is a convolutional neural network, is used as an inference mechanism to project the observations to the latent space. We learn the (variational) parameters of the recognition model by optimising the likelihood’s lower bound. This means that we are “allowed” to overfit the variational parameters as long as the bound gets tighter. This can possibly result in degraded performance during the policy update. Furthermore, the variational distribution of the latent state, i.e., q(z_t | s_t) is assumed to be mean field across time (independent z’s), while clearly this is not the case in the posterior. You somehow mitigate that by augmenting the observed state (feeding consecutive frames to the network), but still this is not ideal. Finally, is there a reason why we only use the mean of the recognition model to fit the cost on the projected latent states? Why are we throwing away the uncertainty? Especially since you do not use an exact solver and follow a stochastic gradient.

* In the end of Section 2.1, the authors argue regarding the fact that the prior work assumes access to a compact low-dimensional representation which does not allow them to perform well on images. Reference is needed.

* In the related work the authors mention modelling bias as a downside of prior work. Can you please elaborate on that? Where does the bias come from and, more importantly, how does your approach overcome this issue?

* In the experiment and specifically in Figure 4 am I right in assuming that the distance to target is measured in actual pixels? Furthermore, why the relevant plot for the reacher task is depicting rewards instead of the distance to target. To me this suggests that the task is not solved. In general what I find very upsetting in the field are plots that only depict accumulated reward for a specific task. There are many situations where the agent learns a weird behaviour that happens to give good rewards (e.g., spinning around the cart-pole), and unfortunately such behaviours are not spotted on the reward plots.

Overall, the paper is nicely presented and definitely an interesting work. However, given the fact that methodologically we have not learned anything new from this paper and in combination with the not satisfying experimental evaluation I warrant for rejection.","The sentiment of the review is mixed. The reviewer acknowledges the interesting problem the paper addresses and the soundness of the methodology, but also points out significant issues with the experimental evaluation and clarity of certain sections. The sentiment score is therefore slightly negative at -20. The politeness of the language is generally high, as the reviewer uses polite phrases such as 'I would expect', 'would it make sense', and 'I am really interested in the authors opinion on that'. The politeness score is 80.",-20,80
"Review for CO-MANIFOLD LEARNING WITH MISSING DATA
Summary:
This paper proposes a two-stage method to recovering the underlying structure of a data manifold using both the rows and columns of an incomplete data matrix. In the first stage they impute the missing values using their proposed co-clustering algorithm and in the second stage they propose a new metric for dimension reduction.
The overall motivation for how they construct the algorithm and the intuition behind how all the pieces of the algorithm work together are not great. The paper also has significant specific clarity issues (listed below). Currently these issues seem to imply the proposed algorithm has significant logic issues (mainly on the convex/concave confusions); however depending on how they are addressed, this may end up not being an issue. The experimental results for the two simulated datasets look very good. However for the lung dataset, the results are less promising and it is less clear of the advantage of the proposed algorithm to the two competing ones. 
Novelty/Significance:
The overall idea of the algorithm is sufficiently novel. It is very interesting to consider both rows and column correlations. Each piece of the algorithm seems to draw heavily on previous work; bi-clustering, diffusion maps, but overall the idea is novel enough. The algorithm is significant in that it addresses a relatively open problem that currently doesn’t have a well established solution.
Questions/Clarity:
Smooth is not clearly defined and not an obvious measure for a matrix. Figure 1 shows smooth matrices at various levels, but still doesn’t define explicitly what smoothness is. Does smoothness imply all entries are closer to the same value? 
 “Replacing Jr(U) and Jc(U) by quadratic row and column Laplacian penalties” – The sentence is kind of strange as Laplacian penalties is not a thing. Graph Laplacian can be used as an empirical estimate for the Laplace Beltrami operator which gives a measure of smoothness in terms of divergence of the gradient of a function on a manifold; however the penalty is one on a function’s complexity in the intrinsic geometry of a manifold. It is not clear how the proposed penalty is an estimator for the intrinsic geometry penalty. It seems like the equation that is listed is just the function map Omega(x) = x^2, which also is not a concave function (it is convex), so it does not fit the requirements of Assumption 2.2.
Proposition 1 is kind of strangely presented. At first glance, it is not clear where the proof is, and it takes some looking to figure out it is Appendix B because it is reference before, not after the proposition. Or it might be more helpful if it is clearly stated at the beginning of Appendix B that this is the proof for Proposition 1.
The authors write: “Missing values can sabotage efforts to learn the low dimensional manifold underlying the data. … As the number of missing entries grows, the distances between points are increasingly distorted, resulting in poor representation of the data in the low-dimensional space.” However, they use the observed values to build the knn graph used for the row/column penalties, which is counter-intuitive because this knn graph is essentially estimating a property of a manifold and the distances have the same distortion issue.
Why do the author’s want Omega to be concave functions as this makes the objective not convex. Additionally the penalty sqrt(|| ||_2) is approximately doing a square root twice because the l2-norm already is the square root of the sum of squares. Also what is the point of approximating the square root function instead of just using the square root function? It is overall not clear what the nature of the penalty term g2 is; Appendix A, implies it must be overall a convex function because of the upper bound.
Equation 5 is not clear that it is the first order taylor approximation. Omega’ is the derivative of the Omega function? Do the other terms cancel out? Also what is the derivative with respect to; each Ui. for all Uj. ?
 “first-order Taylor approximation of a differentiable concave function provides a tight bound on the function” – Tight bound is not an appropriate term and requires being provable. Unless the function is close to linear, a first order Taylor approximation won’t be anything close to tight.
The authors state the objective in 1 is not convex. Do they mean it is not strictly convex? In which case, by stationary points, they are specifically referring to local minima? Otherwise, what benefits does the MM algorithm have on an indefinite objective i.e. couldn’t you end up converging to a saddle point or a local maxima instead of a local minima, as these are all fixed points. 
It is not clear what the sub/super scripts l, k mean. Maybe with these defined, the proposed multi-scale metric would have obvious advantages, but currently it is not clear what the point of this metric is.
Figure 4 appears before it is mentioned and is displayed as part of the previous section.
For the Lung data, it does not look like the proposed algorithm is better than the other two. None of the algorithms seem to do great at capturing any of the underlying structure, especially in the rows. It also is not super clear that the normal patients are significantly further from the cancer patients. Additionally are the linkage results from figure 3 from one trial? Without multiple trials it is hard to argue that this not just trial noise.
How big are N1 and N2 in the linkage simulations. The Lung dataset is not very large, and it seems like the proposed algorithm has large computation complexity (it is not clear). Will the algorithm work on even medium-large sized matrices (10^4 x 10^4)?
","The sentiment of the review is mixed. While the reviewer acknowledges the novelty and significance of the proposed algorithm, they also highlight several significant issues with the clarity and logic of the paper. The sentiment score is therefore slightly negative. The language used in the review is generally polite and constructive, although it does point out numerous flaws and areas for improvement. The politeness score is positive as the reviewer maintains a professional tone throughout.",-20,60
"# Summary of the paper

Inspired by the success of deep filter banks, this paper presents a designed deep filter bank for graphs that is based on random walks.  More precisely, the technique uses lazy random walks, expressed in terms of the graph Laplacian, and re-frames this in terms of graph signal processing. Similarly to wavelets, graph node features are calculated at different scales and subsequently summed in order to remain invariant under permutations. Several experiments on graph data sets demonstrate the performance of the new technique.

# Review

This paper is written very well and explains its method with high clarity. The principal issues I see are as follows:

- The originality of the contributions is not clear
- Missing theoretical discussion
- The experimental setup is terse and slightly confusing

Concerning the originality of the paper, the differences to Gama et al., 'Diffusion Scattering Transforms on Graphs' are not made clear. Cursory reading of this publication shows a large degree of similarity. Both of the papers make use of diffusion geometry, but Gama et al. _also_ define a multi-scale filter bank, similar to Eq. 4 and 5. The paper needs to position itself more clearly vis-à-vis this other publication. Is the present approach to be seen more as an application of the theory that was developed in the paper by Gama et al.? What are the key similarities and differences? In terms of space, this could be added to Section 3.2, which could be rephrased as a generic 'Differences to other methods' section and has to be slightly condensed in any case (see my suggestions below). Another publication by Zou & Lerman, 'Graph Convolutional Neural Networks via Scattering', is also cited as an inspiration, but here the differences are larger in my understanding and do not necessitate further justification. Last, the publication 'Graph Capsule Convolutional Neural Networks' by Verma & Zhang is also cited for the definition of 'scattering capsules'. Again, cursory reading of the publication shows that this approach is similar to the presented one; the only difference being which features are used for the definition of capsules. I recommend referring to the invariants as 'capsules' and link it back to Verma & Zhang so that the provenance of the terminology is clear.

Concerning the theoretical part of the paper, I miss a discussion of the complexity of the approach. Such a discussion does not have to be long, but in particular since the paper mentions that the applicability of scattering transforms for transfer learning (and also remarks about the universality of them in Section 4), some space should be devoted to theoretical considerations (memory complexity, runtime complexity). This would strengthen the paper a lot, in particular in light of the complexity of other approaches! Furthermore, an additional experiment about the stability of scattering transforms appears warranted. While I applaud the experimental description in the paper (number of scales, how the maximum scale is chosen, ...), an additional proof or experiment in the appendix should deal with the stability. Let's assume that for extremely large graphs, I am content with 'almost-but-not-quite-as-good' classification performance. Is it possible to achieve this by limiting the number of scales? How much to the results depend on the 'right' choice here?

Concerning the experimental setup, I think that the way (average) accuracies are reported at present is slightly misleading. The paper even remarks about this in footnote 2. While I understand the need of demonstrating the universality of these features, I think that the current setup is not optimal for this. I would recommend (in addition to reporting accuracies) a transfer learning setup rather in which the beneficial properties of the new method can be better explored. More precisely, the claim from Section 4, 4th paragraph ('Since the scattering transform...') needs to be further explored. This appears to be a unique feature of the new method. The current experimental setup does not exploit it. As a side-note, I realize that this might sound like a standard request for 'show more experiments', but I think the paper would be more impactful if it contained one scenario in which its benefits over other approaches are clear.

# Suggestions for improvement

The paper flows extremely well and it is clear that care has been taken to ensure that everything can be understood. I liked the discussion of invariance properties in particular. There are only a few minor things that can be improved:

- 'covariant' and 'equivariant', while common in (graph) signal processing, could be briefly explained to increase accessibility and impact
- 'order' and 'layer' are not used consistently: in the caption of Figure 2a, the term 'order' is used, but for Eq. 4 and 5, for example, the term 'layer' is employed. Since 'layer' is more reminiscent of a DNN, I would suggest to use 'order' throughout the paper, because it meshes better with the way the scattering invariants are defined.
- the notation $Sx$ is slightly overloaded; in Figure 2a, for example, it is not clear at first that the individual cascades are supposed to form a *set*; this is only explained at the end of Section 3.1; to make matters more consistent, the figure should be updated and the combination of individual cascades should be made clear
- In Eq. 5, the bars of the absolute value are not set correctly; the absolute value should cover $\psi_j x(v_i)$ and not $(v_i)$ itself.
- minor 'gripe': $\psi^{(J)}$ is defined as a set in Eq. 2, but it is treated as a matrix or an operator (and also referred to as such); this should be more consistent
- The discussion of the aggregation of multiple statistics in Section 3.2 appears to be somewhat redundant in light of the discussion for Eq. 4 and Eq. 5 in the preceding section
- in the appendix, more details about the training of the FCN should be added; all other parts of the experiments are described in sufficient detail, but the training process requires additional information about learning rates etc.","The sentiment of the review is generally positive, as the reviewer acknowledges that the paper is well-written and explains its method with high clarity. However, the reviewer also points out several significant issues that need to be addressed, such as the originality of the contributions, missing theoretical discussion, and a slightly confusing experimental setup. Therefore, the sentiment score is not fully positive but leans towards the positive side. The politeness of the language used in the review is very high. The reviewer provides constructive criticism and suggestions for improvement in a respectful and professional manner, without any rude or harsh language.",60,90
"This paper presents a way use using FSA-augmented MDPs to perform AND and OR of learned policies. This idea is motivated by the desirability of compositional policies. I find the idea compelling, but I am not sure the proposed method is a useful solution. Overall, the description of the method is difficult to follow. With more explanations (perhaps an algorithm box?), I would consider increasing my score.

The experiments demonstrate that this method can outperform SQL at skill composition. However, it is unclear how much prior knowledge is used to define the automaton. If prior knowledge is used to construct the FSA, then a missing comparison would be to first find the optimal path through the FSA and then optimize a controller to accomplish it. As the paper is not very clear, that might be the method in the paper. 

Questions:
- How do you obtain the number of automaton states? 
- In Figure 1, are the state transitions learned or handcoded? Are they part of the policy's action space?
- In section 3.2, you state  s_{t:t+k} |= f(s)<c ⇔ f(s_t)<c    What does s without a timestep subscript refer to? Why does this statement hold?

Can you specify more clearly what you assume known in the experiments? What is learned in the automata? In Figure 5, does SQL have access to the same information as Automata Guided Composition?","The sentiment of the review is mixed but leans slightly positive. The reviewer finds the idea compelling but expresses significant concerns about the clarity and utility of the proposed method. Therefore, the sentiment score is 10. The language used is polite and constructive, with suggestions for improvement and specific questions aimed at clarifying the work. Thus, the politeness score is 80.",10,80
"In this paper, an efficient re-training algorithm for neural networks is proposed. The essence is like Hinton's distillation, but in addition to use the output of the last layer, the outputs of intermediate layers are also used. The core idea is to add 1x1 convolutions to the end of each layer and train them by fixing other parameters. Since the number of parameters to train is small, it performs well with the small number of samples such as 500 samples. 

The proposed method named FKSD is simple yet achieves good performance. Also, it performs well with a few samples, which is desirable in terms of time complexity. 

The downside of this paper is that there is no clear explanation of why the FKSD method goes well. For me, adding 1x1 convolution after the original convolution and fitting the kernel of the 1x1 conv instead of the original kernel looks a kind of reparametrization trick. Of course, learning 1x1 conv is easier than learning original conv because of a few parameters. However, it also restricts the representation power so we cannot say which one is always better. Do you have any hypothesis of why 1x1 conv works so well?



Minor:

The operator * in (1) is undefined.

What does the boldface in tables of the experiments mean? I was confused because, in Table 1, the accuracy achieved by FKSD is in bold but is not the highest one.
","The sentiment of the review is generally positive, as the reviewer acknowledges the efficiency and good performance of the proposed method, especially with a small number of samples. However, there is a slight critique regarding the lack of a clear explanation for why the method works well. The politeness of the language is high, as the reviewer uses polite and constructive language, even when pointing out the downsides and asking for clarification. The reviewer also provides specific minor suggestions in a respectful manner.",70,90
"This paper introduces LIT, a network compression framework, which uses multiple intermediate representations from a teacher network to guide the training of a student network. Experiments are designed such that student networks are shallower than teacher networks, while maintaining their width. The method is validated on CIFAR-10 and 100 as well as on Amazon Reviews.

The paper is clearly written and easy to follow. The main novelty of the paper is essentially using the teacher intermediate representations as input to the student network to stabilize the training, and applying the strategy to recent networks and tasks.

The authors claim that they are only concerned with knowledge transfer between layers of the same width, that is teacher and student network been designed (by model construction) to have the same number of downsampling operations, while maintaining the same number of stages (referred to as sections in the paper). However, resnet-based architectures have been shown to perform iterative refinement of their features between downsampling operations (see e.g. https://arxiv.org/pdf/1612.07771.pdf and https://arxiv.org/pdf/1710.04773.pdf ). Moreover, these models were also shown to be good regularizers, since they can reduce their model capacity as needed (see https://arxiv.org/pdf/1804.11332.pdf).  Therefore, having experiments skipping stages would be interesting, and may allow to further compress the networks (by skipping layers or stages which do not incorporate much transformation). Following https://arxiv.org/pdf/1804.11332.pdf, for the sake of completeness, it might also be interesting to compare LIT results to the ones obtained by just removing layers in the teacher network which have small weight norms.

In method, the last sentence before ""knowledge distillation loss"" suggests the training of student networks might not be done end-to-end. Could the authors clarify this?
It seems there might be a typo in the KD loss of ""knowledge distillation loss"", equation (2). Shouldn't the second term of the equation be a function of p^T and q^T (with temperature)?

I would suggest changing ""sections"" to stages, as previously introduced in https://arxiv.org/pdf/1612.07771.pdf .

As for the experiments, it would be more interesting to see this kind of analysis on ImageNet (pretained resnet models are readily available).
Figure 3, why not add hint training as well?
Figure 4, what's the dataset used here?

In Section 4.2, it seems that the choice of the IR layer in the analysis could have a significant impact. How was the layer chosen for the ablation study experiments?

There are a few overstatements in the paper:
- page 5, paragraph 2: FitNets proposes a general framework to transfer knowledge from a teacher network to a student network through intermediate layers. Thus, the framework itself does not require the student networks to be deeper and thinner than the teacher network.
- page 6, ""LIT can compress GANs"": authors claim to overcome limitations of KD when it comes to applying knowledge transfer to pixel-wise architecture that do not output distributions. It seems that changing the loss and using a l2 loss instead is a rather minor change, especially since performing knowledge transfer by means of l2 (although at intermediate layers) has already been explored in FitNets.

Please add references for inception and FID scores.
Please fix references format in page 10.","The sentiment of the review is generally positive, as the reviewer acknowledges that the paper is clearly written and easy to follow, and appreciates the novelty of the approach. However, the reviewer also provides several critical comments and suggestions for improvement, which slightly temper the overall positive sentiment. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer uses polite language and constructive criticism throughout the review. The reviewer phrases their suggestions and criticisms in a respectful and professional manner, which warrants a politeness score of 90.",60,90
"Summary of the paper:
This paper proposes to use structured gradient regularization to increase adversarial robustness of neural network. Here, the gradient regularization is to regularize some norm of the gradients on neural network input. ""structured"" means that instead of just minimizing the L2 norm of the gradients, a ""mahalanobis norm"" is minimized. The covariance matrix is updated continuously to track the ""structure"" of gradients/perturbations. Whitebox attack and blackbox attack 

The paper is well written, both theory and experiments are well explained. The analysis of LRC attack on SGR trained models are interesting.

However, I believe the paper has major flaws in several aspects.

The whitebox robustness evaluation is weak. Whitebox PGD with 10 iterations is not enough for discovering true robustness of a neural network, which makes the experiments unconvincing. PGD with 100 iterations and 50 random starts would make the evaluation much convincing wrt to whitebox attack. https://github.com/MadryLab/mnist_challenge
I noticed that in Table 1, the authors reported averaged results across different epsilons. Although I see the motivation to give equal weights to small and large perturbations, it makes it hard to compare with previous papers. I think the authors should a least report commonly used eps in the literature, including MNIST eps=0.1, 0.2, 0.3 and CIFAR10 eps=8/255. Currently, for MNIST eps=32/255=0.125 is much below the standard eps for benchmarking MNIST.

In my opinion, when evaluating robust optimization / gradient regularization methods, robustness under the strongest whitebox should be the major benchmark. Because ""intrinsic"" robustness is their goal. In contrast, black-box results are less important. This is because 1) evaluating black-box robustness on a few attacks hardly give any conclusive statements; 2) if we're pursuing black-box robustness, there're many randomization methods that boosts black-box robustness under various settings. How does a gradient regularization method help on top of those should be at least evaluated.
So if the paper wants to claim black-box robustness, it needs at least include experiments like 2), so it provides useful benchmarks to practitioners.

There're also a few problems in the motivation / analysis. 
""""""A remedy to these problems is through the use of regularization. The basic idea is simple: instead of sampling virtual examples, one tries to calculate the corresponding integrals in closed form, at least under reasonable approximations.""""""
The adversarial robustness problem is not about integral over a neighborhood, it is about the maximum loss over a neighborhood. This is likely why previous attempts on gradient regularization and adversarial training on FGSM attack fails. And the success is of PGD training is largely due to that the loss minimize over the adversarial example that gives the maximum loss.

""""""Thus, under the assumption that \phi \approx \phi^* and of small perturbations (such that we can ignore higher order terms.""""""
The Bayes optimal assumption seems to be arbitrary to me. If \phi is nearly Bayes-optimal, why would we worry about adversarial examples?



Other relatively minor problems

In the caption of Figure 1, """"""Covariance matrices of PGD, FGSM and DeepFool perturbations as well as CIFAR10 training set (for comparison). The short-range structure of the perturbations is clearly visible. It is also apparent that the first two attack methods yield perturbations with almost identical covariance structure.""""""
PGD and FGSM have very different attack power. If they are similar by any measure, wouldn't that mean the measure (covariance structure) is too coarse?

In Section 3.1, the paper talks about both centered and uncentered adversarial examples.
I assumed that the authors mean that the distribution of perturbations are centered?
First, I think this the authors should make this more explicit.
Second, I think this is not a realistic to assume the perturbations to be centered, because for image data, the epsilon-ball usually intersects with data domain boundary. So I'm wondering in the experiments, which version was used? centered or uncentered?

Figure 5 shows periodic patterns on covariance matrices. I didn't find explanation of the periodic patterns in the covariance matrices. It would nice if the authors can explain it or point me the relevant sections in the paper.

I don't fully get the idea of LRC attack. Is it purely sampling? are there optimization involved?

Figure 3, I suggest the authors show perturbations with different decay lengths on the same original images, which would make it easier to compare.","The sentiment of the review is mixed but leans towards the negative side. The reviewer acknowledges that the paper is well-written and finds some aspects interesting, but they also point out several major flaws and weaknesses in the evaluation and methodology. Therefore, the sentiment score is -30. The politeness of the language is generally respectful and constructive, offering specific recommendations and avoiding harsh language, so the politeness score is 70.",-30,70
"Summary: Given two sets of data, where one is unlabelled and the other is a reference data set with a particular factor of variation that is fixed, the approach disentangles this factor of variation from the others. The approach uses a VAE whose latents are split into e that represents the factor of variation and z that represents the remaining factors. A symmetric KL loss that is approximated using the density-ratio trick is optimised for the learning, and the method is applied to MNIST digit style disentangling and AffectNet facial expression disentangling.

Pros:
- Clearly written
- Results look promising, both quantitative and qualitative.

Cons:
- Mathieu et al disentangle a specific factor from others without explicit labels but by drawing two images with the same value of the specified factor (i.e. drawing from the reference set) and also drawing a third image with a any value of the specified factor (i.e. drawing from the unlabelled set). Hence their approach is directly applicable to the problem at hand in the paper. Although Mathieu et al use digit/face identity as the shared factor, their method is directly applicable to the case where the shared factor is digit style/facial expression. Hence it appears to me that it should be compared against.
- missing reference - Bouchacourt - explicit labels aren’t given and data is grouped where each group shares a factor of var. But here the data is assumed to be partitioned into groups, so there is no equivalent to the unlablled set, hence difficult to compare against for the outlined tasks.
- Regarding comparison against unsupervised disentangling methods, there have been more recent approaches since betaVAE and DIP-VAE (e.g. FactorVAE (Kim et al) TCVAE (Chen et al)). It would be nice to compare against these methods, not only via predictive accuracy of target factors but also using disentangling metrics specified in these papers.

Other Qs/comments
- the KL terms in (5) are intractable due to the densities p^u(x) and p^r(x), hence two separate discriminators need to be used to approximate two separate density ratios, making the model rather large and complicated with many moving parts. What would happen if these KL terms in (5) are dropped and one simply uses SGVB to optimise the resulting loss without the need for discriminators? Usually discriminators tend to heavily underestimate density ratios (See e.g. Rosca et al), especially densities defined on high dimensions, so it might be best to avoid them whenever possible. The requirement of adding reconstruction terms to the loss in (10) is perhaps evidence of this, because these reconstruction terms are already present in the loss (3) & (5) that the discriminator should be approximating. So the necessity of extra regularisation of these reconstruction terms suggests that the discriminator is giving poor estimates of them. The reconstruction terms for z,e in (5) appear sufficient to force the model to use e (which is the motivation given in the paper for using the symmetric KL), akin to how InfoGAN forces the model to use the latents, so the necessity of the KL terms in (5) is questionable and appears to need further justification and/or ablation studies.
- (minor) why not learn the likelihood variance lambda?

************* Revision *************
I am convinced by the rebuttal of the authors, hence have modified my score accordingly.","The sentiment of the review is generally positive, as indicated by the initial summary and the acknowledgment of the promising results. However, the reviewer also provides several critical points and suggestions for improvement, which slightly temper the overall positivity. Therefore, the sentiment score is 60. The language used in the review is polite and constructive, with the reviewer offering detailed feedback and suggestions without being rude or dismissive. The reviewer also acknowledges the authors' rebuttal and adjusts their score accordingly, which further indicates a polite and respectful tone. Therefore, the politeness score is 80.",60,80
"The authors propose a method for learning vector representations for graphs. The problem is relevant to the ICLR community. 

The paper has, however, three major problems:

The motivation of the paper is somewhat lacking. I agree that learning representations for graphs is a very important research theme. However, the authors miss to motivate their specific approach. They mention the importance of learning on smaller graphs and applying the learned models to larger graphs (i.e., extrapolating better). I would encourage the authors to elaborate on some use cases where this is important. I cannot think of any at the moment. I assume the authors had use cases in combinatorial optimization in mind? Perhaps it might make sense to motivate the use of GNNs to solve vertex cover etc. 

I’m not sure about the correctness of some of the theorems. For instance, Theorem 2 states 
“For any fixed k > 0, there exists a function f(·) and an input graph instance G such that no k-LOCAL-GATHER algorithm can compute f(G) exactly.”  I’m not claiming that this is a false statement. What I am suspecting at the moment is that the proof might not necessarily be correct. For instance, it is known that what you call 1-LOCAL-GATHER can compute the 1-Weisfeiler-Leman partition of the nodes (sometimes also referred to as the 1-WL node coloring). Now consider the chain graph 1 - 2 - 3 - 4 - 5. Here, the partition that puts together 1-WL indistinguishable nodes are {1, 5}, {2, 4} and {3}. Hence, the 1-WL coloring is able to distinguish say nodes 2 and 3 even their 1-neighborhood looks exactly the same. A similar argument might apply to your example pairs of graphs but I haven’t checked it yet in detail. What is for sure though: what you provide in the appendix is not a proper formal proof of Theorem 2. This has to be fixed. 

The experiments are insufficient. The authors should compare to existing methods on common benchmark problems such as node or graph classification datasets. Comparing to baselines on a new set of task is not enough. Why not compare your method also on existing datasets?
If you motivate your method as one that performs well on combinatorial problems (e.g., vertex cover) you should compare to existing deterministic solvers. I assume that these are often much faster at least on smaller graphs. ","The sentiment of the review is moderately negative. The reviewer acknowledges the relevance of the problem but points out three major issues: lack of motivation, questionable correctness of theorems, and insufficient experiments. The sentiment score is -40 because the review is critical but not entirely dismissive. The politeness score is 50 because the reviewer uses polite language, such as 'I would encourage,' 'I’m not sure,' and 'I assume,' which softens the criticism.",-40,50
"This paper builds on the (Alemi et al 2018) ICML paper and presents a formal framework for representation learning. The authors use a graphical model for their representation learning task and use basic information theoretic inequalities to upper-bound their measure of performance which is a KL divergence. The authors then define the optimal frontier which corresponds to the lowest possible upper-bound and write it as an optimization problem. Written with Lagrange multipliers, they obtain several known cost functions for different particular choices of these parameters.
Then the authors make a parallel with thermodynamics and this part is rather unclear to me. As it is written, this section is not very convincing:
- section 4.1 after equation (27) which function is 'smooth and convex'? please explain why.
- section 4.1 '...the actual content of the law is fairly vacuous...'
- section 4.2 the explanation of equation (30) is completely unclear to me. Please explain better than 'As different as these scenarios appear (why?)...'
- section 4.2 'Just as in thermodynamics, these susceptibilities may offer useful ways to characterize...'
- section 4.2 'We expect...'
- section 4.3 ends with some unexplained equations.
As illustrated by the examples above, the reader is left contemplating this formal analogy with thermodynamics and no hint is provided on how to proceed from here. 

","The sentiment of the review appears to be neutral to slightly negative. The reviewer acknowledges the framework presented by the authors but expresses significant confusion and dissatisfaction with the clarity and convincingness of the thermodynamics analogy section. The sentiment score is therefore -20. The politeness of the language used is relatively neutral. The reviewer uses phrases like 'please explain' and 'completely unclear to me,' which are direct but not impolite. The politeness score is 0.",-20,0
"This paper seems to be an exposition on the primary performance affecting aspects of generative adversarial networks (GANs).  This can possibly affect our understanding of GANs, helping practitioners get the most in their applications, and perhaps leading to innovations that positively affect GAN performance.

Normally, expositions such as this I find difficult to recommend for publication. In these times, one can find ""best practices"" with a reasonable amount of rigor on data science blogs and such. An exposition that I would recommend for publication, would need to exhibit a high sense of depth and rigor for me to deem it publication worthy. This paper, for me, achieves this level of quality.

The authors start off by giving a precise, constrained list of hyperparameters and architectural components that they would explore. This is listed in the title and explained in detail in the beginning of the paper. The authors are right in explaining that they could not cover all hyperparameters and chose what I feel are quite salient ones. My one ask would have been a survey of how activations might affect performance. I sense that everyone has settled upon LeakyReLUs for internal layers, but a survey of that work and experimentation within the authors' framework would have been nice.

The authors then explain the metrics for evaluation and datasets. The datasets offered a healthy variety for typical image recognition tasks. It would be interesting to see what these metrics would reveal when applied to other types of data (e.g. scientific images).

The  authors explain, with graphs, the results of the loss, normalization, and architectures. I feel the discussion on loss was rushed, and I gained no insight on what the authors thought was a prominent difference between the three losses studied. Perhaps the authors had no salient observations for loss, but explicitly stating such would be useful to the reader. The only observation I gained as far as this is that non-saturating loss would possibly be stable across various datasets.

Regularization and normalization are discussed in much more detail, and I think the authors made helpful and interesting observations, such as the benefits of spectral normalization and the fact that batch normalization in the discriminator might be a harmful thing. These are good takeaways that could be useful to a vast number of GANs researchers.

For architectures to be a main pillar of the paper, I feel that this area could have been explored in greater detail. I feel that this discussion devolved into a discussion, again, about normalization rather than the architectural differences in performance. Unless I am misunderstanding something, it seems that the authors simply tested one more architecture, for the express purpose of testing whether their observations about normalization would hold.

As a bonus, the authors bring up some problems they had in making comparisons and reproducing results. I think this is an extremely important discussion to have, and I am glad that the authors detailed the obstacles in their journey. Hopefully this will inspire other researchers to avoid adding to the complications in this field.

The graphs were difficult to parse. I was able to make them out, but perhaps separating the top row (FID and diversity graphs) into separate figures, separate lines, or something would have reduced some confusion. In addition, different charts presenting only one loss function, with their spectral normalization and gradient penalty variants, would have made the effects of the normalization more obvious on the FID distribution graphs. If this can be changed before publication, I would strongly suggest it.

I appreciate that the authors provided source code via GitHub. However, in the future, the authors should be careful to provide an anonymous repository for review purposes. I had to be careful not to allow myself to focus on the author names which are prominent in the repository readme, and one of whom has his/her name in the GitHub URL itself. I didn't immediately recognize the names and thus it was easy for me not to retain them or focus on them. However, if it had been otherwise, it might have risked biasing the review.

In all, I think this is a good and useful paper from which I have learned and to which I will refer in the future as I continue my research into GANs and VAEs. I would suggest changing the title to be more appropriate and accurate (the researchers are primarily focused on showing the positive and negative effects of normalization across various loss functions and architectures). But altogether, I believe this is a paper worth publishing at ICLR.","The sentiment of the review is generally positive, as the reviewer acknowledges the paper's quality and usefulness, and ultimately recommends it for publication. However, there are several critical points and suggestions for improvement, which slightly temper the overall positivity. Therefore, the sentiment score is 70. The politeness of the language is high, as the reviewer provides constructive feedback in a respectful and professional manner, using phrases like 'I appreciate,' 'I would suggest,' and 'hopefully this will inspire.' Thus, the politeness score is 90.",70,90
"-> Summary

The authors propose to extend the analysis of Shwartz-Ziv & Tishby on the information bottleneck principle in artificial neural network training to realistic large-scale settings. They do so by replacing otherwise intractable quantities with tractable bounds in forms of classifiers for I(y;h) and Pixel CNNs for I(x;h). In conclusion, they observe two phases during training, one that maximizes mutual information between input and hidden representation and a second one that compresses the representation at the end of training, in line with the predictions from toy tasks of Shwartz-Ziv & Tishby.

-> Quality

The paper is very well written, all concepts are well-motivated and explained.

-> Significance

The main novelty is to replace intractable quantities in the analysis of the information bottleneck with tractable bounds in form of auxiliary models. The idea is neat and makes a lot of sense. On the other hand, some of the results and the bounds themselves are well-known and can thus not be considered novel. The main contribution is thus the empirical analysis itself and given some overly confident claims on qualitative results and missing ablation on the quantitative side, I am not convinced that the overall results are very conclusive.

-> Main Concerns

The authors make a lot of claims about the qualitative diversity of samples from deeper layers h4 of the network as compared to h1 and h2. However, I do not agree with this. When I look at the samples I see a lot of variations early in training and also in layers h1 and h2. The difference to h4 seems marginal at best and not as clear cut as the authors present it. Thus, these claims should be softened.

In figure 1 I tend to say that samples at epoch 1 are more varied than at epoch 200. In figure 5 (b) seems pretty color invariant and not only (f) as claimed. In fact (f) seems pretty stable and consistent to me.

The bound in equation (2) might be quite loose, depending on the quality of the classifier or pixel CNN. Even though there is no way to test this, it should be discussed.

What is the effect of weight decay here? I suspect that weight decay plays a crucial role in the final compression phase observed in e.g. figure 3 (c), but might not be a necessary condition to make the network generalize. An ablation experiment verifying or falsifying this statement would be important to conduct and without it I am not convinced that the shown curves are conclusive.

-> Minor

- You seem to use a weird math font, is this on purpose? It does not seem to be the ICLR standard.
- The bound in equation (2) is a standard variational bound and has been used many times, the authors make it sound like it is their contribution. You should maybe cite basic work and recent work on variational information bottleneck here.","The sentiment of the review is mixed. The reviewer acknowledges the strengths of the paper, such as its well-written nature and the neatness of the idea, which suggests a positive sentiment. However, the reviewer also points out significant concerns and expresses doubts about the conclusiveness of the results, which introduces a negative sentiment. Therefore, the sentiment score is balanced at 20. The politeness of the language is high. The reviewer uses polite and constructive language throughout the review, even when pointing out flaws and making recommendations. Thus, the politeness score is 80.",20,80
"First off, the paper presents a relatively straight-forward extension to video from the work done in image compression. The work uses 3D volumes instead of 2D images, and exploits this structure by adding a secondary network to both the encoder/decoder.

The work is therefore *marginally* novel, but it is one of the first to propose neural methods for compressing video.

My biggest complaint about this paper, however, is about evaluation. I don't think it's possible to take this paper seriously as is, due to the fact that the metrics use in the evaluation are absolutely skipped.

Given that this is such a crucial detail, I don't think we can accept this paper as is. The metrics need to be described in detail, and they should follow some previously used protocols (see below). 

For example, in libvpx and libaom (which is the current best performing method for video compression - AV1), there are two versions of PSNR: Global and Average PSNR respectively, and this is what gets reported in publications/standards meetings.

Global PSNR: Compute MSE for the entire sequence combining Y, Cb, Cr components, and then compute PSNR based on the combined MSE.
Average PSNR: Compute MSE for each frame combining Y, Cb, Cr, components; then compute PSNR for the frame based on the combined MSE and cap it to a max of 100. Then average the PSNR over all the frames.

MPEG uses something like computing Average PSNR for each component (similar to what I mentioned above, but for each component) and then combine the Y-, Cb- and Cr- PSNRs using a weighted average. For 420 that will be equivalent to [4*MSE(y) + MSE(Cb) + MSE(Cr)/6. For 422 that will be equivalent to [2*MSE(y) + MSE(Cb) + MSE(Cr)/4. For 444 that will be equivalent to [MSE(y) + MSE(Cb) + MSE(Cr)/3.  Additionally, when using YCbCr, the authors also need to refer to which version of the color standard is employed, since there are multiple ITU recommendations, all of which differ in how to compute the color space transforms.

Please note that video codecs DO NOT OPTIMIZE FOR RGB reconstruction (humans are much more sensitive to brightness details than they are to subtle color changes), so comparing against them in that color space puts them at a distinct disadvantage. In the video compression literature NOBODY reports RGB reconstruction metrics.

Please note that I computed the PSNR (RGB) for H.264, on the resized MCL-V dataset (640x360) as the authors proposed and I observed that the metric has been ***MISREPRESENTED*** by up to 5dB. This is absolutely not OK because it makes the results presented not be trustworthy at all.

Here is the bpp/RGB PSNR that I obtained for H.264 (for completeness, this was computed as follows: used version 3.4.2 of ffmpeg, and the command line is ""ffmpeg -i /tmp/test.y4m -c:v h264 -crf 51 -preset veryslow"", tried many settings for crf  to be able to get roughly the same bpp per video, then compute RGB PSNR for each frame per video, aggregate over each video, then average cross videos):

BPP, Average PSNR RGB (again, not a metric I would like to see used, but for comparison's sake, I computed nonetheless -- also, note that these numbers should not be too far off from computing the average across all frames, since the video length is more or less the same)):
0.00719, 23.46
0.01321, 26.38
0.02033, 28.92
0.03285, 31.14
0.05455, 33.43

Similar comments go for MS-SSIM. 

Lastly, it is unfair to compare against H263/4/5 unless the authors specify what profiles were used an what kind of bitrate targeting methods were used. ","The sentiment of the review is mixed. While the reviewer acknowledges the marginal novelty and the pioneering aspect of the work in neural methods for video compression, they express significant concerns about the evaluation metrics, which they find unacceptable and misleading. This results in a sentiment score of -20. The politeness of the language is generally neutral to slightly negative. The reviewer uses strong language such as 'absolutely skipped,' 'absolutely not OK,' and 'MISREPRESENTED,' which indicates frustration and a lack of politeness. However, the review is still professional and provides detailed feedback, resulting in a politeness score of -20.",-20,-20
"The paper considers the problem of obtaining reliable predictive uncertainty estimates. The authors propose noise contrastive priors — the idea being to explicitly encourage high uncertainties for out of distribution (OOD) data through a loss in the data space.  OOD data is simulated by adding noise to existing data and the model is trained to maximize the likelihood wr.t. training data while being close in the KL sense to a (wide) conditional prior p(y | x) on the OOD responses (y).  The authors demonstrate that the procedure leads to improved uncertainty estimates on toy data and can better drive active learning on a large flight delay dataset.

The paper is well written and makes for a nice read. I like the idea of using “pseudo” OOD data for encouraging better behaved uncertainties away from the data. It is nice to see that even simple schemes for generating OOD data (adding iid noise) lead to improved uncertainty estimates. 

My main concern about this work stems from not knowing how sensitive the recovered uncertainties are to the OOD data generating mechanism and the parameters thereof. The paper provides little evidence to conclude one way or the other.  The detailed comments below further elaborate on this concern.

Detailed Comments: 
a) I like the sensitivity analysis presented in Figure 4, and it does show for the 1D sine wave the method is reasonably robust to the choice of \sigma_x. However, it is unclear how problem dependent the choice of sigma_x is. From the experiments, it seems that \sigma_x needs to be carefully chosen for different problems, \sigma^2_x < 0.3 seems to not work very well for BBB + NCP for the 1D sine data, but for the flight delay data \sigma^2_x is set to 0.1 and seems to work well. How was \sigma_x chosen for the different experiments?

b) It is also interesting that noise with a shared scale is used for all 8 dimensions of the flight dataset. Is this choice mainly governed by convenience — easier to select one hyper-parameter rather than eight? 

c) Presumably, the predictive uncertainties are also strongly affected by both the weighting parameter \gamma and the prior variance sigma^2_y . How sensitive are the uncertainties to these and how were these values chosen for the experiments presented in the paper? 

d) It would be really interesting to see how well the approach extends to data with more interesting correlations. For example, for image data would using standard data-augmentation techniques (affine transformations) for generating OOD data help over adding iid noise. In general, it would be good to have at least some empirical validation of the proposed approach on moderate-to-high dimensional data (such as images).

==============
Overall this is an interesting paper that could be significantly strengthened by addressing the comments above and a more careful discussion of how the procedure for generating OOD data affects the corresponding uncertainties.","The sentiment of the review is generally positive, as indicated by phrases like 'well written,' 'nice read,' and 'I like the idea.' However, the reviewer also expresses some concerns and provides constructive criticism, which slightly tempers the overall positivity. Therefore, the sentiment score is 60. The politeness of the language is very high, as the reviewer uses polite phrases such as 'I like,' 'It is nice to see,' and 'It would be really interesting to see,' and provides feedback in a respectful manner. Thus, the politeness score is 90.",60,90
"Analysis of Spectral Bias of ReLU networks

The paper uses Fourier analysis to study ReLU network utilizing its continuous piecewise linear structure.

Main finding is that these networks are biased towards learning low frequency which authors denote `spectral bias’.  This provides another theoretical perspective of neural networks preferring more smooth functions while being able to fit complicated function. Also shows that in terms of parameters networks representing lower frequency modes are more robust. 

Pro: 
- Nice introduction to Fourier analysis providing non-trivial insights of ReLU networks.
- Intuitive toy experiments to show spectral bias and its properties 
- Thorough theoretical analysis and empirical support

Con: 
- The analysis is clearly for ReLU networks although the title may provide a false impression that it corresponds to general networks with other non-linearities. It is an interesting question whether the behaviour characterized by the authors are universal. 
- At least for me, Section 4 was not as clearly presented as other section. It takes more effort to parse what experiments were conducted and why such experiments are provided.
- Although some experiments on real dataset are provided in the appendix, I personally could not read much intuition of theoretical findings to the networks used in practice. Does the spectral bias suggest better way of training or designing neural networks for example?

Comments/Questions:
- In Figure 1, two experiments show different layerwise behaviour, i.e. equal amplitude experiment (a) shows spectral norm evolution for all the layers are almost identical whereas in increasing amplitude experiment (b) shows higher layer change spectral norm more than the lower layer. Do you understand why and does Fourier spectrum provide insights into layerwise behaviour?
- Experiment 3 seems to perform binary classification using thresholding to the logits. But how do you find these results also hold for cross-entropy loss?
“The results confirm the behaviour observed in Experiment 2, but in the case of classification tasks with categorical cross-entropy loss.”


Nit: p3 ReLu -> ReLU / p5 k \in {50, 100, … 350, 400} (close bracket) / p5 in Experiment 2 and 3 descriptions the order of Figure appears flipped. Easier to read if the figure appears as the paper reads / p7 Equation 11 [0, 1]^m


********* updated review *************

Based on the issues raised from other reviewers and rebuttal from authors, I started to share some of the concerns on applicability of Thm 1 in obtaining information on low k Fourier coefficients. Although I empathize author's choice to mainly analyze synthetic data, I think it is critical to show the decays for moderately large k in realistic datasets. It will convince other reviewers of significance of main result of the paper.
","The sentiment of the review is generally positive, as indicated by the praise for the introduction, intuitive experiments, and thorough analysis. However, there are some critical points raised, particularly regarding the clarity of Section 4 and the applicability of the findings to real-world datasets. Therefore, the sentiment score is 50. The language used in the review is polite and constructive, with suggestions for improvement and specific questions for clarification. The reviewer also acknowledges the authors' efforts and provides detailed feedback, which indicates a politeness score of 80.",50,80
"This paper uses deep reinforcement learning (DRL) for malware detection. It can get better performance than LSTM or GRU based models.

Deep reinforcement learning (DRL) has already used for classification or detection. I am not sure about the main contribution of this work. The new application of DRL can not convince me.

As the dataset is not a public dataset, it is difficult to evaluate the performance. As for the comparing models, i think some CNN based methods should be included. If the task is a detection, i think some attention methods should also be investigated and compared. LSTM combined with attention should already be well investigated in other classification/detection tasks.","The sentiment of the review is slightly negative. The reviewer expresses doubts about the main contribution of the work and is not convinced by the new application of DRL. Additionally, the reviewer points out the difficulty in evaluating the performance due to the use of a non-public dataset and suggests the inclusion of CNN-based methods and attention methods for comparison. The politeness of the language is neutral; the reviewer does not use any harsh or rude language but also does not include any particularly polite phrases.",-40,0
"This paper empirically finds that the distribution of activations in quantized networks follow  Gaussian or Laplacian distribution, and proposes to determine the optimal clipping factor by minimizing the quantization error based on the distribution assumption.

The pros of the work are its simplicity, the proposed clipping and quantization does not need additional re-training. However, while the key of this paper is to determine a good clipping factor, the authors use uniform density function to represent the middle part of both Gaussian and Laplacian distributions where the majority of data points lie in, but exact computation for the tails of the distributions at both ends. Thus the computation of quantization error is not quite convincing. Moreover, the authors do not compare with the other recent works that also clip the activations, thus it is hard to validate the efficacy of the proposed method.

For the experiments, the authors mention that a look-up table can be pre-computed for fast retrieval of clipping factors given the mean and sigma of a distribution.  However, the mean and sigma are continuous numbers, how is the look-up table made?  Moreover, how is the mean and std estimated for each weight tensor and what is  the complexity?
","The sentiment of the review is slightly negative. While the reviewer acknowledges the simplicity and the lack of need for additional re-training as positive aspects, they express significant concerns about the methodology and the lack of comparison with recent works. This results in a sentiment score of -30. The politeness of the language is neutral to slightly polite. The reviewer raises their concerns in a straightforward manner without using harsh or rude language, resulting in a politeness score of 20.",-30,20
"The author proposed an extended version of MNIS where they introduced thickening/thinning/swelling/fracture. The operation is done using binary morphological operations.

* Providing benchmark data for tasks such disentanglement is important but I am not sure generating data is sufficient contribution for a paper. 
* I am not sure what conclusion I should draw from Fig 5 and Fig 6 about the data.
* Eventually this data can become a benchmark data when it is paired with a method. Then that method/data are a benchmark.

 ","The sentiment of the review appears to be slightly negative, as the reviewer expresses doubts about the sufficiency of the contribution and the clarity of the conclusions. The sentiment score is therefore -30. The politeness of the language is neutral; the reviewer does not use any rude or overly critical language but also does not use particularly polite or encouraging language. The politeness score is 0.",-30,0
"# Summary

This paper proposes to improve the sample efficiency of transfer learning for Deep RL by mapping a new visual domain (target) onto the training one (source) using GANs. First, a deep RL policy is trained on a source domain (e.g., level 1 of the Atari Road Fighter game). Second, a GAN (e.g. UNIT or CycleGAN) is trained for unsupervised domain adaptation from target images (e.g., level 2 of Road Fighter) to source ones. Third, the policy learned in the source domain is applied directly on the GAN-translated target domain. The experimental evaluation uses two Atari games: i) transfer from Breakout to Breakout with static visual distractors inpainted on the screen, ii) from one Road Fighter level to others. Results suggest that this transfer learning approach requires less images than retraining from scratch in the new domain, including when fine-tuning does not work.


# Strengths

Controlled toy experiments of Deep RL generalization issues:
The experiments on Breakout quantify how badly A3C overfits in this case, as it shows catastrophic performance degradation even with trivial static visual input perturbations (which are not even adversarial attacks). The fine-tuning experiments also quantify well how brittle the initial policy is, motivating further the importance of the problem studied by the paper.

Investigating the impact of different GANs on the end task:
The experiments evaluate two different image translation algorithms: one based on UNIT, the other based on CycleGAN. The results suggest that this choice is key and depends on the target domain. This suggests that the adaptation is in fact task dependent, confirming the direction pursued by others in task-specific unsupervised domain adaptation (cf. below).


# Weaknesses

Discrepancy between quantitative and qualitative results:
The good quantitative results (accumulated rewards) reported in the experiments are not reflected in the qualitative results. As can be seen from the videos, these results seem more to be representative of a bias in the data. For instance, in the Road Fighter videos, one can clearly see that the geometry of the road (width, curves) and dynamic obstacles are almost completely erased in the image translation process. The main reasons the quantitative results are good seem to be i) in the non-translated case the agent crashes immediately, ii) the ""translated"" image is a wide straight road identical to level 1 where the policy just keeps the car in the middle (thus crashing as soon as there is a turn or a collision with an obstacle). Even in the breakout case, there are catastrophic translation failures for some of the studied variations although the domain gap is static and small. The image translation results look underwhelming compared to state of the art GANs used for much more complex tasks and environments (e.g., the original CycleGAN paper and follow-up works, or the ICLR'18 progressive growing of GANs paper). This might be due to a hyper-parameter tuning issue, but it is unclear why the adaptation results seem not on par with previous results although the paper is in a visually simpler domain (Atari games).

Does not address the RL generalization issues:
Although it is the main goal of the paper, the method is fundamentally side-stepping the problem as it does not improve in any way the policy or the Deep RL algorithm (they are left untouched). It is mapping the target environment to the source one, without consideration for the end task besides tuning GAN hyper-parameters. If the initial policy is very brittle (as convincingly shown in section 2), then just mapping to the source domain does not improve the generalization capabilities of the Deep RL algorithm, or even improves transfer learning: it just enables the policy to be used in other contexts that can be reduced to the training one (which is independent of the learning algorithm, RL or otherwise). So it is unclear whether the main contribution is the one claimed. The contribution seems instead an experimental observation that it might be easier to reduce related domains to the training one instead of retraining a new (specialised and brittle) policy. Existing works have actually gone further, learning jointly the image translation and task network, including for very challenging problems, e.g. in unsupervised sim-to-real visual domain adaptation (e.g., Unsupervised Pixel-Level Domain Adaptation with Generative Adversarial Networks from Bousmalis et al at CVPR'17, which is not cited here).

Experimental protocol:
The experimental conclusions are not clear and lack generality, because the optimal methods (e.g., choice of GAN, number of iterations) vary significantly depending on the task (cf. Table 3 for instance). Furthermore, the best configurations seem selected on the test set for every experiment.

Data efficiency vs actual training efficiency:
The main claim is that it is better to do image translation instead of fine-tuning or full re-training. The basis of that argument is the experimentally observed need for less frames to do the image translation (Table 2). However, it is not clear that training GANs for unsupervised image translation is actually any easier / faster. What about training instability, mode collapse, hyper-parameter tuning, and actual training time comparisons on the same hardware?



# First Recommendation

Using image translation via GANs for unsupervised domain adaptation is a popular idea, used in the context of RL for Atari games here. Although the experiments show that mapping a target visual domain to a source one can enable reusing a deep RL policy as is, the qualitative results suggest this is in fact due to a bias in the data used here and the experimental protocol does not yield general insights. Furthermore, this approach is not specific to RL and its observed generalization issues. It does not improve the learning of the policy or improve its transferability, thus having only limited new insights compared to existing approaches that jointly learn image translation and target task-specific networks in much more challenging conditions.

I believe this submission is at the start of an interesting direction, and requires further work on more challenging tasks, bigger domain gaps, and towards more joint training or actual policy transfer to go beyond this first set of encouraging but preliminary results.


# Post-rebuttal Recommendation

Thanks to the authors for their detailed reply. The clarifications around overfitting, UNIT-GAN in Section 4, and the paper claims are helpful. I  also agree that the quantitative experiments are serious. I have bumped my score by +1 as a result.

Nonetheless, the results still seem preliminary and limited in scope for the aforementioned reasons. The discussion in the comments about the learned policies and transfer are ad-hoc. A lot of the shortcomings mentioned in the review are outright dismissed (e.g., ""de facto standard in RL""), downplayed (esp. generalization, which is puzzling for a transfer learning paper), or left for future work.

As there is no strong technical contribution beyond the experimental observations in the current submission, I suggest the authors try to address the GAN shortcomings both mentioned in reviews and their reply, instead of  just observing  / reporting them. As this paper's main focus is to use image translation in the proposed RL setting (with standard GAN and RL methods), I do not think it is just someone else's problem to improve the image translation part. Proposing a technical contribution there would make the paper much stronger and appealing to a broader ICLR audience.  This might also require adding a third game to ensure more generalizable experimental insights.","The sentiment of the review is mixed but leans towards the positive side. The reviewer acknowledges the strengths of the paper, such as the controlled experiments and the investigation of different GANs, but also points out significant weaknesses, including the discrepancy between quantitative and qualitative results, and the lack of addressing RL generalization issues. The sentiment score is therefore set at 10, indicating a slightly positive but cautious outlook. The politeness of the language is high; the reviewer uses polite and constructive language throughout, even when pointing out weaknesses. The reviewer also thanks the authors for their detailed reply and provides suggestions for improvement in a respectful manner. Therefore, the politeness score is set at 90.",10,90
"In this paper the authors propose a method called “All learning rates at once” (Alrao) which aims to save the time needed to tune learning rate for DNN models testing. The method sets individual learning rate to each feature in each layer of a network using the values sampled from truncated log-uniform distribution. The only cost of the method is the creation of several branches of the classifier layer. Each of the branches is trained with a predefined learning rate value, and the final predictions are obtained by model averaging. In the presented experiments Alrao demonstrates performance comparable to SGD with optimal learning rate and more stable results compared to Adam. The authors indicate limitations of Alrao caused by the overhead in the final layer which complicates the application of the method for models with large classifier layer.

Overall, the paper is written clearly and organized well. However, Equation (2) needs to be corrected. The denominator in the normalizing constant of log-uniform distribution should be \log\eta_{max} - \log\eta_{min}.

My main concern is related to the experimental evaluation of the method. I find the experimental evidence for the effectiveness of Alrao insufficient. As the authors propose to employ the method to quickly evaluate models and select best models to further training it would be beneficial to have more results in order to ensure that the method is reliable in this setting. Other demonstrations which would show possibly that the method enhances performance of architecture search methods may emphasize significance of the proposed method. Also, more experiments comparing Alrao against sampling learning rates per weight are needed. Given the current results, it is still unclear whether the proposed method performs better. Finally, I recommend to include comments explaining how much more time is needed in practice to train model with Alrao compared to SGD training.","The sentiment of the review is generally positive, as the reviewer acknowledges that the paper is clearly written and well-organized. However, the reviewer also points out several areas for improvement, particularly in the experimental evaluation of the method. This suggests a balanced sentiment, leaning slightly positive. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, offering specific recommendations for improvement without being harsh or dismissive.",30,80
"Review: This paper proposed ""Dopamine"", a new framework for DeepRL.  While this framework seems to be useful and the paper seems like a useful guide for using the framework, I didn't think that the paper had enough scientific novelty to be an ICLR paper.  I think that papers on novel frameworks can be suitable, but they should demonstrate that they're able to do something or provide a novel capability which has not been demonstrated before.  

Strengths: 

-Having a standardized tool for keeping replay buffers seems useful.  

-The Dopamine framework is written in Python and has 12 files, which means that it should be reasonably easy for users to understand how it's functioning and change things or debug.  

-The paper has a little bit of analysis of how different settings effect results (such as how to terminate episodes) but I'm not sure that it does much to help us in understanding the framework.  I suppose it's useful to understand that the settings which are configurable in the framework affect results?  

-The result on how sticky actions affect results is nice but I'm not sure what it adds over the Machado (2018) discussion.  

Weaknesses: 

-Given that the paper is about documenting a new framework, it would have been nice to see more comprehensive baselines documented for different methods and settings.  

-I don't understand the point of 2.1, in that it seems somewhat trivial that research has been done on different architectures and algorithms.  

-In section 4.2, I wonder if the impact of training mode vs. evaluation mode would be larger if the model used a stochastic regularizer.  I suspect that in general changing to evaluation mode could have a significant impact.  
","The sentiment of the review is mixed but leans towards the negative side. The reviewer acknowledges some strengths of the paper, such as the usefulness of the framework and its ease of use, but ultimately concludes that the paper lacks sufficient scientific novelty for acceptance. This results in a sentiment score of -30. The language used in the review is generally polite and constructive, offering specific feedback and suggestions for improvement without being harsh or dismissive. Therefore, the politeness score is 70.",-30,70
"Quality (5/10): This paper proposes DppNet, which approximates determinantal point processes with deep networks by inhibitive attention mechanism. The authors provided a theoretical analysis under some condition that the DppNet is of log-submodularity.

Clarity (9/10): This paper is well written and provides a clear figure to demonstrate their network architecture.

Originality (6/10): This paper is mainly based on the work [Vaswani et al, Attention is all you need, 2017]. It computes the dissimilarities by subtracting attention in the original work from one, and then samples a subset by an unrolled recurrent neural network. 

Significance (5/10): This paper uses negative log-likelihood as the measurement to compare DppNet with other methods. Without further application, it is difficult to measure the improvement of this method over other methods.

Pros: 
(1) This paper is well written and provides a figure to clearly demonstrate their network architecture.

(2) This paper provides a deep learning way to sample a subset of data from the whole data set and reduce the computation complexity.

There are some comments.
(1) Figure 4 shows the sampled digits from Uniform distribution, DppNet (with Mode) and Dpp. How about the sampled digits from k-Medoids? Providing the sampled digits from k-Medoids can make the experiments more complete.

(2) The object of DppNet is to minimize the negative log-likelihood. The DPP and k-Medoids have other motivations, not directly optimizing the negative log-likelihood. This may be the reason why DppNet has a better performance on negative log-likelihood, even than DPP. Could the authors provide some other measures (like the visual comparison in figure 4) to compare these methods?

(3) Does GenDpp Mode in Table 2 mean the greedy mode in Algorithm 1? A clear denotation can make it more clear.","The sentiment of the review is moderately positive. The reviewer acknowledges the clarity and the well-written nature of the paper, as well as the innovative approach to reducing computational complexity. However, the reviewer also points out that the originality and significance of the work are somewhat limited, and provides constructive feedback for improvement. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, offering specific suggestions for improvement without being harsh or dismissive.",50,80
"The paper considers adaptive regularization, which has been popular in neural network learning.  Rather than adapting diagonal elements of the adaptivity matrix, the paper proposes to consider a low-rank approximation to the Gram/correlation matrix.

When you say that full-matrix computation ""requires taking the inverse square root"", I assume you know that is not really correct?  As a matter of good implementation, one never takes the inverse of anything.  Instead, on solves a linear system, via other means.  Of course, approximate linear system solvers then permit a wide tradeoff space to speed things up.

There are several issues convolved here: one is ``full-matrix,'' another is that this is really a low-rank approximation to a matrix and so not full matrix, another is that this may or may not be implementable on GPUs.  The latter may be important in practice, but it is orthogonal to the full matrix theory.

There is a great deal of discussion about full-matrix preconditioning, but there is no full matrix here.  Instead, it is a low-rank approximation to the full matrix.  If there were theory to be had here, then I would guess that the low-rank approximation may work even when full matrix did not, e.g., since the full matrix case would involve too may parameters.

The discussion of convergence to first order critical points is straightforward.

Adaptivity ratio is mentioned in the intro but not defined there.  Why mention it here, if it's not being defined.

You say that second order methods are outside the scope, but you say that your method is particularly relevant for ill-conditioned problems.  It would help to clarify the connection between the Gram/correlation matrix of gradients and the Hessian and what is being done to ill-conditioning, since second order methods are basically designed for ill-conditioned problems..

It is difficult to know what the theory says about the empirical results, given the tweaks discussed in Sec 2.2, and so it is difficult to know what is the benefit of the method versus the tweaks.

The results shown in Figure 4 are much more interesting than the usual training curves which are shown in the other figures.  If this method is to be useful, understanding how these spectral properties change during training for different types of networks is essential.  More papers should present this, and those that do should do it more systematically. 

You say that you ""informally state the main theorem.""  The level of formality/informality makes it hard to know what is really being said.  You should remove it if it is not worth stating precisely, or state it precisely.  (It's fair to modularize the proof, but as it is it's hard to know what it's saying, except that your method comes with some guarantee that isn't stated.)","The sentiment of the review is mixed but leans towards negative. The reviewer points out several issues and inaccuracies in the paper, such as the incorrect statement about full-matrix computation and the lack of clarity in the discussion of convergence and adaptivity ratio. The reviewer also mentions that the empirical results are difficult to interpret due to the tweaks discussed in the paper. However, the reviewer does acknowledge that the results shown in Figure 4 are interesting and suggests that more papers should present such results systematically. The politeness of the language is relatively neutral, with some polite suggestions for improvement, such as clarifying the connection between the Gram/correlation matrix and the Hessian and stating the main theorem more precisely.",-30,10
"Summary
------------------

The authors propose a new method to sparsify DNNs based on a dropout induced by a Beta-Bernoulli prior. They further propose a data-dependent dropout by linking the Beta-Bernoulli prevalence to the inputs, achieving a higher sparsification rate. In the experimental section they show that the proposed method achieves better compression rates than other methods in the literature. However, experiments against some recent methods are missing. Also, some additional experiments using data-dependent dropouts not based on the Beta-Bernoulli prior would help to better disentangle the effects of the two contributions of the paper. Overall, the paper is well-written but the mentioning of the IBP is confusing. The authors devote quite a bit of space to the IBP when it is actually not used at all.

 Detailed comments
-------------------------

1)	Introduction

The paper is well motivated and the introduction of the paper clearly states the two main contributions of the paper: a Beta-Bernoulli dropout prior and a dependent Beta Bernoulli dropout prior. 

2)	Background

Section 3.1 is a nice summary of variational inference for BNNs. On the other hand, Section 3.2 is misleading. The authors use this section to introduce the IBP process (a generative sequential process to generate samples from a random measure called the Beta-Bernoulli process). However, this is not used in the paper at all. Then they introduce the Beta-Bernoulli prior as a finite Beta-Bernoulli process. I find this quite convoluted. I would suggest to introduce the Beta-Bernoulli distribution as a prior directly, and state that for alpha/K this is a sparse-inducing prior (where the average number of features is given by \frac{\alpha}{1 + \frac{\alpha}{K} ). No need to mention the IBP or the Beta Bernoulli process. 

3)	Main Contribution

I think the design of a link function that allows to implement a data-dependent Beta-Bernoulli dropout is one of the keys of the paper and I would suggest that the author clearly state this contribution at the beginning of the paper. I would also like to see the application of this link-function to other sparsity inducing priors different than the Beta-Bernoulli. This would allow to further understand the data-dependent contribution to the final performance and how transferable this is to other settings. Also, Have the authors try to train the data-dependent Beta-Bernoulli from scratch, i.e. without the two steps approach? I am assuming the performance is worse, but I would publish the results for completeness.

4)	Experiments

The main issues with the experimental section are:
a)	I am missing some recent methods (some of them even cited in the related work section): e.g. Louizos et al. (2017). I would be interested in comparisons against the horshoe-prior and a data-dependent version of it. Also, a recent paper based on the variational information bottleneck have been recently published outperforming the state of the art in the field (http://proceedings.mlr.press/v80/dai18d.html).
b)	Table 1 should report the variance or other uncertainty measure: Given that they run the experiments 5 times, I do not understand why they only report the median. I would encourage the authors to publish the mean and the variance (at least).
In addition, one of my main question about the method is, once the network has been sparsified, how does this translate into a real performance improvement (in terms of memory and speed). In term of memory, you can always apply a standard compression algorithm. If the sparsity is about a certain threshold, you can resort to sparse-matrix implementations. However, regarding the speed only when you reach a certain sparsity level you would get a tangible improvement if your DL framework support sparse matrices. However, if you get an sparsity level below this threshold, e.g. 20%, you cannot resort to sparse matrices and therefore you would not get a speed improvement, unless you enforce structure sparsity or you optimize to low-level matrix multiplication routines. Are the Speedup/Memory results reported in Table 1 real or theoretical?

","The sentiment of the review is generally positive, as the reviewer acknowledges the novelty and potential of the proposed method, as well as the clarity of the paper's main contributions. However, the reviewer also points out several areas for improvement, such as the need for additional experiments and clarifications. The politeness of the language is high, as the reviewer provides constructive feedback and suggestions in a respectful and professional manner, without any harsh or dismissive language.",60,80
"In this paper, the authors introduce a sampling strategy that aims to combine the benefits of with- and without-replacement SGD. With-replacement strategies add more randomness to the process, which the authors claim helps convergence, while the without-replacement strategies ensure equal usage of all datapoints. The authors present numerical results showing better convergence and improved final accuracy. While I found the idea appealing, I felt that the paper needs more work before it can be published. I detail some of my primary concerns below:

- The entire motivation of the paper is predicated on the hypothesis that more randomness is better for training. This is not generally true. Past work has shown that specific kinds of random noise aid convergence through exploration/saddle point avoidance/escaping spurious minima while others either make no change, or hurt. Noise from sampling tends to be a structured noise that aids exploration/convergence over batch gradient descent, but it is not immediately clear to me why the choice between with- and without-replacement should imply exploration. 

- Maybe it's obvious but I'm not grasping why, mathematically, the number of accessible configurations for SRS is the same as original replacement sampling (4th paragraph on page 3).

- Given that the central motivation of the work was to enable with-replacement strategies while still ensuring equal usage, I recommend that the authors include a histogram of datapoint usage for three strategies (with, without, hybrid). This should help convince the reader that the SRS indeed improves upon the usage statistics of with replacement. 

- If one were to create a hybrid sampling strategy, one that is natural is doing 50-50 sampling with and without replacement. In other words, for a batch size of 64, say, 32 are sampled with replacement and 32 without. By changing the ratio, you can also control what end of the sampling spectrum you want to be on. Did you try such a strategy? 

- For the numerical experiments, as I see it, there are 3 differences between the SRS setup and the baseline: location of batch normalization, learning rate, and batch size. The authors show (at the bottom of Page 6) that the performance boost does not come from learning rate or mini-batch size, but what about the placement of the BN layer? Seems like that still remains as a confounding factor?

- ""SRS leads to much more fluctuations, and hence significantly more covariate shift"". How do the authors define covariate shift? Can the authors substantiate this claim theoretically/empirically?

- The authors claim that the method works better when the dataset size is low compared to number of classes. Again, can the authors substantiate this claim theoretically/empirically? Maybe you can try running a sub-sampled version of CIFAR-10/100 with the baselines?

- The writing in the paper needs improving. A few sample phrases that need editing: ""smaller mini-batch means a larger approximation"", ""more accessible configurations of mini-batches"", ""hence more exploration-induction"", ""less optimal local minimum""

- Minor comment: why is the queue filled with repeated samples? In Figure 1, why not have the system initialized with 1 2 3 in the pool and 4 5 in the queue? Seems like by repeating, there is an unnecessary bias towards those datapoints.  ","The sentiment of the review is mixed but leans towards the negative side. The reviewer acknowledges the appeal of the idea but expresses significant concerns and suggests that more work is needed before publication. Therefore, the sentiment score is -30. The politeness of the language is quite high; the reviewer uses polite and constructive language throughout, even when pointing out issues. Therefore, the politeness score is 80.",-30,80
"The paper introduces a new approach to combine small RBMs that are pretrained in order to obtain a large RBM with good performance. This will bypass the need of training large RBMs and suggests to break them into smaller ones. The paper then provides experimental evidence by applying the method on ""invertible boolean logic"". MCMC is used to find the the solution to large RBM and compare it against the combined solutions of smaller RBMs.


The paper motivates the problem well, however, it is not well-written and at times it is hard to follow. The details of the approach is not entirely clear and no theoritcal results are provided to support the approach. For instance, in the introduced approach, only an example of combination is provided in Figure 1. It is not clear how smaller RBMs (and their associated parameters) are combined to obtain the larger RBM model. From the experimental perspective, the experimental evidence on ""invertible boolean logic"" does not seem to be very convincing for validating the approach. Additionally, the details of the settings of the experiments are not fully discussed. For example, what are the atomic/smaller problems and associated RBMs? what is the larger problem and how is the corresponding RBM obtained? Overall, the paper seems to be a report consisting of a few interesting observations rather than introducing a solid and novel contribution with theoretical guarantees.

Remark: 
The term ""Combinatorial optimization"", which is used in the title and throughout the body of paper, sounds a bit confusing to the reviwer. This term is typically used in other contexts.

Typos:
** Page 2 -- Paragraph 2: ""Therefore, methods than can exploit...""
** Page 3 -- 2nd line of math: Super-scripts are missing for some entries of the matrices W^A and W^{A+B}
** Page 5 -- Last paragraph: ""...merged logical units is more likly to get get stuck in a ...""
** Page 5 -- Last paragraph: ""...and combining their distributions using the mulistart heuristic...""
","The sentiment of the review is mixed. The reviewer acknowledges that the paper motivates the problem well but criticizes its clarity, lack of theoretical support, and insufficient experimental validation. Therefore, the sentiment score is slightly negative. The language used in the review is constructive and polite, with specific suggestions and corrections provided without harsh or rude language.",-20,80
"The paper presents an intuitive architecture for learning cross-lingual sentence representations. I see weaknesses and strengths: 

(i) The approach is not very novel. Using parallel data and similarity training (siamese, adversarial, etc.) to facilitate transfer has been done before; see [0] and references therein. Sharing encoder parameters across very different tasks is also pretty standard by now, going back to [1] or so. 
(ii) The evaluation is strong, with a nice combination of standard benchmark evaluation, downstream evaluation, and analysis. 
(iii) While the paper is on cross-lingual transfer, the authors only experiment with a small set of high-resource languages, where transfer is relatively easy. 
(iv) I think the datasets used for evaluation are somewhat suboptimal, e.g.: 
a) Cross-lingual retrieval and multi-lingual STS are very similar tasks. Other tasks using sentence representations and for which multilingual corpora are available, include discourse parsing, support identification for QA, extractive summarization, stance detection, etc. 
b) Instead of relying on Agic and Schluter (2017), why don’t the authors use the XNLI corpus [2]?
c) Translating the English STS data using Google NMT to evaluate an architecture that looks a lot like Google NMT sounds a suspicious. 
(v) While I found the experiment with eigen-similarity a nice contribution, there is a lot of alternatives: seeing whether there is a linear transformation from one language to another (using Procrustes, for example), seeing whether the sentence graphs can be aligned using GANs based only on JSD divergence, looking at the geometry of these representations, etc. Did you think about doing the same analysis on the representations learned without the translation task, but using target language training data for the tasks instead? The question would be whether there exists a linear transformation from the sentence graph learned for English while doing NLI, to the sentence graph learned for German while doing NLI. 

Minor comments: 
- “Table 3” on page 5 should be Table 2. 
- Table 2 seems unnecessary. Since the results are not interesting on their own, but simply a premise in the motivating argument, I would present these results in-text. 

[0] http://aclweb.org/anthology/W18-3023","The sentiment of the review is mixed, with both positive and negative aspects highlighted. The reviewer acknowledges the strengths of the paper, such as the strong evaluation and the interesting experiment with eigen-similarity, but also points out several weaknesses, including the lack of novelty and the suboptimal choice of datasets. Therefore, the sentiment score is slightly negative. The language used in the review is polite and constructive, providing specific recommendations and suggestions for improvement without being harsh or dismissive.",-20,80
"
Summary:
The authors present a video prediction model called SAVP that combines a Variational Auto-Encoder (VAE) model with a Generative Adversarial Network (GAN) to produce more realistic and diverse future samples.

Deterministic models and certain loss functions such as Mean Squared Error (MSE) will produce 
blurry results when making uncertain predictions. GAN predictions on the other hand usually are more visually appealing but often lack diversity, producing just a few modes. The authors propose to combine a VAE model with a GAN objective to combine their strengths: good quality samples (GAN) that cover multiple possible futures (VAE).

Strengths:
[+] GANs are notoriously unstable to train, especially for video. The authors formulate a VAE-GAN model and successfully implement it.

Weaknesses:
[-] The combination of VAEs and GANs, while new for videos, had already been proposed for image generation as indicated in the Related Work section and its formulation for video prediction is relatively straightforward given existing VAE (Denton & Fergus 2018) and GAN models (Tulyakov et al. 2018).

[-] The results indicate that SAVP offers a trade-off between the properties of GANs and VAEs, but does not go beyond its individual parts. For example, the experiment of Figure 5 does not show SAVP being significantly more diverse than GANs for KTH (as compared to VAEs). Furthermore, Figure 6 and Figure 7 in general show SAVP performing worse than SVG (Denton & Fergus 2018), a VAE model with a significantly less complex generator, including for the metric (VGG cosine similarity) that the authors introduce arguing that PSNR and SSIM do not necessarily indicate prediction quality.

While the use of a GAN in general will make the results less blurry and visually appealing, it does not necessarily mean that the samples it generates are going to be plausible or better. Since a direct application of video prediction is model-based planning, it seems that plausibility might be as important as sample quality. This work proposes to combine VAEs and GANs in a single model to get the benefits of both models. However, the experiments conducted generally show that SAVP offers only a trade-off between the visual quality of GANs and the coverage of VAEs, and does not show a clear advantage over current VAE models (Denton & Fergus, 2018) that with simpler architectures obtain similar results. While the presentation is clear and the evaluation of the model is thorough, I am unsure of the significance of the proposed method.

In order to better assess this model and compare it to its individual parts and other VAE models, could the authors:

1) Compare SAVP to the SVG-LP/FP model on a controlled synthetic dataset such as Stochastic Moving MNIST (Denton & Fergus, 2018)?
2) Comment on the plausibility of the samples generated by SAVP? Do some samples show imagined objects – implausible interactions for the robotic arm dataset? If so, what would be the advantage over blurry but plausible generations of a VAE?","The sentiment of the review is mixed but leans towards the negative side. The reviewer acknowledges the strengths of the paper, such as the successful implementation of a VAE-GAN model for video prediction, but also points out significant weaknesses, including the lack of novelty and the underwhelming performance compared to existing models. Therefore, the sentiment score is -30. The language used in the review is polite and constructive, offering specific recommendations for improvement without being harsh or dismissive. Thus, the politeness score is 80.",-30,80
"======== Summary ============

The authors consider a setup where there is a set of trajectories (s_t, a_t, r_t) where r_t is a *vector* of rewards. They assume that each agent is trying to maximize \sum_t \gamma^t (\phi . r_t) where \phi is a preference vector that lives on the simplex. Their goal is to calculate \phi (and maybe also an optimal policy under \phi?). The 

The authors first prove that this problem can be decomposed into finding Q functions for optimal policies for each component of r_t individually, and then solving for \phi that rationalizes the trajectory of actions in terms of these Q functions. Given the entire collection of trajectories, they perform off-policy Q-learning on each component of r_t in order to learn the Q function for that component, and then use linear programming to solve for \phi based on these Q function.

========== Comments =============

I think it's a worthwhile direction to combine IRL with modeling a diversity of preferences among agents. I can imagine several reasons you might want to do this, but the authors are not clear what their goal is besides ""to propose methods that can help to understand the intricacy and complexity of human motivations and their behaviors"". Is the goal to do better policy prediction? To do better policy prediction conditional on \phi? To infer \phi to understand people's preferences from a social science perspective? These all seems reasonable but not sufficiently teased out in the work. (For comparison, IRL is typically - although not always - interested in learning the reward function in order to construct robust policies that maximize it). The authors also don't seem to solve a particular task of importance on the WoW dataset.

The theoretical approach seems sound, and I liked the way their algorithm was motivated and the way the problem was decomposed into off-policy Q-learning and then solving for \phi.

However, I found myself quite confused in the experimental section (4.3). The authors evaluate their approach by action prediction. Given the trajectories, is \phi computed for each player and then compute actions based on that value of \phi? Is \phi computed on the same trajectory data used for evaluation or a different subset? Or is action prediction performed in aggregate across the entire population? The experimental setup was never clarified for this (main) experiment.

I was also confused about the motivation for Figure 2 and Appendix D. The authors are showing that their predictions about which reward is motivating the players is consistent with external factors. But wouldn't you see the same thing if you just plotted the observed *rewards* themselves? E.g. players in a guild will achieve more Relationship reward. 
The proposed approach takes the vector of reward, learns which actions are consistent with achieving each reward, then infers from the actions which reward is trying to be achieved. What advantages does this have vs. just looking at the empirical trajectory of rewards for each player/group?
I can certainly imagine that the IRL approach has certain advantages over looking at the empirical reward stream, but the authors have not talked about this nor compared against it experimentally.

The writing could also use some improvement for a future iteration, I've listed a few points below:

pg.1, Neither Brown & Sandholm nor Moravcik et al use ""RL algorithms""
pg.1, Finn et al unmatched )
pg.1, ""a scalar reward despite observed or not"" -> ""a scalar reward whether observed or not""
pg.2, ""Either the range of"" -> ""Both the range of"" (and this sentence needs further cleanup)
pg.2, ""which records the pathing of players"" ??
Theorem 3: ""each of the set e_i has an unique element..."" This isn't clear. I think you mean ""For each e_i there is a unique vector v^\pi(s) for all \pi \in \Pi_{e_i} . The equality holds if these vectors are distinct for each e_i"".
pg. 5 ""If otherwise all elements in \phi are generative"" how can they be negative if they are on the simplex?
pg.5 ""we do not perform any scalarization on the reward...the model assumption is easier to be satisfied"" I think this is a strange comparison to IRL because in IRL you're trying to find a (possibly parametric) function (s,a) -> R, whereas here you're *given* the vector R and are trying to find \phi. So while you have more degrees of freedom by adding \phi, you lose the original degrees of freedom in the reward function.

","The sentiment of the review is generally positive, as the reviewer acknowledges the worthwhile direction of the research and appreciates the theoretical approach and algorithm motivation. However, there are several critical points raised regarding the clarity of the goals, experimental setup, and certain motivations, which slightly temper the overall positivity. The politeness of the language is high, as the reviewer provides constructive feedback and suggestions for improvement without being harsh or dismissive.",60,80
"Main contribution: devising and evaluating a theoretically-sound algorithm for quantifying the semantic similarity between two pieces of text (e.g., two sentences), given pre-trained word embeddings (glove).

Clarity:
The paper is generally well-written, but I would have liked to see more details regarding the motivation for the work, description of the prior work and discussion of the results. As an example, I could not understand what were the differences between the online and offline settings, with only a reference to the (Arora et al. 2016) paper that does not contain neither ""online"" nor ""offline"". The mathematical derivations are detailed, which is nice.

Originality:
The work looks original. It proposes a method for quantifying semantic similarity that does not rely on cosine similarity.

Significance:
I should start by saying I am not a great reviewer for this paper. I am not familiar with the STS dataset and don't have the mathematical background to fully understand the author's algorithm.
I like to see theoretical work in a field that desperately needs some, but overall I feel the paper could do a much better job at explaining the motivation behind the work, which is limited to ""cosine similarity [...] is not backed by a solid theoretical foundation"".
I am not convinced of the practicality of the algorithm either: the algorithm seems to improve slightly over the compared approaches (and it is unclear if the differences are significant), and only in some settings. The approach needs to remove stop-words, which is reminiscent of good old feature engineering. Finally, the paper claims better average time complexity than some other methods, but discussing whether the algorithm is faster for common ranges of d (the word embedding dimension) would also have been interesting.
","The sentiment of the review is mixed but leans slightly positive. The reviewer acknowledges the originality and theoretical soundness of the work but expresses concerns about the clarity, motivation, and practical significance of the algorithm. Therefore, the sentiment score is 20. The language used in the review is polite and constructive, with suggestions for improvement rather than harsh criticism, so the politeness score is 80.",20,80
"The authors introduce Morph-Net, a single layer neural network where
the mapping is performed using morphological dilation and erosion.
I was expecting something applied to convolutional networks as such operators
are very popular in image processing, so the naming is a bit misleading.

It is shown that the proposed network can approximate any smooth function, 
assuming a sufficiently large number of hidden neurons, that is a nice result.

Clarity should be improved, for example it is mentioned that the structuring
element is learned but never clearly explained how and what difficulties it poses.
In the main text it is written that alpha is {-1, 1}, which would result in a
combinatorial search, but never explained how it is learned in practice.
This is shown only in the appendix but it is not clear to me that using a binarization
with the weights is not prone to degenerate solutions and/or to learn at all
if proper initialization is not used.
Did the authors experiment with smooth versions or other form of binarization with
straight-through estimator or sampling?

In the proof for theorem 1 it is not clear if the convergence of the proposed
network is faster or slower than that of a classic single layer network.

The main result of the paper is that the structuring element can be learned,
but there is no discussion on what it is learned. Also, there is no comparison
on related approaches that try to learn the structuring element in an end-to-end
fashion such as [1].

Experiments lack a more thorough comparison with state-of-the-art and at least
an ablation study to show that the proposed approach is effective and has merit.
For example, what is the relative contribution of using dilation and erosion
jointly versus either one of them.
What is the comparison with a winner-take-all unit over groups of neurons
such as max-pooling?

It seems that extending the work to multiple layers should be trivial but it is
not reported and is left to future investigations. This hints at issues with
the optimization and should be discussed, is it related to the binarization
mentioned above?

Overall the idea is interesting but the way the structuring element is learned
should be discussed in more details and exemplified visually. Experiments need
to be improved and overall applicability is uncertain at this stage.

=======
[1] Masci et al., A Learning Framework for Morphological Operators Using Counter--Harmonic Mean.
","The sentiment of the review is mixed, leaning towards neutral. The reviewer acknowledges the interesting idea behind Morph-Net and the positive result of approximating any smooth function, which contributes to a positive sentiment. However, the review also highlights several significant issues and areas for improvement, such as clarity, experimental thoroughness, and applicability, which contribute to a negative sentiment. Therefore, the sentiment score is balanced around neutral. The politeness of the language is generally respectful and constructive, with the reviewer providing specific recommendations and questions without using harsh or dismissive language. The reviewer maintains a professional tone throughout, which contributes to a high politeness score.",0,80
"ML models are trained on a predefined dataset formed by a set of classes. Those classes use to be the same ones for training and testing. However, what happen when during testing time images with classes unseen during training are shown to the model? This article focus in this problem which is not currently taking much attention by the mainstream research community and is of great importance for the real world applications.

This article tries to detect areas of the image where those out-of-distribution situations appear in semantic segmentation applications. The approach used is by training a classifier that detects which pixels are out of distribution. For training two datasets are used: the dataset of interest and another different one. The classifier learns to detect if a pixel is from the dataset of interest or from another distribution.

The main problem I found with this article is that I couldn't fully understand it. Maybe because the text needs a bit more of review and improvement or maybe because Im not very familiar with the topic. Moreover the article is 10 pages while it is encouraged to be 8. I find that the method of the paper is quite simple and can be explained more straight forward and in less pages. The related work section overlaps a lot with the intro, I suggest to combine both. First two paragraphs of the method seam that should be in the intro. Model details from the experiments I consider that should be explained in the method. I miss a figure explaining the architecture of the model. Why using the semantic segmentation model proposed and no something standard? For instance Tiramisu (That is also based on dense layers). Note that the method used for semantic segmentation is 10 points lower than the SOTA in Cityscapes. Figure 1 is impossible to read as the captions are too small. The representations of figures 2-5 are difficult to interpret. There is no comparison to SOTA

","The sentiment score is derived from the overall tone of the review. The reviewer acknowledges the importance of the problem addressed by the article and provides constructive feedback, but also points out several significant issues, including difficulty in understanding the text, excessive length, and lack of clarity in figures. This mixed feedback results in a sentiment score of -20. The politeness score is based on the language used. The reviewer uses polite language, such as 'I suggest' and 'I miss,' and avoids harsh or rude comments, resulting in a politeness score of 60.",-20,60
"A Stackelberg competition is a nonzero-sum game where 1) each player has their own objective, which do not sum up to a constant, and 2) there is an order at which the players interact. The proposed formulation only assumes that parameters of one player (data generator) partition in I tuples \gamma_i of parameters, where each tuple parameterizes a different data generator component (e.g., a separate neural network). Further, each of those components is assumed to contribute a term to the game's objective that only depends on the corresponding parameter tuple \gamma_i, and the other player's parameters \theta (e.g., weights of the discriminator). From a game theoretic perspective, this still yields a 2-player zero-sum game where the action space of the data generator is the product space of the I tuple spaces. Hence, I have doubts about the general finding that more data generating components decreases the duality gap.

The gap between the a maximin and minimax solution is determined by the shape of the objective \phi(\gamma,\theta) and is zero, for example, if \phi is (quasi) convex in \gamma=[\gamma_1, ..., \gamma_I], and (quasi) concave in \theta. The authors bound the violation of this property w.r.t. the data generator components' parameters \gamma_i, and argue that this degree of violation is the same for the whole data generator parametrized by \gamma=[\gamma_1, ..., \gamma_I] if the data generator components are from the same family of mappings (e.g., having the same network architecture). While this conclusion is true under worst cast assumption, e.g., the globally maximal possible gap, this would also imply that all data generator components find the same global best solution, that is, yield the same mapping, in which case the gap would be identical to just using one of those components.

Intuitively, the only reason to have multiple data generator components is to learn different mappings such that the joint data generator -- mixing the outputs of the different components -- is more expressiv than just a single mapping. If the different mappings only result from the inability of finding the global best solution, a worst case argument is not very insightful; in this case, one should study the duality gap in the neighborhood of the starting solutions. On the other hand, if we assume a different family of mappings for each component, the convexity violation of the joint data generator is higher than for each component; hence, the gap does not necessarily decrease with more components.

So why do multiple data generator components help in practice, and why does the proposed model outperform single-component GANs and the multi-branch GAN in the experiments? Solving a maximin/minimax problem for highly non-convex-concave functions is challenging; there is an infinity of saddle point solutions which yield different ""performances"". The multi-branch GAN can be seen as a model averaging approach giving more stable results, whereas the proposed GAN seems more of an ensemble approach to stabilize the result. Though, this is speculative and I would encourage the authors to study this in-depth; the reasoning in Remark 1 is not convincing to me.

UPDATE:

I read the revision and stick to my vote. In the discussion, I wasn't able to get my points across, e.g., that bounding the worst case duality gap is not enough to conclude that the observed duality gap does not grow for multiple local optimal GANs, where the duality gap is expected to be much smaller. A simple experiment could be to actually measure the duality gap (flip the order of the players and measure the difference of the objectives, when starting with the same initialization). If the authors were right, the maximum of those gap should stay constant when adding more data generators. To justify a Stackelberg setting, the authors may provide an example instantiation that cannot be cast into a standard zero-sum game with minimax solution. I can't see such an example but I'm happy to be proven wrong.","The sentiment of the review is mixed but leans towards the negative side. The reviewer expresses doubts and concerns about the general findings and reasoning presented by the authors. They also mention that the reasoning in Remark 1 is not convincing and suggest further in-depth study. The sentiment score is -40 because while the reviewer acknowledges some aspects, they predominantly express skepticism and request additional evidence. The politeness score is 60 because the reviewer uses polite language, such as 'I would encourage the authors' and 'I'm happy to be proven wrong,' which indicates a respectful tone despite the critical feedback.",-40,60
"This paper develops a reinforcement learning approach for negotiating coalitions in cooperative game theory settings.  The authors evaluate their approach on two games against optimal solutions given by the Shapley value.

The work builds upon a substantial and growing literature on reinforcement learning for multiagent competitive and cooperative games. The most novel component of the work is a focus on the process of negotiation within cooperative coalition games. The two game environments studied examine a ""propose-accept"" negotiation process and a spatial negotiation process.

The main contribution of the work is the introduction of a reinforcement learning approach for negotiation that can be used in cases where unlimited training simulations are available.  This approach is a fairly straightforward application of RL to coalition games, but could be of interest to researchers studying negotiation or multiagent reinforcement learning, and the authors demonstrate the success of RL compared to a normative standard.

My primary concerns are:
- The authors advertise the work as requiring no assumptions about the specific negotiation protocol, but the learning algorithms used are different in the two cases studied, so the approach does require fine-tuning to particular cases.
- Maybe I missed it, but how many training games are required?
- In what real applications do we expect this learning algorithm to be useful?  
- The experiments where the RL agents are matched against bots include training against those specific bot types. How does the trained algorithm perform when matched against agents using rules outside its training set?  
- Since the Shapley value is easily computable in both cases studied.  If the bots are all being trained together, why wouldn't the bots just use that to achieve the optimal solution?
- Why are only 20 game boards used, with the same boards used for training and testing?  How do the algorithms perform on boards outside the training set?

Overall, the paper is somewhat interesting and relatively technically sound, but the contribution seems marginal.
","The sentiment of the review is moderately positive, as the reviewer acknowledges the novelty and potential interest of the work but also points out several concerns and limitations. The sentiment score is therefore 20. The politeness of the language is quite high, as the reviewer uses polite and constructive language throughout the review, even when pointing out issues. The politeness score is 80.",20,80
"The paper proposes Partial VAE to handle missing data and a variable-wise active learning method. The model combines Partial VAE with the acquisition function to design an intelligent information acquisition system. The paper nicely combines the missing value problem with an active learning strategy to in an acquisition pipeline and demonstrate the effectiveness on several datasets.

I have following comments/questions:

1.  Does p(x_i | z) include parameters? How do these parameters be trained?

2. Does sample from p(x_i | x_o) follow by sampling z from q(z|x_o) then sample x_i from p(x_i | z)? How to sample from p(x_\phi | x_i, x_o) in Eq (7)?

3. In Eq (9), it uses q(z_i|x_o), q(z_i | x_i, x_o),  q(z_i | x_i, x_o, x_\phi) while in Eq (4) it only shows how to learn q(z|x_o). Does it need to learn multiple partial inference networks for all combination of i and \phi ?

4. The comparison with similar algorithms seems to be weak in the experiment section. RAND is random feature selection, and SING is global feature selection by using the proposed method. These comparison methods cannot provide enough information on how well the proposed methods performs. There are plenty of works in the area of “active feature acquisition” and also many works in feature selection dated back to Lasso which should be considered as comparison targets.

5. In the “personalized” implementation of EDDI on each data instances, is the model trained independently for each data point or share some parameters across different data? If so, what are the shared parameters?","The sentiment of the review is generally positive, as the reviewer acknowledges the novelty and effectiveness of the proposed method. However, the reviewer also raises several technical questions and points out weaknesses in the experimental comparison, which slightly tempers the overall positive sentiment. Therefore, the sentiment score is 50. The language used in the review is polite and professional, with the reviewer posing questions and making suggestions in a respectful manner. Hence, the politeness score is 80.",50,80
"The paper proposes to include within regular programs, learned parameters that are then tuned in an online manner whenever the program is invoked. Thus learning is continuous, integration with the ML backend seamless. The idea is very interesting however, it seems to me that while we can replace native variables with learned parameters, the hyperparameters involved in the learning become new native variables (e.g. the value of feedback). Perhaps with some effort we can replace the  hyperparameters with predicted variables too. Other concerns of mine stem from the programmer in me. I think of a program as something deterministic and predictable. With continuous, online, self-tuning, these properties are gone. How do the authors propose to assuage folks with my kind of mindset? Is debugging programs with predicted variables an issue? Consider a situation where the program showed some behavior with a certain setting of q which has since been tuned to another value and thus the same behavior doesn't show up. I find these to be very interesting questions but don't see much of a discussion in the current draft. Also, how does this work relate to probabilistic programming?","The sentiment of the review is mixed but leans towards positive. The reviewer finds the idea very interesting and acknowledges its potential, which suggests a positive sentiment. However, the reviewer also raises several concerns and questions about the implementation and implications of the proposed approach, which tempers the overall positivity. Therefore, the sentiment score is 20. The language used in the review is polite and constructive. The reviewer uses phrases like 'it seems to me,' 'perhaps with some effort,' and 'I find these to be very interesting questions,' which indicate a respectful and considerate tone. Therefore, the politeness score is 80.",20,80
"I found the paper difficult to follow. The method proposed is not well motivated, and  the literature review explains well the novelty. Here are some questions/points for discussion:

- the token-level MLE training is not what causes the exposure bias: one can train with MLE and still avoid it by generating appropriate sequences using the RNN, as in scheduled sampling. The problem with MLE (or cross entropy) is that the labels to be predicted might not be the correct ones. See the paper by Ranzato et al. (ICLR2016) for a good discussion of the issue: https://arxiv.org/pdf/1511.06732.pdf

- The criticism against previous works for not comparing agains CRFs seems odd: CRFs are given the number of labels, words, etc. to predict, typically the same as the number of words to be tagged. If one  has this, as well as binary rewards for each decision, then there is little benefit for RL/IL based approaches to be used. The point for them is the use of non-decomposable loss functions such as BLEU, which are not common in tagging, but in tasks like MT, where CRFs can't be used. In fact, for the transliteration experiments in the paper, the CRF approach is padded to perform the task, which highlights that it is not the right comparison. 

- the approach proposed seems very similar to MIXER, which also learns a regressor to predict the reward for each action. A direct comparison both in terms of how the approaches operate and empirically is needed.

- why is it a problem that previous works by Ranzato, Bahdanau and Paulus combine MLE and RL? You are using the same supervision, ie. the labeled corpus.

- the adjusted training seems to essentially not reward correct predictions (top branch in the equation). Why is this a good idea?

- In figure 1 it is not clear at all that the proposed approach works; depending on the epoch the ranking among the three variants differs


- what does it mean for one method to surpass the other in flexibilty? If anything the requirement for immediate rewards after every action restricts flexibility, as one can't use non-decomposable loss functions such as BLEU which are prety common in NLP.

- How is the training efficiency measured in the paper?

- Why not compare against MIXER, as well as more recent work by Leblonde et al. (2018): https://arxiv.org/abs/1706.04499 ? I don't see why the Rennie et al. 2017 method is picked for comparison.

- It is not true that in IL one needs a gold standard policy, one can learn with sub-optimal policies, see Sun et al. (2018): https://arxiv.org/pdf/1703.01030.pdf

- It is odd to say that an approach proposed earlier (Dagger) reduces to a variant of a later proposed one (Scheduled sampling), the reduction should be the other way around

- are the randomly initialized character embeddings for transliteration tuned during training?

- How were the alignments for training the CRF obtained?
","The sentiment of the review is generally negative, as the reviewer expresses difficulty in following the paper and points out several significant issues with the methodology, comparisons, and clarity. The sentiment score is -70. The politeness of the language is relatively neutral to slightly polite, as the reviewer uses formal language and avoids personal attacks, but the tone is critical and direct. The politeness score is 10.",-70,10
"Overall Score: 7/10.
Confidence Score: 7/10.

Detailed Comments: This paper introduces various Deep Recurrent Gaussian Process (DRGP) models based on the Sparse Spectrum Gaussian Process (SSGP) models and the Variational Sparse Spectrum Gaussian Process (VSSGP) models. This is a good paper and proposed models are very sound so I recommend for acceptance although as main weakness I can say that is very technical so it can be difficult to follow. Adding more intuitive ideas, motivation and maybe a figure for each step would be a solution. Apart from that it is a really good paper, congratulations.

Related to: RNN models and Sparse Nystrom approximation.

Strengths: Models are very sound, solutions are solid, the proposed methodology is correct and the empirical results and experiments are valid and properly done.

Weaknesses: It is too difficult to follow and it is written in an extreme technical way. More intuitions and a proper motivation both in the abstract and introduction may be put in order to make the paper easier to read and, hence, more used by researchers and data scientists.

Does this submission add value to the ICLR community? : Yes it does, the experiments show the efficiency of the proposed methods in some scenarios and are valid methodologies.

Quality:
Is this submission technically sound?: Yes it is.
Are claims well supported by theoretical analysis or experimental results?: Experimental results prove empirically the methods and appendixes show the analysis performed in a clear and elegant way.
Is this a complete piece of work or work in progress?: Complete piece of work.
Are the authors careful and honest about evaluating both the strengths and weaknesses of their work?: Yes, and I would enfatize that I have liked that some experiments are won by other methods such as GP-LSTM, they are very honest.

Clarity:
Is the submission clearly written?: Yes, but it is difficult for newcomers due to the reasons that I have stated before.
Is it well organized?: Yes it is.
Does it adequately inform the reader?: Yes it is.

Originality:
Are the tasks or methods new?: Yes, they are sound.
Is the work a novel combination of well-known techniques?: Yes it is.
Is it clear how this work differs from previous contributions?: Yes.
Is related work adequately cited?: Yes, being a strength of the paper.

Significance:
Are the results important?: I would argue that they are and are a clear alternative to consider in order to solve these problems.
Are others likely to use the ideas or build on them?: If the paper is written in a more friendly way, yes.
Does the submission address a difficult task in a better way than previous work?: Yes I think.
Does it advance the state of the art in a demonstrable way?: Yes, empirically.

Arguments for acceptance: Models are very sound, solutions are solid, the proposed methodology is correct and the empirical results and experiments are valid and properly done

Arguments against acceptance: Clarity of the paper.

Minor issues and typos:
-> (V)SS not defined before being used.
-> Abstract should be rewritten adding a motivation and focusing more on the problems being solved and less in the details of the solutions.
-> Recurrent indexes that go backwards (i) of Eq. 1. should be explained why are going backwards before being used like that. Newcomers may be confused.
-> Section 2 writing style lacks a bit of cohesion, relating the paragraphs may be a solution.
-> Q is not defined in section 3.1 paragraph 1.
-> A valid covariance function must produce a PSD matrix, put that in section 3.1. 
-> I do not see how U marginalizes in Eq. 7, kind of confused about that, I think that it should be p(y|X,U).
-> Section 3.4 statistics should be explained.

Reading thread and authors response rebuttal decision:
=================================================

I consider that the authors have perfomed a good rebuttal and reading the other messages and the authors response I also consider that my issue with clarity is solved. Hence, I upgrade my score to 7 and recommend the paper for publication.","The sentiment of the review is generally positive. The reviewer praises the soundness of the models, the correctness of the methodology, and the validity of the empirical results. However, they also note that the paper is very technical and difficult to follow, suggesting improvements for clarity. The politeness of the language is high, as the reviewer provides constructive feedback in a respectful and encouraging manner, even congratulating the authors on their work. The sentiment score is 70, reflecting a positive but not overwhelmingly enthusiastic sentiment. The politeness score is 90, indicating very polite and considerate language.",70,90
"The paper proposes node embedding methods for applications where nodes are sequentially related. An example application is the ""Wikispeedia"" dataset, in which nodes are connected in a graph, but a datapoint (a wikispeedia ""game"") consists of a sequence of nodes that are visited. Each node is further attributed with textual information.

The methods proposed are most closely related to skipgrams, whereby the sequence of nodes are treated like words in a sentence. Then, node attributes (i.e., text) and node representations must be capable of predicting neighboring nodes/words. (Fig.s 1/2 are a pretty concise overview of the proposed architecture).

Positively, this is a quite sensible extension and modification of existing ideas in order to support a new (or different) problem setting.

Negatively, I'd say the applications for this technique are fairly niche, which may limit the paper's readership. The method is mostly fairly straightforward and not methodologically groundbreaking (probably borderline in terms of expected methodological contribution for ICLR). I also didn't understand whether the theoretical claims were significant.

The wikispedia/physics experiments feel a bit more like proofs-of-concept rather than demonstrating that the technique has compelling real-world uses. The experiments are quite well fleshed-out and detailed though.
","The sentiment of the review is mixed, with both positive and negative aspects highlighted. The reviewer appreciates the sensible extension and modification of existing ideas but also points out the niche application and lack of groundbreaking methodology. Therefore, the sentiment score is around -10, indicating a slightly negative overall sentiment. The politeness of the language is quite high, as the reviewer uses polite and constructive language throughout the review, even when pointing out negative aspects. Thus, the politeness score is 80.",-10,80
"The paper proposes a novel exploration strategy for self-supervised imitation learning. An inverse dynamics model is trained on the trajectories collected from a RL-trained policy. The policy is rewarded for generating trajectories on which the inverse dynamics model (IDM) currently works poorly, i.e. on which IDM predicts actions that are far (in terms of mean square error) from the actions performed by the policy. This adversarial training is performed in purely self-supervised way. The evaluation is performed by one-shot imitation of an expert trajectory using the IDM: the action is predicted from the current state of the environment and the next state in the expert’s trajectory. Experimental evaluation shows that the proposed method is superior to baseline exploration strategies for self-supervised imitation learning, including random and curiosity-based exploration. 

Overall, I find the idea quite appealing. I am not an expert in the domain and can not make comments on the novelty of the approach. I found the writing mostly clear, except for the following issues:
- the introduction has not made it crystal clear that the considered paradigm is different from e.g. DAGGER and GAIL in that expert demonstrations are used at the inference time. A much wider audience is familiar with the former methods, and this distinction should have be explained more clearly.
- Section 4.2.: “As opposed to discrete control domains, these tasks are especially challenging, as the sample complexity grows in continuous control domains.” - this sentence did not make sense to me. It basically says continuous control is challenging because it is challenging. 
- I did not understand the stabilization approach. How exactly Equation (7) forces the policy to produce “not too hard” training examples for IDM? Figure 4 shows that it is on the opposite examples with small L_I that are avoided by using \delta > 0.
- Table 1 - it is a bit counterintuitive that negative numbers are better than positive numbers here. Perhaps instead of policy’s deterioration you could report the relative change, negative when the performance goes down and positive otherwise?

I do have concerns regarding the experimental evaluation:
- the “Demos” baseline approach should be explained in the main text! In Appendix S.7 I see that 1000 human demonstrations were used for training. Why 1000, and not 100 and not 10000?  How would the results change? This needs to be discussed. Without discussing this it is really unclear how the proposed method can outperform “Demos”, which it does pretty often.
- it is commendable that 20 repetitions of each experiment were performed, but I am not sure if it is ever explained in the paper what exactly the upper and lower boundaries in the figures mean. Is it the standard deviation? A confidence interval? Can you comment on the variance of the proposed approach, which seems to be very high, especially when I am looking at high-dimensional fetch-reach results?
- the results of “HandReach” experiments, where the proposed method works much worse than “Demos” are not discussed in the text at all
- overall, there is no example of the proposed method making a difference between a “working” and “non-working” system, compared to “Curiosity” and “Random”. I am wondering if improvements from 40% to 60% in such cases are really important. In 7 out of 9 plots the performance of the proposed method is less than 80% - not very impressive. ""Demos"" baseline doesn't perform much better, but what would happen with 10000 demonstrations?
- there is no comparison to behavioral cloning, GAIL, IRL. Would these methods perform better than learning IDM like ""Demos"" does?

I think that currently the paper is slightly below the threshold, due to evaluation issues discussed above and overall low performance of the proposed algorithm. I am willing to reconsider my decision if these issues are addressed.
","The sentiment of the review is moderately positive, as the reviewer finds the idea appealing and acknowledges the clarity of the writing, but also points out several issues and concerns that need to be addressed. Therefore, the sentiment score is 20. The politeness of the language is quite high, as the reviewer uses polite phrases such as 'I found the writing mostly clear,' 'it is commendable,' and 'I am willing to reconsider my decision if these issues are addressed.' Thus, the politeness score is 80.",20,80
"Summary:
Eslami et al. (2018) proposed a deep neuronal framework for a scene representation and renderer (the Generative Query Networks: GQN), which generate an image from a scene representation and a query camera pose. In this work, the authors use the GQN to estimate the camera pose from a target image. Existing learning approaches are discriminative, meaning that they are trained to output the camera pose in an end-to-end fashion, while this paper proposes a generative method more in the line of hand-crafted methods which still largely outperform learning approaches. Using the GQN with the proposed attention mechanism, the method captures an implicit mapping of the environment at a more abstract level. This implicit representation is then used to optimize the likelihood of the target pose in a probabilistic graphical model framing. They compare their solution to a discriminative baseline, based on a reversed GQN.

Pros:
- As shown in Figure 7, the generative approach seems to capture better the implicit representation associated to the mapping from the scene geometry and the image. 
- The proposed generative solution seems to be more accurate than the discriminative baseline. 
- As shown in Table 1, the proposed attention mechanism allows to focus on relevant parts of the context images, giving flexibility for more complex scenes.
- Unlike classical discriminative methods, the proposed solution can be easily used in new scenes (different from the one used for the learning) thanks to the representation network. 

Cons:
- The contribution seems incremental with respect to Eslami et al. (2018). 
- Lack of comparisons to state of the art, in particular a comparison with PoseNet is necessary.
- The results are shown only on simple datasets of small images (32x32 pixels). 
- Tradeoff between precision and time computing is necessary to handle large environments because of space discretization. Then, the method seems to be far to be exploited in a real life SLAM application (e.g. autonomous vehicle). 
","The review starts by summarizing the work and then lists both pros and cons. The pros are highlighted first, indicating a generally positive sentiment towards the work. However, the cons are also significant and point out important limitations, which tempers the overall positivity. The language used is polite and constructive, aiming to provide helpful feedback rather than criticize harshly.",30,80
"This paper introduces a data dependent strategy to mask parts of the partial derivatives in the chain rule computation. 

Typically with papers proposing modifications of the training regime of the neural network one would expect one of three outcomes:
 - a well justified, mathematically sound method, well tested in simple cases and with some proof of concept results on proper tasks
 - a more heuristic, empirical driven research, where strong results on proper tasks
 - method, however justified, allows us to do something previously impossible, removing some limitations/constraints (like biologically plausible learning etc.)

In its current form paper seems to lack any of these characteristics. On one hand method lacks any guarantees and on the other paper does not present significant improvements under any approved metrics, nor it introduces new which can be properly quantified. In fact, authors explicitly claim that empirical section ""Note that in these experiments, the purpose is not to achieve state of the art performance, but to exemplify how backdrop can be used and what measure of performance gains one can expect."".  

With methods like this it is almost obvious that resulting update is not an unbiased gradient estimator of any function. Consequently convergence/learning guarantees that we have for GD or SGD no longer apply. Do authors have any thoughts on how bad can it get? As noted in the text, other methods of ""dropping"" data (such as dropout) don't have this issue as they still estimate proper gradients. Here, since dropping is done inside the network only on backwards pass, resulting estimates could, in principle, lead to oscilations, divergence and other issues. If these are not encountered in practice it might be interesting to understand why. 

If authors prefer to go through more empirical path, one would expect at least to see some baselines for tasks proposed, rather than comparing Backdrop to SGD. There are many methods that could be applied in scenarios like this, including dozens forms of dropout (which, as authors note, is not aimed at the same goals, but this does not mean that it will not shine under the metrics introduced, as they are non-standard and so - noone tested them in this exact regime).

I am happy to revisit my rating given authors restructure paper towards one of these paths (or other one which is not listed here).","The sentiment of the review is generally negative, as the reviewer points out several deficiencies in the paper, such as the lack of guarantees, significant improvements, and proper baselines. The reviewer also questions the validity of the method and its potential issues. However, the reviewer does offer constructive feedback and expresses a willingness to revisit the rating if the authors make improvements. Therefore, the sentiment score is -60. The politeness of the language is relatively high, as the reviewer uses polite phrases such as 'I am happy to revisit my rating' and provides constructive criticism without being rude or dismissive. Therefore, the politeness score is 60.",-60,60
"The paper provides a dynamic sparse reparameterization method allowing small networks to be trained at a comparable accuracy as pruned network with (initially) large parameter spaces. Improper initialization along with a fewer number of parameters requires a large parameter model, to begin with (Frankle and Carbin, 2018).  The proposed method which is basically a global pooling followed by a tensorwise growth allocates free parameters using an efficient weight re-distribution scheme, uses an approximate thresholding method and provides automatic parameter re-allocation to achieve its goals efficiently. The authors empirically demonstrate their results on MNIST, CIFAR-10, and Imagenet and show that dynamic sparse provides higher accuracy than compressed sparse (and other) networks.

The paper is addressing an important problem where instead of training and pruning, directly training smaller networks is considered. In that respect, the paper does provide some useful tricks to reparameterize and pick the top filters to prune. I especially enjoyed reading the discussion section.

However, the hyperbole in claims such as ""first dynamic reparameterization method for training convolutional network"" makes it hard to judge the paper favorably given previous methods that have already proposed dynamic reparameterization and explored. This language is consistent throughout the paper and the paper needs a revision that positions this paper appropriately before it is accepted.

The proposed technique provides limited but useful contributions over existing work as in SET and DeepR. However, an empirical comparison against them in your evaluation section can make the paper stronger especially if you claim your methods are superior.

How does your training times compare with the other methods? Re-training times are a big drawback of pruning methods and showing those numbers will be useful.","The sentiment of the review is moderately positive. The reviewer acknowledges the importance of the problem addressed by the paper and appreciates the useful tricks and the discussion section. However, the reviewer also points out issues with the hyperbolic claims and suggests revisions to position the paper appropriately. The sentiment score is therefore 40. The politeness of the language is quite high. The reviewer uses polite language throughout, providing constructive criticism and suggestions for improvement without being rude or dismissive. The politeness score is therefore 80.",40,80
"i have change my rating from 5 to 6 after reading the numerous and thorough rebuttals from the authors. I hope they will incorporate these clarifications and additional experiments into the final version of the paper if accepted.

The purpose of this paper is presumably to approximate the margin of a sample as accurately as possible. This is clearly an intractable problem. Thus all attacks make some kind of approximation, including this paper. I am still a bit confused about the difference between ""zero-confidence attacks"" and those that don't fall into that category such as PGD. Since all of these are approximations, and we cannot know how far we are from the true margin, I don't see why these categories help. The authors spend two paragraphs in the introduction trying to draw a distinction but I am still not convinced. 

The proofs provided by the authors assume that convexity and many assumptions, which makes it not very useful for the real world case. What would have been helpful is to show the accuracy of their margin for simple binary toy 2D problems, where the true margin and their approximation can be visualized. This was not done. This reduces the paper to an empirical exercise rather than a true understanding of their method's advantages and limitations.

Finally, the experimental results do not show any significant advantage over PGD, either in running time (they are slower) or norm perturbation. Thus their novelty rests on the definition of zero confidence attack, and of the importance of such a attack. So clarifying the above question will help to judge the paper's novelty.

","The sentiment of the review is moderately positive, as indicated by the initial statement of changing the rating from 5 to 6 after reading the rebuttals. However, the reviewer expresses confusion and skepticism about the distinctions made in the paper and points out significant limitations and areas for improvement. Therefore, the sentiment score is 20. The politeness of the language is quite high; the reviewer uses polite phrases such as 'I hope,' 'would have been helpful,' and 'clarifying the above question will help,' which indicates a respectful tone. Thus, the politeness score is 80.",20,80
"The paper proposes a compositional generative model for GANs. Basically, assuming existence of K objects in the image, the paper creates a latent representation for each object as well as a latent representation for the background. To model the relation between objects, the paper utilizes the multi-head dot-product attention (MHDPA) due to Vaswani et a. 2017. Applying MHDPA to the K latent representations results in K new latent representations. The K new representations are then fed into a generator to synthesize an image containing one object. The K images together with the synthesized background image are then combined together to form the final image. The paper compares to the proposed approach to the standard GAN approach. The reported superior performance suggest the inductive bias of compositionality of scene leads to improved performance.

The method presented in the paper is a sensible approach and is overall interesting. The experiment results clearly shows the advantage of the proposed method. However, the paper does have several weak points. Firs of all, it misses an investigation of alternative network design for achieving the same compositionality. For example, what would be the performance difference if one replace the MHDPA with LSTM. Another weak point is that it is unclear if the proposed method can be generalize to handle more complicated scene such as COCO images as the experiments are all conducted using very toy-like image datasets. ","The sentiment of the review is generally positive, as the reviewer acknowledges that the method presented is sensible and interesting, and that the experimental results clearly show the advantage of the proposed method. However, the reviewer also points out several weak points, suggesting areas for improvement. This balanced feedback indicates a sentiment score of 50. The language used in the review is polite and constructive, as the reviewer provides specific recommendations without being harsh or dismissive, leading to a politeness score of 80.",50,80
"The paper studies the benefit of an anisotropic gradient covariance matrix in SGD optimization for training deep network in terms of escaping sharp minima (which has been discussed to correlate with poor generalization in recent literature). 

In order to do so, SGD is studied as a discrete approximation of stochastic differential equation (SDE). To analyze the benefits of anisotropic nature and remove the confounding effect from scale of noise, the scale of noise in the SDE is considered fixed during the analysis. The authors identify the expected loss around a minimum as the efficient of escaping the minimum and show its relation with the hessian and gradient covariance at the minimum. It is then shown that when all the positive eigenvalues of the covariance matrix concentrate along the top eigenvector and this eigenvector is aligned with the top eigenvector of the Hessian of the loss w.r.t. the parameters, SGD is most efficient at escaping sharp minima. These characteristics are analytically shown to hold true for a 1 hidden layer network and experiments are conducted on toy and real datasets to verify the theoretical predictions.

Comments:

I find the main claim of the paper intuitive-- at any particular minimum, if noise in SGD is more aligned with the direction along which loss surface has a large curvature (thus the minimum is sharp along this direction), SGD will escape this minimum more efficiently. On the other hand, isotropic noise will be wasteful because a sample from isotropic noise distribution may point along flat directions of the loss even though there may exist other directions along which the loss curvature is large. However, I have several concerns which I find difficult to point out because *many equations are not numbered*. 

1. In proposition 2, it is assumed under the argument of no loss of generality that both the loss at the minimum L_0=0 and the corresponding theta_0 =0. Can the authors clarify how both can be simultaneously true without any loss of generality?
2. A number of steps in proposition 2 are missing which makes it difficult to verify. When applying Ito's lemma and taking the integral from 0 to t, it is not mentioned that both sides are also multiplied with the inverse of exp(Ht).
3. In proposition 2, when computing E[L(theta_t)] on page 12, the equalities after line 3 are not clear how they are derived. Please clarify or update the proof with sufficient details.
4. It is mentioned below proposition 2 that the maximum of Tr(H. Sigma) under constraint (6) is achieved when Sigma* = Tr(Sigma). lambda_1 u1.u1^T, where lambda_1 is the top eigenvalue of H. How is lambda_1 a factor in Sigma*? I think Sigma* should be Tr(Sigma). u1.u1^T because this way the sum of eigenvalues of Sigma remains unchanged which is what constraint (6) states.
5. The proof of proposition 5 is highly unclear.Where did the inequality ||g_0(theta)||^2 <= delta.u^TFu + o(|delta|) come from? Also, the inequality right below it involves the assumption that u^Tg_0 g_0u <= ||g_0||^2 and no justification has been provided behind this assumption.


Regarding experiments, the toy experiment in section 5.1 is interesting, but it is not mentioned what network architecture is used in this experiment. I found the experiments in section 5.3 and specifically Fig 4 and Fig 7 insightful. I do have a concern regarding this experiment though. In the experiment on FashionMNIST in Fig 4, it can be seen that both SGD and GLD 1st eigvec escapes sharp minimum, and this is coherrent with the theory. However, for the experiment on CIFAR-10 in Fig 7, experiment with GLD 1st eigvec is missing. Can the authors show the result for GLD 1st eigvec on CIFAR-10? I think it is an important verification of the theory and CIFAR-10 is a more realistic dataset compared with FashionMNIST.

A few minor points:

1. In the last paragraph of page 3, it is mentioned that the probability of escaping can be controlled by the expected loss around minimum due to Markov's inequality. This statement is inaccurate. A large expected loss upper bounds the escaping probability, it does not control it.
2. Section 4 is titled ""The anisotropic noise of SGD in deep networks"", but the sections analyses a 1 hidden layes network. This seems inappropriate.
3. In the conclusion section, it is mentioned that the theory in the paper unifies various existing optimization mentods. Please clarify.

Overall, I found the argument of the paper somewhat interesting but I am not fully convinced because of the concerns mentioned above.","The sentiment of the review is mixed. The reviewer acknowledges the intuitive nature of the main claim and finds some experiments insightful, but also raises several significant concerns and points out numerous areas needing clarification. Therefore, the sentiment score is slightly negative. The language used in the review is generally polite and constructive, with the reviewer asking for clarifications and suggesting improvements without being rude or dismissive.",-20,80
"This paper proposes to use 8/4-bit approximation of activations to save the memory cost during gradient computation.  The proposed technique is simple and straightforward. On the other hand, the proposed method only saves up to a constant cost of the storage. With the constant factor (4x, 8x) depending on whether fp16 or fp32 is used during computation. Notably, there is a small but noticeable accuracy drop in the final trained model using this mechanism.

The alternative method, gradient checkpointing, can bring sublinear memory improvement, with at most 25%  compute overhead, with no loss of accuracy drop.

As a result, the proposed method has a limited use case. The author did mention, during the response that the method could be combined further with the sublinear checkpointing. However, since sublinear checkpointing already brings in significant savings, it is unclear whether low bit compression is necessary.

Given the limited technical novelty(can be described as oneliner ""store forward pass in 4/8 bit fixed point""),  limited applicable scenarios, and limited improvement it can buy(4x memory saving with accuracy drop), I think this is a boarder-line paper

On the positive side, the empirical result could still be interesting to some readers in the ICLR community, the paper could be further improved by comparing more numerical representations, such as fp16 and other floating point formats such as unum.
","The sentiment of the review is mixed, leaning slightly towards the negative side. The reviewer acknowledges the simplicity and straightforwardness of the proposed technique but highlights its limited use case, technical novelty, and the accuracy drop associated with it. The sentiment score is therefore -20. The politeness of the language is generally respectful and constructive, with suggestions for improvement and acknowledgment of the potential interest to some readers. The politeness score is 60.",-20,60
"Revision:

The authors have took in the feedback from myself and the other reviewers wholeheartedly, and have clearly worked hard to improve the results, and the paper during the revision process. In addition, their code release encourages easy reproducibility of their model, which imo is needed for this work given the non-conventional nature of the model (that being said, the paper itself is well written and the authors have done sufficiently well in explaining their approach, and also the motivation behind it, as per my original review). The code is relatively clear and self-contained demonstrating their experiments on MNIST, CelebA demonstrating the use of the visual sketch model.

I believe the improvements, especially given the compute resources available to the authors, warrant a strong accept of this work, so I revised my score to 9. I also believe this work will be of value to the ICLR community as it offers alternate, less explored approaches compared to methods that are typically used in this domain. I'm excited to see more in the community explore biologically inspired approaches to generative models, and I think this work along with the code base will be an important base for other researchers to use as a reference point for future work.

Original Review below:

Summary: They propose a biologically motivated short term attentive working memory (STAWM) generative model for images. The architecture is based on Hebbian Learning (i.e. associative memories are represented in the weight matrices that are dynamically updated during inference by a modified version of Hebbian learning rule). These memories are sampled from glimpses on an input image (using attention on contextual states, similar to [1]), in addition to a latent, query state. This model learns a representation of images that can be used for sequential reconstruction (via a sequence of updates, like a sketchpad, like DRAW [1], trained in an unsupervised manner). These memories produced by drawing can also be used for semi-supervised classification (achieves very respectable and competitive results for MNIST and CIFAR-10).

This paper is beautifully written, and the biological inspiration, motivation behind this work, and links to neuroscience literature as well as relation to existing ML work (even recent papers) is well stated. The main strength of this paper is that the author went from a biologically inspired idea to a complete realization of the idea in algorithmic form. The semi-supervised classification results are competitive to SOTA, and although the CIFAR-10 reconstruction results are not great (especially compared to generative adversarial models or recent variation models [2]), I think the approach is coming from a very different angle that is different enough compared to the literature to warrant some attention, or at least a glimpse, so to speak, from the broader community. The method may offer new ways to interpret ML models that is current models lack, which in itself is an important contribution. That being said, the fact that most adversarial generative models achieved a far better performance raises concern on the generalization ability of these memory-inspired learned representations, and I look forward to seeing future work investigate this area in more detail.

The authors also took great care in writing details for important parts of the experiments in the Appendix section, and open sourced the implementation to reproduce all their experiments. Given the complex nature of this model, they did a great job in writing a clear explanation, and provided enough details for the community to build biologically inspired models for deep networks. Even without the code, I felt I might have been able to implement most of the model given the detail and clarity of the writing, so having both available is a great contribution.

I highly recommend this paper for acceptance, with a score of 8 (edit: revised to 9 after rebuttal period). The paper might warrant a score of 9 if they had also achieved higher quality results for image generation, on Celeb-A or demonstrated results on ImageNet, and provided more detailed analysis about drawbacks of their approach vs conventional generative models.

[1] https://arxiv.org/abs/1502.04623
[2] https://arxiv.org/abs/1807.03039

","The sentiment of the review is highly positive, as evidenced by phrases like 'strong accept,' 'beautifully written,' and 'great contribution.' The reviewer also revised their score from 8 to 9, indicating a very favorable opinion of the paper. Therefore, the sentiment score is 90. The politeness of the language is also very high. The reviewer uses polite and encouraging language throughout, such as 'I highly recommend,' 'great job,' and 'excited to see more.' There are no instances of rudeness or harsh criticism, so the politeness score is 100.",90,100
"
The authors provide new generalization bounds for recurrent neural networks.
Their main result is a new bound for vanilla RNNs, but they also have
bounds for gated RNNs.

They claim that their vanilla bound improves on an earlier
bound for RNNs in Section 6 of an ICML'18 paper by Zhang, et al.
The main result of the submission is incomparable in strength with the earlier result,
because this submission assumes that the activation functions in the hidden
layers are bounded, where the earlier paper did not.  Part of the difference in the results
(roughly speaking, the ""min"" in the bound) can be traced to this difference in the assumptions. 
 (This paper uses this assumption in the second-to-last line of the proof of Lemma 6.)

I think that the root cause of the remaining difference is that this paper,
at its core, adapts the more traditional analysis, used in Haussler's
1992 InfComp paper.  New analyses, like from the Bartlett, et al
NIPS'17 paper, strove for a weak dependence in the number of parameters,
but this proof technique appears to lead to a worse dependence on the
depth.  I think that, if you unwind the network, to view the function
from the first t positions of the input to output number t as a
depth t network, and apply Haussler's bound, you will get a qualitatively
similar result (in particular with bounds that scale polynomially with
d and t).  I think that Haussler's proof technique can be adapted to
take advantage of the weight sharing between layers in the unrolled
network.  

It is somewhat interesting to note that the traditional bounds have
a better dependence on depth, with correspondingly better dependence
on the length of the output sequence of the RNN.

I also do not see that substantial new insight is gained through the
analysis that incorporates gating.

I do not see much technical novelty in this paper.



","The sentiment of the review is generally negative, as the reviewer points out several limitations and lacks enthusiasm for the contributions of the paper. The reviewer mentions that the main result is 'incomparable in strength' with earlier results and that there is 'not much technical novelty.' The politeness of the language is relatively high, as the reviewer uses phrases like 'I think' and 'it is somewhat interesting to note,' which soften the critique and make it more constructive.",-60,80
"The paper presents a novel hierarchical clustering method over an embedding space. In the presented approach, both the embedding space and the hierarchical clustering are simultaneously learnt. The hierarchical clustering algorithm aims to recover complex clustering hierarchies which cannot be captured by previously proposed methods. 

The paper address a relevant problem, which is of great interest for extracting knowledge from data. In general, the quality of the paper is high. The presented approach is based on a sound formalization of hierarchical clustering and deep generative models. The paper is easy to follow in spite of the technical difficulty. The experimental evaluation is really extensive. It compares against many state-of-the-art methods. And the results are promising from both a quantitative and qualitative point view. 

The only issue with this paper is its degree of novelty, which is narrow. The proposed method adapt a previously presented hierarchical clustering method in the ""standard space"" (Griffiths et al., 2004) to an embedding space defined by a variational autoencoder model. The inference algorithm builds on standard techniques of deep generative models and, also, on previously proposed methods (Wand and Blei, 2003) for dealing with the complex hierarchical priors involved in this kind of models.","The sentiment of the review is largely positive, as the reviewer praises the relevance of the problem addressed, the quality of the paper, the sound formalization, the clarity despite technical difficulty, and the extensive experimental evaluation with promising results. The only criticism is about the degree of novelty, which is mentioned as narrow. This minor criticism does not significantly detract from the overall positive sentiment. The language used is polite and constructive, with no negative or rude remarks. The reviewer provides specific references to previous work, which is done in a respectful manner.",80,90
"Authors provide a variant of WGAN, called PC-GAN, to generate 3D point clouds. The drawback of a vanilla GAN with a DeepSet classifier is analyzed. The rationality that decoupling the point generator with the object generator is also discussed. 
A sandwiching objective function is proposed to achieve a better estimation of Wasserstein distance. 
Compared with AAE and the simplified variants of the proposed PC-GAN, the proposed PC-GAN achieves incremental results on point cloud generation.

Comments:
1. Authors calculate W_U in a primal form via solving an assignment programming problem. Have authors ever tried Sinkhorn iteration? To my knowledge, sinkhorn iteration is a very popular method to solve OT problem effectively. It would be nice if authors can provide some reasons and comparisons for their choice on the optimizer of W_U. 

2. Authors proved that the sandwiching object W_s is closer to the real Wasserstein distance, but it increases the variance of the loss function. Specifically, the dynamics of W_U, and W_L, according to lemma1, is (epsilon2-epsilon1)*w(P, G) while the dynamics of W_s is 2*epsilon1 * w(P, G), and 2epsilon1 > epsilon2 - epsilon1 (according to the assumption in lemma 1). Does it mean that the W_s is not as stable as W_L or W_U during training?  Additionally, authors combined W_U with W_L with a mixture 20:1, i.e., the s in Eqs(6, 13, 14) is smaller than 0.05. In such a situation, both the value and the dynamics of W_s will be very close to that of W_U. Does it mean that W_L is not so important as W_U? Authors should analyze the stability of their method in details.

Essentially, the proposed method is a variant of WGAN, which estimates Wasserstein distance with lower bias but may suffer from worse stability. In the experiments, both the setting and the experimental results show that the proposed W_s will be very close to W_U. As a result, the improvement caused by the proposed method is incremental compared with its variants. 

Typos:
- The end of the 2nd line of lemma 1: P, G should be \mathbb{P}, \mathbb{G}
- The 3rd line of lemma 1: epsilon1 -> epsilon_1
- Page 14, Eq(14), \lambda should be s
- Page 14, Eqs(13, 14), w(\mathbb{P}, \mathbb{G}) should appear on the right.
","The sentiment of the review is generally neutral to slightly positive. The reviewer acknowledges the incremental improvements made by the proposed PC-GAN but also points out several areas for improvement and clarification. The politeness of the language is high, as the reviewer uses polite phrases such as 'It would be nice if authors can provide' and 'Authors should analyze,' and provides constructive feedback without being harsh or dismissive.",10,80
"This paper focuses on the alignment of different Knowledge Graphs (KGs) obtained from multiple sources and languages - this task is similar to the Link Prediction setting, but the objective is learning a mapping from the entities (or triples) in one Knowledge Graph to another. In particular, the paper focuses on the setting where the number of available training alignments is small.

The model and training processes are slightly convoluted:
- Given a triple in the KG A, the model samples a candidate aligned triple from a KG B, from a (learned) alignment distribution.
- The objective is a GAN loss where the discriminator needs to distinguish between real and generated alignments.
The objective is non-differentiable (due to the sampling step), and it's thus trained via policy gradients.

Question: to me it looks like the whole process could be significantly simpler, and end-to-end differentiable. For instance, the loss may be the discrepancy between the alignment distribution and the training alignments. As a consequence, the whole procedure would be significantly more stable; there would be no need of sampling; or tricks for reducing the variance of the gradient estimates. What would happen with such a model? Would it be on par with the proposed one?

The final model seems to be better than the considered baselines.
","The sentiment of the review appears to be neutral to slightly positive. The reviewer acknowledges the complexity of the model and training processes but also suggests a potentially simpler and more stable alternative. The final remark that the model seems to be better than the considered baselines adds a positive note. Therefore, the sentiment score is 20. The politeness of the language is quite high. The reviewer poses their critique and suggestions in a constructive and respectful manner, using phrases like 'to me it looks like' and 'what would happen with such a model?' which indicate a polite and inquisitive tone. Therefore, the politeness score is 80.",20,80
"The paper proposes the inclusive neural random field model. Compared the existing work, the model is different because of the use of the inclusive-divergence minimization for the generative model and the use of stochastic gradient Langevin dynamics (SGLD) and stochastic gradient Hamiltonian Monte Carlo  (SGHMC) for sampling. Experimental results are reported for unsupervised, semi-supervised, and supervised learning problems on both synthetic and real-world datasets. Specific comments follow:

1. A major concern of the reviewer is that, given the related work mentioned in Section 3, whether the proposed method exerts substantial enough contribution to be published at ICLR. The proposed method seems like an incremental extension of existing works.

2. A major claim by the authors is that the proposed techniques can help explore various modes in the distribution. However, this claim can only seem easily substantiated by experiments on synthetic data. It is unclear whether this claim is true in principle or in reality.

Other points:
3. the experimental results of the proposed method seems marginally better or comparable to existing methods, which call in question the necessity of the proposed method.

4. more introduction to the formulation of the inclusive-divergence minimization problem could be helpful. The presentation should be self-contained.

5. what makes some of the statistics in the tables unobtainable or unreported?


============= After Reading Response from Authors ====================

The reviewer would like to thank the authors for their response. However, the reviewer is not convinced by the authors’ argument. 

“The target NRF model, the generator and the sampler are all different.”
It is understandable that modeling continuous data can be substantially different from modeling discrete data. Therefore, it is non-surprising that the problem formulations are different.

As for SGLD/SGHMC and the corresponding asymptotic theoretical guarantees, this reviewer agrees with reviewer 2’s perspective that it is a contribution made by this paper. But this reviewer is not sure whether such a contribution is substantial enough to motivate acceptance.

The explanation for better mode exploration of the proposed method given by the authors are the sentences from the original paper. The reviewer is aware of this part of the paper but unconvinced.

In terms of experiments, sample generation quality seems to be marginally better. Performances in multiple learning settings are comparable to existing methods.

A general advice on future revision of this paper is to be more focus, concrete, and elaborative about the major contribution of the paper. The current paper aims at claiming many contributions under many settings. But the reviewer did not find any of them substantial enough.

","The sentiment of the review is generally negative, as the reviewer expresses significant concerns about the novelty and substantial contribution of the proposed method. The reviewer repeatedly questions the necessity and impact of the work, indicating a sentiment score of -60. The politeness of the language used is relatively high, as the reviewer thanks the authors for their response and provides constructive feedback without using harsh or rude language, indicating a politeness score of 80.",-60,80
"This paper poses and addresses the problem of language drift in multi-agent communication paradigms. When two pretrained natural-language agents are jointly optimized to communicate and solve some external non-linguistic objective, their internal communication often diverges to a code-like, unnatural communication system. This paper solves this “language drift” problem by requiring that the messages between agents be usable as inputs to an image caption retrieval system. They demonstrate that the jointly optimized agents perform best when regularized in this manner to prevent language drift.

1. Framing: I’m uncertain about the framing of this paper. The authors pose the problem of “language drift,” which is indeed a frequent problem in multi-agent communication tasks where the principle supervision involves non-linguistic inputs and outputs. They then design a three-language MT task as a test case, where the inputs and outputs are both linguistic. Why attack this particular task and grounding solution? I can imagine some potential goals of the paper, but also see more direct ways to address each of the potential goals than what the authors have chosen:
1a. Study how to counter language drift in general — why not choose a more intuitive two-agent communication task, e.g. navigation, game playing, etc.?
1b. Study how to counter language drift in the MT task — aren’t there simpler solutions to prevent language drift in this particular task? e.g. require “cycle-consistency” – that it be possible to reconstruct the French input using the French output? Why pick multimodal grounding, given that it imposes substantial additional data requirements?
1c. Build a better/more data-efficient machine translation system — this could be an interesting goal and suitable for the paper, but this doesn’t seem to be the framing that the authors intend.

2. Interpretation of first results:
2a. Thanks for including standard deviation estimates! I think it’s also important that you do some sort of significance testing on the comparison between PG+LM+G and PG+LM performance for Fr->En->De — these numbers look pretty close to me. You could run e.g. a simple sign test on examples within each corpus between the two conditions.
2b. It would also be good to know how robust your results are to hyperparameter settings (especially the entropy regularization hyperparameter).

3. Token frequency results: These are intriguing but quite confusing to me!
3a. How sensitive are these results to your entropy regularization setup? How does PG behave without entropy regularization?
3b. Table 6 shows that the PG model has very different drift for different POS categories. Does this explain away the change in the token frequency distribution? What do the token frequency effects look like for PG within the open-class / content word categories (i.e., controlling for the huge difference in closed-class behavior)?

4. Minor comments:
4a. There’s a related problem in unsupervised representation learning for language. Work on VAEs for language, for example, has shown that the encoder often collapses meaning differences in the latent representation, and leans on an overly powerful decoder in order to pick up all of the lost information. It would be good to reference this work in your framing (see e.g. Bowman et al. (2015)).
4b. In sec. 3.1 you overload notation for R. Can you subscript these so that it’s especially clear in your results which systems are following which reward function?
4c. Great to show some qualitative examples in Table 7 — can you explicitly state where these are from (dev set vs. test set?) and whether they are randomly sampled?

References:
Bowman et al. (2015). Generating sentences from a continuous space. https://arxiv.org/abs/1511.06349
","The sentiment of the review is mixed but leans towards positive. The reviewer acknowledges the importance of the problem addressed and appreciates the inclusion of standard deviation estimates and qualitative examples. However, they also express uncertainty about the framing and suggest alternative approaches. The sentiment score is 20 because the review contains both positive and critical elements but maintains a constructive tone. The politeness score is 80 because the reviewer uses polite language, thanks the authors for certain aspects, and provides constructive feedback without being rude or dismissive.",20,80
"This paper proposed a progressive weight pruning approach to compress the learned weights in DNN. My major concerns about the paper are as follows:

1. Novelty: The proposed approach heavily relies on the one in (Zhang et. al. 2018b) as shown in Sec. 2.2 for 1 page, making the paper as being an incremental work, like finding better initialization for (Zhang et. al. 2018b).

2. Faster convergence: First of all, from Fig. 3 I do not believe that both methods converged, as both performances vary a lot with significant gaps. In terms of being faster, I do not think that it makes sense by comparing numbers of epochs in training with only one approach. There is no theoretical or empirical evidence (e.g. running time) to support this claim.

3. I do not understand how the proposed approach is motivated by DP. To me it is more like a greedy search algorithm, while DP has the ability to locate global maximum. Does the proposed approach guarantee to find the maximum accuracy? Also, in Fig. 2 why was the best partial model replaced with the new one, rather than the worse one? There is no explanation to this at all. Besides, I do think this approach is very heuristic, same as some other approaches in the related work.

4. Experiments: Since the performance varies a lot as shown in Fig. 3, how are the numbers calculated? Average? Best one? With/without cross-validation to tune parameters? How much gain in terms of running time in testing can you get with more compact models in practice? A training/testing behavior analysis is highly appreciated.
","The sentiment of the review is generally negative, as the reviewer expresses major concerns about the novelty, convergence, motivation, and experimental validation of the proposed approach. The reviewer questions the originality of the work, the validity of the claims regarding faster convergence, the theoretical foundation of the approach, and the robustness of the experimental results. The politeness of the language used is relatively neutral. While the reviewer is critical, they do not use rude or overly harsh language. The feedback is direct and focused on specific issues, which is typical for scientific peer reviews.",-70,10
"This paper proposed a query-synthesis-based active learning algorithm that uses GAN to generate high entropy sample; instead of annotating the synthesized sample, the paper proposed to find the most similar unlabeled data from the pool via nearest neighbor search, with the latter is the main contribution of the paper.

Pros: 
(1)	the paper is well written and easy to follow;
(2)	evaluations look reasonable and fair

Cons:
(1)	The idea of using GAN for active query synthesis isn’t new. As the authors pointed out, this idea is mainly from GAAL (Zhu & Bento 2017). The main difference is sample matching that searches the nearest neighbor from pool and add the real unlabeled data for AL. So the novelty of the paper isn’t significant.
(2)	In terms of accuracy comparison, on Cifar-10-ten classes experiments, all ASAL variants have similar accuracies as random sampling, while traditional pool-based max-entropy clearly works much better. Although the former is much faster (O(1) vs. O(N)), this benefit is mainly due to GAAL (Zhu & Bento 2017).

The paper provides additional evidence showing that GAN-based active learning might be an interesting research direction for active query synthesis. However, given the reasons above, particularly novelty, I think the authors might need to additional work to improve the method.
","The sentiment of the review is mixed but leans slightly positive. The reviewer acknowledges that the paper is well-written and the evaluations are reasonable, which contributes positively. However, the reviewer also points out significant drawbacks, such as the lack of novelty and the underperformance in accuracy comparison, which brings down the sentiment. Therefore, the sentiment score is around -20. The politeness of the language is quite high; the reviewer uses polite language throughout, such as 'well written,' 'easy to follow,' and 'might need additional work,' which indicates a high level of politeness. Therefore, the politeness score is 80.",-20,80
"In this paper, the authors present how to integrate replay buffer and on-policy trust region policy optimization (TRPO) by generalizing Q/V/advantage function and then empirically show the proposed method outperforms TRPO/DDPG.

The generalization of advantage function is quite interesting and is well written. One minor issue is that d^{\pi_n} (s) is confusing since it appears after ds. 

The theory in Section 3.1 makes sense. However, due to the limitation in Theorem 1 that $\theta$ is the joint parameters, applying Theorem 1 can be difficult. In Eq (25), what is the $\theta$ here? And what does $\nabla_\theta \pi_n$ mean? Does $\pi_n$ uses $\theta$ for computation? One of the problems of using replay buffers in on-policy algorithms is that the stationary distribution of states changes as policy changes, and at least the writing doesn't make it clear on how to solve distribution mismatching issue. Further explanation on Eq (25) might help. If the distributions of states are assumed to match, then the joint distribution of states and actions may mismatch so additional importance sampling might help, as suggested in [1] Eq (3). 

Another problem is on the barrier function. In Eq (26), if we only evaluate $\rho_b(\theta)$ (or its gradient w.r.t. $\theta$) at the point $\theta_old$, it doesn't differ with or without the barrier function. So in order to show the barrier function helps, we must evaluate $\rho_b(\theta)$ (or its gradient) at a point $\theta \neq \theta_old$. As far as I know, the underlying optimizer, K-FAC, just evaluates the objective's (i.e., $\rho_b$) gradients at $\theta_old$. Both Conjugate Gradient (CG), which TRPO uses, and K-FAC are trying to solve $F^{-1} g$ where $g$ is the gradient of the objective at the current point. 

The experiments show significant improvement over TRPO/DDPG. However, some experiments are also expected.
1. How is the proposed algorithm compared to PPO or Trust PCL? 
2. How does the barrier function help? More importantly, what's the comparison of the barrier function to [1] Eq (5)? 

The proposed algorithm seems more like a variant of ACKTR instead of TRPO since line search is missing in the proposed algorithm and the underlying optimizer is K-FAC instead of CG.

Ref:
[1]: Proximal Policy Optimization Algorithms, by John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov. 
","The review starts with a positive sentiment by acknowledging that the generalization of the advantage function is interesting and well-written. However, it quickly transitions into a series of critical points and questions about the theoretical aspects and the experimental results. The reviewer raises several concerns and requests further explanations, which indicates a mixed sentiment overall. The language used is polite and constructive, as the reviewer uses phrases like 'One minor issue,' 'Further explanation might help,' and 'As far as I know,' which soften the critique and suggest improvements rather than outright dismissing the work.",20,80
"Summary: The authors study building models for edits in source code. The application is obvious: a system to accurately predict what the next edit should be would be very valuable for developers. Here, edits are modeled by two types of sequences: one that tracks the state of all edits at each time step (and is thus very long), and one that contains the initial step and a changelist that contains the minimal information required to derive the state at any time. The authors train models on top of both of these representations, with the idea being to match the performance of the explicit (heavy) model with the implicit model. This is shown to be challenging, but a clever model is introduced that achieves this, and is thus the best of both worlds. There are synthetic and real-world code (text) edit experiments.

Strengths: The problem is well-posed and well-motivated. There's a nice application of powerful existing models, combined and tailored to the current work. The writing is generally quite clear. The number of experiments is quite solid. 

Weaknesses: The main flaw is that nothing here is really specifically for souce code; the authors are really just modeling edits in text sequences. There's not an obvious way to integrate the kinds of constraints that source code typically satisfies either. There's some confusion (for me) about the implicit/explicit representations. More questions below.

Verdict: This is a pretty solid paper. It doesn't quite match up to its title, but it sets out a clearly defined problem, achieves its aims, and introduces some nice tricks. Although it doesn't produce anything genuinely groundbreaking, it seems like a nice step forward.

Comments and Questions:

- The problem is written in the context of source code, but it's really setup just for text sequences, which is a broader problem. Is there a way the authors take can advantage of the structural requirements for source code? I don't see an obvious way, but I'm curious what the authors think.

- What's the benefit of using the implicit representation for the positions? The explicit/implicit  position forms are basically just using the permutation or the inverse permutation form, which are equivalent. I don't see directly what's saved here, the alphabet size and the number of integers to store is the same.

- Similar question. The implicit likelihood is s^0, e^(1),...,e^(t-1), with the e^(i)'s being based on the implicit representations of the positions. Seems like you could do this with the *explicit* positions just fine, they carry enough information to derive s^(i) from s^(i-1). That is, the explicit/implicit problems are not really related to the explicit/implicit position representations.

- Just wanted to point out that this type of approach to sequences and edits has been studied pretty often in the information/coding theory communities, especially in the area of synchronization. There, the idea is to create the minimal ""changelist"" of insertions/deletions from two versions of a file. This could come in handy when building the datasets. See, for example, Sala et al ""Synchronizing files from a large number of insertions and deletions"".

- The problem statement should be stated a bit more rigorously. We'd like to say that the initial state is drawn from some distribution and that the state at each time forms a stochastic process with some transition law. As it stands the problem isn't well-defined, since with no probability distribution, there's nothing to predict and no likelihood.

- The ""analogical decoder"" idea is really nice.

- For the synthetic dataset, why are you selecting a random initial string, rather than using some existing generative text or source code model, which would get you synthetic data that more closely resembles code?

- I really liked the idea of using an oracle that gives the position as upper bound. Would it make sense to also have the opposite oracle that gives the edit symbol, but doesn't tell the location? I'm really curious which is the ""harder"" task, predicting the next symbol or the next location. In the information-theory setting, these two are actually equally hard, but the real-world setting might be pretty different. It would also be interesting to train models on top of the POMP. That would produce genuine upper bounds to the model performances. 

- The explicit baseline model performs very well on all the edit types in Table 1. Are there cases where even this explicit case works poorly? Is the improved implicit model *always* upper bounded by the explicit model (to me it seems like the answer should always be yes, but it would be interesting to check it out for cases where explicit is not very high accuracy). ","The sentiment of the review is generally positive. The reviewer acknowledges the strengths of the paper, such as the well-posed problem, clear writing, and solid number of experiments. However, they also point out some weaknesses and areas for improvement, such as the lack of specificity to source code and some confusion regarding the implicit/explicit representations. The overall verdict is that the paper is solid and a nice step forward, even if not groundbreaking. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, even when pointing out weaknesses and asking questions. Therefore, the politeness score is 90.",60,90
"Summary:
The authors present a straightforward method to improve generative quality of GANs that can allow for fewer parameters by separating the task into a basic-generation followed by a chain of multiple edits to the base generation with different editor networks. The fact that separating the task allows for smaller networks is key to the reduction in parameters.  Each editor is trained separately in alternance, with its associated critic. Authors test their approach mostly on CIFAR10, as well as CelebA and MNIST.

Pros:
- The proposed method is simple and makes intuitive sense. Having editor generators acting like highly-conditioned GANs should make their job easier to produce better samples.

- Empirical results show that when removing editors for evaluation, some well-known architecture (DCGAN - WGAN+GP) can be outperformed with less parameters when comparing IS scores.

- Interesting leads and negative results are discussed in Section 5.

Cons : 
- Comparisons with end-to-end training seems inadequate. The authors invalidate the end-to-end approach with two arguments; (1) that it doesn’t allow for the removal of superfluous editors once the training is done (Section 3.3), and (2) that IS scores are significantly lower (Section 4.2, Table1). It seems that both these statements are true simply because end-to-end learning is performed with a single score outputted by a single critic at the end of the chain, while it would be entirely possible and simple to keep all discriminators and associated losses and train end-to-end. This would still push all editors to produce good samples, thus allowing removal of editors at test-time, and would probably yield better results than those reported in Table 1. It could also invalidate the results shown in Figure 4 (left). For me this is an important missing comparison, as it might even yield better results than the proposed approach and invalidates one of the proposed advantage of the method.

- I think this idea of sequential generation has been explored before, (e.g. StackGAN [1, 2] or LAPGAN [3] and others?), in which unconditional image generation is performed on relatively complicated datasets with a somewhat more principled way of actually simplifying the task of the base generator. Therefore, I think important citations and valid comparisons are missing.

- The only reported metric is the Inception Score (IS), while most of the recent literature agrees that the Fréchet Inception Distance is a better metric. I think FID should be presented as well to be better aligned with recent literature, even in cases where comparison with previously reported performance is impossible (if previous works only presented IS). If you want to be compared to in future work, I think this is necessary.

- It would be a good addition to have FID/IS scores for each editor output, as we could see the quantitative increase in performance at each editing step.

- In Section 4.2, you specify that all experiments are done using the WGAN-GP training formulation. Looking at Table 1 this is unclear, as you specify this training scheme only for model 6 and model 9. If all models use the same training scheme, this information should be absent from the Table.

- CelebA results. Experiments are reported in the main text, without any results, which are only in the Appendix. These results don’t show any quantitative metrics and are visually disappointing. It is hard to see if the editors actually improve the generation.

- Some of the main results or discussions are based on Section 7, which is not the main article even though it is used as another Section instead of an Appendix. I think Section 7 should be separated into Appendices A, B, etc. Maybe some important aspects of the research presented could fit into the main text, given some removal of repetitions, and some compression of the intro to GANs, which should be vastly known by the ICLR community by now.

- Wrong citation format at the end of Section 2.1.

- Section 3.3 : “train the critic more than the generator in each iteration”. This could be clarified by stating exactly what you do (training the critic for k steps for each generator steps).

- It’s not always clear what the boldface represents throughout tables.

- The discussion in Section 5 about promising techniques explored is somewhat disappointing as efforts to investigate why training failed are not apparent.

- Every result shown from the proposed method is performed with ‘small’ or ‘tiny’ versions of existing architectures. This method could have additional value if it could boost performance on the same architecture, even if there are added editors and trainable parameters. The fact that such results are absent makes me suspicious of such a behavior.

Overall I think this paper presents a relevant and interesting idea. However I think this idea has been explored before with more convincing results and with a more principled approach. There are some important flaws in the comparisons made to assess the advantages of the method, and the overall results fail to convince of any important benefit. Based on the pros/cons stated above, I think this paper does not reach the required quality for acceptance in ICLR.

[1] StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks (Zhang et al. 2017)
[2] StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks (Zhang et al. 2017)
[3] Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks (Denton et al. 2015)
","The sentiment of the review is mixed but leans towards negative. The reviewer acknowledges some positive aspects of the paper, such as the simplicity and intuitive nature of the proposed method, as well as some empirical results. However, the majority of the review focuses on significant criticisms, including inadequate comparisons, missing citations, and the lack of certain metrics. The overall conclusion is that the paper does not meet the required quality for acceptance. Therefore, the sentiment score is -40. The politeness of the language is generally high. The reviewer uses polite and professional language throughout, even when pointing out flaws and making critical comments. Therefore, the politeness score is 80.",-40,80
"The authors propose to include phrases (contiguous n-grams of wordpieces) in both the self-attention and encoder-decoder attention modules of the Transformer model (Vaswani et al., 2017). In standard multi-head attention, the logits of the attention distribution of each head is computed as the dot-product between query and key representations, which are position-specific. In the phrase-based attention proposed here, a convolution is first computed over the query, key, value sequences before logits are computed as before (a few variants of this scheme are explored). Results show an improvement of up to 1.3 BLEU points compared to the baseline Transformer model. However, the lack of a controlled experiment sheds substantial doubt on the efficiacy of the model (see below).

Contributions
-------------------
Proposes a simple way to incorporate n-grams in the Transformer model. The implementation is straightforward and should be fully replicable in an afternoon.

Having an inductive bias towards modeling of longer phrases seems intuitively useful, in particular when using subword representations, where subword units are often ambiguous. This is also motivated by the fact that prior work has shown that subword regularization, where sampling different subword segmentations during training can be useful.

Improvements in BLEU scores are quite strong.

Issues
---------
The experiments do not control for parameter count. The phrasal attention model adds significant number of parameters (e.g., ""interleaved attention"" corresponds to 3x the number of 1D convolution parameters in the attention layer). It is well established that more parameters correspond to increased BLEU scores (e.g., the 2x parameter count in the ""big"" Transformer setting from Vaswani et al. (2017) results in over 1 BLEU point improvement). This needs to be fixed!

The model is a very modest extension of the original Transformer model and so its value to the community beyond improved numbers is somewhat questionable.

While an explicit inductive bias for phrases seem plausible, it may be that this can already be fully captured by multi-head attention. With positional encodings, two heads can easily attend to adjacent positions which gives the model the same capacity as the convolutional phrase model. The result in the paper that trigrams do not add anything on top of bigrams signals to me that the model is already implicitly capturing phrase-level aspects in the multi-head attention. I would urge the authors to verify this by looking at gradient information (https://arxiv.org/abs/1312.6034).

There are several unsubstantiated claims: ""Without specific attention to phrases, a particular attention function has to depend entirely on the token-level softmax scores of a phrase for phrasal alignment, which is not robust and reliable, thus making it more difficult to learn the mappings."" - The attention is positional, but not necessarily token-based. The model has capacity to represent phrases in subsequent layers. WIth h heads , a position in the k-th layer can in principle represent h^k grams (each slot in layer 2 can represent a h-gram and so on).

The differences in training setup compared to Vaswani et al. (2017) needs to be explicit (""most of the training settings"" is too handwavy). Please list any differences.

The notation is somewhat cumbersome and could use some polishing. For example, the input and output symbols both range over indices in [1,n]. The multi-head attention formulas also do not match the ones from Vaswani et al. (2017) fully. Please ensure consistency and readability of symbols and formulas.

The model inspection would be much improved by variance analysis. For example, the numbers in table 3 would be more useful if accompanied by variance across training runs. The particular allocation could well be an effect of random initialization. I could also see other reasons for this particular allocation than phrases being more useful in intermediate layers (e.g., positional encodings in the first layer is a strong bias towards token-to-token attention, it could be that the magnitude of convolved vectors is larger than the batch-normalized unigram encodings, so that logits are larger.

Questions
--------------
In ""query-as-kernel convolution"", it is unclear whether you map Q[t, :] into n x d_q x d_k convolution kernel parameters, or if each element of the window around Q[t] of width n is mapped to a convolution kernel parameter. Also what is the exact form of the transformation. Do you transform the d_q dimensional vectors in Q to a d_q x d_k matrix? Is this done by mapping to a d_q * d_k dimensional vector which is then rearranged into the convolution kernel matrix?

Does the model tend to choose one particular n-gram type for a particular position, or will it select different n-gram types for the same position?

""The selection of which n-gram to assign to how many heads is arbitrary"" - How is this arbitrary? This seems a rather strong inductive bias?

""However, the homogeneity restriction may limit the model to learn interactions between different n-gram types"" - How is the case? It seems rather that the limitation is that the model cannot dynamically allocate heads to the most relevant n-gram type?

I do not understand equation 14. Do you mean I_dec = I_cross = (...)?

""Phrase-to-phrase mapping helps model local agreement, e.g., between an adjective and a noun (in terms of gender, number and case) or between subject and verb (in terms of person and number)."" Is this actually verified with experiments / model inspection?

""This is especially necessary when the target language is morphologically rich, like German, whose words are usually compounded with sub-words expressing different meanings and grammatical structures"" This claim should be verified, e.g. by comparing to English-French as well as model inspection.
","The sentiment of the review is mixed but leans towards the negative side. While the reviewer acknowledges some positive aspects, such as the simplicity and potential usefulness of the proposed method, they also express significant concerns about the lack of controlled experiments, the modest extension of the original model, and several unsubstantiated claims. Therefore, the sentiment score is -30. The politeness of the language is generally respectful and constructive, with the reviewer providing detailed feedback and suggestions for improvement without using harsh or rude language. Thus, the politeness score is 70.",-30,70
"The paper proposes a new representation of Wasserstein AutoEncoder and provides the formal analysis of learning autoencoders with optimal transport theory. The proposed model, SAE, employs the constraints on the equality of prior and posterior latent spaces with a Sinkhorn distance. Moreover, the proposed model is also backed up with some theoretical guarantees.

The paper is well-written and easy to follow. The experimental results with different priors have demonstrated the effectiveness of the newly formulated model. However, it is not convinced that what is the advantages of the proposed model with WAE. Can the authors provide more insight and comparison with its counterpart, WAE?

In term of time-complexity, computing Sinkhorn distance in Alg 1 introduces computation overhead, especially with small \epsilon. In compared with WAE, what is the computation overhead of the proposed model? Can the authors provide some theoretical analysis of time-complexity and experimental results?

In Table 2, there are only FID values for WAE with MMD cost. Can the authors show the numbers with WAE-GAN on these datasets?

Conclusion: The theoretical and experimental contributions are significant to publish at the venue.","The sentiment of the review is generally positive, as indicated by phrases like 'well-written and easy to follow' and 'theoretical and experimental contributions are significant to publish at the venue.' However, there are some critical points raised, such as the need for more insight and comparison with WAE, and concerns about time-complexity and computation overhead. Therefore, the sentiment score is not extremely high but still positive. The language used is polite and constructive, with requests framed as questions and suggestions rather than demands, indicating a high politeness score.",60,90
"This paper uses a mixed strategy perspective for GANs. With this formulation the non-convex game formulation of GANs can be transformed into a infinite dimensional problem analog to a finite dimensional bilinear problem.  

I really like this approach, that tries to find methods that converge globally to (mixed) Nash equilibriums. However I have some concerns. 

- I'm concerned about the definition of a $O(T^{-1})-NE$. Actually, this merit function is not standard for game. It can be 0 even if $x_t,y_t$ is far from the equilibrium (for instance for the problem $\min_{x \in \Delta_d}\max_{y \in \Delta_d} x^\top y$ with $x_t = (1,0,\ldots,0)$ and $y_t= (F(x_{NE},y_{NE}),1-F(x_{NE},y_{NE}),0,\ldots,0)$ we have $F(x_t,y_t) = F(x_{NE},y_{NE})$ but $x_{NE} = y_{NE} =(1/d,\ldots,1/d)$). One merit function that could be considered is $\max_{y} F(x,y_t) - \min_{y} F(x_t,y)$. 

- There is a gap between the theory and the practical method that could be bridged. Actually Theorem 2 assume that the stochastic derivatives are unbiased but since your Langevin dynamics gives you an *approximate* of the next distribution an analysis taking into account this bias would provide much stronger results. More precisely, it would be interesting to have a result similar as Theorem 2 with conditions on $\epsilon_t$ and $K_t$. For instance, if the theoretical $K_t$ is too large it would reduce the interest of your algorithm. I think this analysis is key since it allows to claim that you can properly approximate the distributions of interest.

If you are able to ease these concerns I'm eager to increase my grade.


- ""(5) is exactly the infinite-dimensional analogue of (1):"" Actually it is not exactly the analogue since $<.,.>$ is not a scalar product anymore (particularly, $<g,\mu>$ is not defined) but the canonical pairing between a space and its dual (we are loosing something going to infinite dimension).
I think it should be clarified somewhere. 

Minor comments: 
- on the updates rules of $\theta$ and $\omega$ (Page 6) the Gaussian noises are missing. 
- On algorithm 3,4,5 and 6 the Gaussian noise is too wide and causes an Overfull.

=== After Authors response ===
The authors fixed some major issues. That is why I improved my grade. 
However I'm still concerned about the scalability of this algorithm
","The sentiment of the review is generally positive, as indicated by phrases like 'I really like this approach' and the willingness to increase the grade if concerns are addressed. However, the reviewer does express some significant concerns and provides detailed feedback for improvement. Therefore, the sentiment score is 50. The politeness of the language is high, as the reviewer uses polite language throughout, such as 'I'm concerned about,' 'it would be interesting to,' and 'I think this analysis is key.' The reviewer also acknowledges improvements made by the authors in the response. Therefore, the politeness score is 80.",50,80
"Brief Summary:
The authors present a novel adversarial attack on node embedding method based on random walks. They focus on perturbing the structure of the network. Because the bi-level optimization problem can be highly challenging, they refer to factorize a random walk matrix which is proved equivalent to DeepWalk. The experimental results show that their approach can effectively mislead the node embedding from multi-classification. 

Quality:
This paper is well-written except for some minor spelling mistakes

Clarity:
This paper is quite clear and easy to follow.

Originality:
This work follow the proposal Qiu et al.(WSDM'18)'s proof and present a novel approach to calculate the loss when the network changes by A'.

Pros:
1. Detailed proofs presented in appendix
2. They present 6 questions and answer them with effective experiments.
3. They present a new way to attack node embedding by factorizing a equivalent matrix.

Cons:
1. I have noticed that Zügner et al.(KDD'18) present an adversarial attack method on GCN for graph data. I think it is reachable by the day the authors submitted this paper. This is opposite to the first sentence ""Since this is the first work considering adversarial attacks on node embeddings there are no known
baselines"" said in Section 4.
2. The author present the time analysis of their approach but the efficiency result of their approach is not presented.
3. To enforce misclassification of the target node t, the author set the candidate flip edges as edges around t. Does this mean only the node's local edges can mislead the target node from downstream tasks? I think the authors should consider more candidate edges but this may lead to larger time complexity.
4. Figure. 4 tells that low-degree nodes are easier to mis-classify. If the baseline method B_{rnd} randomly select edges to flip among the local area of node t, I think the result should be similar to the proposed approach on low-degree nodes because the flipped edges should be the same. 

== I have read the rebuttal. Thanks for the response.","The sentiment of the review is generally positive, as indicated by phrases like 'well-written,' 'quite clear and easy to follow,' and 'novel approach.' However, there are some critical points raised in the 'Cons' section, which slightly temper the overall positivity. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer uses polite and constructive language throughout, such as 'I think the authors should consider' and 'Thanks for the response.' Therefore, the politeness score is 90.",60,90
"The main idea behind the paper is to use random projections as the initial word representations, rather than the vocab-size 1-hot representations, as is usually done in language modeling. The benefit is that the matrix which projects words into embedding space can then be much smaller, since the space of random projections can be much smaller than the vocab size. The idea is an interesting one, but this work is at too much of a preliminary stage for a top-tier conference such as ICLR. In its present state it would make for a potentially interesting paper at a targeted workshop.

More specific comments
--

The initial description of the language modeling problem assumes a particular decomposition of the joint probability, according to a particular application of the chain rule, but of course this is a modeling choice and not the only option (albeit the standard one).

The main problem with the paper is the use of simple baseline setups as the only experimental configuration:

o feedforward rather than recurrent network;
o use of the Penn Treebank dataset only;
o use of a small n for the n-grams.

All or at least some of these decisions would need to be relaxed to make a convincing paper.

The reasons for the use of the energy-based formulation are not clear to me. Is the energy-based model particularly well-suited to the random-projection setup, or are there other reasons for using it, independent of the use of random projections?

Just before equation 6 it says that the resulting vector representation is the *sum* of all the non-zero entries. But there are some minus ones in the random projection? 

The PPL expression at the bottom of p.5 doesn't look right. The index over which the sum happens is n, but n is fixed? So this looks like a sum with just one component in it, namely the first n-gram.

It looks like all the results are given on the test set. Did you not do any tuning on the validation data?

The plots in figure 4 are too small. It would be useful to have a table, like the one on the last page, which clearly shows the baseline vs. the random-projection model, with some description of the results in the main body of the text.

The overall presentation could be better, and I would encourage the authors to tidy the paper up in any subsequent submission. For example, there are lots of typos such as ""instead of trying to probability of a target word"".
","The sentiment of the review is mixed but leans towards the negative side. The reviewer acknowledges the interesting idea behind the paper but ultimately deems it too preliminary for a top-tier conference, suggesting it might be more suitable for a workshop. This indicates a sentiment score of around -30. The politeness of the language is generally high, as the reviewer provides constructive criticism and suggestions for improvement without being harsh or dismissive. This indicates a politeness score of around 70.",-30,70
"This paper introduces an interesting unifying perspective on several sequence generation training algorithms, exposing both MLE, RAML and SPG as special cases of this unified framework. This enables insightful new interpretations of standard issues in MLE training in terms of exploration for instance.
Based on this new perspective, a new algorithm is introduced. Its performance is analysed on a machine translation and a text summarisation task.

==> Quality and clarity
The paper is overall well-written, although it can be improved upon (see details below). The bibliography for instance does not reference the conference/journals where the articles were published and lists many (>10) published papers as arXiv preprints.

The ideas are clearly presented, which is crucial in a paper trying to unify different approaches, and the new perspective on exploration is well motivated.

==> Originality and significance
The unifying framework is interesting, and helps shed new light on some standard issues in sequence generation.
On the other hand, the new algorithm and its analysis seem like a slightly rushed attempt at leveraging the unifying framework. 
The experiments, in particular, present several issues.
- For instance, it's clear from Figure 3 that both MLE and RAML are overfitting and would benefit from more dropout (in the literature, 0.3 is commonly used for this type of encoder-decoder architecture). Having access to these experimental results is important, since it would enable the reader to understand whether the benefits of the new approach are subsumed by regularisation or not.
- Further, the performance of the competing methods seems a bit low. MLE reports 26.44 BLEU, which is a bit surprising considering that: 
   - with beam-search (beam of size 10, not 5, admittedly), Bahdanau et al (2016) get 27.56 BLEU, and this is without dropout.   
   - with dropout 0.3 (but without beam search), Leblond et al (2018) get 27.4 BLEU.
Making a strong case for the benefits of the new algorithm requires more thorough experiments.

Overall, the first half of the paper is interesting and insightful, while the second would benefit from more time. 

Pros
- clarity of the ideas that are presented
- interesting unifying perspective on sequence generation algorithms
- insightful new interpretations of existing algorithms in terms of exploration

Cons
- the example new algorithm is not very original
- the associated experiments are incomplete

==> Details
1. page 2, ""Dayan & Hinton (1997); Levine (2018); Abdolmaleki et al. (2018) study in a probabilistic inference perspective."" is an incomplete sentence.
2. at the beginning of section 3.1, policy optimisation is a family of algorithmS
3. page 7 in the setup of the experiments, ""We use the Adam optimizer for SGD training"" is incorrect since SGD is not a family but a specific algorithm, which is different from Adam.","The sentiment of the review is generally positive, as it acknowledges the interesting and insightful nature of the unifying perspective presented in the paper. However, it also points out significant issues with the new algorithm and its experimental validation, which tempers the overall sentiment. Therefore, the sentiment score is 40. The language used in the review is polite and constructive, offering specific recommendations for improvement without being harsh or dismissive. Thus, the politeness score is 80.",40,80
"This paper introduces a new convolutional layer named the Temporal Gaussian Mixture (TGM) layer, and present how it can be used for activity recognition. The kernels of the new layer are controlled by a set of (temporal) Gaussian distribution parameters, which significantly reduce learnable parameters. The results are complete on four benchmarks and show consistent improvement. I just have minor comments. 

1. I am curious what the learned feature look like. As the author mentioned, ""The motivation is to make each temporal Gaussian distribution specify (temporally) ‘where to look’ with respect to the activity center, and represent the activity as a collection/mixture of such temporal Gaussians convolved with video features."" So does the paper achieve this goal? 

Another thing is, can the authors extract the features after TGM layer, and maybe perform action recognition on UCF101 to see if the feature really works? I just want to see some results or visualizations to have an idea of what TGM is learning. 

2. How long can the method actually handle? like hundreds of frames? Since the goal of the paper is to capture long term temporal information. 

3. It would be interesting to see an application to streaming video. For example, surveillance monitoring of human activities. 



","The sentiment of the review is positive, as indicated by the acknowledgment of the complete results on four benchmarks and consistent improvement. The reviewer also mentions having only minor comments, which suggests a generally favorable view of the paper. Therefore, the sentiment score is 80. The politeness of the language is very high, as the reviewer uses phrases like 'I am curious,' 'I just want to see,' and 'It would be interesting,' which are polite and constructive. Thus, the politeness score is 90.",80,90
"This paper proposed a 3D scene parsing that takes both objects and their relations into account, extending the Factor3D model proposed by Tulsiani et al 18. Results are demonstrated on both synthetic and real datasets.

The paper is in general well written and clear. The approach is new, the results are good, the experiments are complete. However, I am still lukewarm about the paper and cannot champion it. I feel the paper interesting but not exciting, and it’s unclear what we can really learn from it. 

Approach-wise, the idea of using pair-wise relationship as an inductive bias is getting popular. This paper demonstrated that it can be used for scene parsing, too, within a neural net. This is good to know, but not surprising given what have been demonstrated in the extensive literature in the computer graphics and vision community. In particular, the authors should discuss many related papers from Pat Hanrahan’s group and Song-Chun Zhu’s group (see some examples below). Apart from that, this paper doesn’t have an obvious technical innovation that can inspire future work. This is different from Factor3D, which is the first voxel-based semantic scene parsing model from a single color image, with modern neural architecture.

The results are good, but are on either synthetic data, or using ground truth bounding boxes. Requiring ground truth boxes greatly restricts the usage of these models. Would that be possible to include results under the detection setting on NYU-D or Matterport 3D? The authors claimed that the gain of 6 points is significant; however, a simple interaction net achieves a gain of 5 points, so the technical contribution of the proposed model is not too impressive.

In general, I’m on the border but leaning slightly toward rejection, because this paper is very similar to Tulsiani et al, and the proposed innovation has been explored in various forms in other papers.

A minor issue:
-	In fig 5. The object colors are not matched for GT and Factor3D and ours.

Related work
Holistic 3D Scene Parsing and Reconstruction from a Single RGB Image. ECCV’18.
Configurable 3D Scene Synthesis and 2D Image Rendering with Per-pixel Ground Truth Using Stochastic Grammars. IJCV’18.
Characterizing Structural Relationships in Scenes Using Graph Kernels. SIGGRAPH’11.
Example-based Synthesis of 3D Object Arrangements. SIGGRAPH Asia’12.
","The sentiment of the review is mixed, leaning slightly towards negative. The reviewer acknowledges that the paper is well-written, clear, and presents good results, but expresses a lack of excitement and questions the novelty and technical innovation of the work. The sentiment score is therefore -20. The politeness of the language is quite high; the reviewer uses polite and constructive language throughout the review, even when pointing out weaknesses and suggesting improvements. The politeness score is 80.",-20,80
"This paper presents a new quasi-Newton method for stochastic optimization that solves a regularized least-squares problem to approximate curvature information that relaxes both the symmetry and secant conditions typically ensured in quasi-Newton methods. In addition to this, the authors propose a stochastic Armijo backtracking line search to determine the steplength that utilizes an initial steplength of 1 but switches to a diminishing steplength in later iterations. In order to make this approach computationally tractable, the authors propose updating and maintaining a Cholesky decomposition of a crucial matrix in the Hessian approximation. Although it is a good attempt at developing a new method, the paper ultimately lacks a convincing explanation (both theoretical and empirical) supporting their ideas, as I will critique below.

1. Stochastic Line Search

Determining a steplength in the stochastic setting is a difficult problem, and I appreciate the authors’ attempt to attack this problem by looking at stochastic line searches. However, the paper lacks much detail and rigorous reasoning in the description and proof for the stochastic line search.

First, the theory gives conditions that the Armijo condition holds in expectation. Proving anything about stochastic line searches is particularly difficult, so I’m on board with proving a result in expectation and doing something different in practice. However, much of the detail on how this is implemented in practice is lacking. 

How are the samples chosen for the line search? If we go along with the proposed theory, then when the function is reevaluated in the line search, a new sample is used. If this is the case, can one guarantee that the practical Armijo condition will hold? How often does the line search fail? How does the choice of the samples affect the cost of evaluating the line search?

The theory also suggests that the particular choice of c is dependent on each iteration, particularly the inner product between the true search direction and the true gradient at iteration k. Does this allow for a fixed c to be used? How is c chosen? Is it fixed or adaptive? What happens as the true gradient approaches 0?

The algorithm also places a limit on the number of backtracks permitted that decreases as the iteration count increases. What does the algorithm do when the line search fails? Does one simply take the step although the Armijo condition does not hold?

In deterministic optimization, BFGS typically needs a smaller steplength in the beginning as the algorithm learns the scale of the problem, then eventually accepts the unit steplength to obtain fast local convergence. The line search proposed here uses an initial steplength of $\min(1, \xi/k)$ so that in early iterations, a steplength of 1 is used and in later iterations the algorithm uses a $\xi/k$ steplength. When this is combined with the diminishing maximum number of backtracking iterations, this will eventually yield an algorithm with a steplength of $\xi/k$. Why is this preferred? Are the other algorithms in the numerical experiments tuned similarly?

The theory also asks for a descent direction to be ensured in expectation. However, it is not the case that $E[\hat{p}_k^T \hat{g}_k] = E[\hat{p}_k]^T g_k$, so it is not correct to claim that a descent direction is ensured in expectation. Rather, the condition is requiring the angle between the negative stochastic gradient direction and search direction to be acute in expectation.

All the proofs also depend on a linear Taylor approximation that is not well-explained, and I’m wary of proofs that utilize approximations in this way. Indeed, the precise statement is that $\hat{f}_{z’} (x + \alpha \hat{p}_z) = \hat{f}_{z’} + \alpha \hat{p}_z’ \hat{g}_z(x + \bar{\alpha} \hat{p}_z)$, where $\bar{\alpha} \in [0, \alpha]$. How does this affect the proof?

Lastly, I would propose for the authors to change the name of their condition to the “Armijo condition” rather than using the term “1st Wolfe condition” since the Wolfe condition is typically associated with the curvature condition (p_k’ g_new >= c_2 p_k’ g_k), hence referring to a very different line search. 

2. Design of the Quasi-Newton Matrix

The authors develop an approach for designing the quasi-Newton matrix that does not strictly impose symmetry or the secant condition. The authors claim that this done because “it is not obvious that enforced symmetry necessarily produces a better search direction” and “treating the [secant] condition less strictly might be helpful when [the Hessian] approximation is poor”. This explanation seems insufficient to me to explain why relaxing these conditions via a regularized least-squares approach would yield a better algorithm, particularly in the noisy or stochastic setting. The lack of symmetry seems particularly strange; one would expect the true Hessian in the stochastic setting to still be symmetric, and one would still expect the secant condition to hold if the “true” gradients were accessible. It is also unclear how this approach takes advantage of the stochastic structure that exists within the problem.

Additionally, the quasi-Newton matrix is defined based on the solution of a regularized least squares problem with a regularization parameter lambda. It seems to me that the key to the approximation is the balance between the two terms in the objective. How is lambda chosen? What is the effect of lambda as a tuned parameter, and how does it affect the quality of the Hessian approximation? It is unclear to me how this could be chosen in a more systematic way.

The matrix also does not ensure positive definiteness, hence requiring a multiple of the gradient direction to be added to the search direction. In this case, the key parameter beta must be chosen carefully. What is a typical value of beta that is used for each of these problems? One would hope that beta is small, but if it is large, it may suggest that the search direction is primarily dominated by the stochastic gradient direction and hence the quasi-Newton matrix is not useful. The interplay of these different parameters needs to be investigated carefully.

Lastly, since (L-)BFGS use a weighted Frobenius norm, I am curious why the authors decided to use a non-weighted Frobenius norm to define the matrix. How does changing the norm affect the Hessian approximation?

All of these questions place the onus on the numerical experiments to see if these relaxations will ultimately yield a better algorithm.

3. Numerical Experiments

As written, although the range of problems is broad and the numerical experiments show much promise, I do not believe that I could replicate the experiments conducted in the paper. In particular, how is SVRG and L-BFGS tuned? How is the steplength chosen? What (initial) batch sizes are used? Is the progressive batching mechanism used? (If the progressive batching mechanism is not used, then the authors should refer to the original multi-batch paper by Berahas, et al. [1] which do not increase the batch size and use a constant steplength.)

In addition, a more fair comparison would include the stochastic quasi-Newton method in [2] that also utilize diminishing steplengths, which use Hessian-vector products in place of gradient differences. Multi-batch L-BFGS will only converge if the batch size is increased or the steplength diminished, and it’s not clear if either of these are done in the paper.

Typos/Grammatical Errors:
- Pg. 1: Commas are needed in some sentences, i.e. “Firstly, for large scale problems, it is…”; “…compute the cost function and its gradients, the result is…”
- Pg. 2: “Interestingly, most SG algorithms…”
- Pg 3: Remove “at least a” in second line
- Pg. 3: suboptimal, not sup-optimal
- Pg. 3: “Such a solution”, not “Such at solution”
- Pg. 3: Capitalize Lemma
- Pg. 4: fulfillment, not fulfilment
- Pg. 7: Capitalize Lemma
- Pg. 11: Before (42), Cov \hat{g} = \sigma_g^2 I
- Pg. 11: Capitalize Lemma

Summary:

In summary, although the ideas appear to provide better numerical performance, it is difficult to evaluate if the ideas proposed in this paper actually yield a better algorithm. Many algorithmic details are left unanswered, and the paper lacks mathematical or empirical evidence to support their claims. More experimental and theoretical work is needed before the manuscript can be considered for publication.

References:
[1] Berahas, Albert S., Jorge Nocedal, and Martin Takác. ""A multi-batch l-bfgs method for machine learning."" Advances in Neural Information Processing Systems. 2016.
[2] Byrd, Richard H., et al. ""A stochastic quasi-Newton method for large-scale optimization."" SIAM Journal on Optimization 26.2 (2016): 1008-1031.
[3] Schraudolph, Nicol N., Jin Yu, and Simon Günter. ""A stochastic quasi-Newton method for online convex optimization."" Artificial Intelligence and Statistics. 2007.","The sentiment of the review is mixed but leans towards the negative side. The reviewer acknowledges the attempt and some positive aspects but ultimately finds significant issues with the theoretical and empirical support of the paper's claims. Therefore, the sentiment score is -40. The politeness of the language is quite high; the reviewer uses polite language and constructive criticism throughout the review, even when pointing out flaws. Thus, the politeness score is 80.",-40,80
"The paper proposes a second order method to represent images. More exactly, multiple (low-dimensional) projections of Kronecker products of low-dimensional representations are used to represent a limited set of dimensions of second-order representations. It is an extension of HPBP (Kim et al., ICLR 2017) but with codebook assigment. 

The main advantage of the method is that, if the number of projection dimensions is small enough, the number of learned parameters is small and the learning process is fast. The method can be easily used as last layers of a neural network. Although the derivations of the method are straightforward, I think the paper is of interest for the computer vision community. 

Nonetheless, I think that the experimental evaluation is weak. Indeed, the article only considers the specific problem of transfer learning and considers only one evaluation metric (recall@k). However, recent papers that evaluate their method for that task also use the Normalized Mutual Information (NMI) (e.g. [A,B]) or the F1-score [B] as evaluation metrics. 
The paper does not compare the same task and datasets as (Kim et al., ICLR 2017) either.
It is then difficult to evaluate whether the proposed representation is useful only for the considered task. Other tasks and evaluation metrics should be considered.
Moreover, only the case where D=32 and R=8 are evaluated. It would be useful to observe the behavior of the approaches for different values of R. 
In Section 3.2, it is mentioned that feature maps become rapidly intractable if the dimension of z is above 10. Other Factorizations are then proposed. How do these factorizations affect the second order nature of the representation of z? Is the proposed projection in Eq. (10) still a good approximation of the second order information induced by the features x?


The paper says that the method is efficient but does not mention training times. How does the method compare in terms of clockwork times compared to other approaches (on machines with similar architecture)?

In conclusion, the experimental evaluation of the method is currently too weak.


[A] Hyun Oh Song, Stefanie Jegelka, Vivek Rathod, Kevin Murphy: Deep Metric Learning via Facility Location. CVPR 2017
[B] Wang et al., Deep Metric Learning with Angular Loss, ICCV 2017

after rebuttal:
The authors still did not address my concern about testing on only one task with only one evaluation metric.","The sentiment of the review is moderately positive, as the reviewer acknowledges the potential interest of the paper to the computer vision community and highlights the advantages of the proposed method. However, the sentiment is tempered by significant criticism regarding the experimental evaluation, which is described as weak and insufficient. Therefore, the sentiment score is 20. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, providing specific recommendations for improvement without being dismissive or rude. Therefore, the politeness score is 80.",20,80
"This paper makes two different contributions in the field of adversarial training and robustness.
First the authors introduce a new type of attack that exploits second-order information while traditional attacks typically rely on first-order information.
Another contribution is a theorem that using the Renyi divergence certifies robustness of a classifier by adding Gaussian noise to pixels.

Overall, I find that the paper lacks clarity and does not properly contrast their work to existing results. They are also some issues with the evaluation results. I provide detailed feedback below.

1) Prior work
a) Connection between adversarial defense and robustness to random noise
This connection is established in Fawzi, A., Moosavi-Dezfooli, S. M., & Frossard, P. (2016). Robustness of classifiers: from adversarial to random noise. In Advances in Neural Information Processing Systems (pp. 1632-1640).
b) Connection between minimal perturbation required to confuse classifier and its confidence was discussed for the binary classification in Section 4 of
Fawzi, Alhussein, Omar Fawzi, and Pascal Frossard. ""Analysis of classifiers’ robustness to adversarial perturbations."" Machine Learning 107.3 (2018): 481-508.
c) The idea to compute the distribution of classifier outputs when the input is convolved with Gaussian noise was already “anticipated” in Section V of the following paper which relates the minimum perturbation needed to fool a model to it’s misclassification rate under Gaussian convolved input:
Lyu, Chunchuan, Kaizhu Huang, and Hai-Ning Liang. ""A unified gradient regularization family for adversarial examples."" Data Mining (ICDM), 2015 IEEE International Conference on. IEEE, 2015.

These papers should be discussed in the paper, please elaborate how you see your contribution regarding the results derived there.

2) Second-order attack introduced in the paper
I think they are a number of important details that are ignored in the presentation.
a) Regarding the assumption that the gradient vanishes in the difference of the loss, I think the authors should elaborate as to why this is a reasonable assumption to make. If we assume that the classifier has been trained to optimality then expanding the function at this (near-)optimum would perhaps indeed yield to a gradient term of small magnitude (assuming the function is smooth). However, nothing guarantees that the magnitude of the gradient term is negligible compared to the second-order information. The boundary of the classifier could very well be in a region of low-curvature.
b) The approximation of the second-order information is rather crude. However, the update derived is very similar to PGD with additional noise. In optimization, the use of noise is known to extract curvature, see e.g. (Xu & Yang, 2017) who showed that noisy gradient updates act as a noisy Power method that extracts negative curvature direction.
Xu, Y., & Yang, T. (2017). First-order Stochastic Algorithms for Escaping From Saddle Points in Almost Linear Time. arXiv preprint arXiv:1711.01944.

3) Issue of ""degenerate global minimum"": The authors argue that multistep attacks also suffer from this issue. However, the PGD attack of Madry is also initialized at a random point within the uncertainty ball around x, i.e. PGD attack first adds random noise to x before iteratively ascending the loss function. This PGD update + noise at first iteration seem rather similar to the update derived by the authors that uses random noise at every iteration. It could therefore be that the crude approximation of second-order information is not so different from previous work. This should be further investigated either theoretically or empirically.

4) Lack of details regarding some important aspects in the paper
a) “Note the evaluation requires adjustment and computing confidence intervals for p(1) and p(2), but we omit the details as it is a standard statistical procedure”
The authors seem to sweep this under the carpet but this estimation procedure gives only an estimate of the required quantities p(1) and p(2), which I think would require adjusting the result in the theorem to be a high probability bound (or an expectation bound) instead of a deterministic result.

b) “the noise is not necessarily added directly to the inputs but also to the first layer of a DNN. Given the Lipschitz constant of the first layer, one can still calculate an upper bound using our analysis. We omit the details here for simplicity”
What exactly changes here? How do you estimate the Lipschitz constant in practice?


5) Main theorem needs to be contrasted to previous results
The main Theorem uses the Renyi divergence certifies robustness of a classifier by adding Gaussian noise to pixels. There are already many results in the field of robust optimization that already derive similar results, see e.g.
Namkoong, H., & Duchi, J. C. (2017). Variance-based regularization with convex objectives. In Advances in Neural Information Processing Systems (pp. 2971-2980).
Gao, R., & Kleywegt, A. J. (2016). Distributionally robust stochastic optimization with Wasserstein distance. arXiv preprint arXiv:1604.02199.
Can you elaborate on the difference between your bounds and these ones? You do mention some of them require strong assumptions such as smoothness but this actually seems like a mild assumption (although some activation functions used in neural nets are indeed not smooth).

6) Adversarial Training Overfit to the Choice of norms
The main theorem derived in the paper uses the l_2 norm. What can be said regarding other norms?

7) Experiments:
a) the authors only report accuracies for attacks whose l2-norm is smaller than a fixed constant 0.8. However, this makes the results difficult to interpret and the authors should instead state the signal to noise ratio, i.e. dividing the l2-norm of the perturbation by the l2-norm of the image. Otherwise, it is not clear how strong or weak such perturbations are. (In particular, the norm depends on the dimension of the image, so l2-norms of perturbations for MNIST and CIFAR10 are not comparable).
b) In Section 6.2, the authors state that an l_infty trained model is vulnerable against l_2 perturbations. Why not training the model under both l_infty and l_2 perturbations?
c) Figure 1
Based on the results predicted in Theorem 2, it seems it would be more interesting to evaluate the largest L for which the classifier predictions are the same. Why did you report a different results?

8) Other comments
section 2.1: “Note this distribution is different from the one generated from softmax”. Why/How is this different?
connection to EOT attack’: authors claim: E_{d∼N(0,σ2I)} [∇_x L(θ, x, y)|x+d] = ∇_x E_{d∼N(0,σ2I)} [∇_x L(θ, x, y)|x+d]. There is a typo on the RHS where ∇_x is repeated twice. This is also the common reparametrization trick so could cite 
Kingma, D. P., & Welling, M. (2013). Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114.
","The sentiment of the review is mixed but leans towards negative. The reviewer acknowledges the contributions of the paper but highlights several significant issues, such as lack of clarity, insufficient contrast with existing work, and problems with evaluation results. This results in a sentiment score of -40. The politeness of the language is generally respectful and constructive, with the reviewer providing detailed feedback and suggestions for improvement without using harsh or dismissive language. This results in a politeness score of 60.",-40,60
"The authors proposed a novel probabilistic framework to model adversarial attacks on deep networks with discrete inputs such as text. The proposed framework assumes a two step construction of an adversarial perturbation: 1) finding relevant features (or dimensions) to perturb (Eq. 3); 2) finding values to replace the features that are selected in step 1 (Eq. 4). The authors approximate some terms in these two equations to make the optimization easier. For example, it is *implicitly* assumed that given the i-th feature is removed from consideration, the probability of attack success does not change *on average* under probabilistic *adversarial* attack on other features (Eq. 5). It is not clear why that should hold and under what conditions that assumption would be reasonable (given that the attacks on other features are adversarial, although being probabilistic). 
The proposed framework allows one to solve the computation vs. success rate trade-off by either estimating the best attack from the network (called greedy attack Eq. 6) or using a parametric estimation that does not require model evaluation (called Gumbel attack). Experimental results suggest that Gumbel attack has better or competitive attack rate on models developed for text classification while having the most computationally efficiency among other methods. It is also noticeable that the greedy attack achieves the best success rate with a large margin among all the tested methods. ","The sentiment of the review is generally positive, as it acknowledges the novelty of the proposed framework and highlights the competitive performance of the Gumbel attack and the superior success rate of the greedy attack. However, it also points out a significant assumption that lacks clarity, which slightly tempers the overall positivity. Therefore, the sentiment score is 60. The language used in the review is polite and constructive, as it provides specific feedback and suggestions without being dismissive or harsh. Thus, the politeness score is 80.",60,80
"The paper introduces hierarchical attention, where they propose to weighted combine all the intermediate layers of multi-level attention. The idea is simple and seems to be promising, however originality seems incremental.

In order to fully demonstrate the significance of the proposed algorithm, the authors should conduct more comparisons, for example, to multi-level attention. Just comparing with one-level attention seems unfair given the significant increase of computation. Another aspect of comparison may be to consider computation and performance improvements together and discuss the best trade-off. The authors should also include some standard benchmark datasets for comparisons. The current ones are good but it is not so clear what is the best state-of-the-arts results on them when compared with all other methods.

The analysis on the network's representation and convergence is nice but it does not bring much insights. The argument for decreasing global minimal of the loss function in terms of increasing parameter size can be made for nearly all models but it is of little practical use since there is no guarantee one can reach the global optimal of these models.

I recommend the authors to analyze/demonstrate how effective this weighted combination is. For example, the paper can benefit from some clear examples that show the learned weights across the layers and which ones are more important.

The presentation of the paper needs some polishing. For example, there are numerous typos, grammatical errors everywhere.","The sentiment of the review is mixed. While the reviewer acknowledges that the idea is promising, they also describe it as incremental and point out several areas for improvement. Therefore, the sentiment score is slightly negative. The politeness of the language is generally respectful and constructive, with suggestions for improvement rather than outright criticism, so the politeness score is positive.",-20,60
"The paper proposes PDDPG, a combination of prioritized experience replay, parameter noise exploration, and DDPG. Different combinations are then evaluated on MuJoCo domains, and the results are mixed. 

The novelty of the work is limited, and the results are hard to interpret: sometimes PDDPG performs better, sometimes worse, and the training curves are only obtained with a single random seed. Also presented results are substantially worse than current state of the art (e.g., TD3, SAC).
","The sentiment of the review is slightly negative, as it points out the limited novelty and mixed results of the paper. The reviewer also mentions that the results are hard to interpret and are worse than the current state of the art. Therefore, the sentiment score is -40. The politeness of the language is neutral; the reviewer does not use any rude or overly harsh language, but they also do not use particularly polite or encouraging language. Thus, the politeness score is 0.",-40,0
"The paper studies the standard denoising problem under the assumption that the unknown n-dimensional signal can be written as the output of a known d-layer neural network G mapping k dimensions to n dimensions. The paper specifies an algorithm to perform this denoising and the algorithm is based on a variant of the usual gradient method. Then, under additional assumptions on the neural network G, the paper proves that their algorithm produces a denoised signal that achieves a mean squared accuracy of k/n. Because the input signal has ""effective"" dimensionality k (as it can be written as G(x) for some k-dimensional x), it is nice that it can be recovered at the accuracy k/n by Gradient Descent despite the complicated nature of G. In this respect, the result is quite interesting. However, the underlying assumptions are too strong in my opinion as described below: 

1. It is assumed that the Weights of the neural network G are all Gaussian (and also specific Gaussians with mean zero and variances determined by the layer dimensions). This of course is highly impractical. In practice, these network weights are pre-learned (say based on similar datasets) and there is hardly any reason to believe that they will satisfy the Gaussian assumption. 
2. It is assumed that the network is expansive in some sense with an expansivity constant \epsilon. This \epsilon then gets into the accuracy bound which basically means that \epsilon has to be set very small. Unfortunately, this leads to the expansivity condition being quite stringent which will further lead to k being very small (especially if d is large). It is unrealistic to believe that real-world signals will come from a neural network with small k. 

Given that there do not seem to be other such results for the accuracy of neural network denoising, the paper might still be considered interesting despite the above shortcomings. However, I believe that the theoretical result has near-zero relevance to a practical neural network denoiser.

Another concern is that the paper seems to borrow quite a lot of ideas from the paper ""Global Guarantees for Enforcing Deep Generative Priors by Empirical Risk"" by Hand and Voroninski. It will be good if the authors can explain the essential differences between the present paper and this earlier paper. ","The sentiment of the review is mixed but leans towards the negative side. The reviewer acknowledges that the result is interesting and that the paper might still be considered interesting despite its shortcomings, which suggests some positive sentiment. However, the reviewer also points out significant issues with the assumptions and practical relevance of the theoretical results, leading to an overall negative sentiment. Therefore, the sentiment score is -30. The politeness of the language is quite high. The reviewer uses polite language such as 'in my opinion,' 'it will be good if,' and 'might still be considered interesting,' which indicates a respectful tone despite the critical feedback. Therefore, the politeness score is 80.",-30,80
"This paper offers the argument that dropout works not due to preventing coadaptation, but because it gives more gradient, especially in the saturated region. However, previous works have already characterized how dropout modifies the activation function, and also the gradient in a more precise way than what is proposed in this paper. 

## Co-adaptation
co-adaptation does not seem to mean correlation among the unit activations. It is not too surprising units need more redundancy with dropout, since a highly useful feature might not always be present, but thus need to be replicated elsewhere.

Section 8 of this paper gives a definition of co-adaptation,
based on if the loss is reduced or increased based on a simultaneous change in units.
https://arxiv.org/abs/1412.4736
And this work, https://arxiv.org/abs/1602.04484, reached a conclusion similar to yours
that for some notion of coadaptation, dropout might increase it.

## Gradient acceleration
It does not seem reasonable to measure ""gradient information flow"" simply as the norm of the gradient, which is sensitive to scales, and it is not clear if the authors accounted for scaling factor of dropout in Table 2.

The proposed resolution, to add this discontinuous step function in (7) with floor is a very interesting idea backed by good experimental results. However, I think the main effect is in adding noise, since the gradient with respect to this function is not meaningful. The main effect is optimizing with respect to the base function, but adding noise when computing the outputs. Previous work have also looked at how dropout noise modifies the effective activation function (and thus its gradient). This work, http://proceedings.mlr.press/v28/wang13a.html, give a more precise characterization instead of treating the effect as adding a function with constant gradient multiplied by an envelop. In fact, the actual gradient with dropout does involve the envelope by chain rule, but the rest is not actually constant as in GAAF. 
","The review starts with a critical assessment of the paper's main argument, noting that previous works have already addressed the topic in a more precise manner. This indicates a somewhat negative sentiment. However, the reviewer acknowledges an interesting idea in the proposed resolution and backs it with good experimental results, which adds a positive note. The language used is professional and constructive, providing specific references and suggestions for improvement, which indicates a high level of politeness.",-20,80
"This paper studies the implicit bias of minimizers of a regularized cross entropy loss of a two-layer network with ReLU activations. By combining several results, the authors obtain a generalization upper bound which does not increase with the network size. Furthermore, they show that the maximum normalized margin is, up to a scaling factor, the l1 svm margin over the lifted feature space of an infinite-size network. Finally, in a setting of infinite-sized networks, it is proved that perturbed Wasserstein gradient flow finds a global minimum in polynomial time.

I think that the results are interesting and relevant to current efforts of understanding neural networks. The techniques and ideas seem promising and may be applied in more general settings. The paper is mostly clearly written, but there are some issues which I outline below.
1.	It is not clear what is the novelty in sections 2 and 3.1 except the combination of all the results to get a generalization bound which does not increase with network size (which on its own is non-trivial). Specifically, 
a.	What is the technical contribution in Theorem 2.1 beyond the results of the two papers of Rosset et al. (journal paper and the NIPS paper which was mentioned in the comment on missing prior work)?
b.	How does Theorem 3.1 compare with previous Rademacher bounds for neural networks which are based on the margin? In Neyshabur et al. (2018), it is shown that margin-based generalization bounds empirically increase with network size. Does this hold for the bound in Theorem 3.1?

2.	In the work of Soudry et al. (2018) section 4.3, they consider deep networks with an unregularized loss and show that gradient descent converges to an l2 max margin solution under various assumptions. What is the connection between this result and the l1 max margin result in section 3.3?

3.	What are the main proof ideas of Theorem 4.3? Why is the perturbation needed?

4.	What is the size of the network that was trained in Section 5 in the experiments of Figure 3? Only the size of the ground truth network is mentioned.


---------Revision------------

I have read the author's response and other reviews. I am not changing the current review.
I have one technical question. In the new generalization bound (Proposition 3.1), the authors claim that the product of Frobenius norms is replaced with a sum. However, I don't see any sum in the proof. Could the authors please clarify this?","The sentiment of the review is generally positive, as the reviewer finds the results interesting and relevant, and acknowledges the promise of the techniques and ideas presented. However, the reviewer also points out several specific issues and questions that need to be addressed, which slightly tempers the overall positivity. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, even when pointing out issues and asking for clarifications. Therefore, the politeness score is 90.",60,90
"This paper presents an empirical evaluation of continual learning approaches for generative modelling. Noting that much of previous work focuses on supervised tasks, the paper evaluates various combinations of continual learning strategies (EWC, rehearsal/replay-based, or generative replay) and generative models (GANs or likelihood-based).
The experiments evaluate all combinations on MNIST and Fashion MNIST, and the resulting best-performing combination on CIFAR.
The paper is well-written and structured, and although there are no new proposed algorithms or measures, I think this has the potential to be a useful empirical study on the relatively unstudied topic of continual learning with generative models.

However, my main concern is in the detail of analysis and discussion: for an empirical study, it would be much more beneficial to empirically investigate *why* certain combinations are more effective than others. For example:
- Is the reason GANs are better than likelihood models with generative replay purely because of sample quality? Or is it sufficient for the generator to learn some key characteristics for a class that lead to sufficient discriminability?
- Why is rehearsal better for likelihood models? (And how does this relate to the hypothesis of overfitting to a few real examples?)

The CIFAR-10 results also require more work - it is unclear why the existing approaches could not be made to work, and whether this is a fundamental deficiency in the existing approaches or other factors (hyperparameters, architecture choices, lack of time, etc). Presuming the sample quality is as good as in the WGAN-GP work (given the original implementation is used for experiments), why is this insufficient for generative replay? More detailed analysis / discussion, or another combinatorial study, would help for CIFAR too.

Some comments:
- The poor performance of EWC across the board is concerning. Was this implemented by computing the Fisher of the ELBO with respect to parameters? Was the empirical or true Fisher used? Why does the performance appear so poor compared to Seff et al (2017)? This suggests that either more thought is required on how to best protect parameters of generative models, or the baseline has not been properly implemented/tuned.
- Given this is an entirely empirical study, I would strongly encourage the authors to release code sooner than the acceptance deadline - this can be achieved using an anonymous repository.
- Figure 2 and 3 plots are a little difficult to parse without axis labels.","The sentiment of the review is generally positive, as the reviewer acknowledges that the paper is well-written, structured, and has the potential to be a useful empirical study. However, the reviewer also points out significant areas for improvement, particularly in the depth of analysis and discussion. Therefore, the sentiment score is 50. The politeness of the language is high, as the reviewer uses constructive criticism and provides specific recommendations without being rude or dismissive. Therefore, the politeness score is 80.",50,80
"This paper identifies bias of commercial Face detection API (Microsoft, Google, Face++, IBM) by sending face images generated from AirSim, in which different face attributes (e.g., skin color, age, face orientation, lighting conditions, etc) can be controlled and explored. The paper shows that for darker skin color and old age, the classifier tends to have a higher false negative rate (miss the face more). This is in particular more apparent if Bayesian Optimization is used to explore the parameter space based on the previous detection results to find the failure cases. 

There are several concerns:

1. Bayesian Optimization might itself create a bias in the input data distribution, since it selectively pick some parameter configuration over the others.   

2.  Using simulator might create additional biases. Maybe the faces generated by the simulator using the parameters of skin color of the minority / old age are less realistic than other faces, which lead to higher misclassification rate. In the paper there is no analysis in that aspect. 

Overall I feel this is an interesting paper and it may identify important problems in the existing commercial AI system. However,  I am not an expert in this field so I am less confident about the thoroughness of experiments, as well as the fairness of approaches. 

Minor issue:

Fig. 4 “Age”, skin detection => age. 
","The sentiment of the review is generally positive, as the reviewer finds the paper interesting and acknowledges its potential to identify important problems in existing commercial AI systems. However, the reviewer also expresses some concerns and admits a lack of expertise in the field, which slightly tempers the overall sentiment. Therefore, the sentiment score is 40. The politeness of the language is quite high; the reviewer uses polite and constructive language, even when pointing out concerns and potential issues. Thus, the politeness score is 80.",40,80
"Main idea:
This paper studies a problem of the importance weighted autoencoder (IWAE) pointed out by  Rainforth 18, that is, tighter lower bounds arising from increasing the number of particles improve the learning of the generative model, but worsen the learning of the inference network. The authors show that the reweighted wake-sleep algorithm (RWS) doesn't suffer from this issue. Moreover, as an alternative to control variate scheme and reparameterization trick, RWS doesn't suffer from high variance gradients, thus it is particularly useful for discrete latent variable models.   
To support the claim, they conduct three experiments: 1) on ATTEND, INFER, REPEAT, a generative model with both discrete and continuous latent variables; 2) on MNIST with a continuous latent variable model; 3) on a synthetic GMM.

Clarity issues:
1. ""branching"" has been used many times, but AFAIK, this seems not a standard terminology. What do ""branching on the samples"", ""conditional branching"", ""branching paths"" mean?
2. zero-forcing failure mode and delta-WW: I find this part difficult to follow. For example, the following sentence 
""the inference network q(z|x) becomes the posterior for this model which, in this model, also has support at most {0, . . . , 9} for all x"". 
However, this failure mode seems an interesting finding, and since delta-WW outperforms other methods, it deserves a better introduction. 

Questions:
1. In Fig 1 (right), how do you estimate KL(q(z|x) || p(z|x))?
2. In Sec 4.2, why do you say IWAE learns a better model only up to a point (K = 128) and suffers from diminishing returns afterwards?  
3. In Fig 4, why WS doesn't achieve a better performance when K increasing?

Experiments:
1. Since the motivating story is about discrete latent variable models, better baselines should be compared, e.g. RBM, DVAE, DVAE++, VQ-VAE etc. 
2. All experiments were on either on MNIST or synthetic data, at least one large scale experiment on discrete data should be made to verify the performance of RWS. 
","The sentiment of the review appears to be neutral to slightly positive. The reviewer acknowledges the importance of the problem studied and the potential usefulness of the reweighted wake-sleep algorithm (RWS), but also points out several clarity issues and questions about the experiments. The sentiment score is therefore 10. The politeness of the language is generally neutral to polite. The reviewer uses phrases like 'I find this part difficult to follow' and 'it deserves a better introduction,' which are constructive and not rude. The politeness score is 50.",10,50
"summary--
The paper focuses on improving object localization, though the title highlights ""interpreting deep neural network"" which is another area. It analyzes the classifier weights for image classification, and compute the derivative of the feature maps from the network for a sensitivity map of the image. Then it learns a simple linear mapping over the sensitivity map for bounding box regression. Experiments report competitive performance.

However, there are several major concerns.

1) The paper appears misleading from multiple claims. For example, [abstract] ""common approaches to this problem involve the use of a sliding window,... time consuming"". However, current state-of-the-art methods accomplish detection in a fully convolutional manner using CNN, and real-time performance is achieved. the paper claims that ""computer vision can be characterized as presenting three main tasks... (1) image classification, (2) image localization and (3) image detection"". This appears quite misleading. There are way more topics, from low-level vision to mid-level to high-level, e.g., stereo, boundary detection, optical flow, tracking, grouping, etc. Moreover, just in ""localization"", this could be object localization, or camera localization. Such misleading claims do not help readers learn from the paper w.r.t related work in the community.


2) The approach ""is rooted in the assertion that any deep CNN for image classification must contain, implicit in its connection weights, knowledge about the location of recognized object"". This assertion does not appear obvious -- an reference should be cited if it is from other work. Otherwise, recent work shows that deep CNN can overfit random training data, in which case it is hard to imagine why the object location can be implicitly captured by the CNN [R1]. Similarly, the paper claims that ""once weights are found, the gradient... with regard to X would provide information about the sensitivity of the bounding box loss function with regard to the pixels in the images"". This is not obvoius either as recent work show that, rather than the whole object, a part of it may be more discriminative and captured by the network. So at this point, why the gradient can be used for object location without worrying that the model merely captures a part? 

[R1] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, Oriol Vinyals, Understanding deep learning requires rethinking generalization, ICLR 2017.
[R2] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, Antonio Torralba, Learning deep features for discriminative localization, CVPR 2016.

3) The paper admits in Section 2.2 that ""we have not yet done a formal comparison of these two approaches to constructing the sensitivity map"". As the two approaches are suggested by the authors, why not comparing in this paper. It makes the paper less self-contained and not ready to publish. A formal comparison in the rebuttal may improve the rating of the paper.

4) In Equation 3, how to represent the bounding box coordinate? Are they any transforms? What does it mean by ""bias weights""? Are they different from Cartesian coordinates, or the one used in Equation (2)?


5) The experiments are not convincing by merely reporting the metric of IoU>0.5 without any in-depth analysis. Perhaps some visualization and ablation study improve the quality of experiments.

6) In Section 3.2, why using two different aggregation methods for producing the final sensitivity map -- max-pool along the channel for PACAL VOC 2017 dataset and avg-pool for ImageNet dataset, respectively? Are there some considerations?

7) In Table 1, it shows the proposed method outperforms the other methods significantly, achieving 41% better than the second best method. However, there is no in-depth analysis explaining why the proposed method performs so well for this task. Moreover, from Figure 1 and Figure 3, it is straightforward to ask how a saliency detection model performs in object detection given that the images have clean background and objects are mostly centered in the image.

8) What does it mean by ""CorLoc (mAP)"" in Table 2? As defined in Equation 4, CorLoc acounts the portion of detection whose IoU greater than 0.5 compared to the ground-truth. But mAP accumulates over a range of IoU threshold and precision across classes.

9) As the proposed method is closely related to the CAM method, how does CAM perform on these datasets? This misses an important comparison in the paper.


10) The readability of the paper should be improve. There are many typos, for example --
1. What does ""..."" mean above and below Equation (2)?
2. inconsistent notation, like $w_{ki}$ and ${\bf w}_{ki}$ in Equation (2).
3. conflicted notation, w used in Equation 2 and Equation 3.","The sentiment of the review is largely negative, as it highlights several major concerns and misleading claims about the paper. The reviewer points out multiple issues with the methodology, claims, and experimental results, indicating a lack of confidence in the paper's contributions. The politeness of the language is relatively neutral to slightly polite. The reviewer uses formal and professional language without resorting to rudeness, but the tone is firm and critical, focusing on the shortcomings of the paper.",-70,20
"The paper tries to provide an explanation for a memorization phenomenon observed in convolutional autoencoders. In the case of memorization, the autoencoder always outputs the same fixed image for any input image, even when the input image is random noise. The authors provide an empirical analysis that connects such a phenomenon to strides in convolutional layers of the autoencoder. Then, a possible theoretical explanation is given in the form of conjecture with some empirical evidence.

The paper presents very interesting idea, however presentation and theoretical foundation can be significantly improved.

- Please elaborate on how different initializations influence memorization effect. Currently the paper only mentions initialization approaches for which memorization can or cannot occur without going into deeper analysis.
- Having linear operator extraction described in the paper somehow breaks the flow, please consider moving it to Appendix.
- The comment after the Proposition section is not very clear. What does it mean that the Proposition does not imply that A_X must obtain rank which is given in the Conjecture? Please explain how is Proposition providing any theoretical support for Conjecture then.

- Minor comments
1. “2000 iteration” -> “2000 iterations”
2. The text says “Network ND trained on frog image” while the following next sentence says that “the network reconstructed the digit 3”. Please clarify.
3. “Network ND reconstructed the digit 3 with a training loss of 10^-4 and Network ND with loss 10^-2”. It seems that one of these should be “Network D”.
4. “(with downsamling)” ->  “(with downsampling)”","The sentiment of the review is moderately positive. The reviewer acknowledges that the paper presents a very interesting idea but also points out that the presentation and theoretical foundation need significant improvement. This suggests a sentiment score of around 40. The politeness of the language used is quite high. The reviewer uses polite phrases such as 'please elaborate' and 'please consider,' and the tone is constructive and respectful, indicating a politeness score of 80.",40,80
"The authors introduce the problem of Model Completion (MC) to the machine learning community.  They provide a thorough review or related works, and convincingly argue that existing solutions to this sort of task (i.e., homomorphic encryption and multi-party computation) are not fully satisfactory in the domain of neural network learning.

The authors also provide extensive numerical experiments attempting to quantify their proposed measure of hardness-of-model-completion, MC-hardness_T(\alpha) on a diverse set of Supervised and RL-related tasks, and they provide extensive analysis of those results.

I find the paper to raise more questions than it answers (in a good way!).  The authors note that their measure depends strongly on the peculiarities of the particular (re)training scheme used.  Do the authors worry that such a measure could end up being too loose--essentially always a function of whatever the fastest optimization scheme happens to be for any particular architecture?  

More broadly, there's an additional axis to the optimization problem which is ""How much does the training scheme know about the particulars of the problem?"", ranging from ""Literally has oracle access to the weights of the trained model (i.e., trivial, MC-hardness = 0 always)"" to ""knows what the architecture of the held-out-layer is and has been designed to optimize that particular network (see, e.g., learned optimizers)"" to ""knows a little bit about the problem structure, and uses hyperparameter tuned ADAM"" to ""knows nothing about the problem and picks a random* architecture to use for the held out weights, training it with SGD"".

Model completion seems, morally (or at least from a security stand-point) slightly under-specified without being more careful about what information each player in this game has access to.  As it stands, it's an excellent *empirical* measure, and captures a very interesting problem, but I'd like to know how to make it even more theoretically grounded.

An excellent contribution, and I'm excited to see follow-up work.



* We of course have tremendous inductive bias in how we go about designing architectures for neural networks, but hopefully you understand my point.","The sentiment of the review is positive, as indicated by phrases like 'thorough review,' 'convincingly argue,' 'extensive numerical experiments,' and 'excellent contribution.' The reviewer also expresses excitement about follow-up work, which further underscores the positive sentiment. Therefore, the sentiment score is 80. The politeness of the language is very high, as the reviewer uses polite and constructive language throughout the review, such as 'I'd like to know,' 'excellent empirical measure,' and 'excited to see follow-up work.' The reviewer also provides constructive criticism in a respectful manner, so the politeness score is 90.",80,90
"Authors argue that the main issue with stability in GANs is due to the discriminator becoming too powerful too quickly. To address this issue they propose to make the task progressively more difficult: Instead of providing only the samples to the discriminator, an additional (processed) bitstring is provided. The idea is that the bitstring in combination with the sample determines whether the sample should be considered true or fake. This in turn requires the decision boundary of the discriminator to become more complicated for increasing lengths of the bitstring. In a limited set of experiments the authors show that the proposed approach can improve the FID scores.

Pro:
- A simple idea to make the problem progressively more difficult.
- The writing is relatively easy to follow.
- Standardized experimental setup.

Con:
- Ablation study of the training tricks is missing: (1) How does the proposed approach perform when no progressive scheduling is used? (2) How does it perform without the linear model for increasing p? (3) How does the learning rate of G impact the quality? Does one need all of these tricks? Arguably, if one includes the FID/KID to modify the learning rates in the competing approaches, one could find a good setup which yields improved results. This is my major issue with this approach.
- Clarity can be improved: several pages of theory can really be summarized into “learning the joint distribution implies that the marginals are also correctly learned’ (similar to ALI/BIGAN). This would leave much more space to perform necessary ablation studies. 
- Comparison to [1] is missing: In that model, it seems that the same effect can be achieved and strongly improves the FID. Namely, they introduce a model in which observed samples pass through a ""lens"" before being revealed to the discriminator thus balancing the generator and discriminator by gradually revealing more detailed features.
- Can you provide more convincing arguments that the strength of the discriminator is a major factor we should be fixing? In some approaches such as Wasserstein GAN, we should train the discriminator to optimality in each round. Why is the proposed approach more practical then approaches such as [2]?

[1] http://proceedings.mlr.press/v80/sajjadi18a.html
[2] https://arxiv.org/abs/1706.08500","The sentiment of the review is mixed but leans towards the positive side. The reviewer acknowledges the simplicity and potential of the proposed idea, the clarity of writing, and the standardized experimental setup. However, they also point out several significant shortcomings, such as the lack of an ablation study, the need for improved clarity, missing comparisons to related work, and the need for stronger arguments regarding the discriminator's strength. Therefore, the sentiment score is 20. The politeness of the language is quite high. The reviewer uses polite and constructive language throughout the review, providing specific recommendations and avoiding any harsh or dismissive comments. Therefore, the politeness score is 80.",20,80
"
The authors proposed a new model Adaptive Neural Trees(ANTs) by combining the representation learning and gradient optimization of neural networks with architecture learning of decision trees. The key advantage of the new model ANTs  over the existing methods(Random forest, Linear classifier, Neural decision forest, et al) is: it may achieve high accuracy(above $90\%$) with relatively much smaller number of parameters, as shown by the experiments on the datasets MNIST and CIFAR-10. Besides, the authors proposed single-path inference based on the greedily-selected leaf node to approximate the multi-path inferences with the full predictive distribution. The experiments show the single-path inference doesn't lose much accuracy but it saves memory and time. This paper is acceptable after minor modification.


Questions:
In the second line below equation (1), $n$ in $t_{e_{n(j)}}^{\psi}$ is not defined. Also, should $t_{e_{1}}^{\psi}$ be $t_{e_{n(1)}}^{\psi}$? ","The review is generally positive, highlighting the advantages of the proposed model and its performance improvements over existing methods. The sentiment score is high because the reviewer acknowledges the model's high accuracy and efficiency. The politeness score is also high as the reviewer uses polite language, such as 'acceptable after minor modification' and provides constructive feedback without any negative or harsh language.",80,90
"The paper proposes an RNN architecture inspired from deterministic pushdown automata. An RNN is extended to use soft attention at every time step to choose from several learnable centroids.

In general, the paper is well written and the proposed model is theoretically grounded. Unfortunately, the proposed approach shines only on specifically designed benchmarks. It is not a surprise that a CF can be learned by an architecture very similar to DPDA (with addition of learnable parameters). There is a number of specifically designed tasks to test long-term memorization, such as copy/addition, etc. Furthermore, RNNs are mostly used for natural language processing tasks. This paper only conducts experiments on IMDB sentiment analysis ignoring better benchmarked tasks, such as language modelling.

It is not absolutely clear why authors claim that cell is playing the role of memory. It is always possible to rewrite LSTM formulas with h' which is concatenation of hidden state h and cell c. Results on ""peephole connection""-inspired SR-LSTM-p should be benchmarked against an LSTM with peephole connections.

The claim repeated several times that RNNs operate like DFAs, not DPDAs. This is an important point in the paper and should be verbalized more. Does it mean that it is easier to learn regular languages with RNNs?

While intuitive, theorems 3.1-3.2 are very vague to be theorems. Otherwise, they should be proven or provided a sketch of proof. For example, how do you formalize ""state dynamics""?

The quality of writing of the related work section is worse that the rest of the paper. Authors should explore more other hidden state regularization methods. And, perhaps, give less attention to stochastic RNNs since the final version of the proposed model is not stochastic.

To summarize, this paper provides an interesting direction but lacks in terms of experimentation and global coherence of what is claimed and what is shown.

Minor points:
- Citation of Theano is missing
- Give a sentence explaining what is hidden state ""drifting""
- a-priori -> a priori","The sentiment of the review is mixed. The reviewer acknowledges that the paper is well-written and theoretically grounded, which is positive. However, they also point out significant limitations in the experimentation and coherence of the claims, which is negative. Therefore, the sentiment score is slightly negative. The language used is generally polite, with constructive criticism and suggestions for improvement, without any rude or harsh language.",-20,80
"This paper presents an extension of Capsule Networks, Siamese Capsule Networks (SCNs), that can be applied to the problem of face verification. Results are reported on the small AT&T dataset and the LFW dataset. 

I like the direction that this paper is taking. The original Capsules work has been looking at fairly simple and small scale datasets, and the natural next step for this approach is to start addressing harder datasets, LFW being one of them. Also face verification is a natural problem to look at with Capsules.

However, I think this paper currently falls short of what I would expect from an ICLR paper. First, the results are not particularly impressive. Indeed, SCN doesn't outperform AlexNet on LFW (the most interesting dataset in the experiments). Also, I'm personally not particularly compelled by the use of the contrastive loss as the measure of performance, as it is sensitive to the scaling of the particular representation f(x) used to compute distances. Looking at accuracy (as in other face verification papers, such as DeepFace) for instance would have been more appropriate, in my opinion. I'm also worried about how hyper-parameters were selected. There are A LOT of hyper-parameters involved (loss function hyper-parameters, architecture hyper-parameters, optimizer hyper-parameters) and not much is said about how these were chosen. It is mentioned that cross validation was used to select some margin hyper-parameters, but results in Table 1 are also cross-validation results, which makes me wonder whether hyper-parameters were tuned on the performance reported in Table 1 (which of course would be biased).

The paper is also pretty hard to read. I recognize that there is a lot of complicated literature to cover (e.g. prior work on Capsule Networks has introduced variations on various aspects which are each complicated to describe). But as it currently reads, I can honestly say that I'm not 100% sure what exactly was implemented, i.e. which components of previous Capsule Networks were actually used in the experiments and which weren't. For example, I wasn't able to figure out which routing mechanism was used in this paper. The paper would strongly benefit from more explicitly laying out the exact definition of SCN, perhaps at the expense of enumerating all the other variants of capsules and losses that previous work has used.

Finally, regardless of the clarify of the paper, the novelty in extending Capsule Networks to a siamese architecture is arguably pretty incremental. This wouldn't be too much of a problem if the experimental results were strong, but unfortunately it isn't the case.

In summary:

Pros
- New extension of Capsule Networks, tackling a more challenging problem than previous work

Cons
- Novelty is incremental
- Paper lacks clarity and is hard to read
- Results are underwhelming

For these reasons, I'm afraid I can't recommend this paper be accepted.

Finally, I've noted the following typos:
- hinton1985shape => use proper reference
- within in => within
- that represent => that represents
- a Iterated => an Iterated
- is got => is obtained
- followed two => followed by two
- enocded => encoded
- a a pair => a pair
- such that to => such as to
- there 1680 subjects => there are 1680 subjects
- of varied amount => of the varied amount
- are used many => are used in many
- across the paper: lots of in-text references should be in parenthesis

","The sentiment score is derived from the overall tone of the review. The reviewer acknowledges the positive direction of the research and the relevance of the problem tackled, which contributes positively. However, the reviewer also points out several significant shortcomings, such as underwhelming results, lack of clarity, and incremental novelty, leading to a recommendation against acceptance. This mixed but ultimately critical stance results in a sentiment score of -20. The politeness score is based on the language used throughout the review. The reviewer provides constructive criticism and specific recommendations without using harsh or rude language. The tone remains professional and respectful, leading to a politeness score of 80.",-20,80
"This paper analyzes the relationship between ""adversarial vulnerability"" with input dimensionality of neural network. The paper proves that, under certain assumptions, as the input dimensionality increases, neural networks exhibit increasingly large gradients thus are more adversarially vulnerable. Experiments were done on neural networks trained by penalizing input gradients and FGSM-adversarial training. Similar trends on vulnerability vs dimensionality are found.

The paper is clearly written and easy to follow. I appreciate that the authors also clearly stated the limitation of the theoretical analysis.

The theoretical analyses on vulnerability and dimensionality is novel and provide some insights. But it is unlikely such analysis is significant There are a few reasons:
- This analysis only seems to work for ""well-behaved"" models. For models with gradient masking, obfuscated gradients or even non-differentiable models, it is not clear that how this will apply. (and I appreciate that the authors also acknowledge this in the paper.) It is unclear how this specific gradient based analysis can help the understanding of the adversarial perturbation phenomena. After all, the first order Taylor expansion argument on top of randomly initialized weights is oversimplifying the complicated problem.
- One very important special case of the point above: the analysis probably cannot cover the  adversarially PGD trained models [MMS+17] and the certifiably robust ones. Such models may have small gradients inside the box constraint, but can have large gradients between different classes.


On the empirical results, the authors made a few interesting observations, for example the close correspondence between ""Adv Train"" and ""Grad Regu"" models. 
My concern is that the experiments were done on a narrow range of models, which only have ""weak"" adversarial training / defenses.
Adversarial robustness is hard to achieve. What matters the most is ""why the strongest model is still not robust?"" not ""why some weak models are not robust?"" 
It is especially worrisome to me that the paper does not cover the adversarially-augmented training based iterative attacks, e.g. PGD TRAINED models [MMS+17] which is the SOTA on MNIST/CIFAR10 L_\infty robustness benchmark.
Without comprehensive analyses on SOTA robust models, it is hard to justify the validity of the theoretical analysis in this paper, and the conclusions made by the paper.
For example, re: the last sentence in the conclusion: ""They hence suggest to tackle adversarial vulnerability by designing new architectures (or new architectural building blocks) rather than by new regularization techniques."" The reasoning is not obvious to me given the current evidence shown in the paper.

[MMS+17] Madry A, Makelov A, Schmidt L, Tsipras D, Vladu A. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083","The sentiment of the review is mixed but leans towards positive. The reviewer acknowledges the clarity and novelty of the paper, which suggests a positive sentiment. However, the reviewer also points out significant limitations and concerns, which tempers the overall positivity. Therefore, the sentiment score is 20. The politeness of the language is high; the reviewer uses polite phrases such as 'I appreciate' and 'My concern is,' and provides constructive criticism without being rude. Therefore, the politeness score is 80.",20,80
"I have to say that this paper is not well organized. It describes the advantage function and CMA-ES, but it does not describe PPO and PPO-CMA very well. I goes through the paper twice, but I couldn't really get how the policy variance is adapted. Though the title of section 4 is ""PPO-CMA"", only the first paragraph is devoted to describe it and the others parts are brief introduction to CMA.

The problem of variance adaptation is not only for PPO. E.g., (Sehnke et al., Neural Networks 2009) is motivated to address this issue. They end up using directly updating the policy parameter by an algorithm like evolution strategy. In this line, algorithm of (Miyamae et al. NIPS 2010)  is similar to CMA-ES. The authors might want to compare PPO-CMA with these algorithms as baselines.","The sentiment of the review is negative, as the reviewer points out several issues with the paper, such as poor organization and lack of clarity in describing key concepts. The reviewer also suggests that the authors compare their work with other algorithms, indicating that the current work is insufficient. The politeness of the language is neutral; while the reviewer is critical, they do not use rude or overly harsh language. They provide constructive feedback and suggestions for improvement.",-60,0
"This paper focuses on the extraction of high-quality model-agnostic saliency maps. The authors argue that when an extracted saliency map is directly dependent on a model, then it might not be useful for a different classifier and thus not general enough. To overcome this problem, they consider all the possible classifiers weighted by their posterior probabilities. This problem cannot be solved explicitly, and the authors suggest a scheme to approximate the solution using two networks. That is, pretrain an initial classifier and then, following an adversarial training procedure, one network is trying to confuse the classifier and the other one to maximize its accuracy. Using this formulation, the authors report state-of-the-art results for salience map extraction.

SUMMARY/OVERALL COMMENTS
The authors present a simple and effective way to produce classifier-agnostic saliency maps. The argument for the approach is well justified and the results seem convincing on a first read. However, the novelty of the method is a concern given the previous work of Fan et al. (2017), and the manuscript is not upfront about the differences between the two works. The experiments are another cause for concern: Fan et al. should have been tested as a baseline with similar implementation (controlling for architecture and \lambda), and implementation differences in prior works of Table 1 make it difficult to draw conclusions. 


RELATED WORKS
* In the introduction, the authors mention related works but fail to mention the work of Fan et al. (2017) which is clearly the most relevant. The first mention of Fan et al is on page 4 in a very specific discussion the regularization coefficient. The problem formulation in Section 2 and the approach is Section 3 is largely borrowed from Fan et al but not acknowledged until the last page. This introduces bias and confusion to the reader in regards to the novelty of the approach. Please, mention the work of Fan et al. (2017) in the introduction and clearly delineate the differences in the works earlier in the text. (--)

* Du et al. (2018), “Towards Explanation of DNN-based Prediction with Guided Feature Inversion”, use the VGG models for saliency map extraction and achieve a LE of 38.2. Note that Du et al. (2018), suggest that this modification could lead to SOTA results. I would like to see a comparison with this method. (-)

* The work of Kindermans, et al. (2017), “Learning how to explain neural networks: PatternNet and Pattern Attribution”, although they do not aim for weakly supervised localization and thus, they do not present the LE, they produce saliency maps. I would like to see a LE comparison with that method. (minor -)

* In the introduction, p1 (last paragraph) other methods are briefly mentioned (Extracted saliency maps show all the evidence….superpixels), etc.) without references. Please add references when needed. (-)

* The framework presented in this paper was first proposed by Fan et al. (2017). The authors claim four main differences in their approach. In my eyes, not all of them are major or novel - probably the most impactful is removing superpixels as it simplifies the problem and implementation. (+)


APPROACH
* The authors aim for simplicity (strong +)

* The authors justify their approach and present their arguments clearly (strong ++)

* In the algorithm section, the authors first mention the sampling procedure and then their motivation. Please alter the ordering of these to be conceptually easier to understand your approach

* In the first sentence after the equation 6 I guess that “(cf. Alg. 1)” is a typo and should be modified to (Alg. 1)”

* After Equation 6 it is argued that the method resembles the training procedure of GANs (Godfellow et al., 2014) but not the work of Fan et al., 2017. (--)


EXPERIMENTS
* The authors define as their baseline the F thinning strategy (i.e. use only the first classifier) which is a model dependent salience map. While this is a useful comparison against the classifier dependent methods, given the similarity to the work of Fan et al. (2017), experiments comparing the proposed model to Fan et al. are necessary. It is important to control for network architecture (ResNet-50) and choice of \lambda to properly determine if the four changes outlined in Section 6 result in any real improvement over Fan et al. (strong --)

* The authors use the Table 1 (borrowed from Fong and Vedaldi (2017)) to compare their results against other methods. This comparison is problematic as different approaches are using different models as classifiers which may lead to increase or decrease of the LE. (--)

* In Table 2 the authors do not report how many times they run the same experiments to get these values. They also run less experiments with non-shared weights and they report only the LE. In my eyes it looks that the authors are trying to force their argument that the sharing weights helps (probably because it is one of their novelties). Please report the statistics of your experiments and fill the empty entries in the table. (--)

* In Table 3, what does the last row represent?

* Table 1 errors: (1) You write “Localization evaluation using OM, LE and F1 scores”. Please remove the F1 score as you do not report it. Also, correct the first sentence of the “Localization” subsection which states that you use three different metrics to “two different metrics”. (2) The LE from Fong and Vedaldi (2017) should be 43.2 and not 43.1.

* Regarding the unseen classes (section 5): (1) Please report in the appendix the classes that you are using in each subset. Are there classes correlated? (-)  (2) I see that there is a strong correlation between the LE on subset A and E. It looks like you are training on E and you generalize on A.


NOVELTY/IMPACT
* Novelty is a strong concern, given the work of Fan et al. (2017) (strong --). Nevertheless, the authors propose some changes that can be seen as more general, but the effectiveness of the changes is clearly established.

* This paper’s strongest point is the simplicity (conceptually and implementation-wise) of the method, an advantage over previous works (+)


OTHER COMMENTS
* Fan et al. (2017), use an adaptive λ that pushes the mask to 10% of the image whereas you are using a fixed one that pushes the mask to approximately 50% of the image. How can you make sure that this is not the reason that you are getting better results?

If the authors can clearly and fairly demonstrate that the changes they propose over Fan et al (2017) result in improved performance, and the manuscript is adjusted to be more upfront about this prior work, I would consider increasing my rating.
","The sentiment of the review is mixed, leaning towards negative. The reviewer acknowledges the simplicity and effectiveness of the method (+), but expresses strong concerns about the novelty and the experimental comparisons (--). The sentiment score is therefore -30. The politeness of the language is generally high, with the reviewer using polite phrases such as 'please' and providing constructive feedback without being rude. The politeness score is 80.",-30,80
"This paper explores the idea of utilizing a secret random permutation in the Fourier phase domain to defense against adversarial examples. The idea is drawn from cryptography, where the random permutation is treated as a secret key that the adversarial does not have access to. This setting has practical limitations, but is plausible in theory.

While the defense technique is certainly novel and inspired, its use case seems limited to simple datasets such as MNIST. The permuted phase component does not admit weight sharing and invariances exploited by convolutional networks, which results in severely hindered clean accuracy -- only 96% on MNIST and 45% on CIFAR-10 for a single model. While the security of a model against adversarial attacks is important, a defense should not sacrifice clean accuracy to such an extent. For this weakness, I recommend rejection but encourage the authors to continue exploring in this direction for a more suitable scheme that does not compromise clean accuracy.

Pros:
- Novel defense technique against very challenging white-box attacks.
- Sound threat model drawn from traditional security.
- Clearly written.

Cons:
- Poor clean accuracy makes the technique very impractical.
- Insufficient baselines. While the permutation is kept as a secret, it is plausible that the adversary may attempt to learn the transformation when given enough input-output pairs. Also, the adversary may attack an ensemble of PPD models for different random permutations (i.e. expectation over random permutations). The authors should introduce an appropriate threat model and evaluate this defense against plausible attacks under that threat model.","The sentiment of the review is mixed but leans towards the negative side due to the recommendation for rejection. The reviewer acknowledges the novelty and inspiration behind the defense technique but criticizes its practical limitations and poor clean accuracy. Therefore, the sentiment score is -30. The politeness of the language is quite high; the reviewer provides constructive feedback and encourages the authors to continue their research despite recommending rejection. The language is respectful and professional, so the politeness score is 80.",-30,80
"This paper proposed a retrieval model based on the residual network and evaluated the use of ELMo word embedding with/without IDF weight. The results showed that there are significant gain when adding the residual network on top of the word embedding. 

Pros:
* This work set a strong baseline for the retrieving target paragraph for question answering on the SQuAD dataset.
* The experiments were sounds and leverage interesting points -- the use of word embedding itself as the feature representation didn't have as much impact to retrieval performance as the distance function.
* The studied problem -- retrieval for answering question rather than getting the most relevant document worth more attention.

Cons:
* The motivation of using the word embedding and contextual word embedding over the TF-IDF feature wasn't clear. Results on using simple feature like TF-IDF maybe useful to give readers better judgement of the use of word embedding.
* The choice of dataset, SQuAD over more retrieval based QA like TrivialQA also wasn't strongly motivated. Also, it would be nice to see how the QA result would be improve with better retrieval model. 
* Another use of TF-IDF/IDF and embedding is to use TF-IDF/IDF to identify the related document and then use word embedding to resolve semantic ambiguity. Do you have theoretical/empirical reason why this shouldn’t be considered?

Comment on writing:
    - In Section 3.1: the dimension of the tensor should reflect the meaning (vocab size, embedding size or the number of documents) rather than numbers.
    - In Section 3.1: since the weighting for each document is not shared, it would be clearer to just use M and W for each document instead of M’, W'
    - In Section 3.1: Evaluation metrics, e.g., recall@k, ROC, AUC; technical details, for example, tensor dimension, optimizer hyperparameters should be moved to the experiment section","The sentiment of the review is generally positive, as indicated by the acknowledgment of the strong baseline set by the work and the soundness of the experiments. However, there are some critical points raised regarding the motivation and choice of dataset, which slightly temper the overall positivity. Therefore, the sentiment score is 60. The language used in the review is polite and constructive, offering specific suggestions for improvement without being harsh or dismissive. Therefore, the politeness score is 80.",60,80
"This work introduces a framework for learning implicit models that is robust to mode collapse. It consists in learning an explicit model of the implicit model through maximum likelihood while the later is used to teach the explicit model to better match the data distribution. The resulting bi-level optimization is carried out with truncated unrolled stochastic gradient descent.

# Quality

The method combines an interesting set of ideas. It is validated on some reasonable experiments. 

However after reading the paper, I remain with too many unanswered questions:
- Why should the method avoid mode collapse? Experiments clearly show that it indeed is resilient to mode collapse, but I have would have been curious in seeing some more discussion regarding this point. What is the exact mechanism that solves the issue?
- What is the effect of K? Is mode collapse solved only because of the unrolled gradients?
- What is the effect of M? How does the method behave for M=1, as usually done in GANs?
- What if the explicit model has not enough capacity?
- The original Unrolled GAN paper presents better results for the ring problem. Why are results worse in the experiments?

More fundamentally what is the main benefit of this approach with respect to models that can be trained straight with maximum likelihood? (e.g., flow-based neural generative models; and as required for the explicit model) Is it only to produce generative models that are fast (because they are implicit)? Why not training only the explicit model directly on the data?

# Clarity

The paper is in general well-written, although some elements could be removed to actually help with the presentation.
- The development around influence functions could be removed, as the method ends up instead making use of truncated unrolled gradients.
- The theoretical analysis is straightforward and could be compressed in a single paragraph to motivate the method.

# Originality

The method makes use of several ideas that have been floating around and proposed in different papers. As far as I know, the combination proposed in this work is original.

# Significance

Results show clear resistance to mode collapse, which is an improvement for implicit models. However, other types of generative models generally do not suffer from this issue. Significance is therefore limited.
","The sentiment of the review can be considered moderately positive, as the reviewer acknowledges the interesting ideas and reasonable experiments but also raises several critical questions and concerns. Therefore, the sentiment score is 20. The politeness of the language is quite high; the reviewer uses polite and constructive language throughout, even when pointing out weaknesses and asking questions. Thus, the politeness score is 80.",20,80
"The paper proposes an alternative to commonly used ReLU activated networks. The ""gating"" and ""amount"" effects of the weights are decoupled. The authors claim that such architectures are easier to theoretically understand. That might be the case indeed, but I fail to see much value in obtaining such understanding of very contrived objects that are not being used in practice. Unless such architectures can be proven to be interesting from a practical standpoint I do not think there is much of a point in studying them. The argument provided by the authors that they can - in a simple situation - have as much expressive power as a standard ReLU activated architecture is insucfficient, in my opinion, to justify researching them. Also, if a strong, deep theorem was proven using GaLU networks was proven, I would be inclined to recommend the paper to be accepted. As is - I do not find the paper to be a contribution significant enough for ICLR.","The sentiment of the review is generally negative, as the reviewer expresses skepticism about the value and practical relevance of the proposed architecture. The reviewer explicitly states that they do not find the paper to be a significant contribution. The politeness of the language is relatively neutral; while the reviewer is critical, they do not use rude or disrespectful language. They provide clear reasons for their critique and suggest conditions under which they might recommend acceptance.",-60,10
"The authors propose a multi-objective neural architecture search based on an evolutionary algorithm. The contradicting objective functions are optimized by ranking the candidates by Pareto-dominance, replace the bottom 50% with new candidates generated by the top 50% candidates through random mutations. The multi-objective function considers classification accuracy and an approximation of the inference speed. The method is compared to MobileNet and Mobile NASNet on ImageNet indicating an improvement with respect to search time.

The authors admit that their work is incremental and a combination of existing work. Furthermore, they admit that Dong et al. (2018) is the closest related work, however, they do not compare to them in the experimental section. The method by Dong et al. requires only 8 GPU days (Dvolver requires 50) yielding very similar results. Why this has been ignored remains unclear.

The paper is not self-contained, important methodological aspects of the method are insufficiently described. I recommend at least to formally define the crowding distance. It would be also reasonable to define your objective functions already in Section 3 instead of mentioning them in the caption of Figure 3 and its axis labels.

I think it's fair to call your approach evolutionary but you might want to discuss its relationship to beam search and in this scope discuss [A].

The comparison in Table 2 is not fair. You use the swift activation function and do not report the corresponding numbers for MobileNet or Mobile NASNet. Ramachandran et al. (2017) report these (75% and 74.2% for NASNet and MobileNet).
Comparing the Dvolver architecture with ReLU activations to MobileNet does not indicate any improvements.

You mention that most previous approaches are only keeping track of the best solution while you evolve over a population. Maybe this sentence is not well written and something else is meant but now this statement is wrong.

[A] Thomas Elsken, Jan Hendrik Metzen, Frank Hutter: Simple And Efficient Architecture Search for Convolutional Neural Networks. CoRR abs/1711.04528 (2017)","The sentiment of the review is moderately negative. The reviewer acknowledges the proposed method and its comparison to existing methods but highlights several critical issues, such as the lack of comparison to closely related work, insufficient methodological descriptions, and unfair comparisons in the results. The sentiment score is -40 because the review contains more criticism than praise. The politeness score is 20 because the language used is formal and professional, but it is not particularly polite or encouraging. The reviewer uses phrases like 'remains unclear,' 'not self-contained,' and 'not fair,' which are critical but not rude.",-40,20
"This paper proposes a way to speed up initial training a model.  The key idea
is to:

1. Train an autoencoder on the full dataset and select a subset of training 
examples.  The subset is the union of examples that maximally activate each of
the dimensions of the autoencoder's low-dimensional embedding.

2. Then a target classifier is trained on the subset,

3. followed by final fine-tuning on the full dataset.

The paper is understandably written, although some crucial experimental details
need a bit of guesswork.

Their proposal is evaluated on only one dataset, CIFAR10, using an autoencoder  
and classifier of roughly similar design from the initial convolutional layers. 

They mention a baseline [classifier training, I presume] classifier training 
over ~200 epochs in 736 s (~12 min) to get 83% accuracy.  This skips steps 1. 
and 2.  Since this is already fast, CIFAR10 is perhaps too small a dataset
to spur readers to use their proposed method (which does require them to
additionally train an autoencoder) when tackling more ambitious problems.

They do not report the time taken to train their autoencoder for 800 epochs
(step 1.).  For larger networks and images, it might also be important to
investigate whether an autoencoder considerably simpler than the classifier
model can suffice for subset selection; for example, if I want to train a
Resnet-152 classifier can I use a poorer quality autoencoder?  Since
using a randomly selected subset 20% of the original size works about
as well as step 1 for CIFAR10, I cannot judge whether the time taken to
set up and train an autoencoder makes it worthwhile to further reduce
the training subset from 20% to ~8% of the original size.

They do not consider alternative subset selection (1.) methods.  For example,
one might use a pretrained network to select examplar images by a clustering
method (ex. [2]), possibly providing representative images per class.  Other 
selection criteria are also possible -- for example, [1] evaluates subset 
selection based on ""representativeness"" vs ""diversity"" criteria.

They do not compare with many existing approaches to training set compression.
Instead, they dismiss (Sec. 3 ""Related Work"") most previous work on selecting a 
small subset of training examples.  However, googling will quickly find many
papers on subset selection (exactly what they do) as well as related dataset
optimization techniques (such loss-based revisiting of training examplars, or
training example weighting etc.).  For example, review-type article [3] 
provides a good introduction to existing subset selection techniques, as well
as references to earlier papers.

It is unclear whether the autoencoder training time is included in their
experiments that fix the total training time to 7 minutes and compare results
with different numbers of fast epochs (step 2.).

No guidelines are given for how to select the dimensionality of the autoencoder
embedding, and how the selection procedure should be done in cases with large
numbers of classes, although they mention the possibility of using combinations
of activations for subset selection.  I do not understand how in problems with
larger numbers of classes I can guarantee that the training subset will contain
at least one representative from each class.  Some alternative subset selection
methods can provide such guarantees, which might be important for training
datasets with class imbalance.

Given that they do not use a very large dataset, where their technique would
really be needed, and that they provide no comparison with other possibly
faster and better ways to select a subset of training examples, I cannot argue
for acceptance of this paper.


[1] ""Learning From Less Data: Diversified Subset Selection and
Active Learning in Image Classification Tasks"", Kaushal et al.
https://arxiv.org/abs/1805.11191

[2] Li, D., & Simske, S. (2011). Training set compression by incremental
clustering. Journal of pattern recognition research, 1, 56-64.

[3] Borovicka, T., Jirina Jr, M., Kordik, P., & Jirina, M. (2012). Selecting
representative data sets. In Advances in data mining knowledge discovery and
applications. InTech.
","The sentiment of the review is generally negative. The reviewer points out several significant shortcomings of the paper, such as the lack of crucial experimental details, the limited evaluation on only one dataset, the absence of comparisons with existing approaches, and the unclear guidelines for selecting the dimensionality of the autoencoder embedding. The reviewer concludes by stating that they cannot argue for the acceptance of the paper. However, the language used is polite and professional. The reviewer provides constructive criticism and suggests alternative methods and references for the authors to consider.",-60,80
"Paper summary: The paper presents a 2-step approach to generate strong adversarial examples at a far lesser cost as compared to recent iterative multi-step adversarial attacks. The authors show the improvements of this technique against different attacks and show that the robustness of their 2-step approach is comparable to the iterative multi-step methods. 

The paper presents an interesting technique, is nicely written and easy to read. The fact that their low-cost 2-step method achieves is robust enough to iterative multi-step methods that are expensive is significant.  

Pros: 
1) The technique is low-cost as compared to other expensive techniques like PGD and IFGSM 
2) The technique tries to use the categorical distribution of the generated example in the first step to generate an example in the second step, such that the generated image is most different from the first. This is important and different from the most common technique of iteratively maximizing the loss between the generated samples. 
3) The authors show the effetiveness  and improvement of the approach to various attack methods as compared to existing defense techniques
4) The authors evaluate their technique on MNIST and SVHN datasets


Cons or shortcomings/things that need more explanation :
1) It would have been really good to the kind of adversarial examples generated by this technique look like as compared to the examples generated by the other strategies. 
2) In table 2, for the substitute models of FGSM trained on H and S labels (rows 2 and 5), it is unclear why the accuracies are so low when attacked on FGSM (hard) and FGSM(soft) models. 
 ","The sentiment of the review is quite positive, as indicated by phrases like 'interesting technique,' 'nicely written,' and 'significant.' The reviewer appreciates the low-cost nature of the technique and its robustness, which are highlighted as significant contributions. The politeness of the language is also high, as the reviewer uses polite and constructive language throughout, even when pointing out areas for improvement. The suggestions for improvement are framed in a helpful manner rather than as criticisms.",80,90
"In the manuscript entitled ""Likelihood-based Permutation Invariant Loss Function for Probability Distributions"" the authors propose a loss function for training against instances in which ordering within the data vector is unimportant.  I do not find the proposed loss function to be well motivated, find a number of confusing points (errors?) in the manuscript, and do not easily follow what was done in the examples.

First, it should be noted that this is a very restricted consideration of what it means to compare two sets since only sets of equal size are under consideration; this is fundamentally different to the ambitions of e.g. the Hausdorff measure as used in analysis.  The logsumexp formulation of the proposed measure is unsatisfactory to me as it directly averages over each of the independent probabilities that a given element is a member of the target set, rather than integrating over the combinatorial set of probabilities for each set of complete possible matches.  Moreover, the loss function H() is not necessarily representative of a generative distribution.

The definition of the Hausdorff distance given is directional and is therefore not a metric, contrary to what is stated on page 2.

I find the description of the problem domain confusing on page 3: the space [0,1]^NxF is described as binary, but then values of log y_i and log (1-y_i) are computed with y in [0,1] so we must imagine these are in fact elements in the open set of reals: (0,1).

Clarity of the examples could be greatly improved, in particular by explaining precisely what is the objective of each task and what are the 'ingredients' we begin with.","The sentiment of the review is largely negative, as the reviewer expresses dissatisfaction with the motivation, clarity, and correctness of the manuscript. Specific criticisms include the restricted consideration of set comparison, the unsatisfactory logsumexp formulation, and errors in the definition of the Hausdorff distance. The politeness of the language is relatively neutral; while the reviewer points out several issues, they do so in a straightforward and factual manner without using derogatory or overly harsh language.",-70,10
"This paper proposed the use of uncertainty measure evaluated by the prior network framework in (Malinin and Gales 2018) to detect adversarial inputs. Empirically, the best detector against three L_infinity based attacks (FGSM, BIM and MIM), is a prior network that is adversarially trained with FGSM, in both white-box and black-box settings. The results also showed superior performance over a detector based on Monte Carlo Dropout methods (MCDP). Although the idea is interesting and the presented results seem promising, there are some key experiments lacking that may prevent this work from making its claims on robustness and detectability. The detailed comments are as follows.

1. Detection performance against high-confidence adversarial examples is lacking : In many of Carlini-Wagner papers, they showed that some detection methods become weak by simply increasing the confidence parameter (kappa) in the CW attack. The three attacks considered in this work, FGSM, BIM, and MIM are all L_infinity attacks, which are known to introduce unnecessary noises due to the definition of L_infinitiy norm. On the other hand, CW attack is a strong L2 attack and it also offers a way of tuning confidence of the adversarial example. In addition, a variant of CW L2 attack, called Elastic-Net attack https://arxiv.org/abs/1709.04114, is able to generate L1-norm based adversarial examples that can bypass many detection methods. Without the results of attack performance vs different confidence levels against strong L1 and L2 attacks, the detection performance is less convincing. 

2. Lack of comparison to existing works - there are several detection works that already used uncertainty in detection. A representative paper is MagNet https://arxiv.org/abs/1705.09064 . MagNet paper showed that detection against FGSM/BIM is easy (even without adversarial training), and shows some level of robustness against CW L2 attack when the attacker is unaware of the detection. Later on, MagNet has been bypassed if the detection is known to the adversary https://arxiv.org/abs/1711.08478. Since MagNet and this paper have similar detection methodology using uncertainty, and the detection performance seems similar, the authors are suggested to include MagNet for comparison.

3. The objective of adaptive adversarial attack is unclear - inspecting how MagNet's detection performance is degraded when the attacker knows the detection mechanism https://arxiv.org/abs/1711.08478, the authors should do an adaptive attack that directly includes eqn (8) as one of the attack loss term, rather than using the KL term. In addition, if there is randomness in calculating the MI term for adaptive attacks, then averaged gradients over randomness should be used in adaptive attacks. Lastly, CW L2/EAD L1 attacks with an additional loss term using (8) should be compared.

4. The white-box attacks in Fig. 2 (b) to (c) seem to be quite weak - not be able to reach 100% success rate (saturates around 90%) when using BIM and MIM on the undefended model (DNN) with large attack strength. This might suggest some potential programming errors or incorrect attack implementation. 

5. What black-box attack is implemented in this work? It's not clear what kind of black-box attack is implemented in this paper: is it transfer attack? score-based black-box attack? or decision-based black-box attack? Can the proposed method be robust to these three different settings?

6. This paper heavily relies on the work in  (Malinin and Gales 2018), and basically treats adversarial input detection as an out-of-distribution detection problem. Please emphasize the major differences and differentiate the contributions between these two works.

7. In Fig. 2, it seems that adversarial training with FGSM is actually the key factor that makes the detection work (by comparing PN vs PN-ADV in (b) and (c)). To justify the utility of the proposed metric in detection adversarial inputs, the authors are suggested to run MCDP on FGSM-trained model and compare the performance with PN-ADV.","The sentiment of the review is mixed but leans towards positive. The reviewer acknowledges the interesting idea and promising results but highlights several significant shortcomings and missing experiments. Therefore, the sentiment score is 20. The language used in the review is polite and constructive, offering specific suggestions for improvement without being dismissive or rude. Thus, the politeness score is 80.",20,80
"In this paper, authors proposed an ensemble approach for query reformulation (QR).  The basic idea is that 1) train a bunch of models/sub-agents on subsets, e.g., randomly partitioned, of the training data; 2) and then train an additional meta model/meta-agent to aggregate the results from the step 1).  They conduct experiments on document retrieval and question answering tasks to show the effectiveness of the proposed model.

This paper is well written and easy to follow.  
However there are several my concerns. 

1. It is counter intuitive, e.g., why sub-agents trained on full training dataset obtain worse results than on its subset. Regarding diversity, one may use different random seeds or different dropout rates instead of sample a subset of training data. 

2. The baseline is much lower than the current SOTA systems. Such as the best result on SearchQA in this paper is 50.5 in terms of F1 score. However R3 and Re-Ranker obtains 55.3 and 60.6 respectively. Could the proposed approach be adapted on those models? Note that those SOTA systems are released.

3. The proposed system is quite similar to Nogueira& Cho 2017 and Buck et al. 2018. I'm not very sure the contribution of this work and its novelty.  

Questions:
1. Why the authors didn't use beam search during the sub-agent training? 
2. It seems that the proposed framework is a pipeline model: firstly it trains a bunch of sub-agents; and then trains meta-agent. Is it possible to fine-tune the model jointly?
3. What is Extra Budget in Table 1?    ","The sentiment of the review is generally positive, as indicated by the statement 'This paper is well written and easy to follow.' However, the reviewer also expresses several concerns and provides critical feedback, which slightly tempers the overall positivity. Therefore, the sentiment score is 40. The politeness of the language is quite high; the reviewer uses polite phrases such as 'However there are several my concerns' and 'Could the proposed approach be adapted on those models?' which indicates a respectful tone. Thus, the politeness score is 80.",40,80
"TITLE
A VARIATIONAL AUTOENCODER FOR PROBABILISTIC NON-NEGATIVE MATRIX FACTORISATION

REVIEW SUMMARY

Well written, interesting new idea, modest technical contribution, limited demonstration.

PAPER SUMMARY

The paper presents an approach to NMF within a variational autoencoder framework. It uses a Weibull distribution in the latent space. 

QUALITY

The work appears technically sound except for minor typos. 

CLARITY

Overall the paper is a pleasure to read. Only the presentation of the standard vae could be more clear.

ORIGINALITY

The method is (to my knowledge) novel. 

SIGNIFICANCE

I think this paper is a significant contribution. I feel I have learned something from reading it, and am motivated to try out this approach. I believe there should be a wide general interest. The technical contribution is perhaps somewhat modest, as the paper fairly straightforwardly includes non-negativity in a vae setting, but I think this is a good idea. The demonstration of the algorithm is also quite limited - I would have enjoyed seeing this applied to some more reaslistic, practical problems, where perhaps the quantification of uncertaincy (which is one of the main benefits of a vae-based nmf) would come more directly into play. 

FURTHER COMMENTS

page 3

The presentation of the VAE objective is a bit oblique. The statement ""they require a different objectiv function"" is not wrong, but also not very precise. The equality in eq. (2) is incorrect (I assume this is meant to be a stochastic approximation, i.e. the expectation over q approximated by sampling?)

""with \hat v  the reconstructed vector"" Not clear. I assume \hat v is reconstructed from a sample from q given v ?

There is a typo in eq. 3. The first factor in the second to last term should be (lambda_1/\lambda_2)^(k_2)

","The review starts with a positive sentiment, highlighting that the paper is well-written and presents an interesting new idea. The reviewer acknowledges the technical soundness and clarity of the paper, although they note minor issues such as typos and a somewhat limited demonstration. The language used is polite and constructive, offering specific suggestions for improvement without being overly critical. The reviewer expresses appreciation for the paper's contribution and indicates a desire to explore the approach further, which reinforces the positive sentiment.",80,90
"The authors present two methods for learning a similarity score between pairs of graphs. They first is to use a shared GNN for each graph to produce independent graph embeddings on which a similarity score is computed. The authors improve this model using pairs of graphs as input and utilizing a cross-graph attention-mechanism in combination with graph convolution. The proposed approach is evaluated on synthetic and real world tasks. It is clearly shown that the proposed approach of cross-graph attention is useful for the given task (at the cost of extra computation).

A main contribution of the article is that ideas from graph matching are introduced to graph neural networks and it is clearly shown that this is beneficial. However, in my opinion the intuition, effect and limitations of the cross-graph attention mechanism should be described in more detail. I like the visualizations of the cross-graph attention, which gives the impression that the process converges to a bijection between the nodes. However, this is not the case for graphs with symmetries (automorphisms); consider, e.g., two star graphs. A discussion of such examples would be helpful and would make the concept of cross-graph attention clearer.

The experimental comparison is largely convincing. However, the proposed approach is motivated by graph matching and a connection to the graph edit distance is implied. However, in the experimental comparison graph kernels are used as baseline. I would like to suggest to also use a simple heuristics for the graph edit distance as a baseline (Riesen, Bunke. Approximate graph edit distance computation by means of bipartite graph matching. Image and Vision Computing, 27(7), 2009).


There are several other questions that have not been sufficiently addressed in the article.

* In Eq. 3, self-attention is used to compute graph level representations to ""only focus on important nodes in the graph"". How can this be reconciled with the idea of measuring similarities across the whole graph? Can you give more insights in how the attention coefficients vary for positive as well as negative examples? How much does the self-attention affects the performance of the model in contrast to mean or sum aggregation?
* Why do you chose the cross-graph similarity to be non-trainable? Might there be any benefits in doing so?
* The note on page 5 is misleading because two isomorphic graphs will lead to identical representations even if communication is not reduced to zero vectors (this happens neither theoretically nor in practice).
* Although theoretical complexity of the proposed approach is mentioned, how much slower is the proposed approach in practice? As similarity is computed for every pair of nodes across two graphs, the proposed approach, as you said, will not scale. In practice, how would one solve this problem given two very large graphs which do not fit into GPU memory? To what extent can sampling strategies be used (e.g., from GraphSAGE)? Some discussion on this would be very fruitful.


In summary, I think that this is an interesting article, which can be accepted for ICLR provided that the cross-graph attention mechanism is discussed in more detail.


Minor remarks:

* p3: The references provided for the graph edit distance in fact consider the (more specific) maximum common subgraph problem.","The sentiment of the review is generally positive, as the reviewer acknowledges the usefulness and benefits of the proposed approach, despite suggesting several improvements and additional discussions. The reviewer appreciates the visualizations and the main contributions of the paper. The sentiment score is therefore 70. The politeness of the language is high, as the reviewer uses polite phrases such as 'I would like to suggest' and 'In my opinion,' and provides constructive feedback without being harsh or dismissive. The politeness score is 90.",70,90
"The paper proposes deep learning extension of the classic paradigm of 'conformal prediction'. Conformal prediction is similar to multi-label classification, but with a statistical sound way of thresholding each (class-specific) classifier: if our confidence in the assignment of an x to a class y is smaller than \alpha, then we say 'do not know / cannot classify'). This is interesting when we expect out of distribution samples (e.g., adversarial ones).

I think this paper, which is very well written, would make for nice discussions at ICLR, because it is (to my knowledge) the first that presents a deep implementation of the conformal prediction paradigm.  However, there are a couple of issues, which is why I think it is definitely not a must have at ICLR. The concrete, deep implementation of the approach is rather straightforward and substandard for ICLR: Features are taken from an existing, trained SOTA DNN, then input KDE, based on which for each class the quantiles are computed (using a validation set). Thus, feature and hypothesis learning are not coupled, and the approach requires quite a lot of samples per class (however, oftentimes in multi-label prediction we observe a Zipf law, ie many classes have fewer than five examples). Furthermore, there is no coupling between the classes; each class is learned separately; very unlikely this will work better than a properly trained multi-class or (e.g., one-vs.-rest) multi-label classifier in practice. Since a validation set is used to compute the quantiles, substantial 'power' is lost (data not used very efficiently; although that could be improved at the expense of expensive CV procedures).
","The sentiment of the review is moderately positive. The reviewer acknowledges that the paper is well-written and presents an interesting and novel approach to conformal prediction using deep learning, which would make for nice discussions at ICLR. However, the reviewer also points out several significant issues with the implementation, suggesting that it is not a must-have for the conference. Therefore, the sentiment score is 40. The politeness of the language is quite high. The reviewer uses polite and constructive language, providing specific feedback and suggestions for improvement without being dismissive or harsh. Therefore, the politeness score is 80.",40,80
"===========================
Since the authors did not provide a proper response to my questions, I have lowered my score from 7 to 6. I think this paper will have a good chance to be a good paper if evaluated more comprehensively, as suggested by reviewers. 
===========================

Contributions:

The main contribution of this paper is the study of the currently adopted evaluation metrics for textual GAN models. It was shown that BLEU and Self-BLEU scores used by previous work are insufficient to evaluate textual GAN models, and the authors propose that Frechet Distance and reverse Language Model scores can be a good complement to the above BLEU score evaluations. 

Detailed Comments:

(1) Novelty: It seems to me that this paper is timely, as developing GAN models for text generation gains more and more attention in the research community, and it is indeed much needed to provide good evaluation methods. The proposed new metrics seem proper, and the observation that most GAN models do not yield obviously better results than conventional LM is also insightful. 

(2) Presentation: This paper is generally well-written and easy to follow. However, when discussing related work in section 3.1, I think one literature [*] is missed. It uses annealed softmax to approximate argmax for textual GAN. 

[*] Adversarial Feature Matching for Text Generation, ICML 2017

(3) Evaluation: The experiments are generally well-executed, with some questions listed below.

Questions:

(1) I have some concerns in terms of human evaluation. Though human evaluation is the golden metric, it seems that presenting individual sentences to human raters does not account diversity into consideration. Therefore, systems that generate high quality samples but with less diversity will get a high score in terms of human evalution. Can the authors provide some discussion on this? And if this is the case, how will this change the conclusions in this paper?

(2) I understand why the authors use simplified GAN models for evaluation. However, if the models are not simplified, what the performance will be for LeakGAN and MaskGAN, for example? This seems to be relatively easy to evaluate since the code is open sourced. 

Minor issues:

(1) I think the citation format needs to be changed. For example, in many places, it is more natural to use ""(Hassan et al., 2018)"" than ""Hassan et al. (2018)"" for example. ","The sentiment of the review is moderately positive. The reviewer acknowledges the potential of the paper to become a good one if evaluated more comprehensively and appreciates the contributions and novelty of the work. However, the sentiment is slightly tempered by the lowering of the score due to the authors not providing a proper response to previous questions. Therefore, the sentiment score is 40. The politeness of the language is quite high. The reviewer provides constructive feedback, acknowledges the strengths of the paper, and phrases their concerns and suggestions in a respectful manner. Thus, the politeness score is 80.",40,80
"The paper intends to utilize natural gradient induced by Wasserstein-2 distance to train the generator in GAN. Starting from the dynamical formulation of optimal transport, the authors propose the Wasserstein proximal operator as a regularization, which is simple in form and fast to compute. The proximal operator is added to training the generator, unlike most other regularizations that focus on the discriminator. This is an interesting direction. 

The motivation is clear but by so many steps of approximation and relaxation, the authors didn’t address what is the final regularization actually corresponding to? Personally I am not convinced that theoretically the proposed training method is better than the standard SGD. The illustration example in the paper is not very helpful as it didn’t show how the proposed proximal operator works. The proximal operator serves as a regularization and it introduces some error, I would like to know how does this carry over to the whole training procedure. 

In GAN, the optimal discriminator depends on the current generator. Many approaches to GAN training (i.e. WGAN-GP) advocates to update the generator once in every “outer-iteration”. I am not sure how the proposed approach fit in those training schemes.

In the simulation, the difference is not very significant, especially in FID vs iteration number. This could be due to parameter tuning in standard WGAN-GP. I encourage more simulation studies and take more GAN structures into consideration. 

Lastly, the stability mentioned in the paper lacks a formal definition. Is it the variance of the curves? Is it how robust the model is against outer iterations?","The sentiment of the review is mixed. The reviewer acknowledges the interesting direction and clear motivation of the paper but expresses significant concerns about the theoretical justification, practical implementation, and empirical results. Therefore, the sentiment score is slightly negative. The language used is polite and constructive, with the reviewer providing specific feedback and suggestions for improvement without being dismissive or rude.",-20,80
"The paper proposes a technique (well, two) to prune convolutional layers to reduce the required amount of computation when  the convolutions are done using the winograd algorithm. Winograd convolutions first transform the image and the filter, apply a multiplication in the transformed space, and then retransform the image back to the intended image space. The transformation of the filter, however, means that sparsity in the regular domain does not translate to sparsity in the winograd domain.

This paper presents two techniques to achieve sparsity in the winograd domain: approximating winograd sparsity based on sparsity in the regular domain (thereby pruning with a non uniform cost model) and pruning in winograd space directly. The actual implementation alternates the first pruning technique and retraining the network with fixed sparsity followed by alternating winograd-space pruning and retraining. The tricky part is retraining in winograd space, which seems to require fine tuned per coordinate learning rates.

My main concern is that the method feels fairly fragile and hyperparameter-heavy: tuning all the learning rates and sparsity rates for all these iterated levels of pruning doesn't seem easy. Similarly, it's unclear why the first stage of pruning is even needed if it's possible to prune and fine tune in winograd space directly. It's unclear from reading the paper how, given a computational budget, to decide the time spent in each phase of the process.

","The sentiment of the review appears to be slightly negative. The reviewer acknowledges the proposed techniques but expresses significant concerns about the method's fragility, hyperparameter complexity, and the unclear necessity of the first pruning stage. Therefore, the sentiment score is -30. The politeness of the language used is relatively high. The reviewer provides constructive criticism without using harsh or rude language, and the tone remains professional and respectful throughout. Thus, the politeness score is 80.",-30,80
"This paper presents a technique for embedding words in hyperbolic space, which extends previous non-euclidean methods to non-structured data like free text. The authors provide a new gradient based method for creating the embeddings and then evaluate them on standard word embedding benchmarks. Overall the paper is very well written and well executed. They find that in the low dimensions the approach outperforms standard Euclidean space methods while in higher dimensions this advantage disappears.

The results do not try to claim state of the art on all benchmarks, which I find refreshing and I appreciate the authors candor in giving an honest presentation of their results. Overall, I enjoyed this paper and am eager to see how the authors develop the approach further. 

However, along these same lines it would be great to have the authors provide more discussion about the next steps and potential applications for this approach. Is the interest here purely methodological? Are there potential use cases where they believe this approach might be superior to Euclidean approaches? More detail in the discussion and intro about the trajectory of this work would help the reader understand the methodological and application-specific implications. 

Pros:
- Clearly written and results are presented in a straightforward manner. 
- Extension of analogy reasoning to non-euclidean spaces.

Cons:
- Lack of clear motivation and compelling use case. 
- It would be nice to have a visualization of the approach in 2-dimensions. While Figure 3 is instructive for how analogies work in this space, it would be great to visualize an entire dataset. I'm sure that the proposed embeddings would result in  a very different space than euclidean embeddings (as the Poincare embedding paper showed), so it would be great to have at least one visualization of an embedded dataset. Presumably this would play to the strengths of the approach as it excels in lower dimensions. 
-  The largest of embedding dimension tested was 100, and it is common to use much larger embeddings of 500-d. Do the trends they observe continue to larger dimensions, e.g. is the performance gap even larger in higher dimensions? ","The sentiment of the review is highly positive, as indicated by phrases like 'very well written and well executed,' 'I appreciate the authors candor,' and 'I enjoyed this paper and am eager to see how the authors develop the approach further.' The reviewer also provides constructive feedback and suggestions for improvement, which are framed in a positive and encouraging manner. Therefore, the sentiment score is 90. The politeness of the language is also very high, as the reviewer uses polite and respectful language throughout the review, such as 'it would be great to have the authors provide more discussion' and 'I appreciate the authors candor.' Therefore, the politeness score is 95.",90,95
"This paper proposes a WAE variant based on a new statistical distance between the encoded data distribution and the latent prior distribution that can be computed in closed form without drawing samples from the prior (but only when it is Gaussian). The primary contribution is the new CW statistical distance, which is the l2 distance between projected distributions, integrated over all possible projections (although not calculated as so in practice).  
  
Plugging this distance into the WAE produces similar performance to existing WAE variants, but does not really advance the existing achievable performance.  Overall, I quite liked the paper and think it is well-written, but I believe the authors need to highlight at least one practical advance introduced by the CW distance (in which case I will raise my score). Some potential options include:

1) Faster training times. It seems to me one potential advantage of the closed-form distance would be that the stochastic WAE-optimization can converge faster (due to lower-variance gradients).  However, the authors only presented per-batch processing times as opposed to overall training time for these models.   

2) Stabler training. Perhaps sampling from the prior (as needed to compute statistical distances in the other WAE variants) introduces undesirable extra variance in the training procedure. The authors could run each WAE training process K times (with random initialization) to see if the closed-form distance enables more stable results.

3) Usefulness of the CW distance outside of the autoencoder context.
Since the novelty of this work lies in the introduction of the CW distance, I would like to see an independent evaluation of this distance as a  general statistical distance measure (independently of its use in CWAE). Can you use this distance as a multivariate-Gaussian goodness of fit measure for high-dimensional data drawn from both Gaussian and non-Gaussian distributions and show that it actually outperforms other standard statistical distances (e.g. in two-sample testing power)?

Without demonstrating any practical advance, this work becomes simply another one of the multitude of V/W-AE-variants that already exist.

Other Comments:

- While I agree that standard WAE-MMD and SWAE require some form of sampling to compute their respective statistical distance, a variant of WAE-MMD could be converted to a closed form statistical distance in the case of a Gaussian prior, by way of Stein's method or other existing goodness-of-fit measures designed specifically for Gaussians. See for example: 

Chwialkowski et al: https://arxiv.org/pdf/1602.02964.pdf

which like CW-distance is also a quadratic-time closed-form distance between samples and a target density.

Besides having closed form in the case of a Gaussian prior (which other statistical distances could potentially also achieve), it would be nice to see some discussion of why the authors believe their CW-distance is conceptually superior to such alternatives. 

- Silverman's rule of thumb is only asymptotically optimal when the underlying data-generating distribution itself is Gaussian. Perhaps you can argue here that due to CLT: the projected data (for high-dimensional latent spaces) should look approximately Gaussian?

After reading the revision: I have raised my score by 1 point and recommend acceptance.","The sentiment of the review is generally positive, as indicated by phrases like 'I quite liked the paper' and 'well-written.' However, the reviewer also points out that the paper does not advance the existing achievable performance and suggests several areas for improvement. This mixed feedback results in a sentiment score of 50. The politeness of the language is high, with the reviewer using polite and constructive language throughout, such as 'I believe the authors need to highlight' and 'it would be nice to see some discussion.' Therefore, the politeness score is 90.",50,90
"This paper proposed a general framework, DeepTwist, for model compression. The so-called weight distortion procedure is added into the training every several epochs. Three applications are shown to demonstrate the usage of the proposed approach.

Overall, I think the novelty of the paper is very limited, as all the weight distortion algorithms in the paper can be formulated as the proximal function in proximal gradient descent. See http://www.stat.cmu.edu/~ryantibs/convexopt-S15/scribes/08-prox-grad-scribed.pdf for a reference.

Specifically, the proposed framework can be easily reformulated as a loss function plus a regularizer for proximal gradient. Using gradient descent (GD), there will be two steps: (1) finding a new solution using GD, and (2) project the new solution using proximal function. Now in deep learning, since SGD is used for optimization, several steps are need to locate reasonable solutions, i.e. the Distortion Step in the framework. Then proximal function can be applied directly after Distortion Step to project the solutions. In this way, we can easily see that the proposed framework is a stochastic version of proximal gradient descent. Since SGD is used for training, several minibatches are needed to achieve a relatively stable solution for projection using the proximal function, which is exactly the proposed framework in Fig. 1.

PS: After discussion, I think the motivation of the method is not clear to understand why the proposed method works.  ","The sentiment of the review is generally negative, as the reviewer expresses that the novelty of the paper is very limited and suggests that the proposed framework can be easily reformulated using existing methods. The reviewer also mentions that the motivation of the method is not clear. The politeness of the language is relatively neutral; the reviewer provides constructive criticism without using harsh or rude language.",-60,10
"The paper proposes the use of Survival Continuous Ranked Probability score instead of maximum likelihood estimation for personalised probabilistic forecasts of time-to-event data, thus estimating a distribution over future time. The authors describe the evaluation their method using (1) proper scoring rule objectives; (2) evaluation of calibration using sharpness as a metric; (3) the survival precision recall curve. The authors then apply these techniques to predicting time-to-mortality using an RNN that takes EHR patient records to predict the probability of death at a given time point. It’s not clear how this is related to the Survival CRPS model or how this model is incorporated into the RNN.
Overall, this is an important framework for estimating personalised predictions of survival events for patients with interval-censored data. The authors present a well thought-out paper with clearly and realistically articulated modelling  assumptions. The authors also give an excellent critique of the underlying assumptions of current state-of-the-art survival methods. The authors are also to be commended for the mathematical elegance 
Although the paper is very well written and extremely well structured, I struggled with the lack of experiments available in the paper.
The text embedded in Figure 3 is too small. 
The results section is somewhat sparse. Although the mathematical formulation is well-motivated and structured, it’s not clear what the contribution of this work is. The difference between CRPS-INTVL and MLE-INTVL is incremental and it’s unclear what the significant benefits are of CRPS vs MLE. What would the interpretation of these differences in a real-world setting? 
","The sentiment of the review is generally positive, as the reviewer acknowledges the importance of the framework, the well-thought-out nature of the paper, and the mathematical elegance. However, there are some critical points raised, such as the lack of clarity on how the model is incorporated into the RNN, the small text in Figure 3, and the sparse results section. These criticisms are constructive and aimed at improving the paper. The politeness of the language is high, as the reviewer uses phrases like 'the authors are to be commended' and 'well thought-out paper,' and provides constructive feedback without being harsh.",60,80
"This paper aims to test the robustness of generative classifiers [1] w.r.t. adversarial examples, considering their use as a potentially more robust alternative to adversarial training of discriminative classifiers. To achieve this, *Deep Bayes*, a generalization of the Naive Bayes classifier using a latent variable model and trained in a fashion similar to variational autoencoders [2] is introduced, and 7 different latent variable models are compared, covering a spectrum of generative or discriminative classification models, with or without bottlenecks. Their DFX and DBX architectures in particular closely match traditional discriminative classifiers, without and with a latent bottleneck.

These 7 models are compared against a large range of adversarial attacks, depending on the kind of noise added (l_2 or l_inf) and how much the adversary can access (the full gradients of the model, its output on training data, or only the model as a black-box). The performance of the models is assessed depending on two criteria: how the performance of the classifier resists to adversarial noise, and how quickly the model can detect adversarial samples. Three methods for detecting adversarial samples are compared: the first (only applicable to generative classifiers) discards samples with a low likelihood, according to the off-manifold assumption [3], the second discards samples for which the classifier has low confidence in its classification (p(y|x) is under some threshold), and the third compares the output probability vector of the classifier on a sample to the mean classification vector of this class over the train data, and discards the sample if the two vectors are too dissimilar (meaning the classifier is over-confident or under-confident).

The main contribution of this paper is the extensive experiments that have been done to compare the models against the various adversarial attacks. While experiments were only done on small datasets like MNIST and CIFAR (generative classifiers don't scale as easily on large image datasets), they nonetheless give very interesting insights and the authors provided encouraging results on applying generative classifiers on features learned by discriminative classifiers. Theirs result shows that generative architecture are in general more robust to the current state-of-the-art adversarial attacks, and detect adversarial examples more easily. The authors also recognize that these results may be biased by the fact that current adversarial attacks have been specifically optimized towards discriminative classifier.

This is a solid paper in my opinion. The experimental setup and motivations are clearly detailed, and the paper was easy to follow. Extensive results and description of the experimental protocol are provided in the appendices, giving me confidence that the results should be reproducible. The results of this paper give interesting insights regarding how to approach robustness to adversarial examples in classification tasks, and provide realistic ways to try and apply generative classifiers in real-worlds tasks, using pre-learned features from discriminative networks.


[1] http://papers.nips.cc/paper/2020-on-discriminative-vs-generative-classifiers-a-comparison-of-logistic-regression-and-naive-bayes.pdf
[2] https://arxiv.org/abs/1312.6114
[3] https://arxiv.org/abs/1801.02774","The sentiment of the review is highly positive. The reviewer describes the paper as 'solid,' praises the clarity of the experimental setup and motivations, and expresses confidence in the reproducibility of the results. The reviewer also highlights the interesting insights and realistic applications provided by the paper. Therefore, the sentiment score is 90. The politeness of the language is also very high. The reviewer uses polite and respectful language throughout, acknowledges the authors' efforts, and provides constructive feedback without any negative or rude remarks. Thus, the politeness score is 100.",90,100
"Summary

The authors aim to do continual learning to solve dependent tasks using ""single-stage end-to-end learning"". The resulting ""Unicorn"" agent trains on all tasks simultaneously. The idea is to use multi-task ""off-policy learning"", which uses (old) trajectories (experience) from task A to help learning on a related task B. Authors further distinguish between goals (inputs to Q) and tasks (different reward functions). A goal might a color/shape of an object to pick up.

The core model is a UVFA that learns a goal-conditioned Q-function Q(s,a,g). 

Some technical aspects: 
- use n-step returns.
- when training Q on goal g_i, authors also trajectories that were generated using Q conditioned on another goal(s) g_j. They then truncate the returns for task i when an action taken is not optimal under Q(s,a,g_j) conditioned on goal j. The intuition (seems) to be that this 
- authors do not use experience replay or a target Q-function, since the parallelized implementation is reported to be stable enough.
- unicorn sees all train task reward functions during training (but not hold out task rewards).
- unicorn is tested on several 3d maze environments with key-lock etc semantics. The tasks / goals seem simple, and the dependency is defined by changing colors / shapes of objects to be picked up. Authors argue unicorn has to learn to relate task rewards to these goal features.
- unicorn is compared against baselines that 1) do single-task learning (expert) 2) learn on a sum of task rewards (glutton), 3) uniformly random baseline. 
- authors show that 1) unicorn performs better on train tasks 2) performs better on hold-out tasks. Also, authors show results for zero-shot transfer learning, with adding abstract tasks (extra reward for picking up any object) improving performance, 

Pro
- Simple approach (e.g., no experience replay etc), and uses only a limited set of techniques (e.g., reward truncation). 
- Reward performance suggests the model has more properly related goal features to different payoffs.
- Analysis of qualitative behavior is nice.

Con
- The writing is a bit dense in places, e.g., the discussion of baselines is a bit hard to read.
- Description of algorithm is wrapped in long text, a clear algorithm box would make the approach much clearer.
- Not clear what kind of hyperparameters are introduced / used / tuned for Unicorn. 
- Authors say ""deep dependency"", but this seems to just refer to different colors / shapes between objects in the env used in the paper. How is ""dependency"" between goals and tasks defined in general? 
- The experimental setting seems a bit limited, authors only show results on a single domain, and do not offer rigorous definitions. This makes the scope of the paper rather limited.  

Reproducibility: 
- It's not clear what the variance in the baseline performance is (variance only shown for unicorn).","The sentiment of the review appears to be generally positive, as the reviewer acknowledges the strengths of the approach, such as its simplicity and effective performance. However, there are also several criticisms regarding the writing clarity, experimental scope, and lack of detailed information on hyperparameters. Therefore, the sentiment score is moderately positive. The politeness of the language is quite high, as the reviewer uses constructive language and provides specific suggestions for improvement without being harsh or dismissive.",40,80
"Summary:
The paper proposes a system of semantic segmentation based on sequential processing of the image in a patch-wise manner with multiple ""actors"", sharing a common external memory. This approach stands in contrast to the more usual approach of single-shot prediction for the whole image, where encoder-decoder architectures or dilated convolutions are used to capture the global context. The authors then discuss three-variants of this method, out of which two use external memory (Bi-MANN, SHAMANN), and one uses memory shared between actors (SHAMANN). Results are presented on segmentation of lung X-ray data and on MNIST digit completion.

Comments:
The paper is easy to read. The authors cite the relevant literature on the baseline semantic segmentation methods, as well as neural networks with external memories. However, similar patch-wise and sequential methods have been presented in the literature (e.g. https://arxiv.org/abs/1506.07452), including ones with external storage (e.g. https://www.nature.com/articles/s41592-018-0049-4), but these are not discussed as prior work.

Overall, the proposed approach is interesting, but significantly more complex than both the baselines and prior work. As is, the experimental results are not compelling enough to justify this (lack of clear quantitative improvement over state of the art). My recommendation would be to conduct additional experiments on semantic segmentation benchmark datasets. The proposed method seems promising for volumetric data as the authors note, but this also needs to be demonstrated experimentally.

Some more specific & technical questions follow:
- In Table 1, how is the confidence interval for the Dice score computed?
- Have any experiments been done with more than 2 actors?
- How exactly is the patch sequence formed, i.e. what is the spatial order of the patches? How much to the results depend on this order, if at all?
- In the discussion on page 6, it seems to be implied that the reduced parameter count should allow more efficient application to volumetric data. This is a bit surprising, since with modern networks it is usually the input size that is limiting, not the number of network parameters.
- Have experiments with Bi-MANN and Bi-LSTM been done on the X-ray segmentation data? How do the results compare to SHAMANN?
- How does the inference and training time compare to the baseline methods?","The sentiment of the review is moderately positive. The reviewer acknowledges that the paper is easy to read, cites relevant literature, and proposes an interesting approach. However, the reviewer also points out that similar methods have been presented in the literature and that the experimental results are not compelling enough to justify the complexity of the proposed method. The reviewer suggests additional experiments to strengthen the paper. Therefore, the sentiment score is 20. The politeness of the language is high. The reviewer uses polite and constructive language throughout the review, providing specific recommendations and questions to help the authors improve their work. Therefore, the politeness score is 80.",20,80
"The paper is easy to read and the presentation is clear, and I really appreciate this.

The authors address the very important topic of feature extraction and state representation learning. New results in this area are always valuable and welcome. However, my feeling is that the paper falls short in terms of making sufficient new contributions for an ICLR paper. 

1. The authors propose to learn a state representation by either training using a combined loss function, or training several representations using multiple loss functions followed by stacking. These are standard and well-known techniques in machine learning. The key contribution one looks for is in terms of new insights on why and when each approach works. The paper fails to provide much insight in this regard. Take this simple scenario: Suppose my input image is actually generated by a linear map plus gaussian noise on the true states. Then I can simply use a PCA as my ""auto encoder"" and happily learn a high quality state representation close to the ground truth. We know why this works. In the real task, the image is a complex non-linear transformation of the true states. What insights do I gain from this work in terms of how I should tackle this?

2. Section 3 states some desirable characteristics in constructing a state representation. These are well-known and fundamental aspects of machine learning -- applicable to almost all models that we want to learn. In this sense, I do not find the section very informative.

3. The empirical results (say, Table 1) seem too noisy to interpret (other than that using the ground truth provides the best performance). It almost seems to suggest that one should simply use random features (as done in the ""extreme learning machine"" approach). Again, not much insight to draw from this.

4. Last comment. Suppose I have a new robotic goal-directed task and my inputs are camera images. Does this work tell me something that I don't already know in terms of learning new feature representation that is highly suitable for my task?



","The sentiment score is determined by the initial positive remarks about the paper's readability and the importance of the topic, but it is tempered by the subsequent critical feedback on the lack of new contributions and insights. Therefore, the sentiment score is slightly negative. The politeness score is high because the reviewer uses polite language throughout, such as 'I really appreciate this' and 'my feeling is,' and provides constructive criticism without being rude.",-20,80
"The reviewer finds that the proposed method interesting. The model is very clean, and the implication in causal inference is significant. The writing is also clean and clear. The reviewer has several concerns:

1) the algorithm seems not very scalable. In the two subproblems, there is one solved by a large number of parallel SDRs. SDR is quite expensive, and for each column in the data matrix one has to solve an SDR in each iteration. This is too much for large scale recommender systems. In fact, in the experiment 1 on MovieLens, the algorithm was only tested on a not-so-large dataset and run 5 iterations. The reviewer feels that more scenarios should be tested (e.g., more iterations, various sizes of dataset, etc.). Fixing the number of iterations also sounds a bit funny since it is more intuitive to stop the algorithm using some validation set or when the algorithm converges under a certain criterion.

2) The algorithm works with *probability* of binary data. This is quite hard to estimate in practice. For example, people ``'likes'' a movie for only once. It is hard to tell what is the probability of generating this ````""like"". It seems that the experiment part of this paper did not clearly state how to obtain the probability that the algorithm needs.

3) The proposed method is a special nonnegative matrix factorization, which could be unidentifiable. How to circumvent such situation? Since identifiability of NMF affects interpretability a lot.
 ","The sentiment of the review is generally positive, as the reviewer finds the proposed method interesting, the model clean, and the implications significant. However, the reviewer also raises several concerns about scalability, practical implementation, and identifiability, which slightly temper the overall positive sentiment. Therefore, the sentiment score is 50. The language used in the review is polite and constructive, with the reviewer providing specific feedback and suggestions for improvement without being rude or dismissive. Thus, the politeness score is 80.",50,80
"The paper proposes a new method for anomaly detection using deep learning. It works as follows. 

The method is based on the recent Multiple-Hypotheses predictions (MHP) model, the impact of which is yet unclear/questionable. The idea in MHP is to represent the data using multiple models. Depending on the part of the space where an instance falls, a different model is active. In this paper this is realized using VAEs. The details are unclear (the paper is poorly written and lacks some detailed explainations), but I am assuming that for each hypothesis (ie region of the space) different en- and decoder parameters are learned (sharing the same variational prior??). The authors mention that below this final layer all hypothesis share the same network parameters. An adversarial loss is added to the model (how that is done is not described; the relevant equation (5) uses L_hyp which is not defined) to avoid the mode collapse.

What is interesting about the paper:
- First of all, pushing the the MHP framework towards AD could be relevant by its own right for a very small subcommunity that is interested in this method
- The idea of using the adv loss for avoiding mode collapse can be useful in other settings; this is def a that I learned from the paper
- The method might actually work rather well in practice

Votum. As outlined above, the paper makes some rather interesting points, but is not well written and lacks some details. I am not entirely convinced that AD and MHP is a killer combination, but the experimental results are ok, nothing to complain here (except the usual bla: make it larger, more, etc), but honestly they really fine (maybe compare also again against more related work, e.g., Ruff et al ICML 2018).","The sentiment of the review is mixed. The reviewer acknowledges some interesting points and potential usefulness of the method, but also highlights significant issues with the clarity and detail of the paper. Therefore, the sentiment score is slightly positive. The politeness of the language is generally neutral to slightly polite. The reviewer uses some polite phrases like 'interesting points' and 'nothing to complain here,' but also includes more critical language such as 'poorly written' and 'lacks some detailed explanations.' Therefore, the politeness score is slightly positive.",20,10
"This paper introduces actions as a co-predictor of next-states and the predicted (from current and next state) in the context of (model-based) RL. In addition they incorporate the idea of using a JSD-based objective do prediction (as the Deep InfoMax paper), which is novel to RL. The enforce a linear structure between current / next states and actions with an additional sparse nonlinear term computed from both current states and actions. From this, they are able to quantify the amount of novelty in the representation space as a measure of exploration, which can be used as an intrinsic reward.

I found the paper to be very well-written and easy to understand. The prediction part is similar to that used in CPC structurally, except they include the action in two different prediction tasks and they have some built-in intrinsic rewards, which is good.

I had some issues with the motivations of some of the loss functions. 
- The JSD-based objective makes sense, but I don't think it's correct to call it an ""approximation"" to the KL (this is only true where the log-ratio of the joint and the product of marginals is small). Rather, it would be better to describe this choice as simply using a different measure between the joint and marginals.
- It seems like the best motivation for having linear relations is you can do multiple predictions using the same state / action encodings.
- For measuring exploration (11) couldn't one just use the predictor models T? How does the output of T (perhaps correctly normalized with the marginals) correlate with (11)?

Other notes:
Page 2:
Figure 1 is awfully confusing. Could this be clarified a little bit? I’m not sure what the small dots or their colors are supposed to represent.

Could diversity also be added by adding a prior to the state representations (as is done in Deep InfoMax)?

Why were the vision experiments stopped at 500 x 100k (500 million) frames?  I can’t validate the SOTA claims, but it seems like the model is still improving: are there’s further experiments?

An ablation study would be nice comparing the different hyper parameters (intrinsic rewards, diversity, etc).","The sentiment of the review is generally positive, as the reviewer found the paper to be well-written and easy to understand, and appreciated the novel aspects introduced. However, there are some critical points regarding the motivations of certain loss functions and the clarity of Figure 1. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, offering suggestions and asking clarifying questions in a respectful manner.",70,90
"The paper proposes a meta algorithm to train a network with noisy labels.
It is not a general algorithm but a simple modification of two proposed methods.  It is presented as a heuristics and it  would be helpful to derive a theoretical framework or motivation for the proposed algorithm. 

My main concern is related to the experiment results. The results of the baseline method look strange. Why there is a strong decrease in the MNIST test accuracy after 20 epochs? Standard training of neural network is very robust to label noise.  In case of  20% symmetric error  (figure 2c)  the performance degradation using standard training should be very small. 
Hence it is difficult to evaluate to performance of the proposed method.
At the beginning of the experiment section you mentioned several algorithms   for training with noisy labels.  I expect to compare your results to at least one of them. 
    
","The sentiment of the review is slightly negative, as it points out significant concerns with the experimental results and the lack of theoretical framework for the proposed algorithm. The reviewer questions the validity of the baseline results and suggests that the proposed method's performance is difficult to evaluate. The politeness of the language is relatively high, as the reviewer uses polite phrases such as 'It would be helpful' and 'I expect,' and does not use any harsh or rude language.",-30,70
"Summary:
This paper effectively learns a variant of a Deep Value Network (Gygli et al 2017), a model consisting of an energy network that assigns scores to input-output tuples that is trained to mimic a task-specific loss. The primary differences between the model presented in this work (titled LDRSP) and DVNs are twofold: first, the initial label prediction used at test time for inference is the output of a model rather than being initialized to all zeros. Second, a GAN-inspired loss is used to train both the scoring function and the initial prediction estimator. This new setup is compared against a variety of recent structured prediction methods on the tasks of multilabel classification, semantic segmentation, and 3-class face segmentation.

Comments:
I think the ideas presented in this paper are interesting, but I think their presentation could be a bit clearer. As mentioned in the summary, what you’re presenting is still more or less a deep-value network with some additions - however, you don’t refer to it as such in the body of the paper anywhere I saw. The first addition is the use of a learned model to produce the initial prediction; this is a natural extension to Deep Value Networks, and on its own is somewhat incremental in nature. I do not think you adequately explained why you chose to use a GAN-like loss to learn these models. Another baseline that would have helped justify its use would be to train your G model to predict structured outputs in the standard way (max-margin or cross-entropy loss) and then train your energy function in the DVN way. 

The experimental settings are somewhat small in scope but follow the precedent set by previous structured prediction papers, which is fine. You make appropriate comparisons against previous structured prediction models as well as against different types of GAN-like losses. But, as I mentioned before, I think you needed to have more comparisons against different ways of training these networks that do not follow a GAN-inspired framework. 

Overall, I like the new ideas in this paper but I think a few more experimental settings are required before they should be published.

=== after rebuttal ===

I appreciate the response, but I still think further analysis of the model is needed to understand where the gains in performance are coming from. The claim is that this is due to the adversarial loss used, but without further ablations I feel this is too strong a claim to be making given the current evidence.
","The sentiment of the review is moderately positive. The reviewer acknowledges the interesting ideas and appropriate comparisons made in the paper but also points out areas needing improvement, such as clarity in presentation and additional experimental settings. The sentiment score is therefore 40. The politeness of the language is high; the reviewer uses phrases like 'I think,' 'I appreciate,' and 'I like,' which are considerate and constructive. The politeness score is 80.",40,80
"Summary: Authors proposed a model for input method for mobile or desktop devices. The goal is to convert the input sequence (from one language to another) or predict the next word. Their model is based on an LSTM with modified softmax activation function that is adjustable for large vocabulary sizes. They showed experimental results on Japanese BCCWJ data set.

Clarity: Paper is well-written and well-organized. Notions and methods are clearly expressed. 

Originality: This paper builds on an LSTM model without enough work or idea to show novelty. 

Significance: It is below average. Using LSTM is a well-known method for these types of tasks in the literature. Incremental selective softmax is potentially a good approach, however, this work lacks showing significant improvement. The experiments are limited and are done only on one data set.

More detailed comments:

- My concerns about this work are both on modeling aspects and experiments. Authors mainly focus on highlighting the benefits comparing to n-gram models, and briefly discuss the ongoing developments in neural based models. For example sequential modelings using RNN's have shown promising results in capturing long-term dependencies [1]. Unfortunately authors did not include any discussion on how their approach would compare to that framework nor did they present any experimental comparisons to them.

- Although mentioned briefly in the introduction and related work sections, no analytical or experimental comparisons are made to machine translation approaches when their work is closely related to it. I strongly suggest that authors compare their experimental results to some of benchmarks in neural based machine translation discussed in the related works.

- In the incremental selection softmax, they use ""match"" to return all lexicon items matching the partial sequence. How is this done and what are the effects of it on the computational time of the algorithm? Also, It is not clear how authors correct old probabilities in IS softmax step. As mentioned, they add logits of missing vocabulary to the denominators, how do they keep the properties of softmax so that it sums up to 1? And later in the discussion authors mentioned that in practice they compute union of all missing vocabularies, it is not clear how this is done since the advantage of using IS softmax is expressed to be incremental increasing. 

[1] A.B. Dieng, C. Wang, J. Gao and J. Paisley. TopicRNN: A recurrent neural network with long-range semantic dependency, International Conference on Learning Representations (ICLR), 2017.","The sentiment of the review is mixed but leans towards the negative side. The reviewer acknowledges that the paper is well-written and organized, which is positive. However, the reviewer criticizes the originality and significance of the work, stating that it lacks novelty and significant improvement over existing methods. The detailed comments further highlight several concerns and suggestions for improvement. Therefore, the sentiment score is -40. The politeness of the language is quite high. The reviewer uses polite language, such as 'I strongly suggest' and 'It is not clear,' and provides constructive feedback without being rude or dismissive. Therefore, the politeness score is 80.",-40,80
"This manuscript extends the direct feedback alignment (DFA) approach to convolutional neural networks (CNN) by (1) only applying DFA to FC layers with backpropagation (BP) in place for convolutional layers (2) using binary numbers for feedback matrix.

Originality wise, I think (1) is a very straightforward extension to the original DFA approach by just applying DFA to places where it works. It still does not solve the ineffectiveness of DFA on convolutional layers. And there is no much insight obtained. (2) is interesting in that a binary matrix is sufficient to get good performance empirically. This would indeed save memory bandwidth and storage. This falls into the category of quantization or binarization, which is not super novel in the area of model compression. 

The experimental results show that the proposed approach is better than BP based on accuracy. However, these results might be called into question because the shown accuracies on CIFAR10 and CIFAR100 are not state-of-the-art results. For example, the top 1 accuracy of CIFAR10 in this paper 81.11%. But with proper tuning, a CNN should be able to get more than 90% accuracy. See this page for more details.
http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html
Therefore, though the claimed accuracy of the proposed method is 89%, it is still not the state-of-the-art result and it seems to be lack of tuning for the BP approach to perform similar level of accuracy. The same conclusion applies to CIFAR100. In fact, from figure 4, the training accuracy gets 100% while the testing accuracy is around 40% for BP, which seems to be overfitting. With these results, it is hard to judge the significance of the manuscript.

Minor typos:
In Equation 1, the letter i is overloaded.","The sentiment of the review is mixed, leaning towards negative. The reviewer acknowledges some interesting aspects of the manuscript, such as the use of a binary matrix for feedback, but overall finds the contributions to be lacking in novelty and significance. The reviewer also questions the validity of the experimental results due to subpar accuracy compared to state-of-the-art methods. Therefore, the sentiment score is -40. The politeness of the language is generally respectful and constructive, providing specific feedback and suggestions without being overly harsh or dismissive. Thus, the politeness score is 60.",-40,60
"The privacy definition employed in this work is problematic. The authors claim that ""Privacy can be quantified by the difficulty of reconstructing raw data via a generative model"". This is not justified sufficiently. Why larger reconstruction error achieves stronger privacy protection? I could not find any formal relationship between reconstruction error and privacy. 

The proposed method is not appropriately compared with the other methods in experiments.  In Fig. 3 the author claim that the proposed method dominates the other methods in terms of privacy and utility but this is not correct. At the specific point that the proposed method is evaluated with MNIST and Sound, it achieves better utility and better ""privacy"". However, the Pareto front of the proposed method is concentrated on a specific point. For example, the proposed method does not achieve high ""privacy"" as ""noisy"" does. In this sense, the proposed method is not comparable with ""noisy"". In my understanding, this concentration occurs because the range of \lambda is inappropriately set. This kind of regularization parameter should be exponentially varied so that the privacy-utility Pareto front covers a wide range. 

--
Minor:
In Eq. 1, the utility is evaluated as the probability Yi=Yi'. What randomness is considered in this probability?
In Eq 2, privacy is defined as maxmin of |Ii - Ii'|. Do you mean privacy guaranteed by the proposed method is different for each data? This should be defined as expectation over T or max over T. 

In page 4. ""The reason we choose this specific architecture is that an exactly reversed mode is intuitively the mode powerful adversarial against the Encoder."" I could not find any justification for this setting. Why ""exactly reversed mode"" can be the most powerful adversary? What is an exactly reversed mode?

Minimization of Eq. 3 and Eq. 4 contradict each other and the objective function does not converge obviously. The resulting model would thus be highly affected by the setting of n and k.  How can you choose k and n?","The review is critical of the work, pointing out several significant issues with the privacy definition, experimental comparisons, and parameter settings. The sentiment is negative as the reviewer finds multiple flaws without acknowledging any strengths. The language used is direct and critical but not rude, maintaining a professional tone throughout.",-70,20
"In the paper, WGAN with a squared zero centered gradient penalty term w.r.t. to a general measure is studied. Under strong assumptions, local stability of a time-continuous gradient ascent/descent dynamical system near an equilibrium point are proven for the new GP term. Experiments show comparable results to the original WGAN-GP formulation w.r.t. FID and inception score.

Overall, I vote for rejecting the paper due to the following reasons:
- The proven convergence theorem is for a time-continuous ""full-batch"" dynamical system, which is very far from what happens in practice (stochastic + time discrete optimization with momentum etc). I don't believe that one can make any conclusions about what is actually happening for GANs from such an idealized setting. Overall, I don't understand why I should care about local stability of that dynamical system.
- Given the previous point I feel the authors draw too strong conclusions from their results. I don't think Theorem 1 gives too many insights about the success of gradient penalty terms.
- There are only marginal improvements in practice over WGAN-GP when using other penalty measures. 

Further remarks:
- In the introduction it is claimed that mode collapse is due to JS divergence and ""low-dimensionality of the data manifold"". This is just a conjecture and the statement should be made more weak.

- The preliminaries on measure theory are unnecessarily complicated (e.g. partly developed in general metric spaces). I suggest that the authors try to simplify the presentation for the considered case of R^n and avoid unnecessarily complicated (""mathy"") definitions as they distract from the actual results. 

==after rebuttal==
After reading the authors rebuttal I increased the my rating to 6 as they addressed some of my doubts. I still think that the studied setting is too idealized, but it is a first step towards an analysis.","The sentiment of the review is negative, as the reviewer votes for rejecting the paper and provides several critical points regarding the theoretical and practical contributions of the work. The sentiment score is -60 because the reviewer acknowledges some merit in the rebuttal but still maintains a largely negative stance. The politeness score is 20, as the reviewer uses polite language and provides constructive feedback, even though the overall sentiment is negative. The reviewer avoids rude or harsh language and offers specific suggestions for improvement.",-60,20
"The paper presents an improvement on the previous work by [Neyshabur et el, ICLR 2018].
More precisely, an emprical generalization bound is provided by using PAC-Bayesian empirical 
bounds. To obtain the claimed improvement over the works [Barlett et al, NIPS 2017] and 
[Neyshabur et el, ICLR 2018], the authors have paid attention carefully (by putting some 
conditions) on the change of the layers (layer and interlayer cushion) as well as the activation 
contraction. It is also worth noting that the paper is using a differrent loss function comparing 
to [Neyshabur et el, ICLR 2018], which the author called Optimal Margin Distribution Loss.
Although the results seem interesting, the analysis is not convincible for me.
A plus point is that the paper presents interesting numerical experiments showing the promising of the approach.

Major comments:
1) The statement of the Theorem 1 is not clear: 
is it just under the assumptions of the lemmas
or is it under all definitions and lemmas?
2) The proof of Theorem 1 is not clear:
 how do you get the inequality (5)?
how do you get an upper bound on the KL divergence?
 This is not trivial for me!
3) What is \rho in Theorem 1 and in Definition 2?
4) Your remark after Theorem 1 is not clear for me.
  you claim that the product is (3) is large, what if we restrict all the spectral norms equal to 1?
 a simple counter example would fit better the explanation here, I guest.

Minor comments:
1) The Lemma 1 and 2 are almost the same to Lemma 1 and 2 in [Neyshabur et el, ICLR 2018]
without precisely citations. I wonder how do you obtain your Lemma 1?
2) page3, after formula (1), your loss will first DECREASING, not ""increasing"".
Check the sentence ""Fig. 1 shows, equation 1 will produce a linear loss increasing progressively with the margin distance....""
","The sentiment of the review is mixed. The reviewer acknowledges the interesting results and numerical experiments, which is positive, but also expresses significant doubts about the clarity and validity of the analysis, which is negative. Therefore, the sentiment score is around -20. The politeness of the language is generally neutral to slightly polite. The reviewer uses phrases like 'not clear for me' and 'I wonder,' which are polite ways to express confusion or disagreement. Therefore, the politeness score is around 20.",-20,20
"This paper studies the generalization properties of a two layer neural network for a nonlinear regression problem where the target function has a finite spectral norm. The generalization bound comprises of an approximation term (dependent on the width) and an estimation term (dependent on spectral norm of the target and scaling as 1/sqrt{n}). 

The key contribution is the derivation of the generalization bound where the estimation term depends on the properties of the target function rather than properties of the class of two layer neural networks. These bounds are instantiated for a class of regularized estimators (with path norm regularization).

1. Theoretical Novelty: While both Theorem 3 (approximation) and Theorem 4 (a posterior generalization) were mostly following known results, the key development seems to be the bound on the path norm of the regularized solution in terms of the spectral norm of the target function. Given that the estimator is a path-norm regularized estimator, this seemed to be an incremental contribution. What would be more interesting is to obtain such a bound for an unregularized estimator: either saying something about the optimization procedure or relating this kind of regularization to properties of the dataset over which it is trained.

2. Regression vs Classification: While the focus of the paper is on a regression problem, the experiments and problem motivation seems to arise from a classification setting. This creates a mismatch between the what the paper is about and the problem that has been motivated. Would it be possible to extend these results to loss functions (other than squared loss ) like cross-entropy loss or hinge loss which indeed work in the classification setting?

3. Comparison with Klusowski & Barron (2016): In the comparison section, it is mentioned that  Klusowski & Barron (2016) analyze a ""similar"" problem and obtain worse generalization bounds. It would be important to know the exact setting in which they obtained their bounds and how do their assumptions compare with the ones made in this paper. The comparison seems incomplete without this.

4. The experiments showcase that the regularized estimator has a better path norm (and expectedly so) but almost similar (in case of MNIST actually better) test accuracy. This defeats the purpose of  showing that the regularized estimator has better generalization ability which is claimed in the introduction as well as the experiment section (calling it ""well-posed""). What this indeed shows is that even though the path norm might be big, the generalization of the estimator is till very good contradicting the statements made. 

5. The numbers shown in Figure 1 and the numbers reported in Table 2 do no match: while the plot shows that the scaled path norm is around 60 for both MNIST and CIFAR-10, the corresponding numbers in the table are 507 and 162. Can you please point out the reason for this discrepancy?

6. Theorem 5 seems to suggest that in the noiseless case, the estimation error would scale as the spectral norm of f^*. Rather, in the noiseless setting, it seems that the correct scaling of the generalization error should be with respect to properties of the regularized estimator and the function class. Even though the spectral norm can be arbitrarily high, the generalization bound should only be dependent on the complexity of functions which can fit the current data well. It would be good to have a comment in the draft on why the current dependence is a better thing and examples where such generalization bounds are indeed better. ","The sentiment of the review is moderately positive, as it acknowledges the key contributions and theoretical novelty of the paper, but also points out several areas for improvement and clarification. The sentiment score is 30. The language used in the review is polite and constructive, offering specific recommendations without being harsh or dismissive. The politeness score is 80.",30,80
"Paper summary: 

As is made clear in the title, this paper sets out to answer the following question: “Why do deep convolutional networks generalize so poorly to small image transformations?”. It focuses on natural image transformations on translation and scaling (rotation is missing though).

The paper proposes two main explanations: 
-	Strided convolution, called subsampling in the paper, ignores the classical sampling theorem,
-	CNNs will not learn invariance because of the (photographers') biases contained in the datasets.

On a general level, the paper is a good read, it is well written and the figures clearly convey the message they’re intended to. Adversarial attacks and robustness of CNNs in general is a very interesting and important topic in ML. The originality of this work is in the approach of the problem, the paper tries to explain the reasons why CNNs are vulnerable. Related works put more emphasis on coming up with novel attacks/defense strategies. Considering natural attacks as done in this submission is particularly interesting as it is probably a more surprising shortcoming of CNNs compared to optimally designed attacks or highly unnatural perturbations. The argument about subsampling (stride) being the reason of not having translational invariance is nice, especially the theoretical insight with the Shannon-Nyquist theorem and the more figurative example on part detectors. There are nevertheless a few major concerns about this work:

Major Concerns:

Theoretical arguments:
The theoretical argument made in this paper is interesting but to make the point stronger a more in-depth explanation would be needed.
-	The step from Eq (2) to Eq (3) is not entirely clear “K does not depend on x_i”, maybe one extra sentence to explain this step would be useful. 
-	Terms introduced such as the basis function B and the set of transformations T could be better defined.
-	For the extension to other types of transformations “While the claim focuses on global translation, it can also be extended to piecewise constant transformations.” it would be important to point out what type of natural transformations can be included in this set.

Empirical evidence:
Experiments are not fully convincing. Additional empirical evidence would be beneficial and necessary to support the claims of this:
-	“A natural criticism of these results is that they are somehow related to the image resizing and inpainting procedures that we used.” This is a very good point and the authors following arguments are not fully convincing. Results with different transformation procedures mentioned in the rest of the paragraph (and probably more) should be included to convince the reader.
-	The theoretical argument that translation invariance is not guaranteed because of the stride (subsampling) is not fully convincing and needs further explanation and experimental verification. In fact, feature maps of the CNNs that the authors consider do indeed contain many high frequencies.
-	The argument made in part 4 about the photographer’s bias seems valid for general natural transformations, but it does not apply to small transformations such as 1-pixel translations presented in the paper. Also, evidence that datasets without (or less) photographers' bias are less susceptible to natural attacks would make the argument in the paper a lot stronger. 
-	When using 6x6 avg pooling for the VGG16 architecture ”recognition performance decreases somewhat” . Results are only preliminary in the paper, but this statement needs a more thorough experimental backing. It should come with convincing quantitative evidence.
-	Please include some results or citation on other work about test time augmentation to support the statement “still only provides partial invariance”.

References and phrasing:
Generally previous work is well referenced in this paper, although there are some formulations that can be slightly modified to make a clear distinction between what is novel and what is previous work:
-	As is very well shown in the introduction, there is a lot of work on generating adversarial examples that drastically change the output of a CNN. This should be made clear in the abstract, in fact the sentence “In this paper we show that modern CNNs [...] also happens with other realistic small image transformations”  seems to indicate that this is the novel work in the paper. This is also why I believe the first sentence “Deep convolutional network architectures are often assumed to guarantee generalization for small image translations and deformations.” is somewhat contestable. 
-	“We find that modern deep CNNs are not invariant to translations, scalings and other realistic image transformations” as the paper points out earlier this is not a novel finding, so I would use a formulation that makes that clear and gives more emphasis to your own arguments as of why this happens.

Further Comments :
-	Part 5 ""Implications for Practical Systems"" could be moved to discussion as there is no new point and it seems more a reflection on what was already stated.
-	The final sentence of the abstract “Taken together our results suggest that the performance of CNNs in object recognition falls far short of the generalization capabilities of humans.” is not necessary, this is clearly true but it isn’t really contested in the ML community.
-	“despite the architecture being explicitly designed to provide such invariances” I agree that this has motivated the use and design of CNNs in the first place, but modern architectures are mostly designed to surpass the results on the common benchmarks rather than to provide such invariances.
-	”jaggedness is greater for the modern, deeper, networks compared to the less modern VGG16 network” might be worth interesting to consider if the residuals have anything to do with it.
","The sentiment of the review is generally positive, as indicated by phrases like 'the paper is a good read,' 'well written,' and 'figures clearly convey the message.' However, the reviewer also points out several major concerns and areas for improvement, which tempers the overall positivity. Therefore, the sentiment score is 50. The politeness of the language is high, as the reviewer uses polite and constructive language throughout, such as 'it would be important to point out,' 'please include,' and 'might be worth interesting to consider.' Thus, the politeness score is 90.",50,90
"The paper talks about a method to combine preconditioning at the per feature level and Nesterov-like acceleration for SGD optimization.

The explanation of the method in Section 3 should be self-contained.  The main result, computational context, etc., are poorly described, so that it would not be easily understandable to a non-expert.

What was the reason for the choice of the mini batch size of 128.  I would guess that you would actually see interesting differences for the method by varying this parameter.

How does this compare with the FLAG method of Chen et al from AISTATS, which is motivated by similar issues and addresses similar concerns, obtaining stronger results as far as I can tell?

The figures and captions and inserts are extremely hard to read, so much so that I have to trust it when the authors tell me that their results are better.

The empirical evaluation for ""convex problems"" is for LS regression.  Hmm.  Is there not a better convex problem that can be used to illustrate the strength and weaknesses of the method.  If not, why don't you compare to a state-of-the-art least squares solver.

For the empirical results, what looks particularly interesting is some tradeoffs, e.g, a slower initial convergence, that are shown.  Given the limited scope of the empirical evaluations, it's difficult to tell whether there is much to argue for the method.  But those tradeoffs are seen in other contexts, e.g., with subsampled second order methods, and it would be good to understand those tradeoffs, since that might point to where and if a methods such as this is useful.

The conclusions in the conclusion are overly broad.","The sentiment of the review is generally negative, as it highlights several significant issues with the paper, such as poor explanation of the method, unclear figures, and overly broad conclusions. The reviewer also questions the choice of certain parameters and the empirical evaluation methods. The politeness of the language is relatively neutral to slightly polite. While the reviewer points out many flaws, they do so in a professional manner without using harsh or rude language.",-60,20
"This paper studied the Donsker-Varadhan lower bound of KL-divergence. The authors show that with high probability, the DV lower bound is upper bounded by log of the sample size, so if the true KL-divergence is very large, then exponential sample size is needed to make the DV lower bound tight. The same argument holds true for any distribution-free high-confidence lower bound (such as DV lower bound) for KL divergence. Then the authors proposed to use an upper bound for entropy instead of lower bound for mutual information. 

The idea of the paper is interesting and the proof of Theorems 1 and 2 are valid. Especially I like the idea of Theorem 1, which proves that any distribution-free high-confidence lower bound for KL divergence is upper bounded by log of sample size. This idea is similar to the paper in (Gao et al 15') which shows that mutual information estimator is upper bounded by log(N).

However, this paper contains many fatal flaws, which significantly weaken the quality of this paper. Precisely,

1. The DV lower bound is just an alternative of mutual information, helping MMI predictive coding algorithm to find good coding functions C_x and C_y. The goal of MMI predictive coding is not to estimate the mutual information I(C_x(x), C_y(y)) precisely, instead, the goal is to find good coding functions. The fact that DV lower bound is small means that we can not estimate mutual information through DV lower bound, but it does not directly imply that we can not find the coding functions. I expect some experiments to show that when mutual information is large, MMI predictive coding using DV lower bound can not find good coding functions.

2. In Section 3, the formula after the proof of Outlier Risk Lemma and before Theorem 1 (btw, it is better to have numbers for these formula) seems to be problematic. The first formula shows that E_{z~q} e^{F(z)} >= (1/N)e^{F_max}, then we plug it in (4). But in (4) there is negative ln of E_{z~q} e^{F(z)}, so we should have KL(P,Q) <= something, correct? This may be a typo but this typo is so important such that it affect the readability a lot. Theorem 1 is correct but the paragraphs before Theorem 1 confuse the reader a lot.

3. In Section 4, are you considering classical entropy for discrete random variables, or differential entropy for continuous random variables? I assume you are considering the latter, since most people of machine learning community are interested in continuous random variables. Then your statement of I(X;Y) <= H(X) is incorrect, since for continuous random variables, H(X|Y) can be negative. See (Thomas & Cover, Chapter 8) for a reference.

4. Related to problem 3, if you are considering continuous random variables, then the statement I(X;Y)=H(X)+H(Y)-H(X,Y) is not always correct. There are cases that H(X) is infinite, H(Y) is infinite, H(X,Y) is infinite but I(X;Y) is finite. These cases does not only exist in mathematical books, but also exists in practice, especially when the data is located on a low-dimensional manifold embedded in a high-dimensional space. Therefore, your approach of decompose the mutual information is not always possible.

5. Regarding to your proposed optimization problem in Section 6 (also it is better to have a number), I have some concerns. Since it involves max over \Psi outside, and inf over \Theta and inf over \Phi inside, so I wonder how do you solve this problem? Can you guarantee that the solution can provide good coding functions C_x and C_y? Also, it seems that this optimization problem is proposed as an improvement over the DV lower bound method, so I wish to see some experiment showing that this method is better than the DV lower bound method, at least for some synthetic datasets.

Because of the above mentioned flaws (especially 3 and 4, and lack of experiments), I think the paper is below the standard of ICLR conference.

References:
[1] Efficient Estimation of Mutual Information for Strongly Dependent Variables, by Gao, Ver Steeg and Galstyan, AISTATS15'
[2] Elements of Information Theory, 2nd edition, by Thomas and Cover.



","The sentiment of the review is mixed. The reviewer acknowledges the interesting idea and the validity of the proofs, which is positive. However, the review also points out several significant flaws and issues, which are critical and negative. Therefore, the sentiment score is slightly negative. The language used is generally polite, with phrases like 'I expect,' 'it is better to,' and 'I have some concerns,' indicating a respectful tone despite the critical feedback.",-20,60
"In the paper, the authors combine the federated method, sparse compression, quantization and propose Sparse Binary Compression method for deep learning optimization.  Beyond previous methods, the method in this paper achieves excellent results in the experiments. The paper is written very clearly and easy to follow. 

The following are my concerns,
1. In the introduction, the authors emphasize that there is a huge compression in the upstream communication. How about the downstream communication, I think the server should also send gradients to clients. The averaged gradient is not compressed anymore. 

2. I think the method used in the paper is not federated learning. Federated learning averages the models from multiple clients. however, in the paper, the proposed methods are averaging gradients instead. It is called local updates, and is a well-known tradeoff between communication and computation in the convex optimization.

3. I want to point out that the similar local update (federated learning) technique has already explored, and proved not work well. In [1] the authors showed that deploying the local update simply may lead to divergence. Therefore, the iterations of the local update are constrained to be very small. e.g. less than 64.  Otherwise, it leads to divergence. I also got similar results in my experience.  The temporal sparsity in the paper looks very small. I am curious about why it works in this paper.

4. Another issue is the results in the experiments. It is easy to find out that resnet50 can get 76.2% on Imagenet according to [2]. However, the baseline is 73.7% in the paper.  I didn't check the result for resnet18 on cifar10 or resnet34 on cifar 100, because people usually don't use bottleneck block for cifars.

5. In Table 2, Federated average always has worse results than other compared methods. Could you explain the reason?  If using federated average is harmful to the accuracy, it should also affect the result of the proposed method. 


[1] Zhang, Sixin, Anna E. Choromanska, and Yann LeCun. ""Deep learning with elastic averaging SGD."" Advances in Neural Information Processing Systems. 2015
[2]https://github.com/D-X-Y/ResNeXt-DenseNet","The sentiment of the review is generally positive, as the reviewer acknowledges the clarity and ease of following the paper, as well as the excellent results achieved in the experiments. However, the reviewer also raises several concerns and questions about the methodology and results, which slightly temper the overall positive sentiment. Therefore, the sentiment score is 60. The politeness of the language used in the review is quite high. The reviewer uses polite phrases such as 'I think,' 'I want to point out,' and 'could you explain,' which indicate a respectful and constructive tone. Therefore, the politeness score is 80.",60,80
"This paper studies the problem of making predictions with a model trained using dropout. Authors try to provide a theoretical foundation for using dropout when making predictions. For this purpose, they show that when using dropout training we are maximizing a common lower bound on the objectives of a family of models, including most of the previously used methods for prediction with dropout. 

I find that the paper addresses a relevant problem and try to apply a novel approach. But, in general, I find the paper is not easy to follow and to grasp the main ideas. 

Here I detail my main concerns:


1. This is one of my main concerns. The contraposition between the geometric and the average model. I don't like this contraposition. The average model is just the standard marginalization operation over the weights, $p(y|x) = \int p(y|x,w)p(w|\Theta)dw$. This is the natural solution for the prediction problem to the problem if we accept the generative model given in Eq (3). 

In the case of the variational dropout, we depart from the same generative model, but we employ an approximation. It is the variational approximation the one that induces the geometric mean provided in eq (6). I.e. if we want to compute the posterior over the label y* for a sample x*, after training, we should compute the associated lower bound
$\ln p(y*|x*) >= E_q[\ln p(y*|x*,w)] - KL(q|p)$
In this case, q(w) = p(w|\Theta), as stated Eq (3) and in the corresponding equation provided in page 2 (the q(w) is not learnt because it only depends on the dropout rate, while the $\Theta$ are learnt by maximum log-likelihood and do not have a $q$ associated).  This gives rise to the geometric mean approximation provided in Eq (6).  I.e. the geometric mean prediction is simply the result of using a variational approximation at prediction time.   

My problem here is that authors employ convoluted arguments to introduce this geometric mean prediction and the average prediction, without making the connection discussed above. 

3. Section 3.3 and 3.4 introduces new arguments for modifying the dropout rate (and the alpha) parameter at test time. But, again, I find the arguments convoluted. We consider the dropout rate a hyper-parameter of the model, the standard learning theory tells us to fix the parameters with the training data and evaluate them later when making predictions. Why should we use different dropout rates at training and testing? Authors arguments about the tightness of the bound of Eq (8) and Eq(9). are not convincing to me. 

So, I don't find authors provide convincing answers to the raised questions at the beginning of the paper about the use of dropout when making predictions. 

Minor comments:

1. The generative model for Variational dropout is the same than the generative model for the ""conditional model"", eq. (3). 

2. In Eq. (7) authors are defining the weighted power mean. I think it would be clearer to directly introduce the weighted power mean instead of the standard power mean in Section 3.2.

3. Section 3.3. I find some parts are difficult to understand. ""suppose we pick a base model from the power mean family and have a continuum of subvariants with gradually reduced variance in their predictions but the same expectation."" Later, I can understand authors are referring to the possibility of reducing the dropout rate. ","The sentiment of the review is mixed. The reviewer acknowledges that the paper addresses a relevant problem and attempts a novel approach, which is positive. However, the reviewer also expresses significant concerns about the clarity and coherence of the arguments presented, which is negative. Therefore, the sentiment score is slightly negative. The politeness of the language is generally high. The reviewer uses phrases like 'I find,' 'I don't like,' and 'I don't find convincing,' which are polite ways to express disagreement or concern. The reviewer also provides detailed feedback and suggestions, which indicates a constructive and respectful tone.",-20,80
"The paper at hand describes an approach to enable neural networks to take arbitrary structured data (basically any data that is not easily represented as a fixed-dimensional vector) as input. The paper describes how such data can be represented as a set (e.g. a sequence is represented as a set of index + data) and then an auxiliary network called the set aggregating network (SAN)  is used to represent the data in a high dimensional latent space. In addition to the idea, the paper provides a theoretical analysis of the approach which shows that with a sufficiently high dimensional representation the network is able to learn a unique representation for each input example. 

Good in this paper: 
 - nicely on topic - this is truly about learning representations
 - interesting idea with some (albeit not-surprising) theoretical analysis

Not yet great in this paper: 
 - the paper feels a bit premature in multiple ways, to me most importantly the experiments appear to be really rushed. 
  Looking at table 1 - it is really unclear how to read this. The table is hardly explained and it would be good to actually compare the method to the state of the art. I understand that the authors care more about generality here - but it's much easier to build something generic that is very far from the state of the art than to build something that is closer to the state of the art. Also - I feel it would have been interesting to allow for a more direct comparison of the SAN results with the other methods. Similarly, in Table 2 - how far away is this from the state of the art.
- the variable size image-recognition task seems a bit artificial - I believe that scaling images to a fixed size is a reasonable idea and is well understood and also something that humans can work with. Dealing with variable size images for the purpose of not having a fixed size vector seems artificial and unnecessary - in this case here specifically the images are same size to begin with. By using SAN you loose a lot of the understanding of computer vision research of the last decade (e.g. it's clear that CNNs are a good idea - with SAN - you cannot really do that anymore) - so, I feel this experiment here doesn't add anything. 

I feel these comments can probably be addressed by rethinking the experimental evaluation. At this point, I think the paper provides a nice idea with a theoretical analysis - but it doesn't provide enough experimental evidence that this works. 

Minor comments: 
 - p1: Analogically -> just drop the word
- p1: citation of Brin & Page -> this seems a bit out of place - yes, it is a method working on graphs, but it is not relevant in the context of the paper - to the best of my knowledge there are no neural networks in this. 
- p2: where Radon - >where the Radon
- p2: ""One can easily see "" -> I cannot. Please ellaborate","The sentiment of the review is mixed but leans towards positive. The reviewer acknowledges the interesting idea and theoretical analysis, but also points out significant shortcomings in the experimental evaluation and some aspects of the methodology. Therefore, the sentiment score is 20. The politeness of the language is quite high; the reviewer uses polite and constructive language, even when pointing out flaws. Therefore, the politeness score is 80.",20,80
"# Summary

This submission proposes a multi-task convolutional neural network architecture for end-to-end driving (going from an RGB image to controls) evaluated using the CARLA open source simulator. The architecture consists of an encoder and three decoders on top: two for perception (depth prediction and semantic segmentation), and one for driving controls prediction. The network is trained in a two-step supervised fashion: first training the encoder and perception decoders (using depth and semantic segmentation ground truth), second freezing the encoder and training the driving module (imitation learning on demonstrations). The network is evaluated on the standard CARLA benchmark showing better generalization performance in new driving conditions (town and weather) compared to the CARLA baselines (modular pipeline, imitation learning, RL). Qualitative results also show that failure modes are easier to interpret by looking at predicted depth maps and semantic segmentation results.


# Strengths

Simplicity of the approach: the overall architecture described above is simple (cf. Figure 1), combining the benefits of the modular and end-to-end approaches into a feed-forward CNN. The aforementioned two-stage learning algorithm is also explained clearly. Predicted depth maps and semantic segmentation results are indeed more interpretable than attention maps (as traditionally used in end-to-end driving).

Evaluation of the driving policy: the evaluation is done with actual navigation tasks using the CARLA (CoRL'18) benchmark, instead of just off-line behavior cloning accuracy (often used in end-to-end driving papers, easier to overfit to, not guaranteed to transfer to actual driving).

Simple ablative analysis: Table 2 quantifies the generalization performance benefits of pretraining and freezing the encoder on perception tasks (esp. going from 16% to 62% of completed episodes in the new town and weather dynamic navigation scenario).


# Weaknesses

## Writing

I have to start with the most obvious one. The paper is littered with typos and grammatical errors (way too many to list). For instance, the usage of ""the"" and ""a"" is almost non-existent. Overall, the paper is really hard to read and needs a thorough pass of proof-reading and editing. Also, please remove the acknowledgments section: I think it is borderline breaking the double-blind submission policy (I don't know these persons, but if I did that would be a breach of ICLR submission policy). Furthermore, I think its contents are not very professional for a submission at a top international academic venue, but that is just my opinion. 


## Novelty

This is the main weakness for me. The architecture is very close to at least the following works:
- Xu, H., Gao, Y., Yu, F. and Darrell, T., End-to-end learning of driving models from large-scale video datasets (CVPR'17): this reference is missing from the paper, whereas it is very closely related, as it also shows the benefit of a segmentation decoder on top of a shared encoder for end-to-end driving (calling it privileged training);
- Codevilla et al's Conditional Imitation Learning (ICRA'18): the only novelty in the current submission w.r.t. CIL is the addition of the depth and segmentation decoders;
- Müller, M., Dosovitskiy, A., Ghanem, B., & Koltun, V., Driving Policy Transfer via Modularity and Abstraction (CoRL'18): the architecture also uses a shared perception module and segmentation (although in a mediated way instead of auxiliary task) to show better generalization performance (including from sim to real).

Additional missing related works include:
- Kim, J. and Canny, J.F., Interpretable Learning for Self-Driving Cars by Visualizing Causal Attention (ICCV'17): uses post-hoc attention interpretation of ""black box"" end-to-end networks;
- Sauer, A., Savinov, N. and Geiger, A., Conditional Affordance Learning for Driving in Urban Environments (CoRL'18): also uses a perception module in the middle of the CIL network showing better generalization performance in CARLA (although a bit lower than the results in the current submission).
- Pomerleau, D.A., Alvinn: An autonomous land vehicle in a neural network (NIPS'89): the landmark paper for end-to-end driving with neural networks!


## Insights / significance

In light of the aforementioned prior art, I believe the claims are correct but already reported in other publications in the community (cf. references above). In particular, the proposed approach uses a lot more strongly labeled data (depth and semantic segmentation supervision in a dataset of 40,000 images) than the competing approaches mentioned above. For instance, the modular pipeline in the original CARLA paper uses only 2,500 labeled images, and I am sure its performance would be vastly improved with 40,000 images, but this is not evaluated, hence the comparison in Table 1 being unfair in my opinion. This matters because the encoder in the proposed method is frozen after training on the perception tasks, and the main point of the experiments is to convince that it results in a great (fixed) intermediate representation, which is in line with the aforementioned works doing mediated perception for driving.

The fine-tuning experiments are also confirming what is know in the litterature, namely that simple fine-tuning can lead to catastrophic forgetting (Table 3).

Finally, the qualitative evaluation of failure cases (5.3) leads to a trivial conclusion: a modular approach is indeed more interpretable than an end-to-end one. This is actually by design and the main advocated benefit of modular approaches: failure in the downstream perception module yields failure in the upstream driving module that builds on top of it. As the perception module is, by design, outputting a human interpretable representation (e.g., a semantic segmentation map), then this leads to better interpretation overall.


## Reproducibility

There are not enough details in section 3.1 about the deep net architecture to enable re-implementation (""structure similar to SegNet"", no detailed description of the number of layers, non-linearities, number of channels, etc).

Will the authors release the perception training dataset collected in CARLA described in Section 4.2?



# Recommendation

Although the results of the proposed multi-task network on the CARLA driving benchmark are good, it is probably due to using almost two orders of magnitude more labeled data for semantic segmentation and depth prediction than prior works (which is only practical because the experiments are done in simulation). Prior work has confirmed that combining perception tasks like semantic segmentation with end-to-end driving networks yield better performance, including using a strongly related approach (Xu et al). In addition to the lack of novelty or new insights, the writing needs serious attention.

For these reasons, I believe this paper is not suitable for publication at ICLR.","The sentiment of the review is mixed but leans towards negative. The reviewer acknowledges the strengths of the paper, such as the simplicity of the approach and the evaluation of the driving policy. However, the weaknesses, particularly in writing quality and novelty, are emphasized more strongly. The sentiment score is therefore -40. The politeness of the language is generally maintained, with the reviewer providing constructive criticism and suggestions for improvement without being rude. The politeness score is 60.",-40,60
"The paper presents a VAE that uses labels to separate the learned representation into an invariant and a covariant part. The method is validated using experiments on the MNIST dataset.

The writing in this paper is somewhat problematic. Although it is hard to put the finger on a particularly severe instance, the paper is filled with vague and hyperbolic statements. Words like ""efficiently"", ""meaningful"", ""natural"", etc. are sprinkled throughout to confer a positive connotation, often without having a specific meaning in their context or adding any information. Where the meaning is somewhat clear, the claims are often not supported by evidence. Sometimes the claims are so broad that it is not clear what kind of evidence could support such a claim.

A relatively large amount of space is used to explain the general concept of invariant/covariant learning, which, as a general concept, is widely understood and not novel. There are other instances of overclaiming, such as ""The goal of CoVAE is to provide an approach to probabilistic modelling that enables meaningful representations [...]"". In fact, CoVAE is a rather specific model(class), rather than an approach to probabilistic modelling.

The paper is at times meandering. For instance, the benefits of and motivation for the proposed approach are not simply stated in the introduction and then demonstrated in the rest of the paper, but instead the paper states some benefits and motivations, explains some technical content, mentions some more benefits, repeats some motivations stated before, etc.

Many researchers working on representation learning hope to discover the underlying learning principles that lead to representations that seem natural to a human being. In this paper, labels are used to guide the representation into the ""right"" representation. It is in my opinion not very surprising that one can use labels to induce certain qualities deemed desirable in the representation.

To conclude, because of the writing, limited novelty, and limited experiments, I think this paper currently does not pass the bar for ICLR.","The sentiment of the review is moderately negative, as it highlights several issues with the paper, including problematic writing, vague and unsupported claims, overclaiming, and limited novelty and experiments. The reviewer concludes that the paper does not meet the standards for ICLR. Therefore, the sentiment score is -60. The politeness of the language is relatively high, as the reviewer uses polite and constructive language to point out the issues, without being rude or dismissive. Therefore, the politeness score is 60.",-60,60
"This is nicely written paper analyzing the effect of various pre-training methods and shows that language models are very effective on sequence tagging tasks (POS, CCG). The experiments are well motivated and well described.

Regarding Table 1: which one of the ""LM forward"" models was used in the subsequent experiments? 

Are the input embeddings for the random init LSTM pre-trained or are they also randomly initialized?","The review starts with a positive sentiment, praising the paper for being nicely written and for its well-motivated and well-described experiments. This indicates a positive sentiment. The language used is polite, as the reviewer asks questions in a respectful manner without any negative or harsh language.",80,90
"The paper is well written with many relevant references and easy to read. Some points that need clarification and mentioned below. 
The main points of this paper are the use of the convolution operator to perform the message passing mean field inference. Using this operation allows us to get away from the permutohedral lattice and yet allows speed up of 100x. This also means, that training will be able to done faster. Besides this the training parameters can also be learnt. These are the main contributions. The denoising task experiment shows positive results. The idea could be used in the future by others looking for faster model inference and training.

If a Manhattan distance d is used i.e. dx,dy<k in equation (6), why is this a FullCRF? It seems like the new CRF is no longer a fully connected one. 

Page 5, first paragraph describing how the reorganization in the GPU is avoided is not very clear. It would be helpful to a reader to have more details and explanations about this.

It is not clear from the experimental results how much improvement allowing to train the CRF parameters gets or might get. Comparing to the Deeplab results etc for the non-trained case, the non-trained model still seems to be performing competitively. Table 2 of Table 3 does not really bring out the advantage of training. The +C, +T, +CT don't seem to be hugely different in terms of validation metrics. Note that Table 3 does not mention other models that might not be trained (assuming that those results are in Table 2) but the text also mentions that the training is not completely fair.

In section 5, Unary, it is mentioned that the network is not trained on larger datasets like other work, why?
And under CRF, what does iterations are unrolled mean?

In section 5.1, why does the random flipping help in simulating inaccuracies?


Minor points:
Abstract: Add space after ""GPUs."".
Would be good to define what Q, *, ' indicate in paragraph 4, page 2.
""hight"" -> ""height"" in section 4.1

","The sentiment of the review is generally positive, as indicated by phrases like 'well written', 'relevant references', and 'positive results'. The reviewer acknowledges the strengths of the paper and its potential future use. However, the review also contains several critical points and suggestions for improvement, which slightly temper the overall positivity. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, such as 'It would be helpful', 'It is not clear', and 'Would be good to define'. The reviewer avoids harsh or rude language, making the politeness score 90.",60,90
"The main idea is to incorporate linguistic-based constrains in the form of dependency trees into different variations of relation networks.
In general, the paper is well written and organized, the presented problem is well motivated, and the approach is very strait forward. The experimental setting is comprehensive, and the results are indeed competitive in a wide range of tasks.
I think that using linguistic knowledge to improve Neural networks performance is very promising field, I think that you could get a much more substantial gains when applying your method in less resource-rich setups (maybe using some small subset of training for the SNLI and question duplication datasets).
It seems that your method relies heavily on previous works (RN, RNN-RN, latent dependency trees ,intra-sentence attention), can you please state clearly what your contribution is? does your model has any advantages over current state-of-the-art methods?   

edit: I'm still not convinced about this article novelty, I really like the overall idea but it seems that this kind of contribution is better suited for short paper. ","The sentiment of the review is generally positive, as the reviewer acknowledges that the paper is well-written, organized, and presents a well-motivated problem with competitive results. However, there are some concerns about the novelty and the heavy reliance on previous works, which slightly dampens the overall sentiment. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer uses polite phrases such as 'I think,' 'can you please,' and 'I really like,' even when pointing out areas for improvement. Thus, the politeness score is 80.",60,80
"Summary: 

This paper proposes a method to get feedback from humans in an autonomous vehicle (AV). Labels are collected such that the human actually moves a steering wheel and depending on the steering wheel angle disagreement with the direction the vehicle is actually moving a feedback value is collected which is used to weight the scalar loss function used to learn from these demonstrations. 

Experiments on a simple driving simulator is presented. 

Comments: 

I think this paper is attempting to address an important problem in imitation learning that is encountered quite often in DAgger, AggreVate and variants where the expert feedback is provided on the state distribution induced by the learnt policy via a mixture policy. In DAgger (where the corrections are one-step as opposed to AggreVate where the expert takes over and shows the full demonstration to get Q(s,a)) it is difficult to actually provide good feedback especially when the expert demonstrations are not getting executed on the vehicle and hence hard for humans to ascertain what would be the actual effect of the actions they are recommending. In fact there is always a tendency to overcorrect which leads to instability in DAgger iterations. 

The paper proposes using a modified feedback algorithm on page 6 whose magnitude and sign is based on how much the correction signal is in agreement or disagreement with the current policy being executed on the vehicle. 

Unfortunately this paper is very confusingly written at the moment. I had to take multiple passes and still can't figure out many claims and discussions: 

- ""To the best of our knowledge, no research so far has focused on using any kind of human feedback in the context of AV control with learning from demonstration"" - This is not true. See: 

""Learning Monocular Reactive UAV Control in Cluttered Natural Environments, Stephane Ross, Narek Melik-Barkhudarov, Kumar Shaurya Shankar, Andreas Wendel, Debadeepta Dey, J. Andrew Bagnell, Martial Hebert"" who used DAgger for autonomous driving of a drone with human pilot feedback. 

- Lots of terms are introduced without definition or forward references. Example: \theta and \hat{\theta} are provided early-on are refered to on page 3 in the middle of the page but only defined at the end of the page in 3.1. 

- Lots of confusing statements have been made without clear discussion like ""...we could also view our problem as a contextual bandit, since the feedback for every action falls in the same range..."" This was a baffling statement since contextual bandit is a one-step RL problem where there is no credit assignment problem unlike sequential decision-making settings as being dealt with in this paper. Perhaps something deeper was meant but it was not clear at all from text. 

- The paper is strewn with typos, is really verbose and seems to be written in a rush. For example, ""Since we are off-policy the neural network cannot not influence the probability of seeing an example again, and this leads can lead to problems.""

- The experiments are very simple and it is not clear whether the images in figure 2 are the actual camera images used (which would be weird since they are from an overhead view which is not what human safety drivers would actually see) or hand-drawn illustrations.

","The sentiment score is derived from the overall tone and content of the review. The reviewer acknowledges the importance of the problem being addressed and the proposed method, which indicates a positive sentiment. However, the review is heavily critical of the paper's clarity, accuracy, and presentation, which brings the sentiment down to a more neutral level. Therefore, the sentiment score is set to -20. The politeness score is based on the language used in the review. The reviewer uses polite language, even when pointing out significant flaws and errors in the paper. There are no rude or harsh words, and the critique is presented in a constructive manner. Therefore, the politeness score is set to 60.",-20,60
"This is an empirical study on the ability for DQNs trained with/without regularization to perform well on variants of the same environment (e.g. increasing difficulty of a game). The paper is well written, the experimental methodology is clear & sound, and the significance is around improved sample efficiency via warm starting from a regularized DQN to fine tune. The error bounds for the regularized models results seem uncomfortably large in some cases. Overall it looks like a good methodological paper that can inform others on taking regularization more seriously when training DQNs. Evaluating on a modified ALE environment is great, but it would have been better to see this having similar impact in real life applications.","The sentiment of the review is generally positive, as the reviewer praises the paper for being well-written, having a clear and sound experimental methodology, and for its significance in improving sample efficiency. However, there are some minor criticisms, such as the large error bounds for the regularized models and the lack of real-life application examples. Therefore, the sentiment score is 70. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, even when pointing out areas for improvement. Therefore, the politeness score is 90.",70,90
"This work considers a version of importance sampling of states from the replay buffer.  Each trajectory is assigned a rank, inversely proportional to its probability according to a GMM. The trajectories with lower rank are preferred at sampling.

Main issues:

1. Estimating rank from a density estimator

- the reasoning behind picking VGMM as the density estimator is not fully convincing and (dis)advantages of other candidate density estimators are almost not highlighted.

- it is unclear and possibly could be better explained why one needs to concatenate the goals (what would change if we would not concatenate but estimate state densities rather than trajectories?)

2. Generalization issues

- the method is not applicable to episodes of different length
- the approach assumes existence of a state to goal function f(s)->g
- although the paper does not expose this point (it is discussed the HER paper)

3. Scaling issues

- length of the vector grows linearly with the episode length
- length of the vector grows linearly with the size of the goal vector

For long episodes or episodes with large goal vectors it is quite possible that there will not be enough data to fit the GMM model or one would need to collect many samples prior.

4. Minor issues

- 3.3 ""It is known that PER can become very expensive in computational time"" - please supply a reference
 

- 3.3 ""After each update of the model, the agent needs to update the priorities of the transitions in the replay buffer with the new TD-errors"" - However the method only renews priorities of randomly selected transitions (why would there be a large overhead?). Here is from the PER paper ""Our final implementation for rank-based prioritization produced an additional 2%-4% increase in running time and negligible additional memory usage""
","The sentiment of the review is moderately negative. The reviewer points out several significant issues with the paper, such as the choice of density estimator, generalization issues, and scaling problems. The language used is mostly neutral but leans towards being polite, as the reviewer provides constructive feedback and specific recommendations without using harsh or dismissive language.",-40,20
"This paper propose a new approach to dialogue modeling by introducing two
innovations over an established dialogue model: the HRED (Hierarchical
Recurrent Encoder-Decoder) network. The innovations are: (1) adding a GAN
objective to the standard MLE objective of the HRED model; and (2)
modifying the HRED model to include an attention mechanism over the local
conditioning information (i.e. the ""call"" before the present ""response"").  

Writing: The writing was mostly ok, though there were some issues early in
Section 2. The authors rather awkwardly transition from a mathematical
formalism that included the two halves of the dialogue as X (call) and Y
(response), to a formalism that only considers a single sequence X. 

Novelty and Impact:  The proposed approach explicitly combines an established
model with two components that are themselves well-established.
It's fair to say that the novelty is relatively weak. The model development
is sensible, but reasonably straightforward. It isn't clear to me that a
careful reader of the literature in this area (particularly the GAN for
text literature) will learn that much from this paper. 

Experiments: Overall the empirical evaluation shows fairly convincingly
that the proposed model is effective. I do wonder why would the hredGAN
model outperform the hred model on perplexity. The hred model is
directly optimizing MLE which is directly related to the perplexity
measure, while the hredGAN include an additional objective that should
(perhaps) sacrifice likelihood. This puzzling result was not discussed and
really should be.

The generated responses, given in table 3 -- while showing some improvement
over hred and Vhred (esp. in terms of response length and specificity) --
do not fit the context particularly well. This really just shows we still
have some way to go before this challenging task is solved. 

It would be useful if the authors could run an ablation study to help
resolve the relative contributions of the two innovations (GAN and
attention) to the improvements in results. Perhaps the improvement in
perplexity (discussed above) is do to the use of attention. 

Detailed comments / questions

- In the paragraph between Eqns 2 and 3, the authors seem to suggest that
  teacher forcing is an added heuristic -- however this is just the
  correct evaluation of the MLE objective. 

- In discussing the combined MLE-GAN objective in Eqn. 8 Does the MLE
  objective use teacher forcing? Some earlier text (discussed above) leads
  me to suspect that it does not. 
","The sentiment of the review is mixed, leaning towards neutral. The reviewer acknowledges the effectiveness of the proposed model but also points out several weaknesses, such as the lack of novelty and the puzzling results that were not discussed. Therefore, the sentiment score is 0. The politeness of the language is generally respectful and constructive, with suggestions for improvement and specific questions for clarification. The reviewer uses polite language and avoids any harsh criticism, so the politeness score is 80.",0,80
"This paper proposes a deep probabilistic model for temporal data that leverages latent variables to switch between different learned linear dynamics. The probability distributions are parameterized by deep neural networks and learning is performed end-to-end with amortized variational inference using inference networks.

There has been a lot of recent research trying to combine probabilistic models and deep learning to define powerful transition models that can be learned in an unsupervised way, to be used for model-based RL. This paper fits in this research area, and presents a nice combination of several interesting ideas presented in related works (switching variables, structured inference networks, merging updates as in the Kalman filter). The novelty of this paper in terms of original ideas is limited, the novel part lies in the clever combination of known approaches.

The paper reads well, but I found the explanation and notation in section 4 quite confusing (although easy to improve). The authors propose a structured variational approximation, but the factorization assumptions are not clear from the notation (I had to rely on Figure 2a to fully understand them). 
- In the first line of equation 7 it seems that the variational approximation q_phi for z_t only depends on x_t, but it is actually dependent also on the future x through s_t and q_meas
- The first line of section 4.1.1 shows that q_phi depends on x_{1:T}. However from figure 2a it seems that it only directly depends on x_{t:T}, and that the dependence on x_{1:t-1} is modelled through the dependence on z_{t-1}. 
- Is there a missing s_t in q_trans in the first line of (7)?
- why do you keep the dependence on future outputs in q_meas if it is not used in the experiments and not shown in figure 2a? It only makes the notation more confusing.
- You use f_phi to denote all the function in 4.1.1 (with different inputs). It would be clearer to use a different letter or for example add numbers (e.g. f^1_\phi) 
- Despite being often done in VAE papers, it feels strange to me to introduce the inference model (4.1) before the generative model (4.2), as the inference model defines an approximation to the true posterior which is derived from the generative model. One could in principle use other type of approximate inference techniques while keeping the generative model unchanged. 

It is difficult for me to understand how useful are in practice the switching variables. Reading the first part of the paper it seems that the authors will use discrete random variables, but they actually use for s_t continuous relaxiations of discrete variables (concrete distribution), or gaussian variables. As described in appendix B2 by the authors, training models with such continuous relaxations is often challenging in terms of hyper-parameter tuning. One may even wonder if it is worth the effort: could you have used instead a deterministic s_t parameterized for example as a bidirectional LSTM with softmax output? This may give equivalent results and remove a lot of complexity. Also, the fact that the gaussian switching variables perform better in the experiments is an indication that this may be the case.

To be able to detect walls the z variables basically need to learn to represent the position of the agent and encoding the information on the position of the walls in the connection to s_t.  Would you then need to train the model from scratch for any new environment?

Minor comment:
- in the softmax equation (6) there are missing brackets: lambda is at the denominator both for g and the log
","The sentiment of the review is generally positive, as the reviewer acknowledges the paper's fit within the research area and appreciates the combination of several interesting ideas. However, the reviewer also points out that the novelty is limited and provides several specific criticisms and suggestions for improvement. Therefore, the sentiment score is 40. The politeness of the language is quite high; the reviewer uses polite and constructive language throughout the review, even when pointing out flaws and making suggestions for improvement. Therefore, the politeness score is 80.",40,80
"+ Theoretic explanation for the scoring function.
+ (Promise for) Online provided source code.
+ The paper is well-written.

- The authors missed [1] which also introduces a generative model for knowledge graph embeddings. 
- The use of the datasets FB15k-237 and WN18RR instead of FB15k and WN18 (without inverse relations) would enable a better empirical evaluation. By using the flawed FB15k and WN18 datasets, the evaluation is biased towards the usage of inverse relations which should not exist in a link prediction evaluation dataset.
- The authors are not mentioning and comparing to walk based approaches like node2vec [2], Deepwalk [3], and rdf2vec [4]. 


Due to the missing comparisons to the mentioned references above and the possible bias in the evaluation, I am leaning towards rejecting the paper.


Minor comments:

. Abbreviations like h for head are used before they are introduced.
. ""The the"" -> ""The""
. ""triples are likely too be obvious examples"" -> ""triples are likely to be obvious examples""


[1] Xiao, Han, Minlie Huang, and Xiaoyan Zhu. ""TransG: A generative model for knowledge graph embedding."" Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Vol. 1. 2016.

[2] Grover, Aditya, and Jure Leskovec. ""node2vec: Scalable feature learning for networks."" Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2016.

[3]  Perozzi, Bryan, Rami Al-Rfou, and Steven Skiena. ""Deepwalk: Online learning of social representations."" Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2014.

[4] Petar Ristoski, Jessica Rosati, Tommaso Di Noia, Renato De Leone, and Heiko Paulheim. ""RDF2Vec: RDF Graph Embeddings and Their Applications."" SWJ http://www.semantic-web-journal.net/content/rdf2vec-rdf-graph-embeddings-and-their-applications-1","The sentiment of the review is mixed but leans towards negative due to the significant issues pointed out, such as missing references and potential bias in the evaluation. The reviewer explicitly states they are leaning towards rejecting the paper. Therefore, the sentiment score is -40. The politeness of the language is generally respectful and constructive, with the reviewer providing specific recommendations and minor comments without using harsh or rude language. Thus, the politeness score is 80.",-40,80
"Good results; providing some insights on the selection of activation function.  

This paper builds upon two previous works B.Poole etc. and S.S. Schoenholz etc. who initialized the study of random initialized neural network using a mean field approach (or central limit theorem.)
The two principal results of this paper are 
1. Initializing the network critically on the edge of chaos.  
2. Identifying some conditions on the activation functions which allow good ""information flow"" through the network.   

The first result is not new in general (already appeared in Schoenholz etc. and many follow up mean field papers). However, the results about ReLU (initializing (weigh_variance, bias_variance)=(2, 0)) seems to be new. The author also shows that the correlations converge to 1 at a polynomial rate (proposition 3), which is interesting. 

The second one is a novel part of this paper (proposition 5). If I understand correctly, the authors are trying to identify a class of activation functions (and suitable hyper-parameters) so that the network can propagate the sample-to-sample correlations (i.e. kernel) almost isometrically (please correct me if I am wrong). This is only possible 1) the activation functions are linear; OR 2) in the regime q->0, where the activation function has small curvature (i.e. almost linear). I think the results (and insights) are quite interesting. However, I don't think the authors provides enough theoretical or empirical evidence to support the claim that such activation functions can perform better.  



cons:
1. I don't think the experimental results are convincing enough for the reasons below:
    1.1. All experiments are conducted over MNIST with testing accuracy around 96%.  The authors should consider using large datasets (at least Cifar10).
    1.2 The width (<=80) of the network is too small while the theory of the paper assumes the width approaches infinity. Width>=200 should be a reasonable choice. It should be possible to train a network with depth~200 and width ~200 and batch_size~64 in a single machine.   
    1.3. Figure 6(b) seems unconvincing. ReLU network should be trainable with depth>=200; see figure 4 of the paper: ""Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice"" 


2. The claim that swish is better than tanh because the latter suffers from vanishing of gradients is unconvincing. It has been shown in Schoenholz etc and many follow-up papers that ultra-deep tanh networks (>=1000 layers) can be trained with critical initialization. 

3. Again, I don't think it is convincing to make the conclusion that swish is better than ReLU based on the empirical results on MNIST. 

4. Using a constant learning rate (lr=0.001) for all depths (and all widths) is incorrect. I believe the gradients will explode as depth increases. Roughly, the learning rate should decay linearly with depth (and width) when the network is initialized critically.  


In sum, the paper has some interesting theoretical results but the empirical results are not convincing.  


Other comments:
1. The authors should explain the significance and motivation of proposition 4. In particular, explain why we need f(x)~x. 
2. Consider replacing ""Proposition 4"" by  ""Theorem"", since it is the main result of the paper.  

","The sentiment of the review is mixed. The reviewer acknowledges some interesting theoretical results and insights, which is positive, but also points out significant shortcomings in the empirical evidence and experimental design. Therefore, the sentiment score is 10. The language used in the review is generally polite and constructive, offering specific suggestions for improvement without being rude or dismissive. Therefore, the politeness score is 80.",10,80
"This very interesting paper is based on the sensorimotor contingency theory, which grounds the perception of the agent (motor perception and sensory perception) in the capacity to learn to predict future sensory experience and to build a compact internal representation of proprioception and motor states. They show that through active experience (exploration) of an environment, an agent can encode its motor state in a way that captures both the topology and the relative distances. The authors show that in the case of an environment consistent with sensorimotor transitions and with some changes in the environment that are not inconsistent with the sensorimotor transitions (resets?), the network can learn the representation h of motor that project to the actual position of the agent in the environment (albeit in very contrived 2D toy tasks).

The model does not rely on RL at all; rather, it uses the representation of motor states (proprioception) to make predictions about the sensory observations of the environment. If such an agent were to act, I presume that there would be a search procedure across motor states to make the agent reach a desired state. The paper merits publication at ICLR provided very extensive revisions are made, and I am listing here the improvements to be made.

It is cool to cite Kant, Poincaré and Nicod, whose philosophical work underlies subsequent work on representing space and sensory experience. When citing Kant, please cite the primary source, not the 1998 re-edition.

There are missing references to Wayne et al (2018) ""Unsupervised Predictive Memory in a Goal-Directed Agent"" arXiv:1803.10760 and Ha & Schmidhuber ""World Models"" arXiv:1803.10122, where an agent is shown to build a representation of the world that can be decoded into spatial position and even a map of obstacles, on previously unseen environments, using only prediction of images, rewards and actions. Please include them in your work as they considerably change the narrative of section 2 (related work). Essentially, while the claims of the paper are interesting and relevant for the representation learning community, similar work has already been done, at much larger scale, from visual observations, using RL and self-supervised learning.

Section 2 is also somewhat overly critical of previous work: in (Banino et al, 2018; Cueva & Wei, 2018) ""rely[ing] on extraneous spatial supervision signals [do not] counter any claim of autonomy"", first because these signals can come from sensory perception (e.g., smell) and also because the agent is still autonomous at test time. Similarly, the depth prediction task in (Mirowski et al, 2017) is rather intuitive (stereo-vision).

Part 3 is difficult to parse: it would help to use the word proprioception (or explain why it is inappropriate) when talking about motor states, and exteroception (sensing the environment). I understood the first assumption, which is local continuity in sensory and motor space as well as the ambiguity of redundant motor systems that can generate the same sensory states, but not the second assumption. From what I understand, there are two invariants in building the model of proprioception: invariance to the topology of the environment and to the distances between objects, but then this is hard to reconcile with the setup of (in)consistent transitions in a moving environment and consistent transitions in a static environment. Please rewrite this section in a way that is easier to parse for people who know state-space models and RL for navigation and grasping (who may be your audience). Moreover, all the references point to a single work, which suggests that it is a very peculiar way of approaching a much more general problem of sensorimotor prediction, and therefore begs for a clear and simple explanation.

The architecture of the model is interesting: typically deep RL papers encode the sensory observations s into a hidden representation h, to take actions and produce a motor state m. Here, the current and future motor states m_t and m_{t+1} are embedded into h_t and h_{t+1} using a siamese MLP and used, in combination with the current sensory observation s_t, to make a prediction of s_{t+1}. This is somewhat related to learning the dynamics in model-based RL; please look into and cite Pathak et al (2017) ""Curiosity-driven exploration by self-supervised prediction"", ICML and other work on intrinsic curiosity.

The experiments are in very simplistic 2D grid world environments, but it makes the analysis and understanding of the 3D representations h much more simpler to follow. On the other hand, the discrete world task is very contrived (especially the weird mapping from m to h and from p to s) and hard to relate to existing work. One difficult problem that is solved by (Banino et al, 2018) is that of path integration in 2D from egocentric velocity. Could the authors present results on such a nonlinear case?

Revision: Score updated from 6 to 7.","The sentiment of the review is generally positive, as indicated by phrases like 'very interesting paper' and 'The paper merits publication at ICLR provided very extensive revisions are made.' However, the reviewer also points out several areas for improvement and provides constructive criticism. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer uses polite language and constructive feedback throughout the review. Phrases like 'please cite the primary source' and 'please rewrite this section' indicate a polite tone. Therefore, the politeness score is 80.",60,80
"This paper proposes to introduce a new domain, the uncertain domain, to better handle the division between seen/unseen domains in open-set and generalized zero-shot learning. The approach handles test samples estimated to be from the seen class in one way, and ones that belong to either the unseen or uncertain domain in another way. This idea handles the problem that test instances may incorrectly be attributed to one of the seen classes. The authors evaluate their approach on several relevant datasets against a wide variety of methods for OSL and ZSL, and show convincing results. 

I have three concerns. One is that the method sections of the paper are fairly lengthy, including an extensive explanation of prior work, e.g. EVT, so time is spent reading before the reader gets to the interesting part of the proposed method, and this time could be better focused around the contributions of *this* work. 

For the G-ZSL experiments, most of the methods seem to be older methods tackling ZSL not G-ZSL so perhaps more relevant baselines could be found.

On a related note, it would be good to include some qualitative examples that might reveal some intuitive reasons for the large margin between the performance of the proposed work, and other approaches; in some cases this margin seems rather large, and while the authors attempt to explain it, something beyond a textual explanation might be useful. ","The sentiment of the review is generally positive, as the reviewer acknowledges the convincing results and the relevance of the datasets used. However, the reviewer also points out three specific concerns, which slightly temper the overall positive sentiment. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer uses polite and constructive language to provide feedback and suggestions for improvement. There are no harsh or rude comments, and the tone is respectful. Therefore, the politeness score is 80.",60,80
"Summary: This work provides an analysis of the directional distribution of of stochastic gradients in SGD. The basic claim is that the distribution, when modeled as a von Mises-Fisher distribution, becomes more uniform as training progresses. There is experimental verification of this claim, and some results suggesting that the SNR is more correlated with their measure of uniformity than with the norm of the gradients.

Quality: The proofs appear correct to me. 

Clarity: The paper is generally easy to read.

Originality & Significance: I don't know of this specific analysis existing in the literature, so in that sense it may be original. Nonetheless, I think there are serious issues with the significance. The idea that there are two phases of optimization is not particularly new (see for example Bertsekas 2015) and the paper's claim that uniformity of direction increases as SGD convergence is easy to see in a simple example. Consider f_i(x) = |x-b_i|^2  quadratics with different centers. Clearly the minimum will be the centroid. Outside of a ball of certain radius from the centroid all of the gradients grad f_i point in the same direction, closer to the minimum they will point towards their respective centers. It is pretty clear, then that uniformity goes up as convergence proceeds, depending on the arrangement of the centers.

The analysis in the paper is clearly more general and meaningful than the toy example, but I am not seeing what the take-home is other than the insight generated by the toy example. The paper would be improved by clarifying how this analysis provides additional insight, providing more analysis on the norm SNR vs uniformity experiment at the end. 

Pros:
- SGD is a central algorithm and further analysis laying out its properties is important
- Thorough experiments.

Cons:
- It is not entirely clear what the contribution is.

Specific comments:
- The comment at the top of page 4 about the convergence of the minibatch gradients is a bit strange. This could also be seen as the reason that analysis of the convergence of SGD rely on annealed step sizes. Without annealing step-sizes, it's fairly clear that SGD will converge to a kind of stochastic process.

- The paper would be stronger if the authors try to turn this insight into something actionable, either by providing a theoretical result that gives guidance or some practical algorithmic suggestions that exploit it.

Dimitri P. Bertsekas. Incremental Gradient, Subgradient, and Proximal Methods for Convex Optimization: A Survey. ArXiv 2015.","The sentiment of the review is moderately positive. The reviewer acknowledges the correctness of the proofs, the clarity of the paper, and the importance of further analysis of SGD. However, the reviewer also points out significant issues with the originality and significance of the work, suggesting that the main insights are not particularly novel. The politeness of the language is high, as the reviewer provides constructive feedback and specific suggestions for improvement without using harsh or dismissive language.",30,80
"Volumetric Convolution, Automatic Representation Learning in Unit Ball

This work proposes to tackle the challenging problem of learning on unit balls. The method uses volumetric convolutions based on the Zernike polynomial trick, which makes it convenient to use on convolution networks. Invariance to 3D rotation enables a transformation to a volumetric space, where convolutions could be used in a conventional process. Clarity of the methodology may benefit from a motivation discussed from a global perspective. The reader is currently facing heavy mathematical concepts fairly quickly with global rationale on the proposed choices, in particular, in the explanation of symmetry analysis. Clarity on the use of 2D and 3D features could also benefit from more details on what is exactly proposed. Results are shown on an object recognition task achieving performance comparable with the state-of-the-art. 

Positive
+ Tackles the difficult problem of extending graph learning to arbitrary topologies, particularly on unit balls
+ The contributions are multifold -therotical framework for modeling volumetric convolutions over functions defined on unit-balls, -derivation of the formulation, to make it usable by neural nets, -measures of axial symmetry on unit-balls

Specific comments
- How to handle mixed topologies, for instance, with random presence of holes in the meshes
- Extension beyond unit balls?
- Fundamentaly, arbitrary genus-0 meshes are topologically equivalent to a sphere, however, there can be severe metric distorsion when transforming shapes to a sphere (e.g, transforming a banana to a sphere, the ends gets severely atrophied) - Does this pose a problem - how to handle these metric distorsion?
- Zernike polynomials are based on the spherical harmonics - could this be generalized to arbitrary graph harmonics? Beyond spherical shapes?



","The sentiment of the review is generally positive, as it acknowledges the challenging problem tackled by the work and highlights multiple contributions. However, it also points out areas for improvement and poses several questions. Therefore, the sentiment score is 60. The language used in the review is polite and constructive, offering specific comments and suggestions without being harsh or dismissive. Thus, the politeness score is 80.",60,80
"This paper first proposed a variant of experience replay to achieve better data efficiency in off-policy RL. The RACER algorithm was then developed, by modifying the approximated advantage function in the NAF algorithm. The proposed methods were finally tested on the MuJoCo environment to show the competitive performance.

This paper is in general well written. The ideas look interesting, even though they are mostly small modification of the previous works. The experiments also show the promise of the proposed methods. One of my concerns is regarding the generality of ReF-ER. I am wondering if it can be also applied to the Atari domain to boost the performance there, similar to the prioritized experience replay paper. I understand that the requirement of GPUs is beyond the hardware configuration in this work, but that would be an important contribution to the community. My other questions and comments are as follows.
- Regarding the parametric form of f^w in Eq. (7), what are the definitions for L_+ and L_-? What are the benefits of introducing min and max there, compared with the form in Eq. (11), as used in NAF? Does it cause any problems during optimization?
- The y axis in Figure 3 is for KL (\pi || \mu), while the text below used KL(\mu || \pi) and the description regarding the change of C also seems to be inaccurate. 
- In Figure 4, do you have any explanation why using PER leads to worse performance for NAF?
- For the implementation, did you use any parallelization to speed up the algorithm?","The review starts with a positive sentiment, acknowledging that the paper is generally well-written and the ideas are interesting, even though they are mostly small modifications of previous works. The reviewer also appreciates the promise shown by the experiments. However, the reviewer raises concerns about the generality of ReF-ER and provides several specific questions and comments for improvement. The language used is polite and constructive, aiming to help the authors improve their work.",60,80
"The submission proposes a new method for agent design to learn about the behaviour of other fixed agents inhabiting the same environment. The method builds on imitation learning (behavioural cloning) to model the agent’s behaviour and reinforcement learning to learn a probing policy to more broadly explore different target agent behaviours. Overall, the approach falls into the field of intrinsic motivation / curiosity-like reward generation procedures but with respect to target agent behaviour instead of the agent’s environment. While learning to model the target agent’s inner state, the RL reward is generated based on the difference of the target agent’s inner state between consecutive time steps.

The approach is evaluated against a small set of baselines in various toy grid-world scenarios and a sorting task and overall performs commensurate or better than the investigated baselines. Given its limitation to small and low-dimensional environments, it cannot be said how well the approach will scale with respect to these factors and the resulting, more complex agent behaviours. It would be highly beneficial to evaluate these aspects. Furthermore, it would be beneficial to provide more information about the baselines; in particular the type of count-based exploration. For the generated figures, it would be beneficial to include standard deviation and mean over multiple runs to not only evaluate performance but also robustness. 

Overall, while the agent behaviour modelling focused on a type of inner state (based on past trajectories) provides benefits in the evaluated examples, it is unsure how well the approach scales to more complex domains based on strong similarity and simplicity of the tested toy scenarios (evaluation on sorting problems is an interesting step towards to address this shortcoming). One additional aspect pointing towards the necessity of further evaluation is the strong dependence of performance on the dimensionality of the latent, internal state (Fig.4). 

Minor issues:
- Reward formulations for the baselines as part of the appendix.
- Same scale for the y-axes across figures

","The review provides a balanced assessment of the submission, acknowledging the novelty and potential benefits of the proposed method while also highlighting its limitations and areas for improvement. The sentiment is generally positive but tempered by the need for further evaluation in more complex environments. The language used is polite and constructive, offering specific recommendations without being overly critical or dismissive.",50,80
"===============================
I have read the authors' response and other reviewers' comments carefully. Thank you for taking great efforts to improve the paper, including providing additional results on human evaluation. (Btw, Table 1 and Table 2 are also much nicer now.)

However, from the reviews it seems that all the reviewers agree that the novelty of this paper is limited, and the contribution is incremental.  I understand that this paper is the first and only work using adversarial framework for persona multi-turn conversation models. However, from the modeling perspective, I still think the novelty is limited.

As a summary, I have updated the score from 4 to 5 to reflect the efforts that the authors have been taken to improve the paper. However, due to reasons above, I still prefer a rejection recommendation. 

===============================

Contributions:

The main contribution of this paper is the proposed phredGAN, which is a persona-based GAN framework for multi-turn dialogue modeling. Specifically, a persona-based HRED generator is developed, with two different kinds of discriminator design. Experiments are conducted on both the UDC and the TV series transcript datasets.  

Weaknesses:

(1) Novelty: I would say the novelty of this paper is rather limited. This paper heavily rely on the previous hredGAN work (Olabiyi et al., 2018), and extends it by injecting attributes into the system, borrowing ideas from the persona-based Seq2seq model (Li et al, 2016b). 

phredGAN_a is a straightforward extension of hredGAN, while phredGAN_d further introduces a collaborative discriminator that tries to predict the attribute that generated the input utterance. However, in summary, I think this paper is not novel enough. 

(2) Presentation: The paper is generally easy to follow and understand. However, I would say the paper is poorly written, and needs further polishing. For example, Table 1 & 2 are pretty ugly. 

(3) Evaluation: Generally, I think the experiments are not convincing and also not well-executed, with detailed comments listed below. 

Questions:

(1) In phredGAN_a, as shown in Eqn. (4), the attribute is used as input of the discriminator, while in phredGAN_d, as shown in Eqn. (5) & (6), the attribute is used as the target of the discriminator. My question is: why not use the attribute as both input & output? That is, why not combine (4) & (6), instead of using (5) & (6)? Please clarify this. 

(2) In experiments, Section 3.1, the authors mention that the generator and the discriminator use a shared encoder. However, the generator and discriminator has a different role. Since the encoder is shared, then: in one step, we update the encoder to minimize the GAN objective, in the alternative step, we update the encoder again to maximize the GAN objective. So, how to deal with this conflicting role of encoder during the training? Please clarify this. 

(3) From Table 2, it seems that it is difficult to see that phredGAN is better than hredGAN. Can you provide some explanations here?

(4) In Table 4, if the responses generated by hredGAN can be provided, that would be better to demonstrate the advantage of phredGAN. How does phredGAN compare with hredGAN qualitatively?

(5) From Table 1 & 2, it seems to me there is no metric that is specifically designed to evaluate whether the model captures the attribute information. Is there a way to quantitatively evaluate this? For example, pretrain an attribute classifier, or use the collaborative discriminator in the phredGAN_d model to measure how the generated response reflect the attribute. If we can observe the performance of phredGAN is better than that of hredGAN, that would be helpful for the paper.  

(6) Since the task is challenging, and the automatic metrics designed for this task is not perfect, like other papers, I think human evaluation is essential and desired for this task. However, such human evaluation is lacked in this paper.
  ","The sentiment of the review is mixed. The reviewer acknowledges the efforts made by the authors to improve the paper and provides some positive feedback on the improvements in the tables and the additional results on human evaluation. However, the reviewer also expresses significant concerns about the novelty and contribution of the paper, ultimately recommending rejection. Therefore, the sentiment score is slightly negative. The politeness of the language is generally high. The reviewer uses polite phrases such as 'Thank you for taking great efforts' and 'I understand,' and provides constructive feedback without being rude or dismissive.",-20,80
"This paper proposes an augmentation of traditional neural network learning to allow for the inference of causal effects. Specifically, they modify the data sampling procedure of SGD during training to use matched samples that are paired via propensity score matching. Experimental results on a number of dataset show that the proposed methodology is comparable to alternative machine learning based causal inference methods. 

Overall, I think this is a nice idea. I have two main concerns: 
(1) The use of small batches for matching. Figure 2 does alleviate this concern to an extent, but there is a large literature in statistics and the social sciences on the effect that the quality of matches have on the final causal estimand. It is quite possible that this particular dataset is more amenable to PSM. It is also worth noting that while there is bias reduction shown in figure 2, it is not overwhelming. 

(2) The use of propensity scores for matching. One of the insights from the heterogeneous treatment effect literature is that it is not difficult to find cases where the propensity of treatment is identical for two sets of covariates that otherwise do not obey any real balance. This can lead to large biases in the final estimate. Given that PSM is still a relatively widely used practice, I don’t think that its use is a ground for rejection in itself, but given that neural networks are often used to estimate complex causal relations when they are used and this paper is interested in individual treatment effects it is worth noting. 

I found the experimental setup to do a very good job in covering large portions of the behavior of the algorithm. The final results are a little underwhelming–the proposed method does not appear to clearly define a new state of the art for the tasks it is applied to–but it is often competitive and the paper presents an interesting idea.
","The sentiment of the review is generally positive, as indicated by phrases like 'Overall, I think this is a nice idea' and 'the paper presents an interesting idea.' However, the reviewer also expresses some concerns and notes that the final results are 'a little underwhelming.' Therefore, the sentiment score is not extremely high but still positive. The politeness of the language is quite high, as the reviewer uses polite and constructive language throughout, such as 'I have two main concerns' and 'it is worth noting.' The reviewer provides specific feedback without being rude or dismissive.",40,80
"Summary. The paper considers the robustness of neural nets against adversarial attacks. More precisely, the authors experimentally investigate the robustness of ensembles of neural nets. They empirically show that adversarially trained ensembles of 2 neural nets are more robust than ensembles of 2 adversarially trained neural nets.

Pros.
* Robustness of neural nets is a challenging problem of interest for ICLR
* The paper is easy to read
* Experimental results compare different algorithms for 2 neural nets

Cons.
* The study is experimental
* It is limited to gradient-based attacks
* It is limited to ensembles of size 2
* The Ensemble2Adv is a single NN model and not an ensemble model. 

Evaluation.
The problem is significant and the use of ensemble methods for robustness against adversarial attacks is a promising line of research. The experimental study in this paper opens new lines of research in this direction. But, in my opinion, the paper is not ready for publication at ICLR. Detailed comments follow but the study is limited to k=2; the main finding is limited to the comparison between bagging two adversarially trained neural nets (SeparateEnsemble2Adv) and learning adversarially the average of two neural nets (Ensemble2adv). In my opinion, Ensemble2adv is a single model of double size and not an ensemble model thus somehow contradicting the main claim of the paper.

Detailed comments.
* Introduction, end of §2, it is said that non-gradient based attacks are still effective. But in the sequel you only consider gradient-based attacks and never discussed this question.
* Introduction, contributions, it should be made clear at the beginning of the paper that you will consider ensembles of size 2 and only gradient-based attacks.
* Section 2. The momentum-based attack should be cited and could be considered. ""Boosting adversarial attacks with momentum, Dong et al, CVPR18""
* Section 3, §2, the discussion on ensemble methods is unprecise. Ensemble methods have different objectives. For instance, Bagging-like methods  aim at reducing the generalization error while others as Boosting aim at augmenting the capacity of individual models.
* Section 3. Here is my main concern on this paper. The classical method would be bagging of neural nets with different initializations. The neural nets could be adversarially trained. This would lead to the so-called SeparateEnsemble2Adv. Here, the authors consider another method. Their method can be viewed as k(=2) copies of the same neural network with different initializations and an additional layer computing the average of the k(=2) outputs. Then adversarially learn the obtained model which leads to the so-called Ensemble2Adv algorithm. This algorithm is not an ensemble method as such. In my opinion for k=2, it is equivalent to doubling the size of a neural net, adding averaging of the outputs, and adversarially training the obtained neural net.
* Note recent advances in ensemble NNs with papers such as Averaging weights leads to wider optima ..., Izmailov et al, UAI18; Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs, Garipov et al, arXiv:1802.10026
* Section 4.1. Here comes the limitation k=2. The case k=4 is considered in table 1 but is not discussed elsewhere in the paper.
* Section 4.1. I am not convinced by DoubleAdv. It is one way of doubling the size of a neural net but I am not convinced that this is the more efficient. As said before, in my opinion, Ensemble2Adv is another way for doubling the size. And many more should exist.
* Section 5. In my opinion, the main comparisons should concern SeparateEnsemble2Adv and Ensemble2Adv. Also other methods doubling the size should be considered. 
* Section 5. For k greater than 2, SeparateEnsemblekAdv should be the better method because the adversarial learning phase could be easily parallelized.
* I am not convinced by the discussion in Section 6.
* Typos. and -> an l-13, p5; IFGSM5, l-19 p6; then l-6 p7; to due l-6 p9
* Biblio. Please give complete references
","The sentiment of the review is mixed but leans towards the negative side. The reviewer acknowledges the significance of the problem and the potential of the research direction, which is positive. However, the reviewer also points out several limitations and expresses that the paper is not ready for publication, which is negative. Therefore, the sentiment score is -20. The politeness of the language is quite high. The reviewer uses polite language, provides constructive feedback, and avoids any rude or harsh comments. Therefore, the politeness score is 80.",-20,80
"This paper has two main contributions: 
 1) it extends normalizing flows to discrete settings (exploiting relaxation ideas from Jang et al and Maddison et al).
 2) it presents an approximate fixed-point update rule for autoregressive time-series that can exploit GPU parallelism.

Overall, I think the work is solid. Contribution 1 isn't very novel, but is useful and the authors did a good job there.

Contribution 2 seems more interesting, but is not as well studied. When is the fixed point update expected to work? 
What assumptions does it imply? How does performance improve with the number of steps K? Does simulating for a finite steps emphasize the effect of early z's?
I'm a bit surprised that the authors did not attempt to study this part of their algorithm in isolation. They make a claims 
but never look at this in detail.

That said, the authors do a good job showing the method ""works"", and figures 3F and 3G are particularly nice. 
In 3G, is ""autoregressive"" supposed to converge to flow eventually?
Why don't the authors also use time as the x-axis in figure 2F (like 3F)?

My biggest complaint about the paper is the writing, which does not introduce and present ideas in a clear sequential 
manner, making the paper hard to read. I realize ELBO is standard, but at least some description of the setup in equation 1 
is warranted. What is x,z,\theta etc? Any paper should aim to be minimally self-contained. This continues throughout the paper, which does not really attempt to place the contribution in the larger literature, but rather just reports what the authors did and observed.

Some more examples:

Page 3: ""so we need to evaluate \hat{Q}"". This isn't defined. The authors should mention what \hat{Q} and \bar{Q} are.
Similarly for P. After a couple of passes through the paragraph, I could figure what the authors meant, but they 
should introduce the notation they use.

In section 2, while defining their model, they do not mention the dimension of z_t until after equation 8
(and even here, it has to be inferred).

What is x in 6b? What is the generative model they are doing inference on?

Section 2.2: it's not clear to me how convergence is defined even in the discrete case. I feel this discussion 
also really belongs to section 2.1

While I can understand what section 2.3 is trying to say, I could not really follow the notation.

I could not understand figure 1E and the associated sentence in section 2.4

What is the take-away of section 2.3 and 2.4? The authors seem to imply working with the discrete model is 
better in their experiments. Maybe forewarn the reader here?

The experiments are a bit hard to follow. It is inspired by a neuroscience application, but uses only simulated data. This is fine, but rather than describe the setup in mathematical/time-series language, it is complicated the with neuroscience jargon. As such, it feels disjointed and disconnected from the rest of the paper. I already complained that earlier sections do not describe the modeling setup, this is one way the paper could be improved.

In figure 2A and 3A, are the s's actually z's?","The sentiment of the review is generally positive, as the reviewer acknowledges the solid work and useful contributions made by the authors. However, there are notable criticisms regarding the novelty of the first contribution and the lack of detailed study on the second contribution. The reviewer also points out significant issues with the clarity and organization of the writing. The politeness of the language is high, as the reviewer provides constructive feedback and suggestions for improvement without being rude or dismissive.",40,80
"The paper presents a new Generative Adversarial Network (GAN) for learning a  
target distribution that is defined as the difference between two other 
distributions. Applications in semi-supervised learning and adversarial training 
are considered in the experimental evaluation and results are presented in 
computer vision tasks. 

The paper is not very well written and can be hard to follow. One very important 
issue for me was motivation for defining the target distribution as a difference 
between two other distributions. I am not familiar with this area, but reading 
through the introduction it was never clear to me why this is a useful scenario, 
in practice. Furthermore, some statements in the introduction felt quite 
arbitrary. For example, the authors state that PixelCNN ""does not have a latent 
representation"" in a manner that makes it sound as if that is a bad thing. If 
indeed it is, then why so? It would be very helpful to motivate the setting more 
and to provide a couple of examples of where this method would be useful, in the 
introduction. Also, regarding the MNIST example in the end of page 1, what is 
the ""universal set""? This paragraph also felt a bit arbitrary and unclear.

Some comments about the rest of the paper:
  - The theoretical results of section 3 are just stated/listed, but are not 
    connected to algorithm 1. Please connect them to the different parts of the 
    algorithm and state in a couple sentences what they imply for the algorithm.
  - Right after theorem 1, which assumption are you referring to when you say 
    ""the assumption in Theorem 1""?
  - The reformulation of section 3.1 is never justified. What led you to use 
    this reformulation and why do you think it is more stable in practice?
  - You should mention in the caption of table 4, what quantity you are 
    computing.

Note that my evaluation for this paper is based mainly on the way it is written 
as, in its current state, it is hard for me to judge what is novel and what is 
useful, and what readers are supposed to take in by reading this paper. The main 
question that the paper definitely needs to answer, but does not do so currently 
(in my opinion) is:

  When is this method useful to readers? For solving which problems and under 
  what conditions? And also, when is this method bad and should not be used?

== Experiments ==

Section 5.1 is hard to follow and I don't quite get how it connects to the rest.

Also, in section 5.1.2 you mention that in comparison to Dai et al. (2017) your 
method does not need to rely on an additional density estimation network. Even 
if that is true, I cannot see how it is a useful remark given that the method of 
Dai et al. seems to always beat your method.

== Style ==

In figure 1, no labels or legends are provided making it hard to figure out 
what's going on at a glance. It would be very helpful to include labels and a 
legend.

Equation 2 is not written correctly. The equals sign only refers to ""V(G, D)"" 
and not the min-max of that, right? Please make that explicit by first defining 
""V(G, D)"" alone.","The sentiment of the review is moderately negative. The reviewer points out several significant issues with the paper, including its lack of clarity, insufficient motivation for the proposed method, and unclear theoretical justifications. The reviewer also mentions that the paper is hard to follow and that it is difficult to judge its novelty and usefulness. The politeness of the language used is neutral to slightly polite. The reviewer provides constructive feedback and specific recommendations without using harsh or rude language, but the overall tone is critical.",-60,20
"The main contribution of this paper is a new proposed score to evaluate models that yield uncertainty values for regression.

As constituted, the paper can not be published into one of the better ML conferences. The novelty here is very limited. Furthermore there are other weaknesses to the study.

First, the stated goal of the ""metric"" is that ""reflects the correlation between the true error and the estimated uncertainty ... (and is) scale independent and robust to outliers."" Given this goal (and the name of the paper) it is perplexing why the correlation (https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) of true error and predicted error (\sigma_i) was not tried as baseline score. The correlation would have some ""scale independence"" and I'm sure that there are robust estimates (a simple thing would be to consider the median instead of mean, but there are probably more sophisticated approaches). This just feels like an obvious omission. If one wants to mix both predictive quality and correctness of uncertainty assessments then one could just scale the mean absolute error by the correlation: MAE/Corr, which would lead to a direct comparison to the  proposed MeRCI.

Second, the paper does a poor job of justifying MeRCI. On toy data MeRCI is justified by a confluence with existing scores. Then on the depth prediction task, where there are discrepancies among the scores, MeRCI is largely justified qualitatively  on a single image (Figure 4). A qualitative argument on a single instance in a single task can not cut it. The paper must put forth some systematic and more comprehensive comparison of scores.

Even with the above issues resolved, the paper would have to do more for publication. I would want to see either some proof of a property of the proposed score(s), or to use the proposed score to inform training, etc.","The sentiment of the review is generally negative, as the reviewer points out several significant weaknesses in the paper, including limited novelty, poor justification of the proposed score, and lack of comprehensive comparison. The reviewer also states that the paper cannot be published in a top ML conference in its current form. Therefore, the sentiment score is -70. The politeness of the language is relatively neutral to slightly polite. The reviewer uses phrases like 'it is perplexing' and 'this just feels like an obvious omission,' which are critical but not rude. The reviewer also provides constructive feedback and suggestions for improvement, which indicates a polite tone. Therefore, the politeness score is 20.",-70,20
"Summary
The manuscript proposes to use power iterations in an approximate ""whitening layer"" to optimize the slowness objective of SFA in a very general setting. A set of experiments illustrates that this way of doing nonlinear SFA is meaningful.

Quality
Although the idea is pretty straight forward and the paper shows qualitative results on a number of datasets, the relative merit of the approach is empirically not well characterized.

Clarity
The manuscript is in general well written and the technical content is well accessible. However the description of the whitening layer implementation needs some more details.

Originality
The idea of using a whitening layer together with the slowness objective has not been explored before. There is a second ICLR 2019 submission (Pfau et al.) with a very similar idea, though.

Empirical Evaluation
The approximate whitening should lead to a trade-off between whitening and slowness optimization. I miss an experiment illustrating that trade-off. Also the comparison to nonlinear SFA using expansion or kernelization of hierarchical SFA is empirically not properly characterized. In the end, if one takes the slowness objective seriously, one would use the method yielding slower results.

Significance
The manuscript introduces a way of running nonlinear SFA with approximate constraints in a general deep learning setting with a differentiable implementation using a dedicated whitening layer based on power iterations.

Reproducibility
The data is either synthetic or publicly available. The Keras implementation of the PowerWhitening layer as well as the entire neural network along with its optimization schedule is not shared. Hence, there should be some effort involved to reproduce the experiments.

Pros and Cons
1+) The idea of an approximate whitening layer is conceptually simple and clear.
2-) The description of the practical implementation of the power iteration is slightly imprecise.
3-) The algorithm scales badly in the number of output dimensions. This scaling is bad in a computational sense and also in a statistical sense.

Details
a) Section 6.1: Why do you need to add the noise term? What is the statistical meaning of this added noise?
b) Section 6.1: the solutions if comparable -> the solutions is comparable
c) References: Shaham -> ICLR 2018 paper
d) References: nyström -> Nyström
e) The name for the algorithm ""Power SFA"" is a little bit bold.","The sentiment of the review is moderately positive. The reviewer acknowledges the manuscript's novel approach and its conceptual simplicity, but also points out several areas needing improvement, such as empirical characterization and implementation details. Therefore, the sentiment score is 30. The language used in the review is polite and constructive, offering specific suggestions for improvement without being harsh or dismissive. Thus, the politeness score is 80.",30,80
"This paper proposes a new metric called the image score that compares the similarity of activation between a given image with a pool of groundtruth images. The paper finds it useful for semi-supervised learning with self-teaching, where the network picks the most confident sample and use the network prediction as the label. It finds that the proposed method is better than 1) not using the unlabeled data and 2) using softmax as an indicator for model prediction certainty.

Motivation: The introduction begins by motivating the interpretability story of deep learning, but I don’t see gaining any more interpretability by reading the rest of the paper. The paper proposes to improve interpretability by assigning a score to each individual example, but then the obtained scores are not properly analyzed in the paper, and only final classification accuracy is evaluated. What are the training samples that makes the model make certain decision at test time? How to measure the correlation between the usefulness of training samples and the proposed image score? These questions left unanswered in the paper. Figure 1 helps a little bit, but then the top row is not necessarily the bad images, but maybe hard examples that needs extra attention to learn. Therefore, I think the end results presented in the experiments do not align with the motivation. Rather than shooting for interpretability, this is just another semi-supervised learning paper.

Models: The major issue of this paper is the model formulation that is not well motivated. The intuition of how the authors come up with the equation for computing the image score is not well explained. Hence the formulation seems very ad-hoc, and it is unclear why this is the selected method.

Experiments: As a semi-supervised learning paper, a common setting for CIFAR-10 is to use 4k labeled images. Here, the method uses 30k, which is 7.5x the size of the usual setting. It also does not compare to prior semi-supervised learning work (e.g. one of the recent one is: https://arxiv.org/abs/1711.00258). The only two baselines discussed here are weak. Also the improvement from the baselines by using the proposed method is not very significant.

Comparison: Figure 2-4 shows some positive correlation between the accuracy and score, which is fair, but it doesn’t compare to any baselines--the only one we have is softmax baseline and it is not shown in the figure.

In conclusion, I couldn’t see how the paper improves interpretability as claimed in the introduction. The proposed method seems ad-hoc, without any justification. Being considered as a semi-supervised learning paper, it lack significant amount of comparison to prior work and adopting a common semi-supervised benchmark. Due to the above reasons, I recommend reject.

---
Minor points:
“...almost all of the existed works investigate only the models and ignore the relationship between models and samples”. This is over-exaggerated. I believe most of the visualization techniques are dependent on the actual input samples. It is true to say about “training samples” not “samples” in general.

“all correctly classified images should have similar chain of activation, while incorrectly classified images should have very different activations both within themselves and with correctly classified images”. This claim seems not backed up. How do you know it is the case for “all” correctly classified images? What defines similar/different?","The sentiment of the review is largely negative. The reviewer points out several significant issues with the paper, including a lack of alignment between the motivation and the results, an ad-hoc model formulation, insufficient experimental comparisons, and weak baselines. The recommendation to reject the paper further underscores the negative sentiment. Therefore, the sentiment score is -80. The politeness of the language used in the review is relatively high. The reviewer provides constructive criticism and specific recommendations without using harsh or rude language. The tone remains professional and respectful throughout the review. Therefore, the politeness score is 80.",-80,80
"The paper provides a new system that combines a number of neural networks to predict chemical reactions. The paper brings together a number of interesting methods to create a system that outperforms the state of the art.

Good about this paper: 
 - reported performance: the authors report a small but very consistent performance improvement.
 - the authors propose an approach that puts together many pieces to become an effective approach to chemical reaction prediction. 
 -

Problematic with this paper
 - this paper is impossible to understand if you only refer to the ten pages of content. There are at least 5 pointers in the paper where the authors refer to the appendix for details. Details in many of these cases are necessary to even understand what is really being done:  p3: rewards p4: message passing functions, p5: updating states, p9: training details. Further, The paper has some details that are unnecessary - e.g. the discussion of global vs. local network on p4 - this could go into the appendix (or be dropped entirely)

 - the model uses a step-wise reward in the training procedure (p3) -> positive reward for each correct subaction. It is not clear from the paper whether the model requires this at test time too (which should not be available). It's not clear what the authors do in testing. I feel that a clean RL protocol would only use rewards during training that are also available in testing (and a final reward)

 - eq 7: given there is an expontential in the probability - how often will the sampling not pick the top candidate? feels like it will mostly pick the top candidate. 

 - eq 9: it's unclear what this would do if the same pair of atoms is chosen twice (or more often)

 - the results presented in table 3: it appears that GTPN alone (and with beam search) is worse than the previous state of the art. only the various post processing steps make it better than the previous methods. It's not clear whether the state of the art methods in the table use similar postprocessing steps or whether they would also improve their results if the same postprocessing steps were applied. 
 

minor stuff: 
p2: Therefore, one can view GTPN as RL -> I don't think there is a causality. Just drop ""Therefore""
p2: standard RL loss -> what is that? 
eq. 2: interestin gchoice to add the vectors - wouldn't it be easier to just concatenate?
p4: what does ""NULL"" mean? how is this encoded?
p4 bottom: this is quite uncommon notation for me. Not a blocker but took me a while to parse and decrypt.
p5: how are the coefficients tuned?","The sentiment of the review is generally positive, as the reviewer acknowledges the novelty and performance improvement of the proposed system. However, the review also contains several critical points regarding the clarity and completeness of the paper, which slightly dampens the overall positive sentiment. Therefore, the sentiment score is 40. The politeness of the language used is quite high; the reviewer provides constructive criticism without being rude or dismissive. The language is professional and aimed at helping the authors improve their work, so the politeness score is 80.",40,80
"The author analyze the convergence properties of batch normalization for the ordinary least square (OLS) objective. They also provide experimental results on the OLS objective as well as small scale neural networks. First of all, understanding the properties of batch normalization is an important topic in the machine learning community so in that sense, contributions that tackle this problem are of interest for the community. However, this paper has a significant number of problems that need to be addressed before publication, perhaps the most important one being the overlap with prior work. Please address this point clearly in your rebuttal.

1) Overlap with Kolher et al. 2018: The authors erroneously state that Kolher et al. considered the convergence properties of BNGD on linear networks while after taking a close look at their analysis, they first derive an analysis for least-squares and then also provide an extension of their analysis to perceptrons. The major problem is that this paper does not correctly state the difference between their analysis and Kolher et al who already derived similar results for OLS. I will come back to this aspect multiple times below.

2) Properties of the minimizer
The authors should clearly state that Kolher et al. first proved that a^* and w^* have similar properties to Eq. 8. If I understand correctly, the difference seem to be that the algorithm analyzed in Kohler relies on the optimal a^* while the analysis presented here alternates between optimizing a and w. Is this correct? Is there any advantage in not using a^*? I think this would be worth clarifying.

3) Scaling property
I find this section confusing. Specifically,
a) The authors say they rely on this property in the proof but it is not very clear why this is beneficial. Can you please elaborate?
b) It seems to me this scaling property is also similar to the analysis of Kolher et al. who showed that the reparametrized OLS objective yields a Rayleigh quotient objective. Can you comment on this?
c) The idea of “restarting” is not clear to me, are you saying that one the magnitude of the vector w goes above a certain threshold, then one can rescale the vector therefore going back to what you called an equivalent representation? I don’t see why the text has to make this part so unclear. Looking at the proof of Theorem 3.3, this “property” seem to be used to simply rescale the a and w parameters.
d) The authors claim that “the scaling law (Proposition 3.2) should play a significant role” to extend the analysis to more general models. This requires further explanation, why would this help for say neural networks or other more complex models?

4) Convergence rate
It seems to me that the results obtained in this paper are weaker than previous known results, I would have liked to see a discussion of these results. Specifically,
a) Theorem 3.3 is an asymptotic convergence result so it is much weaker than the linear rate of convergence derived in Kolher et al. The authors require a sufficiently small step size. Looking at the analysis of Kolher et al., they show that the reparametrized OLS objective yields a Rayleigh quotient objective. Wouldn’t a constant step size also yield convergence in that case?
b) Proposition 3.4 also only provides a local convergence rate. The authors argue BNGD could have a faster convergence. This does seem to again be a weaker result. So again, I think it would be very beneficial if the authors could clearly state the differences with previous work.

5) Saddles for neural nets
The authors claim they “have not encountered convergence to saddles” for the experiments with neural networks. How did you check whether the limit point reached by BNGD was not a saddle point? This requires computing all the eigenvalues of the Hessian which is typically expensive. How was this done exactly?

6) Extension of the analysis to deep neural networks
The analysis provided in this paper only applies to OLS while Kolher et al. also derived an analysis for neural networks. Can the authors comment on extending their own analysis to neural nets and how this would differ from the one derived in Kolher et al.?

7) Experiments
How would you estimate the range of suitable step sizes (for both a and w) for BNGD for a neural network?
","The sentiment of the review is mixed but leans towards negative. The reviewer acknowledges the importance of the topic and the potential interest of the contributions but highlights a significant number of problems, particularly the overlap with prior work. This results in a sentiment score of -40. The politeness of the language is generally high. The reviewer uses polite phrases such as 'please address this point clearly' and 'I think this would be worth clarifying,' which indicates a respectful tone despite the critical feedback. This results in a politeness score of 80.",-40,80
"The scope of the paper is interesting: to additionally learn the nonlinear activation function of the neuron.

The insights provided in section 2 with eqs (2)-(5) are interesting and naturally build on the previous work of Poggio & Girosi (1990) and Smola (1998). I found this a nice new insight and the strongest part of the paper. It is e.g. revealing to see to which P and L the rectifier nonlinearity is corresponding.

On the other hand I also have a number of suggestions for further improvement:

- Section 1: related to the overall function to be learned, the authors state ""this general problem has been already solved"". I think this statement is not completely correct, because depending on the choice of the stabilizer one obtains different optimal representations (e.g. Gaussian RBF or thin plate splines) as explained in Poggio & Girosi (1990). The theory does not tell what the best stabilizer is.

Additional relevant work that would be good to mention at this point, in the area of kernel methods, is e.g. learning the kernel.

- It seems that no other existing work on deep kernel machines has been mentioned in the paper, while in the conclusions the authors state ""In this paper we have introduced Kernel-based deep neural networks"". 

- Related to the training set T_N the notation e^kappa is not explained. It is not clear how this is related to eq (1).

- It would be good to comment on the difference between (3)(4) and Poggio & Girosi (1990).

- unnumbered eq after (5): are there multiple solutions to the problem (non-convex)? 

- The explanation of the recurrent network at the end of section 2 is too limited. Moreover, LSTM is not just a neuron nonlinearity, but a recurrent network with a particular structure. To which P and L would LSTM correspond?

- Fig.2: some of the nonlinearities look quite complicated and some of them are oscillatory (is this desirable? it reminds us of overfitting). Often one is interested in activation functions with a ""simple shape"" like sigmoid, tanh, relu. A more complicated nonlinearity may reduce the interpretability of the model.

- The examples given are rather conceptual (though nice) examples of the proposed method. However, no comparisons with other methods have been made yet in terms of generalization performance, e.g. on a few standard classification benchmark data sets, in comparison with other deep or shallow models.

A possible drawback of the proposed method might be (or maybe not) that additional unknown parameters need to be learned, which could possibly lead to worse generalization. It might be good to further investigate this.
","The review starts with a positive sentiment, appreciating the interesting scope of the paper and highlighting the strong insights provided in section 2. However, it transitions into a series of constructive criticisms and suggestions for improvement. The language used is polite and professional, aiming to help the authors improve their work rather than dismissing it outright. The reviewer acknowledges the strengths of the paper while also pointing out areas that need clarification or additional work. This balanced approach indicates a generally positive sentiment with a high level of politeness.",60,90
"The key idea of this paper is to expand the network for training on new tasks which is termed as C-Net, and train an additional generative model which is used for predicting task id (which is called H-Net), and use the task id for selecting weights from the C-Net.

Pros:
1. It is relatively easy to understand the paper. 
2. The originality of this paper lies in the usage of generative model to predict task id (H-Net). To my knowledge has not been proposed before.
3. In contrast to previous works in multi-task learning, which assumes task id is available both during training and inference, this work tries to remove the need of task id during inference, which makes it closer to the general definition of continual learning.

Cons:
1. Expanding the network for new tasks is not a novel contribution of this paper, it has already been proposed in previous works on multi-task learning. Doing expansion on all of the layers does not qualify for a major contribution in my opinion.
2. The experimental comparison is not very fair in my opinion, 
     a. Comparing accuracy of C-Net to other methods is not very useful. Because this methods expands the network for every new task, while other methods (EWC, LwF) has limited to no expansion in the network. Given that the single network result is far from state of the art (table 3), I suppose model size could contribute to the accuracy boost.
     b. It is not explicitly stated in the paper whether the output neurons are shared between tasks or an individual set of output neurons are used for different tasks, but from the rest of the paper I suppose disjoint neurons are used. Then the comparison between EWC and this work is not fair because EWC shares the output neurons among tasks.
This is not to blame this paper for not making fair comparison, since given different assumptions between methods (availability of task id, shared output neurons etc.), it is usually difficult to fairly compare between continual learning methods.  This problem is raised in another submission https://openreview.net/forum?id=ByGVui0ctm. The point here is that the accuracy of C-Net is not a good measure of how good this method is.
3. I disagree with the point that MNIST and SVHN are similar, they have very different distributions and are very easy to tell apart with a model. One concern is that the generative H-Net may fail to work once the distributions of the tasks overlap to some extent. e.g. cifar10 vs cifar100.

As a conclusion, the key contribution of this work is using generative model to determine task id which removes the need for task id during inference. It is relatively insufficient for publication on ICLR.","The sentiment of the review is mixed. The reviewer acknowledges some positive aspects of the paper, such as its originality in using a generative model to predict task id and its attempt to remove the need for task id during inference. However, the reviewer also points out several significant drawbacks, including the lack of novelty in expanding the network for new tasks, unfair experimental comparisons, and concerns about the generative model's performance on overlapping distributions. Therefore, the sentiment score is slightly negative. The language used in the review is generally polite and constructive, with the reviewer providing specific feedback and avoiding harsh or rude language. The reviewer even acknowledges the difficulty in making fair comparisons between different methods, which adds to the politeness of the review.",-20,80
"In this paper, the authors propose a SOSELETO (source selection for target optimization) framework to transfer learning and training with noisy labels. The intuition is some source instances are more informative than the others. Specifically, source instances are weighted and the weights are learned in a bilevel optimization scheme. Experimental studies on both training with noisy label problems and transfer learning problems demonstrate the effectiveness of the proposed SOSELETO.

Overall, this paper is well-written, and easy to follow. The intuition is clear and reasonable, although it is not new. Regarding the technical section, I have the following comments:
(1)	The paper assumes that the source and target domains share the same feature representation parameters \theta. This is a widely used assumption in the existing works. However, these works usually have a specific part to align two domains to support the assumption, e.g. adversarial loss or MMD. In objective of SOSELETO, I do not see such a domain alignment part. I am wondering whether the assumption is still valid in this case. From the experimental study, I find SOSELETO achieves very good results in transfer learning problems. I am wondering whether the performance would be further improved if a domain alignment objective is added in the weighted source loss.
(2)	Each source has a weight, and thus there are n^s \alpha. As mini-batch is used in the training, I am wondering whether batches are overlapping or not. If overlapping, how to decide the final \alpha_i for x^s_i as you may obtain several \alpha_i in batches. 
(3)	Another point is abouth \lambda_p. In the contents, you omit the last term Q \alpha_m \lambda_p in eq.(4) as you use the fact that it is very small. I am not convincing on this omission as \lambda_p is also a weight for the entire derivative. Moreover, if \lambda_p is very small, the convergence would be very slow. In the experimental studies, you use different \lambda_p for different problems. Then, what’s the rule of setting \lambda_p given a new problem?

Regarding the experimental results, the experimental settings for the section 4.2 are not very clear to me. You may need to clearly state the train and test set (e.g. data size) for each method.  
","The sentiment of the review is generally positive. The reviewer acknowledges that the paper is well-written, easy to follow, and that the proposed framework demonstrates effectiveness in experimental studies. However, the reviewer also points out several technical concerns and areas for improvement, which slightly tempers the overall positivity. Therefore, the sentiment score is 60. The politeness of the language is high; the reviewer uses polite and constructive language throughout the review, making suggestions and asking questions in a respectful manner. Thus, the politeness score is 90.",60,90
"This paper describes a method of training neural networks without update locking. The idea is a small modification on top of Czarnecki et al. Critic training, where instead of using final loss as a critic target, one bootstraps from critics on other layers. In particular, if only one module is present, these two approaches are actually identical. To be more precise, the only difference between these two methods is that (7) in Critic training would change to l(L_i, L_N). As a consequence, method becomes forward unlocked too. It is worth noting, that in the appendix of Czarnecki et al. it is shown that this particular method (critic training) under simple conditions actually ""degenerates"" to deep supervision (which is forward unlocked too). Consequently unlocking property as such is not a big contribution of the proposed method. Rest of the paper includes following elements:
- empirical evaluation showing improvement over critic training by 0.4% in CIFAR10 and 0.9% in CIFAR100 when using 3 splits.
- expansion on using the model for progressive inference.

Given standard deviation of errors in Table 1. it is not clear how significant these improvements are. How many samples were used to estimate these quantities? It is worth noting, that Critic training was showed to be outperformed by Sobolev Training in the same paper authors cite, but its performance is not reported despite looking like a well defined baseline. In particular, can these two methods be combined? 

I believe that this is an interesting research direction, however paper in its current form seems as a small incremental improvement over sota, and could be significantly improved by for example:
- providing more comprehensive evaluation (including estimating accuracy to lower std errors)
- adding other baseline solutions (such as Sobolev training, cDNI, or deep supervision)
- considering any form of convergence/dynamics analysis of the proposed approach



","The sentiment of the review is moderately positive, as the reviewer acknowledges the interesting research direction and the potential for significant improvement, but also points out several areas where the paper could be improved. The sentiment score is 20. The politeness of the language is quite high, as the reviewer uses polite and constructive language throughout the review, offering specific suggestions for improvement without being harsh or dismissive. The politeness score is 80.",20,80
"This paper deals with the open set classification problem, where in addition to the known classes, the method should also be able to recognize the unknown class. The main idea is based on two parts: learning a discriminative representation, and a threshold based detection rule. To learn the embedding, the authors propose to minimize the inner class distance (between each instance to its center) and enlarge the distance between centers. The outlier score of an instance is computed as the minimum distance between known class prototypes. Experiments on various datasets show the ability of the learned method.

I'm not completely sure whether the whole approach is novel or not in the open set recognition domain, but both parts are not novel enough. Pulling similar instances together and pushing dissimilar ones away is the main idea in embedding learning. The ii-loss is similar to the triplet-center loss in the paper ""He et al. Triplet-Center Loss for Multi-View 3D Object Retrieval. CVPR18"". 

Although in the experiments the proposed method achieves good results in most cases, the reviewer suggests the authors comparing with more baselines to make the work solid.
1. Comparing with other embedding learning methods with the same outlier detection score. 
The authors should prove that the proposed embedding is important enough in the open set case. For example, using the center loss (Wen et al. A discriminative feature learning approach for deep face recognition. ECCV16), triplet-center loss, triplet loss (computing class centers after embedding).

2. Discuss more on the outlier score part. 
How to differentiate the known class outlier and new class? Will the problem be more difficult when the unknown class contains more heterogeneous classes? The authors can also apply existing open set recognition rule on the learned embedding.

Some detailed questions:
1. What's the difference between ""the network weights are first updated to minimize on ii-loss and then in a separate step updated to minimize cross entropy loss"" and optimize both loss terms simultaneously?
2. ""We assume that a certain percent of the training set to be noise/outliers"", how to determine the concrete value? Is 1% the helpful one for all cases?
3. Since there is not optimize over the unknown classes in training, could the reason for ""the unknown class instances fully occupy the open space between the known classes"" is the unknown classes are randomly sampled from the whole class set? For example, if classes about animals are known classes and classes about scene compose the unknown class, will the unknown class also occupy the whole space in this case?
4. What is the motivation of making ""the unknown class instances fully occupy the open space between the known classes""?","The sentiment of the review is mixed. The reviewer acknowledges the potential of the proposed method and its good results in experiments, but also expresses doubts about the novelty of the approach and suggests several improvements. Therefore, the sentiment score is slightly positive. The language used in the review is polite and constructive, offering specific recommendations and detailed questions without being dismissive or rude.",20,80
"The authors describe q-activation functions, stochastic relatives of common activation functions used in neural networks.  It seems like the main argument is to use them because you get a performance improvement with them. 

While the experiments appear to show better training at early epochs, none of the models appear to have been trained to convergence.  Additional justifications for why (or when) to use this should be described.

Why does the method outperform particularly when dropout is included?

I also expect the lack of monotonicity in the q-activation functions to lead to the creation of (exponentially) more local minima.  Any comments?

Quality: the experiments need some further work.
Clarity: aside from a few points, the paper is written clearly.
Originality: the work appears original to me
Significance: TBD, but the main argument appears to be that it leads to empirical comparative gains (but on networks not designed to be SOTA).

Small points:
""By prop 2, g_q(x) agrees with with original activation function"".  What does ""agrees with"" mean?
""Fig 2. Darker color --> lighter color?""
""(Conclusion) ... can goes[sic] deeper on the error surface."" To me, the experiments only show marginally better performance","The sentiment of the review appears to be slightly negative, as the reviewer points out several issues with the experiments and the clarity of the paper. The reviewer acknowledges some positive aspects, such as the originality of the work, but the overall tone suggests that significant improvements are needed. Therefore, the sentiment score is -30. The politeness of the language is generally neutral to slightly polite. The reviewer uses polite language and phrases like 'I also expect' and 'Any comments?' which indicate a respectful tone, but there are also direct criticisms. Therefore, the politeness score is 20.",-30,20
"This paper presents a method for increasing the efficiency of sparse reward RL methods through a backward curriculum on expert demonstrations. The method in the paper is as follows: assuming access to expert demonstration and a resettable simulator, the start state of the agent in the beginning of training is sampled from end of demonstration (close to the rewarding state) where the task of achieving the goal is easy. Then gradually through a curriculum this is shifted backwards in the demonstration, making the task gradually harder. 

The proposed method is closely related to 1) “Learning Montezuma’s Revenge from a Single Demonstration” a blog post and open-source code release by Salimans and Chen (OpenAI Blog, 2018) where they show that constructing a curriculum that gradually moves the starting state back from the end of a single demonstration to the beginning helps solve Montezuma’s revenge game 2) “Reverse Curriculum Generation for Reinforcement Learning” by Florensa et al. (CoRL 2017) , where they start the training to reach a goal from start states nearby a given goal state and gradually the agent is trained to solve the task from increasingly distant start states. 

The approach is evaluated on a pair of tasks, a maze environment and a stochastic four-player game, Pommerman. In the maze environment, they compare to vanilla PPO and Uniformly sampled starting points across the expert trajectory. The Backplay method outperforms the vanilla baseline, however, from the training curves (~3500 epochs) in the appendix A4, it looks like the Uniform sampling baseline is doing as well or better than the proposed method. As pointed out by the authors themselves the reverse curriculum does not seem necessary in this environment. Also, it is unclear to me whether the curves shown is comparable as the starting point of the agent, at least in the beginning of training, is close to the goal with higher success rate for the Backplay method compared to baselines. A good convincing assessment would be to report success rate against the same starting point for all methods preferably not from the starting point of the demonstrations to assess generalisation of these methods for which authors briefly report unsuccessful results. 

The Pommerman environment is more complex and the results reported are more interesting. Figure 3 shows the results on four different maps for which expert demonstrations are generated from a Finite-Machine Tree-search method (a competitive method in this environment). I’m slightly confused by the plots and the significant drops in performance once the curriculum is finished and agent encounters the start position of the demonstration trajectory (epoch 250). Is this affected by the schedule of the curriculum? Also, the choice of terminating training at epoch 550 is not clear as the method does not seem to have converged yet (the variance is quite high) and would be interesting to observe the dynamics of learning as the training proceeds and whether it converges to a stable policy at all. I am also slightly unclear regarding the performance difference between Standard method in Figure 3(c) and 3(d). If the Standard method is still the same baseline, vanilla PPO, why such huge performance difference? In my understanding, only the Uniform and Backplay methods should be affected by the quality of demonstrations? I believe this figure needs more explanation and clarity. I am also not clear on why Standard method is terminated at epoch 450 while other methods are trained until epoch 550. Figure 4 reports results of generalisation to 10 unseen maps but again the choice of terminating training after 550 epochs is not clear to me as the method again does not seem to have converged. 

Overall, the choice of parameters is not well motivated, these include the window size for sampling the start point, the schedule for shifting the start point, batch size (102400 seems large to me and this choice is never explained), horizon (in appendix A3 reported to be 1707 for Maze while in the main text it is reported as 200 steps), termination of training (3500 for Maze, Figure 7, and 550 in Pommerman, Figure 3). 

I commend the authors for honestly reporting their method’s shortcomings such as failure in generalisation, however, I find that the work lacks significance and quality. There is not much novelty in the proposed method and there is a clear lack of comparisons to existing sample efficient LfD techniques such as Generative Adversarial Imitation Learning (GAIL). I believe this paper requires substantial improvements for publication and is not up to the ICLR standards in its current form.
","The sentiment of the review is mixed but leans towards negative. The reviewer acknowledges some positive aspects, such as the honest reporting of the method's shortcomings and the interesting results in the Pommerman environment. However, the overall tone is critical, pointing out significant issues with the methodology, lack of novelty, and insufficient comparisons to existing techniques. Therefore, the sentiment score is -40. The politeness of the language is relatively high. The reviewer uses polite language throughout, such as 'commend the authors,' 'slightly confused,' and 'I believe,' which indicates a respectful tone despite the critical feedback. Thus, the politeness score is 80.",-40,80
"This is a clear and well written paper that attempts to improve our ability to predict in the setting of massive multi-label data which, as the authors highlight, is an increasingly import problem in biology and healthcare. 

Strengths:
The idea of using the hierarchical structure of the labels is innovative and well-motivated. The experimental design and description of the methods is excellent. 

Weaknesses:
Overall the results are not consistently strong and there is a key baseline missing. The approach only seems help in the ""rare label, small data"" regime, which limits the applicability of the method but is still worthy of consideration. 

My biggest reservation is that the authors did not include a baseline where the classes are reweighted according to their frequency. Multilabel binary cross-entropy is very easy to modify to incorporate class weights (e.g. upweight the minority class for each label) and without this baseline I am unable to discern how well the method works relative to this simple baseline.

One more dataset would also strengthen the results, and since I am suggesting more work I will also try to be helpful and be specific. Predicting mesh terms from abstracts would qualify as a massive multilabel task and there is plenty of public data available here: https://www.nlm.nih.gov/databases/download/pubmed_medline.html 

Finally, there is one relevant paper that the authors may wish to consider in their review section: https://www.biorxiv.org/content/early/2018/07/10/365965","The sentiment of the review is generally positive, as indicated by phrases like 'clear and well written paper' and 'innovative and well-motivated.' However, the reviewer also points out some significant weaknesses, such as inconsistent results and a missing key baseline, which tempers the overall positivity. Therefore, the sentiment score is 50. The politeness of the language is high, as the reviewer uses polite and constructive language throughout, such as 'worthy of consideration,' 'I will also try to be helpful,' and 'the authors may wish to consider.' Therefore, the politeness score is 90.",50,90
"The papers proposed to use normalizing flow policies instead of Gaussian policies to improve exploration and achieve better sample complexity in practice. While I believe this idea has not specifically been tried in previous literature and the vague intuition that NF leads to more exploration that helps learning a better policy, the novelty of combining these two seems limited, and the paper does not seem to provide enough justification to using NF policies instead of alternative policy distributions both in theory and in the experiments.

1. About Section 4.2. I believe that the normalizing flow in question would transform the volume of a Gaussian? So there would exist some parameter setting for a flow model that also shrinks volume, thereby resulting in lower variance policies? The arguments would thereby depend heavily on the specific architecture and initialization of the flow model, which is not discussed in detail. 

Also, why is finding a high variance policy better in terms of the trust region argument? Isn't the whole point of using trust region that the new policy should be closer to old policy to prevent performance degradation? I also think that a fair comparison would be compare KL between normalizing flow policies, instead of KL between NF and Gaussian.

2. The TRPO experiments seem wrong -- at least the results don't match what is reported in the ACKTR paper for Reacher and InverseDoublePendulum envs -- there the TRPO policy at least learns something. Also TRPO in general does not perform as bad as it may seem, see ""Deep RL that matters"" paper by Henderson et al. Maybe this is because of using OpenAI baselines code which seems to have worse TRPO performance.

There is also no experiments on ACKTR on the small Mujoco tasks (even in the Appendix), which seems to be a rather big oversight given the authors have already done even harder tasks for ACKTR + NF.

Moreover I think a fair comparison is to use almost the same architecture for implicit and gaussian, where the only difference is where you sample the noise. For Gaussian with flows, you can first use an MLP to produce deterministic outputs and then use flow to generate the mean actions. Otherwise it is impossible to say whether the architecture or the implicit distribution contributes more to the success.

One could also use truncated Gaussian distributions / Beta distributions / Gaussian + tanh, since Mujoco actions beyond (-1, 1) is treated as -1 or 1, so Gaussian should already be bad. It is unclear whether NF is able to outperform these settings. 

Minor points:

- Fix citations. Please use \citep throughout.
- Is Equation (6) correct? Seems like \Sigma_i should be the inverse of g_i(\epsilon)? Also this is the ""change of variables formula"" not ""chain rule"".
- Why is normalizing flow not part of the background?
- Add legends in Figure (1)
- Figure 2(c), I believe with max entropy you could already obtain diverse ant trajectories?
- I believe in the context of generative models, ""implicit"" typically means the case where likelihood is not tractable? Here the likelihood is perfectly tractable.","The sentiment of the review is moderately negative. The reviewer acknowledges the novelty of the idea but criticizes the lack of justification and detailed discussion. The reviewer also points out several issues with the experiments and comparisons, indicating a general dissatisfaction with the paper's execution. Therefore, the sentiment score is -40. The politeness of the language is relatively high. The reviewer uses polite language, such as 'I believe,' 'seems,' and 'please,' and provides constructive feedback without being rude or dismissive. Thus, the politeness score is 80.",-40,80
"This paper proposes a simple method for knowledge distillation. The teacher and student models are matched using MMD objectives, the author demonstrates different variants of matching kernels specializes to previously proposed variants of knowledge distillation.

- The extensive evaluation suggests that the MMD with polynomial kernel provides better results than the previously proposed method.
-  It is interesting to see that MMD based transfer has more advantage on the object detection tasks.
- Can the author provides more insights into the behavior of different kernels, for example visualizing, the gradient map might help us to understand why certain kernel works better than another one?
- Did you consider translation invariance or other spatial properties when designing your kernels?

In summary, this is an interesting paper with good empirical results. The technique being used generalization is quite straightforward, but the paper also includes a good amount of discussion on why the proposed approach could be better and I think that really helps the reader.
","The sentiment of the review is generally positive. The reviewer acknowledges the simplicity and effectiveness of the proposed method, and appreciates the extensive evaluation and interesting findings, particularly in object detection tasks. The sentiment score is therefore 80. The politeness of the language is also high, as the reviewer uses polite requests and constructive feedback, such as 'Can the author provide more insights' and 'Did you consider,' which indicates a polite tone. The politeness score is therefore 90.",80,90
"[Relevance] Is this paper relevant to the ICLR audience? yes

[Significance] Are the results significant? somewhat

[Novelty] Are the problems or approaches novel? reasonably

[Soundness] Is the paper technically sound? yes, I think. I did not thoroughly check the equations.

[Evaluation] Are claims well-supported by theoretical analysis or experimental results? somewhat

[Clarity] Is the paper well-organized and clearly written? yes, except the experiments

Confidence: 2/5

Seen submission posted elsewhere: No (but I did find it on arXiv after writing the review)

Detailed comments:

In this work, the authors propose a new type of memory cell for RNNs which account for multiple types of time series. In particular, the work combines uniformly sampled observations (“normal” RNN input), time-decaying observations, non-decaying observations which may change, and static features which are not time-dependent. Empirically, the proposed approach outperforms an RNN with TLSTM cells in some cases.

=== Comments

I found the proposed approach for incorporating the different types of time series reasonable. This work definitely leans heavily on ideas from TLSTM and others, but, to the best of my knowledge, the specific combination and formulation is novel, especially concerning the “non-decaying time series” observations.

 However, I had difficult coming up with an example of a “static decay feature”. It would be helpful to give a concrete example of one of these in the main text. (It is also not clear to me why the difference in time between the time of the last event in a sequence and the prediction time for that sequence would be considered a “static decay” feature rather than just a “static” feature.)

My main concern with the paper is that the experimental design and results are not especially easy to follow; consequently, they are not as convincing as they might be. First, the sparsity mechanism is rather simple. In many domains (e.g., the medical domain considered in several of the cited papers), missingness is non-uniform and is often meaningful. While “meaningfulness” may be difficult to simulate, burstiness (non-uniformity) could be simulated. Second, for the groups, it is not clear whether all combinations of, e.g., 2 (informative) feature were sparsely sampled or if only one group of 2 was chosen. If the former, then some measure of variance should be given to help estimate statistical significance. Third, the particular classification task here is, essentially, forecasting one of the input variables. While that is certainly a relevant problem, many other time series classification or regression problems are not tied so directly to observations. It is not clear if these results are relevant in that setting.

=== Typos, etc.

The plots and figures in the paper are very difficult to read. Larger versions, or at least versions with increased fonts, should be used.
","The sentiment of the review is moderately positive. The reviewer acknowledges the relevance, novelty, and technical soundness of the paper, although they express some reservations about the significance and clarity of the experimental results. The sentiment score is therefore 30. The politeness of the language used is high; the reviewer provides constructive feedback and suggestions for improvement without being dismissive or harsh. The politeness score is 80.",30,80
"This paper shows that gradient descent mostly happens in a tiny subspace which is spanned by the top eigenvectors of the Hessian. Empirical results are shown to support the claim. This finding is interesting and provides us some insights to design more efficient optimization algorithms. Overall, this paper is interesting and easy to follow. 

The experiments in section 2 do a decent job supporting the claim that gradient descent happens in a tiny subspace and the subspace is mostly preserved over long periods of training. However, I would like to add a couple more points to the discussion: 

- It's not surprising that the magnitude of gradient is larger in the high curvature directions, which means that the learning would always first happen in top subspace if the learning rate is small enough. It would be interesting to tune the parameter of learning rate to see if the phenomena would occur across different learning rate (especially large learning rate).
- The argument of gradient descent happening in a tiny space is quite obvious if the Hessian has only a few large eigenvalues. Therefore, it would be interesting to discuss the spectrum of the Hessian a little bit.
- Contrary to plain gradient descent, natural gradient is able to learn low curvature directions (small eigenvalues). It would be interesting to show some experiments with natural gradient methods.

Following section 2, the authors give a toy model to further backup their claims. However, I find the example is too restrictive and may not explain why the subspace would be preserved over the training. If I understand right, the loss function of the toy model is convex and Hessian is a constant over time. For this kind of toy model, the Hessian (or equivalently the Fisher matrix) only depends on the input distribution, so it's easy to see that the Hessian would be low-rank and preserved throughout the training. However, neural networks are highly non-convex, so it's unclear to me whether the implications of the toy model would generalize. I encourage the authors to analyze more complicated models. 

To summarize, I think this paper is interesting and well-written. However, it lacks convincing explanation why the subspace would preserve over the training (to me, it's more interesting than the point that gradient descent happens in tiny subspace). Anyway, it is not completely reasonable to expect all such possible discussions to take place at once. ","The sentiment of the review is generally positive, as the reviewer describes the paper as 'interesting and easy to follow' and acknowledges that the experiments do a decent job supporting the claims. However, the reviewer also points out several areas for improvement and additional discussion, which tempers the overall positivity. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer uses phrases like 'I would like to add,' 'it would be interesting,' and 'I encourage the authors,' which are respectful and constructive. Therefore, the politeness score is 80.",60,80
"This paper gives a new algorithm for learning a two layer neural network which involves a single convolutional filter and a weight vector for different locations. The algorithm works on any symmetric input data. The techniques in this paper combines two previous approaches: 1. the algorithm Convotron for learning a single convolutional filter (while the second layer has fixed weight) on any symmetric input distributions; 2. non-convex optimization for low rank matrix factorization.

The main observation in the paper is that if the overlap in the convolutions is not large (in the sense that each location of the convolution has at least one input coordinate that is not used in any other locations), then the weight that corresponds to the non-overlapping part and the weights in the second layer can be computed by a matrix factorization step (the paper gives a way to estimate a gradient that is similar to the gradient for a linear neural network, and then the problem is very similar to a rank-1 matrix factorization). After this step, we know the second layer and the algorithm can generalize the previous Convotron algorithm to learn the full convolutional filter.

This is an interesting observation that allows the algorithm to learn a two-layer neural network. On the other hand this two layer neural network is still a bit limited as there is still only one convolutional filter, and in particular there is only one local and global optimum (up to scaling the two layers). The observation also limited how much the patches can overlap which was not a problem in the original convotron algorithm. 

Overall I feel the paper is interesting but a bit incremental.","The sentiment of the review is moderately positive. The reviewer acknowledges the interesting observation and the potential of the algorithm to learn a two-layer neural network. However, they also point out the limitations and incremental nature of the work. Therefore, the sentiment score is 30. The language used in the review is polite and constructive, with no harsh or rude comments. The reviewer provides their critique in a respectful manner, so the politeness score is 80.",30,80
"Summary
=======
This paper introduces spread divergences. Spread divergences are obtained by taking the divergence between smoothed/noisy versions of two distributions. A spread divergence between two distributions of non-overlapping support can be defined even when the corresponding divergence is not. The authors discuss conditions under which the data generating process can be identified by minimizing spread divergences and apply spread divergences to the examples of PCA, ICA, and noiseless VAE.

Review
======
With a lot of papers focusing on generative modeling, divergence minimization is of great relevance to the ICLR community. Adding noise to distributions to ensure overlapping support is intuitive and has been used to stabilize training of GANs, but I am unaware of any work focusing on questions of identifiability and efficiency. I especially like the example of slowing EM in ICA with small noise. Here, some empirical results are lacking which analyze the speed/correctness of the identification of parameters for various choices of divergence/model noise. These would have greatly enhanced the paper. Instead, the available space was used to show model samples, which I find less helpful.

In Section 3.2 and Section 6 the authors argue that choosing noise which maximizes the spread divergence is optimal or at least preferable. This seems counterintuitive, given that the point of the noise was to make the distributions more similar. Please elaborate on why maximizing the divergence is a good strategy.

Minor
=====
The paper seems hastily written, with some grammar issues, typos, and sloppy use of LaTeX, e.g.:

– ""-\log"" instead of ""\log"" in definition of KL divergence in the introduction
– ""Section 2.2"" not ""section(2.2)"", ""Equation 24"" not ""equation(24)""
– ""model (Tipping & Bishop, 1999)"" instead of ""model Tipping & Bishop (1999)""
– ""\mid"" instead of ""|""
– ""x"" instead of ""y"" in Equation 28

Please provide a reference for the EM algorithm of ICA.","The sentiment of the review is generally positive, as the reviewer acknowledges the relevance of the paper's topic to the ICLR community and appreciates the intuitive approach of adding noise to distributions. However, the reviewer also points out some significant areas for improvement, such as the lack of empirical results and the need for further elaboration on certain arguments. Therefore, the sentiment score is 40. The politeness of the language is high, as the reviewer provides constructive feedback and specific recommendations without using harsh or rude language. The reviewer also uses polite phrases such as 'please elaborate' and 'I especially like,' which indicates a respectful tone. Therefore, the politeness score is 80.",40,80
"This paper proposes an effective attack technique for the widely used optical flow based classification models in white-box and black-box settings. The most interesting result is on the sparsity and frame salience, which could have a lot of applications. But the main idea is to transfer the standard attack techniques from image to video domain. I have some concerns as below. 

1. Page 3, ""...while the temporal stream alone achieves 83.7%. Thus, if the motion stream can be fooled, the entire model is compromised.""

This statement is not exactly correct. Motion stream is better on UCF101 and HMDB51 dataset, which are two medium scale action recognition dataset. On other large-scale datasets like Sports1M, Kinetics, ActivityNet, etc., motion stream performs much worse than spatial stream. Hence, the motivation of the paper is unclear. Especially for real-world applications, due to real-time requirement, people usually just use the spatial stream. Hence, the current flow attack setting has limited usage. It is very important to show attack results on spatial stream as well. 

2. For FlowNet2, authors use gradient of the loss wrt the input images. However, FlowNet2 is a very large network consisting of 5 FlowNets. I am curious what the gradients will look like after the long back-propagation. Can authors comment on this by drawing a figure of magnitude distribution of gradients in the very early layers? 


Due to limited novelty, I recommend an initial rating of 5. 



","The sentiment of the review is moderately positive, as the reviewer acknowledges the effectiveness of the proposed attack technique and its interesting results on sparsity and frame salience. However, the reviewer also expresses concerns about the motivation and applicability of the paper's main idea, which slightly tempers the overall sentiment. The politeness of the language used is high, as the reviewer provides constructive feedback and specific recommendations without using harsh or dismissive language.",30,80
"The primary purpose of this paper, from what I understand, is to show that fake samples created with common generative adversarial network (GAN) implementations are easily identified using various statistical techniques. This can potentially be useful in helping to identify artificial samples in the real world.

I think that the reviewers did an excellent job of probing into different statistical perspectives, such as looking at the continuity of the distribution of pixel intensities and the various higher moments of spectral data. I also must applaud the fact that they did not relegate themselves to image data, but branched out to speech and music data as well.

One of the first findings is that, with MNIST and CIFAR, the pixel intensities of fake samples are noticeably different when viewed from the perspective of a Kolgomorov-Smirnov Test or Jensen-Shannon Divergence comparison. This is an interesting observation, but less so than it would be if compared to something such as a variational autoencoder (VAE), which fits a KL distribution explicitly. IWGAN and LSGAN are using different metrics in their loss functions (such as Wassertein and least squares), and thus the result is not surprising or novel. I think if the authors had somehow shown how they worked their metrics into IWGAN or LSGAN to achieve better results, this could have been interesting.

Another observation the authors make is about the smoothness of the GAN distributions. This may not be so easily wrapped into the loss function, but it seems easily remedied as a post-processing step, or perhaps even a smoothing layer in the network itself. Nevertheless, this is an observation that I have not seen discussed in the literature so there is merit to at least noting the difference. It is confusing that on page 4, the authors state that they hypothesized that the smoothness was due to the pixel values themselves, and chose to alter the distribution of the original pixels in [0,1]. However, they state that in Figure 5, the smoothness remained ""as expected."" Did the authors misspeak here?

I found the music and speech experiments very interesting. The authors note that the synthetic Bach chorales, for instance, introduce many transitions that are not seen in the training and testing set of real Bach chorales. This, again, is interesting to note, but not surprising as the authors are judging the synthetic chorales on criteria for which they were not explicitly optimized. I do not believe these observations to be paper-worthy by themselves. However, the authors I believe have a good start on creating papers in which they specifically address these issues, showing how they can create better synthetic samples by incorporating their observations.

As to the writing style, there are many places where the writing is not quite clear. I would suggest getting an additional party to help proofread to avoid grammatical mistakes. I do not believe that the mistakes are so egregious as to impede understanding. However, it could distract from the importance on the authors future innovations if not corrected.

One last note. The title of the paper is ""TequilaGAN: How to Easily Identify GAN Samples."" This makes it seem as if the authors were introducing another type of GAN, like LSGAN or DCGAN. However, they are not. As a matter of fact, nowhere else in the paper is the word ""TequilaGAN"" mentioned. This title seems a bit sensational and misleading.

In the end, although I did find this paper to be an interesting read, I cannot recommend it for publication in ICLR.

----

Edit - November 29, 2018: Increasing my rating from a 4 to a 5 after discussion with the authors. Though their insights are not unknown, I think the authors are right in the fact that this is not explicitly discussed, at least not in the peer-review research with which I am familiar. But I don't think this by itself merits an ICLR publication.","The sentiment of the review is generally positive, as the reviewer acknowledges the interesting aspects of the paper and the good start the authors have made. However, the reviewer also points out several areas for improvement and ultimately does not recommend the paper for publication, which tempers the overall positivity. Therefore, the sentiment score is 20. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, offering suggestions for improvement without being harsh or dismissive. Therefore, the politeness score is 80.",20,80
"In this paper, the authors investigate the accuracy-efficiency tradeoff for neural language models. In particular, they explore how different compression strategies impact the accuracy (and flops), and more interestingly, also how it impacts the power use for a RaspberryPi. The authors consider the QRNNs and SRUs for this purpose and use standard datasets for their analysis. I am torn about this paper. On one hand, I feel that the analysis is interesting, thoughtful and detailed; the power usage statistics bring a different perspective to the compression community. The section on inference time pruning was especially interesting to read. On the other hand however, there is limited novelty in the setup. The authors use standard, well known, compression algorithms on common neural language modeling architectures and datasets and use out-of-the-box tools for their ultimate analysis. Further, the paper needs additional work before it can be accepted in my opinion. I detail my arguments below:

- The authors begin by discussing SwiftKey and similar apps but I'm not sure if its clear that they use neural language modeling as the backend. Do the authors have a source to validate this claim?
- Knowledge distillation is another algorithm that has been found to be quite competitive in compressing models into smaller versions of themselves. Have the authors experimented with that? 
- The R@3 is an good metric but I suggest that the authors look at mean reciprocal rank (MRR) instead. This removes the arbitrary-ness of ""3"" while ensuring that the metric of interest is the accuracy and not probability of being correct (perplexity). 
- Can you comment on the sensitivity of the results to the RPi frameworks? For instance, the RPi deployment tools, architecture, and variance in the predictions? 
- Along the same line, I'm curious how generalizable the RPi results are for other computing architectures. For those of us who are not experts on hardware, it would be nice to read about whether similar tradeoffs will exist in other architectures such as mobile phones, GPUs or CPUs. 
- Could the authors add some meta-analysis about the results? If the perplexity goes up as a consequence of compression, what kinds of tokens it that localized to? Is it primarily rare words that the model is less confident about, or are the probabilities for most words getting skewed?
- Finally, I feel that such an exploration will catch on only if the tools are open-sourced and made easy to replicate/use. If there were a blog or article summarizing the steps needed to replicate the power measurement (including sources from where to buy the necessary hardware), more people would be inclined on adding such an analysis to future neural language modeling work. 

I am willing to revisit my rating, as necessary, once I read through the rebuttal. 


UPDATE:

After reading the rebuttal, I am increasing my score to 6. The authors alleviated some of my concerns but my major concerns about their novelty and the impact of their results remains. ","The sentiment of the review is mixed but leans slightly positive. The reviewer acknowledges the interesting and thoughtful analysis, particularly the power usage statistics, but also points out the limited novelty and the need for additional work. Therefore, the sentiment score is 20. The politeness of the language is quite high. The reviewer uses polite language throughout, such as 'I suggest,' 'I'm curious,' and 'I feel,' and offers constructive feedback without being harsh or dismissive. Therefore, the politeness score is 80.",20,80
"The authors argue that catastrophic forgetting may cause mode collapse and oscillation, and propose a novel plug-and-play  regularizer that can be applied to a variety of GANs' training process to counter catastrophic forgetting of the discriminator. The regularizer is a clever adaption of EWC and IS into the context of GAN training. With the authors' formulation, this regularizer will account for the discriminator's parameter from all previous ""tasks"" (snapshots taken at certain iterations) with extra memory budget of only one set of parameters, while assigning higher regularization strengths to parameters learned from recent tasks. Experiments demonstrate such regularizer improves GAN models including DCGAN, SN-DCGAN, WGAN-GP on image generation tasks and textGAN on text generation tasks.

Pros:
The paper is well-written. The formulation of online memory and controlled forgetting are clever, giving rise to the adaption of EWC and IS as a practical regularizer to overcome the problem of catastrophic forgetting in GANs. The experiments also demonstrate the regularizer is superior than historical averaging and SN on the synthetic dataset, and it is able to improve multiple GAN models in both image and text generation tasks.

Cons/Suggestions:
1. Although I can see the method is working, the empirical evidence to support ""mode oscillation"" is not strong enough for me. I think in order for continual learning to make perfect sense, mode oscillation should be an obvious issue for GANs; otherwise, we probably don't need remembering the history, as the generator is probably evolving towards the right direction even in the vanilla approach. Still, since there have been several papers showing history is important, it should be helpful in some sense. In Figure 1, I cannot tell whether in (d), the generator returned to the previous space (probably refers to (a)). Even the centers of mass of (a) and (d) look different for me. Figure 2 (left) only shows the distribution of generated data is changing as the training proceeds in vanilla GANs, since few of them (some shallow blue lines) have low peaks in previous datasets. If the mode oscillates and the generator returns to previous state, there should at least be another peak along the line, which is missing in curves on later datasets (darker blue ones). (I guess I have understood this figure correctly, but Figure 2 seems horizontally flipped to me. Since you are testing on previous fake datasets, and the accuracy should drop on previous datasets; however, the accuracy drops on later datasets in the figure.)

2. I doubt the authors may not have tried enough sets of hyper parameters for baseline models. In table 1, the variance of GAN, GAN + l2 weight and GAN + SN are significantly higher than the others. I don't think with l2 weight regularizer, the model will be much more unstable than the authors' approach.

3. The authors didn't give the results of their regularizer with LeakGAN on text generation. Currently their model has lower test BLEU than LeakGAN, which indicates lower fluency, but its self BLEU is lower than LeakGAN, which indicates higher diversity. It would be much better if the proposed method can surpass LeakGAN on both metrics.

4. Using inception score on mixture of eight Gaussians may not make much sense, if they are using the ImageNet pre-trained model, since such a model is not trained to fit this distribution. Still, the author has reported symmetric KL. 

5. The authors did not specify their inception score on real Celeb-A and CIFAR10 images. 


Overall, I tend to accept this paper for its contribution on methods. It would be even better if my concerns could be addressed.

Edit: after seeing the review of Reviewer 3, I find the proposed method seems to be the same as Online EWC and I have downgraded the rating. The authors should address these concerns.","The sentiment of the review is generally positive, as the reviewer acknowledges the cleverness of the proposed regularizer and its effectiveness in improving GAN models. However, the sentiment becomes more neutral to slightly negative towards the end, especially after the reviewer notes the concerns raised by Reviewer 3. Therefore, the sentiment score is 20. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, even when pointing out weaknesses and suggestions for improvement. Therefore, the politeness score is 80.",20,80
"This paper gives various PAC-Bayesian generalization guarantees and some
empirical results on parameter perturbation in training using an algorithm
motivated by the theory.

The fundamental issue addressed in this paper is whether parameter
perturbation during training improves generalization and, if so, what
theoretical basis exists for this phenomenon.  For continuously
parameterized models, PAC-Bayesian bounds are fundamentally based on
parameter perturbation (non-singular posteriors).  So PAC-Bayesian
theory is naturally tied to parameter perturbation issues.  A more
refined question is whether the size of the perturbation should be
done on a per-parameter bases and whether per-parameter noise levels
should be adaptive --- should the appropriate noise level for each
parameter be adjusted on the basis of statistics in the training data.
Adam and RMS-prop both adapt per-parameter learning rate eta_i to be
proportional to 1/((E g_i^2) + epsilon) where E g_i^2 is some running
estimate of the expectation over a draw of a training point of the
square of the gradient of the loss with respect to parameter i.  At
the end of the day, this paper, based on PAC-Bayesian analysis,
proposes that a very similar adaptation be made to per-parameter noise
during training but where E g_i^2 is replaced by the RMS value \sqrt{E
g_i^2}.  It seems that all theoretical analyses require the square
root --- the units need to work.  A fundamental theoretical question,
perhaps unrelated to this paper, is why in learning rate adaptation the
square root hurts the performance.

This paper can be evaluated on both theoretical and empirical grounds.
At a theoretical level I have several complaints.  First, the
theoretical analysis seem fairly mechanical and without theoretical
innovation. Second, the analysis obscures the prior being used (the
learning bias). The paper first states an assumption that each
parameter is a-priori taken to be uniform over |w_i| <= \tau_i and the
KL-divergence in the PAC-Bayes bound is then log tau_i/sigma_i where
sigma_i is the width of a uniform posterior over a smaller interval.
But later they say that they approximate tau_i by |w_i| + kappa_i with
kappa_i = \gamma |w_i| + epsilon.  I believe this works out to be
essentially a log-uniform prior on |w_i| (over some finite range of
log |w_i|).  This seems quite reasonable but should be made explicit.

The paper ignores the possibility that the prior should be centered at
the random initialization of the parameters.  This was found to be
essential in Dziugaite and Roy and completely changes the dependence
of k_i on w_i.

Another complaint is that the Hoefding bound is very loose in cases
where the emperical loss is small compared to its upper bound.  The
analysis can be more intuitively related to practice by avoiding the
rescaling of the loss into the interval [0,1] and writing expressions
in terms of a maximum bound on the loss L_max.  When hat{L} << L_max
(almost always the case in practice) the relative Chernoff bound is
much tighter and significantly alters the analysis.  See McAllester's
PAC-Bayesian tutorial.

The theoretical discussion on re-parameterization misses an important
point, in my opinoin, relative to the need to impose a learning bias
(the no-free-lunch theorem).  All L_2 generalization bounds can be
interpreted in terms of a Gaussian prior on the parameters.  In all
such cases the prior (the learning bias) is not invariant to
re-parameterization.  All L_2 generalization bounds are subject to the
same re-parameterization criticism.  A prior tied to a particular
parameterization is standard practice in machine learning for in all
L_2 generalization bounds, including SVMs.  I do think that a
log-uniform prior (rather than a Gaussian prior) is superior and
greatly reduces sensitivity to re-parameterization as noted by the
authors (extremely indirectly).

I did not find the empirical results to very useful.  The value of
parameter perturbation in training remains an open question. Although
it is rarely done in practice today, it is an important fundamental
question. A much more thorough investigation is needed before any
conclusions can be drawn with confidence. Experimentation with
perturbation methods would seem more informative than theory given the
current state of the art in relating theory to practice.
",The sentiment of the review is mixed but leans towards the negative side. The reviewer acknowledges the relevance of the topic and the connection to PAC-Bayesian theory but expresses several significant complaints about the theoretical analysis and empirical results. The sentiment score is -40 because the review contains more criticism than praise. The politeness of the language is relatively high; the reviewer uses formal and respectful language even when pointing out flaws. The politeness score is 60 because the reviewer maintains a professional tone throughout the review.,-40,60
"This paper presented a novel approach for modeling a sequence of characters as a sequence of latent segmentations. The challenge here was how to efficiently compute the marginal likelihood of a character sequence (exponential number different of segmentations). The author(s) overcame this by having a segment generation process independent from the previous segment (only depends on a sequence of characters). The inference is then required a forward algorithm. To generate a segment, a model can either select a lexical unit (pre-processed from a training corpus) or generate character by character. 

On the experiments, the author(s) showed that the model recovered semantical segmentation on many word segmentation dataset (including phonemes). The lexical memory and the length regularization both contribute significantly as shown in the analysis. The language modeling result (BPC) was also competitive with LSTM-based LMs. 

I think the overall model is interesting and well motivated, though it is a bit disappointing that the author(s) needed to use an extra regularizer to constraint the segment length (from the lexical memory?). Perhaps, the way they build a lexical memory should be investigated further. The experiment should also show an evidence that SNLM(+memory, -length) was overfitted as claimed.

The validation and test dataset have been modified to remove ""samples"" containing OOV characters. How many have been removed? The author(s) could opt for an unknown character similar to many word-level datasets.

The use of word segmentation data was quite clever, but this also downplayed other work that is not aimed to recover human-semantic segmentations. For example, a segment ""doyou"" on page 10 might be considered as a valid segmentation since it appears a whole lot. HM-LSTM though did poorly on the segmentation task but performed rather well on PTB LM task, but the author(s) decided to omit this comparison.

Some minor comments:
- A typo in the introduction ""... semi-Markov model. The the characters inside ..."".
- Eq 3 is a bit hard to follow. Perhaps, a short derivation should be presented.
- Is it possible to efficiently generate a sequence?

[Updated after reconsidering other reviews]
Although this paper misses some related work and comparison models, I think it still has a valid contribution to language modeling: a character-level language model that produces plausible word segmentation.","The sentiment of the review is generally positive, as the reviewer acknowledges the novelty and interesting aspects of the model, as well as its competitive performance. However, there are some criticisms and suggestions for improvement, which slightly temper the overall positivity. Therefore, the sentiment score is 60. The politeness of the language is quite high, as the reviewer uses polite phrases such as 'I think,' 'perhaps,' and 'could,' and provides constructive feedback without being harsh or dismissive. Thus, the politeness score is 80.",60,80
"This is very solid work and the framework allows one to plug-in existing complexity measures to provide complexity upper bounds for (some) DNNs. The main idea is to rephrase an empirical risk minimization problem in terms of a binary optimization problem 
using a discretization of the continuous variables. Then this formulation is used to provide a as a moderate-sized linear program of its convex hull. 

In my opinion, every paper that provides insights into the complexity and generalization of deep learning is an important contribution. Moreover, the present paper is based on 
a recent insight of the authors, i.e., it is based on solid grounds. However, it would have been nice to also show some practical insights. The main take-aways message is that we need exponential time. Is this practical for networks with with millions of parameters? Or does this imply that deep learning is hopeless (in theory)? To be fair, the authors touch upon this in the conclusions, but only 1-2 sentences. This discussion should be extended. Nevertheless, I agree that the bridge built is important and may indeed trigger some very important future contributions. 

The authors should, however, also review other work on linear programming for deep networks coming from the machine learning community such as 

Brandon Amos, Lei Xu, J. Zico Kolter:
Input Convex Neural Networks. 
ICML 2017: 146-155

Given the background of the average ICLR reader, the authors should also introduce (at least the intuitions) improper and proper learning setups in the introduction before using them.   This also holds for other terminology from complexity theory. Indeed, the authors cannot introduce/review all complexity theory. However, they should try their best and fill the rest by a reference to an introductionary book or directly to the appendic. Without, while important for the ICLR community, the authors run the risk that the paper would better be suited by a learning theory venue. ","The sentiment of the review is generally positive, as the reviewer acknowledges the solid work and the importance of the contribution to the field of deep learning complexity and generalization. The sentiment score is 70 because while the reviewer appreciates the work, they also point out areas for improvement, such as the need for practical insights and a more extended discussion on the implications of the findings. The politeness score is 80 because the reviewer uses polite language throughout the review, offering constructive criticism and suggestions for improvement without being rude or dismissive.",70,80
"Summary: The paper suggests a method to reducing the space consumption of training neural nets, in exchange for additional training time. The method stores in memory only a subset of the intermediate tensors computed in the forward step, and then in the backward step it re-computes the missing tensors as they are needed by interpolating forward (again) between the stored tensors. The paper also gives a combinatorial algorithm for choosing which tensors to store on a given computation DAG annotated with vertex costs.

Evaluation: I generally like the paper. The proposed method is simple and straightforward, and seems to lead to a noticeable improvement in space usage during training.
The part related to decomposing a DAG into ""close sets"" looks like it might overlap with existing literature in graph theory; I don't have concrete references but the authors may want to check this. The ACG solver algorithm looks somewhat wasteful in terms of the degree of the polynomial running time, but since actual computation graphs are tiny in computational terms, I guess this is not really an issue.
One point not addressed in the experiments is what is the overhead incurred in training time by the two space-efficient methods over usual training. I suppose one expects the training time be less than twice longer, since the re-forwards amount to one additional complete forward step per forward-backward pair.
Another thing that would be interesting to see is the actual stored vertices (V_R) that were chosen empirically for the rows in table 1 (or at least some rows). Since the computational graphs of the tested networks are small, and some are well-known, I imagine it should doable (though this is merely a suggestion).

Conclusion: The paper is simple and looks reasonable, with no major issues that I could detect.","The sentiment of the review is generally positive, as indicated by phrases like 'I generally like the paper' and 'The proposed method is simple and straightforward, and seems to lead to a noticeable improvement in space usage during training.' The reviewer also mentions that the paper is 'simple and looks reasonable, with no major issues that I could detect.' However, there are some minor criticisms and suggestions for improvement, which slightly temper the overall positivity. Therefore, the sentiment score is 70. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, such as 'the authors may want to check this' and 'this is merely a suggestion.' There are no rude or harsh comments, so the politeness score is 90.",70,90
"I read the other reviewers' comments as well as the rebuttal. I think that the other reviewers make a number of valid points, especially with regards to the theoretical analysis of the paper. Therefore, I do not feel confident in championing this paper. 

PS: I am downgrading my confidence in my evaluation.

---

Paper 93 proposes an empirical evaluation of the memorization properties of convnets. More specifically, it evaluates three aspects:
-	First it evaluates whether convnets can learn to distinguish images from two different sets by training a binary classifier. The conclusion is that, indeed, deep convnets can learn to make such a decision. As could be guessed from intuition, the larger the capacity of the network and the smaller the size of the sets, the higher the accuracy.
-	Second, it evaluates whether we can detect that a group of samples of a dataset was used to train a model. For this purpose, it is proposed to compute the distribution of maximal activation scores of the output softmax layer and to make use of the Kolmogorov-Smirov distance between the cumulative distributions. It is shown experimentally that one can detect (even partial) leakage with such a technique.
-	Third, it evaluates whether we can detect that a single images was used to train a convnet. Two simple techniques are proposed. The first one considers that a sample is part of the training set if it correctly classified. The second one considers that a sample is part of the training set if its loss is below a threshold. It is shown experimentally that one can make such a decision with moderate accuracy.

On the positive side:
-	This is a topic that should be of broad interest to the ICLR community.
-	The paper is generally well-written.
-	The experiments are reported on large-scale datasets on high-capacity networks which is more realistic than small-scale settings.

On the negative side:
-	It is unclear whether the data augmentation techniques is applied only at training time or also at test time. In other words: at test time, do you present the original images only or transformed images too?
-	In section 4, it is unclear why only the maximal activation of the softmax layer is used to characterize a sample? Why not considering the full distribution that should contain richer information? Why just focusing on the output layer and why not using the info available at intermediate layers?
-	Section 5 is somewhat less clear than the previous sections. The authors should more clearly define what the private, public and evaluation sets are, right from the beginning. The purpose of the public set is explained only in section 5.2.
-	The experimental results of section 5.2 are somewhat disappointing. Even with no data augmentation, and even with the original networks, membership can only be assessed with a 90% accuracy. Results are much lower in less favorable cases, sometimes close to random (see last line of Table 3). This seems to be too low to be of practical use. This might be because the Bayes and MAT attacks are too simplistic. Again, why not using the distribution of the outputs of all layers? Why focusing only on the output of the last layer?
","The sentiment score is derived from the overall tone and content of the review. The reviewer acknowledges the positive aspects of the paper, such as its relevance to the community, the quality of writing, and the realistic experimental setup. However, the reviewer also points out several significant weaknesses and expresses a lack of confidence in championing the paper. This mixed feedback results in a sentiment score of -20, indicating a slightly negative sentiment. The politeness score is based on the language used throughout the review. The reviewer uses polite and constructive language, even when pointing out weaknesses. There are no instances of rudeness or harsh criticism, leading to a politeness score of 80.",-20,80
"To generate a sequence of high-level visual elements for recreation or translation of images, the authors propose differentiable ""canvas"" networks and ""drawer"" networks based on convolutional neural networks. One of the main ideas is the replacement of the ""canvas"" networks instead of non-differentiable ""renderer"" to end-to-end train the whole model with mean-squared error loss. It seems to be a novel approach to optimize drawing actions. It is reasonable to use separate networks to approximate the behavior of renderer and to fix the parameters of the ""canvas"" networks to maintain the pretrained rendering capability.

Integrating the high-level visual constructs for recreation or translation of images is to eliminate or attenuate visual artifacts and blurriness, as mentioned in the introduction of the paper. Qualitative comparison with the other state-of-the-art methods is shown in Figure 6f; however, it fails to show significant improvement over them. Quantitative results do not include in the comparison, but only for the ablation study to determine the proposing method. Although the paper proposes an interesting approach to enhance an image generation task, the provided evidence is weak to support the argument, which should be useful for their criteria.

Moreover, experimental details fall short to ensure the validity of experiments. How do you split the dataset as train/val/test? Are the reporting figures (L2 loss) from test results? How are the statistics of the datasets you used?

In Related Work, the authors describe ""reinforcement learning methods can be unstable and often depend on large amounts of training samples."" Many RL methods use various techniques to stabilize the learning, and this argument alone cannot be the grounding that the supervised approach is better than RL. Unsupervised learning also needs a large amount of data. What is the point of this paragraph (the second paragraph in Related Work)?


Quality: 
  Figure 1-3 are taking too much space, which might lead to exceeding 8 pages. 

Clarity:
  The experimental procedure is not clear. Please clarify the issues mentioned above. It is not hinder to understand the content; however, the writing can be improved by proof-reading and correcting a few grammatical errors.

Originality and significance:
  Using the differentiable ""canvas"" networks to avoid non-differentiable ""renderer"" is a novel approach as far as I know. 

Pros:
  Differentiable drawing networks are underexplored in our community.

Cons:
  It failed to show the excellency over pixel-wise generation methods and limited to simple visual elements, line drawings or box generations. This work does not explore ""brush strokes"" in paintings.


Minor comments:

- In Related Work, the inline citation should be ""Simhon & Dudek (2004)"" instead of ""(Simhon & Dudek, 2004)"", and this may apply to the others.

- In Figure 2, the Hint should be x_n, the current state, or target image X for regeneration (X' for translation)?

- In 4.1, a typo, ""Out state consists of"" to ""Our state consists of"".","The sentiment of the review is mixed. The reviewer acknowledges the novelty of the approach and its potential significance, which gives a positive sentiment. However, they also point out several weaknesses, such as the lack of significant improvement over state-of-the-art methods, insufficient experimental details, and the need for better clarity and proof-reading. Therefore, the sentiment score is slightly positive. The politeness of the language is high. The reviewer uses polite language throughout, providing constructive criticism and suggestions for improvement without being rude or dismissive.",20,80
"The paper studies failure modes of deep and narrow networks. I find this research extremely valuable and interesting. In addition to that, the paper focuses on as small as possible models, for which the undesired behavior occurs. That is another great positive, too much of a research in DL focuses on the most complex and general cases in my opinion. I would be more than happy to give this paper a very strong recommendation, if not for numerous flaws in presentation. If those get improved, I am very eager to increase my rating. Here are the things that I think need an improvement:
1. The formulation of theorems.
The paper strives for mathematical style. Yet the formulations of the theorems are very colloquial. Expression ""by assuming random weights"" is not what one wants to see in a rigorous math paper. The formulations of the theorems need to be made rigorous and easy to understand, the assumptions need to be clearly stated and all concepts used strictly defined.
2. Too many theorems
9 (!) theorems is way too much. Theorem is a significant contribution. I strongly suggest having 1-2 strong theorems, and downgrading more technical lemmas to a lemma and proposition status.
In addition - the problem studied is really a study of bad local minimas for neural networks. More mentions of the previous work related to the topic would improve the scientific quality additionally, in my opinion.
","The sentiment of the review is generally positive, as the reviewer finds the research valuable and interesting, and expresses eagerness to give a strong recommendation if the presentation flaws are addressed. This indicates a sentiment score of 70. The politeness of the language is also high, as the reviewer uses phrases like 'I find this research extremely valuable,' 'I would be more than happy,' and 'I strongly suggest,' which are polite and constructive. This indicates a politeness score of 80.",70,80
"The paper proposes a data-dependent regularization method which is coupled with softmax loss to train deep neural networks for classification. The paper turns to Orthogonal Low-rank Embedding (OLE) loss for the geometric constraint that one class of data/feature are assumed to reside on a low-rank subspace that subspaces of different classes are orthogonal ideally. The probability in the softmax is then modeled as cosine similarity between data feature and the class-specific subspaces. In this way, geometric loss and softmax loss have the common goal for optimization. Moreover, during training, the geometry enforced on one batch of features is simultaneously validated on a separate batch using a validation loss. The experiments seem to suggest such a model helps avoid overfitting/memorizing noisy training data. The paper reads well and is easy to follow.

However, the paper is limited in technical novelty and practical significance. Here are some concerns -- 

1) The paper only studies one method based on OLE, though it cites the center loss [19]. How does the center loss behave in face of noisy training label? Would it also be able to refuse to fit the noisy training data?

2) As each class has its own (low-rank) subspace, and the rank is reduced by imposing the nuclear norm. It seems that the proposed method is hard to extend to many classes (class number is larger than the dimension)?

3) The datasets in the experiments are quite small in scale and class number. It is not persuasive unless tested on larger scale data or with large class number.

4) The proposed method seems to be limited in deal with discrete labels (e.g., classification), is it easy to extend to continuous target, say regression problems like depth estimation and surface normal estimation?

5) While the authors claim as a main contribution that the proposed GRSVNet is a general framework, it is hard to see how this framework can be used in other tasks other than classification.

6) The experiments are less persuasive. It's better to add the error bar to see the improvement by the proposed method is not due to random initialization. Running time should also be compared, as nuclear norm seems to be time consuming.","The sentiment of the review is mixed. The reviewer acknowledges that the paper reads well and is easy to follow, which is positive. However, the majority of the review focuses on the limitations and concerns regarding the technical novelty, practical significance, and experimental validation of the proposed method. Therefore, the sentiment score is slightly negative. The language used in the review is polite and constructive, as the reviewer provides specific recommendations and questions without using harsh or dismissive language.",-20,80
"To exploit the near neighbor/manifold features, this paper proposes to combine k-nearest neighbors of each training data point into the neural network models.  Specifically, the authors propose two families of models built on the popular sequence to sequence neural network models and memory network models, which mimic the k-nearest neighbors model in model learning. Besides, the final label of the classification task will be learned, a sequence of nearest neighbor labels and a sequence of out-of-sample feature vectors (for oversampling) will be also learned in the same time, similar with the multi-task approaches. Since the proposed models are based k-nearest neighbor calculations, which is time-consuming, they also design an algorithm for the ‘out-of-core’ situation, say load a small portion of data each time to approximately calculate the neighbors. Experiments show that some proposed models work better than baselines in classification and oversampling.
Strong points:
(1) As similar with the multi-task setting, the proposed model can output some side useful results, such as oversampling vectors.
(2) The proposed models work well on the ‘out-of-core’ situation, which shows that the models are robust.
Concerns or suggestions:
(1) The training data $x$ is just one data point, it is not a sequence of data. So the idea to model it in a sequence to sequence setting does not make sense.
(2) K-nearest neighbors are a set but not a sequence. To model them as a sequence is also strange. The i-th nearest neighbor does not necessarily dependent on the i-1-th nearest neighbor. For example, we consider the one-dimensional case, the focus data may lie between its first and second nearest neighbors. In this case, there is no clear sequence dependence from the second neighbor to the first neighbor. 
(3) The experiments are not sufficient. They only compare with some weak baselines, such as KNN. As the classification task, there are many state-of-the-art models. Besides of these standard classification models, we strongly suggest comparing with the previous method, Wang et al. (2017), which also proposes to combine the k-nearest neighbors into memory network models. I am surprised that the authors did not compare with this very related work. In my opinion, the idea of utilizing nearest neighbors as external memory in Wang et al. (2017) makes more senses.
(4) The experimental results of some proposed sub-models (key parts of final models) are even worse than the basic kNN model. I should say that the results are not good enough to support the proposed methods. ","The sentiment of the review is mixed but leans towards negative. While the reviewer acknowledges some strong points of the paper, such as the ability to output useful side results and robustness in 'out-of-core' situations, the majority of the review focuses on significant concerns and suggestions for improvement. These include fundamental issues with the modeling approach, insufficient experiments, and poor performance of some proposed sub-models. Therefore, the sentiment score is -40. The politeness of the language is relatively high. The reviewer uses polite language, such as 'suggest' and 'I am surprised,' and avoids harsh or rude expressions. Thus, the politeness score is 80.",-40,80
"The authors propose a data-dependent dropout variant that produces dropout candidates based on their predictive saliency / relevance. Results are reported for 4 datasets (Cifar10, Cifar100, Caltech256 and UCF101) and 4 different models (CNN-2, AlexNet, VGG16 and VGG19), and suggest an increase in generalization performance over other dropout approaches (curriculum dropout, standard dropout, no dropout), as well as increase in the network's plasticity as measured by some existing metrics from the literature. The authors conclude that Excitation Dropout results in better network utilization and offers advantages for network compression (in the sense of neuron pruning).

Overall I find the idea to be interesting and fairly novel, and commend the authors for the fluid writing style. However, I find key issues with the testing and experiments. Specifically, the lack of confidence bounds for individual results makes it impossible to determine whether the reported incremental improvements are actually significant over those of existing approaches. Likewise, I criticize the choice of methods the authors have chosen to compare against, as several other data-dependent dropout approaches (e.g. Information Dropout) exist that may be conceptually closer (and therefore more comparable) to the proposed approach. I also question the choice of tested network architectures and the placement of the dropout layer.

The paper could be of high significance if all claims in the paper could be backed up by experiments that show the advantage of Excitation Dropout to be not a random effect. I will therefore give a lower score for the paper in its current form, but am willing to revise my rating the major points below are properly addressed.

Pros:
+ novel mechanism to improve dropout, results seemingly superior over other methods
+ achieves better utilization of network resources and achieves robustness to dropping out neurons at test time

Cons:
- results without error bars, unclear if advantage is significant
- did not compare against most relevant competing methods


MAJOR POINTS
Section 2 - The comparison to Moreiro et al. is not entirely clear. A fairer comparison would be with some of the other methods listed which also focus on answering the question of which neurons to dropout, or approaches which determine the dropout policy based on information gained from the data, such as Information Dropout (Achille & Soatto). The authors state that Morerio et al are the state-of-the-art in dropout techniques, however based on the results presented here (Figure 3) it seems to perform just as well as standard dropout. Perhaps there are architecture-specific or data-specific issues? In any case this example undermines the confidence of the claims.

Section 3.2, equation 3 - is there some theoretical underpinning as to how this equation was modelled, or was it chosen simply because it covers the expected corner cases described in paragraph 4 of this section? Also, given the intuition in this paragraph (e.g. p_EB = 1 / N), it is correct to assume this equation models the dropout probability but only for fully connected layers? What about dropout in convolutional layers? Though some previous statements do point to the usage of dropout predominantly for fully connected layers, I feel that this context is missing here and should be explicitly addressed. The caption to e.g. Table 1 seems to imply the authors add a single dropout layer in one of the fully connected layers, however this begs the question as to why this positioning was chosen - why only one dropout layer, and why precisely at that location? The scope of the claims should be adapted accordingly.

Section 4.2 - ""After convergence, ED demonstrates a significant improvement in performance compared to other methods"". If five trained models were used, then some sense of measure of uncertainty should be given throughout. For example, in the Cifar10 results for Figure 3, it is difficult to say whether the marginal improvement from about 80% (standard dropout and curriculum dropout) to about 82% (excitation dropout) is significant or not. Perhaps this would be less of an issue if the authors had worked with e.g. ImageNet, but for these smaller datasets it would definitely be worth to be on the safe side. I highly suspect that statistically speaking (perhaps with the exception of the results on Caltech256), the effects of all of these dropout variants are indistinguishable from each other. I urge the authors to include a measure of the standard deviation / 95% confidence interval across the models that were tested.

The results presented sub-section 4.3 do not justify the claim that the models trained with Excitation Dropout tend to be more informative. Perhaps the definition of ""informative"" should be expanded upon in length. Can the authors show that the alternative paths learned by the models augmented with Excitation Dropout indeed carry complimentary information and not just redundant information?

Figure 5 shows interesting results, but once again begs the question of whether there is any significant difference between standard dropout and curriculum dropout. I encourage the authors to include confidence bounds for each trace. Likewise, there is an inherent bias in the results, in that the leftmost figure compares EB and CD in the context in which EB was trained, i.e. dropping of ""most salient"" neurons. The comparison is one-sided, however, as no results are reported from the context in which CD was trained, i.e. dropping neurons more frequently as training progresses. Comparing these results would bring to light whether the performance boost see in Figure 5 is a function of ""overfitting"" to the training manner or not.
Also, I believe the results for the second column (dropping out least relevant neurons) are misleading. To the best of my understanding, as p_c increases, at some point neurons start to be dropped that actually have high relevance. This could explain why all curves start out similarly and EB slowly begins to stick out - at this point the EB models once again start to be used in the context within which they were trained, in contrast to the other approaches. The authors should perhaps also explicity clarify why this second column gives any more information than the first.



MINOR POINTS

The authors propose Excitation Dropout as a guided regularization technique. Batch normalization is another standard regularization technique which is often compared with dropout. In particular, for deep CNNs, batch normalization is known to work very well, often better than the standard dropout. In the experiments here, to what extent was batch normalization and / or any other widely utilized network regularizers used? Is it possible that the regularizing effect found here actually comes from one of these? I.e. were the models that were not trained from scratch trained with batch normalization? It would be good if more data could be provided for EB vs. other regularizing techniques, if the claim is that EB is a novel regularizer.

Section 3.1 - ""We choose to use EB since it produces a valid probability distribution for each network layer"". Though this is a nice property, were there any other considerations for choosing the saliency method? Recent work (Adebayo et al, ""Sanity Checks for Saliency Maps"") has shown that even some well established saliency techniques are actually independent from both the model and data. As this approach relies heavily on the correctness of EB, I feel that a further justification should be given to validate its use for this scenario other than just based on the type of output it produces.

Section 3.1, equation 2 - more detail and reasoning should be given as to why connections with negative weights are excluded from the computation of the conditional probability, if possible without referring the reader to the EB paper. Why is this justified? Is this probability modelled for a specific activation function? 

The authors do not provide the details of the CNN-2 architecture (even in the appendix) and simply refer to another article. If the majority of the results presented in the paper are based on this network (including a reference made to a specific layer of the network in subsection 4.2) – which is not commonly known – why not to detail the network architecture and save additional effort for the reader?

How are the class-wise training and test images chosen for Caltech256 dataset?

The authors test the CNN-2 architecture on Cifar10 and Cifar100, and AlexNet, VGG16, and VGG19 on UCF101. I feel that at least a couple architectures should be validated with more than a single dataset, or the authors should justify the current matching between architectures and datasets. Table 2 is unclear regarding what models were used for what datasets (caption could be interpreted to mean that VGG16 was also used for Cifar and Caltech, however other statements seem to say otherwise).

""To prove that the actual boost in accuracy with ED is not provided by the choice of specific masks,..."" I suggest that the authors rephrase or explain this sentence in more detail. To the best of my understanding, it is precisely the fact that different masks are used, each reflective of the particular input used to generate the forward activations, that gives boost in performance over ""standard"" dropout methods by identifying salient paths in the network.

Although it is a very important experimental detail, only in the end of sub-section 4.2, it becomes clear in which layers Excitation Dropout was applied. 

Y-axis labels are missing for the left panels in Figure 3.

The authors randomly choose to abbreviate Excitation Dropout as ED in some paragraphs, while write the full form in others.  

Table 2 - It is not clear that the ""Neurons ON"" metric refers to the ""average percentage of zero activations"" explained below. 

Table 2 - How is peak p_EB measured? Is this an average over a set of test images after convergence? If so, I similarly suggest for confidence bounds to be introduced. It would be interesting to compare this to intermediate values (e.g. after every epoch) during training. Same question for entropy of activations and entropy of pEB. This information would be useful for reproducibility.

Table 2 - Where do the delta values in Table 2 come from? If empirically determined, it should be stated explicitly.

Table 2 - In general, because the metrics provided in Table 2 are averages (second paragraph of this Section 4.3), both (to the best of my understanding) across input subsets (e.g. averging results over many test inputs) and models (caption to Table 1), I feel Table 2 in its current form raises confusion given the lack of confidence bounds. I recommend the authors to clarify what type of averaging was done and to introduce e.g. standard deviations across all reported scores. The authors should refrain from using the term ""significantly"" while describing results if no statistical testing was done, or explicitly clarify their usage of this term.

Table 2 - In general, Table 2 reports results on selected metrics which, if the authors' hypothesis is correct, should have a clear trend as training progresses. An interesting idea to explore would be to include an analysis (in the appendix) of how these factors change over the course of the training procedure. Intuitively, it seems plasticity is something that should be learned slowly over time, so if these plots were to reveal something different, it would be indicative that something else is going on.

Figure 4 - Judging heatmaps is difficult as it depends on the visual perception of the reader. Thus, it is difficult to judge whether, as the authors claim, ED is indeed less ""peaky"" than the other alternatives. I suggest that the authors use a perceptually uniform heatmap, and to acompany these figures with e.g. the histogram of the heatmap values. Likewise, it is unclear how the multi-model aspect of the testing plays a role in generating these results. From the 5 originally trained models, how was the model selected that generated these results? Was there averaging of any kind?

Figure 5: the text is too small to be readble

Is ""re-wiring"" the most appropriate term to use to describe what is happening at inference time? Although different paths may be used, the network connections themselves are fixed and thus this is a potential source for confusion.

What do numbers in Table-4 in the appendix represent? Test accuracy? ","The sentiment of the review is mixed. The reviewer acknowledges the novelty and potential significance of the proposed method, which is positive, but also highlights several critical issues with the experimental design and reporting, which is negative. Therefore, the sentiment score is around -20. The politeness of the language is quite high. The reviewer uses polite and constructive language throughout, offering suggestions for improvement and expressing willingness to revise the rating if the issues are addressed. Therefore, the politeness score is 80.",-20,80
"The paper studies discrete time dynamical systems with a non-linear state equation.  They assume the non-linear function is assumed to be \beta-increasing like leaky ReLU. Under this setting, the authors prove that for the given state equation for stable systems with random gaussian input at each time step, running SGD on a fixed length trajectory gives logarithmic convergence.

The paper is well-written and proves strong convergence properties. The deterministic result does not seem very novel and uses the idea of one-point strong convexity which has been studied in various prior works. However the bounding of the condition number of the data matrix is interesting and guarantees are near-optimal. The faster convergence for odd activations is a good observation. Overall, I think the paper is good. I do list some concerns:
Questions/concerns:
- The deterministic theorem (Theorem 4.1) seems similar to Theorem 3 in [1] with SGD instead of GD. Also under the distribution being symmetric, it can be derived from [2] with $k=1$. 
- Can the ideas be extended to other commonly used activations such as ReLUs/Sigmoids? Sigmoids have exponentially small slope near origin.
- The proof seems to rely on the fact that due to the gaussian input added each time step and stable system assumption after a sufficient number of time steps, the input-output pairs will not be highly correlated. So the data is sufficiently uncorrelated taking enough data. What happens if this data at each step is not gaussian?
- In the unstable setting, the solution proposed just samples from different trajectories which by default are independent hence correlation is not an issue, this seems a bit like cheating. 
- In RNNs, the motivation of the work, the hidden vectors are not observed, thus this setting seems a bit restrictive.
- If SGD was performed on only one truncated series, do the results still hold?

Other comments:
- There has been previous work on generalized linear models which work in more general settings like GLMtron [3]. The authors should update prior work on generalized linear models as well as neural networks.
- Typo on Page 2 y_t = h_{t+1} not y_t = h_t.

[1] Dylan J. Foster, Ayush Sekhari, and Karthik Sridharan. Uniform Convergence of Gradients for Non-Convex Learning and Optimization. NIPS 2018.
[2] Surbhi Goel, Adam Klivans, and Raghu Meka. Learning One Convolutional Layer with Overlapping Patches. ICML 2018.
[3] Sham M. Kakade et al. Efficient learning of generalized linear and single index models with isotonic regression. NIPS 2011.


--------------
I would be maintaining the same score. I agree that the paper has nice convergence results that could possibly be building steps towards the harder problem of unobserved hidden states however, there is more work that could be done for unstable systems and possible extension to ReLU and other activations to take it a notch higher. ","The sentiment of the review is generally positive, as the reviewer acknowledges that the paper is well-written and proves strong convergence properties. However, the reviewer also points out that some results are not very novel and raises several concerns and questions about the work. Therefore, the sentiment score is not fully positive but leans towards the positive side. The politeness of the language is quite high; the reviewer uses polite and constructive language throughout the review, even when pointing out concerns and suggesting improvements.",60,90
"This paper presents a new approach to an active learning problem where the idea is to train a classifier to distinguish labeled and unlabeled datapoints and select those that look the most like unlabeled.

The paper is clearly written and easy to follow. The idea is quite novel and evokes interesting thoughts. I appreciated that the authors provide links and connections to other problems. Another positive aspect is that evaluation methodology is quite sound and includes comparison to many recent algorithms for AL with neural networks. The analysis of Section 5.5 is quite interesting.
However, I have a few concerns regarding the methodology. First of all, I am not completely convinced by the fact that selecting the samples that resemble the most unlabeled data is beneficial for the classifier. It seem that in this case just the data from under-explored regions will be selected at every new iteration. If this is the purpose, some simpler methods, for example, relying on density sampling, can be used. Could you elaborate how you method would compare to them? I can see this method as a way to measure the representativeness of datapoints, but I would see it as a component of AL, not an AL alone. What would happen it is combined with Uncertainty and you use it to labeled the points that are both uncertain and resemble unlabeled data? 
Besides, the proposed approach does not take the advantage of all the information that is available to AL, in particular, it does not use at the information about labels. I believe that labels contain a lot of useful information for making an informed selection decision and ignoring it when it is available is not rational.  
Next, I have conceptual difficulties understanding what would happen to a classifier at next iteration when it is trained on the data that was determined by the previous classifier. Seems that the training data is non-iid and might cause some strange bias. In addition to this, it sounds a bit strange to use classification where overfitting is acceptable.
Finally, the results of the experimental evaluation do not demonstrate a significant advantage of the proposed method and thus it is unclear is there is a benefit of using this method in practice. 

Questions:
- Could you elaborate why DAL strategy does not end up doing just random sampling?
- Nothing restrict DAL from being applied with classifiers other than neural networks and smaller problems. How do you think DAL would work on simpler datasets and classifiers?
- How does the classifier (that distinguished between labeled and unlabeled data) deal with very unbalanced classes? I suppose that normally unlabeled set is much bigger than labeled. What does 98% accuracy mean in this case?
- How many experiments were run to produce each figure? Are error bars of most experiments so small that are almost invisible?

Small comments:
- I think in many cases citep command should be used instead of cite. 
- Can you explain more about the paragraph 3 of related work where you say that uncertainty-based approach would be different from margin-based approach if the classifier is neural network?
- Last sentence before 3.1: how do you guarantee in this case that the selected examples are not similar to each other (that was mentioned as a limitation for batch uncertainty selection, last paragraph on page 1)?
- It was hard to understand the beginning of 5.5, at first it sounds like the ranking of methods is going to be analysed.
- I am not sure ""discriminative"" is a good name for this algorithm. It suggested that is it opposite to ""generative"" (query synthesis?), but then all AL that rank datapoints with some scoring function are ""discriminative"".","The sentiment of the review is generally positive, as the reviewer appreciates the novelty of the idea, the clarity of the writing, and the sound evaluation methodology. However, the reviewer also raises several concerns and questions about the methodology and the practical benefits of the proposed approach. Therefore, the sentiment score is not fully positive but leans towards the positive side. The politeness of the language is high, as the reviewer uses polite language, provides constructive feedback, and asks questions in a respectful manner.",60,90
"This paper centers around adding a reward term that, as I understand it, rewards the agent for having seen sequences of rewards that have low variability. This is an interesting idea, however I find the clarity of the main part of the paper (section 4.1, where this new term is defined) quite poor. That section makes several seemingly arbitrary choices that are not properly explained, which makes one wonder if those choices were made mostly to make the empirical results look good or if there are some more fundamental and general concepts being captured there. In particular, I have to wonder where the 100 in the definition of R_H comes from, and also how sigma_max would be determined (it is very hard to get a good intuition on such quantities as an RL practitioner).  

The paper also introduces “hot-wire exploration”, basically trying the same action for a while during the initial stage, which is a nice exploration heuristic for Atari, but I am not sure how generally applicable the idea is beyond the Atari testbed.

In general, I am always a bit wary of experimental results that were obtained as a result of introducing additional hyper-parameters or functional forms. However, the results look pretty good, and the authors do manage to show some amount of hyperparameter robustness, which makes me wish the design choices had been more clearly explained..
","The sentiment score is determined based on the overall tone and content of the review. The reviewer acknowledges the interesting idea and the good empirical results, but also points out significant issues with clarity and unexplained choices, leading to a mixed but slightly positive sentiment. Therefore, the sentiment score is 20. The politeness score is based on the language used, which is constructive and professional, even when pointing out flaws. The reviewer uses phrases like 'I find', 'makes one wonder', and 'I have to wonder', which are polite ways to express criticism. Therefore, the politeness score is 80.",20,80
"This paper proposes a technique for channel-wise quantization of CNNs
to 8-bit, fixed point precision. The authors propose several
techniques for analyzing the statistical properties of output channel
activations in order to select the best fractional bit length for each
channel. Experimental results on eleven different CNN architectures
demonstrate that the approaches proposed result in significantly less
accuracy loss when compared to a layer-wise baseline.

The paper has the following strengths:

 1. The experimental results on eleven different architectures (of
    varying depth and breadth) are convincing, and are consistently
    better than layer-wise MAX for choosing fractional bit length.

The paper has the following weak points:

 1. There is not much coherence between the description of the
    approach in section 2.1, Figure 1, Algorithm 1, and
    Figure 2. Notation is used in Algorithm 1 which is never defined.
 2. Related to the previous point, the proposed technique has a lot of
    moving parts and I don't feel that it would be easy to reproduce
    the results of the paper. There are some vague statements, like
    ""We resolve this complication by pre-coordinating the fractional
    lengths of the weights"", which require significantly more
    precision. This issue -- one of the main issues with channel-wise
    versus layer-wise quantization -- is never returned to in the
    definition of the method.
 3. The experimental comparison with layer-wise quantization is
    somewhat lacking. Is layer-wise MAX the state-of-the-art in CNN
    quantization? The results comparing channel-wise and layer-wise
    MAX are already convincing, but are the moment-analysis approaches
    not equally applicable to layer-wise quantization?
    State-of-the-art results that are less sensitive to outliers
    should be included in Table 1. A comparison with layer-wise
    approaches would be nice to have also in Figure 4 to show
    sensitivity to profiling set size.

The experimental results in the paper are impressive, and the analysis
motivating the approach is convincing. However, there are presentation
and clarity issues in the technical development, and the comparative
analysis is lacking broader comparisons with the state-of-the-art (to
be fair, the authors recognize that layer-wise MAX as a baseline is
particularly susceptible to outliers). These two aspects combined,
however, lead me to the opinion that this work is just not quite ready
for publication at ICLR.
","The sentiment of the review is mixed but leans towards positive. The reviewer acknowledges the strengths of the paper, particularly the impressive experimental results and convincing analysis, but also points out significant weaknesses in presentation, clarity, and comparative analysis. Therefore, the sentiment score is 20. The language used in the review is polite and constructive, offering specific recommendations for improvement without being harsh or dismissive. Thus, the politeness score is 80.",20,80
"This paper analyzes the surface of the complete orthogonal dictionary learning problem, and provides converge guarantees for randomly initialized gradient descent to the neighborhood of a global optimizer. The analysis relies on the negative curvature in the directions normal to the stable manifolds of all critical points that are not global minimizer.

Exploring the surface of a function and analyzing the structure of the negative curvature normal to the stable manifolds is an interesting idea. However, I believe I miss some thing important in this paper. This paper seems not to be self contained. I do not understand the paper very well. See details below. Therefore, I have reservations about the paper.

*) The terminology ""stable manifolds"" is used from the first page, while its formal definition is given on page 4.
*) P3, the dictionary learning problem is not formally given. It is stated in the paper that the task is to find A and X, given Y. However, what optimization problem does the author consider? Is it \min_{A, X} \|Y - A X\|_F^2? assuming both dictionary A and sparse code X are unknown or \min_{A} \|Y - A X\|_F^2 assuming only dictionary is unknown?
*) P3, second paragraph in Section 3: what is the variable q? It is not defined before.
*) P3, third paragraph in Section 3: What is the function row()? Why does row(Y) equal row(X)?
*) P3: How does the dictionary learning problem reformulate into the problem in the third paragraph of Section 3? If I understand correctly, the task is to find A, X such that A^* Y = X since A is orthogonal. Consider the first column in A and denote it by q. Then the first column of X is approximated by q^* Y. Since X is sparse, the task is to find q so that q^* Y as sparse as possible. But how about the other columns in matrix $A$? 
*) The Riemannian gradient algorithm is not stated in this paper.


","The sentiment score is derived from the overall tone and content of the review. The reviewer acknowledges the interesting idea behind the paper but expresses significant confusion and reservations about its clarity and completeness. This results in a sentiment score of -40, indicating a somewhat negative sentiment. The politeness score is based on the language used in the review. The reviewer uses polite language, such as 'I believe,' 'I do not understand,' and 'See details below,' which indicates a respectful tone despite the critical feedback. This results in a politeness score of 60.",-40,60
"PAPER SUMMARY:

This paper introduces a biologically motivated black-box attack algorithm. 
The target model in this case is DNN applied to the ASR context (automatic speech recognition system). 

NOVELTY & SIGNIFICANCE:

The proposed approach extends the previous genetic approach of (Alzantot et al., 2018) to attack a more complicated ASR system (that handles phrases and sentences). The new contribution here is an add-on momentum mutation component on top of the existing genetic programming architecture of (Alzantot et al., 2018) as illustrated in Figure 3.

This however appears very incremental seeing that integrating the mutation component into existing system is straight-forward and that mutation is not even a new concept -- it has always been a vital component in genetic programming paradigm.

It is also unclear how this mutation component improves over the existing work (more on this in the sections below).

Another issue is this work seems to ignore the recent literature on adversarial black-box attacks to DNN model. To list a few:

Chen, P.-Y.; Zhang, H.; Sharma, Y.; Yi, J.; and Hsieh, C.-J. 2017b.
ZOO: Zeroth-order optimization-based  black-box attacks to deepneural networks without training substitute models. 
In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security (15-26) ACM

Cheng,  M.;  Le,  T.;  Chen,  P.-Y.;  Yi,  J.;  Zhang,  H.;  and  Hsieh,C.-J.2018.
Query-efficient hard-label black-box attack:  An optimization-based approach. arXiv preprint arXiv:1807.04457

While these works have not been used to attacking ASR system, they should be directly applicable to such system since after all, they are black-box attacks. I think the proposed method needs to be compared with these works.

TECHNICAL SOUNDNESS:

I find it surprising that even though the proposed method is claimed to be a black-box attack but in the end, it actually exploits the fact that the target model uses CTC decoder. This pertains specifically to the target model's internal architecture and a black-box attack is not supposed to know this.

CLARITY:

The paper is clearly written.

EMPIRICAL RESULTS:

I do not understand this statement:

""That 35% of random attacks were successful in this respect highlights the fact that black box
adversarial attacks are definitely possible and highly effective at the same time""

Why is 35% successful attack rate a positive result? The result tends to suggest that this is an attack with low success rate. 

The 2nd paragraph in 3.2 seems to give a vague explanation: ""the vast majority of failure cases are only a few edit distances away from the target. 

This suggests that running the algorithm for a few more iterations could produce a higher success rate, although at the cost of correlation similarity"".

Given the above statement, I do not see why the authors didn't actually ""run the algorithm for a few more iterations"" to verify it ...

I am also curious why is the success rate of the proposed method is significantly lower than that of the existing system -- I assume ""single word black box"" is the work of (Alzantot et al., 2018).

I find the empirical evaluation somewhat sloppy: why are the tested method not compared on the same benchmark? How do we interpret the results then?

REVIEW SUMMARY:

The paper misses the recent literature on black-box attack. The authors need to compare with those to demonstrate the efficiency of their proposed work. I also find the contribution of this paper too incremental & its empirical evaluation appears somewhat sloppy and not convincing (see my specific comments above). ","The sentiment of the review is generally negative. The reviewer points out several significant issues with the paper, including its incremental contribution, lack of comparison with recent literature, and unclear empirical results. The reviewer also questions the technical soundness of the proposed method. However, the reviewer acknowledges that the paper is clearly written. Therefore, the sentiment score is -60. The politeness of the language used is relatively neutral. The reviewer provides constructive criticism without using rude or overly harsh language, but the tone is firm and critical. Therefore, the politeness score is 0.",-60,0
"This manuscript joins a crowded space of methods for low bit quantization to enable inference on more efficient hardware. In the past, these methods often were limited to 8-bit quantization, or smaller networks, or result in accuracy degradation. This paper is part of a recent crop of methods that achieve full accuracy on ResNet50 with 4-bit weights and activations. 

The method in this paper is based around a simple, yet powerful observation: Fine-tuning at low precision introduces noise in the gradient. Using the relationship between noise, batch-size and learning rate, that has recently been receiving a lot of attention in the context of large batch training, they compensate for this added noise by increasing the batch size. 

I like the simplicity and effectiveness, and believe that this method will be a useful addition to the toolbox for low-precision inference. 

Overall, the paper is well written, and the claims are well supported experimentally. Results are demonstrated on a wide range of networks, including various configurations of ResNet, DenseNet, Inception. It's not clear whether these experiments are from a single run. If they are, with sub 1% differences between methods we are getting close to the run-to-run variability, and it would be preferable to see results averaged across multiple runs. 

Ultimately, I am on the fence if this is a sufficient contribution for acceptance. In particular, this paper claims ""first evidence ... matching the accuracy of full precision"". While this may in a narrow technical sense be the case, PACT https://arxiv.org/abs/1805.06085 also works on ResNet50 without an accuracy drop. While this is not published work, it was rejected at ICLR last year, making it hard to recommend acceptance here. There is also work concurrently submitted to this forum (which I obviously don't expect the authors to cite or take into account, but want to mention for the sake of completeness) such as https://openreview.net/forum?id=HyfyN30qt7 which achieves the same or better results, and does not require 8-bit BN scale factors and 32-bit bias. 

This manuscript could be made stronger in multiple ways, e.g. by combining with the recently proposed clipping techniques like Choi et al. (2018), and pushing towards 2 or 3 bit training, or eliminating all larger bit-width parameters to make for easier hardware design. 
","The sentiment of the review is generally positive, as the reviewer appreciates the simplicity and effectiveness of the method and acknowledges that the paper is well-written and experimentally supported. However, there are some reservations about the novelty and sufficiency of the contribution, which slightly tempers the overall positive sentiment. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, even when pointing out areas for improvement or expressing doubts about the paper's contribution. Thus, the politeness score is 90.",60,90
"This paper suggests a new metric for assessing the quality of hierarchical clustering. Dasgupta recently suggested the first such metric with interesting properties. This has encouraged a number of recent works about designing algorithms that work well for the metric and other similar metrics. This paper suggests a new metric for evaluating hierarchical clustering of given graph data. 

Here are the main comments about the paper:
- I am not convinced about the advantages of the new metric over the previously suggested metrics by Dasgupta and Cohen-Addad et al.
    - Theoretical analysis shows properties of the new metric that are similar to that of Dasgupta (since the metric itself has similarities). However, the advantage of the new metric is not very clear. 
    - Experimental analysis just shows that the new metric is different from Dasgupta’s but again there is no evidence to suggest why the new metric may be better. 

- In the abstract it is mentioned that “The best representation of the graph for this metric in turn yields a novel hierarchical clustering algorithm.” I do not understand this. Which novel algorithm is being referred to? 

- Again, it is mentioned in the abstract that “Experiments on both real and synthetic data illustrate the efficiency of the approach”. What efficiency is being referred to here and what is the approach? What I see is that known clustering algorithms are used to compare the new metric with the previous one by Dasgupta.

Overall, I think more work is needed in this paper. There are some non-trivial observations but unless the authors make the motivation for defining this new metric for evaluation more clear.

Other comments:
- Section 5: NP-hardness has not been shown and it is just mentioned that the authors believe that the problem is NP-hard just as the problem associated with the cost function of Dasgupta et al.","The sentiment of the review is moderately negative. The reviewer expresses skepticism about the advantages of the new metric over existing ones and points out several unclear aspects of the paper. The sentiment score is -40 because while the reviewer acknowledges some non-trivial observations, they ultimately believe more work is needed. The politeness score is 50 because the reviewer uses polite language, such as 'I am not convinced' and 'I do not understand,' which indicates a respectful tone despite the critical feedback.",-40,50
"In this work, the authors attempt to unify existing adaptive gradient methods under the Bayesian filtering framework with the dynamical prior.  In Ollivier, 2017, a framework is proposed to connect Bayesian filtering and natural gradient.  On the other hand,  in Khan et al., 2018. an approach is proposed to connect natural gradient and adaptive gradient methods.  The main contributions of this work are (1)  introducing a dynamical prior and (2) recovering RMSProp and Adam as special cases. 

However, the proposed dynamical prior is very similar to the fading memory technique used in Ollivier, 2017. (see Proposition 3 of Ollivier, 2017) 
Furthermore, the authors argue that this work recovers a root-mean-square form while Khan et al., 2018 recovers a different sum-square form. Unfortunately, the authors have to use a series of unnatural approximations to recover the root-mean-square form. In fact, as mentioned in Khan, 2017b  this proposed method without these approximations is also a mean-square form. (also see Eq (2.28-2.29) of Ollivier, 2017)

Since the authors mainly follow Ollivier, 2017 and make unnatural approximations,  the work has a limited impact.  To get a higher rating, the authors should clearly give justifications and insights of these approximations.

Detailed comments:
(1) On Page 1,  ""The typical approach to Bayesian filtering, where we infer a distribution, ... jointly, forces us to use extremely strong, factorised approximations, and it is legitimate to worry that these strong approximations might meaningfully disrupt the ability of Bayesian filtering to give close-to-optimal updates.   ... we instead consider ... that incorporates factorisation into the problem setting, and therefore requires fewer approximations downstream. ""
The proposed method is equivalent to jointly perform Kalman filtering with full-covariance with an additional diagonal-approximation step. This additional step might also meaningfully disrupt the ability of Bayesian filtering. Furthermore, such approximation ignores the off-diagonal terms in the low-rank approximation at Eq (8). 

Minor: You should use \approx at Eq (8) since a rank-1 approximation is used.  

(2) On page 2, ""It has been noted that under specific circumstances, natural gradient is approximate Bayesian filtering (Ollivier, 2017), allowing us to link Bayesian filtering to the rich literature on natural gradients.  However, this only occurs when the dynamical prior in the Bayesian filtering problem has a specific form: the parameters being fixed over time (i.e.  arguably an online data, rather than a true Bayesian filtering setting)."" 
The authors should comment the difference between the dynamical prior and the fading memory technique (see Proposition 3 of Ollivier, 2017) where at page 14 of Ollivier, 2017, Ollivier mentions that ""this is equivalent ... or to the addition of an artificial process noise ... in the model"".  I think Ollivier's idea is very similar to the dynamical prior used at Eq (1) of this submission.  Furthermore, the second-order Taylor expansion with a Fisher information-based estimation of Hessian (see the equation below Eq(1) of this submission) is exactly the same as Ollivier's Extended Kalman filter (see  Eq 2.25 at Lemma 9  and Lemma 10 of Ollivier, 2017).  The authors should cite Ollivier, 2017.

Minor: Eq (6) should be E_p [ - \nabla_z^2 \log p(d|z) ] = E_p  [ e e^T ], where ""-"", the negative sign is missing. Please see the definition of the Fisher information matrix.
 
(3) On page 2, ""While there have been attempts to use natural gradients to recover the Adam or RMSprop root-mean-square form for the gradient normalizer, in practice a different sum-square form emerges (Khan & Lin, 2017; Khan et al., 2018). In contrast, we show that to recover the Adam or RMSprop form for the gradient normalizer."" 
Khan et al., 2018 is a mean-square form for variational inference due to the entropy term of the variational distribution. (see Sec 3 and 5 of  Khan et al., 2018 and Khan, 2017b )
Unfortunately, the ""root-mean-square form"" does not appear naturally in this submission. In practice, the proposed update is also a mean-square form  (see Eq (2.28-2.29) of Ollivier, 2017 and Khan, 2017b) without a series of unnatural approximations used in this submission.
To justify these assumptions, the authors should explain when ""the steady state posterior variance"" (see sec 2.21) and  ""a self-consistent solution"" (see sec 7.1) achieve.  As far as I know, \sigma^2_t = \sigma^2_{t+1} in sec 2.2.1 only holds in the limit case when t-> \inifity.  Why does the equality hold at each time step t? The authors should give a justification or an intuition about these approximations since this paper is a theory paper. Please also see my next point.

(4) Section 7.1 is also confusing.
In sec 7.1, the authors assume that A \in O(\eta). However, A=\eta^2/(2\sigma^2) in sec 2.2 and A_{1,1} =  ( \eta_w^2+\eta^2 )/ (2\sigma^2) at Eq (14). In both cases, A can be \in O(\eta^2). This is very *critical* since the authors argue that O(\eta^3) can be neglected in sec 7.1.  The authors use this point to show that Adam is a special case. 
If A \in O(\eta^2), we know that ""A \Sigma_{post}"" \in O(\eta^3) should be neglected. At the last equation on page 10,  the authors do not neglect ""A \Sigma_{post}"". Why?  The authors should clarify this point to avoid doing *selective* neglection.  Again, the impact of this paper should be inspiring new adaptive methods.
The authors also mention that the second-order term in A is neglected in sec 7.2. Any justification? 


References
[1] Ollivier, Yann. ""Online Natural Gradient as a Kalman Filter."" arXiv preprint arXiv:1703.00209 (2017).
[2] Khan, Mohammad Emtiyaz, and Wu Lin. ""Conjugate-computation variational inference: Converting variational inference in non-conjugate models to inferences in conjugate models."" arXiv preprint arXiv:1703.04265 (2017).
[3] Khan, Mohammad Emtiyaz, et al. ""Vprop: Variational Inference using RMSprop."" arXiv preprint arXiv:1712.01038 (2017b).
[4] Khan, Mohammad Emtiyaz, et al. ""Fast and Scalable Bayesian Deep Learning by Weight-Perturbation in Adam"" (2018)

","The sentiment of the review is generally neutral to slightly negative. The reviewer acknowledges the attempt and contributions of the authors but points out significant issues and limitations, suggesting that the work has limited impact without further justification and insights. The politeness of the language is quite high; the reviewer uses formal and respectful language throughout, providing constructive criticism and specific recommendations without being rude or dismissive.",-20,80
"This paper proposes a generalization of variational auto-encoders to account for meta-data (attributes), learning new ones, in a way that these can be controlled to generate new samples. The model learns how to decouple the attributes in an adversarial way by means of a discriminator. The problem is interesting, but I found two main issues with this paper:
1.- Lack of clarity: I found the paper difficult to follow, even after reading Sec. 2 and 3 several times.
2.- Almost absence of experiments: The paper only has one experiment, which is in the appendix, and is about sampling using the MNIST dataset. Given that this paper proposes a model, whose properties can be assessed by means of experiments, the fact that there is nothing of the kind provides no support to any benefits the model may have.

Other points:
What in the model prevents the solution of z_* being just random (independently of x)?

This paper seems relevant Esser, Patrick, Ekaterina Sutter, and Björn Ommer. ""A Variational U-Net for Conditional Appearance and Shape Generation."" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.","The sentiment of the review is mixed but leans towards negative due to the identification of two main issues: lack of clarity and almost absence of experiments. The reviewer acknowledges the interesting problem but is critical of the paper's execution. Therefore, the sentiment score is -40. The politeness of the language is relatively neutral but slightly polite, as the reviewer provides constructive criticism without using harsh or rude language. Therefore, the politeness score is 20.",-40,20
"Post-rebuttal update: The review process has identified several issues such as missing citations and lack of clarity with respect to aims of the paper. Although the authors have failed to update the paper within the rebuttal period, their responses show an understanding of the issues that need to be addressed as well as a broad appreciation of work in EC that would be included in a final version, making it a useful resource for the wider ML community. On top of this they will include an even larger amount of empiricial data from the experiments they have already run, which is a valuable resource considering the amount of compute needed to obtain this data.

---

The current landscape of reinforcement learning - particularly in domains with high-dimensional structured input spaces such as images or text - relies heavily on backpropagation-based reinforcement learning algorithms.  An alternative that has re-emerged is ES, due to its simplicity and scalability. However, ES can also be considered a gradient-based method. In this paper, the authors apply a similar treatment to GAs, another simple method at its most basic. The authors claim to have 3 main contributions: extending the scale to which GAs can operate, suggesting that gradient-based methods may not achieve the best performance, and making available a vast array of techniques available from the EC literature; they demonstrate the latter by utilising novelty search (NS).

In my opinion the authors do indeed have a valuable contribution in a) demonstrating that a simple GA can successfully be applied to larger networks than was previously thought to be possible and b) introducing a novel software implementation that allows GAs to be efficiently scaled/distributed (similar in nature to the work of Salimans et al.). This is by itself valuable, as, along with recent work on ES, it potentially extends the range of problems that are perhaps best tackled using black-box optimisation techniques. Going against the prevailing trends in order to investigate alternative methods is an underappreciated service to the community, and I believe the evaluation of the methods and the choice of comparative methods to be just about satisfactory. As exemplified by NS, there is a wealth of techniques from the EC literature that could be applied to many topical problems, and the authors' main contributions opens up the road for this.

A lot of care has been put into evaluation on Atari games. The details in the main paper and supplementary material, with, e.g., clear definitions of ""frames"", make me believe that fair comparisons have taken place. All methods in the table, including GAs, perform best at some games (except for RS, which is a necessary baseline for GAs). It would be better to provide more data points that relate to prior works - such as scores at 200M frames to evaluate sample complexity (indeed, the authors note that good solutions can be found by GAs within a few generations, so it would be best to tabulate this) and at ~ 4d to evaluate wall-clock time (is it possible to push performance even further?). Since the GA presented is very rudimentary, I consider the baselines in the main paper to be reasonable, but it would be misleading to not present newer work. The authors do so in the supplementary material, and it is promising to note that the GA still achieves state-of-the-art performance in a few games even when compared to the most sophisticated/distributed state-of-the-art DRL algorithms developed in a concentrated effort over the last few years. Despite having an NS variant, it is a shame that the authors did not show that this could potentially improve performance on Atari, when BCs such as the game RAM or preferably random CNN features are easily available.

The authors also evaluate on a maze that is a staple task in the EC literature to demonstrate the power of NS. While the results are unsurprising, it is a reasonable sanity check. The final evaluation is on a difficult continuous control task, in which GAs solve the task, but have much poorer sample complexity than ES. Given the range of continuous control tasks used to benchmark RL algorithms nowadays, the authors would do well to present results across more of these tasks. Again, NS was not evaluated on this task.

A major weakness of this paper is the presentation. The authors discuss some interesting findings, but would be better served by being more concise and focused. In particular, the emphasis should be more on showcasing quantitative results. Doing so, with more continuous control tasks, would make the claims of this paper more substantiated.","The sentiment of the review is generally positive, as the reviewer acknowledges the valuable contributions of the paper, such as demonstrating the scalability of GAs and introducing a novel software implementation. However, the review also points out several areas for improvement, such as the need for more concise presentation and additional quantitative results. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer provides constructive feedback in a respectful and encouraging manner, acknowledging the authors' efforts and contributions while suggesting improvements. Therefore, the politeness score is 80.",60,80
"The authors focus solely on universal adversarial perturbations, considering both epsilon ball attacks and universal adversarial patches. They propose a modified form of adversarial training inspired by game theory, whereby the training protocol includes adversarial examples from previous updates alongside up to date attacks.

Originality: I am not familiar with all the literature in this area, but I believe this approach is novel. It seems logical and well motivated.

Quality and significance: The work was of good quality. However I felt the baselines provided in the experiments were insufficient, and I would recommend the authors improve these and resubmit to a future conference.

Clarity: The work was mostly clear.

Specific comments:
1) At the top of page 5, the authors propose an approximation to fictitious play. I did not follow why this approximation was necessary or how it differed from an stochastic estimate of the full objective. Could the authors clarify?

2) The method proposed by the authors is specifically designed to defend against universal adversarial perturbations, yet all of the baselines provided defend against conventional adversarial perturbations. Thus, I cannot tell whether the gains reported result from the inclusion of ""stale"" attacks in adversarial training, or simply from the restriction to universal perturbations. This is the main weakness of the paper.

3) Note that as a simple baseline, the authors could employ standard adversarial training, for which the pseudo universal pertubations are found across the current SGD minibatch.


","The sentiment of the review is generally positive, as the reviewer acknowledges the novelty and logical motivation of the approach, as well as the good quality of the work. However, the reviewer also points out significant weaknesses, particularly regarding the baselines used in the experiments. Therefore, the sentiment score is not fully positive but leans towards the positive side. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, providing specific recommendations for improvement without being harsh or dismissive.",40,90
"The paper gives convergence guarantees to the true neural network classifier for the networks that are explicitly regularizing the (exact) Lipschitz constant of the network. The bound stated decays as ( log(n)/n )^{1/m}, where n is the number of training points and m is the dimension of the data manifold. Thus the decay rate is pretty slow when the data lies on a high-dimensional manifold. I believe it should also depend on the volume of the data manifold.

Computing the exact Lipschitz constant of the network is intractable. All the theorems apply to minimization problem defined in (1) with the exact Lip(u,X) being regularized, and not to the minimization problem of the lower bound (2). I believe there are also no convergence guarantees how quickly this lower bound on the Lipschitz constant approaches Lip(u,X), unless some assumptions are made on the smoothness of the data manifold (comments and insights would be appreciated). Thus in my opinion the current results have little practical importance. Nevertheless, it’s still interesting to see some ideal-setting guarantees being established.


Theorem 2.7: Is m == m_0 (the dimension of the data manifold)? Shouldn’t the bound be 2*C*L_0….? (assuming lemma 2.9 is correct)

Cor 2.8: Where does the volume Vol(M) of the manifold disappear? Is C in equation (6) the same C as in Theorem 2.7? Also, it looks like the bound should bound should have C^2 (assuming theorem 2.7 is correct, and the C’s are the same).


Introduction: I assume u(x,w) is not the last layer map, but a map from the input space to the labels (i.e., the whole neural network function and not just the map from the last hidden layer to the labels). If I am correct, it’s misleading to refer to u(x,w) as the last layer map. And if it is the last layer map, please justify why it is enough to consider the Lipschitz constant of the last layer.

The term “clean data” is never defined. My guess is that “clean data” refers to the realizable  setting, and “noisy” to agnostic, where the hypothesis space consist of neural networks of arbitrary size.


“Our analysis can avoid the dependence on the weights because we make the assumption that there are enough parameters so that u can perfectly fit the training data. The assumption is justified by Zhang et al. (2016).”
Note, that the network used in practice achieve zero classification error, as demonstrated by Zhang et al, but I doubt the cross entropy loss (that is usually being minimized) is exactly zero.

Remark 1.4 “will result in an expected loss..”  (there is also a typo here) should specify that you are talking about empirical error (0-1 loss), since I don’t think the loss function is fixed anywhere earlier in the text.

Remark 2.2 : Just wanted to note, that it is more common to call L(u,\rho) risk. The gap between L(u,\rho) and the empirical risk L(u,\rho_n) is usually called the generalization error (and only in the case of zero empirical risk, L(u,\rho) is equal to the generalization error). I did check the reference in Goodfellow et al. book, and I see that it is consistent with your definition.

Just below Remark 2.2:
“We would also expect the sequence of generalization losses L[u_n ; \rho] to converge to zero in the case of perfect generalization.”
Once again, this is true only in the realizable setting.

Could the authors comment on the connection to Cranko et al. 2018 work?

Typos:
In the abstract, no which: “..corrupted labels which we prove convergence to...”
“...a candidate measure for the Rademacher complexity, which a measure of generalization...”.
“1-Lipschitz networks with are also important for Wasserstein-GANs”
Section 2.1 “is it clear that u0, is a solution of “, should be “it is”


---------
[UPDATE]

Regarding the comment ""Our paper resolves the question posed in ICLR Best paper 2017 ""Understanding deep learning requires rethinking generalization"""", I don't think that analyzing networks with explicit regularization resolves the questions stated in Zhang et al paper. As other reviewers mentioned, there are a number of other papers that formally define quantities that correlate with the generalization error, and are larger for random vs true labels. There are also other papers showing that one can tune some parameters of the optimization algorithm to avoid overfitting on random labels (while it is a modification to the algorithm, it is still similar to explicitly regularizing the Lipschitz constant of the network) (see e.g., Dziugaite et al work on SGLD).

Therefore, the claim in the abstract ""A second result resolves a question posed in Zhang et al. (2016): how can a model distinguish between the case of clean labels, and randomized labels?"" needs to be toned down a bit.

In my opinion, the work presented in this paper is a valuable contribution to learning theory. The new version of the paper is easy to read. Therefore, I recommend acceptance if the authors change the claim about resolving  the questions posed by Zhang et al.

Another typo:
 - for convergence, we require that the network also grow(s), ","The review provides a balanced critique of the paper, acknowledging its contributions while pointing out several areas for improvement. The sentiment score is slightly positive because the reviewer recognizes the paper's value and recommends acceptance with revisions. The politeness score is high as the reviewer uses polite language, phrases like 'I believe,' 'comments and insights would be appreciated,' and 'in my opinion,' and provides constructive feedback without being harsh or dismissive.",20,80
"This paper looks to predict ""unstructured"" set output data. It extends Rezatofighi et al 2018 by modeling a latent permutation.

Unfortunately, there is a bit of an identity crisis happening in this paper. There are several choices that do not follow based on the data the paper considers. 
1) The paper claims to want to predict unordered sets, yet the model is clearly indicating a dependence in the order of the outputs and the input p_m(\pi | x_i, w) (1); this feels like a very odd choice to me. The outputs are either unordered sets, where you would have a permutation invariant (or exchangeable) likelihood, or they are ordered sequence where the order of the outputs does matter, as some are more likely than others.
2) The paper still makes very odd choices even if one ignores the above and wants to model some orderings as more likely than others. The way the permutation, or the order of the data, accounts in the likelihood (2) does not make sense. Conditioned on the permutation of the set, the points are exchangeable. Let's just consider a 2 element ""set"" at the moment Y = (y_1, y_2). Order matters, so either this is being observed as pi=(1, 2) or pi=(2, 1), both of which depend on the input x. However, the likelihood of the points does not actually depend on the order in any traditional sense of the word. we have:
p_\pi((1, 2) | x, w) p_y(y_1 |  x, w, (1, 2)) p_y(y_2 |  x, w, (1, 2)) + p_\pi((2, 1) | x, w) p_y(y_1 |  x, w, (2, 1)) p_y(y_2 |  x, w, (2, 1))
*Note that in here (as in eq. 2) the output distribution p_y does not know what the index is of what it is outputting, since it is iid.* So what does this mean? It means that the order (permutation) can only affect the distribution in an iid (exchangeable, order invariant) way. Essentially the paper has just written a mixture model for the output points where there are as many components as permutations. I don't think this makes much sense, and if it was an intentional choice, the paper did a poor job of indicating it.
3) Supposing even still that one does want a mixture model with as many components as permutations, there are still some issues. It is very unclear how the dependence on \pi drops out when getting a MAP estimate of outputs in section 3.3. This needs to be justified.

There are some stylistic shortcomings as well. For example, the related works paper would read better if it wasn't one long block (i.e. break it into several paragraphs). Also, the paper claims that it will use a super script m to denote a known cardinality, yet omits \mathcal{Y}_i^{m_i} in the training set of the first sentence in 3.1. But these and other points are minor.

The paper should not be published until it can resolve or make sense of the methodological discrepancies between what it says it looks to do and what it actually does as described in points 1), 2), and 3) above.","The sentiment of the review is predominantly negative, as the reviewer points out several critical methodological issues and describes the paper as having an 'identity crisis.' The reviewer also mentions that the paper makes 'very odd choices' and 'does not make sense,' which further indicates a negative sentiment. The sentiment score is -80. The politeness of the language, however, is relatively high. Despite the critical feedback, the reviewer uses polite language, such as 'unfortunately,' 'odd choice,' and 'needs to be justified,' and provides constructive criticism without being rude. The politeness score is 60.",-80,60
"The authors are tackling sample efficiency in the reinforcement learning setting by designing a reward function that encourages exploration. To achieve this they propose you use the successor function which basically counts how often a state has been visited. At first the show this for discrete settings and extend their approach to the continuous state spaces in the Atari 2600 environments. 

The paper is well written and the motivation and methods are clear from the beginning. 

My biggest concerning is regarding the experimental results of this work. In Table 1 the authors show the results for the tabular games River Swim and Six Arms and copare their approach which they dub ESSR to three methods (E3, R-MAX, MBIE). The numbers in the table indicate that their method ESSR is outperforming E3 and R-MAX on both environments but is itself outperformed by MBIE. The authors don't mention this at al in the respective paragraph nor do the provide a reason as to why this could be case. Also, they neither introduce any of these methods nor do the explain the meaning of the acronyms. Only in the section 6 (of 7) they talk about related works are R-MAX and E3 introduced briefly. But yet again, MBIE is not mentioned. 

I have similar concerns about the results presented for the Atari benchmarks. In table 2 the authors compare their method to the classic DQN approach and two more approaches. While their approach outperforms DQN in almost all tasks, this does not hold for the remaining algorithms. Their method is being outperformed in all but one (Venture) task, where they report a higher variance and a small performance boost compared to DQN_e^MMC. Also it is not clear to me where the numbers for the DNQ_e^MMC come from. The authors just say ""[...] denotes another baseline used in the comparison"". Is this the proposed method of this work but without the successor representation?

In my opinion this work is lacking some clear and convincing results.  Is the main benefit of this method that it does not rely on domain-specific knowledge? If so, then it is not communicated clearly. The authors mention this briefly in the conclusion but provide no further analysis","The sentiment of the review is mixed. The reviewer acknowledges that the paper is well-written and that the motivation and methods are clear, which is positive. However, the reviewer expresses significant concerns about the experimental results and the clarity of the comparisons made in the paper. This mixed feedback results in a sentiment score of 20. The language used in the review is generally polite, with phrases like 'My biggest concern' and 'In my opinion,' which indicate a respectful tone. Therefore, the politeness score is 80.",20,80
"This paper introduced a GAN-based method to learn language universal representations without parallel data. The model architecture is analogous to an autoencoder. The encoder is a compound of language-universal mapper plus a language-specific LSTM. For decoding, another language-universal module first map language-universal representation back to language-specific embedding space, then another LSTM decoder generates the original sentence. The authors used GAN to encourage intermediate representation to be language-universal. The authors tested the proposed method on zero-shot semantic analysis and NLI tasks and showed nice results.

Overall the proposed method is novel and nice, and experiment results are good. On both tasks the proposed method performs better than NMT methods on target languages while still achieving competitive performance on source languages. The paper is also clearly written and could be useful for future research on multilingual transfer.

My main complaint is around Figure 5, Table 3, and the corresponding analysis.
1. In Figure 5, does it make more sense to show the perplexity of a standard LM. That is, train 7 independent LMs and report averaged perplexity. My concern is that, even with \lambda=0.0, the model still have modules u and h that are shared across languages, and therefore I'm not sure if it implies ""representative power of UG-WGAN grows as we increase the number of languages"". It could be that the language-universal impose more constraints to model all languages, so the two variation (\lambda=0.0 or 0.1) come closer to each other.

2. In Figure 3, the perplexity difference is huge when number of languages is 2. In Table 3, however, the authors show no fundamental differences between the English and Spanish language models. I feel the two arguments contradict to each other. Is it because of the language pairs are different? The authors should provide more explanation on that.

Minor:
1. Equation 1 and 2 in page 2. Are they both compound functions? Why the first one use \circ and the second one use parenthesis?","The sentiment of the review is generally positive, as indicated by phrases like 'the proposed method is novel and nice' and 'experiment results are good.' The reviewer acknowledges the strengths of the paper and its potential usefulness for future research. However, the review also includes some critical feedback, particularly around Figure 5, Table 3, and their corresponding analysis. The politeness of the language is high, as the reviewer uses polite and constructive language, such as 'My main complaint is around' and 'The authors should provide more explanation on that,' which indicates a respectful tone while providing criticism.",70,80
"The proposed method is advantageous in that it only requires changes to some parts of the original ResNet or LSTM, without having to significantly change the network structure or training algorithm. It also reports empirical success of using high-precision skip connections in ResNet and cell/hidden state updates in LSTMs.

However, it is unclear why it is necessary to keep a high-precision activation/gradient flow. What is the problem with existing quantized networks that do not have these high-precision-flow? Also, how does the high-precision flow interact with the rest of the network (with low-precision operations)?

Moreover, the proposed method has limited novelty as the use of full-precision skip connections has been proposed in Bi-Real (Liu et al. 2018).

Minor:
- It is hard to tell that the weight histogram in Figure 3 is similar to a Laplacian distribution. It can also be approximated by other distributions (such as Gaussian or piecewise-linear distributions).
- What kind of activation quantization is used?
- In the experiments, when is the cosine similarity between the quantized and full-precision networks computed? after training or on an intermediate training step?
- What are the axes in Figure 5? Why is there only one local minimum in Figure 5(d)? Why the training with PH converges even slower than without PH at the early stage of training?","The review starts with a positive note, highlighting the advantages of the proposed method, which suggests a positive sentiment. However, it quickly transitions to critical points, questioning the necessity and novelty of the method, which brings the sentiment closer to neutral. The language used is polite and professional, with no rude or harsh words, indicating a high politeness score.",10,90
"The paper attempts to examine the reasons behind the strong generalisation performance of DNNs trained via SGD. The authors propose an analysis which offers a fresh view to this problem. This view has been articulated very well, and is based on sound mathematical arguments. 

On the other hand, since there is no formal theorem to support the introduced assumptions, the authors have attempted to provide empirical evidence through experiments with standard DNN architectures and benchmark datasets. However, this is where the weakness of this paper lies: The provided empirical evidence, while nicely executed, is not enough to convince the critical reader. We need experiments with more diverse datasets and experimental setups. 

Although I accept the claim of the authors concerning the lack of space, they could also trim the Introduction so as to free up some space, as well as provide an indefinite number of extra supporting evidence in the form of Supplementary Material/Appendices. 


","The sentiment of the review is generally positive, as the reviewer acknowledges the fresh perspective and sound mathematical arguments presented by the authors. However, the sentiment is tempered by the critique regarding the lack of sufficient empirical evidence, which suggests a need for improvement. Therefore, the sentiment score is 50. The politeness of the language is high, as the reviewer provides constructive feedback in a respectful manner, suggesting ways to improve the paper without being dismissive or harsh. Thus, the politeness score is 80.",50,80
"The paper analyzes the strategy that a visual question answering model (FiLM) uses to verify statements containing the quantifier ""most"" (""most of the dots are red""). It finds that the model is sensitive to the ratio of objects that satisfy the predicate (that are red) to objects that do not; as the ratio decreases (e.g. 10 red dots compared to 9 blue dots), the model's performance decreases too. This is consistent with human behavior.

Strengths:
* The introduction lays out an ambitious program of comparing humans to deep neural networks.
* The experimental results are interesting (although of modest scope) and support the hypothesis that the network is not counting the objects but rather is using an approximation that is sensitive to the ratio between the red and non-red items.

Weaknesses:
* The architecture of the particular model is described very briefly, and at multiple points there’s an implication that this is an investigation of “deep learning models” more generally, even though those models may vary widely. While the authors are using an existing model, they shouldn't assume that the reader has read the paper describing that model. I would like to see more discussion of whether it is at all plausible for this model to acquire the pairing strategy, compared to alternative VQA models (e.g., using relation networks).
* I found it difficult to follow the theoretical motivation for performing the work. The goal seems to be to test whether the network is performing the task in way that ""if not human-like, at least is cognitively plausible"". I don't understand what is meant by cognitively plausible but not human-like; perhaps an example of a cognitively implausible mechanism would help clarify this issue. Later in the same paragraph, the authors argue that ""in the case of a human-centered domain like natural language, ultimately, some degree of comparability to human performance is indispensable"". This assertion is not justified, and seems surprising to me; we have very useful natural language processing systems that do not perform in a way that is comparable to humans (the hedge ""some degree of"" is really neither here nor there). In general, I don't understand why we would want a visual question answering system that returns approximate answers -- isn't it better to have it count exactly how many red dots there are compared to non-red dots?
* The authors assume that explicit counting is not ""likely to be learned by the 'one-glance' feed-forward-style neural network"" evaluated in the paper. What is this statement based on? Why would a ""one-glance"" network have trouble counting objects? (What is a “one-glance network”?)
* Another vague concept that is used without clarification: it is argued that if the network implements something like the Approximate Number System, that shows that it can ""learn and utilize higher-level concepts than mere pattern matching"". What is ""pattern matching"" and how does it differ from ""higher-level concepts""?
* Why would the pairing strategy in a neural network be affected by the clustering of the objects? I understand why a human who needs to saccade back and forth between the two groups of objects might lose track of the objects that have been paired so far, but I don't understand why that would affect the architecture in question.

Minor comments:
* Is the definition of ""most"" really a central piece of evidence for ""the apparent importance of a cardinality concept to human cognition""? Our ability to count seems sufficient to me. Perhaps I'm not understanding what the authors have in mind here.
* Please use the terms ""interpretation"" and ""verification"" consistently.
* ""One over the other strategy"" -> ""one strategy over the other"".
* The paper is almost 9 pages long, but the contribution does not appear more substantial than a standard 8-page submission.
","The sentiment of the review is moderately positive, as the reviewer acknowledges the strengths of the paper, such as the ambitious program and interesting experimental results, but also points out several weaknesses and areas for improvement. Therefore, the sentiment score is 20. The politeness of the language is quite high, as the reviewer uses polite language and constructive criticism throughout the review, even when pointing out weaknesses. Therefore, the politeness score is 80.",20,80
"This work presents a method to translate non-systematic names of chemical compounds into their systematic equivalents. Beyond that, a corpus of systematic and non-systematic chemical names is introduced that were extracted from chemistry and manually labelled.

The paper is well-structured and the authors introduce the problem setting very nicely to a machine learning audience, explain the challenges and motivate the architecture of their model. The model is a combination of tested approaches such as a spelling error correction, a byte pair encoding tokenizer and a sequence-to-sequence model consisting of (Bi)LSTMs and attention mechanisms. 

The evaluation appears solid. The model achieves significantly improved results on the proposed corpus, even though compared to a underwhelming baseline. The analysis could be improved by showing and explaining some examples of failed translation attempts. It would also be interesting to see how the failed attempts are distributed over the error types (spelling, order, common name, synonym). The authors suggest a scenario where non-systematic names are converted and checked against a database of systematic names. For this, it would be interesting to know whether there are cases where a non-systematic name was translated into the wrong (but valid) systematic name.

Concluding, the paper presents an interesting application for machine translation, a new dataset and a method that successfully labels 50% of the given corpus.

Minor issue: The color scale of Fig. 5 is hard to recognize due to low contrast.","The sentiment of the review is generally positive, as the reviewer appreciates the structure of the paper, the introduction of the problem, and the solid evaluation. However, there are some criticisms regarding the baseline comparison and suggestions for improvement in the analysis. Therefore, the sentiment score is 70. The politeness of the language is very high, as the reviewer uses polite and constructive language throughout the review, even when pointing out areas for improvement. Thus, the politeness score is 90.",70,90
"This paper has proposed a new algorithm for semi-supervised learning, which incorporate biased negative data into the existing PU learning framework.

The paper was written in clarity and easy to follow overall. However, the original motivation for having biased negative data are not explained very clear. The relation to dataset shift was very interesting, but it’s unclear what’s the exact connection between the proposed algorithm and the dataset shift. Maybe the authors can elaborate a little more on their point here in the future revision.

The paper has made some assumption about the relation between the latent random variable and the label in section 2.4. In the experiment, data sets are generated following the exact assumption. That’s not surprising to see that the proposed algorithm that fits the assumption will perform better than the previous methods without this assumption. In practice, there’s no way to really verify this assumption. Thus, it’s more interesting to see how the algorithm performs under the more generic semi-supervised learning setting, with unbiased, or biased negatives that don’t really fit the exact assumption in this paper.

Moreover, I’d like to see more intuition on why adding biased negative data will further improve upon nnPNU. The author provided some explanation in section 4.3, which seems just observations on the FPR and FNR, rather than the fundamental explanation for the advantage of this algorithm.

Choice of baseline methods is also limited. The original paper [1] for PNU has included a bunch of benchmark algorithms for semi-supervised learning. The authors should also include more benchmark algorithms for comparison, e.g. those listed in Section 5.2 in [1].

[1] Sakai, Tomoya, et al. ""Semi-supervised classification based on classification from positive and unlabeled data."" arXiv preprint arXiv:1605.06955 (2016).","The sentiment of the review is moderately positive. The reviewer acknowledges the clarity and ease of following the paper, which is a positive remark. However, the review also contains several critical points and suggestions for improvement, indicating that the reviewer sees significant areas for enhancement. Therefore, the sentiment score is 30. The politeness of the language is quite high. The reviewer uses polite language such as 'maybe the authors can elaborate,' 'I’d like to see more intuition,' and 'the authors should also include,' which indicates a respectful and constructive tone. Thus, the politeness score is 80.",30,80
"In this paper, the authors try to interpret the prediction mechanism of Layered Neural Networks (LNNs). The authors proposed to first define a feature vector that represents the roles of each hidden layer unit, via computing Pearson correlation coefficient. Then a hierarchical clustering method is applied to the generated feature vectors, such that tree-structured relationships among hidden layer units are revealed.

The purpose of the paper is to understand the prediction mechanism of Layered Neural Networks (LNNs). But based on the results in the experiments, I do not think the model achieves this purpose. Given the tree structure of LNN for the MNIST data set, I am still not able to understand how this LNN distinguishes the digit 0 from other digits. I am also not able to understand why a particular sample is classified as 0 rather than 6.

In Section 1, the authors mension that there are existing clustering-based methods that interpret LNN. The authors do not compare the proposed methods with these existing methods, either quantitatively or qualitatively. So I am also not sure the contribution of this paper, provided the existing methods.

In Section 3.1, the authors state that ""there is no method that can reveal whether an increase in the input dimension value has a positive or negative effect on the output value of a hidden layer unit"". I do not agree with this statement, because Ross et.al (2017) has proposed to measure it via gradient, although they are trying to solve a slightly different problem. Since the output of a hidden unit is a non-linear function of the input, I am not convinced that the proposed method that computes Pearson correlation coefficient is better choise than computing the gradient.

The proposed method provides a tree structure to describe the relationships between the hidden layer units. The authors also do not illustrate why learning the tree structure is particularly important. We can also run k-means with cosine similarity on the generated vector $v$, and learn the number of clusters via Bayesian information criterion (BIC). The authors do not explain why the tree-structured clustering results are more superior than the k-means clustering results.

In summary, I recommend rejection of this paper, because 1) I do not think the proposed method achieve its purpose; 2) It is not appropriately compared with existing methods; and 3) I am not convinced that the method is designed properly.


References
Ross, Andrew Slavin, Michael C. Hughes, and Finale Doshi-Velez. ""Right for the right reasons: training differentiable models by constraining their explanations."" Proceedings of the 26th International Joint Conference on Artificial Intelligence (IJCAI). 2017.","The sentiment of the review is negative, as the reviewer recommends rejection of the paper and provides several critical points regarding the methodology, comparison with existing methods, and the overall contribution of the paper. The sentiment score is -80 because the reviewer clearly expresses dissatisfaction with the paper's results and contributions. The politeness score is 20 because, while the reviewer is critical, they maintain a professional tone and provide constructive feedback without resorting to rude or disrespectful language.",-80,20
"General comment
==============
The authors describe an attention mechanism for training with images of different sizes. The paper is hard to understand due to major grammatical errors and unclear descriptions. Methods for training with images of different sizes have been proposed before, e.g. spatial pyramid networks. I also have concerns about their evaluation. Overall, I believe that the paper is not ready to be submitted to a conference or journal.

Major comments
=============
1. Methods for training with images already exists, e.g. spatial pyramid pooling (http://arxiv.org/abs/1406.4729) or fully-convolutional networks (https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf). These are not cited in the paper and not included as baselines in their evaluation.

2. The attention mechanisms looks similar to classificat soft-attention (https://arxiv.org/abs/1502.), which is not cited in the paper.

3. The paper contains major spelling and grammatical errors, making it hard to understand important aspects.

4. I can not see a clear improvement of their method over ResNet and DenseNet when the same number of model parameters is about the same. Without making sure that the number of model parameters is about the same, it is unclear if the performance gain is due the increased number of model parameters or the methodology.","The sentiment of the review is quite negative, as the reviewer points out several major issues with the paper, including grammatical errors, lack of citations, and unclear improvements over existing methods. The sentiment score is -80 because the reviewer believes the paper is not ready for submission. The politeness score is 20 because, while the reviewer is critical, they do not use rude or offensive language. The comments are direct but not impolite.",-80,20
"The focus of this paper is to show that finite-width deep neural networks with fully connected layers and ReLU activations are rate-distortion optimal approximators of certain classes of functions, meaning the approximation error decays exponentially in the number of neurons in the network. The function classes explored in this paper are: 1-d polynomials (on bounded intervals), 1-d sinusoidal functions (on bounded intervals), and other 1-d functions built from compositions or linear combinations of these, such as the so-called class of “oscillatory textures” and a class of continuous but nowhere differentiable functions known as Weierstrass functions. Finally, the paper also shows that as the desired approximation accuracy goes to zero finite-width deep ReLU networks require asymptotically fewer neurons than finite-depth wide ReLU networks in approximating a broad class of smooth functions.


The paper is well-written and the technical results are presented in a way that is easy to understand. The results are somewhat novel, although they do build off other recent works, namely Yarotsky (2016) and Telgarsky (2015). However, the authors were careful to cite when they reuse proof techniques from these and other works. The results in the main text appear to be technically sound. I did not check carefully all the proofs in the supplemental materials.


My major criticism is that the focus on certain specific function classes (oscillatory textures, Weierstrauss functions) seems arbitrary, and leaves open many questions. For example, there is existing work on the approximation ability of deep ReLU networks for functions in more general Holder and Sobolev spaces:

Hadrien Montanelli and Qiang Du. Deep ReLU networks lessen the curse of dimensionality. arXiv preprint arXiv:1712.08688, 2017.

J. Schmidt-Hieber. Nonparametric regression using deep neural networks with ReLU activation function. ArXiv e-prints, August 2017.

I was left wondering how the present results relate to these works, and what insight we get from understanding these particular function classes that we don't get from understanding Holder or Sobolev spaces.


Major comments


In Section 3, I found the progression of the results from approximation of x^2, to multiplication xy, and to general smooth functions to be very natural and well-motivated. However, sections 4 and 5 seem lack somewhat in motivation, since here the authors focus on very specific function classes (sinusoidal functions, oscillatory textures, and Weierstrass functions). While these results are still interesting, focusing on such specific functions is less satisfactory, since it raises questions about the true scope of the results (e.g., will similar approximation rates extend to other fractal functions, or just Weierstrauss functions?). Could the authors give further justification for why these function classes are interesting to focus on, or why they limit themselves in this way? Can the authors also put these results more into context with existing results on the approximation with ReLU networks?


The authors state multiple times that “all our results apply to the multivariate case” but that they restrict themselves to the univariate case for simplicity of presentation. While this is fine, some indication of how the results are altered in the multivariate case would be useful. For example, does the fixed-width M in multivariate generalizations of Prop 3.1--3.3 need to be bigger, smaller, or the same? What other constants are dimensionally dependent? Do the multivariate generalization of their results bear the ""curse of dimensionality"", i.e., does the number of neurons needed to reach epsilon accuracy depend geometrically on the dimension?

Minor comments


A conclusion or discussion section summarizing the overall technical contribution would be useful for the reader. Also, it would be useful to include some discussion on remaining open problems or future work.

On pg. 2, the authors state “the approximation results throughout the paper guarantee that the magnitude of the weights in the network does not grow faster than polynomially in the cardinality of of the domain over which the approximation takes place”. What does “cardinality of the domain” here mean? I think the authors mean the size D of the interval [-D,D] over which the approximation is valid.

On pg. 7, the authors say “We note that this result allows to show that local cosine bases (cite) can be approximated by deep ReLU networks with exponential error decay…”. I think the authors mean to say “...this result allows us to show…” or “this result allows one to show…”. Although it’s not clear to me whether this means it has been shown (it’s a direct corollary), or could possible be shown (it’s a corollary, but needs some non-trivial work). Also, one line to specify what a “local cosine basis” is would be helpful.","The sentiment of the review is generally positive, as the reviewer acknowledges that the paper is well-written, the technical results are easy to understand, and the results are somewhat novel. However, there are some criticisms regarding the focus on specific function classes and the need for further justification and context. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, even when pointing out criticisms. Therefore, the politeness score is 90.",60,90
"This paper presents the following main insight (quoting the authors): Resnets classify input patterns based on the sum of the transient dynamics of the residuals in different layers.
The authors formulate this insight mathematically, and experimentally demonstrate its validity on a toy binary classification problem. They also show that the behaviour of a Residual network trained on MNIST is in line with the claims. Finally, a method to adapt the depth of a Residual network during training is proposed and applied to the toy classification problem.

The paper is generally of good quality and easy to understand. My only complaint: the introduction (including related work) is too long and I think it will be unclear to a general reader before reading section 2, where the terms used in the paper are explained and clarified. I think it will be an improvement to leave out detailed discussion of related work for a separate section, and focus on clarifying what the paper is about.

Overall, while the paper is related to an interesting and potentially fruitful perspective of neural networks (as dynamical systems), in my view the contributions are not significant at this stage. That the sum of transients determines network outputs is almost by design, and can be shown without a dynamical systems perspective. Using the paper’s notation, one can sum over the equations for all the layers to obtain this.

x(1) = x(0) + y(1)
…
x(T) = x(T-1) + y(T)
————————————————————————
x(T) = x(0) + sum(y(t))

Since the classification is performed using x(T), it is clear that the sum of transients is what causes the change in representation and that y(T) can be the same or not for different class inputs.

Based on my understanding, I don’t find the findings to be significantly novel or surprising. In particular, I don’t see any concrete benefits of employing a dynamical systems perspective here. Nevertheless, the experimental analysis is interesting, and I’m hopeful that this direction will lead to more insights in the future.

The final contribution is a new method to learn the depth of Residual networks during training, but this is insufficiently explored (only tested on the toy dataset) so its practical significance can not be evaluated at this stage.


Minor notes:
- Note that many citations are textual when they should be parenthetical.
- The reference “No & Liao (2016)” has incorrect author name.","The sentiment of the review is mixed. The reviewer acknowledges the quality and clarity of the paper but expresses concerns about the novelty and significance of the contributions. The sentiment score is therefore slightly positive. The language used is polite and constructive, offering specific suggestions for improvement without being harsh or dismissive.",20,80
"The authors describe a framework of how to learn a ""fair"" (demographic parity) representation that can be used to train certain classifiers, in their case facial expression and activity recognition. The method describes an adversarial framework with a constraint that bounds the distortion of the learned representation compared to the original input.

Clarity:
The paper is well written and easy to follow. The appendix is rather extensive though and contains some important parts of the paper, though the paper can be understood w/o it.

I didn't quite follow Sec 3. It is a bit sparse on the details and the final conclusion isn't entirely clear. It also isn't clear to me how general the conclusions drawn from the Gaussian mixture model are for more complex cases.

Novelty:
Adversarial fairness methods are not new, but in my opinion the authors do a good job of summarizing the literature and formalizing the problem. I am not fully familiar with the space to judge if this is enough novelty.

Using the distortion constraint is interesting and seems to work according to the experiments. Generally though, I think that distortion can be a very restrictive constraint. One could imagine representations with a very high distortion (e.g. by completely removing the sensitive attribute) and predictive qualities equivalent to the original representation. Some further discussion of this would be good.

Experiments:
The experiments are somewhat limited, but show the expected correlations (e.g. distortion vs predictiveness). 

Overall, I do believe that this work is in the right direction in this more and more popular area of great importance. I also think that contributions compared to other works could be made more clear, as well as additional experiments and discussions of the shortcomings of this approach may be added.","The review starts with a neutral to slightly positive sentiment, acknowledging that the paper is well-written and easy to follow. However, it also points out several areas for improvement, such as the clarity of Section 3, the generality of conclusions, and the limitations of the experiments. The reviewer provides constructive feedback and suggestions for improvement, which indicates a polite tone. The overall sentiment is slightly positive because the reviewer believes the work is in the right direction and acknowledges its importance, despite the noted shortcomings.",20,80
"This paper proposes an objective function for auto-encoding they
call information maximizing auto encoding (IMAE).  To set the stage
for my review I will start with the following ""classical"" formulation
of auto-encoding as the minimization of the following where we are
training models for P(z|x) and P(x|z).

beta H(z) + E_{x,z sim P(z|x)} -log P(x|z) (1)

Here H(z) is defined by drawing x from the population and then drawing
z from P(z|x).  This is equivalent to classical rate-distortion coding
when P(x|z) is an isotropic Gaussian in which case -log P(x|z) is just
the L2 distortion between x and its reconstruction.  The parameter
beta controls the trade-off between the compression rate and the L2
distortion.

This paper replaces minimizing (1) with maximizing

beta I(x,z) + E_{x,z sim P(z|x)} log P(x|z) (2)

This is equivalent to replacing H(z) in (1) by -I(x,z).  But (2)
admits a trivial solution of z=x.  To prevent the trivial solution this
paper proposes to regularize P(z) toward a
desired distribution Q(z) and replacing I(x,z) with KL(P(z),Q(z))
by minimizing

beta KL(P(z),Q(z)) + E_{x,z sim P(z|x)} - log P(x|z) (3)

The paper contains an argument that this replacement is reasonable
when Q(z) and P(z|x) are both Gaussian with diagonal covariances.  I
did not verify that argument but in any case it seems (3) is better than (2). 
For beta large (3) forces P(z) = Q(z) which fixes H(z) and the a-priori value
H(Q).  The regularization probably has other benefits.

But these suggestions are fairly simple and any real assessment of their
value must be done empirically.  The papers experiments with MNIST
seem insufficient for this.
","The sentiment of the review is somewhat positive but cautious. The reviewer acknowledges the proposed method and its potential benefits but also points out the simplicity of the suggestions and the insufficiency of the experiments. Therefore, the sentiment score is 20. The language used in the review is polite and professional, with no rude or harsh comments. The reviewer provides constructive feedback and suggests areas for improvement without being dismissive or disrespectful. Therefore, the politeness score is 80.",20,80
"This work proposes a learning method based on deep subspace clustering. The method is formulated by identifying a deep data embedding, where clustering is performed in the latent space by a revised version of k-means, inspired by the work [1]. In this way, the proposed method can adapt to account for uni-modal distributions. The authors propose some variations of the framework based on soft cluster assignments, and on cumulative learning of the cluster means.
The method is tested on several scenarios and datasets, showing promising results in prediction accuracy.

The idea presented in this work is reasonable and rather intuitive. However, the paper presentation is often unnecessarily convoluted, and fails in clarifying the key points about the proposed methodology. The paper makes often use of abstract terms and jargon, which sensibly reduce the manuscript clarity and readability. For this reason, in my opinion, it is very difficult to appreciate the contribution of this work, from both methodological and applicative point of view. 

Related to this latter point, the use of the term “Bayesian nonparametric” is inappropriate. It is completely unclear in which sense the proposed framework is Bayesian, as it doesn’t present any element related to parameters inference, uncertainty estimation, … Even the fact that the method uses an algorithm illustrated in [1] doesn’t justifies this terminology, as the clustering procedure used here only corresponds to the limit case of a Dirichlet Process Gibbs Sampler when the covariance parameters goes to zero. Moreover, the original procedure requires the iteration until convergence, while it is here applied with a single pass only. The procedure is also known to be sensitive to the order by which the data is provided, and this point is not addressed in this work. 

Finally, the novelty of the proposed contribution is questionable. To my understanding, it may consist in the use of embedding methods based on the approach provided in [1]. However, for the reasons illustrated above, this is not clear. There is also a substantial amount of literature on deep subspace embeddings that proposes very similar methodologies to the one of this paper (e.g. [2-5]).  For this reason, the paper would largely benefit from further clarifications and comparison with respect to these methods.  





[1] Kulis and Jordan,  Revisiting k-means: New Algorithms via Bayesian Nonparametrics, ICML 2012

[2] Xie, Junyuan, Ross Girshick, and Ali Farhadi. ""Unsupervised deep embedding for clustering analysis."" International conference on machine learning. 2016.
[3] Ji, Pan, et al. ""Deep subspace clustering networks."" Advances in Neural Information Processing Systems. 2017.
[4] Jiang, Zhuxi, et al. ""Variational deep embedding: An unsupervised and generative approach to clustering."" IJCAI 2017
[5] Kodirov, Elyor, Tao Xiang, and Shaogang Gong. ""Semantic autoencoder for zero-shot learning. CVPR 2017.","The sentiment of the review is mixed but leans towards negative. While the reviewer acknowledges that the idea is reasonable and the results are promising, they also point out significant issues with the paper's presentation, clarity, and the appropriateness of the terminology used. The reviewer also questions the novelty of the contribution. Therefore, the sentiment score is -40. The language used in the review is critical but remains professional and constructive, suggesting ways to improve the paper. Thus, the politeness score is 50.",-40,50
"This paper proposes an unpaired image-to-image translation method which applies the co-segmentation network and adaptive instance normalization techniques to enable the manipulation on the local regions.

Pros:
* This paper proposes to jointly learn the local mask to make the translation focus on the foreground instead of the whole image.
* The local mask-based highway adaptive instance normalization apply the style information to the local region correctly.

Cons:
* There seems a conflict in the introduction (page 1): the authors clarify that “previous methods [1,2,3] have a drawback of ....” and then clarify that “[1,2,3] have taken a user-selected exemplar image as additional input ...”. 
* As the main experiments are about facial attributes translation, I strongly recommend to the author to compare their work with StarGAN [4]. 
* It is mentioned in the introduction (page 2) that “This approach has something in common with those recent approaches that have attempted to leverage an attention mask in image translation”. However, the differences between the proposed method with these prior works are not compared or mentioned. Some of these works also applied the mask technique or adaptive instance normalization to the image-to-image translation problem. I wonder the advantages of the proposed method compared to these works.
* The experiment setting is not clear enough. If I understand correctly, the face images are divided into two groups based on their attributes (e.g. smile vs no smile). If so, what role does the exemplar image play here? Since the attribute information has been modeled by the network parameters, will different exemplar image lead to different translation outputs? 
* The github link for code should not provide any author information.

[1] Multimodal Unsupervised Image-to-Image Translation
[2] Diverse Image-to-Image Translation via Disentangled Representations
[3] Exemplar Guided Unsupervised Image-to-Image Translation
[4] StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation

Overall, I think the proposed method is well-designed but the comparison and experiment setting are not explained well. My initial rating is weakly reject.","The sentiment of the review is mixed but leans slightly positive. The reviewer acknowledges the strengths of the paper, such as the novel approach to local mask-based adaptive instance normalization, but also points out several significant issues that need to be addressed. Therefore, the sentiment score is 20. The politeness of the language is quite high; the reviewer uses polite phrases like 'I strongly recommend' and 'I wonder,' and provides constructive feedback without being harsh or dismissive. Thus, the politeness score is 80.",20,80
"The paper presents an inference method (implicit distribution particle smoothing) for neural Hawkes processes that accounts for latent sequences of events that influence the observed trajectories.

Quality
+ The paper combines ideas from multiple areas of machine learning to tackle a challenging task of inference in multivariate continuous-time settings.
- The figures reported from the paper are comparative graphs with respect to particle filtering, and so the absolute level of performance of the methods is not characterized.  Reporting of distribution of sample weights and or run-times/complexity would strengthen the paper.

Clarity
- notation is complex replete with symbols ""@"" and text in math formulas
- It's not clear what p (""the data model"") and p_miss (""the missingness mechanism"") represent, and therefore why in equation 1: p(x,z) = p(xvz)p_miss(z| xvz) where v is the union symbol.  In addition, how it's related to MAR and MNAR is unclear. If e.g. following Murphy, one writes MAR as: p(r|x_u, x_o) = p(r|x_o), r is a missingness vector, x_u is x unobserved, and x_o is x observed, then r corresponds to observation or not, whereas in the manuscript p_miss is on the values themselves, i.e. on the space where z={k_{i,j}@t_{i,j}} resides.  We know, from the definition of MNAR that we can't use only the observed data to correctly infer the distributions of the missing values, and so while one can probabilistically predict in MNAR setting, their quality remains unknown.  If none of the experiments touch upon MNAR data, perhaps it is possible to omit this part.

Originality
+ the work is rich, complex, original, and uses leading methods from multiple areas of ML.

Significance
+ the significance of this work could be high, as it may provide a way to conduct difficult inference in an effective way to produce increasingly flexible modeling of trajectories amidst partial observation.
- however the exposition (particularly the experiments) does not fully demonstrate this.","The sentiment of the review is mixed, with both positive and negative aspects highlighted. The reviewer appreciates the originality and potential significance of the work, but also points out several areas for improvement, particularly in clarity and the presentation of experimental results. Therefore, the sentiment score is 20, indicating a slightly positive but cautious outlook. The politeness score is 80, as the language used is constructive and respectful, even when pointing out weaknesses.",20,80
"Overview: 
This nicely written paper contributes a useful variance reduction baseline to make the recent formalism of the DiCE estimator more practical in application. I assess the novelty and scale of the current contribution as too low for publication at ICLR. Also, the paper includes a few incorrect assertions regarding the control variate framework as well as action-dependent baselines in reinforcement learning. Such issues reduce the value of the contribution in its current form and may contribute to ongoing misunderstandings of the control variate framework and action-dependent baselines in RL, to the detriment of variance reduction techniques in machine learning. I do not recommend publication at this time.

Pros:
The paper is well written modulo the issues discussed below. It strikes me as a valuable workshop contribution once the errors are addressed, but it lacks enough novelty for the main conference track.

Issues:

* (p.5) ""R_w and b_w are positively correlated by design, as they should be for variance reduction of the first order gradients.""

This statement is not true in general. Intuitively, a control variate reduces variance because when a single estimate of an expectation of a function diverges from its true value according to some delta, then, with high probability, some function strongly correlated with that function will also diverge with a similar delta. Such a delta might be positive or negative, so long as the error may be appropriately modeled as drawn from some symmetric distribution (i.e. is Gaussian).

Control variates are often estimated with an optimal scaling constant that depends on the covariance of the original function and its control variate. Due to the dependence on the covariance, the scaling constant flips sign as appropriate in order reduce variance for any delta. For more information, see the chapter on variance reduction and subsection on control variates in Sheldon Ross's textbook ""Simulation.""

The fact that a control variate appears to work despite this is not surprising. Biased and suboptimal unbiased gradient estimators have been shown to work well for reasons not fully explored in the literature yet. See, for example, Tucker et al.'s ""Mirage of Action-Dependent Baselines"", https://arxiv.org/abs/1802.10031.

Since the authors claim on page 6 that the baseline is positively correlated by design, this misunderstanding of the control variate framework appears to be baked into the baseline itself. I recommend the authors look into adaptively estimating an optimal scale for the baseline using a rolling estimator of the covariance and variance to fix this issue. See the Ross book cited above for full derivation of this optimal scale.

* The second error is a mischaracterization of the use and utility of action-dependent baselines for RL problems, on page 6: ""We choose the baseline ... to be a function of state ... it must be independent of the action ...."" and ""it is essential to exclude the current action ... because the baselines ... must be independent of the action ... to remain unbiased."" In the past year, a slew of papers have presented techniques for the use of action-dependent baselines, with mixed results (see the Mirage paper just cited), including two of the papers the authors cited.

Cons
* Much of paper revises the DiCE estimator results, arguing for and explaining again those results rather than referring to them as a citation. 
* I assess the novelty of proposed contribution as too low for publication. The baseline is an extension of the same method used in the original paper, and does not generalize past the second order gradient, making the promising formalism of the DiCE estimator as infinitely differentiable still unrealizable in practice.
* The experiments are practically identical to the DiCE estimator paper, also reducing the novelty and contribution of the paper.

*EDIT: 
I thank the authors for a careful point-by-point comparison of our disagreements on this paper so that we may continue the discussion. However, none of the points I identified were addressed, and so I maintain my original score and urge against publication. In their rebuttal, the authors have defended errors and misrepresentations in the original submission, and so I provide a detailed response to each of the numbered issues below:

(1) I acknowledge that it is common to set c=1 in experiments. This is not the same as the misstatements I cited, verbatim, in the paper that suggest this is required for variance reduction. My aim in identifying these mistakes is not to shame the authors (they appear to simply be typos) but simply to ensure that future work in this area begins with a correct understanding of the theory. I request again that the authors revise the cited lines that incorrectly state the reliance of a control variate on positive correlation. It is not enough to state that ""everyone knows"" what is meant when the actual claim is misleading.

(2) Without more empirical investigation, the authors' new claim that a strictly state-value-function baseline is a strength rather than a weakness cannot be evaluated. This may be the case, and I would welcome some set of experiments that establish this empirical claim by comparing against state-action-dependent baselines. The authors appear to believe that state-action-dependent baselines are never effective in reducing variance, and this is perhaps the central error in the paper that should be addressed. See response (3). Were the authors to fix this, they would necessarily compare against state-action-dependent baselines, which would be of great value for the community at large in settling this open issue.

(3) Action-dependent baselines have not been shown to be ineffective. I wish to strongly emphasize that this is not the conclusion of the Mirage paper, and the claim repeated in the authors' response (3) has not been validated empirically or analytically, and does not represent the state of variance reduction in reinforcement learning as of this note. I repeat a few key arguments from the Mirage paper in an attempt to dispel the authors' repeated misinterpretation of the paper.

The variance of the policy gradient estimator, subject to a baseline ""phi,"" is decomposed using the Law of Total Variance in Eq (3) of the Mirage paper. This decomposition identifies a non-zero contribution from ""phi(a,s)"", the (adaptive or non-adaptive) baseline. The Mirage paper analyzes under what conditions such a contribution is expected to be non-negligible. Quoting from the paper:
""We expect this to be the case when single actions have a large effect on the overall discounted
return (e.g., in a Cliffworld domain, where a single action could cause the agent to fall of the cliff and suffer a large negative reward).""
Please see Sec. 3, ""Policy Gradient Variance Decomposition"" of the Mirage paper for further details.
The Mirage paper does indeed cast reasonable doubt on subsets of a few papers' experiments, and shows that the strong claim, mistakenly made by these papers, that state-action-dependence is always required for an adaptive control variate to reduce variance over state dependence, is not true. 

It should be clear from the discussion of the paper to this point that this does _not_ imply the even stronger claim in ""A Better Second Order Baseline"" that action dependence is never effective and should no longer be considered as a means to reduce variance from a practitioner's point of view. Such a misinterpretation should not be legitimized through publication, as it will muddy the waters in future research. I again urge the authors to remove this mistake from the paper.

(4) I acknowledge the efforts of the authors to ensure that adequate background is provided for readers. This is a thorny issue, and it is difficult to balance in any work. Since this material represents a sizeable chunk of the paper and is nearly identical to existing published work, it leads me to lower the score for novelty of contribution simply by that fact. Perhaps the authors could have considered placing the extensive background materials in the appendix and instead summarizing them briefly in the body of the paper, leaving more room for discussion and experimental validation beyond the synthetic cases already studied in the DiCE paper.

(5), (6) In my review I provided specific, objective criteria by which I have assessed the novelty of this paper: the lack of original written material, and the nearly identical experiments to the DiCE paper. As I noted in response (4) above, this reduces space for further analysis and experimentation.","The sentiment of the review is mixed but leans towards negative. The reviewer acknowledges the paper's contribution and writing quality but ultimately finds the novelty and correctness insufficient for publication. The sentiment score is -40 because the review contains both positive and negative elements but leans more towards the negative due to the recommendation against publication. The politeness score is 60 because the reviewer uses polite language, acknowledges the authors' efforts, and provides constructive feedback, even though the review is critical.",-40,60
"This paper proposes a new policy gradient method for reinforcement learning.
The method essentially combines SARAH and trust region method using Fisher information matrix.
The effectiveness of the proposed method is verified in experiments.

SARAH is a variance reduction method developed in stochastic optimization literature, which significantly accelerates convergence speed of stochastic gradient descent.
Since the policy gradient often suffers from high variance during the training, a combination with variance reduction methods is quite reasonable.
However, this work seems to be rather incremental compared to a previous method adopting another variance reduction method (SVRG) [Xu+2017, Papini+2018].
Moreover, the advantage of the proposed method over SVRPG (SVRG + policy gradient) is unclear both theoretically and experimentally.
[Papini+2018] provided a convergence guarantee with its convergence rate, while this paper does not give such a result.
It would be nice if the authors could clarify theoretical advantages over SVRPG.

Minor comment:
- The description of SVRG updates in page 2 is wrong.
- The notation of H in Section 3.1 (""ODE analysis"") is not defined at this time.
","The sentiment of the review is mixed, leaning towards neutral. The reviewer acknowledges the reasonableness of combining variance reduction methods with policy gradients but criticizes the work for being incremental and lacking clear advantages over existing methods. The sentiment score is therefore 0. The politeness of the language is generally respectful and constructive, with suggestions for improvement and minor corrections provided in a courteous manner. The politeness score is 80.",0,80
"The paper considers the problem of (Generalized) Zero-Shot Learning. Most zero-shot learning methods embed images and text/attribute representations into a common space. The main difference here seems to be that Variational AutoEncoder (VAEs) are used to learn the mappings that take different sources as input (images and attributes). 
As in JMVAE (Suzuki et al., 2016) (which was not proposed for zero-shot learning), decoders are then used to reconstruct objects from the latent space to the input sources.

My main concerns are about novelty. The contribution of the paper is limited or not clear at all, even when reading Section C in the appendix. The proposed approach is a straightforward extension of JMVAE (Suzuki et al., 2016) where a loss function is added (Eq. (3)) to minimize the KL divergence between the outputs of the encoders (which corresponds to optimizing the same problem as most zero-shot learning approaches).
The theoretical aspect of the method is then limited since the proposed loss function actually corresponds to optimize the same problem as most zero-shot learning approaches but with VAEs.

Concerning experiments, Generalized Zero shot learning (GZSL) experiments seem to significantly outperform other methods, whereas results on the standard zero-shot learning task perform as well as state-of-the-art methods. 
Do the authors have an explanation of why the approach performs significantly better only on the GZSL task?

In conclusion, the contributions of the paper are mostly experimental. Most arguments in the model section are actually simply intuitions.


after the rebuttal:
After reading the different reviews, the replies of the authors and the updated version, my opinion that the ""explanations"" are simply intuitions (which is related to AnonReviewer3's concern ""Regarding advantages of learning a joint model as opposed to unidirectional mappings"") has not been completely addressed by the authors. Fig. 4 does address this concern by illustrating their point experimentally. However, I agree with AnonReviewer3 that the justification remains unclear.","The sentiment of the review is moderately negative. The reviewer expresses significant concerns about the novelty and theoretical contributions of the paper, stating that the proposed approach is a straightforward extension of existing work and that the theoretical aspect is limited. The reviewer does acknowledge that the experimental results are strong in the GZSL task but questions the lack of explanation for this. The politeness of the language is relatively high; the reviewer uses formal and respectful language throughout, even when expressing criticism. They provide specific feedback and acknowledge the authors' efforts in the rebuttal, although they maintain their concerns.",-40,80
"The proposed approach aims to mitigate catastrophic forgetting in continual learning (CL) problems by structure learning: determining whether to reuse or adapt existing parameters, or initialise new ones, when faced with a new task. This is framed as an architecture search problem, applying ideas from Differentiable Architecture Search (DARTS). The approach is verified on the Permuted MNIST dataset and evaluated on the Visual Decathlon, showing an improvement.

I think this is an interesting idea with potential, and is worth exploring, and the paper is well-structured and easy to follow.

Unfortunately, I feel the paper fails to consider recent work on CL, both in terms of discussion and benchmarking. The only previous work that is compared is EWC, on permuted MNIST, and the Visual Decathlon performance is only compared to simple baselines (such as adding an adapter or fine tuning) which makes it difficult to gauge the contribution.
There are recent works, some with better results on more difficult problems, such as Variational Continual Learning [1], Progress and Compress [2], or (Variational) Generative Experience Replay [3][4].
Given the approach is based on dynamically adding parameters or modules, Progressive Networks and Dynamically Expandable Networks (both cited) are especially relevant and should be compared (I believe the former may be related to the “adapter” baseline, but this should be made explicit).

I have some questions / discussion points:
- What's the intuition behind implementing the “adapt” operator as additive bias over the previous weights, rather than just copying the previous weights and fine tuning?
- In the general case, if the architecture search is a continuous relaxation (softmax combination of operators), why is the ""adapt"" operator necessary? Wouldn't this already be a linear combination of new and old parameters? (In the example case of a 1×1 adaptor it makes sense, but this is a special restricted case which adapts with a smaller set of parameters)
- How is the structure regulariser backpropagated into the parameters of each layer? As I understand, it is composed of a constant discrete term z (number of parameters in each option), multiplied by architecture softmaxes alpha; the gradient with respect to each alpha is a constant, and so this has the effect of scaling the gradients of each operator.
- For the ""reuse - tuned"" case, isn’t the model effectively maintaining a new network for each task?

I also have a number of other comments:
- Reference to figure in page 6 should be figure 4, not 5.
- I think the readability of the paper would benefit from another few proofreads; there are a number of grammatical issues throughout, and several sentence fragments, eg. in the top para of page 2: “..., it has the potential to encourage information sharing. Since now the irrelevant part can be handled…”.

I would encourage the authors to strengthen the experimental comparison by incorporating stronger, external baselines, and improving some of the minor writing issues.

[1] Nguyen, Cuong V., et al. ""Variational Continual Learning."" ICLR, 2018.
[2] Schwarz, Jonathan, et al. ""Progress & Compress: A scalable framework for continual learning."" ICML, 2018.
[3] Shin, Hanul, et al. ""Continual learning with deep generative replay."" NIPS, 2017.
[4] Farquhar, Sebastian, and Yarin Gal. ""Towards Robust Evaluations of Continual Learning."" arXiv, 2018.","The sentiment of the review is generally positive, as the reviewer acknowledges the potential and interesting nature of the idea, as well as the well-structured and easy-to-follow presentation of the paper. However, the sentiment is tempered by significant criticisms regarding the lack of comparison with recent work and the need for stronger experimental baselines. Therefore, the sentiment score is 30. The politeness of the language is high, as the reviewer uses polite phrases such as 'I think,' 'I feel,' and 'I would encourage,' and provides constructive feedback without being harsh or dismissive. Therefore, the politeness score is 80.",30,80
"This paper proposed a LBPNet for character recognition, which introduces the LBP feature extraction into deep learning. Personally I think that this idea is interesting for improving the efficiency of CNNs, as traditionally LBP has been demonstrated its good performance and efficiency in some vision tasks such as face recognition or pedestrian detection. However, I do have the following concerns about the paper:

1. Calculation/Implementation of Eq. 4: I do not quite understand how it derived, and how to use Eq. 3 in calculation. I suggest the authors to explain more details, as this is the key for implementation of LBP layers.

2. Effects of several factors on performance in the experiments are missing: (1) random projection map in Fig. 5, (2) $k$ in Eq. 2, and (3) the order of images for computing RHS of Eq. 3. In order to better demonstrate LBPNet, I suggest to add such experiments, plus training/testing behavior comparison of different networks. 

3. Does this network work with more much deeper?

4. Data: The datasets used in the experiments are all well-aligned. This makes me feel that the RHS of Eq. 3 does make sense, because it will capture the spatial difference among data, like temporal difference in videos. How will the network behave on the dataset that is not aligned well, like affnist dataset?

5. How will this network behave for the applications such as face recognition or pedestrian detection where traditionally LBP is applied?
","The sentiment of the review is generally positive, as the reviewer finds the idea interesting and acknowledges the potential for improving the efficiency of CNNs. However, the reviewer also has several concerns and suggestions for improvement, which slightly tempers the overall positivity. Therefore, the sentiment score is 40. The politeness of the language is quite high, as the reviewer uses phrases like 'I suggest' and 'I think,' which are polite and constructive. The reviewer also provides specific recommendations without being harsh or dismissive. Therefore, the politeness score is 80.",40,80
"The main context for this paper is two recent publications: Giusti et al.’s ""A Machine Learning Approach to Visual Perception of Forest Trails for Mobile Robots” (2016) and Smolyanskiy et al.’s ""Toward Low-Flying Autonomous MAV Trail Navigation using Deep Neural Networks for Environmental Awareness” (2017). 

Giusti introduced a dataset of trail images (later called the “IDSIA dataset”) acquired by having a hiker wear three head-mounted cameras. The forward facing image is associated with a label “go straight”, whereas the two side images are associated with labels for “go left” and “go right”. Giusti then trained a convolutional neural network to predict these labels and used the network to guide a ""quadrotor micro aerial vehicle”. 

Smolyanskiy improves on Giusti’s work by (1) gathering additional trail image data using three cameras mounted to face forward but with lateral offsets and (2) using this additional data to train a 6 output neural network (“Trailnet”) which predicts both view orientation and lateral offset. In addition, they also combined predicted pose relative to the trail with predictions of localized objects and a depth map for potential obstacles. They compared several neural network architectures for predicting the view angle on the IDSIA data as well as the closed-loop performance of each network in avoiding collisions while operating within a UAV on a previously unseen trail. Though Trailnet did not achieve the highest accuracy (84% vs. the max 92% achieved by ResNet-18), it was the only network that achieved 100% collision avoidance on their UAV test course. 

This paper, ""A CASE STUDY ON OPTIMAL DEEP LEARNING MODEL FOR UAVS”, attempts to evaluate two potentially better convolutional neural networks for UAV trail guidance. They fine tune pertained Inception-Resnet and MobileNet models to predict the IDSIA dataset. These then both achieve better accuracy on the IDSIA test set and were analyzed for inference time and power consumption. These two models are then run through a single simulated path, where both seem to perform adequately across 2 turns in the path. 

This paper has a variety of essential flaws.

1. A large portion of the text is devoted to their hardware and UAV control but they were not able to actually run models on a physical UAV ""due to a hardware bug we were facing with the FCU”. 
2. The paper claims to ""introduce to the best of our knowledge, a very first comparative study of three algorithms in order to find a better motion control of a drone for detecting a trail”. This is a confusing claim since a comparison of neural network architectures is a central part of the evaluation in the Smolyanskiy paper. 
3. The higher accuracy on view orientation does not seem relevant since it was also achieved by Smolyanskiy et al. with networks that they then showed performed worse when combined with object detection, obstacle depth inference and combined controller.
4. The sentence ""An important goal of our method is to demonstrate the effectiveness of low cost systems for the complex task of flying an autonomous drone” appears to have been plagiarized from “Learning to Fly by Crashing” (2017) which contains ""A important goal of our method is to demonstrate the effectiveness of low cost systems for the complex task of flying in an indoor environment”. ","The sentiment of the review is quite negative, as it highlights several essential flaws in the paper, including issues with hardware, misleading claims, irrelevant accuracy improvements, and potential plagiarism. The reviewer does not express any positive aspects of the paper. The politeness of the language is relatively neutral; while the reviewer points out significant issues, they do so in a factual and non-confrontational manner without using derogatory or rude language.",-80,10
"=== Post-rebuttal update ===

The authors' rebuttal provided many of the details I was seeking. I asked a few additional questions which were also recently addressed, and I encourage the authors to include these clarifications into the final draft of the paper.

Hence, I've increased my score for this paper.

=== Pre-rebuttal review ===
This paper presents a meta-learning approach to zero-shot learning. The idea is to train a correction module which is trained to produce a correction to the output of a previously trained task module. The hypothesis is that the correction should depend on the nature of the training data of the task module, and so the correction module receives as input a representation of the training data of the task module. An episodic approach is then used for training the correction module, whereby many different task modules are trained on various subsets of the total training data, the rest being used as unseen data for the correction module.

The proposed idea is original and the results are strong. Generally, I'd be inclined to see this paper published.

However right now, the paper lacks A LOT of details on how the experiments were run. I would like to see these answered in the rebuttal, before I consider raising my rating for this paper:
- What are the architectures used for M_T and M_C?
- What distance functions was used for training?
- What optimizer was used for training?
- How was convergence established in the inner and outer while loops of algorithm 1?
- Text mentions that before evaluation, M_T is trained on all data in D_S. How is this done exactly (e.g. how is convergence assessed)?
- How is the T_S computed exactly?
- How expensive is it to run Algorithm 1 (i.e. to train the correction module)? Since a new task module M_T needs to be trained for each subset S^s, it seems like it might be expensive to run... if not, why?

I would also strongly suggest the authors release their code if this paper ends up being published.

In summary:

Pros
- Claims SOTA results on two good benchmarks for zero-shot learning
- Approach is original

Cons
- Paper lacks a lot of methodological and experimental details

Some minor details:

- ""We found the task module performance improves slightly when the output of the task module is feed into a classifier with a single hidden layer that is also trained to classify samples from the task model’s training dataset."" => I don't understand what this means. Isn't the output of the task module already trained to classify samples from its training dataset? So why is this additional single hidden layer needed?
- Typos:
  - on few shot learn => on few shot learning
  - but needs not => but need not
  - image image classification => image classification
  - the the compatibility => the compatibility
  - psuedo => pseudo
  - ""The task module is trained to minimize"" => that reads like an unfinished sentence
  - \hat{\mu}_U \hat{\mu}_U => \hat{\mu}_U 
  - inputted => input
  - FOr => For
  - it's inputs => its inputs
  - otherhand => other hand","The sentiment of the review is generally positive, as indicated by phrases like 'The proposed idea is original and the results are strong' and 'I'd be inclined to see this paper published.' However, the reviewer also points out significant shortcomings in the paper, particularly the lack of methodological and experimental details, which tempers the overall positivity. Therefore, the sentiment score is 50. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, such as 'I would like to see these answered' and 'I encourage the authors to include these clarifications.' Therefore, the politeness score is 80.",50,80
"AFTER REBUTTAL:
I think that in its current version the paper is not yet ready for publication. Several issues have been raised by fellow reviewers as well. I think that they are not trivial and they regard key aspects like paper structure, quality of exposition and experimental analysis. I have detailed my initial opinion in response to the author request for more details.  I hope this will serve as useful  guidelines for improving the paper in the future. 

------------
The method tackles the problem of interpretability that is a very important issue for usually black-box deep networks. Unfortunately it is not very clear how is the achieved. I have read several times the part explaining the influence maps and the clustering based on them and it still doesn't make a lot of sense to me. I think that part has to be better justified and exposed. Moreover, results do not support the claim which makes me doubt even more about how effective the method proposed actually is. In conclusion, I think that better exposition and more solid experimental analysis is needed.  

Also please check some writing problems: 

> Introduction: 
""to acquire a generative function mapping a latent space (such as Rn)"" >  difficult to read, rephrase. 
""making it difficult to add human input"" > confusing. What do you mean by human input? I assume you refer to having control to make decisions about design. 

> Section 3.1
""the internal variable may leave the manifold it is implicitly embedded in as a result of the model’s training"" : not clear, rephrase. 

","The sentiment of the review is moderately negative. The reviewer clearly states that the paper is not ready for publication and highlights several significant issues, including the structure, quality of exposition, and experimental analysis. This indicates a sentiment score of -60. The politeness of the language used is quite high. The reviewer provides constructive feedback and suggestions for improvement without being harsh or dismissive. Phrases like 'I hope this will serve as useful guidelines for improving the paper in the future' and 'please check some writing problems' indicate a polite tone, resulting in a politeness score of 80.",-60,80
"This paper presents an empirical analysis of the convergence of deep NN training (in particular in language models and speech).

Studying the effect of various hyperparameters on the convergence is certainly of great interest. However, the issue with this paper is that its analyses are mostly *descriptive*, rather than conclusive or even suggestive. For example, in Figure 2, it is shown that the convergence slope of Adam is steeper than that of SGD, when the x-axis is the model size. Very naturally I would be interested in a hypothesis like “Adam converges quicker than SGD as we increase the model size”, but there is no discussion like that. Throughout the paper there are many experimental results, but results are presented one after another, without many conclusions or suggestions made for practice. I don’t have a good take-away after reading it.

The writing of this paper also needs to be improved significantly. In particular, lots of statements are made casually without justification. For example,

“If hidden dimension is wide enough to absorb all the information within the input data, increasing width obviously would not affect convergence” -- Not so obvious to me, any reference? 

“Figure 4 shows a sketch of a model’s convergence curve ...” -- it’s not a fact but only a hypothesis. For example, what if for super large models the convergence gets slow and the curve gets back up again?

In general, I think the paper is asking an interesting, important question, but more developments are needed from these initial experimental results.","The sentiment of the review is moderately negative. The reviewer acknowledges the importance of the topic but criticizes the paper for being mostly descriptive and lacking conclusive or suggestive analyses. The reviewer also points out significant issues with the writing, such as casual statements without justification. Therefore, the sentiment score is -40. The politeness of the language is relatively high; the reviewer uses polite language and constructive criticism, even though the feedback is critical. Therefore, the politeness score is 60.",-40,60
"In this paper, the authors propose a heuristic method to overcome the exploration in RL. They store trajectories which result in novel states. 
The final state of the trajectory is called goal state, and the authors train a path function which given a state and a subgoal states (some states in the trajectory) the most probably action the agent needs to take to reach the subgoal. These way they navigate to the goal state. The goal state is claimed to be achieved if the feature representation stoping state is close to goal (or subgoal for subgoal navigation).


The authors mainly combine a few previous approaches ""Self-Imitation Learning,"" ""Automatic Goal Generation for Reinforcement Learning Agents,"" and ""Curiosity-driven exploration by self-supervised prediction"" to design this algorithm which makes this approach less novel.

General comment; there are variable and functions in the paper that are not defined, at least at the time, they have been used. The Rooms environment is not described. What is visit_times[x] and x is not a wall? What is stage avg reward? and many others

The main idea of the algorithm is clear, but the description of the pieces is missing.

It is not clear in stochastic setting how well this approach will perform. 

The authors state that
""Among different choices of the modeling, we choose inverse dynamics (Pathak et al., 2017) as the environment model, which has been proved to be an effective way of representing states under noisy environments.""
I took a look at this paper and could not find neither proof or quantification of ""effective""-ness. Please clarify what the meaning this statement is.

Why s=s' is ambiguous to the inverse dynamics?

What is the definition of acc in fig2?

why (consin+1)^3/8 is chosen?
","The sentiment of the review is slightly negative. The reviewer acknowledges the main idea of the algorithm but points out several issues such as lack of novelty, missing definitions, and unclear explanations. The sentiment score is -40. The politeness of the language is neutral to slightly polite. The reviewer uses formal language and avoids harsh criticism, but the tone is straightforward and lacks positive reinforcement. The politeness score is 20.",-40,20
"The paper presents a new version of CIFAR10 that is labelled by multiple people (the test part of the data). They use it to improve the calibration of several image classifiers through “fine-tuning” and other techniques
The title is too general, taking into account that this setting has appeared in classification in many domains, with different names (learning from class distributions, crowd labellers, learning from class scores, etc.). See for instance,
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3994863/
http://www.cs.utexas.edu/~atn/nguyen-hcomp15.pdf 
Also, at the end of section 2 we simply reach logloss, which is a traditional way of evaluating the calibration of a classifier, but other options exist, such as the Brier score. At times, the authors mention the trade-off between classification accuracy and cross-entropy. This sounds very much the trade-off between refinement and calibration, as one of the possible decompositions of the Brier score.
The authors highlight the limitations of this work, and they usually mention that the problem must be difficult (e.g., low resolution). Otherwise, humans are too good to be useful. I suggest the authors to compare with psychophysics and possible distortions of the images, or time limits for doing the classifications. 
Nevertheless, the paper is not well motivated, and the key procedures, such as “fine-tuning” lack detail, and comparison with other options.
In section 2, which is generally good and straightforward, we find that p(x|c) being non-overlapping as a situation where uncertainty would be not justified. Overlap would simply say that it is a categorisation (multilabel classification) problem rather than a classification problem, but this is different from the situation where labels are soft or given by several users. 
In the end, the paper is presented from the perspective of image recognition, but it should be compared with many other areas in classification evaluation where different metrics, presentation of the data, levels of uncertainty, etc., are used, including different calibration methods, as alternatives to the expensive method presented here based on crowd labelling.
Pros:
-	More information about borderline cases may be useful for learning. This new dataset seems to capture this information.
Cons:
-	The extra labelling is very costly, as the authors recognise.
-	The task is known in the classification literature, and a proper comparison with other approaches is required.
-	Not compared with calibration approaches or other ways where boundaries can be softened with less information from human experts. For instance, a cost matrix about how critical a misclassification is considered by humans (cat <-> dog, versus cat <-> car) could also be very useful, and much easier to obtain.
","The sentiment of the review is mixed. The reviewer acknowledges some positive aspects, such as the potential usefulness of the new dataset for learning borderline cases, but overall, the review is critical of the paper's motivation, detail, and comparisons with existing methods. Therefore, the sentiment score is slightly negative. The language used in the review is generally polite, with suggestions and constructive criticism provided in a respectful manner, although there are some direct critiques. Thus, the politeness score is positive.",-20,60
"Quality: The paper proposed a new method to learn some physics prior in an environment along with a new SpatialNetwork Architecture. Instead of learning a specific dynamics model, they propose to learn a dynamics model that is action-free, purely learning the extrinsic dynamics.  They formulate this problem as a video prediction problem. A series of experiments are conducted on PhysWorld (a new physics based simulator) and a subset of Atari games.
Clarity: The writing is good.
Originality: This work is original as most of the model-based RL works are focusing on learning one environment instead of common rules of physics.
significance of this work: This work propose an interesting direction to pursue.

cons:
1. In Figure 4, the authors show that a pretrained model can learn faster than random initialization. However, it is hard to ablate the factor that causes this effect.  Does the dynamics predictor learn the physics priors or is it just because it learn the visual prior of the shape of the objects, etc? 
2. The baseline for atari games is quite limited. First of all, 3 out of 5  atari games  in the original PPO paper show that ACER performs better than PPO. (asteroid, breakout, DemonAttack). I think it is better to make some improvement upon state-of-the-art methods.
3. All the experiments are shown with only 3 random seeds, without error bar in the main paper. Although the reward plots are shown in Figure 11. 
4. 5 out of 10 atari games are similar to PPO (according to Figure 11). It's hard to be conclusive when half of the experiments are positive and the rest are not. 
5. Lack of discussion about ego-dynamics. There are physics priors for both the environment and the controller. Usually the controller/agent  requires an action to predict its dynamics. Then why should we omit the ego-dynamics and only model the outer world. 
6. Physics prior usually happen in physical environment. The proposed method works well in the physworld environments. But is there some task that are more realistic than atari games that can leverage the power of physics priors more? It's good that this method works in some atari games. But isn't learning the dynamics of atari games a bit off the topic? 
7. The transfer learning experiments should contain a baseline -- maml/reptile. Since you are learning physics prior, it is fair to add meta-learning baselines for comparison.

I think the direction is interesting and the effort is made well. But the experiments are less convincing than the abstract/introduction.
","The sentiment of the review is generally positive, as indicated by phrases like 'The writing is good,' 'This work is original,' and 'This work propose an interesting direction to pursue.' However, the reviewer also lists several cons and areas for improvement, which tempers the overall positivity. Therefore, the sentiment score is 40. The politeness of the language is quite high, as the reviewer uses polite and constructive language throughout the review, such as 'I think it is better to make some improvement' and 'It's good that this method works in some atari games.' Therefore, the politeness score is 80.",40,80
"The paper proposes an alignment of two manifolds that is performed in a low-dimensional parameter space corresponding to a low-pass ""filtering"" of the Graph Fourier transform obtained from the underlying data graphs for the two manifolds. The numerical results show the quality of the alignment for some toy image datasets and a biological dataset.

The derivation of the technical details of the approach is not clear - see the comments below on Pages 5,6 and 9 in particular. The paper is not clear enough for acceptance at this point.

Detailed comments:

Page 2: Grammar error ""that is invariant batch effects"". When denoising is discussed, can you explain whether this is denoising or simply regularization? When is the selected subspace a good approximation for the ""signal subspace""?
Page 3: Should X^(S) be X^(s)? When W and W(s) are defined, do they also rely on a neighborhood graph? It appears that in the definition of psi_j the eigenvectors phi_j should be obtained from W, not P (which is how they are defined earlier in the page).
Page 4: There is an abuse of notation on f, used both as a linear function on X(s) and an element of X(s).
Page 5: Typos ""exlpained"", ""along the along the"". It is not clear what applying a window to eigenvalues means, or what the notation g_xi(lambda) means. The construction of the filters described here needs to be more explicit. h_xi is undefined. How is H in (1) defined when i = 1?
Page 6: M should be M(s1,s2). Typesetting error in Lambdabar(s). Which matrix is referred to in ""the laplacian eigenvalues of each view""? What is the source and target of the embedding E? How is the embedding applied to data x(s1), x(s2)?
Page 7: Figure 1a appears to have an error in the orientation of one of the blue ""3""s. The text on the arrow between the manifold embeddings does not agree with the notation in the paper. In Figure 1b, it is not clear which image is the original point and which images are the neighbors, or why some images are smaller than others. Results for the other algorithms are missing (why no comparison?). Typo ""Wang&Mahadevan"". Can you be more specific as to why that algorithm was ""unable to recover k-neighborhoods"" in certain cases?
Page 8: Why no comparison with Wang & Mahadevan in Figure 2?
Page 9: There is little description as to how manifold learning is applied in the biological data example. What is the ambient dimensionality and the dimension of the manifolds? How are the ""abundances"" extracted from the data? 
""Which we explore in 4"" -> ""Which we explore in Fig. 4""","The sentiment of the review is slightly negative, as the reviewer points out that the paper is not clear enough for acceptance and highlights several technical issues and errors. However, the reviewer does acknowledge the quality of the numerical results. The politeness of the language is relatively high, as the reviewer uses polite and constructive language to provide detailed feedback and suggestions for improvement.",-30,80
"The paper produces a heat-map of ride-share requests in four cities in the USA. For each city 'block' they produce a time-sequence of 2016 images representing a week-long run from combining each 5-minute interval. This is used with a GAN to produce new data. The techniques applied, although not commonly used in the context of ride sharing / hailing, have been used extensively in other literature.

Some major points on the paper:
1) A GAN approach is normally used to generate more data when enough real data is not obtainable. However, here you only use one week of data from a much larger set. Surely, it would be better to make use of all the weeks available?

2) It is not clear how the heat-maps once produced could be used in the future. There is a hint in the results section about how they can be converted back to ride requests, but this is not clearly defined.

3) There are a number of cases where you state that some approach has been found to be better. However, no evidence is presented for how you determined this to be true.

4) The conversion of data to heat-maps has been used extensively in prior research. Although I'm not directly aware of the use in machine learning I am aware of the use in transport - ""Interactive, graphical processing unit- based evaluation of evacuation scenarios at the state scale"". The novelty here seems to be the application to this specific problem.

More specific points:
- ""Our real ride request data sets consist of all the ride requests for an entire week for the four cities."" - it's not clear - are all four cities used to train one model?

- ""Hence the week-long data should be quite representative."" - This fails to take into account such things as national holidays or other major events such as sports. Did your chosen week contain one of these?

- ""Hence we believe the ride request data sets also reflect the overall urban mobility patterns for these cities."" - This is a huge assumption, which would seem to need evidence to back it up.

- ""and lump together all the ride requests within each interval."" - Presumably you mean that all time values are to the granularity of 5 minutes? 

- ""We arbitrarily sized each block to represent an image of 2424 pixels"" - this seems particularly small.

- ""Each image of that block is labeled with a time interval (for our experiments, the hour in a day)."" - Can the variability within an hour not make this more difficult? 

- ""We find that small networks are appropriate for the training data"" - evidence to support this.

- ""This network is pre-trained on the training data"" - which training data are you referring to?

- ""This is found to increase the efficiency of the training process"" - evidence?

- ""In this work we set the block size for each of the four cities to be
1200  1200 meters"" - how was this value arrived at?

- You state that GPUs were no more efficient, it would be good to see more analysis of this.

- ""To help enhancing the scalability"" -> ""To help enhance the scalability""

- ""and other useful functions"" - such as?

- Figure 4 would probably work better as a speedup graph.

- ""Running times for sampling ride requests from the trained models and stitching the images of all the blocks together are significantly less than the training times, and are not included in these results."" - at least some figures to give an idea of scale should be provided.","The review starts with a neutral summary of the paper's content, neither praising nor criticizing it explicitly. The sentiment score is therefore neutral. The specific recommendations are presented in a constructive manner, with the reviewer pointing out areas for improvement and asking for clarifications without using harsh or dismissive language. The politeness score is high due to the respectful and professional tone maintained throughout the review.",0,80
"The presented analysis well characterizes the behavior of the spatially transformed adversarial inputs and the proposed defense is empirically confirmed to achieve more accurate and robust classification under attacks.

One concern is that the defender cannot learn whether the adversary employs spatially transformed AEs or pixel-based AEs (or some others). What happens if the classifier trained with the proposed defense accept pixel-based AEs? I recommend the authors to associate spatially transformed AEs with pixel-based AEs to learn whether the proposed defense performs more robustly compared to existing defenses. If the proposed defense method performs well for spatially transformed AEs but is vulnerable to pixel-based AEs, it is useless.

It should be better to discuss more on computational efficiency of the proposed defense since it contains SDP solving. Is the proposed deense works with larger datasets such as CIFAR100 or ImageNet?

 
","The sentiment of the review is generally positive, as it acknowledges the effectiveness of the proposed defense in achieving more accurate and robust classification under attacks. However, it also raises concerns and provides constructive criticism, which slightly tempers the overall positivity. Therefore, the sentiment score is 60. The language used in the review is polite and constructive, offering recommendations and posing questions in a respectful manner. Thus, the politeness score is 80.",60,80
"This paper proposes to use a  stochastically quantized network combined with adversarial training to improve the robustness of models against adversarial examples. The main finding is that, compared to a full precision network, the quantized network can generalize to unseen adversarial attacks better while training only on FGSM-perturbed input. This provides a modest speedup over traditional adversarial training.

While the findings are certainly interesting, the method lacks experimental validation in certain aspects. The comparison with other adversarial training methods is not standardized across networks, making the efficiency claims questionable. Furthermore, I am uncertain whether the authors implemented expectation over transformations (EoT) for the C&W attack.  Since the network produces randomized output, vanilla gradient descent against an adversarial loss is likely to fail. It is conceivable that by taking an average over gradients from different quantizations, the C&W adversary would be able to circumvent the defense better. I would be willing to reconsider my review if the authors can address the above weaknesses.

Pros:
- Surprising result showing that quantization leads to improved generalization to unseen attack methods.

Cons:
- Invalid comparison to other adversarial training techniques since the evaluated models are very different.
- Lack of evaluation against EoT adversary.
- Algorithm 1 is poorly presented. I'm sure there are better ways of expressing such a simple quantization scheme.
- Figures 2 and 3 are uninteresting. The fact that the model is robust against adversaries implies that the activations remain unchanged when presented with perturbed input.","The sentiment of the review is mixed but leans towards the negative side. The reviewer acknowledges the interesting findings but highlights significant weaknesses in the experimental validation and comparison methods. Therefore, the sentiment score is -20. The language used is generally polite, with constructive criticism and suggestions for improvement, so the politeness score is 60.",-20,60
"The paper makes its intent plainly clear, it wants to remove the assumption that demonstrations are optimal.  Thus it should show that in a case that some demonstrations are bad, it outperforms other methods which assume they are all good. The method proposed, while interesting, well-conceived and potentially novel, is not convincingly tested to this end. 

The paper should also show that the method can detect the bad demonstrations, and select the good demonstrations. 

The experiments are on toy tasks and not existing tasks in the literature. Why not use an existing dataset/domain and simply noise up the demonstrations?

Furthermore, many crucial details are omitted, such as the nature of the heuristic function K, and how precisely the weighting $c_i$ is adapted (section 4.4). Is it done by gradient descent? We would have to know what K is, and if it is differentiable to know this.

Also the writing itself needs a thorough revision.

I think there may well be promise in the method, but it does not appear ready for publication.
","The sentiment of the review is mixed but leans towards the negative side. The reviewer acknowledges the potential and interesting aspects of the method but criticizes the lack of convincing testing, missing crucial details, and the need for a thorough revision of the writing. Therefore, the sentiment score is -40. The politeness of the language is relatively high; the reviewer uses polite phrases such as 'well-conceived,' 'potentially novel,' and 'may well be promise,' even while pointing out significant flaws. Thus, the politeness score is 60.",-40,60
"In this paper, the novel MAOP model is described for self-supervised learning on past video game frames to predict future frames. The presented results indicate that the method is capable of discovering semantically important visual components, and their relation and dynamics, in frames from arcade-style video games. Key to the approach is a multi-level self-learning approach:  more abstract stages focus on simpler problems that are easier to learn, which in turn guide the learning process at the more complex stages.
A downside is that it the method is complex, consisting of many specific sub-components and algorithms, which in turn have again other sub-components. This makes the paper a long read with a lot of repetition, and various times the paper refers to the names of sub-components that are only explained later. Other methodological details that are relevant to understand how the method operates are described in the Appendices. I expect that if the paper would be better structured, it would be easier to understanding how all the parts fit together. Another downside of this complexity is that the method seems designed for particular types of video game frames, with static backgrounds, a fixed set of objects or agents. It is unclear how the method would perform on other types of games, or on real-world videos. While the method therefore avoids the need for manual annotation, it instead encodes a lot of domain knowledge in its design and components.
I also didn't fully understand how the self-supervised model is used for Reinforcement Learning in the experiments. Is the MAOP first trained, and the fixed to perform RL with the learned agent models, or is the MOAP learned end-to-end during RL?

Pros:
+ MAOP seems successful on the tested games in the experiments
+ Demonstrates that, with a sufficiently engineered method, self-supervised learning can be used to discover different types of objects, and their dynamics.

Cons:
- writing could be improved, as the methodology currently reads as a summation of facts, and some parts are written out of order, resulting in various forward references to components that only become clear later. Several times, the paper states that some novel algorithm is used, but then provides no further explanation in the text as all description of this novelty is deferred to an appendix. 
- method does not seem generic, hence it is unclear how relevant this architecture it is to other use cases
- many hyperparameters for the individual components, algorithms. Unclear how these parameter setting affect the results

Below are more detailed comments and questions:

General comments:
* The proposed MOAP method consists of many subalgorithms, resulting in various (hyper)parameters which may impact the results (e.g. see Appendix A, B). Appendix D lists several used hyperparameter settings, though various parameters for the algorithms are still missing (e.g. thresholds alpha, beta in Algo.2). Were the used parameters optimized? How are these hyperparameters set in practice? How does changing them impact your results?
* Methods seems particularly designed for 'video games', where the object and background structures have well defined sizes, appearance, etc. How will the MOAP fair in more realistic situations with noisy observations, occluded objects, changing appearances and lighting conditions, etc.?
* How about changing appearance of an agent during an action, e.g. a 'walking animation' ? Can your method learn the sequence of sprites to accurately predict the next image? Is that even part of the objective?
* Appendix D has important implementation details, but is never mentioned in the text I believe! Didn't realize it existed on first read through.

* Introduction:
	* What prediction horizon are you targeting? 1 step, T steps into the future, 1 to T steps in the future simultaneously?
	What are you trying to predict? Object motion? Future observations?
	* ""... which includes a CNN-based Relation Net to ... "", the names Relation Net, Inertia Net, etc.. are used as if the reader is expected to know what these are already. If these networks were introduced in related work already, please add citations. Otherwise please rephrase to clarify that these are networks themselves are part of your novel design.

* Section 3.1
	* ""It takes multiple-frame video images ... and produce the predictions of raw visual observations."". As I understand from this, the self-supervised approach basically performs supervised learning to predict a future frame (target output) given past frames (input). I do not understand how this relates to Reinforcement Learning (RL) as mentioned in the introduction and Related Work. Is there still some reward function in play when learning the MAOP parameters? Or is the idea to first self-supervised learn the MAOP, and afterwards fix its parameters and use it in separate a RL framework? I believe RL is not mentioned anymore until Section 4.2. This connection between self-supervised and reinforcement learning should be clarified, or otherwise the related work should be adjusted to include other (self-supervised) work on predicting future image frames.
	* ""An object mask describes the spatial distribution of an object ..."" Does the distribution capture uncertainty on the object's location, or does it capture the spread of the object's extent ('mass distribution') ?
	* ""Note that Object Detector uses the same CNN architecture with OODP"". What does OODP stand for? Add citation here. (first mention of OODP is in Experiments section)
	* ""(similar with Section 3.2)"" → ""similar to"". Also, I find it a confusing to say something is similar to what will be done in a future section, which has not yet been introduced. Can you not explain the procedure here, and in Section 3.2 say that the procedure is ""similar to Section 3.1"" instead?
	* ""to show the detailed structure of the Effect Net module."" First time I see the name 'Effect Net', what is it? This whole paragraph different nets are named, with a rough indication of their relation, such as ""Dynamic Net"", ""Relation Net"" and ""Inertia Net"". Is ""Effect Net"" a different name for any of the three previous nets? The paper requires the reader to puzzle from Fig.2 that Relation Net and Inertia Net are parts of Effect Net, which in turn is part of Dynamics Net. This wasn't clear from the text at all.

* Section 3.2:
	* p7.: ""Since DISN leans"" → ""Since DISN learns"" ?
	* There are many losses throughout the paper, but I only see at the end of Section 3.1 some mentioning that multiple losses are combined. How is this done for the other components, .e.g is the total loss for DISN a weighted sum of L_foreground and L_instance ? Are the losses for all three three MAOP levels weighted for full end-to-end learning?
	* This section states various times ""we propose a novel [method]"", for which then no explanation is given, and all details are explained in the Appendix. While the Appendix can hold important implementation details, I would still expect that novelties of the paper are clearly explained in the paper itself. As it stands, the appendix is used as an extension of the methodological section of an already lengthy paper.
	* ""Conversely, the inverse function is ... "" M has a mask for each of the n_o ""object classes"", hence the ""Instance Localization Module"" earlier to split out instances from the class masks. So how can there be a single motion vector STN^-1(M,M') if there are multiple instances for an object mask? How will STN^-1 deal with different amount of instances in M and M' ?

* Section 3.3:
	* What is the output of this level? I expect some mathematical formulation as in the previous sections, resulting in some symbol, that is then used in Section 3.2. E.g. is the output ""foreground masks F"" (found in Appendix A) ?  This paper is a bit of a puzzle through the pages for the reader.

* Section 4: 
	* ""We compare MAOP with state-of-the-art action-conditioned dynamics learning baselines, ..."" Please re-iterate how these methods differ in assumptions, what they model, with respect to your novel method? For instance, is the main difference your ""novel region proposal method"" and such? Is the overall architecture different? E.g. explain here already the AC Model uses ""pixel-level inference"", and that OODP has ""lacks knowledge on object-to-object relations"" to underline their difference to your approach, and provide context for your conclusions in Section 4.1.

* Appendix A:
	* Algorithm 1, line 7: ""sample a pixel coordinate"" → is this non-deterministically sampling?
","The sentiment of the review is mixed, with both positive and negative aspects highlighted. The reviewer acknowledges the success of the MAOP model in the tested games and its potential for self-supervised learning, which is positive. However, the reviewer also points out significant issues with the complexity, structure, and generalizability of the method, which are negative aspects. Therefore, the sentiment score is 0, indicating a neutral overall sentiment. The politeness of the language is generally high, as the reviewer provides constructive criticism and suggestions for improvement without using harsh or rude language. Therefore, the politeness score is 80.",0,80
"The paper proposes a greedy-like algorithm for sparse recovery that uses nearest neighbors algorithms to efficiently identify candidates for the support estimates obtained at each iteration of a greedy algorithm. It assumes that the norms of the columns of the matrix A are one to be able to change the project-and-sort step into a nearest neighbors search.

It is not clear what the value of Fact 1 is, given that none of the sparse recovery algorithms discussed here actually performs ell0 norm minimization. Additionally, it is common in theoretical analysis of sparse recovery to assume that the columns of the matrix A have unit norm. In fact, the RIP implies that the columns of the matrix must have norm within delta of 1. Nonetheless, it would be useful to have a discussion of the effect that having non-unit column norms would have on the proposed approach.

Similarly, Fact 2 is almost self-evident; I suggest to discard the proof.

The equivalence of Definition 1 and the statement involving ps and qs needs to be shown more clearly. The statement in Definition 1 is given in terms of distances (ball radiuses), not counts of neighbors.

I suggest swapping the use of CoSaMP and AIHT - the theoretical results of the paper refer to AIHT, so it is not clear why the algorithm itself is relegated to the supplementary material.

It is not clear how d0 is to be computed to implement Accelerated AIHT.

For Theorem 1, the authors should comment on when the assumption ""xtilde(t) converges linearly to a k-sparse signal with rate c"".

In Figures 1 and 2, does ""residual"" refer to the difference between x and xtilde, or b and Axtilde? 

Minor comments:
Typo in page 5 ""¿""
Grammar error in page 6 ""characterizing of the difficulty"".","The sentiment of the review appears to be neutral to slightly positive. The reviewer acknowledges the proposal of a new algorithm and provides constructive feedback without overtly negative language. However, the review does point out several areas that need clarification or improvement, which suggests a neutral sentiment overall. The politeness of the language is quite high, as the reviewer uses polite suggestions ('I suggest', 'it would be useful') and avoids harsh criticism, focusing on constructive feedback instead.",10,80
"- The authors study the problem of negative transfer in representation learning, and propose to use the formulation proposed by Ganin & Lempitsky '15 for domain adaptation to reduce negative transfer. Instead of defining the domain classification as the adversarial task to learn a domain-independent representation, they collect a set of classification problems irrelevant to the main task as the adversarial tasks, and aim to learn a representation that focuses only on the primary task. There are very little changes compared to the proposal by Ganin & Lempitsky '15, but the application to solve the problem of negative transfer is interesting.  

- My main concern on the whole argument of the paper is whether the benefits we see in the experiments come from the elimination of negative transfer, or just come from having more training labels from different tasks available. In the main formulation of the approach (equation 7), the authors try to learn a feature representation that works well for the primary task but works poorly for the auxiliary(irrelevant) tasks. If we switch the sign for lambda, then it becomes very similar to traditional multi-task learning. I wonder how the multi-task formulation would compare against the adversarial formulation proposed by the authors. There are reasons to suspect the multi-task formulation will also work better than the logistic regression baseline, since more labels from different tasks are available to learn a better joint representation. It is not clear whether the improvements come from modeling the auxiliary tasks using negative transfer (where the adversarial approach should beat the baseline and multi-task approach), or just come from having more information (where both the adversarial approach and the multi-task approach beat the baseline, but have similar performance). 

- From a practical point of view, it is not easy to decide what prediction tasks are irrelevant. For example, in the birds dataset, I would expect the color and patterns in the body parts to have some correlations (primary_color, upperparts_color, underparts_color, wing_color, etc). In the case of occlusion of the relevant body parts, I could make a guess on the color based on the colors on other parts of the bird. In the ideal case for the current method, I would expect the adversarial approach proposed to learn a representation that mask out all the irrelevant parts of the animal or irrelevant contextual information. Apart from showing improved prediction performance, have the authors perform analysis on the image activation patterns similar to the motivation example in Figure 1 to see if the new approach actually focus on the relevant body parts of the animals? 

- The definition of auxiliary tasks are described in the second last paragraph of 3.3, but it would be clearer if it is also mentioned how they are defined in the experiments section. I went through the whole experiments section having trouble interpreting the results because I could not find the definition of adversarial tasks.  

- Overall I like this paper since it attempts to solve an interesting problem in computer vision, but I would like to see the above question on comparison with multi-task learning answered, or some image activation pattern analysis to provide a more solid argument that the improvements come from elimination of negative transfer. 

","The sentiment of the review is generally positive, as the reviewer expresses interest in the problem being addressed and acknowledges the potential of the proposed approach. However, the reviewer also raises significant concerns and questions that need to be addressed, which tempers the overall positivity. Therefore, the sentiment score is 30. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, even when pointing out concerns and suggesting improvements. Therefore, the politeness score is 80.",30,80
"This paper proposes a simple improvement to methods for unit pruning. After identifying a unit to remove (selected by the experimenter’s pruning heuristic of choice), the activation of that unit is approximately incorporated into the subsequent unit by “mean replacement”. The mean unit activation (computed on a small subset of the training set) is multiplied by each outgoing weight (or convolutional filter) and added to each corresponding bias instead. Experiments show this method is generally better than the typical method of zero-replacement before fine-tuning, though the advantage is smaller after several epochs of fine-tuning.

While I find this paper intriguing and applaud the extensive experimentation and documentation, I have some concerns as well:
	1. There are unanswered questions about how this method relates to existing work. It is not clear from the paper how the “mean replacement” method differs from the two most related works (Ye, 2018) and (Morcos, 2018), which propose variations on replacing units with constant values or mean activations, respectively. Also, why does the method in this paper seem to yield good results, while the related method (Morcos, 2018) yields “inferior performance”?
	2. The results are stated to only apply to networks “without batch normalization”. The reason seems intuitive: any change that can be merely rolled into the bias will be lost after normalization (depending perhaps on the ordering of normalization and the non-linearities). This leaves an annually decreasing fraction of networks to which this method is applicable, given the widespread use of batch norm.
	3. Critically, it’s difficult to compare this work against other pruning works given the lack of results reported in terms of final test error and the lack of the ubiquitous “error vs. %-pruned” plot.
	
Overall, this paper is lacking some clarity, may be limited in originality, may be helpful for some common networks and composable with other pruning methods (significance), but has a good quality evaluation (subject to the clarity issues). I’m rating this paper below the threshold given the limitations, but I’m willing to consider an upgrade to the score if these questions are addressed.

Other notes:
	4. What is your definition of a convolutional “pruning unit”? (From context, I’d presume it corresponds to an output activation map.)
	5. In Section 3.1:  replace “in practice, people …” with  something like “in practice, it is common to”.
	6. In Equation 3, is the absolute value of the pruning penalty used in the evaluation?
	7. In the footnote in Section 3.2, how many training samples are needed for a good approximation? How many are used in the experiments?
	8. There are a couple typos in Section 3.2: “replacing -the- these units with zeroes” and “each of these output*s*”.
	9. Presumably the “\Delta Loss after pruning” in Figures 2-6 is validation or test loss, not training loss? Is this the cross-entropy loss? Also, it would be much easier to compare to other papers if test accuracy were reported instead or in addition.
	10. In Figure 4, the cost to recover using fine-tuning seems to be only roughly 2% of the original training time. How much time is lost to the process of computing the average unit activation?

UPDATE: I've raised the score slightly to 5 after the rebuttals and revisions.","The sentiment of the review is mixed but leans slightly positive. The reviewer acknowledges the intriguing nature of the paper and the extensive experimentation, which suggests a positive sentiment. However, the reviewer also lists several significant concerns and limitations, which brings the sentiment down. Therefore, the sentiment score is 10. The politeness of the language is quite high. The reviewer uses polite language throughout, such as 'I find this paper intriguing,' 'I applaud the extensive experimentation,' and 'I’m willing to consider an upgrade to the score if these questions are addressed.' Therefore, the politeness score is 80.",10,80
"Prons: 
This paper provides a simple and economic technique to accelerate adaptive stochastic algorithms. The idea is novel and preliminary experiments are encouraging.

Cons: 
1.	The theoretical analysis for AAMSGrad is standard and inherits from AMSGrad directly. Meanwhile, the convergence rate of AAMSGrad merely holds for strongly convex online optimization, which does not match the presented experiments. Hence, the theoretical contribution is limited. 
2.	The current experiments are too weak to validate the efficacy of the proposed accelerated technique. We recommend the authors to conduct more experiments on various deep neural networks. ","The sentiment of the review is mixed. The reviewer acknowledges the novelty and potential of the proposed technique, which is a positive aspect. However, the reviewer also points out significant limitations in the theoretical analysis and the experimental validation, which are negative aspects. Therefore, the sentiment score is slightly positive at 20. The language used in the review is polite and constructive, as the reviewer provides specific recommendations and uses phrases like 'we recommend,' which indicates a polite tone. Thus, the politeness score is 80.",20,80
"Summary: This paper introduces a functional extension of the Bregman Lagrangian framework of Wibisono et al. 2016. The basic idea is to define accelerated gradient flows on the space of probability distribution. Because the defined flows include a term depending on the current distribution of the system, which is difficult to compute in general, the authors introduce an interacting particle approximation as a practical numerical approximation. The experiments are a proof-of-concept on simple illustrative toy examples.

Quality: The ideas are generally of high quality, but I think there might some typos (or at least some notation I did not understand). In particular
- tilde{F} is not defined for Table 1
- the lyapunov function for the vector column of table one includes a term referring to the functional over rho. I think this is a typo and should be f(x) - f(xmin) instead.

Clarity: The paper is generally clear throughout.

Originality & Significance: The paper is original to my knowledge, and a valuable extension to the interesting literature on the Bregman Lagrangian. The problem of simulating from probability distributions is an important one and this is an interesting connection between that problem and optimization.

Pros:
- An interesting extension that may fuel future study.

Cons:
- This algorithm appears naively to have an O(n^2) complexity per iteration, which is very expensive in terms of the number of particles. Most MCMC algorithms would have only O(n) complexity in the number of particles. This limits its applicability.
","The sentiment of the review is generally positive, as the reviewer acknowledges the high quality of the ideas, the clarity of the paper, and its originality and significance. However, there are some minor criticisms regarding potential typos and the computational complexity of the algorithm. The politeness of the language is very high, as the reviewer provides constructive feedback in a respectful and considerate manner, using phrases like 'I think there might be some typos' and 'This limits its applicability' rather than more direct or harsh language.",70,90
"Summary:

This work tackles few-shot (or meta) learning, providing an extension of the gradient-based MAML method to using a mixture over global hyperparameters. Each task stochastically picks a mixture component, giving rise to task clustering. Stochastic EM is used for end-to-end learning, an algorithm that is L times more expensive than MAML, where L is the number of mixture components. There is also a nonparametric version, based on Dirichlet process mixtures, but a large number of approximations render this somewhat heuristic.

Comparative results are presented on miniImageNet (5-way, 1-shot). These results are not near the state-of-the art anymore, and some of the state-of-art methods are simpler and faster than even MAML. If expensive gradient-based meta-learning methods are to be consider in the future, the authors have to provide compelling arguments why the additional computations pay off.

- Quality: Paper is technically complex, but based on simple ideas. In the case of
   infinite mixtures, it is not clear what is done in the end in the experiments.
   Experimental results are rather poor, given state-of-the-art.
- Clarity: The paper is not hard to understand. What is done, is done cleanly.
- Originality: The idea of putting a mixture model on the global parameters is not
   surprising. Important questions, such as how to make this faster, are not
   addressed.
- Significance: The only comparative results on miniImageNet are worse than the
   state-of-the-art by quite a margin (admittedly, the field moves fast here, but it
   is also likely these benchmarks are not all that hard). This is even though better
   performing methods, like Versa, are much cheaper to run

While the idea of task clustering is potentially useful, and may be important in practical use cases, I feel the proposed method is simply just too expensive to run in order to justify mild gains. The experiments do not show benefits of the idea.

State of the art results on miniImageNet 5-way, 1-shot, the only experiments here which compare to others, show accuracies better than 53:
- Versa: https://arxiv.org/abs/1805.09921.
   Importantly, this method uses a simpler model (logistic regression head models)
   and is quite a bit faster than MAML, so much faster than what is proposed here
- BMAML: https://arxiv.org/abs/1806.03836.
   This is also quite complex and expensive, compared to Versa, but provides good
   results.

Other points:
- You use a set of size N+M per task update. In your 5-way, 1-shot experiments,
   what is N and M? I'd guess N=5 (1 shot per class), but what is M? If N+M > 5,
   then I wonder why results are branded as 5-way, 1-shot, which to mean means
   that each update can use exactly 5 labeled points.
   Please just be exact in the main paper about what you do, and what main
   competitors do, in particular about the number of points to use in each task
   update.
- Nonparametric extension via Dirichlet process mixture. This is quite elaborate, and
   uses further approximations (ICM, instead of Gibbs sampling).
   Can be seen as a heuristic to evolve the number of components.
   What is given in Algorithm 2, is not compatible with Section 4. How do you merge
   your Section 4 algorithm with stochastic EM? In Algorithm 2, how do you avoid
   that there is always one more (L -> L+1) components? Some threshold must be
   applied somewhere.
   An alternative would be to use split&merge heuristics for EM.
- Results reported in Section 5 are potentially interesting, but entirely lack a
   reference point. The first is artificial, and surely does not need an algorithm of this
   complexity. The setup in Section 5.2 is potentially interesting, but needs more
   work, in particular a proper comparison to related work.
   This type of effort is needed to motivate an extension of MAML which makes
   everything quite a bit more expensive, and lacks behind the state-of-art, which
   uses amortized inference networks (Versa, neural processes) rather than
   gradient-based.
","The review provides a balanced critique of the paper, acknowledging the technical complexity and potential usefulness of the proposed method while also pointing out its shortcomings, such as poor experimental results and high computational cost. The sentiment score is slightly negative due to the emphasis on the method's limitations and the lack of compelling results. The politeness score is high because the reviewer uses polite and constructive language throughout the review, offering specific recommendations for improvement without being dismissive or rude.",-20,80
"This paper proposes simple metrics for measuring the ""information density"" in learned representations. Overall, this is an interesting direction. However there are a few key weaknesses in my view, not least that the practical utility of these metrics is not obvious, since they require supervision in the target domain. And while there is an argument to be made for the inherent interestingness of exploring these questions, this angle would be more compelling if multiple encoder architectures were explored and compared. 

+ The overarching questions that the authors set out to answer: How task-specific information is stored and to what extent this transfers, is inherently interesting and important. 

+ The proposed metrics and simple and intuitive.

+ It is interesting that a few units seem to capture most task specific information. 

- The envisioned scenario (and hence utility) of these metrics is a bit unclear to me here. As noted by the authors, transfer is most attractive in low-supervision regimes, w.r.t. the target task. Yet the metrics proposed depend on supervision in the target domain. If we already have this, then -- as the authors themselves note -- it is trivial to simply try out different source datasets empirically on a target dev set. It is argued that this is an issue because it requires training 2n networks, where n is the number of source tasks. I am unconvinced that one frequently enough has access to a sufficiently large set of candidate source tasks for this to be a real practical issue. 

- The metrics are tightly coupled to the encoder used, and no exploration of encoder architectures is performed. The LSTM architecture used is reasonable, but it would be nice to see how much results change (if at all) with alternative architectures.

- The CFS metric depends on a hyperparameter (the ""retention ratio""), which here is arbitrarily set to 80% without any justification.

- What is the motivation for the restriction to linear models? In the referenced probing paper, for example, MLPs were also used to explore whether attributes were coded for 'non-linearly'. 
","The sentiment of the review is mixed but leans towards the positive side. The reviewer acknowledges the interesting direction and the inherent importance of the questions posed by the authors, which contributes to a positive sentiment. However, the review also highlights several key weaknesses and areas for improvement, which tempers the overall positivity. Therefore, the sentiment score is 20. The politeness of the language is quite high. The reviewer uses polite and constructive language, providing specific feedback without being harsh or dismissive. The use of phrases like 'it would be nice to see' and 'I am unconvinced' indicates a polite tone. Therefore, the politeness score is 80.",20,80
"This study explores the class hierarchy to solve many-class few-short learning problem in both traditional supervised learning and meta-learning. The model integrates both the coarse-class and fine-class label as the supervision information to train the DNN, which aims to leverage coarse-class label to assist fine-class prediction. The core part in the DNN is memory-augmented attention model that includes at KNN classifier and Memory Update mechanism. The re-writable memory slots in KNN classifier aim to maintain multiple prototypes used to describe the data sub-distribution within a class, which is insured by designing the memory utility rate, cache and clustering component in Memory Update mechanism. This study presents a relatively complex system that combines the idea of matching networks and prototypical networks.

One of the contributions is that the study puts forward a concept of the many-class few-short learning problem in both supervised learning and meta-learning scenarios, and uses a dataset to describe this problem.

Using the memory-augmented mechanism to maintain multiple prototypes is a good idea. It may be more interesting if its effectiveness can be proved or justified theoretically. Furthermore, it is better to offer some discussion about the learned memory slots in the view of “diverse and representative feature”.

The experiment results in Table 4 and Table 5 compare the MahiNet with Prototypical Net on the mcfsImageNet and mcfsOmniglot dataset. It is better to compare MahiNet with other state-of-the-art works, such as the Relation Network whose performance is higher than Prototypical Net. In addition, if more  challenging datasets  can be further evaluated in the experiments,  the paper  might be more convincing.

In my opinion, the hierarchy information provides the guidance to fine-gained classification, which not only can be added to MahiNet but also the other models. Therefore, to prove its effectiveness, it is better to add hierarchy information to other models for comparison. In addition, regarding the results on the column of 50-5 and 50-10 in Table 4, when the number of class increase to 50, the results are just slightly higher than prototypical network. Considering that the memory update mechanism is of the high resource consumption and complexity, it is better to provide more details about clustering, and training and testing time.
","The review starts by summarizing the study and acknowledging its contributions, which indicates a generally positive sentiment. However, it also includes several constructive criticisms and suggestions for improvement. The language used is polite and professional, with phrases like 'it may be more interesting if' and 'it is better to offer some discussion,' which soften the critique and make it more constructive. Therefore, the sentiment score is moderately positive, and the politeness score is high.",40,80
"The idea of extending  Riemannian Langevin dynamics to functional spaces is elegant, however it is extremely hard to follow the proposed method as details are kept to a minimum. The finite approximation of the posterior distribution is a function of the parameters theta, however it displays parameters lambda. The couple of sentences: ""Then by sampling λ, we sample a functional f equivalently. The Riemannian Langevin dynamics on the functional space can thus be written as: (6)"" come without a single explanation.

Minor comments
* Max and Whye is the casual version for reference Welling and Teh.
* proper nouns in References should be capitalized","The sentiment of the review is slightly negative, as it points out significant issues with the clarity and detail of the proposed method. The reviewer appreciates the elegance of the idea but finds the execution lacking. Therefore, the sentiment score is -40. The politeness of the language is relatively high, as the reviewer uses polite language and provides constructive feedback without being harsh or rude. Thus, the politeness score is 80.",-40,80
"This paper describes multi-grained entity recognition. Experimental results show that the proposed Multi-Grained Proposal Network achieve better performance on NER tasks.

Major comments:

- A major weakness of this paper is lack of citations to recent related studies. There are studies on nested NER published this June:

A. Katiyar and C. Cardie, Nested Named Entity Recognition Revisited, NAACL/HLT 2018, June, 2018.
M. Ju, et al., A neural layered model for nested named entity recognition, NAACL/HLT 2018, June 2018.

You need to compare these conventional methods to your proposed method.

","The sentiment of the review is slightly positive as it acknowledges the achievement of better performance on NER tasks by the proposed Multi-Grained Proposal Network. However, it also points out a significant weakness regarding the lack of citations to recent related studies. Therefore, the sentiment score is 20. The politeness of the language is neutral to slightly polite. The reviewer provides constructive criticism without using harsh or rude language, and the suggestions are presented in a straightforward manner. Therefore, the politeness score is 10.",20,10
"This paper proposes a network architecture inspired by the primate visual cortex. The architecture includes feedforward, feedback, and local recurrent connections, which together implement a predictive coding scheme. Some versions of the network are shown to outperform the similar PredNet and PredRNN architectures on two video prediction tasks: moving MNIST and KTH human actions. Finally, the authors provide neural data from monkeys and argue that their network shows similarities to the biological data.

The paper contains intriguing ideas about the benefits of sparse and predictive coding, and the direct comparison to biological data potentially broadens the impact of the work. However, major claims are unsubstantiated, and accuracy and clarity need to be improved to make the manuscript acceptable.

Major concerns:
1. The authors claim that their architecture is more efficient because it uses sparse coding of residuals. Implementation details and some quantitative arguments, ideally benchmarks, need to be provided to show that their architecture is actually more efficient than PredRNN++ and PredNet.

2. It is unclear whether the PredRNN++ should be compared to the C-C or C-F version of the network. Does the PredRNN++ have access to as many current and future frames as the C-C net? Is this a fair comparison? Please provide a clearer description of the different versions of your network and how they relate to the baseline models. That section in particular has many confusing typos (frame-by-chunk, chunk-by-frame abbreviations mixed up).

3. In Figure 6, the authors claim that more layers lead to “better” representations. What does “better” mean? It is implied that the networks with more layers actually make the different motions more discriminable. Please quantify this. For example, a linear classifier could be trained on the neural activations. Also, how is this related to the rest of the paper? Do the authors claim that this result is unique to the proposed architecture? In that case, please provide a quantitative comparison to the PredNet or PredRNN++.

4. In Figure 9, the presentation is highly confusing. Plots (c) to (h) are clearly made to look like the monkey data in (b) (nonlinear x-axes?), but show totally different timescales (training epochs vs. milliseconds). Please explain why it makes sense to compare these timescales. Also, what does it mean for a training epoch to have a negative value? 

Minor comments:
1. I don’t understand the “tension” between hierarchical feature representations and residual representations brought up in Section 2. Do the PredNet and PredRNN++ not contain a hierarchy of representations?

2. Figure 1 is not fully annotated and could be clearer. What does the asterisk mean? Why are there multiple arrows between the P’s? What do the small arrows next to the big arrows mean? Please expand the legend. Consider using colors to differentiate between components.

3. I don’t understand Figure 4c. According to the text, this plot shows “effectiveness as a function of time”, but the x-axis is labeled “Layer Number”. What does “effectiveness over time” mean? What does the y-label mean (SSIM per day?)? What is “trunk prediction” (not mentioned anywhere in the text)?

4. For Figure 9, it is pointed out that activity is expected to be lower for E neurons, but is also lower for R and P. This is interesting and also applies to Figure 8, so it would be good to see Figure 8 split up by E/R/P, too. 

5. The word “Figure” is missing before figure references.

6. Please proof-read for typography, punctuation and grammar.","The sentiment of the review can be considered mixed but leaning towards negative. The reviewer acknowledges the intriguing ideas and potential impact of the work, which suggests some positive sentiment. However, the majority of the review focuses on major concerns and necessary improvements, indicating a more critical stance. Therefore, the sentiment score is -30. The politeness of the language is quite high. The reviewer uses polite and constructive language throughout, providing specific recommendations without being rude or dismissive. Thus, the politeness score is 80.",-30,80
"This paper presents a methodology to bring together independent subspace analysis and variational auto-encoders. Naturally, in order to do that, the authors propose a specific family of prior distributions that lead to subspace independence the Lp-nested distribution family. This prior distribution is then used to learn disentangled and interpretable representations. The mutual information gap is taken as the measure of disentanglement, while the reconstruction loss measures the quality of the representation. Experiments on the sPrites dataset are reported, and comparison with the state of the art shows some interesting results.

I understand the limitations of current approaches for learning disentangled representations, and therefore agree with the motivation of the manuscript, and in particular the choice of the prior distribution. However, I did not find the answer to some important questions, and generally speaking I believe that the contribution is not completely and clearly described.
P1) What is the shape of the posterior distribution?
P2) How does the reparametrization trick work in your case?
P3) How can one choose the layout of the subspaces, or this is also learned?

Moreover, and this is crucial, the proposed method is not clearly explained. Different concepts are discussed, but there is no summary and discussion of the proposed method as a whole. The reader must infer how the method works from the different pieces. 

When discussing the performance of different methods, and even if in the text the four different alternatives are clearly explained, in figure captions and legens the terminology changes (ISA-VAE, ISA-beta-VAE, beta-VAE, beta-ISA-VAE, etc). This makes the discussion very difficult to follow, as we do not understand which figures are comparable to which, and in which respect.

In addition, there are other (secondary) questions that require an answer.
S1) After (10) you mention the subspaces v_1,...v_l_o. What is the formal definition of these subspaces?
S2) The definition of the distribution associated to ISA also implies that n_i,k = 1 for all i and k, right?
S3) Could you please formally write the family of distributions, since applying this to a VAE is the main contribution of your manuscript?
S4) Which parameters of this family are learned, and which of them are set in advance?
S5) From Figure 4 and 5, I understand that the distributions used are of the type in (7) and not (10). Can you comment on this?
S6) How is the Lp layout chosen?
S7) Why the Lp layout for ISA-beta-VAE in Figure 5 is not the same as in Figure 4 for ISA-VAE?
S8) What are the plots in Figure 4? They are difficult to interpret and not very well discussed.

Finally, there are a number of minor corrections to be made.
Abstract: latenT
Equation (3) missig a sum over j
Figure 1 has no caption
In (8), should be f(z) and not x.
Before (10), I understand you mean Lp-nested
I did not find any reference to Figure 3
In 4.1, the standard prior and the proposed prior should be referred to with different notations.

For all these reasons I recommend to reject the paper, since in my opinion it is not mature enough for publication.","The sentiment score is derived from the overall tone and content of the review. The reviewer acknowledges the motivation and some interesting results of the paper, but also highlights significant issues and ultimately recommends rejection. This mixed feedback results in a sentiment score of -40. The politeness score is based on the language used throughout the review. The reviewer uses polite language, such as 'I understand,' 'could you please,' and 'I recommend,' even when pointing out flaws and suggesting rejection. This results in a politeness score of 60.",-40,60
"The paper tries to describe SGD from the point of view of the distribution p(y',y) where y is (a possibly corrupted) true class-label and y' a model prediction. Assuming TV metric of probabilities, a trajectory is defined which fits to general learning behaviour of distributions.

The issue is that the paper abstracts the actual algorithm, model and data away and the only thing that remains are marginal distributions p(y) and conditional p(y'|y). At this point one can already argue that the result is either not describing real behavior, or is trivial. The proposed trajectory starts with a model that only predicts one-class (low entropy H(y') and high conditional entropy) and ends with the optimal model. the trajectory is linear in distribution space, therefore one obtains initially a stage where H(y') and H(y'|y) increase a lot followed by a stage where H(y'|y) decrease.

This is known to happen, because almost all models include a bias on the output, thus the easiest way to initially decrease the error is to obtain the correct marginal distribution by tuning the bias. Learning the actual class-label, depending on the observed image is much harder and thus takes longer. Therefore no matter what algorithm is used, one would expect this kind of trajectory with a model that has a bias.

It also means that the interesting part of an analysis only begins after the marginal distribution is learned sufficiently well. and here the experimental results deviate a lot from the theoretical prediction. while showing some parabola like shape, there are big differences in how the shapes are looking like.

I don't see how this paper is improving the state of the art, most of the theoretical contributions are well known or easy to derive. There is no actual connection to SGD left, therefore it is even hard to argue that the predicted shape will be observed, independent of dataset or model(one could think about a model which can not model a bias and the inputs are mean-free thus it is hard to learn the marginal distribution, which might change the trajectory)

 Therefore, I vote for a strong reject.","The sentiment of the review is quite negative, as the reviewer points out several fundamental issues with the paper, including the abstraction of the actual algorithm, model, and data, and the lack of improvement to the state of the art. The reviewer also mentions that the theoretical contributions are well known or easy to derive, and that the experimental results deviate significantly from the theoretical predictions. The recommendation for a 'strong reject' further solidifies the negative sentiment. The politeness of the language is relatively neutral. While the reviewer is critical, they do not use rude or disrespectful language. The critique is presented in a straightforward and factual manner without personal attacks or derogatory remarks.",-80,0
"This paper proposed an 


1. For me, the argument of the paper is ambitious. Data augmentation for DNN includes different perspective, including nonlinearity, adversarial etc. Generalization of  spatial and appearance models is not enough. The model formulate from a simple classification setting but does not involve too many for DNN models.  I put more references below. 

2. The experimental results are not strong. Not all strong baselines are included (I put some in the references). The improvements are marginal. Besides, I need more experimental setting information.

3. The writing is not clear. For the related work part, it included many paragraph which are not related to the work, (e.g. GANs). In the introduction part, it did not mention the generalization of both spatial and appearance models, which is the main contribution.  

References:
a. Good Semi-supervised Learning that Requires a Bad GAN
b. Semi-supervised Learning with GANs: Manifold Invariance with Improved Inference 
c. Temporal ensembling for semi-supervised learning","The sentiment of the review is moderately negative. The reviewer points out several significant issues with the paper, including the ambition of the argument, the strength of the experimental results, and the clarity of the writing. The sentiment score is -60 because the feedback is critical but not entirely dismissive. The politeness score is 20 because the language used is somewhat polite but direct and lacks any positive reinforcement or encouragement.",-60,20
"The paper performs model-based reinforcement learning. It makes two main contributions. First, it divides training into two phases: the unsupervised phase for learning transition dynamics and the second phase for solving a task which comes with a particular reward signal. The scope of the paper is a good fit for ICLR.

The paper is very incremental: the ideas of using an ensemble of models to quantify uncertainty, to perform unsupervised pre-training and to explore using an intrinsic reward signal have all been known for many years.

The contribution of the paper seems to be the combination of these ideas and the way in which they are applied to RL. I have the following observations / complaints about this.

1. The paper is very sparse on details. There is no pseudocode for the main algorithm, and the quantity v^i_t (the epistemic variance on page 5) isn't defined anywhere. Without these things, it is difficult for me to say what the proposed algorithm is *exactly*.

2. Sections 1 and 2 of the paper seem unreasonably bloated, especially given the fact that the space could have been more meaningfully used as per (1).

3. The experimental section misses any kind of uncertainty estimates. If, as you say, you only had the computational resources for three runs, then you should report the results for all three. You should consider running at least one experiment for longer. This should be possible - a run of 50K steps of HalfCheetah takes about one hour on a modern 10-core PC, so this is something you should be able to do overnight.

4. The exploration mechanism is a little bit of a  mystery - it isn't concretely defined anywhere except for the fact that it uses intrinsic rewards. Again, please provide pseudocode.

As the paper states now, the lack of details makes it difficult for me to accept. However, I encourage the authors to do the following:
1. Provide pseudocode for the algorithm.
2. Provide pseudocode for exploration mechanism (unless subsumed by (1)).
3. Add uncertainty estimates to evaluation or at least report all runs.

I am willing to re-consider my decision once these things have been done.","The sentiment of the review is mixed but leans towards the negative side. The reviewer acknowledges the relevance of the paper's scope for ICLR and the combination of ideas as a contribution, but they also highlight significant shortcomings, such as the lack of details, bloated sections, and insufficient experimental data. This results in a sentiment score of -30. The politeness of the language is relatively high, as the reviewer uses polite phrases like 'I encourage the authors' and 'I am willing to re-consider my decision,' despite the critical feedback. This results in a politeness score of 70.",-30,70
"The paper reports the results of testing several stepsize adjustment related methods including  vanilla SGD, SGD with Neserov momentum, and ADAM. Also, it compares those methods with hypergradient and without. The paper reports several interesting results. For instance, they found hypergradient method on common optimizers doesn't perform better that the fixed exponential decay method propose by Wilson et al. (2017). 

Though it is an interesting paper, but the main issue with this paper is that it lacks enough innovation with respect to theory or empirical study. It is not deep or extensive enough for publishing at a top conference. 
  
On page 3, it will be better to explain why use mu = 0.9, beta, etc. Why use CIFAR-10, MNIST?

The URL in References looks out of bound. 


","The sentiment of the review is mixed but leans towards the negative side. The reviewer acknowledges that the paper reports interesting results but criticizes it for lacking innovation and depth, which are crucial for publication in a top conference. Therefore, the sentiment score is -40. The language used in the review is polite and constructive, offering specific suggestions for improvement without being harsh or dismissive. Thus, the politeness score is 80.",-40,80
"This work proposes a novel tree structure positional embedding by uniquely representing each path in a tree using a series of transformation, i.e., matmul for going up or down the edges. The tree encoding is used for transformer and shows gains over other strong baselines, e.g., RNNs, in synthetic data and a program translation task.

Pros:

- An interesting approach for representing tree structure encoding using a series of transformation. The idea of transformation without learnable parameters is novel.

- Better accuracy both on synthetic tasks and code translation tasks when compared with other strong baselines.

Cons:

- Computation seems to be larger given that the encoding has to be recomputed in every decoding step. I'd like to know the latencies incurred by the proposed method.

Other comment:

- I'd like to see experimental results on natural language tasks, e.g., syntax parsing.

- Section 2:  ""we see that is is not at all necessary"" -> that is

- Section 3: Notation is a little bit hard to follow, "":"" for D and U, and "";"" in stacking.
","The review starts by acknowledging the novelty and effectiveness of the proposed method, which indicates a positive sentiment. The reviewer appreciates the innovative approach and the improved accuracy over strong baselines. However, the review also points out a significant concern regarding computational overhead and suggests additional experiments, which slightly tempers the overall positivity. The language used is polite and constructive, with specific suggestions for improvement and minor corrections.",70,90
"PRO’s:
+well-written
+nice overall system: GAN framework for super-sampling audio incorporating features from an autoencoder
+some good-sounding examples

CON’s:
-some confusing/weakly-presented parts (admittedly covering lots of material in short space)
-I am confused about the evaluation; would like additional qualitative/observational understanding of what works, including more on how the results differ from baseline

SUMMARY: The task addressed in this work is: given a low-resolution audio signal, generate corresponding high-quality audio. The approach is a generative neural network that operates on raw audio and train within a GAN framework. 
Working in raw sample-space (e.g. pixels) is known to be challenging, so a stabilizing solution is to incorporate a feature loss. Feature loss, however, usually requires a network trained on a related task, and if such a net one does not already exist, then building one can have its own (possibly significant) challenges. In this work, the authors avoid this auxiliary challenge by using unsupervised feature losses, taking advantage of the fact that any audio signal can be downsampled, and therefore one has the corresponding upsampled signal as well.

The training framework is basically that of a GAN, but where, rather than providing the generator with a low-dimensional noise signal input, they provide the generator with the subsampled audio signal. The architecture includes a generator ( G(lo-fidelity)=high-fidelity ), a discriminator ( D(high-fidelity) = real or by super-sampled ? ), and an autoencoder ( \phi( signal x) = features of signal x at AE’s bottleneck). 

COMMENTS:

The generator network appears to be nearly identical to that of Kuleshov et al (2017)-- which becomes the baseline-- and so the primary contribution differentiating this work is the insertion of that network into a GAN framework along with the additional feature-based loss term. This is overall a nice problem and a nice approach! In that light, I believe that there is a new focus in this work on the perceptual quality of the outputs, as compared to (Kuleshov et al 2017). I would therefore ideally like to see (a) some attempts at perceptually evaluating the resulting output (beyond PESQ, e.g. with human subjects and with the understanding that, e.g. not all AMT workers have the same aural discriminative abilities themselves), and/or (b) more detailed associated qualitative descriptions/visualization of the super-sampled signal, perhaps with a few more samples if that would help. That said, I understand that there are page/space limitations. (more on this next)

Given the similarity of the U-net architectures to (Kuleshov et al 2017), why not move some of those descriptions to the appendix? 

For example, I found the description and figure illustrating the “superpixel layers” to be fairly uninformative: I see that the figure shows interleaving and de-interleaving, resulting in trading-off dimensionalities/ranks/etc, and we are told that this helps with well-known checkerboard artifacts, but I was confused about what the white elements represent, and the caption just reiterated that resolution was being increased and decreased. Overall, I didn’t really understand exactly the role that this plays in the system; I wondered if it either needed a lot more clarification (in an appendix?), or just less space spent on it, but keeping the pointers to the relevant references.  It seems that the subpixel layer was already implemented in Kuleshov 2017, with some explanation, yet in the present work a large table (Table 1(b)) is presented showing that there is no difference in quality metrics, and the text also mentions that there is no significant perceptual difference in audio. If the subpixel layer were explained in detail, and with justification, then I would potentially be OK with the negative results, but in this case it’s not clear why spend this time on it here. It’s possible that there is something simple about it that I am not understanding. I’m open to being convinced. Otherwise, why not just write: “Following (Kuleshov et al 2017), we use subpixel layers (Shi et al) [instead of ...] to speed up training, although we found that they make no significant perceptual effects.” or something along those lines, and leave it at that? 

I did appreciate the descriptions of models’ sensitivity to size/structure of the conv filters, importance of the res connections, etc.

My biggest confusion was with the evaluation & results:

Since the most directly related work was (Kuleshov 2017), I compared the super resolution (U-net) samples on that website (https://kuleshov.github.io/audio-super-res/ ) to the samples provided for the present work ( https://sites.google.com/view/unsupervised-audiosr/home ) and I was a bit confused, because the quality of the U-net samples in (Kuleshov 2017) seemed to be perceptually significantly better than the quality of the Deep CNN (U-net) baseline in the present work. Perhaps I am in error about this, but as far as I can tell, the superresolution in (Kuleshov et al 2017) is significantly better than the Deep CNN examples here. Is this a result of careful selection of examples? I do believe what I hear, e.g. that the MU-GAN8 is clearly better on some examples than the U-net8. But then for non-identical samples, how come U-net4 actually generally sounds better than U-net8? That doesn’t make immediate sense either (assuming no overfitting etc). Is the benefit in moving from U-net4 to U-net8 within a GAN context but then stabilizing  it with the feature-based loss? If so, then how does MU-GAN8 compare to U-net4? Would there be any info for the reader by doing an ablation removing the feature loss from the GAN framework? etc. I guess I would like to get a better understanding of what is actually going on, even if qualitative. Is there any qualitative or anecdotal observation about which “types” of samples one system works better on than another? For example, in the provided examples for the present paper, it seemed to be the case that perhaps the MU-GAN8 was more helpful for supersampling female voices, which might have more high-frequency components that seem to get lost when downsampling, but maybe I’m overgeneralizing from the few examples I heard. 

Some spectrograms might be helpful, since they do after all convey some useful information despite not telling much of the perceptual story. For example, are there visible but inaudible artifacts? Are such artifacts systematic?

Were individual audio samples represented as a one-hot encoding, or as floats? (I assume floats since there was no mention of sampling from a distribution to select the value).

A couple of typos:

descriminator → discriminator 

pg 6 “Impact of superpixel layers” -- last sentence of 2nd par is actually not a sentence. “the reduction in convolutional kernels prior to the superpixel operation.”

Overall, interesting work, and I enjoyed reading it. If some of my questions around evaluation could be addressed-- either in a revision, or in a rebuttal (e.g. if I completely misunderstood something)-- I would gladly consider revising my rating (which is currently somewhere between 6 and 7).
","The review starts with positive comments about the paper being well-written and having a nice overall system, which indicates a positive sentiment. However, it also points out some confusing and weakly-presented parts, and expresses confusion about the evaluation, which tempers the overall positivity. The reviewer provides constructive feedback and suggestions for improvement, which is done in a polite and respectful manner. The language used is considerate and acknowledges the challenges faced by the authors, indicating a high level of politeness.",60,90
"The submission describes a sort of hybrid between reinforcement learning and imitation learning, where an auxiliary imitation learning objective helps to guide the RL policy given expert demonstrations.  The method consists of concurrently maximizing an RL objective--augmented with the GAIL discriminator as a reward—and minimizing the GAIL objective, which optimizes the discriminator between expert and policy-generated states.  Only expert states (not actions) are required, which allows the method to work given only videos of the expert demonstrations.  Experiments show that adding the visual imitation learning component allows RL to work with sparse rewards for complex tasks, in situations where RL without the imitation learning component fails.

Pros:
+ It is an interesting result that adding a weak visual imitation loss dramatically improves RL with sparse rewards 
+ The idea of a visual imitation signal is well-motivated and could be used to solve practical problems
+ The method enables an ‘early termination’ heuristic based on the imitation loss, which seems like a nice heuristic to speed up RL in practice

Cons:
+ It seems possible that imitation only helps RL where imitation alone works pretty well already
+ Some contributions are a bit muddled: e.g., the “learning with no task reward” section is a little confusing, because it seems to describe what is essentially a variant of normal GAIL
+ The presentation borders on hand-wavy at parts and may benefit from a clean, formal description

The submission tackles a real, well-motivated problem that would appeal to many in the ICLR community.  The setting is attractive because expert demonstrations are available for many problems, so it seems obvious that they should be leveraged to solve RL problems—especially the hardest problems, which feature very sparse reward signals.  It is an interesting observation that an imitation loss can be used as  a dense reward signal to supplement the sparse RL reward.  The experimental results also seem very promising, as the imitation loss seems to mean the difference between sparse-reward RL completely failing and succeeding.  Some architectural / feature selection details developed here seem to also be a meaningful contribution, as these factors also seem to determine the success or failure of the method.

My biggest doubt about the method is whether it really only works where imitation learning works pretty well already.  If we don’t have enough expert examples for imitation learning to work, or if the expert is not optimizing the given reward function, then it is possible that adding the imitation loss is detrimental, because it induces an undesirable bias.  If, on the other hand, we do have enough training examples for imitation learning to succeed and the expert is optimizing the given reward function, then perhaps we should just do imitation learning instead of RL.  So, it is possible that there is some sweet spot where this method makes sense, but the extent of that sweet spot is unclear to me.

The experiments are unclear on this issue for a few reasons.  First, figure 4 is confusing, as it is titled ‘comparison to standard GAIL', which makes it sound like a comparison to standard imitation learning.  However, I believe this figure is actually showing the performance of different variants of GAIL used as a subroutine in the hybrid RL-IL method.  I would like to know how much reward vanilla GAIL (without sparse rewards) achieves in this setting.  Second, figure 8 seems to confirm that some variant of vanilla imitation learning (without sparse rewards) actually does work most of the time, achieving results that are as good as some variants of the hybrid RL-IL method.  I think it would be useful to know, essentially, how much gain the hybrid method achieves over vanilla IL in different situations.

Another disappointing aspect of the paper is the ‘learning with no task reward’ section, which is a bit confusing.  The concept seems reasonable at a first glance, except that once we replace the sparse task reward with another discriminator, aren’t we firmly back in the imitation learning setting again?  So, the motivation for this section just seems a bit unclear to me.  This seems to be describing a variant of GAIL with D4PG for the outer optimization instead of TRPO, which seems like a tangent from the main idea of the paper.  I don’t think it is necessarily a bad idea to have another discriminator for the goal, but this part seems somewhat out of place.

On presentation: I think the presentation is a bit overly hand-wavy in parts.  I think the manuscript could benefit from having a concise, formal description.  Currently, the paper feels like a series of disjoint equations with unclear connections among them.  The paper is still intelligible, but not without knowing a lot of context relating to RL/IL methods that are trendy right now.  I feel that this is an unfortunate trend recently that should be corrected.  Also, I’m not sure it is really necessary to invoke “GAIL” to describe the IL component, since the discriminator is in fact linear, and the entropy component is dropped.  I think “apprenticeship learning” may be a more apt analogy.

On originality: as far as I can tell, the main idea of the work is novel.  The work consists mainly of combining existing methods (D4PG, GAIL) in a novel way.  However, some minor novel variations of GAIL are also proposed, as well as novel architectural considerations.

Overall, this is a nice idea applied to a well-motivated problem with promising results, although the exact regime in which the method succeeds could be better characterized.","The sentiment of the review is generally positive, as the reviewer acknowledges the interesting results, well-motivated idea, and promising experimental results. However, there are some concerns and doubts raised about the method's effectiveness in certain scenarios, which slightly temper the overall positivity. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, even when pointing out weaknesses or areas for improvement. The reviewer provides specific feedback and suggestions in a respectful manner, leading to a politeness score of 90.",60,90
"This paper proposes a definition for interpretability which is indeed the same as model simplicity using the MDL principle. It has several issues:

1) Interpretability is not the same as simplicity or number of model parameters. For example, an MLP is thought to be more interpretable than an RNN with the same number of parameters.

2) The definition of explainability in Eq. (5) is flawed. It should not have the second term L(M(X)|M^o, X) which is the goodness of M^o's fit. You should estimate M^o using that equation and then report L(M^o|M^p) as the complexity of the best estimate of the model (subject to e.g. linear class). Mixing accuracy of estimation of a model and its simplicity does not give you a valid explainability score. 

3) In Section 2.4.2, the softmax operator will shrink the large negative coefficients to almost zero (reduces the degrees of freedom of a vector by 1). Thus, using softmax will result in loss of information. In the linear observer case, I am not sure why the authors cannot come up with a simple solution without any transformation.

4) Several references in the text are missing which hinders understanding of the paper.","The sentiment of the review is negative, as it points out several significant issues with the paper without any positive remarks. The reviewer highlights flaws in the definition of interpretability, explainability, and the use of the softmax operator, as well as missing references. The politeness of the language is neutral; the reviewer does not use any rude or overly harsh language but also does not employ particularly polite or encouraging language. The feedback is direct and to the point.",-70,0
"The paper introduces a new twist to the activation of a particular neuron. They use a modulator which looks at the input and performs a matrix multiplication to produce a vector. That vector is then used to scale the original input before passing it through an activation function. Since this modulating scalar can look across neurons to apply a per-neuron scalar, it overcomes the problem that otherwise neurons cannot incorporate their relative activation within a layer. They apply this new addition to several different kinds of neural network architectures and several different applications and show that it can achieve better performance than some models with more parameters.


Strengths:
- This is a simple, easy-to-implement idea that could easily be incorporated into existing models and frameworks.
- As the authors state, adding more width to a vanilla layer stops increasing performance at a certain point. Adding more complex connections to a given layer, like this, is a good way forward to increase capacity of layers.
- They achieve better performance than existing baselines in a wide variety of applications.
- The reasons this should perform better are intuitive and the introduction is well written.

Weaknesses:
- After identifying the problem with just summing inputs to a neuron, they evaluate the modulator value by just summing inputs in a layer. So while doing it twice computes a more complicated function, it is still a fundamentally simple computation.
- It is not clear from reading this whether the modulator weights are tied to the normal layer weights or not. The modulator nets have more parameters than their counterparts, so they would have to be separate, I imagine.
- The authors repeatedly emphasize that this is incorporating ""run-time"" information into the activation. This is true only in the sense that feedforward nets compute their output from their input, by definition at run-time. This information is no different from the tradition input to a network in any other regard, though.
- The p-values in the experiment section add no value to the conclusions drawn there and are not convincing.

Suggested Revisions:
- In the abstract: ""A biological neuron change[s]""
- The conclusion is too long and adds little to the paper

","The sentiment of the review is generally positive, as indicated by the praise for the simplicity, ease of implementation, and performance improvements of the proposed method. However, there are some criticisms regarding the fundamental simplicity of the computation and the clarity of certain aspects, which slightly temper the overall positive sentiment. Therefore, the sentiment score is 60. The language used in the review is polite and constructive, offering specific suggestions for improvement without being harsh or dismissive. Thus, the politeness score is 80.",60,80
"This paper proposes a model-based value-centric (MVC) framework for transfer learning in continuous RL problems, and an algorithm within that framework. The paper attempts to answer two questions: (1) ""why are current RL algorithms so inefficient in transfer learning"" and (2) ""what kind of RL algorithms could be friendly to transfer learning by nature""? I think these are very interesting questions to investigate, and researchers that work on transfer learning could benefit from insights on them. However, I am not yet convinced that this paper answers these questions satisfyingly. It would be great to hear the author's thoughts on my questions below. 

The main insight I take away from the paper is that policy gradient methods are not suitable for transfer learning compared to model-based and value-centric methods for some assumptions (the reward function not changing and the transition dynamics being deterministic). This insight and the experiments in the paper are interesting, but I am unsure if the paper as it is presented now passes the bar for ICLR.

In general the paper has two contributions:
A) analysis of value-centric vs policy-centric methods
B) an algorithm that is more useful for transfer learning.

Regarding A)
The authors argue that policy-centric algorithms are less useful for transfer learning than value-centric methods. 

They first illustrate this with an example in Section 3. Since this is just one example, as a reader I wonder if it would not be possible to construct an example that shows the exact opposite, where value iteration fails but policy gradient doesn't. It feels like there are many assumptions that play into the given example (the reward function not changing; the transition dynamics being deterministic; the choice of using policy gradients and value iteration). 

In addition, the authors provide a theoretical justification in the Appendix (which I have briefly scanned) and the intuition behind it in Section 5. From what I understand, the main problem arises from the policy's output space being a Gaussian distribution, which causes the policy being able to get stuck in a local optimum. Further, the authors show (in the Appendix) that under some assumtions the value function always converges. Are there any guarantees on this when we don't have access to the true reward and transition functions (which themselves could get stuck in a local optimum)?

Would the authors say that the phenomenon is more a problem with the algorithm (policy gradient vs value iteration) than policy-centric and value-centric methods in general? Are there other methods that would be able to transfer policies better than policy gradient methods?

Regarding B)
The author's proposed method (MVC) has three components: the value function, the dynamics model and the reward model, all of which are learned by neural networks. It seems like the main advantage comes from using a model (since that's the aspect which changes when having to transfer to an altered MDP). Does the advantage of this method over DDPG and TRPO come from the fact that the dynamics model changes smoothly, and we have an approximation to it? Then it is not surprising that this outperforms a policy gradient method. 

Other comments:

- Could you explain what is meant by ""precise"" and ""imprecise"" when speaking about policies or value functions?
- Could you explain what is meant by the algorithm being ""accessible"" (e.g., Definition 1)?

- Section 2.1: In Property 1, what is f? Could you make explicit why we are interested in the two properties listed? By ""not rigorously"", do you mean that those properties are based on intuition? These properties are used later in the paper and the appendix, so I wonder how strong of an assumption this is.
- Section 2.2: Could you explain what is meant by ""task""? You say that within the MDP, the transition dynamics and reward functions change, but the task stays the same. However, earlier (in the introduction) you state that only the environment dynamics change. I find it confusing that ""the task"" is something hand-wavy and not part of the formal definition of the MDP. In what exact ways can the reward function be influenced by the change in the transition dynamics? 
- Section 3: Replace ""obviously"" with ""hence""; remove ""it is not hard to find that"". This might not be so trivial for some readers.
- Appendix B: Refer to Table 1 in the text.

Clarity: The paper is written well, but I think some assumptions and their affects should be stated more clearly and put into context. The paper misses a discussion / conclusion section. It would be great to see a discussion on some of the assumptions; e.g., what if the low dimensional assumtion breaks down? What if we assume that also the reward function can change? The authors are in a unique position to give insight into these things (even if the results from the paper do not hold after dropping some assumptions) and it would be very helpful to share these with the reader in a discussion section.","The sentiment of the review is mixed. The reviewer acknowledges the interesting questions posed by the paper and the potential benefits to the research community, which is positive. However, the reviewer also expresses significant doubts about whether the paper satisfactorily answers these questions and whether it meets the standards for ICLR, which is negative. Therefore, the sentiment score is around -20. The politeness of the language is high. The reviewer uses polite phrases such as 'It would be great to hear the author's thoughts,' 'Could you explain,' and 'The paper is written well,' indicating a respectful and constructive tone. Therefore, the politeness score is 80.",-20,80
"This is an interesting paper, trying to find the adversarial cases in reinforcement learning agents. The paper discusses several different settings to investigate how generalizable the worst-case environment is across different models and conjectured that it comes from the bias in training the agents. Overall the paper is well-written and the experiments seem convincing. I have two questions regarding the presented result.

1. The search algorithm depicted in section 2 is only able to find a local optimum in the environment space. How robust is the result given different initializations?

2. It is briefly discussed in the paper that the failure in certain mazes might come from the structural bias in the training and the “complex” mazes are under-represented in the training dataset. It is hence natural to ask, if the procedure described in this paper can be incorporated to enhance the performance by some simple heuristics like re-weighting the training samples. I think some discussion on this would be beneficial for verifying the conjecture made here.

3. The authors compared the “hardness” of the mazes based on the number of walls in the maze. But it is arguably a good metric as the authors also mentioned visibility and other factors in measuring the complexity of the task. I would like to see more exploration in different factors that accounts for the complexity and maybe compare different agents to see if they are sensitive in the same set of factors. 

To summarize, I like the idea of the paper and I think the result can be illuminating and worth some more follow-up work to understand the RL training in general.
","The sentiment of the review is positive, as indicated by phrases like 'interesting paper,' 'well-written,' and 'experiments seem convincing.' The reviewer also expresses appreciation for the idea and potential impact of the paper. Therefore, the sentiment score is 80. The politeness of the language is high, as the reviewer uses polite phrases such as 'I have two questions,' 'I think some discussion on this would be beneficial,' and 'I would like to see more exploration.' The reviewer provides constructive feedback in a respectful manner, so the politeness score is 90.",80,90
"The paper theoretically analyzes the sparsity property of the stationary point of layerwise l1-regularized network trimming. Experiments are conducted to show that reaching a stationary point of the optimization can help to deliver good performance. Specific comments follow.

1. While the paper analyzes the properties of the stationary point of the layerwise objective (5), the experiments seem to be conducted based on the different joint objective (8). Experimental results of optimizing (5) seem missing. While the reviewer understands that (5) and (8)  are closely related, and the theoretical insights for (5) can potentially translate to the scenario in (8), the reviewer is not sure whether the theory for (5)  is rigorously justified by the experiments.

2. It is also unclear how tight the bound provided by Theorem 1 is.  Is the bound vacuous? Relevant statistics in the experiments might need to be reported to elucidate this point.

3. It is also unclear how the trade-off in point (b) of the abstract is justified in the experiments.

Minor Points:
page 2, the definition of $X^{(j)}$, the index of $l$ and $j$ seem to be typos.
page 2, definition 1, the definition of the bracket need to be specified. 
page 4, the concept of stationary point and general position can be introduced before presenting Theorem 1 to improve readability.
page 4, Corollary 1, should it be $nnz(\hat{W})\le JN k_{\mathcal{S}}$?
page 7, Table 2, FLOPS should be FLOP? 
page 8, is FLOP related to the time/speed needed for compression? If so, it should be specified. If not, compression runtime should also be reported.




","The sentiment of the review appears to be neutral to slightly positive. The reviewer acknowledges the theoretical analysis and the experiments conducted but raises several concerns and questions about the rigor and clarity of the results. Therefore, the sentiment score is 10. The politeness of the language is quite high; the reviewer uses phrases like 'the reviewer understands' and 'might need to be reported,' which are considerate and constructive. Thus, the politeness score is 80.",10,80
"The paper proposes an imitation learning model able to generate trajectories based on some expert trajectories. The assumption is that observed trajectories contain multi-modal (i.e. style) information that is not naturally captured by existing methods. The authors proposed a VAE based architecture that uses a prior distribution P(z) to simultaneously generate (state-action) pairs based on a LSTM decoder (actually, one LSTM for the states and one interleaved LSTM for the actions). This decoder is learned using a classical VAE auto-encoding loss, observed trajectories being encoder through a bi-LSTM. Experiments are made on three toy examples: a simple 2d Navigation case exhibiting 3 different 'styles', a 2D circle example with also 3 different styles, and a zombie attack scenario with two different styles. The results show that the model is able to capture different clusters of trajectories. 

First of all, the paper does not propose a new model, but an instantiation of an existing model to a particular case. The main difference with SoTA is that the authors propose to both decode states and actions without using a simulator. The contribution of the paper is thus quite light. Moreover, it is unclear how the model can be used to get a policy corresponding to a particular mode. Can we use the learned decoders to generate actions on-the-fly in a real/simulated environment? Right now (section 3.3), actions are generated on generated states, but not on observed ones.  The paper has to clarify this point since just generating trajectories seems to be a little bit useless. In general Section 3.3 lacks of details (e.g the rolling window is also unclear). Also, the model could be described a little bit more in term of architecture, particularly on the critical point about how the two decoding LSTMs are interacting. 

From the experimental point of view, the paper attacks very simple cases, without any comparison with state-of-the-art, and without almost any quantitative results. If Section 4.1 and 4.2 are useful to explore the ability of the model on simple cases, I would recommend the authors to merge these two sections in one smaller one, and then to focus on more realistic experiments. For example, it seems to me that the experimental setting proposed for example in [Li et al.] on driving styles could be interesting, and would allow a comparison with existing methods. Also the model proposed in [Co-Reyes et al.] could be an interesting comparison (at least, keeping the principle of this paper, without the hierarchical structure), particularly because this model is based on the use of a simulator while the proposed one is not. If a performance close to this baseline can be obtained with your model, it would be interesting for the community.

Right now, the experimental part and the too small contribution of the paper are not enough for acceptance. I would suggest the authors to:
* better describe their contribution i.e model architecture and how the model can be used to obtain a real policy
* use 'stronger' use cases for the experiments, and particularly existing use cases
* provide a deep quantitative and qualitative comparison with SoTA

Pro:
* simple method, no need of a simulator

Cons:
* not clear how to move from trajectory generation to a real policy
* small contribution
* too light experimental study without comparison with baselines and state of the art
","The sentiment of the review is moderately negative. The reviewer acknowledges the simplicity and potential of the method but criticizes the lack of novelty, insufficient experimental validation, and unclear practical application. The sentiment score is -40 because the review is more critical than neutral but not entirely dismissive. The politeness score is 50 because the language is constructive and provides specific recommendations for improvement, even though it points out several weaknesses.",-40,50
"The authors study in this paper the approximation capabilities of neural networks for real valued functions on probability measure spaces (and on tree structured domains). 

The first step of the paper consists in extending standard NN results to probability measure spaces, that is rather than having finite dimensional vectors as inputs, the NN considered here have probability measures as inputs. The extension to this case is straightforward and closely related to older extension on infinite dimensional spaces (see for instance the seminal paper of Stinchcombe https://doi.org/10.1016/S0893-6080(98)00108-7 and e.g. http://dx.doi.org/10.1016/j.neunet.2004.07.001 for an application to NN with functional inputs). Nothing quite new here.

In addition, and exactly as in the case of functional inputs, the real world neural networks do not implement what is covered by the theorem but only an approximation of it. This is acknowledged by the authors at the end of Section 2 but in a way that is close to hand waving. Indeed while the probability distribution point is valuable and gives interesting tools in the MIL context, the truth is that we have no reason to assume the bag sizes will grow to infinite or even will be under our control. In fact there are many situations were the bag sizes are part of the data (for instance when a text is embedded in a vector space word by word and then represented as a bag of vectors). Thus proving some form of universal approximation in the multiple instance learning context would need to take this fact into account, something that is not done at all here. 

Therefore I believe the contribution of this paper to be somewhat limited. ","The sentiment of the review is moderately negative. The reviewer acknowledges the extension of standard neural network results to probability measure spaces but criticizes the novelty and practical applicability of the work. The reviewer points out that the extension is straightforward and closely related to older work, implying a lack of significant contribution. Additionally, the reviewer highlights a critical flaw in the paper's assumptions about bag sizes in multiple instance learning contexts. The politeness of the language is neutral to slightly polite. The reviewer uses formal and respectful language, even while expressing criticism, and provides specific references and reasoning for their points.",-40,20
"Pros:
+ Improving joint training of non-differentiable pipelines is a meaningful and relevant problem
+ Using the stochastic computation graph structure to smooth a pipeline in a structured way is a plausible idea

Cons:
+ The main result of the paper concerning sufficient conditions for optimality of the method seems dubious
+ It is not obvious why this method would outperform simple baselines, and baselines for joint training were tried
+ The notation seems unnecessarily bloated and overly formal
+ The exposition spends too much time on prior work, too little on the contribution, and the description of the contribution is confusing

The submission describes a method for smoothing a non-differentiable machine learning pipeline (such as the Faster-RCNN detector), so that gradient-based methods may be applied to jointly train all the parameters of the pipeline.  In particular, the proposal involves recasting the pipeline as a stochastic computation graph (SCG), adding stochastic nodes to this graph, and then using REINFORCE-style policy gradients to perform parameter learning on the SCG.  It is claimed that under certain conditions, the optimal parameters of the resulting SCG are also optimal for the original pipeline.  The method is applied to optimizing the parameters of Faster-RCNN.

I think making non-differentiable pipelines differentiable is an intuitively appealing concept.  A lot of important, practical machine learning systems fall into this category, so devising a nice way to do global parameter optimization for such systems could potentially have significant impact.  In general, we can’t hope to make much meaningful progress on the problem of optimizing general nonlinear, differentiable functions, but it is plausible that a method that targets key non-differentiable components for smoothing—such as this paper—could outperform a generic black-box optimizer.  So, I think the basic idea here is plausible and addresses an important problem.

Unfortunately, I think this work loses sight of that high-level goal: to me, the key question is whether the proposed approach outperforms any other simple method for global parameter optimization in the presence of nonlinearities and nondifferentiability.  The paper fails to answer this question because no baselines for global parameter optimization were tried.  We can just treat the pipeline as a black box mapping parameters to training set performance, and so any black-box optimization method can be applied to this problem.  It is not clear that the proposed method would outperform an arbitrary black box optimization method such as simulated annealing, Nelder-Mead, cross-entropy method, etc.

I think there are also much simpler methods in a similar vein to the proposed method that might also perform just as well as the proposal.  One key conceptual issue here is that reducing the problem to a reinforcement learning problem, as the submission does, is not much of a reduction at all.  First, if the goal is to do global parameter optimization, then we don’t really have to smooth the pipeline itself: we can just smooth the black box mapping parameters to performance, and then optimize that with SGD.  There are many ways to do this--if we want to use policy gradient, we can just express the problem as something in this form:

min_\phi E_{\theta ~ q_\phi} C(\theta)

where C is the black-box mapping parameters \theta to a performance index (such as mean AP), q_\phi is a distribution over parameters (e.g., Gaussian), and \phi are the distribution parameters (e.g., mean, covariance of the Gaussian).  We can then optimize this using REINFORCE policy gradients.

If we want to really smooth the pipeline itself, then it is also easy to do this by devising a suitable MDP and then applying REINFORCE with the usual MDP structure.  We simply identify the state s_t at time t with the output of the t’th pipeline stage, introduce a new ‘action’ variable a_t representing a ’stochastified output’, and trivial dynamics (P(s_{t+1} | s_t, a_t) = \delta(s_{t+1} - a_t)).  If the policy is a Gaussian (P(a_t | s_t) = N(a_t; s_t, \Sigma)), then this is similar to relaxing the constraint that one stage’s output is equal to the input of the next stage, and somehow quadratically penalizing their difference.  In fact, there is a neural network training method based explicitly on this penalization view [A], and it would make yet another great baseline to try.

In fact, the proposed method is essentially similar to what I have just described, but it is unfortunately described in an overcomplicated way that obscures the true nature of the method.  I think the whole SCG framework is overkill here.  Too much of the paper is spent just rehashing the SCG framework, and the very heavy notation again just obscures the essential character of the method.

If there were, as the paper claims, some interesting condition under which the method produces solutions that are optimal under the original pipeline, that would be remarkable and interesting.  However, I have serious doubts about this part of the paper.  The key problem is the statement that “It follows that c(k_c, DEPS_c - k_c) = c(…) + z_c”.  The paper seems to be claiming that if E z = 0, then c(k + z) = c(k) + z, which can’t possibly be true in general.  

The heavy and opaque notation makes it very difficult to understand this section.  Perhaps it would help to consider a very simple example.  Suppose we want to minimize E_{x ~ q} c(y(x)) (where x ~ q means x is distributed as q).  We can introduce only one new stochastic node (k = y + z), between y and c.  Clearly c(y + z) is not generally equal to c(y) + z, even if E z = 0.

In summary, I think the submission needs a lot of work on multiple axes before it can make a significant impact.  The most important issues are a complete lack of relevant baselines and the dubious claims about sufficient conditions for optimality.  The idea could have merit, but it needs to be carefully compared and motivated with respect to existing work (such as [A]) as well as the simple baselines I have mentioned.  The presentation also needs to be revised to find the simplest expression of the method and to focus on the interesting parts.

[A] Taylor, Gavin, et al. ""Training neural networks without gradients: A scalable admm approach."" International Conference on Machine Learning. 2016.","The sentiment of the review is mixed. The reviewer acknowledges the relevance and potential impact of the problem being addressed and finds the basic idea plausible, which are positive points. However, the review also contains significant criticism regarding the main results, the lack of baseline comparisons, and the complexity of the notation and exposition. Therefore, the sentiment score is slightly negative. The politeness of the language is quite high. The reviewer uses polite and constructive language throughout, even when pointing out major flaws and suggesting improvements. The tone is professional and respectful, aiming to help the authors improve their work.",-20,80
"Overall, this is a thorough attempt at a system for evaluating various generative models on synthetic problems vaguely representative of the kinds of problems claimed to be covered by GANs. I think the approach and the conclusions drawn are mostly reasonable, with one major caveat discussed shortly.

I also think it would help in a revision to add evaluations of more recent successors to RealNVP, such as MAF (NIPS 2017, https://arxiv.org/abs/1705.07057 ), Glow ( https://arxiv.org/abs/1807.03039 ), and (although of course this paper came out concurrently with your submission) the promising FFJORD ( https://openreview.net/forum?id=rJxgknCcK7 ). The scale of comparison of GAN variants is also much smaller than that of Lucic et al. or their followup, Kurach et al. ( https://arxiv.org/abs/1807.04720 ), which is not cited here (and should be).

But primarily, I think there are some serious concerns with your choice of metrics that make the results as they are difficult to interpret.


""Note that OT is not a distance in the standard mathematical sense, as for instance the 'distance' between two sets of points sampled from the same distribution is not zero."" -- You've confused some notions here. The Wasserstein-1 distance, which is a scalar times the variant of OT you use here, absolutely is a proper distance metric between distributions: W(P, Q) is a metric. But when you compute the OT distance between *samples*, OT(S, T) with S ~ P and T ~ Q, you're equivalently computing the distance W(\hat{P}, \hat{Q}) between the empirical distributions of the samples, \hat{P} = 1/N \sum_i \delta_{S_i} and the similar \hat{Q}, which of course are not the same thing as the source distributions themselves. You can, though, view OT(S, T) as an *estimator* of W(P, Q); the distance between *distributions* is what we actually care about.

It is well-known that these empirical distributions of samples \hat{P} converge to the true distribution P (in the Wasserstein sense, W(P, \hat{P})) exponentially slowly in the dimension, which is what your example about high-dimensional distributions demonstrates. Incidentally, this is exactly the example used in Arora et al. (ICML 2017, https://arxiv.org/abs/1703.00573 ). This means that, viewed as an estimator of the true distance between distributions, the empirical-distribution OT estimator is strongly biased. Thus it becomes very difficult to tell what the true OT value is at any sample size, and moreover this amount of bias might differ for different distribution pairs even at the same sample size, so *comparing* OT estimates at a fixed sample size is a tricky business. For example, in your Figure 2, when the ""oracle"" score is significantly more than zero, you know that all of your estimates are very strongly biased. There is not, as far as I know, any strong reason to suspect that this amount of bias should be comparable for different distribution pairs, making any conclusions drawn from these numbers suspect.


Your scheme you call ""Two-Sample Test,"" first, should have a more specific name. Two-sample testing is an extremely broad field, with instances including the classical Kolmogorov-Smirnov test and t tests, the popular-in-ML kernel MMD-based tests, and even Wasserstein-based tests (e.g. https://arxiv.org/abs/1509.02237 ). Previous applications of these tests in GANs and generative models include Bounliphone et al. (ICLR 2016, https://arxiv.org/abs/1511.04581 ), Lopez-Paz and Oquab (2016 - which you cite without a venue but which was at ICLR 2017), Sutherland et al. (ICLR 2017, https://arxiv.org/abs/1611.04488 ), Huang et al. (2018), and more, using a variety of schemes. Your name for this should include ""nearest neighbor"" or something along those lines to avoid confusion.

Also, you call this an ""extension of the original formulation,"" but in the common case where n(x) is more often right than wrong, your v is exactly \hat t - 1 of Lopez-Paz and Oquab; see their (2). If it's usually wrong, then v = 1 - \hat t; only when the signs differ per class does it significantly differ from theirs, and in any case I don't see a real motivation to put the absolute values for each class separately rather than just taking |\hat t - 1/2|.

Moreover, it's kind of crazy to term your v statistic a two-sample *test* -- you have nothing in there about its sampling distribution, which is key to hypothesis testing to obtain e.g. a p-value. (Maybe the variance of v is very different between different distributions; this is likely the case. In any case the variance will probably become extremely large as the dimension increases.) Comparing this score is thus difficult, but in any case calling it a ""test"" is potentially very misleading. You could, though, estimate the variance as described by Lopez-Paz and Oquab to construct a test.

Also: you can imagine the statistic v(S, T) as an estimator of the distance between distributions given as
  D(P, Q) = |1/2 - \int ( 1 if p(x) > q(x), 0 o.w.) p(x) dx|
          + |1/2 - \int (-1 if p(x) > q(x), 0 o.w.) q(x) dx|.
But v(S, T) is, like for the OT distance, a biased estimator of this distance, whose bias will get worse with the dimension. Thus, like with the OT, it's hard to meaningfully compare v(S, T) as an attempt to compare *distributions* based on D, which is what we actually care about. Here the oracle score does not show strong bias: assuming a reasonable number of samples, when P = Q the v estimator is always going be approximately 0. But this doesn't mean that other estimators aren't strongly biased, and indeed this is exactly what your Appendix C shows. The strong change in performance for KDE is somewhat hard to interpret, but maybe has something to do with the connection between KDE and NN-based methods?


Your log-likelihood score is an unbiased and asymptotically normal estimate of the true distribution score (the cross-entropy), so it's easy to compare. But it accounts only for a very small portion of comparing distributions.


There is at least one score in common use for this kind of evaluation with easy-to-compare estimators: the squared MMD. It has an easy-to-compute unbiased and asymptotically normal estimator, so it's easy to get confidence intervals for the true value between distributions at any sample size, making comparing the numbers based on a reasonable number of samples easy. There's also a well-devolped theory for how to construct p-values for a test if you want those; Bounliphone et al. above even developed a relative test to compare the MMDs of two models accounting for the correlations due to using the same ""target"" set, though if you use separate target sets (because you can easily sample more points from your synthetic distribution) then it's simpler. The choice of kernel does matter, but I think the median-heuristic Gaussian kernel would be a very reasonable score to add to your repertoire, and for particular distributions you also might be able to pick a better kernel (e.g. based on the causal factors when those exist). See also Binkowski et al. (ICLR 2018, https://arxiv.org/abs/1801.01401 ) for a detailed discussion of these issues in comparison to the FID score.

Using a metric whose estimation can be understood, and whose estimators can be reliably compared, is I think vital to any evaluation process. This also prevents issues like when RealNVP outperforms the oracle, which should be impossible with any proper evaluation metric.



Minor points:

- Why is Pedregosa et al. (2011) cited for fitting multivariate Gaussians by maximum likelihood? This is something that doesn't need a citation, especially not to scikit-learn, which doesn't even (I don't think) contain an implementation of fitting Gaussians beyond (np.mean(X, axis=0), np.cov(X, rowvar=False)).

- Mode coverage and related scores: this is based on assigning sample points to their single most likely clusters? I'd imagine that sometimes a model will output points far from any cluster, in which case the cluster that happens to be closest might happen to be the most likely, but it's strange to really count that point as part of that cluster for these scores. Or similarly, a point might be relatively evenly spaced between two clusters, in which case the assignment could be fairly arbitrary, again making these scores a little strange.","The review starts with a generally positive sentiment, acknowledging the thoroughness and reasonableness of the approach and conclusions, but it quickly transitions to highlighting a major caveat and several serious concerns. The sentiment score is therefore mixed but leans slightly positive due to the initial praise. The language used throughout the review is constructive and professional, offering detailed feedback and suggestions for improvement without being rude or dismissive.",20,80
"Summary
The authors propose a relatively simple approach to mine noisy parallel sentences which are useful to greatly improve performance of purely unsupervised MT algorithms.
The method consists of a) mining documents that refer to the same topic, b) extracting from these documents parallel sentences, c) training the usual unsup MT pipeline with two additional losses, one that encourages good translation of the extracted parallel sentences and another one forcing the distribution of words to match at the document level.

Novelty: the approach is novel.

Clarity: the paper is clearly written.

Empirical validation: The empirical validation is solid but limited. The authors could further strengthen it by testing on low-resource language pairs (En-Ro, En-Ur).
It would also be useful to report more stats about the retrieved sentences in tab. 1 (average length compared to ground truth, BLEU using as reference the translation of a SoA supervised MT method, etc.)

Questions
1) Sec. 3.2 is the least clear of the paper. The notation of eq. 7 is quite unclear because of the overloading (e.g., P refers to both the model and the empirical distribution).
I am also unclear about this constraint about matching the topic distribution: as far as I understood, the model gets only one gradient signal for the whole document. I find then surprising that the authors managed to get any significant improvement by adding this term.
Related to this term, how is it computed? Are documents translated on the fly as training proceeds? Could the authors provide more details?

2) Have the authors considered matching sentences to any other sentence in the monolingual corpus as opposed to sentences in the comparable document?
 ","The sentiment of the review is generally positive. The reviewer acknowledges the novelty and clarity of the paper and describes the empirical validation as solid, although limited. The sentiment score is therefore 70. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, providing specific recommendations and asking questions in a respectful manner. The politeness score is 90.",70,90
"This paper proposes a sequence to sequence model augmented with a multinomial latent variable. This variable can be used to generate multiple candidate translations during decoding. This approach is simpler than previous work using continuous latent variables or modifying beam search to encourage diversity, obtaining more diverse translations with a smaller drop in translation accuracy.   

Strengths:
- Simple model that succeeds in achieving its goal of generating diverse translations.
- Provides insights into training models with categorical latent variables. 
Weaknesses:
- More insight into what the latent variable is learning to represent would strengthen the paper. 

While the model is simple, its simplicity has significant strengths: In contrast to more complex latent space, the latent variable assignments can be enumerated explicitly, which enables it to be used to control the generation and compare outputs. The simplicity of the model will force the latent variable towards capturing diversity - modelling uncertainty in how to express the output rather than uncertainty in the content. 

One question about the model architecture is just whether it is sufficient to feed the latent variable embedding only once, as it effect might be diluted across long output sequences (as opposed to, say, feeding the latent variable at each time step). 

The paper provides some interesting insights, such as the need to do hard EM-style training and turning off dropout when inferring the best latent variable assignment during training, to avoid mode collapse. 

What is the effect of initialization? This often has a large impact in EM-style training, and could also lead to mode collapse, though in this case the restricted parameterization might prevent that. 

What is the training time and computational resource requirements? Are multiple DGX-1s running in parallel required to train the model?

What is not clear enough from the paper is what kind of structure the latent variables learn to capture. In particular this model is not biassed towards any explicit notion of the kind of diversity one would like to learn. While there is some qualitative analysis, further analysis would strengthen the paper. 

Overall this is a very interesting contributions that offer useful insights into designing controllable sequence generation models.","The sentiment of the review is generally positive, as it highlights the strengths of the proposed model and acknowledges its contributions to the field. The reviewer appreciates the simplicity and effectiveness of the model, although they do point out areas for improvement. Therefore, the sentiment score is 70. The politeness of the language is high, as the reviewer uses constructive feedback and polite language throughout the review. They phrase their criticisms in a way that is meant to help the authors improve their work, rather than disparage it. Therefore, the politeness score is 90.",70,90
"

Review Summary
--------------
The paper presents a combination of rule lists, prototypes, and deep representation learning to fit classifiers that are said to be simultaneously ""accurate"" and ""interpretable"". While the topic is interesting and the direction seems novel, I don't think the work is quite polished or competitive enough to be accepted without significant revision. The major issues include non-competitive evaluation of what ""interpretability"" means, ROC AUC numbers that are indistinguishable from standard deep learning (RCNN) pipelines that use many fewer parameters, and many unjustified choices inside the method itself. The paper itself could also benefit from revision to improve flow and introduce technical ideas to be more accessible to readers.


Paper Summary
-------------
The paper presents a new method called ""PEARL"" (Prototype Learning via Rule Lists), which produces a rule list, a set of prototypes, and a deep feed-forward neural network that can embed any input data into a low-dimensional feature space. The primary intended application is classifying subjects into a finite set of possible disorders given longitudinal electronic health records with categorical features observed at T irregular time intervals. 

The paper suggests learning a representation for each subject's data by feeding the EHR time series into a recurrent convolutional NN. The input data is a 2 x T array, with one row representing observed data and second row giving time delay between successive observations. The vector output of an initial convolutional RNN is then fed into a highway network to produce a final vector denoted ""h"". 

Given an encoder to produce feature vectors, and a fixed rule list learned from data itself, the paper suggests obtaining a prototype for each rule by computing the average vector of all data that matches the given rule. The quality of these prototypes and related neural networks (for computing features and predicting labels from features) is then assessed via their loss function in Eq. 1: a weighted combination of how well the prototypes match the learned embeddings (distance to closest prototype) and how well the classifier predicts labels.  The core idea is that the embedding is learned to classify well while creating a latent space that looks like the prototypes of the rule list.

After training an embedding and NN classifier on a fixed rule list, it seems the data is reweighted according to some heuristic procedure to obtain better properties, then a new rule list is trained and the process repeats again. (I admit the reweight procedure's purpose was never clear to me).

Experiments are done on a proprietary heart failure EHR dataset and on a subset of MIMIC data. 

Strengths
---------
* Seems original: I'm unaware of any other method connecting rule lists AND prototypes AND NNs
* Neat applications to healthcare

Limitations
-----------
* Interpretability evaluation seems weak: no human subject experiments, no quantiative metrics, unclear if rule-lists shown is an apples-to-apples comparison
* Prototypes themselves never evaluated 
* Many design choices inside method not justified with experiments -- why highway networks + RCNNs?

Major Issues with Method
------------------------

## M1: Not clear that AUC difference between PEARL and baselines is significant

The major issue is that the presented approach does not seem significantly different in predictive performance than the baseline Recurrent CNN. Comparing ROC AUC, we have PEARL's 0.688 to RCNN'S 0.682 with stddev of 0.009 on the proprietary heart failure dataset, and PEARL's 0.769 to RCNN's 0.766 with stddev of 0.009. When AUCs match this closely, I struggle to believe one model is definitively better, especially given that the RCNN has 2x *fewer* parameters (8.4k to 18.4k). 

If the counterargument is that the resulting ""deep model"" is not ""interpretable"", one should at least compare to a post-processing step where the decision boundary of the RCNN is the reference to which a rule list or decision tree is trained.

## M2: Interpretability evaluation not clear.

Isn't the maximum number of rules set in advance? 

Additionally, prototypes are a key part of this work, but the learned prototypes are not evaluated at all in any figure (except to track avg. distance from prototype while training). If prototypes are so central to this work, I would like to see a formal evaluation of whether the learned prototypes are indeed better (in terms of distance, or inspection of values by an expert, or something else) than alternatives like Li et al.

## M3: Missing a good synthetic/small dataset experiment

Neither of the presented data tasks is particularly easy to understand for non-experts. I'd suggest creating an additional experiment where the audience of ML readers is likely to easily grasp whether a set of rule lists is ""good"" for the problem at hand... maybe create your own synthetic task or a UCI dataset or something, or even use the stop-and-frisk crime dataset from the Angelino et al. 2018 paper. Then you can compare against just a few relevant baselines (rule lists only or prototypes only). I think a better illustrative experiment will help readers grasp differences between methods. 

## M4: How crucial is feature selection?

In each iteration, Algo. 1 performs feature selection before learning rules. Are any other baselines (trees, rule lists) allowed feature selection before the classifier is learned? What would happen to PEARL without feature selection? What method is used for selection? (A search of the document only has 'feature selection' occur once, in the Alg. itself, so it seems explanation is missing).

## M5: Why are multiple algorithm iterations needed?

Won't steps 3 and 4 of Alg. 1 result in the same rules every time? It's not clear then why on subsequent iterations the algorithm would improve. Perhaps it's just the reweighting of data that causes these steps to change?

Minor issues
------------

## Loss function notation confusing

Doesn't the rule list classifier s_R take the data itself X? Not the learned embedding h(X)? Please fix or clarify Eq. 1. I think you might clarify notation by just writing yhat(h(X)) if you mean the predicted label of some example as done by your NNs. Using ""R"" makes folks think the rule list is involved.

## Not clear why per-example reweighting is required

None of the experiments assess why per-example reweighting (lines 6-9 of Algo. 1) is required. Readers would like to see a comparison of performance with and without this step.

## Not clear or justified when ""averaged"" prototypes are acceptable

Are your ""averaged"" prototypes guaranteed to satisfy the rule they represent? Is taking the average of vectors that match a rule always guaranteed to also match the rule? I don't think this is necessarily true. Consider a rule that says ""if x[0] == 0 or x[1] == 0, then ___"".  Suppose the only matching vectors are x_A = [0 1] and x_B = [1 0]. The average vector is [0.5 0.5] which doesn't work.

## Several different measures of distance used without careful justification 

Why use two different distances -- Euclidean distance to assess distance to prototypes for prototype assignment, and then cosine similarity when deciding which examples to upweight or downweight? Why not just use Euclidean distance for both (appropriately transformed to a similarity)?

Comments on Presentation
------------------------
Overall I think every section of the paper needs significant revision to improve a reader's ability to understand main ideas. Notation could be introduced slowly (explain purpose and dimension of every variable), assumptions could be clearly stated (e.g. each individual rule can have ANDs but not ORs), and design choices justified. You might try the test of giving the paper to a colleague and having them explain back the ideas of each section to you... currently I do not believe this version passes this test.

The introduction claims that ""clinicians are often unwilling to accept algorithm recommendations without clarity as to the underlying reasoning"", but I would be careful in blindly asserting this without evidence. For a nice argument about avoiding blind assumptions about what doctor's will and won't accept, see Lipton's 2017 paper ""The Doctor Just Won't Accept That"" (https://arxiv.org/abs/1711.08037)

Additionally, the authors should clarify more precisely what definition of interpretability is needed for their applications. Is it simplicity? Is it conceptual alignment with known medical facts? Is it the ability to transparently list the rules in plain English?

Line-by-line details
--------------------

## Sec. 2

When introducing p_j, should clarify this this is one prototype vector of many.

When defining p_j = f_j(X), can you clarify what dimensionality p_j has? Is it always the same size as each example's data vector x_i?


## Sec. 3

Fig. 2: I don't find this figure very easy-to-understand. It's clear that after embedding raw features to a new space, the learned rules are *different*, but it's not clear they are *better*.  None of the illustrated rules perfectly segments the different colors, for example. I guess the point is all the red dots are within one rule? But they aren't alone (there are blue and orange dots too), so it's still not clear this would be a better classifier.

For EHR datasets, are you assuming that events are always categorical? And that outcomes ""y"" are always discrete (one-of-L) variables? Or could y be real-valued?

Eq. 1: You should make notation clearly indicate which terms depend on \theta. Currently it seems that nothing is a function of \theta.

Eq. 1: Do you also find the prototype set P that minimizes this objective? Or is there another way to obtain P given parameters \theta? This is confusing just from reading the eqn.

What size is the learned representation h(X)? Is it a vector?

Eq. 6: Do you really need a ""network"" to compute the distance to each of the K prototypes? Can't you just compute these distances directly?

## Sec 4

""Mac OS 1.4"" : Do you mean Mac OS version 10.4? Not clear this is relevant.

4.3 Case Study: How do I read these rules? Is this rule applied only if ALL conditions are true? or if any individual one is true (""or"")? This is unclear.","The sentiment of the review is mixed but leans towards the negative side. The reviewer acknowledges the originality and potential applications of the paper but highlights several significant issues that need to be addressed. Therefore, the sentiment score is -30. The politeness of the language is quite high. The reviewer provides constructive criticism and offers detailed suggestions for improvement without being rude or dismissive. Thus, the politeness score is 80.",-30,80
"This paper proposes a new framework for topic modeling, which consists of two main steps: generating bag of words for topics and then using RNN to decode a sequence text. 

Pros:
The author draws lessons from the infoGAN and designed a creative object function with reconstruction loss and categorical loss. As a result, this paper achieved impressive outcome for topic modeling tasks.

Comments:
1. High-level language is used to describe how to train two parts of the model, which is not technically clear. It would be better describe the algorithms in more details by listing steps for your algorithm in the section 3.3.

2. For text generation experiments, why didn’t you compare your model with any other related model such as SeqGAN or TextGAN? It is not so convincing to just use VAE+Wgan-gp as a baseline model.

3. For qualitative analysis part, you just listed some of your generated sentences for proving the fluency and relevance. Why didn’t you use some standard metrics for evaluating the quality of the text? I cannot judge the quality of your model through these randomly selected sentences.

4. As you mentioned in this paper “your model can be easily combined with any current text generation models”, have you done any experiments for demonstrating the original text generation model will get better performance after applying your framework? 

Minor comments:
1. On page 2 and page 4, you mentioned “the third term in (2)”. According to my understanding, this should be equation 1 instead. 
","The sentiment of the review is generally positive, as indicated by the acknowledgment of the creative object function and the impressive outcomes achieved by the proposed framework. However, the reviewer also provides several critical comments and suggestions for improvement, which slightly temper the overall positivity. Therefore, the sentiment score is 60. The politeness of the language used is high, as the reviewer uses polite phrases such as 'It would be better,' 'Why didn’t you,' and 'As you mentioned,' which indicate a respectful and constructive tone. Therefore, the politeness score is 80.",60,80
"This paper proposes GASI to disambiguate different sense identities and learn sense representations given contextual information. 
The main idea is to use scaled Gumbel softmax as the sense selection method instead of soft or hard attention, which is the novelty and contribution of this paper.
In addition, the authors proposed a new evaluation task, contextual word sense selection, which can be used to quantitatively evaluate the semantic meaningfulness of sense embeddings.
The proposed model achieves comparable performance on traditional word/sense intrinsic evaluation and word intrusion test as previous models, while it outperforms baselines on the proposed contextual word sense selection task.

While the scaled Gumbel softmax is the claimed novelty, it is more like an extension of the original MUSE model (Lee and Chen, 2017), which proposed the sense selection and representation learning modules for learning sense-level embeddings.
The only difference between the proposed one and Lee and Chen (2017) is Gumbel softmax instead of reinforcement learning between sense selection and representation learning modules.
Therefore, the idea from the proposed model is similar to Li and Jurafsky (2015), because the sense selection is not one-hot but a distribution.
The novelty of this paper is limited because the model is relatively incremental.

From my perspective, the more influential contribution is that this paper points out the importance of evaluating sense selection capability, which is ignored by most prior work.
Therefore, I expect to see more detailed evaluation on the selection module of the model. 
Also, because the task of this paper is multi-sense embeddings, the traditional word similarity (without contexts) task seems unnecessary. 
Moreover, there is no error analysis about the result on the proposed contextual word sense selection task, which may shed more light on the strength and weakness of the model. 
Finally, I suggest the authors remove the word-level similarity task and try the recently released Word in Context (WiC) dataset, which is a binary classification task that determines whether the meaning of a word is different given two contexts.
It would be better to see that GASI performs well on this task given its better sense selection module.

Overall, the contribution is somewhat incremental and the evaluation/discussion should focus more on the sense selection module. 
Considering the issues mentioned above, I will expect better quality for an ICLR paper.","The sentiment of the review is mixed but leans towards the negative side. The reviewer acknowledges the contributions of the paper but criticizes the novelty and incremental nature of the work. The sentiment score is -30 because the review points out significant limitations and suggests that the paper does not meet the expected quality for an ICLR paper. The politeness score is 50 because the language used is constructive and professional, offering specific recommendations for improvement without being harsh or rude.",-30,50
"This work proposes to train an RL-based agent to simultaneously learn Embodied Question Answering and Semantic Goal Navigation on the ViZDoom dataset. The proposed model incorporates visual attention over the input frames, and also further supervises the attention mechanism by incorporating an auxiliary task for detecting objects and attributes.

Pros:
-Paper was easy to follow and well motivated
-Design choices were extensively tested via ablation
-Results demonstrate successful transfer between SGN, EQA, and the auxiliary detection task

Cons:
-With the exception of the 2nd round of feature gating in equation (3), I fail to see how the proposed gating -> spatial attention scheme is any different from the common inner-product based spatial attention used in a large number of prior works, including  [1], [2], and [3] and many more.
-The use of attribute and object recognition as an auxiliary task for zero-shot transfer has been previously explored in [3]


Overall, while I like the results demonstrating successful inductive transfer across tasks, I did not find the ideas presented in this work to be sufficiently novel or new.

[1] Ask, Attend and Answer: Exploring Question-Guided Spatial Attention for Visual Question Answering, Huijuan Xu, Kate Saenko
[2] Drew A. Hudson, Christopher D. Manning, Compositional Attention Networks for Machine Reasoning
[3] Aligned Image-Word Representations Improve Inductive Transfer Across Vision-Language Tasks, Tanmay Gupta, Kevin Shih, Saurabh Singh, Derek Hoiem","The sentiment of the review is mixed. The reviewer acknowledges the strengths of the paper, such as its clarity, motivation, and successful results, which contributes to a positive sentiment. However, the reviewer also points out significant concerns regarding the novelty of the proposed methods, which detracts from the overall sentiment. Therefore, the sentiment score is moderately positive. The language used in the review is polite and professional, as the reviewer provides constructive feedback without being dismissive or rude. The politeness score is high.",30,80
"This paper formulates a new deep method called deep abstaining classifer. Their main idea is to introduce a new modified loss function that utilizes an absention output allowing the DNN to learn when abstention is a better option. The core idea resemble KWIK framework [1], which has been theoretical justified.

Pros:

1. The authors find a new direction for learning with noisy labels. Based on Eq. (1) (the modified loss), the propose \alpha auto-tuning algorithm, which is relatively novel. 

2. The authors perform numerical experiments to demonstrate the efficacy of their framework. And their experimental result support their previous claims.
For example, they conduct experiments on CIFAR-10 and CIFAR-100. Besides, they conduct experiments on open-world detection dataset.

Cons:

We have three questions in the following.

1. Clarity: in Section 3, the author claim real-world data is corrupted in some non-arbitrary manner. However, in practice, it is really hard to reason the corrpution procedure for agnostic noisy dataset like Clothing1M [2]. The authors are encouraged to explain this point more.

2. Related works: In deep learning with noisy labels, there are three main directions, including small-loss trick [3], estimating noise transition matrix [4,5], and explicit and implicit regularization [6]. I would appreciate if the authors can survey and compare more baselines in their paper.

3. Experiment: 
3.1 Baselines: For noisy labels, the author should compare with [7] directly, which is highly related to your work. Namely, designing new loss function can overcome the issue of noisy labels. Without this comparison, the reported result has less impact. Moreover, the authors should add MentorNet [2] as a baseline https://github.com/google/mentornet

3.2 Datasets: For datasets, I think the author should first compare their methods on symmetric and aysmmetric noisy data. Besides, the authors are encouraged to conduct 1 NLP dataset.

References:

[1] L. Li, M. Littman, and T. Walsh. Knows what it knows: a framework for self-aware learning. In ICML, 2008.

[2] T. Xiao, T. Xia, Y. Yang, C. Huang, and X. Wang. Learning from massive noisy labeled data for image classification. In CVPR, 2015.

[3] L. Jiang, Z. Zhou, T. Leung, L. Li, and L. Fei-Fei. Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels. In ICML, 2018.

[4] G. Patrini, A. Rozza, A. Menon, R. Nock, and L. Qu. Making deep neural networks robust to label noise: A loss correction approach. In CVPR, 2017.

[5] J. Goldberger and E. Ben-Reuven. Training deep neural-networks using a noise adaptation layer. In ICLR, 2017.

[6] T. Miyato, S. Maeda, M. Koyama, and S. Ishii. Virtual adversarial training: A regularization method for supervised and semi-supervised learning. ICLR, 2016.

[7] Z. Zhang and M. Sabuncu. Generalized cross entropy loss for training deep neural networks with noisy labels. In NIPS, 2018.","The sentiment of the review is generally positive, as the reviewer acknowledges the novelty and efficacy of the proposed method and its experimental validation. However, the reviewer also raises several concerns and suggestions for improvement, which slightly temper the overall positive sentiment. Therefore, the sentiment score is 50. The politeness of the language is high, as the reviewer uses phrases like 'the authors are encouraged' and 'I would appreciate,' which are polite and constructive. Thus, the politeness score is 80.",50,80
"This paper analyzes the limitation of probability density distillation with reverse KL divergence, and proposes two practical methods for probability distillation.

Detailed comments:

1) Typo: should be WaveNet, not Wavenet.

2) In Proposition 1. $c_i$ should be $\rho_i$.

3) One may explain “path derivative” with more details. Also, I am really confused by Proposition 1 and its underlying implication. Given p_s and p_t are centered at the origin, isn’t p_s(x) already the optimal if it’s just a unit Gaussian. Why do we need a derivative pointing away from the origin? At least, one need parameterize p_s as N(0, \phi)?

4) In section 3.2, “set $\mu = [2, 2]^T$”? Isn’t $\mu$ a T dimensional vector?

5) A lot of important details are missing in neural vocoder experiment. For x-reconstruction, do you use L1 or L2 loss?  For student model, do you use Gaussian IAF with WaveNet architecture as in ClariNet, or Logistic IAF as in Parallel WaveNet? Following this question, do you compute KLD in closed-form? Do you use the regularization term introduced in ClariNet? Student with KL loss and power loss outperforms x-reconstruction. Did you try x-reconstruction along with power loss?

Pros:
Certainly, there are some interesting ideas in this paper. 

Cons:
The experiment results are not good enough. The paper is poorly written. A lot of important details are missing.  

However, I would like to raise my rating to 6, if these comments can be properly addressed.
","The sentiment of the review is mixed. While the reviewer acknowledges that there are some interesting ideas in the paper, they also point out significant issues such as poor writing, missing important details, and unsatisfactory experimental results. This results in a sentiment score of -20. The politeness of the language is relatively high. The reviewer uses polite language, provides constructive feedback, and even offers to raise their rating if the comments are addressed, resulting in a politeness score of 80.",-20,80
"The paper proposes a feature smoothing technique, which generates virtual data points by interpolating the input space of two randomly sampled examples. The aim is to generate virtual training data points that are close to adversarial examples. Experimental results on both MNIST and Cifar10 datasets show that the proposed method augmented with other regularization techniques are robust to adversarial attacks and obtain higher accuracy when comparing with some testing baselines. Also, the paper presents some theoretical analyses showing that label smoothing, logit squeezing, weight decay, Mixup and feature smoothing all produce small estimated variance of the decision boundary when regularizing the networks. 

The paper is generally well written, and the experiments show promising results. Nevertheless, the proposed method is not very novel, and the method is not comprehensively evaluated with experiments.

Major remarks:

1.	The experiments show that feature smoothing has to combine with other regularizers in order to outperform other testing methods. In this sense the contribution of the feature smoothing along is not clear. For example, without integrating other regularizers, Mixup and feature smoothing obtain very close results for BlackBox-PGD, BlackBoxcw and Clean, as shown in Table 1. In addition, in the paper, the feature smoothing along is only validated on the MNIST (not even tested on Cifar10 in Table2). Consequently, it is difficult to evaluate the contribution of the proposed smoothing technique. 
2.	Experiments are conducted on datasets MNIST and Cifar10 with small number of target classes. Empirically, it would be useful to see how it performs on more complex data set such as Cifar100 or ImageNet.
3.	The argument for why the proposed feature smoothing method works is presented in Theorem4.3 in Section 4.2, but the theorem seems to rely on the assumption that one can add data around the true decision boundary. However, how we can generate samples near the true decision boundary and how we should chose the mixing ratio to attain this goal is not clear to me in the paper. In addition, how we can sure that the adding synthetic data from one class does not collide with manifolds of other classes as suggested in AdaMixup (Guo et al., MixUp as Locally Linear Out-Of-Manifold Regularization)? This is particular relevant if the proposed feature smoothing strategy prefers to create virtual samples close to the true decision boundary.
4.	At the end of page4, the authors claim that both feature smoothing and Mixup generate new data points that are closer to the true boundary. I wonder if the authors could further justify or show that either theoretically or experimentally. 
5.	The proposed method is similar to SMOTE (Chawla et al., SMOTE: Synthetic Minority Over-sampling Technique). In this sense, comparison with SMOTE would be very beneficial.

Minor remarks:

1.	In the paper Mixup, value 1 was carefully chosen as the mixing policy Alpha for Cifar10 (otherwise, underfitting can easily occur as shown in AdaMixUp), and it seems in the paper the authors used a very large value of 8 for Mixup’s Beta distribution, and I did not see the justification for that number in the paper.
2.	Typo in the second paragraph of page2: SHNV should be SVHN
","The sentiment of the review is generally positive, as indicated by phrases like 'the paper is generally well written' and 'the experiments show promising results.' However, the reviewer also points out significant limitations, such as the lack of novelty and comprehensive evaluation, which tempers the overall positivity. Therefore, the sentiment score is 30. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, such as 'I wonder if the authors could further justify' and 'it would be useful to see.' Therefore, the politeness score is 80.",30,80
"The authors propose to use k-DPP to select a set of diverse parameters and use them to search for a good a hyperparameter setting. 

This paper covers the related work nicely, with details on both closed loop and open loop methods. The rest of the paper are also clearly written. However, I have some concerns about the proposed method.
- It is not clear how to define the kernel, the feature function and the quality function for the proposed method. The choices of those seem to have a huge impact on the performance. How was those functions decided and how sensitive is the result to hyperparameters of those functions?
- If the search space is continuous, what is the mixing rate of Alg. 2? In practice, how is ""mixed"" decided? What exactly is the space and time complexity? I'm not sure where k log(N) comes from in page 7.  
- Alg. 2 is a straight forward extension of Alg. 1, just with L not explicitly computed. I think it would have more novelty if some theoretical analyses can be shown on the mixing rate and how good this optimization algorithm is. 

Other small things:
- citation format problems in, for example, Sec. 4.1. It should be \citep instead of \cite. 
- it would be good to mention Figure 2 in the text first before showing it. 

[Post rebuttal]
I would like to thank the authors for their clarifications. However, I am still concerned with the novelty. The absence of provable mixing rate is also a potential weakness. I think a clearer emphasis on the novelty, e.g. current algorithm with mixing rate analyses or more thorough empirical comparisons will make the paper stronger for resubmission.","The sentiment of the review is mixed. The reviewer acknowledges the clarity and thoroughness of the related work and the overall writing but expresses significant concerns about the proposed method's details and novelty. The sentiment score is therefore slightly negative. The politeness of the language is high, as the reviewer uses polite phrases such as 'I would like to thank the authors' and provides constructive feedback without any rude or harsh language.",-20,80
"Summary: The paper considers so-called kernel neural networks where the non-linear activation function at each neuron is replaced by a kernelized linear operation, and analyses a layer-wise training scheme to train such networks. The theoretical claims are that (i) the optimal representation at each hidden layer can be determined by getting the similarity between two kernel matrices and (ii) this procedure gives a more interpretable training procedure and can avoid the vanishing gradient problems. Some small-scale experiments are provided.

Evaluation: I have a mixed feeling about this paper: the theoretical contributions seem interesting but its interpretation and practicality are somewhat non-intuitive and philosophically troubling, in my opinion. I did not check the proofs in the appendix so I might have missed some critical info or have not fully understood the experimental set-up.

- interpretability: it's not clear to me if this training scheme is any more interpretable than backprop training (not to mention it's not clear to me how to define interpretability for neural networks). Whether BP or any layer-wise training schemes is used, isn't the goal is to get S_{l-1} to the state where S_{l-1}s for examples of different classes are far away from each other as this is easier for the classifier?
- function representation: in section 2, fj^i(x) is parameterized as a sum of kernel values evaluated at x and the training points. It's unclear to me what is x here -- input to the network or output of the previous layer? This also has a sum over all training points, so is training kMLPs in a layer-wise fashion more efficient than traditional kernel methods? 
- training scheme: what is the order of layers being trained? input to output or output to input? I'm slightly hazy on how to obtain F^{(l-1)}(S) to compute G_{l-1}. 
- the intuition of layer-wise optimality: on page 4, the paper states that ""the global min of R_l wrt S_{l-1} can be explicitly identified prior to any training"" but intuitively this must condition on some known function/function class F^(l). Could you please enlighten me on this?
- the experiments are of small-scale and, as the paper pointed out, only demonstrating the concepts. What are the main practical difficulties preventing this from being applied to bigger networks/bigger datasets?
- vanishing gradients: I'm not clear how layer-wise training can avoid this issue - could you please explain this?
- some typos: p1 emplying -> employing, p4 supress -> suppress, p5 represnetation -> representation","The sentiment of the review is mixed, as indicated by the phrase 'I have a mixed feeling about this paper.' The reviewer acknowledges the theoretical contributions as interesting but finds the interpretation and practicality non-intuitive and troubling. This suggests a sentiment score around -20, as it leans slightly negative but is not entirely dismissive. The politeness of the language is quite high, as the reviewer uses phrases like 'it's not clear to me' and 'could you please explain,' which are polite ways of expressing confusion or requesting clarification. This suggests a politeness score of 80.",-20,80
"This paper proposes a model-based object-oriented algorithm, SOORL. 
It assumes access to an object detector which returns a list of objects with their attributes, an interaction function which detects interactions between objects, and a set of high-level macro actions. Using a simplified state representation obtained through the object detector, it performs optimistic MCTS while simultaneously learning transition and reward models. The method is evaluated on two toy domains, PongPrime and miniPitfall, as well as the Atari game Pitfall. It achieves positive rewards on Pitfall, which previous methods have not been able to do. 

Despite good experimental results on a notoriously hard Atari game, I believe this work has limited significance due to the high amount of prior knowledge/engineering it requires (the authors note that this is why they only evaluate on one Atari game). I think this would make a good workshop paper, but it's not clear that the contributions are fundamental or generally applicable to other domains. Also, the paper is difficult to follow (see below). 

Pros:
- good performance on a difficult Atari game requiring exploration
- sample efficient method

Cons:
- paper is hard to follow
- approach is evaluated on few environments
- heavily engineered approach
- unclear whether gains are due to algorithm or prior knowledge


Specific Comments:

- Section 3 is hard to follow. The authors say that they are proposing a new optimistic MCTS algorithm to support deep exploration guided by models, but this algorithm is not described or written down explicitly anywhere. Is this the same as Algorithm 3 from Section 5? They say that at each step and optimistic reward bonus is given, but it's unclear which bonus this is (they mention several possibilities) or how it relates to standard MCTS.
In Section 3.1, it is unclear what the representation of the environment is. I'm guessing it is not pixels, but it is discrete states? A set of features? 
The authors say ""we provided the right model class for both experiments"" - what is this model class? 

- Concerning the general organization of the paper, it would be clearer to first present the algorithm (i.e. Section 5), go over the different components (model learning, learning macro actions, and planning), and then group all the experiments together in the same section. 
The first set of experiments in Sections 3.1 and 3.2 can be presented within the experiments section as ablations.  

- Although the performance on Pitfall is good, it's unclear how much gains are due to the algorithm and how much are due to the extra prior knowledge. It would be helpful to include comparisons with other methods which have access to the same prior knowledge, for example with DQN/A3C and  pseudo-count exploration bonuses using the same feature set and macro actions as SOORL uses. 


Minor:
- Page 2: ""Since the model...the new model estimates"": should this be part of the previous sentence?
- Page 5: ""There are reasonable evidence"" -> ""There is reasonable evidence""
- Page 5: "". we define a set of..."" -> "". We define a set of...""
- Page 8: ""any function approximation methods"" -> ""method""","The sentiment of the review is mixed. While the reviewer acknowledges the good performance of the proposed method on a difficult Atari game and its sample efficiency, they also express significant concerns about the paper's clarity, the limited evaluation environments, and the heavy reliance on prior knowledge. This results in a sentiment score of -20. The language used in the review is generally polite, with constructive feedback and specific suggestions for improvement, leading to a politeness score of 80.",-20,80
"This paper studies a very simple and intuitive method to boost the training speed of deep neural networks. The authors first train some light weighted proxy models, using these models to rank the data according to its uncertainty, and then pick the most uncertain subset to train the final model. Experiments on CIFAR10/SVHN/Amazon Review Polarity demonstrates the effectiveness.

In general, I think the authors did a decent job in showing that such a simple idea could surprisingly work well to boost NN training. I believe it will inspire future works on speeding up NN training. However, to form a solid ICLR publication, plenty of future works need to be done.

1)	I will not be fully convinced if an idea aiming to speed up, is only verified on small scale dataset (e.g., CIFAR10). It will be much better if there are large scale experiments conducted such as on ImageNet and WMT neural machine translation. 

2)	Please well position some related works. First, it would be more interesting and informative if some baselines in section 2 (especially those in “Optimization and Importance Sampling’), are compared with. Second, there are important related works omitted such as L2T [1], which also talks/shows the possibly of using partial training data to achieve speed up.

3)	Some writing issues: it would be better to *clearly* demonstrate the final accuracy of different models (i.e. ResNet 164 trained on whole data and selected subset), such as putting them into a table, but not merely showing them vaguely in the curves and text. I’m also note sure about the meaning of `epoch’ in Table 1: does it mean how many epochs the proxy model is trained? If so, I can hardly get the intuition of why smaller epochs works better. I noted a conjecture raised by the authors in the last sentence of paragraph “comparing different proxies”. However, I cannot catch the exact meaning. 

[1] Fan, Y., Tian, F., Qin, T., Li, X. Y., & Liu, T. Y. Learning to Teach. ICLR 2018
","The sentiment of the review is generally positive, as the reviewer acknowledges the effectiveness of the proposed method and its potential to inspire future work. However, the reviewer also points out several areas for improvement, which slightly tempers the overall positivity. Therefore, the sentiment score is 50. The language used in the review is polite and constructive, with suggestions framed in a helpful manner rather than as harsh criticisms. Thus, the politeness score is 80.",50,80
"
Summary: This paper tries to tackle the option discovery problem, by building on recent work on successor representation and eigenoptions. Although this is an extreme important problem, I feel the paper fails to deliver on its promise. The authors propose a way of clustering states via their SR/SF representation and they argue that this would lead to the discovery of subgoals that are fundamentally different from the popular choices in literature, like bottleneck states. They argue that this discovery procedure would lead to states “better for exploration”, “provide greater accessibility to a larger number of states”. Both of which sound promising, but I felt the actual evaluation fails to show or even assess either of these rigorously. Overall, after going through the paper, it is not clear what are the properties of these discovered subgoal states and why they would be better for exploration and/or control.

Clarity: Can be improved significantly! It requires several reads to get some of the important details. See detailed comments.

Originality and Significance: Very limited, at least in this version. The quantitative, and in some cases qualitative, evaluation lacks considerably. The comparison with the, probably most related, method (eigenoption) yield some slight improvement. But unfortunately, I was not conceived that this grain of empirical evidence would transfer to other scenarios. I can’t see why that would that be the case, or in which scenarios this might happen. At least those insights seem to be missing from the discussion of the results. 


Detailed comments and questions:

1) Section 3.1: Latent Learning. There are a couple of design choices here that should have been more well explained or motivated:
i) The SR were built under the uniformly random policy. This is a design choice that might work well for gridworld/navigation type of domains but there are MDP where the evaluation under this particular policy can yield uninformative evaluations. Nevertheless this is an interesting choice that I think deserved more discussion, especially the connection to previous work on proto-value functions and eigenoptions. For instance, if both of these representations -- eigenoptions and the proposed successor option model -- aim to represent the SR under the uniform policy, why does know do (slightly) better than the other? Moreover, how would these compare under a different policy (non-uniform). 
ii) The choice of reward. The notation is a bit confusing here, as it’s somewhat inconsistent with the definitions (2-4). Also, more generally, It is not clear throughout if we are using the discounted or undiscounted version of SR/SFs -- (2-3) introduce the discounted version, (4) seems to be undiscounted. Not clear if (5) refers to the discounted or undiscounted version. Nevertheless, I am guessing this was meant as a shaping reward, thus \gamma=1 for (5). But if that’s the case, according to eq. (2), most of the time I would expect \psi_s(s_{t+1}) and \psi_s(s_{t}) to have the same value. Could you explain why that is not true (at least in your examples)?
iii) Termination set: Q(s,a)<=0. This again seems a bit of an arbitrary choice and it’s not clear which reward this value function takes into account. 

2) Figure 2: The first 2 figures representing the SRs for the two room domain: the values for one of the rooms seems to be zero, although one would expect a smoother transition around the ‘doorway’, otherwise the shaping won’t point in the right direction for progression. Again, this might suggest that more informative(control) policy might give you more signal.  

3) Section 3.2: ‘The policy used for learning the SR is augmented with the previously learnt options‘. Can you be more precise about how this is done? Which options used? How many of them? And in which way are they used? This seems like a very important detail. Also is this augmented policy used only for exploration? 

4) SRmin < \sum_{s’} ψ(s, :) < SRmax. Is this meant to be an expectation over all reachable next states or all states in the environment? How is this determined or translated in a non-tabular setting. Not sure why this is a proxy to how ‘developed’ this learning problem or approximation is. Can you please expand on your intuition here?

5) Section 3.3. The reward definition seems to represent how much the progress between \phi(s_t+1) - \phi(s) aligns with the direction of the goal. This is very reminest of FuN [2] -- probably a connect worth mentioning and exploring.

6) Figure 4: Can you explain what rho is? It seems to be an intermediate representation for shared representation \phi. Where is this used?

7) Experiments:
“a uniformly random policy among the options and actions (typically used in exploration) will result in the agent spending a large fraction of it’s time near these sub-goals”. Surely this is closely linked to the termination condition of the option and the option policy. How is this assessed?

“in order to effectively explore the environment using the exploration policy, it is important to sample actions and options non-uniformly”. It would be good to include such a comparison, or give a reason why this is the case. It’s also not clear how many of the options we are considering in this policy and how extensive their horizons will be. This comes back to the termination condition in Section 3.1 which could use an interpretation. 

“In all our experiments, we fix the ratio of sampling an option to an action as 1:19.” This seems to be somewhat contradictory to the assumption that primitive actions are not enough to explore effectively this environment. 

Figure 8. I think this experiment could use some a lot more details. Also it would be good to guide the reader through the t-SNE plot in Figure 8a. What’s the observed pattern? How does this compare to the eigenoption counterpart.

8) General comment on the experiments: There seems to be several stages in learning, with non-trivial dependencies. I think the exposition would improve a lot if you were more explicit about these: for instance, if the representation continually refined throughout the process; when the new cluster centers are inferred are the option policies learnt from scratch? Or do they build on the previous ones? Does this procedure converge -- aka do the clusters stabilize?

9) Quantitative performance evaluation was done only for the gridworld scenarios and felt somewhat weak. The proposed tasks (navigation to a goal location) is exactly what SFs are trained to approximate. No composition of (sub)tasks, nor tradeoff-s of goals were studied [1,3] -- although they seem natural scenario of option planning and have been studied in previous SFs work. Moreover, if the SFs are built properly, in these gridworlds acting greedily with respect to the SFs (under the uniformly random policy) should be enough to get you to the goal. Also, probably this should be a baseline to begin with.

References:
[1] Andre Barreto, Will Dabney, Remi Munos, Jonathan J Hunt, Tom Schaul, Hado P van Hasselt, and ´ David Silver. Successor features for transfer in reinforcement learning. In Advances in Neural Information Processing Systems, pp. 4055–4065, 2017.

[2] Vezhnevets, A.S., Osindero, S., Schaul, T., Heess, N., Jaderberg, M., Silver, D. and Kavukcuoglu, K., 2017, July. FeUdal Networks for Hierarchical Reinforcement Learning. In International Conference on Machine Learning (pp. 3540-3549).

[3] Barreto, A., Borsa, D., Quan, J., Schaul, T., Silver, D., Hessel, M., Mankowitz, D., Zidek, A. and Munos, R., 2018, July. Transfer in deep reinforcement learning using successor features and generalised policy improvement. In International Conference on Machine Learning (pp. 510-519).
","The sentiment of the review is generally negative, as the reviewer expresses disappointment with the paper's failure to deliver on its promises and highlights several areas where the paper falls short. The reviewer points out issues with clarity, originality, and significance, and provides detailed comments on specific sections that need improvement. The politeness of the language is relatively high, as the reviewer uses polite phrases such as 'I feel,' 'I would expect,' and 'Could you explain,' and provides constructive feedback without being rude or dismissive.",-60,70
"Summary

The authors consider RL with safety constraints, which is framed as a multi-reward problem. At a high-level, this involves finding the Pareto front, which optimally trades off objectives. The paper primarily introduces and discusses a discretization scheme and methods to model the Q-value function as a NIPWC (non-increasing piecewise constant function). NIPWC are stored as values over discrete partitions of state-action spaces. To do so, the authors introduce two data structures DecRect and ContDecRect to store Q function values over geometric combinations of subsets of state-action space.

The authors discuss how to execute elementary operations on these data structures, such as computing max(f(x), g(x)), weighted sums, etc. The goal is to use these operations to compute Bellman-type updates to compute optimal value/policy functions for multi-reward problems. The authors also present complexity analysis for these operations. 

Pro
- Extensive discussion and analysis of discrete representations of Q-functions as NIPWCs. 

Con
- A major issue with this work is that it is very densely written and spends a lot of time on developing the discretization framework and operations on NIPWC. However: 
- There is no clear practical algorithm to solve (simple) multi-reward RL problems with the authors' approach.
- No experiments to demonstrate a simple implementation of these techniques.
- Even though multi-reward settings are the stated problem of interest, authors don't discuss Pareto front computations in much detail, e.g., section 4.3 computing non-dominated actions is too short to be useful.
- The discussion around complexity upper bounds is too dense and uninsightful. For instance, the bounds in section 5 all concern bounds on the Q-value as a function of the action, which results in upper bounds as a function of |A|. But in practice, the action is space is often small, but the state space is high-dimensional. Hence, these considerations seem less relevant. 

Overall, this work seems to present an interesting computational scheme, but it is hard to see how this is a scalable alternative. Practical demonstrations would benefit this work significantly.

Reproducibility
N/A 
","The sentiment of the review is mixed but leans towards the negative side. The reviewer acknowledges the extensive discussion and analysis of discrete representations of Q-functions as a positive aspect. However, the review highlights several significant issues, such as the dense writing, lack of practical algorithms, absence of experiments, insufficient discussion on Pareto front computations, and uninsightful complexity analysis. These criticisms outweigh the single positive point, leading to a sentiment score of -40. The language used in the review is polite and constructive, as the reviewer provides specific feedback and suggestions without using harsh or rude language, resulting in a politeness score of 80.",-40,80
"This paper provides a new method that approximates the confidence distribution of classification probability, which is useful for novelty detection. The variational inference with Dirichlet family is a natural choice.

Though it is principally insightful to introduce the “higher-order” uncertainty, I do see the fundamental difference from the previous research on out-of-distribution detection (Liang, Lee, etc.). They are aimed at the same level of uncertainty.  Consider a binary classier, the only possible distribution of output y is Bernoulli- a mixture of Bernoulli is still Bernoulli.   

In ODIN paper,  the detector contains both the measurement of the extent to which the largest unnormalized output of the neural network deviates from the remaining outputs (U1 in their notation) and another measurement of the extent to which the remaining smaller outputs deviate from each other (U2 in their notation).  In this paper, the entropy term has the same flavor as U2 part?

I am a little bit concerned with the VI approach, which introduces extra uncertainty.  I do not understand why there is another balancing factor eta in equation 6, which makes it no longer a valid elbo. Is the ultimate goal to estimate the exact posterior distribution of p(z) through VI, or purely some weak regularization that enforces uniformity?  Could you take advantage of some recent development on VI diagnostics and quantify how good the variational approximation is?

In general, the paper is clear and well-motivated, but I find the notation sometimes confusing and inconsistent. For example, the dependency on x and D is included somewhere but depressed somewhere else.  alpha_0 appears in equation 4 but it is defined in equation 7. 

I am impressed by the experiment result that the new method almost always dominates best known methods, previously published in ICLR 2018. But I am not fully convinced why it works theoretically.  I would recommend a weak accept.  
","The sentiment of the review is generally positive, as the reviewer acknowledges the novelty and usefulness of the method, and is impressed by the experimental results. However, there are some concerns and suggestions for improvement, which slightly temper the overall positivity. Therefore, the sentiment score is 50. The language used in the review is polite and constructive, with phrases like 'I am a little bit concerned' and 'I would recommend a weak accept,' indicating a respectful tone. Thus, the politeness score is 80.",50,80
"Summary:  This paper studies deep multi-task learning. Prior papers have studied various knowledge sharing approaches for deep multi-task learning including hard and soft sharing schemes. And some soft sharing schemes have used tensor decompositions (including TT, and Tucker). This paper fuses this line of work with the recently proposed Tensor-Ring decomposition in order to obtain Tensor Ring (TR)-based soft sharing for multi-task learning. The results show some improvement over prior deep MTL methods based on other tensor factorisation methods.

Strengths:
+ Nice extension of existing line of work tensor-factorisation based MTL.
+ More flexibility for controlling shared/unshared portions of weights compared to DMTRL. 
+ Improves on previous methods results.
+ Experiments evaluate how MTL methods relate with various amounts of training data on each task.

Weaknesses:
- Novelty/significance is limited. 
- Writing. Many things are not clearly and intuitively explained. Some claims are not adequately justified. 
- Introduces more hyper parameters to tune.
- Results may rely on hyper parameter tuning. 

Comments:
1. Novelty.  Existing studies already established the template of different tensor factorisation methods (TT, Tucker) being possible to plug into deep networks for different kinds of soft-sharing MTL. Meanwhile, TR decomposition is taken off the shelf; (and as it’s been applied for compression before, this is not the first time TR decomposition has been used in a CNN context either). Therefore this is an A+B paper and a high bar should be met for the additional analysis, insight, or performance improvements that should provided.
2. Lots of writing issues:
2.1 Many things are not explained transparently enough at best (or major over-claim at worst). For example: 
2.1.1 Paper claims the benefit that each task can have its own I/O dimensionality. However if TR-decomp is “circularly connected” TT-decomp (Fig 1), then this seems not to happen automatically. So it should be unpacked more clearly how this is achieved. 
2.1.2 Paper claims favourable ability to use more private cores than TT, where only one core is private. However circular TT would also seem to have one private core by default (the core with a task axis). So I suspect something else is going on, but this is completely unclear and should be explained more transparently. Furthermore it should be justified if whatever modifications do enable these properties are definitely a unique property of TR-decomp, or could also be applied to TT-decomp. 
2.1.3 Statement “TRMTL generalizes to allow the layer-wise weight to be represented by a relatively lager number of latent cores” unclear: generalises what? larger number of cores than what? Than TT? The previous presentation suggests TT and TR should have same number of cores.  
2.1.4 Statements like “TR enjoys the property of circular dimensional permutation invariance” are made without any explanation about what is the implication of this for neural networks and multi-task learning.  
2.2 Many claims are inaccurate or not adequately backed up by theory or experiment. EG: (i) Paper claims to include DMTRL as a special case. But it only subsumes DMTRL-TT, not DMTRL-Tucker. Because TR-decomp does not include Tucker-decomp as an exact special case.  (ii) Sentences “TR-ranks are usually smaller than TT-ranks” are assertions without verification. 
2.3 Sentences are taken verbatim from other papers, plagiarism. For example: “TR model is more flexible than TT, because TR-ranks can be equally distributed in the cores, but TT-ranks have a relatively fixed pattern”  is verbatim from Zhao’16 TR-decomp paper. 
3. Hyperparameters: This paper apparently gains some practical benefit due to the notion of shared/unshared cores. However, this also introduces  additional hyper parameters (E.g., each layers private proportion “c”) to tune besides the ranks. Unlike the rank that can be pre-estimated by reconstruction error, this one seems to require tuning by cross-validation. This is not scalable. 
4. Hyperparameters+Tuning: Hyperparameters Private proportion, “sharing pattern”, IO dimension seem to be tuned by accuracy.( “We test different sharing patterns and report the ones with the best accuracies”). This is even less scalable, and additional tuning makes it unsurprising it surpasses other models performance.
5. Insight & Analysis. All the core selection & public/private core selection are treated as black box optimisation. No insight is given about what turns out to be useful to share or not, and how consistent this is, etc.
","The sentiment of the review is mixed, leaning slightly towards the negative side. While the reviewer acknowledges some strengths of the paper, such as the extension of existing work and improvements over previous methods, the weaknesses and comments sections are more extensive and critical. The reviewer points out significant issues with novelty, writing clarity, hyperparameter tuning, and lack of insight and analysis. Therefore, the sentiment score is slightly negative. The politeness of the language is generally maintained, with the reviewer using phrases like 'should be explained more transparently' and 'should be justified,' which indicate a polite tone despite the critical feedback.",-20,60
"This paper proposed to improve the system resource efficiency for super resolution networks. 

First, I am afraid all the techniques considered in this paper have been investigated in previous works. Thus no new idea is proposed in this work. Also, it is also not clear why these improvement is particularly suitable for the task of super resolution. In my viewpoint, these techniques actually can be used to improve a variety of network architectures in both high-level and low-level vision tasks.

Second, the experimental results are also weak. As this work is aiming to address the super resolution tasks, at least visual comparisons between the proposed methods and other state-of-the-art approaches should be included in the experimental part. But unfortunately, no such qualitative results are presented in the manuscript. 

Finally, the presentation of the paper should also be carefully proofread and revised.
","The sentiment of the review is quite negative, as the reviewer points out that the techniques have been previously investigated, the experimental results are weak, and the paper needs proofreading. Therefore, the sentiment score is -80. The politeness of the language is relatively neutral to slightly polite, as the reviewer uses phrases like 'I am afraid' and 'should also be carefully proofread and revised,' which soften the critique. Thus, the politeness score is 20.",-80,20
"This paper proposed a combination of graph neural networks and conditional random field to model the correlation between node labels in the output.  In typical graph neural nets the predictions for nodes are conditionally independent given the node representations.  This paper proposes to use a CRF to compensate for that.  In terms of the approach, the authors used GCNs to produce unary potentials for the CRF, and have the pairwise potentials on each edge to model the correlation of labels of neighboring nodes.  Learning is done by optimizing pseudo-likelihood and the energy loss, while inference is performed through a couple heuristic processes.

Combining neural nets with CRFs is not a new idea, in particular this has been tried before on image and sequence CRFs.  It is therefore not surprising to see an attempt to also try it for graph predictions.  The main argument for using a CRF is its ability to model the correlations of output labels which was typically treated as independent.  However this is not the case for deep neural networks, as it already fuses information from all over the input, and therefore for most prediction problems it is fine to be conditionally independent for the output, as the dependence is already modeled in the representations.  This is true for graph neural networks as well, if we have a deep graph neural net, then the GNN itself will take care of most of the dependencies between nodes and produce node representations that are suitable for conditionally independent output predictions.  Therefore I’m not convinced that CRFs are really necessary for solving the prediction tasks tried in this paper.

The learning and inference algorithms proposed in this paper are also not very convincing.  CRFs has been studied for a long time, and there are many mature algorithms for learning them.  We could do proper maximum conditional likelihood learning, and use belief propagation to estimate the marginals to compute the gradients.  Zheng et al. (2015) did this for convnets, we could also do this for graph CRFs as belief propagation can be easily converted into message passing steps in the graph neural network.  Pseudo-likelihood training makes some sense, but energy loss minimization doesn’t really make sense and has serious known issues.

On the other hand, the proposed inference algorithms does not have good justifications.  Why not use something standard, like belief propagation for inference again?  Our community has studied graphical models a lot in the last decade and we have better algorithms than the ones proposed in this paper.

Lastly, the experiments are done on some standard but small benchmarks, and my personal experience with these datasets are that it is very easy to overfit, and most of the effort will be put in to prevent overfitting.  Therefore more powerful models typically cannot be separated from overly simple models.  I personally don’t care a lot about the results reported on these datasets.  Besides, there are a lot of questions about the proposed model, but all we get from the experiment section are a few numbers on the benchmarks.  I expect studies about this model from more angles.  One more minor thing about the experiment results: the numbers for GraphSAGE are definitely wrong.

Overall I think this paper tackles a potentially interesting problem, but it isn’t yet enough to be published at ICLR due to its problems mentioned above.","The sentiment of the review is moderately negative. The reviewer acknowledges that the paper tackles a potentially interesting problem but expresses significant doubts about the necessity and effectiveness of the proposed approach. The reviewer also questions the learning and inference algorithms, the choice of benchmarks, and the experimental results. The politeness of the language is relatively high; the reviewer provides constructive criticism and avoids derogatory language, even while pointing out several flaws in the paper.",-60,70
"REVISION: thanks for the clarification. I have slightly increased my rating (to 4).

This paper tackles a very interesting subject but lacks sufficient clarity of presentation to allow me to do a proper review.

First, there are many sentences which are not well-formed or are ambiguous (in pretty much all the sections). Then there are terms which are introduced without being first clearly explained or defined. Finally, there are issues with the mathematical clarity as well, with many notations which are used without being explained or defined. Sometimes one can figure out the missing information later (e.g., fig 1 talks about mutual information objectives without stating if we want to maximize or minimize it, but later in the text we figure that out) but it makes reading very difficult.

What is a 'transformed one' (on page 2)
What is a 'geometric intrinsic reward'?
Where are the intrinsic rewards defined?
What is a 'non-parametric classifier'? A neural net? an kernel SVM?

There are also some mathematical problems:
- if f (page 3) has a discrete output, then it will probably lose information, so it cannot be inverted (contrary to the stated assumption that f(x)!=f(y) for x!=y)
- what are the differences between the different Q functions being defined? do the correspond to different action spaces? What is Q_task? What is pi_meta?
- in eqn 2, I do not think that the log q_c term maximizes the mutual information between actions and (G(t),G(t+1)), i.e. it would be missing an entropy term
- what is Z_c in eqn 2?

","The sentiment of the review is slightly positive, as indicated by the reviewer's statement that they have increased their rating and find the subject interesting. However, the review is predominantly critical, pointing out numerous issues with clarity and mathematical rigor. Therefore, the sentiment score is 10. The politeness of the language is relatively neutral to slightly polite. The reviewer uses formal language and avoids harsh criticism, but the feedback is direct and points out many flaws. Therefore, the politeness score is 20.",10,20
"This paper studies extrapolation error in off-policy batch reinforcement learning (RL), where the extrapolation error refers to the overestimation of the value for the state-action pairs that are not in the training data.

The authors propose batch-constrained RL, where the policy is optimize under the constraint that, at each state, only those actions that have been taken in that state in the training data are allowed.  This is then extended to continuous space, where it allows only the state-action pairs that are close to a state-action pair in the training data.  When there is no such action for a given state, the action that is closet to a feasible action at that state is selected.

It makes intuitive sense that the proposed approach works well as long as we only encounter state-action pairs that are closed to one of the state-action pairs in the batch.  However, I do not expect that this is always the case.  The proposed method is to simply choose the closest action in the batch.  Then why does the proposed approach perform well?  Is it because the experiments are performed under rather deterministic settings?  How often are no state-action pairs found in the neighbor?  Is there any mechanism for recovering from ""not in the batch""?

The paper would be much stronger if it study this challenge of ""not in the batch"" more in depth.  Technical contributions in the present paper are rather limited.

A key assumption in the discrete case is that whole episodes are in the batch.  This is rather restricting, because in many applications, it is infeasible to collect a whole episode, and parts of many episodes are collected from many agents.  Although this assumption is stated, it would be nice to emphasize by also stating that the theorems do not hold when this assumption does not hold.  The assumption becomes less important for continuous case, because of approximation.  It might be interesting to study the performance of the proposed approach when the assumption does not hold in the continuous case.
","The sentiment of the review is mixed to slightly negative. The reviewer acknowledges the intuitive sense of the proposed approach but raises significant concerns about its limitations and the depth of the technical contributions. The sentiment score is therefore -20. The politeness of the language is generally respectful and constructive, with suggestions for improvement and no use of derogatory language. The politeness score is 80.",-20,80
"This paper proposes a hierarchical variational autoencoder for modeling paragraphs. The model creates two levels of latent variables; one level of sentence-level latent variables and another single global latent. This avoids posterior collapse issues and the authors show convincing results on a few different applications to two datasets.

Overall, it is an impressive result to be able to convincingly model paragraphs with a useful global latent variable. Apart from some issues with confusing/incomplete notation (see below), my main criticism is that the authors fail to compare their approach to ""A Hierarchical Latent Structure for Variational Conversation Modeling"" by Park et al. As far as I can tell, the approaches are extremely similar, except that Park et al. may not learn the prior parameters and also use a hierarchical RNN encoder rather than a CNN (which may be irrelevant). They also are primarily interested in dialog generation, so the lower-level of their hierarchy models utterances in a conversation rather than sentences in general, but I don't see this as a major difference. I'd encourage the authors to compare to this and potentially use it as a baseline. More generally, it would have been nice to see more ablation experiments (e.g. convolutional vs. LSTM encoder). Finally, I know that space is tight, but other papers on global-latent-variable models tend to include more demonstrations that teh global variable is capturing meaningful information, e.g. with attribute vector arithmetic. The authors could include results of manipulating review sentiment via attribute vector arithmetic, for example.


Specific comments:

- ""The Kullback-Leibler (KL) divergence term ... which can be written in closed-form (Kingma & Welling, 2013), encourages the approximate posterior distribution qφ(z|x) to be close to the multivariate Gaussian prior p(z)."" The prior is not always taken to be a multivariate Gaussian. You should add a sentence stating that the VAE prior is often taken to be a diagonal-covariance Gaussian for convenience.
- 3.2 has a few things which are unclear. In the second paragraph, you define z as the sampled latent code which is fed through an MLP ""to obtain the starting state of the sentence-level LSTM decoder"". But then LSTM^{sent} appears to be fed z at every timestep. LSTM^{sent} is also not defined - am I to assume that its arguments are the previous state and current input, so that z is the input at every timestep? Also, you write ""where h^s_0 is a vector of zeros"" which makes it sound like the starting state of the sentence-level LSTM decoder is a vector of zeros, not the output of the MLP which takes z as input. In contrast, LSTM^{word} takes three arguments as input. Which are the ""state"" and which are the ""input"" to the LSTM?
- I don't see any description of your CNN encoder (only the LSTM decoder in section 3.2, 3.3 only covers the hierarchy of latent variables, not the CNN architecture). What is its structure? Figure 1 shows a CNN encoder generating lower-level sentence embeddings and a high-level global embedding. How are those computed? It is briefly mentioned in 4.1 under ""Datasets"" but this seems insufficient.
- p_\theta(x | z) is defined as the generating distribution, but also as a joint distribution of z_1 and z_2. Unless I am missing something I think you are overloading the notation for p_\theta.
- I don't think enough information is given about the AAE and ARAE baselines. Are they the same as the flat-VAE, except with the KL term replaced by the an adversarial divergence between the prior and approximate posterior?","The sentiment of the review is generally positive, as the reviewer describes the results as 'impressive' and acknowledges the convincing modeling of paragraphs. However, the review also includes some criticisms and suggestions for improvement, which slightly temper the overall positivity. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, such as 'I'd encourage the authors' and 'it would have been nice to see.' The reviewer also provides specific and detailed feedback without being harsh or dismissive. Therefore, the politeness score is 80.",60,80
"This paper presents a novel deep learning module for recurrent processes. The general idea and motivation are generally appealing but the experimental validation is a mess. Architectures and hyper-parameters are casually changed from experiment to experiment (please refer to Do CIFAR-10 Classifiers Generalize to CIFAR-10? By Recht et al 2018 to understand why this is a serious problem.) Some key evaluations are missing (see below). Key controls are also lacking. This study is not the first to look into recurrent / feedback processes. Indeed some (but not all) prior work is cited in the introduction. Some of these should be used as baselines as opposed to just feedforward networks. TBut with all that said, even addressing these concerns would not be sufficient for this paper to pass threshold since overall the improvements are relatively modest (e.g., see Fig. 4 right panel where the improvements are a fraction of a % or left panel with a couple % fooling rates improvements) for a module that adds significant computational cost to an architecture runtime. As a side note, I would advise to tone down some of the claims such as ""our network could outperform baseline feedforward networks by a large margin”...

****
Additional comments:

The experiments are all over the place. What is the SOA on CIFAR-10 and CIFAR-100? If different from VGG please provide a strong rationale for testing the circuit on VGG and not SOA. In general, the experimental validation would be much stronger if consistent improvements were shown across architectures.

Accuracy is reported for CIFAR-10 and CIFAR-100 for 1 and 2 feedback iterations and presumably with the architecture shown in Fig. 1. Then robustness to noise and adversarial attacks tested on ImageNet and with a modification of the architecture. According to the caption of Fig. 4, this is done with 5 timesteps this time! Accuracy on ImageNet needs to be reported ** especially ** if classification accuracy is not improved (as I expect). 

Then experiments on fine-grained with ResNet-34! What architecture is this? Is this yet another number of loops and feedback iterations? When reporting that ""Our model can get a top-1 error of 25.1, while that of the ResNet-34 model is 26.5.” Please provide published accuracy for the baseline algorithm.

For the experiment on occlusions, the authors report using “a multi-recurrent model which is similar to the model mentioned in the Imagenet task”. Sorry but this is not good enough.

Table 4 has literally no explanation. What is FF? What are unroll times?

As a side note, VGG-GAP does not seem to be defined anywhere.

When stating ""We investigated VGG16 (Simonyan & Zisserman, 2014), a standard CNN that closely approximate the ventral visual hierarchical stream, and its recurrent variants for comparison.”, the authors probably meant “coarsely” not “closely"".","The sentiment of the review is predominantly negative, as the reviewer points out several critical flaws in the experimental validation, lack of key evaluations and controls, and modest improvements that do not justify the added computational cost. The reviewer also suggests that even addressing these concerns would not be sufficient for the paper to pass the threshold. The politeness of the language is relatively neutral to slightly negative, as the reviewer uses phrases like 'a mess' and 'all over the place,' which can be perceived as somewhat harsh. However, the reviewer does provide constructive feedback and specific recommendations, which adds a degree of politeness.",-70,-20
"The authors propose a review-style overview of memory systems within neural networks, from simple RNNs to stack-based memory architectures and NTM / MemNet-style architectures. They propose some reductions to imply how one model can be used (or modify) to simulate another. They then make predictions about which type of models should be best on different types of tasks.

Unfortunately I did not find the paper particularly well written and the taxonomy was not illuminating for me. I actually felt, in the endeavor of creating a simple taxonomy the authors have created confusing simplifications, e.g.

""LSTM: state memory and memory of a single external event""

to me is mis-leading as we know an LSTM can compress many external events into its hidden units. Furthermore the taxonomy did not provide me with any new insights or display a prediction that was actually clairvoyant. I.e. it was clear from the outset that a memory network (say) will be much better at bAbI than a stack-augmented neural network. It would be more interesting to me, for example, if the paper could thus formalize why NTMs & DNCs (say) do not outperform LSTMs at language modeling, for example. I found the reductions somewhat shady, e.g. the RAM simulation of a stack is possible, however the model could only learn the proposed reduction if the number of write heads was equal to the number of memory slots --- or unless it had O(N) thinking steps per time step, where N is the number of memory slots, so it's not a very realistic reduction. You would never see a memory network, for example, simulating a stack due to the fixed write-one-slot-per-timestep interface. 

Nit: I'm not sure the authors should be saying they 'developed' four synthetic tasks, when many of these tasks have previously been proposed and published (counting, copy, reverse copy). ","The sentiment score is determined by the overall negative tone of the review. The reviewer expresses dissatisfaction with the paper, stating it is not well written, the taxonomy is confusing, and the reductions are unrealistic. Specific phrases like 'Unfortunately I did not find the paper particularly well written' and 'I found the reductions somewhat shady' contribute to a negative sentiment. Therefore, the sentiment score is -70. The politeness score is assessed based on the language used. Despite the negative feedback, the reviewer maintains a professional tone and avoids personal attacks or overly harsh language. Phrases like 'It would be more interesting to me' and 'I'm not sure the authors should be saying' indicate a polite approach. Thus, the politeness score is 50.",-70,50
"I raised my rating. After the rebuttal.

- the authors address most of my concerns.
- it's better to show time v.s. testing accuracy as well. the per-epoch time for each method is different.
- anyway, the theory part acts still more like a decoration. as the author mentioned, the assumption is not realistic.

-------------------------------------------------------------
This paper presents a method to update hyper-parameters (e.g. learning rate) before updating of model parameters. The idea is simple but intuitive. I am conservative about my rating now, I will consider raising it after the rebuttal.

1. The focus of this paper is the hyper-parameter, please focus and explain more on the usage with hyper-parameters. 
- no need to write so much in section 2.1, the surrogate is simple and common in optimization for parameters. After all, newton method and natural gradients method are not used in experiments.
- in section 2.2, please explain more how gradients w.r.t hyper-parameters are computed. 

2. No need to write so much decorated bounds in section 3. The convergence analysis is on Z, not on parameters x and hyper-parameters theta. So, bounds here can not be used to explain empirical observations in Section 5. 

3. Could authors explain the time complexity of inner loop in Algorithm 1? Does it take more time than that of updating model parameters?

4. Authors have done a good comparison in the context of deep nets.  However,
- could the authors compare with changing step-size? In most of experiments, the baseline methods, i.e. RMSProp are used with fixed rates. Is it better to decay learning rates for toy data sets? It is known that SGD with fixed step-size can not find the optimal for convex (perhaps, also simple) problems. 
- how to tune lambda? it is an important hyper-parameter, but it is set without a good principle, e.g., ""For SGD-APO, we used lambda = 0.001, while for SGDm-APO, we used lambda = 0.01"", ""while for RMSprop-APO, the best lambda was 0.0001"". What are reasons for these?
- In Section 5.2, it is said lambda is tuned by grid-search. Tuning a good lambda v.s. tuning a good step-size, which one costs more?
","The sentiment of the review appears to be cautiously positive. The reviewer acknowledges that the authors have addressed most of their concerns and mentions raising their rating after the rebuttal, indicating a positive shift in their perception. However, the reviewer still has reservations, particularly about the theoretical part and the assumptions made. Therefore, the sentiment score is 20. The language used in the review is generally polite and constructive. The reviewer provides specific recommendations and avoids harsh or rude language, even when pointing out areas that need improvement. Therefore, the politeness score is 80.",20,80
"This paper proposes a new approach to enforcing disentanglement in VAEs using a term that penalizes the synergistic mutual information between the latent variables, encouraging representations where any given piece of information about a datapoint can be garnered from a single latent.  In other words, representations where there is no information conveyed by combinations of latents that is not conveyed by considering each latent in isolation.  As the resultant target is intractable to evaluate, a number of approximations are employed for practical training.

The high-level idea is quite interesting, but the paper itself is quite a long way of convincing me that this is actually a good approach.  Moreover, the paper is a long way of the level of completeness, rigor, clarity, and polish that is required to seriously consider it for publication.  In short, the work is still at a relatively early stage and a lot more would need to be done for it to attain various minimum standards for acceptance.  A non-exhaustive list of specific examples of its shortfalls are given below.

1. The paper is over a page and a half under length, despite wasting large amounts of space (e.g. figures 3 and 4 should be two lines on the same plot)

2. The experimental evaluation is woefully inadequate.  The only quantitative assessment is to compare to a single different approach on a single toy dataset, and even then the metric being used is the one the new method uses to train for making it somewhat meaningless.

3. The introduction is completely generic and says nothing about the method itself, just providing a (not especially compelling) motivation for disentanglement in general.  In fact, the motivation of the introduction is somewhat at odds with the work -- correctly talking about the need for hierarchical representations which the approach actually actively discourages.

4. There are insufficient details on the algorithm itself in terms of the approximations that are made to estimate the synergistic mutual information.  These are mostly glossed over with only a very short explanation in the paragraph after equation 15.  Yes there are algorithm blocks, but these are pretty incomprehensible and lack accompanying text.  In particular, I cannot understand what A_w is supposed to be.  This is very important as I suspect the behavior of the approximation is very different to the true target.  Similarly, it would be good to provide more insight into the desired target (i.e. Eq 15).  For example, I suspect that it will encourage a mismatch between the aggregate posterior and prior by encouraging higher entropy on the former, in turn causing samples from the generative model to provide a poor match to the data.

5. The repeated claims of the approach and results being ""state-of-the-art"" are cringe-worthy bordering on amusing.  Writing like this serves no purpose even when it justified, and it certainly is not here.

6. There are a lot of typos throughout and the production values are rather poor.  For example, the algorithm blocks which are extremely messy to the point where they are difficult to follow, citep/citet mistakes occur almost every other citation, there is a sign error in Equation 16.


This is a piece of work in an exciting research area that,  with substantial extra work, could potentially result in a decent paper due to fact that the core idea is simple and original.  However, it is a long way short of this in its current state.  Along with addressing the specific issues above and improving the clarity of the work more generally, one thing in particular that would need to address in a resubmission is a more careful motivation for the method (ideally in the form of a proper introduction).  

Though I appreciate this is a somewhat subjective opinion, for me, penalizing the synergistic information is probably actually a bad thing to do when taking a more long-term view on disentanglement.  Forcing simplistic representations where no information is conveyed through the composition of latents beyond that they provide in isolation is all well and good for highly artificial and simplistic datasets like dsprites, but is clearly not a generalizable approach for larger datasets where no such simplistic representation exists.  As you say in the first line of your own introduction, hierarchy and composition are key parts of learning effective and interpretable representations and this is exactly what you are discouraging.  A lot of the issue here is one of the disentanglement literature at large rather than this paper (though I do find it to be a particularly egregious offender) and it is fine to have different opinions.  However, it is necessary to at least make a sensible case for why your approach is actually useful.  

Namely, is there actually any real applications where such a simplistic disentanglement is actually useful?  Is there are anyway the current works helps in the longer vision of achieving interpretable representations?  When and why is the synergistic information a better regularizer than, for example, the total correlation?  The experiments you have do not make any inroads to answering these questions and there are no written arguments of note to address them.  I am not trying to argue here that there isn't a good case to be made for the suggested approach in the context of these questions (though I am suspicious), just that if the work is going to have any lasting impact on the community then it needs to at least consider them.","The sentiment score is -60 because the reviewer expresses significant dissatisfaction with the paper's current state, pointing out numerous shortcomings and indicating that the work is far from meeting the standards required for publication. The politeness score is 20 because, while the reviewer is critical, they maintain a professional tone and offer constructive feedback, acknowledging the potential of the core idea and suggesting ways to improve the paper.",-60,20
"The paper studies approximation and estimation properties of CNNs with residual blocks in the context
of non-parametric regression, by constructing equivalent fully-connected architectures (with a block-sparse structure),
and leveraging previous approximation results for such functions.
Explicit risk bounds are obtained for regression functions in Barron and Holder classes.

The main contribution of the paper is Theorem 1, which shows that a class of ResNet-type CNNs
contains a class of ""block-sparse"" fully-connected networks, with appropriate constraints on various size quantities.
This result allows the authors to obtain a general risk bound for the ResNet CNN that minimizes empirical risk
(Theorem 2, which mostly follows Schmidt-Hieber (2017)),
as well as adaptations of the bound for the Barron and Holder classes, by relying on existing approximation results.

The construction of Theorem 1 is interesting, and shows that ResNet CNNs can be quite powerful function approximators,
even with a filter size that is arbitrarily fixed.
However, the obtained CNN approximating architectures look quite unrealistic compared to most practical use-cases of CNNs,
since they specifically try to reproduce a fully-connected architecture, leading to residual blocks of depth ~= D/K,
which is very deep compared to usual CNNs/ResNets (considering, e.g. K=3 and D in the hundreds for images).
In particular, CNNs are typically used when there is some relevant inductive bias such as equivariance
to translations (and invariance with pooling operations) to take advantage of,
so removing this inductive bias by approximating fully-connected architectures seems a bit twisted.
The approach of reducing the function class to be approximated would seem more relevant here,
as in the cited papers Petersen & Voigtlaender (2018) and Yarotsky (2018), and perhaps the results of
the present paper can be useful in such a scenario as well.

Separately, the presentation of the paper could be significantly improved,
for instance by introducing relevant notions more clearly in the introduction and related work sections,
and by providing more insight and discussion of the obtained results in the main paper.

More specific comments:
- Section 1, p.2: define M? define D? M seems to be used for different things in different paragraphs
- Section 2: Explain what is ""s"" in the Barron class, or at least point to the relevant definition in the paper
- Section 3.1:
  * 'estimation error' is usually called '(expected) risk' in the statistical literature (also in the introduction). estimation error would have to do with relating R and R^hat
  * why is the estimator ""regularized""?
- Definition 2: shouldn't it be D_m^(0) = D instead of 1?
- Theorem 1: What is L? Also, it would be helpful to sketch the construction in the main paper given that this is the main result.
- Section 4.2: M_1 is the Lipschitz constant of what function?
- Section 5.1: ""M = 1"" this is confusing, maybe use a different letter for the ridge expansion? The discussion on 'relative scale' could be made clearer.
- Section 5.2, 'if we carefully look at their proofs': more details on this should be provided.
","The sentiment of the review is mixed but leans towards positive. The reviewer acknowledges the interesting construction of Theorem 1 and the potential power of ResNet CNNs as function approximators. However, they also point out significant limitations and practical concerns regarding the obtained CNN architectures. The sentiment score is thus 20. The politeness of the language is quite high. The reviewer uses polite and constructive language throughout, offering specific suggestions for improvement without being dismissive or harsh. The politeness score is 80.",20,80
"The paper presents an approach for an approach to addressing multi-goal reinforcement learning, based on what they call ""one-step path rewards"" as an alternative to the use of goal conditioned value function. 
The idea builds on an extension of a prior work on FWRL. 
The paper presents empirical comparison of the proposed method with two baselines, FWRL and HER. 
The experimental results are mixed, and do not convincingly demonstrate the effectiveness/superiority of the proposed method. 
The idea of the proposed method is relatively simple, and is not theoretically justified. 

Based on these observations, the paper falls short of the conference standard. ","The review expresses a negative sentiment towards the paper, highlighting that the experimental results are mixed and do not convincingly demonstrate the effectiveness or superiority of the proposed method. Additionally, it mentions that the idea is relatively simple and lacks theoretical justification. The language used is neutral and professional, without any rude or overly critical remarks.",-70,0
"This paper presents an instruction-following model consisting of two modules: a
goal-prediction model that maps commands to goal representations, and an
execution model that maps goal representations to policies. The second module is
trained without command supervision via a goal exploration process, while the
first module is trained supervisedly in a metric learning framework.

This paper contains an important core insight---much of what's hard about
instruction following is generic planning behavior that doesn't depend on the
semantics of instructions, and pre-learning this behavior makes it possible to
use natural language supervision more effectively. However, the paper also
contains a number of serious evaluation and presentation issues. It is obviously
not ready to publish (uncaptioned figures, paragraphs interrupted mid-sentence,
etc.) and should not have been submitted to ICLR in its present form.

SUPERVISION AND COMPARISONS

I found comparisons between supervision conditions in this paper difficult to
understand. It is claimed that the natural language instruction following
approaches described in the first paragraph ""require a large amount of human
supervision"" in the form of action sequences. This is not exactly true, as some
approaches (e.g. Artzi 2013), can be trained with only task completion signals.
More problematically, all these approaches are contrasted with reinforcement and
imitation learning approaches, which are claimed to use ""little human
supervision"". In fact, most of the approaches listed in this section use exactly
the same supervision---either action sequences (imitation learning) or task
completion signals (reinforcement learning). Indeed, the primary distinction is
that the ""NLP-style"" approaches are typically evaluated on their ability to
generalize to new instructions, while the ""RL-style"" approaches are evaluated on
the (easier) problem of fitting the complete instruction distribution as quickly
as possible.

This confusion carries into the evaluation of the approach proposed in this
paper, which is compared to RL and IL baselines. It's hard to tell from the
text, but it appears that this is an ""RL-style"" evaluation setting, where we
only care about rapid convergence rather than generalization. But the baselines
are inadequately described, and it's not clear to me that they condition on the
commands at all. More significantly, it's not clear what an evaluation based on
""timesteps"" means for a behavior-cloning approach---is this the number of
distinct trajectories observed? The number of gradient steps taken? Without
these explanations it is impossible to interpret the experimental results.

GENERALITY OF PROPOSED APPROACH

Despite the advantages of the high-level two-phase model proposed, the specific
implementation in this paper has two significant shortcomings:

- No evidence that it works with real language: despite numerous claims
  throughout the paper that the model is designed to interpret ""human
  instructions"", it is revealed on p7 that these instructions consist of one or two
  5-way indicator features. This is an extremely impoverished instruction space,
  especially compared to the numerous papers cited in the introduction that make
  use of large datasets of complex natural-language strings generated by human
  annotators. The present experiments do not support the use of the word ""human""
  anywhere in the paper.

- No support for combinatorial action spaces. Even if we set aside the
  distinctions between human-generated instructions and synthetic command
  languages like used in Hermann Hill & al., the goal -> policy module is
  defined by a buffer of cached trajectories and goal representations. While
  this works for the simple environments considered in this paper, it cannot
  generalize to real-world instruction-following scenarios where the number of
  distinct goal configurations is too large to tractably enumerate. Again, this
  is a shortcoming that existing approaches do not suffer from (given
  appropriate assumptions about the structure of goal space), so the lack of
  comparisons is problematic.

CLARITY

The whole paper would benefit from copy-editing by an experienced English
speaker, but a few sections are particularly problematic:

- The first paragraph of 4.1.1 is extremely difficult to understand What does
  the fingertip do? What exactly is the action space?

- The end of the second paragraph is also difficult to understand; after reading
  it I still don't know what the extra ""position"" targets do.

- 4.1.4 is cut off mid-way through a sentence.

- last sentence of 4.2

The figures are also impossible to interpret: three of the four are captioned
""overview of the proposed framework"", and none are titled.","The sentiment of the review is mixed. The reviewer acknowledges the important core insight of the paper, which is a positive aspect, but also points out numerous serious issues with evaluation, presentation, and clarity, leading to a recommendation against publication in its current form. This results in a sentiment score of -40. The language used in the review is generally polite and constructive, providing specific feedback and suggestions for improvement without being rude or dismissive, leading to a politeness score of 60.",-40,60
"Summary: The authors present a network which facilitates cross-domain
learning for SLU tasks where the the goal is to resolve intents and
slots given input utterances. At a high level, the authors argue that
by fine-tuning a pre-trained version of the network on a small set of
examples from a target-domain they can more effectively learn the
target domain than without transfer learning.

Feedback:

* An overall difficulty with the paper is that it is hard to
distinguish the authors' contributions from previous works. For
example, in Section 3.1, the authors take the model of Goyal et al. as
a starting point but explain only briefly one difference
(contatenating hidden layers). In Section 3.2 the contributions
becomes even harder to disentangle. For example, how does this section
relate to other word-embeddings papers cited in this section? Is the
proposed method a combination of previous works, and if not, what are
the core new ideas?

* Some sections are ad-hoc and should be justified/explained
better. For example, the objective, which ultimately determines the
trained model behaviour uses a product of experts formulation, yet the
authors do not discuss this. Similarly, the overarching message, that
by fine-tuning a suitable model initialisation using small amounts of
data from the target domain is fairly weak as the authors do not
detail exactly how the model is fine-tuned. Presumably, given only a
small number of examples, this fine-tuning runs the risk of
overfitting, unless some form of regularisation is applied, but this
is not discussed.

* Lastly, there are some curious dips in the plots (e.g., Figure 2 bottom left, Figure 3 top left, bottom left), which deserve more explanation. Additionally, the evaluation section could be improved if the scores were to show error-bars. 

Minor: All plots should be modified so they are readable in grey-scale.","The sentiment of the review can be considered neutral to slightly negative. The reviewer acknowledges the work but points out significant issues and areas for improvement without any explicit praise. Therefore, the sentiment score is -20. The politeness of the language used is quite high. The reviewer provides constructive feedback and uses polite language throughout, avoiding any harsh or rude comments. Therefore, the politeness score is 80.",-20,80
"This paper presents a framework to train an end-to-end multi-lingual multi-speaker speech recognition system. Overall, the paper is quite clear written.
- Strengthens:
+ Experimental results show consistent improvements in speech recognition performance and language identification performance.

- Weakness:
+ I'm not sure whether the framework is novel. The authors have just mixed training data from several languages to train an end-to-end multi-speaker speech recognition system.
+ I don't see the real motivation why the authors want to make the task harder than needed. The example provided in figure 1 is very rare in reality.
+ The authors claimed that their system can recognise code-switching but actually randomly mixing data from different languages are not code-switching.
+ In general, it would be better to have some more analyses showing what the system can do and why.","The sentiment of the review is mixed. The reviewer acknowledges the clarity of the paper and the consistent improvements in performance, which is positive. However, the reviewer raises several concerns about the novelty of the framework, the motivation behind the approach, and the validity of the claims regarding code-switching. This results in a sentiment score of around -20, indicating a slightly negative overall sentiment. The politeness of the language used is relatively high. The reviewer uses polite language, such as 'I'm not sure' and 'it would be better,' which softens the criticism. Therefore, the politeness score is around 80.",-20,80
"The paper proposes W2GAN, a GAN where the objective function relies on a W2 distance. Authors state that the discriminator approximate the W2 distance, and that the generator follows an OT map. 
While I did not see any flaws in the development, the paper is quite bushy and hard to follow. Some questions are still open, for instance in the end of the experiments, authors state that the model has ""a strong theoretical advantages"": can you provide more details about those advantages?
The experiments do not show any clear advantages of the method regarding competitors. Regarding Table 1, why are there some points with no arrows? W2-OT seems not to perform better: are there some other advantages (computational?) to use the method? In Figure 1, it is quite difficult to evaluate the results on a single image with no comparisons. Again, providing a strong evaluation of the method would help to strengthen the paper. 

There are some weird statements and typos mistakes that should be corrected. For example in the first 2 pages: (abstract) ""other GANs also approximately following the Optimal Transport"", (Introduction) ""An optimal map has many important implications such as computing barycenters"", ""high-dimenisonal"", ""generator designed"", ""consideral"", ""although the theoretical arguments do not scale immediately"".
The layout of the bibliography should be deeply reviewed.

","The sentiment of the review appears to be slightly negative. The reviewer acknowledges that there are no flaws in the development but criticizes the paper for being hard to follow and lacking clear advantages in the experiments. The reviewer also points out several issues, including unclear statements, lack of detailed advantages, and typos. Therefore, the sentiment score is -30. The politeness of the language used in the review is relatively neutral. The reviewer does not use any harsh or rude language but is straightforward in pointing out the issues. Therefore, the politeness score is 0.",-30,0
"The paper applies graph convolutional networks to Penn Treebank language modeling and provides analysis on the attention weight patterns it uses.

Clarity: the paper is very clearly written!

The introduction states that existing CNN language models are ""not easily interpretable in that they do not explicitly learn the structures of sentences"". Why is this? The model in this paper computes attention values which is interpreted by the authors as corresponding to the structure of the sentence but there are equivalent means to trace back feature computation in other network topologies as well.

My biggest criticism is that the evaluation is done on a very small language modeling benchmark which is clearly out of date. Penn Treebank is the CIFAR10 of language modeling and any claims on this dataset about language modeling are highly doubtful. Models today have tens and hundreds of millions of parameters and training them on 1M words is simply a regularization exercise that does not enable a meaningful comparison of architectures.

The claims in the paper could be significantly strengthened by reporting results on at least a mid-size dataset such as WikiText-103, or better even, the One Billion Word benchmark.","The sentiment of the review is mixed. The reviewer acknowledges the clarity of the paper and the innovative application of graph convolutional networks, which suggests a positive sentiment. However, the reviewer also expresses significant criticism regarding the outdated dataset used for evaluation, which brings the sentiment down. Therefore, the sentiment score is around -20. The politeness of the language is quite high; the reviewer uses polite language, such as 'My biggest criticism' and 'The claims in the paper could be significantly strengthened,' which indicates constructive feedback rather than harsh criticism. Thus, the politeness score is around 80.",-20,80
"
-------------
Summary
-------------
The authors propose to train a policy while concurrently learning a dynamics model. In particular, the policy is updated using both the RL loss (rewards from the environment) and the ""consistency constraint"", which the authors introduce. This consistency constraint is a supervised learning signal, which compares trajectories in the environment with trajectories in the imagined world (produced with the dynamics model). 

---------------------
Main Feedback
---------------------
I feel like there might be some interesting ideas in this work, and the results suggest that this approach performs well. However, I had a difficult time understanding how exactly the method works, and what its advantages are. These are my main questions:

1) At the beginning of Section 4 the authors write ""The learning agent has two pathways for improving its behaviour: (...) (ii) the open loop path, where it imagines taking actions and hallucinates the state transitions that could happen"". Do you actually do this? This is not mentioned in anywhere. And as far as I understand, the reward function is not learned - hence there will be no training signal in the open loop path. Does the reward signal always come from the true environment?
2) Is the dynamics model used for anything else than action-selection during training? Planning? If not, I don't really understand the results and why this works at all (k=20 being better than k=5, for example).
3) Is the dynamics model pre-trained in any way? I find it surprising that the model-free method and the proposed method perform similar at the beginning (Figure 3). If the agent chooses its actions based on the state that is predicted by the dynamics model, this should throw off the learning of the policy at the beginning (when the dynamics model hasn't learned anything sensible yet).

-----------------------
Other Questions
-----------------------
4) How exactly does training without the consistency constraint look? Is this the same as k=1?
5) Could the authors comment on the evaluation protocol in the experimental section? Are the results averages over multiple runs? If so, it would help to see confidence intervals to make a fair assessment of the results. 
6) For the swimmer in Figure 2, the two lines (with consistency and without consistency) start at different initial returns, why is that so? If the same architecture and seed was used, shouldn't this be the same (or can you just not see it in the graph)?

---------
Clarity
---------
The title and introduction initially gave me a slightly wrong impression on what the paper is going to be about, and several things were not followed up on later in the paper.
Title:
8) ""generative models"" reminds of things like a VAE or GAN; however, I believe the authors mean ""dynamics models"" instead
9) ""by interaction"" is a bit vague as to what the contribution is (aren't policies and dynamic models in general trained by interacting with the environment?); the main idea of the paper is the consistency constraint
Abstract / Introduction:
10) The authors talk about humans carrying out ""experiments via interaction"" to help uncover ""true causal relationships"". This idea is not brought up again in the methods section, and I don't see evidence that with the proposed approach, the policy does targeted experiments to uncover causal relationships. It is not clear to me why this is the intuition that motivates the consistency constraint. 
11) As the authors state in the introduction, the hope of model-based RL is better sample complexity. This is usually achieved by using the model in some way, for example by planning several steps ahead when choosing the current action. Could the authors comment on where they would place their proposed method - how does it address sample complexity?
12) In the introduction, the authors discuss the problem of compounding errors. These must be a problem in the proposed method as well, especially as k grows. Could the authors comment on that? How come that the performance is so good for k=20?
13) The authors write that in most model-based approaches, the dynamics model is ""learned with supervised learning techniques, i.e., just by observing the data"" and not via interaction. There's two things I don't understand: (1) in the existing model-based approaches the authors refer to, the policy also interacts with the world to get the data to do supervised learning - what exactly is the difference? (2) The auxiliary loss ""which explicitly seeks to match the generative behaviour to the observed behaviour"" is just a supervised learning loss as well, so how is this different?

For me, it would help the readability and understanding of the paper if some concepts were introduced more formally.
14) In Section 2, it would help me to see a formal definition of the MDP and what exactly is optimised. The authors write ""optimise a reward signal"" and ""maximise its expected reward"", however I believe it should be the expected cumulative reward (i.e., return). 
15) The loss function for the dynamics model is not explicitly stated. From the text I assume that it is the mean squared error for the per-step loss, and a GAN loss for the trajectory-wise loss.
16) Could the authors explicitly state what the overall loss function is, and how the RL and supervised objective are combined? Is the dynamics model f trained only on the supervised loss, and the policy pi only on the RL loss?
17) In 2.3 the variable z_t is not formally introduced. What does it represent?

------------------------
Other Comments
------------------------
18) I find it problematic to use words such as ""hallucination"" and ""imagination"" when talking about learning algorithms. I would much prefer to see formal/factual language (like saying that the dynamics model is used to do make predictions / do planning, rather than that the agent is hallucinating). 

-- edit (19.11.) ---
- updated score to 5
- corrected summary","The sentiment of the review is mixed but leans slightly positive. The reviewer acknowledges that there might be interesting ideas in the work and that the results suggest the approach performs well, but they also express significant confusion and raise many questions about the methodology and its advantages. Therefore, the sentiment score is around 10. The politeness of the language is quite high; the reviewer uses phrases like 'I feel like,' 'could the authors comment,' and 'for me, it would help,' which are polite and constructive. Thus, the politeness score is around 80.",10,80
"This paper considers reinforcement learning tasks that have high-dimensional space, long-horizon time, sparse-rewards. In this setting, current reinforcement learning algorithms struggle to train agents so that they can achieve high rewards. To address this problem, the authors propose an abstract MDP algorithm. The algorithm consists of three parts: manager, worker, and discoverer. The manager controls the exploration scheduling, the worker updates the policy, and the discoverer purely explores the abstract states. Since there are too many state, the abstract MDP utilize the RAM state as the corresponding abstract state for each situation. 

The main strong point of this paper is the experiment section. The proposed algorithm outperforms all previous state of the art algorithms for Montezuma’s revenge, Pitfall!, and Private eye over a factor of 2. 

It is a minor weak point that the algorithm can work only when the abstract state is obtained by the RAM state. In some RL tasks, it is not allowed to access the RAM state. 

================================
I've read all other reviewers' comments and the response from authors, and decreased the score. Although this paper contains interesting idea and results, as other reviewers pointed out, it is very hard to compare with other algorithm. I agree to other reviewers. The algorithm assumptions are strong. ","The sentiment of the review is mixed. The reviewer acknowledges the strong experimental results of the paper but also points out significant limitations and agrees with other reviewers' criticisms. Therefore, the sentiment score is slightly positive at 20. The politeness of the language is generally respectful and constructive, with no rude or harsh language used. The reviewer provides specific feedback and acknowledges the authors' efforts, so the politeness score is 80.",20,80
"As a paper on how to prioritize the use of neurons in a memory this is an excellent paper with important results. 

I am confused by the second part of the paper an attached GAN of unlimited size. It may start out small but there is nothing to limit its size over increased learning. It seems to me in the end it becomes the dominate structure. You start the abstract with ""able to learn from a stream of data over an undefined period of time"". I think it would be an improvement if you can move this from an undefined time/memory size to a limited size for the GAN and then see how far that takes you. ","The sentiment of the review is generally positive, as indicated by the phrase 'an excellent paper with important results.' However, the reviewer expresses confusion and suggests improvements, which slightly tempers the overall positivity. Therefore, the sentiment score is 70. The politeness of the language is quite high, as the reviewer uses phrases like 'I think it would be an improvement' and 'It seems to me,' which are considerate and constructive. Thus, the politeness score is 90.",70,90
"The paper proposes using structured matrices, specifically circulant and diagonal matrices, to speed up computation and reduce memory requirements in NNs. The idea has been previously explored by a number of papers, as described in the introduction and related work.  The main contribution of the paper is to do some theoretical analysis, which is interesting but of uncertain impact.

The experiments compare performance against DeepBagOf`Fframes (DBOF) and MixturesOfExperts (MOE). However, there are other algorithms that are both more competitive and more closely related. I would like to see head-to-head comparisons with tensor-based algorithms such as Novikov et al: https://papers.nips.cc/paper/5787-tensorizing-neural-networks, which achieves huge compression ratios (~200 000x), and other linear-algebra based approaches. 

AFTER READING REBUTTAL
I've increased my score because the authors point out previous work comparing their decomposition and tensortrains (although note the comparisons in Moczulski are on different networks and thus hard to interpret) and make a reasonable case that their work contributes to improve understanding of why circulant networks are effective. 

I strongly agree with authors when they state: ""We also believe that this paper brings results with a larger scope than the specific problem of designing compact neural networks. Circulant matrices deserve a particular attention in deep learning because of their strong ties with convolutions: a circulant matrix operator is equivalent to the convolution operator with circular paddings"".  I would broaden the topic to structured linear algebra more generally. I hope to someday see a comprehensive investigation of the topic.","The sentiment of the review is generally positive, especially after the rebuttal, where the reviewer acknowledges the authors' reasonable case and expresses strong agreement with their statements. The initial sentiment is somewhat neutral to slightly positive, as the reviewer finds the theoretical analysis interesting but of uncertain impact. The politeness of the language is high throughout the review. The reviewer uses polite language, such as 'I would like to see,' 'I strongly agree,' and 'I hope to someday see,' which indicates a respectful and constructive tone.",60,90
"In this paper, a framework for lifelong learning based on Bayesian neural network is proposed. The key idea is to combine iterative pruning for multi-task learning along with the weight regularization. The idea of iterative pruning was first considered by Mallya et al., 2018 and weight regularization was considered for Bayesian neural network by Nguyen et al., 2018.

Pros: 
- Combination of two idea seems novel. I like the idea of considering the weight parameter as the ""global"" random variables and the mask parameters as the task-specific random variables. 

Cons: 
- In general, there is lack of explanation/justification on the combination of two ideas. Especially, there is lack of explanation on how to apply the whole algorithm (e.g., text states that complete algorithm is in Algorithm 3., but there is no Algorithm 3. in the paper). 

- I do not understand how equation (6) is developed, and why hyper-parameters are need for ""regularization of weights"", comparing with the Variational Continual Learning (VCL, Nguyen et al., 2018). More explanation seems necessary for justification of the algorithm.

- More stronger baselines need to be considered for the experiments. Why is there no comparison with the existing continual learning algorithms? At the very least, comparison with the VCL or Elastic Weight Consolidation (EWC, Kirkpatrick et al., 2017) seems necessary since one of the key idea is about regularization for weights.


In general, I think it is a nice idea to combine two existing approaches. However, the algorithm lacks justification in general and experimental results are not very persuasive.   ","The sentiment of the review is mixed but leans slightly positive. The reviewer acknowledges the novelty of combining two ideas and appreciates the conceptual framework, which suggests a sentiment score of around 20. However, the review also highlights significant issues, such as the lack of explanation and justification for the algorithm, missing algorithm details, and insufficient experimental comparisons. The politeness score is high, around 80, as the reviewer uses polite language throughout, such as 'I like the idea' and 'more explanation seems necessary,' and provides constructive criticism without being rude.",20,80
"The authors propose to use importance resampling (IR) in place of importance sampling (IS) for policy evaluation tasks. The method proposed by the authors definitely seems valid, but it isn’t quite clear when this is applicable.

IR is often used in the case of particle filters and other SMC is often used to combat the so-called “degeneracy problem” where a collection of particles (or trajectories) comes to degenerate such that all the mass is concentrated onto a single particle. This does not seem to be the case here, as the set of data (the replay buffer) does not seem to be changing over time. In particular, since the target policy and the behavior policy are fixed, the bigger issue seems to be that the distribution itself will not change over time.

Finally, the results are given for somewhat simple problems. The first two settings show that the difference between IR/IS can be very stark, but it seems like this is the case when the distributions are very different and hence the ESS is very low. The IR methods seem like they can eliminate this deficiency by only sampling from this limited subset, but it is also unclear how to extend this to the policy optimization setting.

Overall I have questions about where these results are applicable. And finally, as stated a moment ago, it is unclear how these results could be extended to the setting of off-policy policy optimization, where now the resulting policies are changing over time. This would necessitate updating the requisite sampling distributions as the policies change, which does seem like it would be difficult or computationally expensive (unless I am missing something). Note that this is not an issue with IS-based methods, because they can still be sampled and re-weighted upon sampling.
","The sentiment of the review is somewhat neutral to slightly negative. The reviewer acknowledges the validity of the proposed method but raises several concerns and questions about its applicability and limitations. This suggests a sentiment score of around -20. The language used in the review is polite and professional, as the reviewer provides constructive feedback without using harsh or dismissive language. Therefore, the politeness score is around 80.",-20,80
"Summary: The paper considers federate learning of neural networks, i.e. data are distributed on multiple machines and the allocation of data points is potentially inhomogenous and unbalanced. The paper proposes a method to combine multiple networks trained locally on different data shards and form a global neural network. The key idea is to identify and reuse neurons that can be used for all shards and add new neurons to the global network if necessary. The matching and combination process is done via MAP inference of a model using a Beta-Bernoulli process. Some experiments on federated learning demonstrate the performance of the proposed method.

General evaluation (+ pro/ - con, more specific comments/questions below):
+ the paper is very well-written -- the BBP presentation is light but very accessible. The experimental set up seems sound.
+ the matching procedure is novel for federated training of neural networks, as far as I know, but might not be if you are a Bayesian nonparametric person, as the paper pointed out similar techniques have been used for topic models.
- the results seem to back up the claim that the proposed is a good candidate for combining networks at the end of training, but the performance is very similar or inferior to naive combination methods and that the global network is way larger than individual local network and nearly as large as simply aggregating all neurons together.
- the comparison to recent federated learning methods is lacking (e.g. McMahan et al, 2017) (perhaps less communication efficient than the proposed method, but more accurate).

Specific comments/questions/suggestions:
- the MAP update for the weights given the assignment matrix is interesting and resembles exactly how the Bayesian committee machine algorithm of Tresp (2000) works, except that the variances are not learnt for each parameter but fixed for each neuron. On this, there are several hyperparameters for the model, e.g. variance sigma_j -- how are these tuned/selected?
- the local neural networks are very mall (only 50 neurons per layer). How do they perform on the test set on the homogeneous case? Is there a performance loss by combining these networks together?
- the compression rate is not that fantastic, i.e. the global network tends to add new neuron for each local neuron considered. Is this because it is in general very hard to identify similar neuron and group them together? In the homogeneous case, surely there are some neurons that might be similar. Or is it because of the MAP inference procedure/local optima?","The sentiment of the review is generally positive, as indicated by the positive comments about the paper being well-written, the experimental setup being sound, and the novelty of the matching procedure. However, there are also some critical points regarding the performance and comparison to other methods, which slightly temper the overall positivity. Therefore, the sentiment score is 50. The politeness of the language is very high, as the reviewer uses polite and constructive language throughout, even when pointing out the cons and making suggestions. Thus, the politeness score is 100.",50,100
"The contributions of this paper are in the field of LSTM, where the authors explore the interpretability of LSTM with multivariate data obtained from various and disparate applications. To this end, the authors endow their approach with tensorized hidden states and an update process in order to learn the hidden states. Furthermore, the authors develop a mixture attention mechanism and a summarization methods to quantify the temporal and variable importance in the data. They validate the forecasting  and interpretability performance of their approach with experiments. 

The parer is interesting, well structured and and clearly written. Also, the addressed topic of interpretability is pertinent. However, I have several concerns.

1. In the related work the authors state that “In time series analysis, prediction with exogenous variables is formulated as an auto-regressive exogenous model ” . This is not always right - it is not imperative to add the auto-regressive terms, this is optional and depends on the way we want to formulate our time series forecasting approach and the known constraints.  
2. In section 3 — Interpretable Multi-Variable LSTM, by stacking exogenous time series and target series, the authors implicitly formulate their algorithms in a way to consider auto-regression. And I have several concerns with this for time series forecasting. Because, the past is not always a predictor of the future even - particularly in time series context and in industrial settings. And in the occasions where the past allows to predict the future we do not necessarily need to use LSTM to forecast (the notion of persistency in forecasting is enough).  Therefore, the power of LSTM in forecasting would have been convincing if you omit the target series in your multi-variable input.
3. In Network Architecture section the authors develop tensorized hidden state and an update scheme. This idea is interesting, I think it would also be good to know what is the algorithmic complexity of this approach? 
4.  In section 3.3 the authors state that ""In the present paper, we choose the simple normalized summation function eq.(9). "" Could the authors justify the reason behind this choice? I am not convinced of the reason behind this, especially the authors mention, right after,  that ""It is flexible to choose alternative functions for f_{agg}""

5. In the experiment section, concerning the prediction performance the authors present a table showing their results, I believe it would have been more compelling to present the prediction results with graphs showing the normalized cumulative errors, as an example.

6. With regard to the interpretation of the results, the authors show the variable importance as a function of the epoch number, it would be equally important to correlate the same figure with the associated prediction results/normalized cumulative errors as a function of the epoch number - this will allow to assess the importance of the interpretability.

I think it would be important to further justify the pertinence of this work in terms of interpretability (the statement in the introduction ""the interpretability of prediction models is essential for deployment and
knowledge extraction"" seems to be limited) for example what does it bring knowing the variance importance  as a function of the epoch number. As an example, the Pearson correlation coefficient can help select relevant features to a model, and restrict the number of inputs to the relevant ones - can we draw inspiration from this and explain what the authors are proposing in terms of interpretability... Here the idea is to have a motivation presenting the merits of this work, which I think is missing - particularly with the experiments presented here.","The sentiment of the review is generally positive, as the reviewer acknowledges that the paper is interesting, well-structured, and clearly written, and that the topic of interpretability is pertinent. However, the reviewer also expresses several concerns and provides detailed recommendations for improvement. Therefore, the sentiment score is 50. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, making suggestions and asking questions in a respectful manner. Therefore, the politeness score is 80.",50,80
"This paper proposes to learn a dynamics model (state to pixels using Generative Adversarial Networks), use this model in conjunction with Monte Carlo Tree Search, model-free reinforcement learning (Q-learning) and a reward prediction network essentially combining model-free with model-based learning. The proposed approach is empirically evaluated on a small subset of the Atari games and theoretical analysis for the bias-variance tradeoff are presented.

It is highly appreciated that this paper presents an idea and discusses why the proposed approach does not result in high performance. This is very valuable and useful for the community. On a high level it would be very useful if Figure 1 would be show less examples but present them much larger since it is almost impossible to see anything in a printout. Further, the caption does not lend itself to understand the figure. Similarly Figure 2 would benefit from a better caption. 

The first part of the discussion (7), the individual building blocks, should be mentioned much earlier in the paper. It would be further useful to add more related work on that abstraction level. This would help to communicate the main contribution of this paper very precisely.

On the discussion of negative results: It is very interesting that Dyna-Q does not improve the performance and the hypothesis for why this is the case seems reasonable. Yet, it would be very useful to actually perform an experiment in a better controlled environment for which e.g. the dynamics model is based on the oracle and assess the empirical effect of different MCTS horizons and rollout estimates. Further, this scenario would allow to further quantify the importance and the required “quality” of the different learning blocks.

In its current form the paper has theoretical contributions and experimental results which cannot be presented in the main paper due to space constraints. Albeit the appendix is already very extensive it would be very useful to structure it into the theoretical derivation and then one section per experiment with even more detail on the different aspects of the experiment. The story of the main paper would benefit from referencing the negative results more briefly and better analyzing the different hypothesis on toy like examples. Further, the introduction could be condensed in order to allow for more in detail explanations and discussions without repetition later on.

As argued in the paper it is clear that image generation is a very expensive simulation mechanism which for games like pong which depend on accurate modeling of small aspects of the image are in itself difficult. Therefore, again although really appreciated, the negative results should be summarized in the main paper and the hypothesis concluded better analyzed. The extensive discussion of hyper parameters and approaches for individual components could be in the appendix and the main paper focuses on the hypothesis analysis.
","The sentiment of the review is generally positive, as the reviewer appreciates the value and usefulness of the paper's contributions, even though it discusses negative results. The reviewer provides constructive feedback and suggestions for improvement, indicating a positive engagement with the work. Therefore, the sentiment score is 70. The politeness of the language is high, as the reviewer uses phrases like 'highly appreciated,' 'very valuable and useful,' and 'it would be very useful,' which are polite and respectful. The reviewer also avoids any harsh or dismissive language, leading to a politeness score of 90.",70,90
"Summary: 
The paper considers finding the most adversarial random noise given multiple classifiers. They formulate the problem as the standard min-max game and apply the multiplicative weight updates. The technical contribution is to clarify the computational complexity of implementing/approximating the response oracle. The authors show experimental results.

Comments: 
I am afraid that the main technical result is already known:

Yoav Freund Robert E. Schapire: Adaptive game playing using multiplicative weights, Games and Economic Behavior, 29:79-103, 1999.

The paper shows that a multiplicative update algorithm can approximately solve the min-max game. If you use the result, you can readily obtain the main results of the present paper. 

After rebuttal:
I read the authors comments and I understood the technical contribution more and raised my score.  Implementing/appriximating the response oracle is non-trivial. For MWU, I still think that the above paper should be cited (citing the Adaboost paper is not enough) since the paper shows MWU solves the min-max game.  
","The sentiment of the review is mixed. Initially, the reviewer expresses a negative sentiment by pointing out that the main technical result is already known and references a prior work. However, after reading the authors' rebuttal, the reviewer acknowledges a better understanding of the technical contribution and raises their score, indicating a shift towards a more positive sentiment. Therefore, the sentiment score is around -20. The politeness of the language is generally neutral to polite. The reviewer uses phrases like 'I am afraid' and 'I understood the technical contribution more,' which are polite and considerate. Therefore, the politeness score is around 50.",-20,50
"This paper is poorly written, and looks like it was not proof-read. 
Presentation of the problem at hand is presented over so many times that it becomes confusing.
Authors ought to better define the image description space of the objects and the haptic space. 
More interesting would have been a good explanation of the different sensors used in the anthropomorphic hand  and the vector built to represent the different sensed objects.
The most expected contribution of this work is barely explained: how the haptic sensors' values/object labels vectors were built and fed to the predictor network, what their values looked like for the various objects, how these vectors clustered for the various objects etc.

Among the many evident weaknesses:
- Domain specific concepts and procedures of most importance to this work are not explained: ""... measure various physical properties of objects using the bio-tac sensor using five different exploration procedures (EP)"".  Page 3, Paragraph 1. Bio-tac sensor and most importantly exploration procedures (EP) should be presented more clearly.
- Incomplete and out of nowhere sentences are common: ""The SHAP procedure
was established for evaluating prosthetic hands and arms. With this idea in mind, prior work (?)
built a prosthetic arm which could ..."" Page 4, Paragraph 1.
- Many references are not well introduced and justified: ""We then trained the network using
ADAM (Kingma & Ba (2014)) with an initial learning rate set to 1e-4."" Page 4, Paragraph 6. In the same paragraph,  authors explain using ""The groundtruth predictions were per-channel averaged haptic forces"" without having defined those channels (that one can guess but shouldn't). Concepts have to be clearly defined prior to their use.


","The sentiment of the review is quite negative, as indicated by phrases like 'poorly written,' 'confusing,' and 'evident weaknesses.' The reviewer points out multiple significant flaws in the paper, which contributes to the negative sentiment. Therefore, the sentiment score is -80. The politeness of the language is also quite low. The reviewer uses direct and somewhat harsh language without much cushioning, such as 'ought to better define' and 'incomplete and out of nowhere sentences.' However, it is not outright rude, so the politeness score is -60.",-80,-60
"This paper propose an extension of the Neural Theorem Provers (NTP) system that addresses the main issues of this method. The contributions of this paper allow to use this model on real-word datasets by reducing the time and space complexity of the NTP model. 

Pro:

The paper is clear and well written and the contribution is relevant to ICLR. NTP systems by combining the advantages of neural models and symbolic reasoning are a promising research direction. Even though the results presented are lower than previous studies, they present the advantage of being interpretable.

Cons:

I'm not convinced by the model used to integrate textual mentions. The evaluation proposed in section 6.3 proposes to replace training triples by textual mention in order to evaluate the encoding module. However, it seems to me that, in this particular case, these mentions are very short sentences.  This could explained why such a simplistic model that simply average word embeddings is sufficient. I wonder if this would still work for more realistic (and thus longer) sentences.

Minor issues:

-Page 1: In particular [...] (NLU) and [...] (MR) in particular, ...
","The sentiment of the review is generally positive, as indicated by the praise for the clarity, relevance, and promising direction of the research. However, there is a notable critique regarding the model used to integrate textual mentions, which slightly tempers the overall positivity. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, even when pointing out issues. Thus, the politeness score is 80.",60,80
"This paper introduces a new way of interpreting the VQ-VAE, 
and proposes a new training algorithm based on the soft EM clustering. 

I think the technical aspect of this paper is written concisely. 
Introducing the interpretation as hard EM seems natural for me, and the extension
to the soft EM training is sound reasonable. 
Mathematical complication is limited, this is also a plus for many non-expert readers. 

I'm feeling difficulties in understanding the experimental part.
To be honest, I think the experimental section is highly unorganized, not a quality for ICLR submission. 
I'm just wondering why this happens, given clean and organized technical sections...

First, I'm confusing what is the main competent in the Table 1. 
In the last paragraph of the page 6, it reads; 
""Our implementation of VQ-VAE achieves a significantly better BLEU score and faster decoding speed compared to (10).""
However, Ref. (10) is not mentioned in the Table 1. Which BLEU is the score of Ref. (10)? 

Second, terms ""VQ-VAE"", (soft?)""EM"" and ""our {model, approach}"" are used in a confusing manner. 
For example, in Table 1, below the row ""Our Results"", there are:
- VQ-VAE
- VQ-VAE with EM
- VQ-VAE + distillation
- VQ-VAE with EM + distillation

The ""VQ-VAE"" is not the proposed model, correct? 
My understanding is that the proposal is a VQ-VAE solved via soft EM, which corresponds to ""VQ-VAE with EM"". 

Third, a paragraph ""Robustness of EM to Hyperparameters"" is mis-leading. 
The figure 3 does not show the robustness against a hyperparameter. 
It shows the BLEU against the number of ""samples"" (in fact, there is no explanation about what the ""samples"" means). 
I think hyperparameters are model constants such as the learning rate of the SGD, alpha-beta params for Adam, dimension of hidden units, number of layers, etc. The number of samples are not considered as a model hyperparameter; it's a dataset property. 
The figure 5 shows the reconstructed images of the original VQ-VAE and the proposed VQ-VAE with EM. 
However, there is no explanation which hyperparameter is tested to assess ""the robustness to hyperparameters"". 

Fourth, there is no experimental report on the image reconstructions (with CIFAR and SVHN) in the main manuscript. 
In fact, there is a short paragraph that mentions about the SVHN results, 
but it only refers to the appendix. 
I think appendix is basically used for additional results or proofs, that are not essential for the main message of the paper. 
However, performance in the image reconstruction is one of the main claims written in the abstract, the intro, etc. 
So, the authors should include the image reconstruction results in the main body of the paper. 
Otherwise, claims about the image reconstructions should be removed from the abstract, etc. 


+ Insightful understanding of the VQ-VAE as hard EM clustering
+ Natural and reasonable extension to soft-EM based training of the VQ-VAE
-- Unorganized experiment section. This simply ruins the quality of the technical part. 


## after feedback

Some of my concerns are addressed the feedback. 
Considering the interesting technical parts, I raise the score upward, to the positive side. ","The sentiment of the review is mixed but leans towards positive. The reviewer appreciates the technical aspects of the paper, describing them as concise and reasonable, which suggests a positive sentiment. However, the reviewer expresses significant concerns about the experimental section, describing it as highly unorganized and not suitable for submission, which introduces a negative sentiment. Overall, the sentiment score is around 20. The politeness of the language is generally high. The reviewer uses phrases like 'I think,' 'I'm feeling difficulties,' and 'I'm just wondering,' which are polite ways to express criticism. The reviewer also provides constructive feedback and suggestions for improvement, which further indicates politeness. Therefore, the politeness score is around 80.",20,80
"This paper tries to draw connections between rate distortion theory and DNNs and use some intuitions from that domain to draw conclusions about robustness and generalization of the DNNs. 

The paper is mostly written in a storytelling narrative with very little rigor. In my opinion, this lack of rigor is problematic for a conference paper that has to be concise and rigorous. Moreover, the story is not told in a cohesive way. In most parts of the paper, there is not much relationship between the consecutive paragraphs. And even within most of the paragraphs, I was lost in understanding what the authors meant. I wish the paper would have been self-contained and made concrete definitions and statements instead of very high-level ideas that are difficult to judge. In the current state, it is very difficult for me to say what exactly is the contribution of the paper in terms of the story other than some loosely related high-level ideas. I feel like most parts the story that the authors are telling is already told by many other papers in other forms(papers that authors have cited and many other ones).


","The sentiment of the review is quite negative, as the reviewer points out several significant issues with the paper, including lack of rigor, cohesion, and clarity. The reviewer also mentions that the paper's contributions are difficult to discern and that similar ideas have been presented in other works. Therefore, the sentiment score is -80. The politeness of the language is relatively neutral; while the reviewer is critical, they do not use harsh or rude language. They express their concerns in a straightforward manner without being overly harsh or dismissive. Therefore, the politeness score is 0.",-80,0
"Revised Review:

The authors of this work has taken my concerns, and concerns of other reviewers, and revised their paper during the rebuttal period. They have increased the quality of the writing / clarity, restructured the presentation (i.e. put many details in the Appendix section), and committed to open-sourcing the platform post publication. For these reasons I believe this work is now at a state that should be published at ICLR, and I revised my score from 5 to 7. I hope other reviewers can reread the work and post their updated comments.

I'm excited about the work, because it incorporates good ideas from A-Life / evolution / open-endedness communities, to introduce new paradigms and new ways of thinking to the RL community. I look forward to using this environment in my own research going forward, regardless of whether this work gets accepted or not. Good luck!

Minor comment: On page 4, the section 5 Experiments, I think ""Technical details"" should be in bold font before the sentence ""We run each experiment using 100 worlds."" so it is distinguished from being part of that sentence.

Original Review:

The authors present a new game environment inspired by MMORPGs. The environment supports a ""massive"" number of agents, each have different neural net brains (*) and have some foraging and combat skills. They use distributed RL to train the policies (using REINFORCE) and over time can observe the dynamics of the population of these artificial life agents interact with each other, where the only reward is survival. There are many interesting insights, such as looking at how multi-agent cooperative (and deceptive) strategies emerge, and how some agents with different niche skills co-evolve with agents with other niche skills. They also plan to open source the platform and I have high hopes that this will be a fantastic research environment. While I'm very optimistic about this work and direction, there are issues with this particular paper, and I feel it is not ready for publication in its current form. While I have no doubt that the software project will be great, as a reviewer I'm evaluating this particular paper, and I want to highlight flaws about the paper and what can be done to fix it during the rebuttal/review period.

My recommendations to improve the article:

(1) Writing - I really enjoyed this work, but frankly, the writing is horrible. It took me days of effort to decipher every paragraph and understand all the terms and what is going on. The article reads like it is written by the person who programmed the game, and played MMORPGs almost every day of his childhood and adult life, so someone who is not reading the article thru the lens of the author might have an incredibly tough time digesting the content. For instance, there are sentences like ""It adds melee, ranged, and magic-based combat""... ""Melee, ranged, and magic combat have maximum Manhattan distance of effect of 1, 2, and 3 cells respectively. They do 10, 2, and 1 damage respectively""... ""This prevents uninteresting 'spawn killing' and is a common technique in human games"". These are only a small selection of samples. There are also terminology like ""#ent and #pop"" which I feel should be replaced by $N_{ent}$ and $N_{pop}$ for a paper. In contrast, older works related to population-based RL training like [2], or RL in games like [3] are examples of clear and understandable writing. I highly recommend you give the draft to someone outside of your team, who is sufficiently isolated from this project (or perhaps to a professional writer if your lab has one), to go over each paragraph, and make the writing more clear. This would benefit the work in the long term as people refer back to the paper when they run your code.

(2) Diagrams - While the diagrams look interesting, IMO they are poorly made. When I look at Figure 1, 4, and 9, it is really difficult to understand what is going on. I recommend redoing the diagrams, perhaps get some inspiration from distill.pub or OpenAI blog posts. There are things that are not clear, like what the inputs are into each agent, and how the training works. I recommend having some pseudocode snippets (like the Gym framework) to explain parts of the overall picture in more detail as figures.

Given a work of this magnitude, I'm personally okay that they went over 8 pages, as long as it is properly used for clarity.

Discussion:

Concepts from Artificial Life and Evolution has been introduced in this work. There is some confusion between what is ""learning"" and what has been ""evolved"" in your setup. Some readers coming from the evolution, or biology fields (who I bet will find your paper interesting to read and experiment with) might interpret ""learning"" to be weight changes during a life time, while ""evolution"" would be changes to the weight parameters from one generation to the next, but I think in policy-gradient RL, ""learning"" means weight changes after an agent dies and is reborn. Should consider clarifying in the introduction, the definition of learning, and whether it is inter-life or intra-life.

You cited some of Stanley's talks on open-endedness, but I wonder if you considered their work [1] where they proposed that having a minimum criteria condition might encourage diversity of solutions. For instance, perhaps in your environment, an agent doesn't have to be the very best, but only manage to survive, to move on to the next generation, which might cause very interesting multi-agent population dynamics. A parallel to modern life is that people (at least those in wealthy nations) live with such a good social safety net that people don't really have to be the best ""agent"" to reproduce and survive, and this might explain the large diverse cultures and ideas we end up with as human species, compared to other animals (where the current game is probably a suitable model of). An experiment to explore an experiment where only the very weakest agents die, but leaving agents with mediocre foraging and combat skills still live on (and pursue their own interests, whatever they may be) will be super interesting, and I encourage you to explore these ideas of open-endedness.

Bugs: In the appendix, the citation for OpenAI Five needs fixing.

Currently it pains me that I can only assign a score of 5 of this work (NOTE: this has been since revised upwards to 7 upon reading revision after rebuttal period), since I don't think the current writing is up to standards. In my opinion, it deserves a score of 7-8. If you work on points (1) and (2) and submit a revised draft with much better writing, visualization, figures to explain the work, I'll happily revise my score and improve it by 1-3 points depending on how much improvement is made.

[1] Brand and Stanley. ""Minimal Criterion Coevolution: A New Approach to Open-Ended Search"" (GECCO 2017) http://eplex.cs.ucf.edu/papers/brant_gecco17.pdf
[2] https://arxiv.org/abs/1703.03864
[3] https://arxiv.org/abs/1804.03720

(*) well, sort of, due to compute limits they are clustered to some extent to their species, so agents within a species have identical brains, unlike the real world.
","The sentiment of the revised review is positive, as the reviewer acknowledges the improvements made by the authors and expresses excitement about the work. The sentiment score is 80 because the reviewer is enthusiastic and supportive but not overwhelmingly so. The politeness score is 90 because the reviewer uses polite and encouraging language throughout the review, even when pointing out areas for improvement. The reviewer offers constructive criticism in a respectful manner and wishes the authors good luck.",80,90
"Strengths:

Well written paper, covers most of the relevant related work
Technique is conceptually easy to understand (~ adversarial training)

Weaknesses:

Unclear set of desiderata properties for a watermarking technique
No formal guarantees are verified, the mechanism is only tested
Attacks tested are not tailored to the technique proposed

Feedback and rebuttal questions:

This submission is easy to read and follow, and motivates the problem of watermarking well in light of intellectual property concerns. The technique proposed exploits unused capacity in the model to train it to associate specific inputs (computed adversarially) to specific outputs (the keys). Watermarking succeeds when the bit error rate between the predicted signature and the expected one is zero. This approach is conceptually easy to understand. 

The experimental setup used to evaluate the approach is however limited. First, it is unclear why desiderata stated in Section 3.1 and summarized in Table 1 are necessary and sufficient. Would you be able to justify their choice in your rebuttal? For instance, the “security” requirement in Table 1 overlaps with “fidelity”. Similarly, the property named “integrity” really refers to only a subset of what one would typically describe as integrity. It basically calls for a low false positive or high precision. 

The attack model described in Section 3.2 only considers three existing attacks: model fine-tuning, parameter pruning and watermark overwriting. These attacks do not consider how the adversary could adapt and they are not optimal strategies for attacking the specific defensive mechanism put in place here. For instance, could you explain in your rebuttal why pruning the smallest weights in the architecture in the final architecture would help with removing adversarial examples injected to watermark the model? Similarly, given that adversarial subspaces have large volumes, it makes sense that multiple watermarks could be inserted simultaneously and thus watermark overwriting attacks would fail.

If the approach is based on exploring unused capacity in the model, the adversary could in fact attempt to use a compression technique to preserve the model’s behavior on the task and remove the watermarking logic. For instance, the adversary could use an unlabeled set of inputs and have them labeled by the watermarked model. Because these inputs will not be “adversarial”, the watermarked model’s decision surface used to encode the signatures will remain unexplored during knowledge transfer and the resulted compressed or distilled model would solve the original task without being watermarked. Is this an attack you have considered in your experiments and if not could you elaborate why one may exclude it in your rebuttal?

Minor comments: 

P3: Typo “Verifiabiity”
P5: Could you add a reference or additional experimental results that justify why transferable keys would be located near the decision boundaries? 
","The review starts with positive comments about the paper being well-written and easy to understand, which indicates a positive sentiment. However, it also points out several weaknesses and provides detailed feedback on areas that need improvement. The language used is constructive and polite, asking questions and suggesting clarifications in a respectful manner. Therefore, the sentiment score is positive, though not extremely high due to the critical feedback, and the politeness score is high due to the respectful tone.",40,90
"This paper presents a generalization of TransE to Riemannian manifolds. While this work falls into the class of interesting recent approaches for using non-Euclidean spaces for knowledge graph embeddings, I found it very hard to digest (e.g. the first paragraph in Section 3.3). Figure 3 and 4 confused me more than helping me to understand the method. Furthermore, current neural link prediction methods are usually evaluated on FB15k and WN18. In fact, often on the harder variants FB15k-237 and WN18RR. For FB15k and WN18, Riemannian TransE seems to underperform compared to baselines -- even for low embedding dimensions, so I have doubts how useful this method will be to the community and believe further experiments on FB15k-237 and WN18RR need to be carried out and the clarity of the paper, particularly the figures, needs to be improved. Lastly, I would be curious about how the different Riemannian TransE variants compare to TransE in terms of speed?

Update: I thank the authors for their response and revision of the paper. To me, results on WN18RR and FB15k-237 are inconclusive w.r.t. to the choice of using Riemannian as opposed to Euclidean space. I therefore still believe this paper needs more work before acceptance.","The sentiment of the review is mixed but leans towards the negative side. The reviewer acknowledges the interesting approach but expresses significant confusion and doubts about the method's utility and clarity. The sentiment score is therefore -40. The politeness of the language is relatively high; the reviewer uses polite phrases such as 'I thank the authors' and 'I would be curious,' indicating a respectful tone despite the critical feedback. The politeness score is 60.",-40,60
"Summary

The paper presents a novel approach for learning a generative model where different factors of variations can be independently manipulated. The method is build upon  the GAN framework where the latent variables are divided into different subsets (chunks) which are expected to encode information about high-level factors of variation. To this end, a Siamese Network for each chunk is trained with a contrastive loss minimizing the distance between generated images sharing the same factor (the latent variables in the chunk are equal), and maximizing the distance between pairs where the latent variables differ. Given that the proposed model fails in this fully-unsupervised setting, the authors propose to add weak-supervision into the model by forcing the Siamese networks to  focus only on particular aspects of generated images (e.g, color, edges, etc..). This is achieved by applying  a basic transformation  over the input images in order to remove specific information. The evaluation of the  proposed model is carried out using the MS-Celeb dataset where the authors provide qualitative results.


Methodology

*Disentangling generative factors without explicit labels is a challenging and interesting problem. The idea of dividing the latent representation in different subsets and using a proxy task involving triplets of images has been already explored in [3]. However, the use of Siamese networks in this context is novel and sound.

*As shown in the reported results, the proposed method fails to learn meaningful factors in the unsupervised setting. However, the authors do not provide an in-depth discussion of this phenomena. Given that previous works [1,2,3] have successfully addressed this problem using a completely unsupervised approach, it would be necessary to give more insights about: (i) why the proposed method is failing (ii) why this negative result is interesting and (iii) if the method could be useful in other potential scenarios. 

*The strategy proposed to introduce weak-supervision is too ad-hoc. I agree that using cues such as the average color of an image can be useful if we want to model basic factors of variation. However, it is unclear how a similar strategy could be applied if we are interested in learning variables with higher-level semantics such as the expression of a face or its pose.

*As far as I understand, the transformations applied to the input images (e.g, edge detection) must be differentiable (given that it is necessary to backpropagate the gradient of the contrastive loss through the generator network). If this is the case, this should be properly discussed in the paper. Moreover, given that the amount of differentiable transformations is reduced, this also limits the application of the proposed method for more interesting scenarios. 

*It is not clear why the latent variables modelling the generative factors are defined using a Gaussian prior. How the case where two images have a very similar latent factor is avoided while generating pairs of images for the Siamese network?  Have the authors considered to use categorical or binary variables? The use of the contrastive loss sounds more appropriate in this case.


Experimental results

*The experimental section is too limited. First of all, only a small number of qualitative results are reported and, therefore, it is very difficult to assess the proposed method and draw any conclusion. For example, when the edge extractor is used, what kind of information is modeled by the latent variables? Is it consistent across different samples?

Moreover, it is not clear why the authors have limited the evaluation to the case where only two “chunks” are used. In principle, the method could be applied with many more subsets of latent variables and then manually inspect them to check it they are semantically meaningful (see [2]) 

*As previously mentioned, there are many recent works addressing the same problem from a fully-unsupervised perspective [1,2,3]. All these works provide quantitative results evaluating the learned representations by using them to predict real labels (e.g, attributes in the CelebA data-set). The authors could provide a similar evaluation for their method by using the feature representations learned by the siamese networks in order to evaluate how much information they convey about real factors of variation. This could clarify the advantages of the weakly-supervised strategy compared to unsupervised approaches.

Review summary

+The addressed problem (learning disentangled representations without explicit labeling) is challenging and interesting.

+The idea of using a proxy task (contrastive loss with triplets of generated images) is somewhat novel and promising.

- The authors report only negative results for the fully-unsupervised version of UD-GAN The paper lacks and in-depth discussion about why this negative result is interesting.

-The strategy proposed to provide weak-supervision to the model is too ad-hoc and it is not clear how to apply it in general applications

-The experimental section do not clarify the benefits of the proposed approach. In particular, the qualitative results are too limited and no quantitative evaluations is provided.


[1] Variational Inference of Disentangled Latent Concepts from Unlabelled Observations (Kumar et al, ICLR 2018)

[2] Beta-vae: Learning basic visual concepts with a constrained variational framework. (Higgins et. al, ICLR 2017)

[3] Disentangling Factors of Variation by Mixing Them. (Hu et. al, CVPR  2018)
","The review starts by acknowledging the novelty and soundness of the proposed method, which indicates a positive sentiment. However, it quickly transitions to pointing out significant shortcomings, such as the failure of the method in an unsupervised setting, the ad-hoc nature of the weak-supervision strategy, and the limited experimental results. The review is constructive and provides specific recommendations for improvement, which suggests a polite tone. Therefore, the sentiment score is moderately positive, and the politeness score is high.",20,80
"This paper proposed to use graph convolutional neural networks for link prediction. The authors proposed to use the dual graph to simultaneously learn node and edge embeddings. The label of the edges (positive or negative) are used as supervised signal for training the GCNs. Experiments on a few small data set prove the effectiveness of the proposed approaches.

Strength:
- important problem

Weakness:
- the novelty of the proposed method is very marginal
- the experiments are quite weak

Details:
- the novelty of the proposed method seems to be very marginal, which simply applies the GCN for link prediction. The existing GCN based method for recommendation shares similar ideas (e.g., Yin et al. 2018, PinSage), though dual hypergraph is not used. But the essential idea is very similar. 
- the data sets used in the experiments are too small
- the node embedding based methods should be compared for link prediction, e.g., DeepWalk, LINE, and node2vec.
",The sentiment of the review is slightly negative. The reviewer acknowledges the importance of the problem but criticizes the novelty and the strength of the experiments. The politeness of the language is neutral to slightly polite. The reviewer uses formal language and provides constructive feedback without being rude or overly critical.,-30,20
"The paper ‘Generative model based on minimizing exact empirical Wasserstein distance' proposes
a variant of Wasserstein GAN based on a primal version of the Wasserstein loss rather than the relying
on the classical Kantorovich-Rubinstein duality as first proposed by Arjovsky in the GAN context.
Comparisons with other variants of Wasserstein GAN is proposed on MNIST.

I see little novelty in the paper. The derivation of the primal version of the problem is already 
given in  
Cuturi, M., & Doucet, A. (2014, January). Fast computation of Wasserstein barycenters. In ICML (pp. 685-693).

Using optimal transport computed on batches rather the on the whole dataset is already used in (among
others)
 Genevay, A., Peyré, G., & Cuturi, M. (2017). Learning generative models with sinkhorn divergences. AISTATS
 Damodaran, B. B., Kellenberger, B., Flamary, R., Tuia, D., & Courty, N. (2018). DeepJDOT: Deep Joint distribution optimal transport for unsupervised domain adaptation. ECCV  

Also, the claim that the exact empirical Wasserstein distance is optimized is not true. The gradients, evaluated on 
batches, are biased. Unfortunately, the Wasserstein distance does not enjoy similar U-statistics as MMD. It is very 
well described in the paper (Section 3): 
https://openreview.net/pdf?id=S1m6h21Cb

Computing the gradients of Wasserstein on batches might be seen a kind of regularization, but it remains to be
proved and discussed.

Finally, the experimental validation appears insufficient to me (as only MNIST or toy datasets are considered).


Typos:
 Eq (1) and (2): when taken over the set of all Lipschitz-1 functions, the max should be a sup ","The sentiment of the review is quite negative, as the reviewer points out several significant issues with the paper, including a lack of novelty, incorrect claims, and insufficient experimental validation. The politeness of the language, however, is relatively high. The reviewer provides specific references and constructive criticism without using derogatory or harsh language.",-70,80
"This paper proposes some new angles to the problem of imitation learning from state only observations (not state-action pairs which are more expensive). 
Specifically, the paper proposes ""self exploration"", in which it mixes the imitation reward with environment reward from the MDP itself in a gradual manner, guided by the rate of learning.
It also proposes a couple of variants of imitation rewards, RTGD and ATD inparticular, which formulate the imitation rewards for random or exhaustive pairs of states in the observation data, as opposed to the rewards proposed in existing works (CSD, SSD), which are based on either consecutive or single states, which constitute the baseline methods for comparison.
The authors then perform a systematic experiment using a particular navigation problem on a grid world, and inspect under what scenarios (e.g. when the action spaces of the expert and learner are the same, disjoint or in a containment relationship) which of the methods perform well relative to the baselines. 
Some moderately interesting observations are reported, which largely confirm one's intuition about when these methods may perform relatively well. 
There is not very much theoretical support for the proposed methods per se, the paper is mostly an empirical study on these competing reward schemes for imitation learning.
The empirical evaluation is done in a single domain/problem, and in that sense it is questionable how far the observed trends on the relative performance of the competing methods generalizes to other problems and domains. 
Also the proposed ideas are all reasonable but relatively simple and unsurprising, casting some doubt as to the extent to which the paper contributes to the state of understanding of this area of research. ","The sentiment of the review can be considered moderately negative, as it highlights several limitations and doubts about the contribution and generalizability of the paper's findings. The reviewer acknowledges some interesting observations but questions the theoretical support and the simplicity of the proposed ideas. Therefore, the sentiment score is -40. The politeness of the language used is relatively high, as the reviewer uses phrases like 'moderately interesting observations' and 'reasonable but relatively simple,' which are polite ways to express criticism. Thus, the politeness score is 60.",-40,60
"Overview:

This paper proposed an approach for zero-shot phoneme recognition, where it is possible to recognise phonemes in a target language which has never been seen before. Rather than just training a phoneme recogniser directly on background data and then applying it to unseen data, phonetic features are first predicted, allowing phonemes not in the source language set to be predicted.

Main strengths:

The paper's main strength lies in that this is a very unexplored area that could assist in the development of speech technology where it is currently not possible. The proposed model (Section 2) has also not been considered in prior work.

Main weaknesses:

The paper's main weakness is in some of its claims and that it misses some very relevant literature. Detailed comments together with a minimal list of references are given below (but I would encourage the authors to also read a bit more broadly). But in short I do not think it is that easy to claim that this is the first paper to do zero-shot learning on speech; many of the zero-resource studies where unlabelled audio is used could be seen as doing some for of zero-shot matching. Specifically [5] is able to predict unseen phoneme targets.  Multilingual bottleneck features can be applied to languages that have never been seen before [2], and the output of phoneme recognisers trained on one language have long been applied to get output on another unseen language. The first one-shot learning speech paper [4] (to my knowledge) is also not mentioned at all. The approach in the paper also still relies on some text data from the target language; if this then can be described as ""zero-shot"" learning, then I think many of these previous studies c also make this claim.

Overall feedback:

There is definitely value in this work, but it should be much better situated within the broader literature. Below I give some editorial suggestions and also outline some suggestions for further experiments.

Detailed comments, suggestions and questions:

- Abstract: It would be useful to have some details of the ""baseline model"" here already, especially since it is such a new task.
- Introduction: ""... but they can hardly predict phones or words directly due to their unsupervised nature."" This is a strong statement that maybe requires more justification. On the one hand, the statement is true, and the high word error rates in e.g. [3] can be cited. On the other hand, it has been shown that at the phone-distinction level, these models perform quite well and sometimes outperform supervised models [1]. Since this paper also considers phone error rate as a metric, I think care should be taken with such statements.
- Introduction: ""While zero-shot learning has attracted a lot of attention in *the* computer vision community, this setup has hardly been studied in speech recognition research especially in acoustic modeling."" Definitely look at some of the studies mentioned below, and also [4] specifically.
- ""However, we note that our model can be combined with a well-resourced language model to recognize words."" How would this be done, since I think this is actually quite a challenging task.
- Section 2: ""... useful the original ESZSL architecture ..."" -> ""... useful in the original ESZSL architecture ...""
- Section 2.2: I assume the small text corpus is at the phone level (and not characters directly)? This should be clarified, and it could raise the question of whether this approach is truly ""zero-shot"".
- Section 3.2: ""We used EESEN framework ..."" -> ""We used the EESEN framework ...""
- Section 4: You could look at the recent work in [2], which uses multilingual bottleneck features trained on 10 languages and applied to multiple unseen languages. It would be interesting to also train your approach on multiple languages instead of only English.

Missing references:

1. M. Heck, S. Sakti, and S. Nakamura, ""Feature Optimized DPGMM Clustering for Unsupervised Subword Modeling: A Contribution to Zerospeech 2017,"" in Proc. ASRU, 2017.
2. E. Hermann and S. J. Goldwater, ""Multilingual bottleneck features for subword modeling in zero-resource languages,"" in Proc. Interspeech, 2018.
3. H. Kamper, K. Livescu, and S. Goldwater, An embedded segmental k-means model for unsupervised segmentation and clustering of speech,"" in Proc. ASRU, 2017.
4. B. M. Lake, C.-Y. Lee, J. R. Glass, and J. B. Tenenbaum, ""One-shot learning of generative speech concepts,"" in Proc. CogSci, 2014.
5. O. Scharenborg, F. Ciannella, S. Palaskar, A. Black, F. Metze, L. Ondel, and M. Hasegawa-Johnson, ""Building an ASR system for a low-resource language through the adaptation of a high-resource language asr system: Preliminary results,""in Proc. ICNLSSP, 2017.

Edit: Based on the rebuttal I've changed my rating from 4 to 5.","The sentiment of the review is mixed but leans towards positive. The reviewer acknowledges the novelty and potential impact of the work, which is a significant strength. However, they also point out several weaknesses, including unsubstantiated claims and missing relevant literature. The sentiment score is therefore 20, indicating a slightly positive but cautious outlook. The politeness of the language is high; the reviewer provides constructive criticism and suggestions for improvement without being harsh or dismissive. The language is respectful and encouraging, leading to a politeness score of 80.",20,80
"
Summary:
========
Theis paper proposes a method for learning decomposable representations in the context of a language modeling task. Using holographic reduced representations (HRR), a word embedding is composed of a role and a filler. The embedding is then fed to an LSTM language model. There is also an extension to chunk-level representations. Experimentally, the model achieves perplexity comparable to a (weak) baseline LSTM model. The analysis of the learned representations shows a separation into syntactic and semantic roles. 

The paper targets an important problem, that of learning decomposable representations. As far as I know, it introduces a novel perspective using HRR and does so in the context of language modeling, which is a core NLP task. The analysis of the learned representations is quite interesting. I do have some concerns with regards to the quality of the language model, the clarity of some of the model description, and the validity of using HRR in this scenario. Please see detailed comments below. 

Comments:
=========
1. Section 2 refers to Plate (1995) for the conditions when the approximate decoding via correlation holds. I think it's important to mention these conditions and discuss whether they apply to the language modeling case. In particular, Plate mentions that the elements of each vector need to be iid with mean zero and variance 1/n (where n is the length of the vector). Is this true for the present case? Typically, word embeddings and LSTM states are do not exhibit this distribution. Are there other conditions that are (not) met?
2. Learning separate bases for different role-filler bindings is said to encourage the model to learn a decomposition of word representation. On the other hand, if I understand correctly, this means that word embeddings are not shared between roles, because s^w_i is also a role-specific vector (not just a word-specific vector). Is that a cause of concern? 
3. It's not clear to me where in the overall model the next word is predicted. Figure 1b has an LSTM that predicts filler embeddings. Does this replace predicting the next word in a vanilla LSTM? Equation 5 still computes a word score. Is this used to compute the probability of the next word as in equation 2?  
4. Comparison to other methods for composing words. Since much of the paper is concerned with composing words, it seem natural to compare the methods (and maybe some of the results) to methods for composing words. Some examples include [2] and the line of work on recursive neural networks by Socher et al., but there are many others. 
5. Perplexity results:
- The baseline results (100.5 ppl on PTB) are very weak for an LSTM. There are multiple papers showing that a simple LSTM can do much better. The heavily tuned LSTM of [1] gets 59.6 but even less tuned LSTMs go under 80 or 80 ppl. See some results in [1]. This raises a concern that the improvements from the HRR model may not be significant. Would they hold in a more competitive model? 
- Can you speculate or analyze in more detail why the chunk-level model doesn't perform well, and why adding more fillers doesn't help in this case? 
6. Motivation: 
- The introduction claims that the dominant encoder-decoder paradigm learns ""transformations from many smaller comprising units to one complex emedding, and vice versa"". This claim should be qualified by the use of attention, where there is not a single complex embedding, rather a distribution over multiple embeddings. 
- Introduction, first paragraph, claims that ""such crude way of representing the structure is unsatisfactory, due to a lack of transparency, interpretability or transferability"" - what do you mean by these concepts and how exactly is the current approach limited with respect to them? Giving a bit more details about this point here or elsewhere in the paper would help motivate the work. 
7. Section 3.3 was not so clear to me:
- In step 1, what are these r_i^{chunk}? Should we assume that all chunks have the same role embeddings, despite them potentially being syntactically different? How do you determine where to split output vectors from the RNN to two parts? What is the motivation for doing this?
- In prediction, how do you predict the next chunk embedding? Is there a different loss function for this? 
- Please provide more details on decoding, such as the mentioned annealing and regularization. 
- Finally, the reliance on a chunker is quite limiting. These may not be always available or of high quality. 
8. The analysis in section 4.3 is very interesting and compelling. Figure 2 makes a good point. I would have liked to see more analysis along these lines. For example, more discussion of the word analogy results, including categories where HRR does not do better than the baseline. Also consider other analogy datasets that capture different aspects. 
9. While I agree that automatic evaluation at chunk-level is challenging, I think more can be done. For instance, annotations in PTB can be used to automatically assign roles such as those in table 4, or others (there are plenty of annotations on PTB), and then to evaluate clustering along different annotations at a larger scale. 
10. The introduction mentions a subset of the one billion word LM dataset (why a subset?), but then the rest of the papers evaluates only on PTB. Is this additional dataset used or not? 
11. Introduction, first paragraph, last sentence: ""much previous work"" - please cite such relevant work on inducing disentangled representations.
12. Please improve the visibility of Figure 1. Some symbols are hard to see when printed. 
13. More details on the regularization on basis embeddings (page 4) would be useful. 
14. Section 3.3 says that each unique word token is assigned a vectorial parameter. Should this be word type? 
15. Why not initialize the hidden state with the last state from the last batch? I understand that this is done to assure that the chunk-level models only consider intra-sentential information, but why is this desired? 
16. Have you considered using more than two roles? I wonder how figure 2 would look in this case. 


Writing, grammar, etc.:
====================== 
- End of section 1: Our papers -> Our paper
- Section 2: such approach -> such an approach; HRR use -> HRR uses; three operations -> three operations*:*
- Section 3.1: ""the next token w_t"" - should this be w_{t+1)? 
- Section 3.2, decoding: remain -> remains 
- Section 3.3: work token -> word token 
- Section 4.1: word analogy task -> a word analogy task; number basis -> numbers of basis
- Section 4.2: that the increasing -> that increasing 
- Section 4.3: no space before comma (first paragraph); on word analogy task -> on a word analogy task; belong -> belongs
- Section 4.4: performed similar -> performed a similar; luster -> cluster 
- Section 5: these work -> these works/papers/studies; share common goal -> share a common goal; we makes -> we make; has been -> have been  

References
==========
[1] Melis et al., On the State of the Art of Evaluation in Neural Language Models
[2] Mitchell and Lapata, Vector-based Models of Semantic Composition
","The review starts with a positive sentiment, acknowledging the importance of the problem addressed by the paper and the novelty of the approach. However, it quickly transitions to expressing concerns about the quality of the language model, clarity of the model description, and the validity of using HRR in this scenario. The detailed comments are constructive and aimed at improving the paper, but they highlight several significant issues. The language used is polite and professional, with suggestions and questions framed in a way that encourages the authors to consider improvements without being dismissive or rude.",20,80
"The paper proposes to integrate sequential information into imitation learning techniques.  The assumption is that mostly all the IL techniques are learning a policy which depends on state at time t, while the information contained in this state may be not sufficient to choose the right action (actually, this is the POMDP setting, the notion of POMDP not appearing in the paper....). The authors thus propose to use a recurrent neural network to encode the state by aggregating past information, instead of just using the features of the state at time t. They thus instantiate this idea on different methods and show that, on some problems, this approach can increase the quality of the final policy.

Actually, the contribution of the paper is a simple extension of existing methods: using a RNN instead of a simple NN in imitation learning models. First of all, when dealing with classical environments such as Atari, many papers propose to use the last N frames as a state encoding (instead of the last frame), following the same intuition. The studied setting thus corresponds to the PO-MDP case and using a RNN in POMDP is for example what is done in  [Merel etal. 2017]. Moreover, the problem of imitation learning (and particularly inverse RL) in POMDP has been of the interest of many papers like [Choi et al. 2008] for instance and many more, and it is unclear what is the positioning of this paper w.r.t existing works. Since the paper proposes just to encode history with a RNN, the proposed solution lacks of originality, and the contribution of the paper in term of model is quite low.  But the authors explain how this can be instantiated in three different settings (IRL, GAIL and BC) -- note that the section concerning the use of Adaboost is not clear and could be better described -- which can be of the interest of the community. 
Concerning the experiments, I don't understand what is the split between training and testing data. Is it pairs of state-action coming from the experts ? or trajectories ? Moreover, I don't understand why these environments correspond to POMDP cases and the authors have to give details on that. For instance, mountain-car is clearly not a POMDP problem in its classical shape, nor Acrobot. As if, it makes the experiments very difficult to reproduce. The interest of using the RNN to encode history does not seem clear for each of the cases since it often degrades the final performance, so I don't know exactly what insights I can extract from the paper.

Pro:
* The approach is proposed for IRL, GAIL and BC

Cons:
* Lack of positionning w.r.t POMDP litterature
* Lack of details in the experiments, and lack of good experimental results
* Low contribution in term of model


[Merel et al. 2017]  Learning human behaviors from motion capture
by adversarial imitation
[Choi et al.] Inverse Reinforcement Learning in Partially Observable
Environments","The sentiment of the review is moderately negative. The reviewer acknowledges the potential interest of the approach for the community but criticizes the lack of originality, insufficient positioning with respect to existing literature, and unclear experimental details. The sentiment score is -40 because the review points out several significant shortcomings while still recognizing some positive aspects. The politeness score is 20 because the language used is mostly neutral and professional, though it does contain some critical remarks that could be perceived as slightly blunt.",-40,20
"Here the authors convert the GRU equations into continuous time and use theory and experiments to study 1- and 2-dimensional GRU networks. The authors showcase every variety of dynamical topology available in these systems and point out that the desirable line and ring attractors are not achievable, except in gross approximation.  The paper is extremely well written.

I am deeply conflicted about this paper.  Is the analysis of 1 or 2 dimensional GRUs interesting or significant? That’s a main question of this paper.  There is no question of quality, or clarity, and I am reasonably certain nobody has analyzed the GRU in this way before.

On the one hand, the authors bring a rigor and language to the discussion of recurrent networks that is both revealing (for these examples) and may to bear fruit in the future.  On the other hand, the paper is exclusively focused on 1- and 2-dimensional examples which have precisely no relevance to the recurrent neural networks as used and studied by machine learning practitioners and researchers, respectively. If the authors have proved something more general for higher dimensional (>2) cases, they should make it as clear as possible.
 
A second, lesser question of relevance is studying a continuous time version.  It is my understanding that discrete time dynamics may exhibit significantly more complex dynamical phenomenon and again practitioners primarily deploy discrete time GRUs.  I understand that theoretical progress often requires retreating to lower dimensionality and (e.g. linearization, etc.) but in this case it is not clear to me that the end justifies the means.  On the other hand, a publication such as this will not only help to change the language of RNNs in the deep learning community, but also potentially bring in more dynamical systems specialists into the deep learning field, which I thoroughly endorse.

Moderate concern

“In order to show this major limitation of GRUs …” but then a 2-gru is used, which means that it’s not a general problem for GRUs with higher dim, right?  Also, won’t approximate slow points would also be fine here? I think this language needs to be more heavily qualified.

Minor

GRU almost always refers to the network, even though it is Gated Recurrrent Unit, this means that when you write ‘two GRUs’, the naive interpretation (to me) is that you are speaking about two networks and not a GRU network with two units.

Side note requiring no response: It might be interesting to study dynamical portrait as a function of training for the two-d GRU.
","The sentiment of the review is mixed. The reviewer acknowledges the quality and clarity of the paper and appreciates the rigor and potential future impact of the work, which suggests a positive sentiment. However, the reviewer also expresses significant concerns about the relevance and significance of the analysis, particularly its focus on 1- and 2-dimensional GRUs, which are not commonly used in practice. This introduces a negative sentiment. Overall, the sentiment score is balanced between these positive and negative aspects. The politeness of the language is high. The reviewer uses polite and respectful language throughout, even when expressing concerns and providing recommendations. The tone is constructive and professional, aiming to help the authors improve their work.",0,80
"This paper proposes that models for different tasks in multi-task learning cannot only share hidden variables but also gradients.

Pros:
- The overall framework is theoretically motivated and intuitive. The idea of passing gradients for multi-task learning is interesting and the execution using fast weights is plausible.
- The experiments are extensive and cover three different task combinations in different domains.
- The results are convincing and the additional analyses are compelling.

Cons:
- I would have liked to see a toy example or at least a bit more justification for the ""pretend-to-share"" problem that models ""collect all the features together into a common space, instead of learning shared rules across different tasks"". As it is, evidence for this seems to be mostly anecdotal, even though this forms the central thesis of the paper.
- I found the use of Read and Write ops confusing, as similar terminology is widely used in memory-based networks (e.g. [1]). I would have preferred something that makes it clearer that updates are constrained in some way as ""writing"" implies that the location is constrained, rather than the update minimizing a loss.

Questions:
- How is the weight list of task similarities \beta learned when the tasks don't share the same output space? How useful is the \beta?
- Could you elaborate on what is the difference between pair-wise gradient passing (PGP) and list-wise gradient passing (LGP)

[1] Graves, A., Wayne, G., & Danihelka, I. (2014). Neural turing machines. arXiv preprint arXiv:1410.5401.","The sentiment of the review is generally positive, as indicated by the praise for the theoretical motivation, extensive experiments, and convincing results. However, there are some critical points raised, such as the need for more justification for the 'pretend-to-share' problem and confusion around terminology. The politeness of the language is high, as the reviewer uses polite and constructive language to provide feedback and ask questions.",70,90
"This paper introduces an approach to pruning the parameters of a trained neural network. The idea is inspired by the Optimal Brain Surgeon method, which relies on second derivatives of the loss w.r.t. the network parameters. Here, the corresponding Hessian matrix is approximated using the Fisher information to make the algorithm scalable to very deep networks.

Strengths:
- The method does not require hyper-parameter tuning.
- The results show the good behavior of the approach.

Weaknesses:

Novelty:
- In essence, this method relies on the work of Marten & Grosse to approximate the Hessian matrix used in the Optimal Brain Surgeon strategy. This is fine, but not of great novelty.

Method:
- It is not clear to me why the notion of binary parameters gamma is necessary. Instead of varying the gammas from 1 to 0, why not directly zero out the corresponding network parameters w?
- In essence, the objective function in Eq. 5 adds an L_1 penalty on the gamma parameters, which would be related to an L_1 penalty on the ws. Note that this strategy has been employed in the past, e.g., Collins & Kohli, 2014, ""Memory Bounded Deep Convolutional Networks"".
- It is not clear to me how zeroing out individual parameters will truly allows one to reduce the model afterwards. In fact, one would rather want to remove entire rows or columns of the matrix W_l, which would truly correspond to a smaller model. This was what was proposed by Wen et al., NIPS 2016 and Alvarez & Salzmann, NIPS 2016, ""Learning the Number of Neurons..."".
- In the past, when dealing with the Hessian matrix, people have used the so-called Pearlmutter trick (Pearlmutter, Neural Computation 2014, ""Fast exact multiplication by the Hessian"". In fact, in this paper, the author mentions the application to the Optimal Brain Surgeon strategy. Is there a benefit of the proposed approach over this alternative strategy?

Experiments:
- While the reported compression rates are good, it is not clear to me what they mean in practice, because the proposed algorithm zeroes out individual parameters in the matrix W_l of each layer.  This does not guarantee entire channels to be removed. As such, I would not know how to make the model actually smaller in practice. It would seem relevant to show the true gains in memory usage and in inference speed (both measured on the computer, not theoretically).

Summary:
I do appreciate the fact that the proposed method does not require hyper-parameters and that it seems to yield higher compression rates than other pruning strategies that act on individual parameters. However, novelty of the approach is limited, and I am not convinced of its actual benefits in practice.
","The sentiment of the review is mixed. The reviewer acknowledges some strengths of the paper, such as the lack of need for hyper-parameter tuning and good compression rates. However, the reviewer also points out several weaknesses, particularly regarding the novelty and practical benefits of the approach. Therefore, the sentiment score is slightly negative. The language used in the review is polite and constructive, with the reviewer providing specific feedback and references to related work, which indicates a high level of politeness.",-20,80
"I do not understand the denomination of nonlinearity coefficient provided in definition 1: although the quantity indeed does equal to 1 under whitened data distribution or orthogonal matrix, the conjecture that it should be close to 1 does not seem to be close at all just under any data distribution. Using a similar construction that section 6, we can rescale a whitened input data with a diagonal matrix D with components all equal to one except for a very large one \lambda and also multiply the input weights by D^{-1} to compensate (and have a similar function). If you look at such construction for the linear case with identity initialization of A, the NLC is sqrt((\lambda^2 + n - 1) (\lambda^{-2} + n - 1)) / n which can grow arbitrarily large with \lambda *for a linear model*. However, because of its low capacity, we would expect a linear model to have reasonable generalization. This seems to compromise the initial NLC being low as a necessary condition for reasonable generalization. 
Conversely, it’s possible to initialize arbitrarily large residual networks such that the resulting initial function is linear (by initializing the output weight of the incrementing block to 0). This initialization may also be done such that the initial NLC becomes close to 1. I would not think this wouldn’t necessarily result in good generalization, which seems to agree with the experimental observation. 
Now given that this initial NLC is neither sufficient nor necessary to predict generalization, one can wonder what is correlating generalization and NLC together in the experiment section. Same remark applies to the correlation between nonlinearity and NLC. This is especially concerning since in the linear case, the NLC can vary whether we chose to whiten the data or not for example, so the other influencing factors need to be discovered. What were the architecture that resulted in small/high NLC?
The experiment section still contains interesting bits, such as successful training of very deep architecture that are very sensitive to input perturbations but they are not part of the main thread of the paper.","The sentiment of the review appears to be neutral to slightly negative. The reviewer expresses confusion and skepticism about the validity of the nonlinearity coefficient (NLC) as a predictor of generalization. They provide detailed counterexamples and express concerns about the experimental results. However, the reviewer also acknowledges that the experiment section contains interesting bits, which prevents the sentiment from being entirely negative. Therefore, the sentiment score is -20. The politeness of the language is neutral to slightly polite. The reviewer uses formal and respectful language, avoiding any rude or harsh terms. They provide constructive criticism and ask questions in a professional manner. Therefore, the politeness score is 20.",-20,20
"The paper presents an empirical study of how accuracy and robustness vary with increasing training data for four different data sets and CNN architectures. The main conclusion of the study is that while training accuracy generally increases with increasing training data, provided sufficient training data is available for training the network in the first place, the robustness on the other hand does not necessarily increase, and may even decrease.

Similar findings were presented previously in Su et al., 2018. Hence, the current paper contains incremental and marginal new findings versus the existing literature. The paper would also have been a lot stronger and significantly advanced our scientific understanding of the problem if the authors had made some attempt at trying to explain their findings theoretically. In its current form the paper does not contain sufficient contributions for acceptance.","The sentiment of the review is slightly negative, as it points out that the findings are incremental and marginal compared to existing literature and that the paper lacks sufficient contributions for acceptance. This is reflected in the sentiment score of -40. The politeness of the language used is relatively high, as the reviewer provides constructive criticism without using harsh or rude language, and suggests ways the paper could be improved. This is reflected in the politeness score of 80.",-40,80
"This paper proposed another GAN-based PU learning method. The mathematics in this paper is not easy to follow, and there are many other critical issues.

*****

The clarity is really an issue. First of all, I cannot easily follow the meanings behind the equations. I guess the authors first came up with some concrete implementation and then formalize it into an algorithm. Given the current version of the paper, I am not sure whether this clarity of equations can be fixed without an additional round of review or not.

Moreover, the logic in the story line is unclear to me, especially the 3rd paragraph that seems to be mostly important in the introduction. There are two different binary classification problems, of separating the positive and negative classes, and of separating the given and generated data. I cannot see why the generated data can serve as negative data. This paragraph is discussing GenPU, PGAN and the proposed method, and consequently the motivation of the current paper does not make sense at least to me.

*****

The paper classified PU learning methods into two categories, one-stage methods and two-stage methods. This is interesting. However, before that, they should be classified into two categories, for censoring PU learning and for case-control PU learning. The former problem setting was proposed very early and formalized in ""learning classifiers from only positive and unlabeled data"", KDD 2008; the latter problem setting was proposed in ""presence-only data and the EM algorithm"", Biometrics 2009 and formalized in ""analysis of learning from positive and unlabeled data"", NIPS 2014. Surprisingly, none of these 3 papers was cited. By definition, GAN-based PU learning belongs to the latter problem setting while Rank Prune can only be applied to the former but was included as a baseline method.

The huge difference between these two settings and their connections to learning with noisy labels are known for long time. To be short, class-conditional noise model corrupts P(Y|X) and covers censoring PU, mutual contamination distribution framework corrupts P(X|Y) and covers case-control PU, and mathematically mutual contamination distribution framework is more general than class-conditional noise model and so is case-control PU than censoring PU. See ""learning from corrupted binary labels via class-probability estimation"", ICML 2015 for more information where the above theoretical result has been proven. An arXiv paper entitled ""on the minimal supervision for training any binary classifier from only unlabeled data"" has some experimental results showing that methods for class-conditional noise model cannot handle mutual contamination distributions. The situation is similar when applying censoring PU methods to case-control PU problem setting.

Furthermore, the class-prior probability pi is well-defined and easy to estimate in censoring PU, see ""learning classifiers from only positive and unlabeled data"" mentioned above. However, it is not well-defined in case-control PU due to an identifiability issue described in ""presence-only data and the EM algorithm"" mentioned above. Thus, the target to be estimated is defined as the maximal theta such that theta*P(X|Y)<=P(X) following ""estimating the class prior and posterior from noisy positives and unlabeled data"", NIPS 2016. BTW, ""mixture proportion estimation via kernel embedding of distributions"" is SOTA in class-prior estimation; the previous NIPS paper was written earlier and accepted later.

In summary, as claimed in the paper and shown in Table 1 in the introduction, all discriminative PU methods and GenPU require to know pi for learning. This is true, but this is because they are designed for a more difficult problem setting---learning classifiers and estimating pi are both more difficult. Lacking some basic knowledge of PU learning is another big issue.

*****

The novelty is to be honest incremental and thus below the bar of ICLR. The significance is similarly poor, due to that the experiments mixed up methods for censoring PU and those for case-control PU. What is more, F1-score is a performance measure for information retrieval rather than binary classification. We all know GANs are pretty good at MNIST but not CIFAR-10. In fact, GenPU has a critical issue of mode collapse, and this is why GenPU reports 1-vs-1 rather than 5-vs-5 on MNIST. Even though, I still think GenPU makes much more sense than PGAN and D-GAN.","The sentiment of the review is quite negative, as the reviewer points out several critical issues with the paper, including clarity, logic, and lack of basic knowledge in PU learning. The reviewer also mentions that the novelty and significance of the paper are below the standards of ICLR. The politeness of the language is relatively neutral; while the reviewer is critical, they do not use rude or offensive language. They provide detailed feedback and suggestions for improvement, which indicates a professional tone.",-80,10
"
MuMoMAML: Model-Agnostic Meta-Learning for Multimodal Task Distributions

This paper proposed multi-modal MAML, which alleviates the single initialization limitation of MAML by modulating task prior with MAML. Below are some comments.

Pros:
1. Overall, the paper is clear written. 
2. By using modulation, there is no need to explicitly control/know the number of modes in advance.
3. The multi-MAML baseline is good for an ablation study, though it is only on a synthetic regression task.
4. MUMOMAML combines the strength of both gradient-based and model-based meta-learners.

Cons.
1. The novelty of the paper seems to be the combinations of MAML and FiLM, which seems a bit limited.
2. I wonder whether the proposed method is mostly useful when there is a clear mode difference as in the synthetic regression/RL tasks of the paper. Moreover, the paper only shows tasks with only two-three modes, what happen when there is a large number of modes?
3. What's the results on the mini-Imagenet? The Omniglot seems to be saturated already.
4. Why tau is not updated in the inner loop of Algorithm 1?

Minor:
1. page 4, 'in to' -> 'into'
2. In page 5, in 'based on the input data samples and then
infers the parameter to modulate the prior model', what does the `input data samples' refers to? Is it the training data of a meta-learning task?
3. Do you stop gradient to the learner in MUMOMAML?","The sentiment of the review is moderately positive. The reviewer acknowledges the clarity of the paper and the strengths of the proposed method, but also points out several limitations and areas for improvement. The sentiment score is 40 because the positive aspects are noted but are balanced by significant concerns. The politeness of the language is quite high, as the reviewer uses polite language and constructive criticism without being harsh or dismissive. The politeness score is 80.",40,80
"Post-rebuttal update:
The authors have clarified their main messages, and the paper is now less vague about what is being investigated and the conclusions of the experiments. The same experimental setup has been extended to use CIFAR-10 as an additional, more realistic dataset, the use of potentially more powerful LSTMs as well as GRUs, and several runs to have more statistically significant results - which addresses my main concerns with this paper originally (I would have liked to see a different experimental setup as well to see how generalisable these findings are, but the current level is satisfying). Indeed, these different settings have turned up a bit of an anomaly with the GRU on CIFAR-10, which the authors claim that they will leave for future work, but I would very much like to see addressed in the final version of this paper. In addition some of the later analysis has only been applied under one setting, and it would make sense to replicate this for the other settings (extra results would have to fit into the supplementary material).

I did spot one typo on page 4 - ""exterme"", but overall the paper is also better written, which helps a lot. I commend the authors on their work revising this paper and will be upgrading my rating to accept.

---

The authors investigate the hidden state dynamics of RNNs trained on a single task that mixes (but clearly separates) pattern recognition and memorisation. The authors then introduce two curricula specific to the task, and study how the trained RNNs behave under different deviations from the training protocol (generalisation). They show that under the curriculum that exhibited the best generalisation, there exist more robust (persisting for long time periods) fixed/slow points in the hidden state dynamics. They then extend the optimisation procedure developed by Sussillo & Barak for continuous-time RNNs in order to find these points. Finally, they use this method to track the speed of these points during the course of training, and link spikes in speed to one of the curricula which introduces new classes over time.

Understanding RNNs - and in particular how they might ""generalise"" - is an important topic of research. As done previously, studying RNNs as dynamical systems is a principled way to do so. In this line of work some natural objects to look into are fixed points and even slow points (Sussillo & Barak) - how long they can persist, and how large the basins of attraction are. While I believe the authors did a reasonable job following this through, I have some concerns about the experimental setup. Firstly, only one task is used - based on object classification with images - so it is unclear how generalisable these findings are, given that the authors' setup could be extended to cover at least another task, or at least another dataset. MNIST is a sanity check, and many ideas may fail to hold when extended to slightly more challenging datasets like CIFAR-10.

Secondly, as far as I can tell, the results are analysed on one network per setting, so it is hard to tell how significant the differences are. While some analyses may only make sense for single networks, e.g. Figure 3, a proper quantification of some of the results over several training runs would be appropriate.

Finally, it is worth investigating LSTMs on this task. This is not merely because they are more commonly used than GRUs, but they are strictly more powerful - see ""On the Practical Computational Power of Finite Precision RNNs for Language Recognition"", published at ACL 2018. Given the results in this paper and actually the paper that first introduces the forget gate for LSTMs, it seems that performing these experiments solely with GRUs might lead to wrong conclusions about RNNs in general.

There are also more minor spelling and grammatical errors throughout the text that should be addressed. For example, there is a typo on the task definition on page 2 - ""the network should *output* a null label.""","The sentiment of the review is generally positive, as the reviewer acknowledges the improvements made by the authors and commends their efforts. The sentiment score is set at 70 because while the reviewer is pleased with the revisions, they still express some concerns and suggest further improvements. The politeness score is 90 because the language used is very polite and constructive, with phrases like 'I commend the authors' and 'I would very much like to see addressed,' indicating a respectful and encouraging tone.",70,90
"UPDATE (after author response):

Thank you for updating the paper, the revised version looks better and the reviewers addressed some of my concerns. I increased my score.

There's one point that the reviewers didn't clearly address:  ""It might be worth evaluating the usefulness of the method on higher-dimensional examples where the analytic forms of q(x|z) and q(z) are known, e.g. plot KL between true and estimated distributions as a function of the number of dimensions."" Please consider adding such an experiment.

The current experiments show that the method works better on low-dimensional datasets, but the method does not seem to be clearly better on more challenging higher dimensional datasets.  I agree with Reviewer1 that ""Perhaps more ambitious applications would really show off the power of the model and make it standout from the existing crowd."" Showing that the method outperforms other methods would definitely strengthen the paper.

Section 5.4: I meant error bars in the numbers in the text, e.g. 13 +/- 5.

---------

The paper proposes a new loss for training deep latent variable models. The novelty seems a bit limited, and the proposed method does not consistently seem to outperform existing methods in the experiments. I'd encourage the authors to add more experiments (see below for suggestions) and resubmit to a different venue.

Section 4:
- q(z) seems to be undefined. Is it the aggregated posterior?
- How is equation (1) related to ELBO that is used for training VAEs?

Some relevant references are missing: I’d love to see a discussion of how this loss relates to other VAE-GAN hybrids.

VEEGAN: Reducing mode collapse in GANs using implicit variational learning
https://arxiv.org/pdf/1705.07761.pdf

Distribution Matching in Variational Inference
https://arxiv.org/pdf/1802.06847.pdf


Section 5.1:
- The quantitative comparison measures MSE in pixel space and inception score, neither of which are particularly good measures for measuring the quality of how well the conditionals match. I’d encourage the authors to consider other metrics such as log-likelihood.

- It might be worth evaluating the usefulness of the method on higher-dimensional examples where the analytic forms of q(x|z) and q(z) are known, e.g. plot KL between true and estimated distributions as a function of the number of dimensions.

Section 5.4: 
- The error bars seem quite high. Is there a reason why the method cannot reliably reduce mode collapse?

Minor issues:
- CIFAT-10 -> CIFAR-10
","The sentiment of the review is mixed but leans towards the positive side. The reviewer acknowledges improvements in the revised version and appreciates that some concerns were addressed, which is a positive sentiment. However, the reviewer still points out significant issues and suggests further experiments and improvements, indicating some level of dissatisfaction. Therefore, the sentiment score is moderately positive. The politeness of the language is high; the reviewer uses polite phrases such as 'Thank you,' 'I'd encourage,' and 'Please consider,' which indicates a respectful and constructive tone.",30,80
"Quality: 
- In 4.4, the authors have vigorously explored the space of hyperparameters. However, they do not describe how to determine the hyperparameters, e.g., set aside a validation set from a part of the training set and determine the hyperparameters using this validation set, while the authors split the two datasets into only training and test sets, respectively. Without this procedure, the results may overfit to the test set via repeated experiments. Even though the used datasets are of few-million, this procedure guarantees a minimum requirement for a reliable outcome from the proposed model. I firmly recommend the authors to update their results using a validation set to determine the hyperparameters and then report on the test set. Please describe these experimental details to ensure that the performed experiments are valid.
   
Clarity:
- Overall, the writing can be improved via proof-reading and polishing the sentences. In Introduction section, ""there is little work applying..."" can be specified or rephrased with ""it is underexplored to apply"", and ""input features are not independent"" can be specified on what there are not independent. Moreover, the last two sentences in the second paragraph in the Introduction section is unclear what the authors want to argue: ""The combinations in linear models are then made by cross product over different fields. Due to the sparsity problem, the combinations rely on much manual work of domain experts.""
- The authors use top-k restriction (Shazeer et al., 2017) to consider sparse relationships among the features. For this reason, have you tried to use the L1 loss on the probability distributions, which are the outputs of softmax function?
- In 4.5, the authors said, they ""are in most concern of complementarity."" What is the reason for this idea and why not the ""relevance""?
- In Table 4, I'm afraid that I don't understand the content (three numbers in parenthesis) of the third column. How does each input x_i or x_j, or a tuple of them get their own CTR?

Originality and significance:
- They apply self-attention to learn multiple categorical features to predict Click-Through-Rate (CTR) with a top-k non-zero similarity weight constraint to adapt to their categorical inputs. Due to this, the scientific contribution to the corresponding community is highly limited to providing empirical results on the CTR task.
- The authors argue that ""most of current DNN-based models simply concatenate all feature embeddings""; however, this argument might be an over-simplified statement for the existing models in section 2.
- Similar works can be found but missed to cite: [1] proposes a general framework to self-attention to exploit sequential (time-domain) and parallel (feature-domain) non-locality. [2] learns bilinear attention maps to integrate multimodal inputs using skip-connections and multiple layers on top of the idea of low-rank bilinear pooling.

Pros:
- Strong empirical results on two CTR tasks using the previous works of self-attention and top-k restriction techniques.

Cons:
- This work fairly lacks its originality since the proposing method heavily relies on the two previous works, self-attention and top-k restriction. They apply them to multiple categorical features to estimate CTR; however, their application seems to be monotonic without a novel idea of task-specific adaptation.

Minor comments:
- In Figure 1, ""the number of head"" -> ""the number of heads"".


[1] Wang, X., Girshick, R., Gupta, A., & He, K. (2018). Non-local Neural Networks. In IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'18).
[2] Kim, J.-H., Jun, J., & Zhang, B.-T. (2018). Bilinear Attention Networks. In Advances in Neural Information Processing Systems 32 (NIPS'18).","The sentiment of the review appears to be slightly negative. The reviewer points out several significant issues with the methodology, clarity, and originality of the work. They recommend substantial changes, such as updating results using a validation set and improving the writing. However, the review does acknowledge strong empirical results, which prevents the sentiment from being entirely negative. Therefore, the sentiment score is -40. The politeness of the language is quite high. The reviewer uses polite phrases like 'I firmly recommend,' 'please describe,' and 'can be improved,' which indicates a respectful tone despite the critical feedback. Thus, the politeness score is 80.",-40,80
"This paper proposes a justification to one observation on VAE: ""restricting the family of variational approximations can, in fact, have a positive regularizing effect, leading to better generalization"". The explanation given in this work is based on Gaussian mean-field approximation.

I had trouble to understand some parts of this paper, since some of the sentences do not make sense to me. For example

- the sentence under eq. (2)
- the sentence ""Bacause the identity of the datapoint can never be learned by ..."" What is the identity of a dat point?

It looks like section 2.1 wants to show the connections between eq. (2) and other popularly used inference methods. Somehow, those connections are not clear to me.

Besides some issues in the technical details, the major problem of this paper is that it uses the data processing inequality (DPI) in a **wrong** way.

As in (Cover and Thomas, 2012), which is also cited in this paper, DPI is defined on a Markov chain X -> Y -> Z and we have I(X,Y) >= I(X,Z). 

However, based on the definition of \theta and \tilde{\theta} given in the first sentence of section 2.3, the relation between \theta, \tilde{\theta} and D should be: D <- \theta -> \tilde{\theta} (if it is a generative model) or D -> \theta -> \tilde{\theta} (if a discriminative model). Either case, I don't think we can have the inequality in eq. (5).  ","The sentiment of the review is generally negative, as it highlights several issues with the paper, including difficulty in understanding certain parts, unclear connections, and a major problem with the use of the data processing inequality. The politeness of the language is relatively neutral, as the reviewer points out specific issues without using overly harsh or rude language.",-70,10
"Summary:
The paper presents a novel combinatorial search algorithm for the discrete target propagation framework developed in Friesen & Domingos (2018). Experiments on small datasets with small models demonstrate some potential for the proposed approach; however, scalability remains a concern.

  Pros:
-	I like the goal of the work and think that if the targeted problem were to be solved it would be an interesting contribution to the field.
-	The proposed search algorithm is reasonable and works OK.
-	The paper is mostly well written and clear.
-	The experiments are reasonably thorough.

  Cons:
-	The paper states that it is a feasibility study on search methods for learning hard-threshold networks, however, it only evaluates the feasibility of one combinatorial search method. 
-	It’s not made clear whether other approaches were also investigated or what the authors learned from their exploration of this approach.
-	The actual algorithm is not very well explained, despite being the main contribution of the paper.
-	The datasets and models are small and not necessarily representative of the requirements of the field.
-	Scalability remains a serious concern with the proposed approach.
-	It’s not clear to me that the paper presents a sufficient enough contribution to warrant acceptance.

Overall, I like the direction but do not feel that the paper has contributed enough to warrant acceptance. The authors should use the experiments they’ve run and also run more experiments in order to fully analyze their method and use this analysis to improve their proposed approach. 


Questions and comments:

1.	Did you try alternative local search algorithms or did you just come up with a single approach and evaluate it? What did you learn from the experiments and the development of this algorithm that will let you create a better algorithm in the next iteration?

2.	I think that it is unfair to say that “it suggests a simpler, independent justification for the performance improvements obtained by their method.” in reference to the work of Friesen & Domingos (2018), given that the straight-through estimator is not well justified to begin with and their work in fact provides a justification for it. I do agree that it is important to investigate alternative heuristics and approaches within the discrete target propagation framework, however.

3.	Sections 2 and 3 do not clearly define what L_i is and where it comes from. Since these do not normally exist in a deep network they should be clearly defined.

4.	“step 2(b)” is not well defined in section 3.1.1. I assume that this refers to lines 4-8 of Algorithm 1? The paper should explain this procedure more clearly in the text. Further, I question the locality of this method, as it seems capable of generating any possible target setting as a neighbor, with no guarantee that the generated neighbors are within any particular distance of the uniform random seed candidate. Please clarify this.

5.	I believe that a negative sign is missing in the equation for T_i in ‘Generating a seed candidate’. For example, in the case where |N| = 1, then T_i = sign(dL/dT_i) would set the targets to attain a higher loss, not lower. Further, for |N|=1, this seems to essentially reduce to the heuristic method of Friesen & Domingos (2018). 

6.	In the ‘Setting the probabilities’ section:
(a) All uses of sign(h) can be rewritten as h (since h \in {-1, +1}), which would be simpler.
(b) The paper contradicts itself: it says here ‘flip entries only when sign(dL/dh) = sign(h)’ but Algorithm 1 says ‘flip entries only when sign(dL/dh) != sign(h)’. Which is it?
(c) What is the value of a_h in the pseudocode? (i.e., how is this computed in the experiments)

7.	In the experiments, the paper says that ‘[this indicates] that the higher dimensionality of the CIFAR-10 data manifold compared to MNIST may play a much larger role in inhibiting the performance of GRLS.’ How could GRLS overcome this? Also, I don’t agree with the claim made in the next sentence – there’s not enough evidence to support this claim as the extra depth of the 4-layer network may also be the contributing factor.

8.	In Table 2, why are some numbers missing? The paper should explain what this means in the caption and why it occurs. Same for the bolded numbers.

9.	The Loss Weighting, Gradient Guiding, Gradient Seeding, and Criterion Weighting conditions are not clearly defined but need to be to understand the ablation experiments. Please define these properly.

10.	The overall structure of the algorithm is not stated. Algorithm 1 shows how to compute the targets for one particular layer but how are the targets for all layers computed? What is the algorithm that uses Algorithm 1 to set the targets and then set the weights? Do you use a recursive approach as in Friesen & Domingos (2018)?

11.	In Figure 2, what dataset is this evaluation performed on? It should say in the caption. It looks like this is for MNIST, which is a dataset that GRLS performs well on. What does this figure look like for CIFAR-10? Does increasing the computation for the heuristic improve performance or is it also flat for a harder dataset? This might indicate that the initial targets computed are useful but that the local search is not helping. It would be helpful to better understand (via more experiments) why this is and use that information to develop a better heuristic.

12.	It would be interesting to see how GRLS performs on other combinatorial search tasks, to see if it is a useful approach beyond this particular problem.

13.	In the third paragraph of Section 4.2, it says ‘The results are presented in Table 3.’ I believe this should say Figure 3. Also, the ordering of Figure 3 and Table 3 should be swapped to align with the order they are discussed in the text. Finally, the caption for Table 3 is insufficiently explanatory, as are most other captions; please make these more informative.

14.	In Section 4.3:
(a), the paper refers to Friesen & Domingos (2018) indicating that zero loss is possible if the dataset is separable. However, what leads you to believe that these datasets are separable? A more accurate comparison would be the loss for a powerful non-binarized baseline network. 
(b) Further, given the standard error of GRLS, it’s possible that its loss could be substantially higher than that of FTPROP as well. It would be interesting to investigate the cases where it does much better and the cases where it does much worse to see if these cases are informative for improving the method.

15.	Why is there no discussion of training time in the experiments? While it is not surprising that GRLS is significantly slower, it should not be ignored either. The existence of the Appendix should also be mentioned in the main paper with a brief mention of what information can be found in it.

16.	In Algorithm 1, line 2 is confusingly written. Also, notationally, it’s a bit odd to use h both as an element and as an index into T.

17.	There are a number of capitalization issues in the references.

18.	The Appendix refers to itself (“additional hyperparameter details can be found in the appendices”).
","The sentiment of the review is mixed but leans towards the negative side. The reviewer acknowledges some positive aspects of the paper, such as the goal of the work, the reasonableness of the proposed algorithm, and the clarity of the writing. However, the reviewer also lists several significant concerns, including the limited scope of the feasibility study, lack of clarity in the algorithm's explanation, small and unrepresentative datasets, and scalability issues. The overall sentiment score is -20, reflecting a slightly negative but not entirely dismissive tone. The politeness score is 80, as the reviewer uses polite language throughout, providing constructive criticism and suggestions for improvement without being rude or harsh.",-20,80
"Paper Summary: This paper studies the zero-shot learning problem with deep generative models. More specifically, it proposed a hybrid framework that combines VAEs (more precisely, the variational hetero-encoder or VHE) and GANs all together. The entire model is composed of an image encoder (Weibull upward-downward variational encoder), a text decoder (Poisson Gamma belief network), and an image generator (generative adversarial network). Once learned, the generative models can be directly used for zero-shot classification and various image generation applications. In the experiments, two benchmark datasets CUB and Oxford-Flowers are used.

==
Novelty/Significance:
Zero-shot learning is a challenging task and he main motivation of the paper (using generative model) is interesting. The text representation in the paper is simply bag-of-words which limits the application to some extent. In a broader context, image captioning using generative model seems quite relevant.

Diverse and Accurate Image Description Using a Variational Auto-Encoder with an Additive Gaussian Encoding Space, Wang et al. In NIPS 2017.

==
Quality:
Overall, reviewer feels this is a very interesting work. However, the results from the paper is quite mixed. It is not yet convincing whether the proposed approach is the state-of-the-art in zero-shot learning or text-to-image generation. 

First, this paper demonstrates the power of generative models in text-to-image generation and other applications. However, reviewer feels that the zero-shot classification result is weak. In Table 1 and Table 2, it seems that GAZSL (Zhu et al. 2018) outperforms the proposed approach. 

Q1: In Table 2, is it possible to report the top-5 accuracy on CUB-easy and top-1 accuracy on Oxford-Flower dataset? Otherwise, it is not very convincing that proposed approach is better than the state-of-the-art approach GAZSL.

Second, the text-to-image generation results look reasonably good. But the resolution and quality of generated images are far from state-of-the-art. One suggestion is to train the VHE model with an improved image generator.

StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks, Zhang et al. In CVPR 2017.

AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks, Xu et al. In CVPR 2018.

Also, reviewer would expect to see an improved image generator can lead to a better ZSL performance.

Typo: In the title: Zero-Short → Zero-Shot.

","The sentiment of the review is generally positive but with some reservations. The reviewer acknowledges the interesting nature of the work and its potential, but also points out significant weaknesses in the results and suggests improvements. Therefore, the sentiment score is 30. The language used in the review is polite and constructive, offering specific suggestions for improvement without being harsh or dismissive. Therefore, the politeness score is 80.",30,80
"This paper experiments with pre-trained language models for common sense tasks such as Winograd Schema Challenge and ConceptNet KB completion. While the authors get high numbers on some of the tasks, the paper is not particularly novel, and suffers from methodology and clarity problems. These prevent me from recommending its acceptance.

This paper shows that pre-trained language models (LMs) can be used to get strong improvements on several datasets. While some of the results obtained by the authors are impressive, this result is not particularly surprising in 2018. In the last year or so, methods based on pre-trained LMs have been shown extremely useful for a very wide number of NLP tasks (e.g., Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2018). Moreover, as noticed to by the authors, Schwartz et al. (2017) demonstrated that LM perplexity can be useful for predicting common-sense information for the ROC story cloze task. As a result, the technical novelty in this paper is somewhat limited. 

The paper also suffers from methodological problems:
-- The main results observed by the author, the large improvement on the (hard!) Winograd schema challenge, is questionable: The GLUE paper (Wang et al., 2018) reports that the majority baseline for this dataset is about 65%. It is unclear whether the authors here used the same version of the dataset (the link they put does not unambiguously decide one way or another). If so, then the best results published in the current paper is below the majority baseline, and thus uninteresting. If this is not the same dataset, the authors should report the majority baseline and preferably also run their model on the (hard) version used in GLUE. 
-- The authors claim that their method on ConceptNet is unsupervised, yet they tune their LM on triplets from the training set, which makes it strongly rely on task supervision.

Finally, the paper suffers clarity issues. 
-- Some sections are disorganized. For instance, the experimental setup mentions experiments that are introduced later (the ConceptNet experiments). 
-- The authors mention two types of language models (word and character level), and also 4 text datasets to train the LMs on, but do not provide results for all combinations. In fact, it is unclear in table 2 what is the single model and what are the ensemble (ensemble of the same model trained on the same dataset with different seeds? or the same model with different datasets?).
-- The authors do not address hyper-parameter tuning. 
-- What is the gold standard for the ""special word retrieved"" data? how is it computed?


Other comments: 
-- Page 2: ""In contrast, we make use of LSTMs, which are shown to be qualitatively different (Tang et al., 2018) and obtain significant improvements without fine-tuning."": 1. Tang et al. (2018) do not discuss fine-tuning. 2. Levy et al. (ACL 2018) actually show interesting connections between LSTMs and self-attention.
-- Schwartz et al. (2017) showed that when using a pre-trained LM, normalizing the conditional probability of p(ending | story) by p(ending) leads to much better results than  p(ending | story). The authors might also benefit from a similar normalization. 
-- Page 5: how is F1 defined?

Minor comments: 
-- Page 2: "" ... despite the small training data size (100K instances)."": 100K is typically not considered a small training set (for most tasks at least)
-- Page 5: ""... most of the constituent documents ..."": was this validated in any way? how?
-- The word ""extremely"" is used throughout the paper without justification in most cases.


Typos and such:
page 1: ""... a relevant knowledge to the above Winograd Schema example, **does** not present ... "": should be ""is""
page 5: ""In the previous sections, we ***show*** ..."": showed
page 7: ""For example, with the ***test*** ..."": ""test instance""
","The sentiment of the review is moderately negative. The reviewer acknowledges some positive aspects, such as the impressive results on certain tasks, but overall, the review highlights significant issues with novelty, methodology, and clarity, leading to a recommendation against acceptance. Therefore, the sentiment score is -60. The politeness of the language is relatively high. The reviewer uses polite and professional language throughout, even when pointing out flaws and making recommendations. There are no rude or harsh comments, and the critique is constructive. Thus, the politeness score is 80.",-60,80
"The authors introduce the Attentive Task-Agnostic Meta-Learning (ATAML) algorithm for text classification.
The main idea is to learn task-independent representations, while other parameters, including the attention mechanism, are being fine-tuned for each specific task after pretraining. 
The authors find that, for few-shot text classification tasks, their proposed approach outperforms several important baselines, e.g., random initialization and MAML, in certain settings. In particular, ATAML performs better than MAML for very few training examples, but in that setting, the gains are significant. 

Comments:
- I am unsure if I understand the contributions paragraph, i.e., I cannot count 3 contributions. I further believe the datasets are not a valid contribution, since they are just subsets of the original datasets.
- Using a constant prediction threshold of 0.5 seems unnecessary. Why can't you just tune it?
- 1-shot learning is maybe theoretically interesting, but how relevant is it in practice? ","The sentiment of the review is moderately positive, as the reviewer acknowledges the authors' contributions and the effectiveness of their proposed approach in certain settings. However, the reviewer also raises several critical points and questions about the contributions, prediction threshold, and practical relevance of 1-shot learning. Therefore, the sentiment score is 20. The politeness of the language is neutral to slightly polite, as the reviewer uses phrases like 'I am unsure' and 'I further believe,' which soften the critique. However, the language is still direct and straightforward, so the politeness score is 10.",20,10
"The paper introduces HR-TD, a variation of the TD(0) algorithm. The variant is meant to ameliorate a problem of ‘over-generalization’ with conventional TD. This problem is briefly characterized, but primarily it is presumed to be established by prior work. The algorithm is simple and a series of experiments are presented with it applied to Mountain Car, Acrobot, and Atari Pong, with both linear function approximation and neural networks (DDQN). It is claimed that the results establish HR-TD as an improvement over TD. However, I found the results unconvincing because they were statistically insufficient, methodologically flawed, and too poorly presented for me to be confident of the meaning of numbers reported. In addition, it is not hard to imagine very simple problems where the HR-TD technique would be counterproductive, and these cases were not included in the experimental testbeds.

The first weakness of the paper is with its characterization of the problem that it seeks to solve: over-generalization. This problem is never really characterized in this paper. It instead refers instead to two other papers, one published only in a symposium and the other with no publication venue identified.

The second weakness of the paper is the claim that it has done a theoretical analysis in Section 4.4. I don’t see how this section establishes anything of importance about the new method.

The problem with the main results, the empirical results, is that they do not come close to being persuasive. There are many problems, beginning with there simply not being clear. I read and reread the paragraphs in Section 5.1, but I cannot see a clear statement of what these numbers are. Whatever they are, to assess differences between them would require a statistical statement, and there is none given. Moreover to give such a statistical statement would require saying something about the spread of the results, such as the empirical variance, but none is given. And to say something about the variance one would need substantially more than 10 runs per algorithm. Finally, there is the essential issue of parameter settings. With just one number given for each algorithm, there are no results or no statement about what happens as the parameters are varied. Any one of these problems could render the results meaningless; together they surely are.

These problems become even greater in the larger problems.

A nice property of HR-TD is that it is simple. Based on that simplicity we can understand it as being similar to a bias toward small weights. Such a bias could be helpful on some problems, possibly on all of those considered here. In general it is not clear that such a bias is a good idea, and regular TD does not have it. Further, HR-TD does not do exactly a bias to small weights, but something more complicated. All of these things need to be teased apart in careful experiments. I recommend small simple ones. 

How about a simple chain of states that are passed through reliably in sequence leading to a terminal state with a reward of 1000 (and all the other rewards 0). Suppose all the states have the same feature representation. If gamma=1, then all states have value 1000, and TD will easily learn and stick at this value even for large alpha, but HR-TD will have a large bias toward 0, and the values will converge to something significantly less than the true value of 1000. 

That would be an interesting experiment to do. Also good would be to compare HR-TD to a standard bias toward small weights to see if that is sufficient to explain the performance differences.","The sentiment of the review is largely negative, as the reviewer finds the results unconvincing due to statistical insufficiency, methodological flaws, and poor presentation. The reviewer also points out several weaknesses in the paper, including the characterization of the problem, the theoretical analysis, and the empirical results. However, the reviewer does acknowledge a positive aspect of the algorithm's simplicity. The politeness of the language is relatively high, as the reviewer provides constructive criticism and specific recommendations for improvement without using harsh or rude language.",-70,80
"The paper describes an imitation reinforcement learning approach where
the primitive actions of the agent are augmented with the most common
sequences of actions perform by experts. It is experimentally shown
how this simple change has clear improvements in the performance of
the system in Atari games. In practice, the authors double the number
of primitive actions with the most frequent double actions perform by
experts. 

A positive aspect of this paper comes from the simplicity of the
idea. There are however several issues that should be taken into
account:
- It is not clear how to determine when the distribution of action
  pair saturates. This is relevant for the use of the proposed approach.
- The total training time should consider both the initial time to
  obtain the extra pairs of frequent actions plus the subsequent
  training time used by the system. Either obtained from a learning
  system (15 hours) or by collecting traces of human experts (< 1
  hour?). 
- It would be interesting to see the performance of the system with
  all the possible pairs of primitive actions and with a random subset
  of these pairs, to show the benefits of choosing the most frequent
  pairs used by the expert.
- This analysis could be easily extended to triplets and so on, as
  long as they are the most frequently used by experts.
- The inclusion of macro-actions has been extensively studied in
  search algorithms. In general, the utility of those macros depends on
  the effectiveness of the heuristic function. Perhaps the authors
  could revise some of the literature.
- Choosing the most frequent pairs in all the game may not be a
  suitable strategy. Some sequences of actions may be more frequent
  (important) at certain stage of the game (e.g., at the beginning/end
  of the game) and the most frequent sequences over all the game may
  introduce additional noise in those cases.

The paper is well written and easy to follow, there are however, some
small typos:
- expert(whose => expert (whose
% there are several places where there is no space between a word and
% its following right parenthesis 
- don't need train => don't need to train
- experiments4. => experiments.
- Atmost => At most
","The sentiment of the review is generally positive, as indicated by the acknowledgment of the simplicity and effectiveness of the proposed approach. However, the reviewer also points out several issues and areas for improvement, which slightly tempers the overall positive sentiment. Therefore, the sentiment score is 50. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, offering suggestions and pointing out issues without being harsh or dismissive. Therefore, the politeness score is 90.",50,90
"This paper proposes an approach for mitigating issues associated with high-frequency/amplitude control signals that may be obtained when one applies reinforcement learning algorithms to continuous control tasks. The approach taken by the paper is to solve a constrained optimization problem, where the constraint imposes a (potentially state-dependent) lower bound on the reward. This is done by using a Lagrangian relaxation that learns the parameters of a control policy that satisfies the desired constraints (and also learns the Lagrange multipliers). The presented approach is demonstrated on a cart-pole swing-up task as well as a quadruped locomotion task.

Strengths:
+ The paper is generally clear and readable.
+ The simulation results for the Minitaur quadruped robot are performed using a realistic model of the robot.

Major concern:
- My biggest concern is that the technical contributions of the paper are not clear at all. The motivation for the work (avoiding high amplitude/frequency control inputs) is certainly now new; this has always been a concern of control theorists and roboticists (e.g., when considering minimum-time optimal control problems, or control schemes such as sliding mode control). The idea of using a constrained formulation is not novel either (constrained MDPs have been thoroughly studied since Altman (1999)). The technical approach of using a Lagrangian relaxation is the standard way one goes about handling constrained optimization problems, and thus I do not see any novelty there either. Overall, the paper does not make a compelling case for the novelty of the problem or approach.

Other concerns:
- For the cart-pole task, the paper states that the reward is modified ""to exclude any cost objective"". Results are then presented for this modified reward showing that it results in high-frequency control signals (and that the proposed constrained approach avoids this). I don't think this is really a fair comparison; I would have liked to have seen results for the unmodified reward function.
- The claim made in the first line of the abstract (applying RL algorithms to continuous control problems often leads to bang-bang control) is very broad and should be watered down. This is the case only when one considers a poorly-designed cost function that doesn't take into account realistic factors such as actuator limits.
- In the last paragraph of Section 3.3, the paper proposes making the lower-bound on the reward state-dependent. However, this can be tricky in practice since it requires having an estimate for Q_r(s,a) as a function of the state (in order to ensure that the state-dependent lower bound can indeed be satisfied). 

Typos:
- Pg. 5, Section 3.4: ""...this is would achieve...""
- Pg. 6: ...thedse value of 90...""","The sentiment score is derived from the overall tone and content of the review. The reviewer acknowledges some strengths of the paper, such as clarity and realistic simulation results, which contributes positively. However, the major concern about the lack of technical novelty and other specific criticisms significantly lower the sentiment score. Therefore, the sentiment score is set at -20, indicating a slightly negative sentiment. The politeness score is determined by the language used in the review. The reviewer uses polite and professional language throughout, even when pointing out major concerns and other issues. Therefore, the politeness score is set at 80, indicating a high level of politeness.",-20,80
"This paper presents a joint optimization approach for the continuous weights and categorical structures of neural networks. The idea is the standard stochastic relaxation of introducing a parametrised distribution over the categorical parameters and marginalising it. The method then follows by alternating gradient descent on the weights and the parameters of the categorical distribution.

This exact approach was proposed in https://arxiv.org/abs/1801.07650 by Shirakawa et al. The only innovation in this work is that it uses categorical distributions with more than two values. This is a minor innovation.

The experiments are however interesting as the paper compares to the latest hyper-parameters optimization strategies for neural nets on simple tasks (eg CIFAR10) and gets comparable results. However, given that this is the biggest contribution of the paper, it would have been nice to see results in more complex tasks, eg imagenet or translation.

I very much enjoyed the simplicity of the approach, but the question of innovation is making wonder whether this paper makes the ICLR bar of acceptance. The paper is also hard to read because of many English typos.   ","The sentiment of the review is mixed. While the reviewer acknowledges the interesting experiments and the simplicity of the approach, they also point out that the main idea is not novel and that the paper has many English typos. This results in a sentiment score of -20. The politeness of the language is generally respectful and constructive, with phrases like 'I very much enjoyed' and 'it would have been nice to see,' leading to a politeness score of 60.",-20,60
"This paper considers parameterizing Dirichlet, Dirichlet-multinomial, and Beta distributions with the outputs of a neural network. They present the distributions and gradients, discuss appropriate activation functions for the output layer, and evaluate this approach on synthetic and real datasets with mixed results. Overall, I found the writing very clear, the main idea sound, and paper generally well executed, but I have serious concerns about the significance of the contributions that lead me to recommend rejection. It would be very useful to me if the authors would provide a concise list of what they consider the main contributions to be and why they are significant. As I see it, the paper does three main things:

1. In section 2, the authors consider parameterizing Dirichlet, Dirichlet-multinomial, and Beta distributions with the outputs of a neural network (Section 2). As the authors note, parameterizing an exponential family distribution with the outputs of a neural network is not a novel contribution (e.g. Rudolph et al. (2016) and David Belanger's PhD thesis (2017)) and though I have never personally seen the Dirichlet, Dirichlet-multinomial, and Beta distributions used, the conceptual leap required is small. Most of section 2 is dedicated to writing down, simplifying, and deriving gradient equations for these three distributions. The simplifications and gradient derivations are well known and appear in many places (e.g. http://jonathan-huang.org/research/dirichlet/dirichlet.pdf, https://arxiv.org/pdf/1405.0099.pdf) and should not be considered contributions in the age of automatic differentiation (see Justin Domke's blog post on autodiff).

2. In section 3, the authors consider the unique challenges of using the proposed networks. They propose targeted activation functions that will improve the stability of learning. I found this to be the most interesting portion of the paper and the most significant contribution. Unfortunately, it is short on details and empirical results are referenced that do not appear in the paper (i.e. the second to last paragraph on page 5). If I were to rewrite this paper, I would focus on answering the question ""What are the unique challenges of parameterizing Dirichlet, Dirichlet-multinomial, and Beta distributions with the outputs of a neural network and how can we address them?"", replacing section 2 with an expanded section 3.

3. In section 4, the authors evaluate the proposed networks on a collection of synthetic and real tasks. In the end, the results are mixed, with the Dirichlet network performing best on the XENON1T task and the standard softmax network performing best on the CIFAR-100 task. In general, I don't mind mixed results and I appreciate that the authors included both sets of experiments; however, it is important that there is a convincing argument for why one would prefer the proposed solution even when accuracy is the same (e.g. it is faster, it is interpretable, etc.). The authors briefly argue that the proposed methods are superior because they provide uncertainty estimates for the output distributions. This may be true, but they only perform evaluations on tasks where the primary goal is accuracy. If the main benefit of the proposed networks is proper uncertainty quantification, then the evaluations (even if they are qualitative) should reflect that.

In summary, I do not think the models proposed in section 2 are sufficiently novel to justify publication alone which means that the authors need to either: (1) evaluate novel methods that are critical for use of these models or (2) present a convincing evaluation that strongly motivates the proposed model's use or that provides some novel insight into the model's behavior. I think that the authors are on their way to achieving (1), but do not achieve (2). I would suggest finding an application that requires uncertainty estimates for the distribution and centering the paper around that application.

Minor comments:

- Figure 2 (right) should include a y-axis label (e.g. ""parameter value"").

- In Figure 3 (right), it is not obvious what the ""Sigmoid"" line corresponds to. 

- It is not clear what the authors are trying to show in section 4.1. The EL activation function is smooth and monotone and the likelihood is convex, so there should be no question that the distribution will concentrate around y.

- Section 4.4 was interesting, but would have been more convincing if paired with an evaluation on real data.","The sentiment of the review is mixed but leans towards the negative side. The reviewer acknowledges the clarity of writing and the soundness of the main idea, but expresses serious concerns about the significance of the contributions, ultimately recommending rejection. This results in a sentiment score of -40. The politeness of the language is high; the reviewer uses polite and constructive language throughout, offering specific suggestions for improvement and acknowledging the strengths of the paper. This results in a politeness score of 80.",-40,80
"This paper presents Partially Mutual Exclusive Softmax (PMES), a relaxation of the full softmax that is commonly used for multi-class data. PMES is designed for positive-unlabeled learning, e.g., language modeling, recommender systems (implicit feedback), where we only get to observe positive examples. The basic idea behind PMES is that rather than considering all the non-positive examples as negative in a regular full softmax, it instead only considers a ""relevant"" subset of negatives. Since we actually don't know which of the negatives are more relevant, the authors propose to incorporate a discriminator which attempts to rate each negative by how hard it is to distinguish it from positives, and weight them by the predicted score from the discriminator when computing the normalizing constant for the multinomial probability. The motivation is that the negatives with higher weights are the ones that are closer to the decision boundary, hence will provide more informative gradient comparing to the negatives that are further away from the decision boundary. On both real-world and synthetic data, the authors demonstrate the PMES improves over some other negative sampling strategies used in the literature. 

Overall the idea of PMES is interesting and the solution makes intuitive sense. However, the writing of the paper at the current stage is rather subpar, to the extend that makes me decide to vote for rejection. In details:
 
1. The motivation of PMES from the perspective of mutual exclusivity is quite confusing. First of all, it is not clear to me what exactly the authors mean by claiming categorical distribution assumes mutual exclusivity -- does it mean given a context word, only one word can be generated from it? Some further explanation will definitely help. Further more, no matter what mutual exclusive means in this context, I can hardly see that PSME being fundamentally different given it's still a categorical distribution (albeit over a subset).

The way I see PMES from a positive-unlabeled perspective seems much more straight-forward -- in PU learning, how to interpret negatives is the most crucial part. Naively doing full softmax or uniform negative sampling carry the assumption that all the negatives are equal, which is clearly not the right assumption for language modeling and recommender systems. Hence we want to weight negatives differently (see Liang et al., Modeling user exposure in recommendation, 2016 for a similar treatment for RecSys setting). From an optimization perspective, it is observed that for negative sampling, the gradient can easily saturate if the negative examples are not ""hard"" enough. Hence it is important to sample negatives more selectively -- which is equivalent to weighting them differently based on their relevance. A similar approach has also been explored in RecSys setting (Rendle, Improving pairwise learning for item recommendation from implicit feedback, 2014). Both of these perspectives seem to offer more clear motivation than the mutual exclusivity argument currently presented in the paper.

That being said, I like the idea of incorporating a discriminator, which is something not explored in the previous work.  

2. The rigor in the writing can be improved. In details:

* Section 3.3, ""Multivariate Bernoulli"" -> what is presented here is clearly not multivariate Bernoulli

* Section 3.3, the conditional independence argument in ""Intuition"" section seems no difference from what word2vec (or similar models) assumes. The entire ""Intuition"" section is quite hand-wavy.

* Section 3.3, Equation 4, 5, it seems that i and j are referred both as binary Bernoulli random variables and categorical random variables. The notation here about i and j can be made more clear. Overall, there are ambiguously defined notations throughout the paper. 

* Section 4, the details about the baselines are quite lacking. It is worth including a short description for each one of them. For example, is PopS based on popularity or some attenuated version of it? As demonstrated from word2vec, a attenuated version of the unigram (raised to certain power < 1) works better than both uniform random, as well as plain unigram. Hence, it is important to make the description clear. In addition, the details about matrix factorization experiments are also rather lacking. 

3. On a related note, the connection to GAN seems forced. As mentioned in the paper, the discriminator here is more on the ""cooperative"" rather than the ""adversarial"" side. 

Minor:

1. There are some minor grammatical errors throughout. 

2. Below equation 3, ""\sigma is the sigmoid function"" seems out of the context.

3. Matt Mohaney -> Matt Mahoney ","The sentiment of the review is mixed but leans towards negative. The reviewer acknowledges the interesting idea behind PMES and its intuitive sense, which is positive. However, the overall sentiment is negative due to the significant criticism of the paper's writing quality and clarity, leading to a recommendation for rejection. Therefore, the sentiment score is -40. The politeness of the language is relatively high. The reviewer provides constructive feedback and specific recommendations without using harsh or rude language. The tone is professional and aimed at helping the authors improve their work. Therefore, the politeness score is 80.",-40,80
"This paper proposes a new variational recurrent model for learning sequences. Comparing to existing work, instead of having latent variables that are dependent on the neighbors, this paper proposes to use independent latent variables with observations that are generated from multiple latent variables. 
The paper further combined the proposed method with multiple existing ideas, such as the shared/prviate representation from VAE-CCAE, adding the hierarchical structure, and prior updating. 

Pros:
The proposed method seems technical correct and reasonable. 
There are many extensions which are potentially useful for many applications 
There are many experimental results showing promising performance. 

Cons:
The framework is very incremental. It is novel but limited. 
The paper claim that the main point to use the simpler variations distribution is to speed up the inference. But no speed comparisons are shown in the experiments section. 
The evaluation shows that prior updating (one extension) seems contributes to the biggest performance gain, not the main proposed method. 

","The sentiment of the review appears to be moderately positive. The reviewer acknowledges the technical correctness and potential usefulness of the proposed method, as well as the promising experimental results. However, the reviewer also points out some limitations, such as the incremental nature of the framework and the lack of speed comparisons. The politeness of the language is quite high, as the reviewer uses respectful and constructive language throughout the review, even when pointing out the cons.",50,80
"I like the idea of trying to qualitatively illustrate the behavior of SGD when optimizing parameters of complex models, such as Deep and Conv Nets, but I think that the contribution is not very substantial. The connection between SGD and diffusion has been pointed out in previous papers, as acknowledged by the Authors. The study of the effect of batch size is interesting, but again somewhat derived from previous works. 

It would helpful to illustrate the difference between ""crossing"" and ""moving over"" a barrier with a simple figure. 

The experimental validation is interesting, although I think it is limited and perhaps the conclusions that can be drawn from it are not so surprising. I believe it would have been interesting to study other important factors that affect the behavior of SGD, such as learning rate and type of momentum. For example, a larger learning rate might allow for more crossing of barriers. Also, different SGD algorithms (ADAGRAD, ADAM, etc...) would behave considerably differently I expect. At the moment these important factors are overlooked. 

It is not clear to me why we would want to avoid larger batch sizes. A larger batch size allows for a lower variance of stochastic gradients, and therefore faster convergence. I think this point requires elaboration, because this forms the motivation behind theoretically grounded and successful SGD works, such as SAGA and the like. I agree that a smaller batch-size is preferable at the beginning of the optimization, but again this is a well known fact (again, see SAGA) and it is for computational reasons mostly (being far away from the (local) mode, a noisy gradient is enough to move in the right direction - no need to spend computations to use an accurate gradient). There is no guarantee that the local optimum close to initialization is a bad local optimum in general, so I don't think that using a large batch size at the beginning is a bad idea for this reason - again it is just computational. 

Another thing missing I think is the discussion around why it is potentially a good thing to cross the barrier, either at the beginning of the exploration or towards convergence to a local optimum. At the moment, the paper seems to report the behavior of SGD without key insights on the importance of crossing or avoiding crossing barriers.

As a concluding remark - there has been a lot of work on the connections between diffusions and MCMC algorithms (see e.g., the Metropolis Adjusted Langevin Algorithm - MALA) and a lot of the considerations made in the paper are somewhat known. That is, random walk/diffusion type MCMC (and even gradient-based MCMC like Hybrid Monte Carlo) struggle a lot in non-convex problems and they hardly move across modes of a posterior distribution (equivalent to crossing barriers of potential). So I'm not at all surprised that SGD does not cross barriers during optimization and I would challenge the statement in the introduction saying ""Intuitively, when performing random walk on a potential, one would expect barriers being crossed quite often during the process.""","The review starts with a positive note, appreciating the idea behind the paper. However, it quickly transitions into a critique, pointing out that the contribution is not substantial and that many of the findings are derived from previous works. The reviewer provides constructive feedback and suggestions for improvement, indicating a willingness to help the authors improve their work. The language used is polite and professional, even when pointing out the limitations and areas for improvement. The reviewer also acknowledges the interesting aspects of the experimental validation, although they believe it is limited.",-20,80
"The authors suggest a reward shaping algorithm for multi-agent settings that adds a shaping term based on the TD-error of other agents to the reward. In order to implement this, each agent needs to keep tack of two different value estimates through different DQN networks, one for the unshaped reward and one for the shaped reward. 

Points of improvement and questions: 
-Can you please motivate the form of the reward shaping suggested in (2) and (3)? It looks very similar to simply taking \hat{r}_a = r_a + sum_{a' not a} z_{'a}. Did you compare against this simple formulation? I think this will basically reduce the method to Value Decomposition Networks (Sunehag ‎2017) 
-The results on the prisoners dilemma seem miss-leading: The ""peer review"" signal effectively changes the game from being self-interested to optimising a joint reward. It's not at all surprising that agents get higher rewards in a single shot dilemma when optimising the joint reward. The same holds for the ""Selfish Quota-based Pursuit"" - changing the reward function clearly will change the outcome here. Eg. there is a trivial adjustment that adds all other agents rewards to the reward for agent i that will will also resolve any social dilemma.
-What's the point of playing an iterated prisoners dilemma when the last action can't be observed? That seems like a confounding factor. Also, using gamma of 0.9 means the agents' horizon is effectively limited to around 10 steps, making 50k games even more unnecessary. 
-""The input for the centralized neural network involves the concatenation of the observations and actions, and optionally, the full state"": This is not true. For example, the Central-V baseline in COMA can be implemented by feeding the central state along (without any actions or local observations) into the value-function. It is thus scalable to large numbers of agents. 
-The model seems to use a feed-forward policy in a partially observable multi-agent setting. Can you please provide a justification for this choice? Some of the baseline methods you compare against, eg. QMIX, were developed and tested on recurrent policies. Furthermore, independent Q-learning is known to be less stable when using feedfoward networks due to the non-stationarity issues arising (see eg. ""Stabilising Experience Replay"", ICML 2017, Foerster et al). In it's current form the concerns mentioned outweigh the contributions of the paper.","The sentiment of the review is generally negative, as the reviewer points out several significant issues and concerns with the paper, such as the misleading results on the prisoners dilemma, the confounding factors in the iterated prisoners dilemma, and the choice of feed-forward policy in a partially observable multi-agent setting. The reviewer also questions the motivation behind the reward shaping and suggests that the current form of the paper's contributions is outweighed by the concerns mentioned. The politeness of the language used is neutral to slightly polite. The reviewer uses phrases like 'Can you please' and 'Can you provide a justification,' which indicate a polite tone, but the overall critical nature of the feedback tempers this politeness.",-60,20
"In this work the authors use a score-based adversarial attack (based on the natural evolution strategy (NES)) to successfully attack a multitude of defended networks, with success rates rivalling the best gradient-based attacks.

As confirmed by the authors in a detailed and very open response to a question of mine, the attack introduced here is actually equivalent to [1]. While the attack itself is not novel (which will require a major revision of the manuscript), the authors point out the following contributions over [1]:

* Attack experiments here go way beyond Ilyas et al. in terms of Lp metrics, different defense models, different datasets and transferability.
* Different motivation/derivation of NES.
* Concept of adversarial distributions.
* Regression network for good initialization.
* Introduction of accuracy-iterations plots.

My main concerns are as follows:
* The review of the prior literature, in particular on score-based and decision-based defences (the latter of which are not even mentioned), is very limited and is framed wrongly. In particular, the statement “However, existing black-box attacks are weaker than their white-box counterparts” is simply not true: as an example, the most prominent decision-based attack [2] rivals white-box attacks on vanilla DNNs as well as defended networks [3].
* The concept of adversarial distributions is not new but is common in the literature of real-world adversarials that are robust to transformations and perturbations (like gaussian noise), check for example [4]. In [4] the concept of _Expectation Over Transformation (EOT)_ is introduced, which is basically the generalised concept of the expectation over gaussian perturbations introduced in this work.
* While I like the idea of accuracy-iterations plots, the idea is not new, see e.g. the accuracy-iterations plot in [2] (sample-based, Figure 6), the loss-iterations plot in [5] or the accuracy-distortion plots in [3]. However, I agree that these type of visualisation or metric is not as widespread as it should be.

Hence, in summary the main contribution of the paper is the application of NES against different defence models, datasets and Lp metrics as well as the use of a regression network for initialisation. Along this second point it would be great if the authors would be able to demonstrate substantial gains in the accuracy-query metric. In any case, in the light of previous literature a major revision of the manuscript will be necessary.

[1] Ilyas et al. (2018) “Black-box Adversarial Attacks with Limited Queries and Information” (https://arxiv.org/abs/1804.08598) 
[2] Brendel et al. (2018) “Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models” (https://arxiv.org/abs/1712.04248)
[3] Schott et al. (2018) “Towards the first adversarially robust neural network model on MNIST” (https://arxiv.org/abs/1805.09190)
[4] Athalye et al. (2017) “Synthesizing Robust Adversarial Examples” (https://arxiv.org/pdf/1707.07397.pdf)
[5] Madry et al. {2017) “Towards Deep Learning Models Resistant to Adversarial Attacks” (https://arxiv.org/pdf/1706.06083.pdf)","The sentiment score is determined by the overall tone and content of the review. The reviewer acknowledges the success of the attack and the detailed response from the authors, which is positive. However, the reviewer also points out significant concerns and the need for major revisions, which introduces a negative aspect. Therefore, the sentiment score is a balanced 0 (neutral). The politeness score is determined by the language used in the review. The reviewer uses polite and professional language throughout, even when pointing out major concerns and necessary revisions. Therefore, the politeness score is 80 (polite).",0,80
"Paper Summary - This paper presents an approach for fine-grained action recognition and video captioning. The authors train a model using both classification and captioning tasks and show that this improves performance on transfer learning tasks. The method is evaluated on the Something-Something v2 dataset as well as a new dataset (proposed in this paper). The authors also evaluate the benefit of using fine-grained action categories vs. coarse-grained action categories on transfer learning.

Paper Strengths
-  Comparing fine-grained vs. coarse-grained action categories for transfer learning is well motivated. Evaluating just this aspect in the context of video classification is helpful (Section 5.1). Establishing the baseline using linear classifiers for feature transfer makes the feature transfer result more robust. The authors have also done a good job of evaluating their method in the coarse-grained and fine-grained settings (Table 1, 2).
- The architectural and experimental design in this paper is well illustrated.
- The 20bn kitchen dataset has interesting categories about intention - pretending to use, using, and using & failing.
- The ablation in Table 1 is helpful in understanding the contribution of 3D vs. 2D convolutions.

Paper Weaknesses
- I believe this paper tries to do too much and as a result fails to show results convincingly. There are too many results and not much focus on analyzing them. In my opinion, the experimental setup in the paper is weak to fully support the authors' claims.
- I now analyze the main contributions of this paper as outlined by the authors in Section 1.
    - Label granularity and feature quality: To me this is the most interesting part of this paper and most related to its title. However, this is also the most under-analyzed aspect. The only result that the authors show is in Sec 5.1 and Fig 5. Apart from using the provided fine-grained vs. coarse-grained labels for evaluation, the authors do not perform many experiments in this domain and neither do they analyze these results. For example, the gain using fine-grained labels is not significant in Figure 5 (2Channel - CG vs. 2Channel - FG). The authors do not explain this aspect. Another missing baseline from Figure 5 is ""2Channel - Captions & CG actions"". This baseline is needed to understand the contribution of FG vs CG actions when also using captioning as additional supervision.
    - Baselines for captioning: The authors do not provide any details for this task. If the intent is to establish baselines there needs to be more effort on analyzing design decisions - e.g. decoding, layers in LSTM. Captioning metrics such as CIDER and SPICE are missing.
    - Captions as a source for transfer learning: This is poorly analyzed in this paper. 1) Can the captions be converted to ""tags"" and then used for supervision? What is the benefit of producing the full sequential text description over this simple approach? 2) Captions for transfer learning are only analyzed in Figure 5 without much explanation. It is hard to claim that captioning is the reason for performance gains without really analyzing it completely.
    - 20bn-kitchenware dataset - This dataset is explained in just one paragraph in Section 6. What is the motivation behind collecting this dataset as opposed to showing transfer learning on some other dataset?
- Missing references
        - There has been work in understanding the effect of fine-grained categories in ImageNet transfer learning - What makes ImageNet good for transfer learning? Huh et al. What is the insight provided over this work?
- Minor comments
    - Section 1: Figure 4 is referenced in points 1 & 3. I think you mean Figure 5.
","The sentiment score is derived from the overall tone of the review. While the reviewer acknowledges some strengths of the paper, such as the motivation behind comparing fine-grained vs. coarse-grained action categories and the architectural design, the majority of the review focuses on the weaknesses and shortcomings of the paper. The reviewer points out several areas where the paper fails to provide convincing results or sufficient analysis, which indicates a generally negative sentiment. Therefore, the sentiment score is -40. The politeness score is based on the language used in the review. The reviewer uses polite and constructive language throughout the review, even when pointing out weaknesses. Phrases like 'I believe,' 'In my opinion,' and 'I think you mean' indicate a respectful and considerate tone. Therefore, the politeness score is 80.",-40,80
"The paper proposes a generative infection cascade model based on latent vector representations. The main idea is to use all possible paths of infections in the model. 

Comments:
- The papers clarity could be much improved. It is not easy to follow, is overflowing with notation, and lengthy. Sec. 2.1 for example can easily be made much more concise. Secs. 3.1 and 3.2 are especially confusing. In the first equation in Sec. 3, what is \phi with and without sub/superscript? In Eq. (2), what is k - a probability, or an index? And what is the formal definition ""infection"" and ""future"" in the description of k stating that it is ""the probability that u infects v in the future""?

- The authors mention that the actual infectors in a diffusion process are rarely observed. While this might be true, in many types of data include infection attempts. This should be worthwhile to model - there are many works on reconstructing cascades from partial data.

- The authors note (rightly) the Eq. (9) is hard to solve, and propose a simple lower bound based on (what I think is) a decomposition assumption.  Unless I misunderstood, this undermines the contribution of the structure of past infections. Could the authors please clarify?

- The results mention 5 (tables?), but only 4 are available, of which one appears floating on the last page.

- Why are methods discussed in the introduction (e.g., DeepCas, Wang 2017a,b 2018) not used as baselines?

Minor:
- Wang 2017a and Wang 2017b are not the same Wang
- Several occurrences of empty parentheses - ()
- ","The sentiment of the review is moderately negative. The reviewer points out several issues with the clarity and structure of the paper, indicating that it is not easy to follow and is overflowing with notation. Specific sections and equations are highlighted as confusing, and there are several questions and requests for clarification. The reviewer also notes missing results and questions the choice of baselines. However, the language used is polite and constructive, with the reviewer providing specific feedback and suggestions for improvement without being rude or dismissive.",-40,60
"The paper suggest a shrinkage-based estimator (James-Stein estimator) to compute policy gradients in reinforcement learning to reduce the variance by trading some bias. Two versions are suggested: The on-policy gradients is shrinked either towards (i) model based gradient, or towards (ii) a delayed average of previous on-policy gradients. Empirically, both methods have better performance than the baseline.

The paper is clearly written and well motivated. Some details are lacking that would be of interest to the reader and to make the results reproducible. For example how is \hat Q estimated? The trick that is referred to in the end of page about only simulating short horizon trajectories deserves more detail. I would suggest providing more details, in the text and/or in the two algorithms.

The authors claim that JS estimator for gradient estimation in RL has not been used before. I am also not aware of any other work, but have also not been looking after that line of work. The paper seems to be a good contribution to the ever increasing literature of how to improve deep RL.

Minors:
\hat \theta on RHS in eq (7) should be \bar \theta ? Otherwise, what is \hat \theta?
section 4.2 Ww -> We

======= After revision =========

I still think this is a very interesting, novel and relevant idea that desires attention. However, on the same time, I agree with the points raised by the other two reviewers which are all well-motivated and relevant concerns. Therefore, I join the view that the paper is not yet ready for publication but I do encourage the authors to improve their work and resubmit to another venue.","The sentiment of the review is generally positive, as the reviewer acknowledges the novelty and relevance of the idea, and appreciates the clarity and motivation of the paper. However, the sentiment is tempered by the acknowledgment of the other reviewers' concerns and the conclusion that the paper is not yet ready for publication. Therefore, the sentiment score is 30. The politeness of the language is high, as the reviewer uses polite and constructive language throughout, encourages the authors to improve their work, and suggests resubmission to another venue. Therefore, the politeness score is 90.",30,90
"
[pros]
- It proposes a general formulation of GAN-type adversarial learning as in (1), which includes the original GAN, WGAN, and IPM-type metrics as special cases.
- It also proposes use of the penalty term in terms of the Lipschitz constant  of the discriminative function.

[cons]
- Some of the arguments on the Wasserstein distance and on WGAN are not sound.
- Theorem 3 does not make sense.
- The proposed scheme is eventually similar to the gradient-penalty-based formulation in Gulrajani et al. (2017).

[Quality]
I found some weaknesses in this paper, so that I judge the quality of this paper not to be high. For example, the criticisms on the Wasserstein distance in Section 2.3 and in Section 4.4, as well as the argument on WGAN at the end of Section 3.1, is not sound. The claim in Theorem 3 does not make sense, if we literally take its statement. All these points are detailed below.

[Clarity]
The main paper is clearly written, whereas in the appendices I noticed several grammatical and spelling errors as well as unclear descriptions.

[Originality]
Despite that the arguments in this paper are interesting, the proposed scheme is somehow eventually similar to the gradient-penalty-based formulation in Gulrajani et al. (2017), with differences being introduction of loss metrics $\phi,\varphi,\psi$ and the form of the gradient penalty, $\max \|\nabla f(x)\|_2^2$ in this paper versus $E[(\|\nabla f(x)\|_2-1)^2]$ in Gulrajani et al. (2017). This fact has made me to think that the originality of this paper is marginal.

[Significance]
This paper is significant in that it would stimulate empirical studies on what objective functions and what types of gradient penalty are efficient in GAN-type adversarial learning.

Detailed comments:

In Section 2.3, the authors criticize use of the Wasserstein distance as the distance function of GANs, but their criticism is off the point. It is indeed a problem not of the Wasserstein distance itself, but of its dual formulation.

It is true mathematically that $f$ in equation (8) does not have to be defined outside the supports of $P_g$ and $P_r$ because it does not affect the expectations in (8). In practice, however, one may regard that $f$ satisfies the condition $f(x)-f(y)\le d(x,y)$ not only on the supports of $P_g$ and $P_r$ but throughout the entire space $\mathbb{R}^n$. It is equivalent to requiring $f$ to satisfy the 1-Lipschitz condition on $\mathbb{R}^n$, and is what WGAN (Arjovsky et al., 2017) tries to do in its implementation of the ""critic"" $f$ via a multilayer neural network with weight clipping.

One can also argue that, if one defines $f$ only on the supports of $P_g$ and $P_r$, then it should trivially be impossible to obtain gradient information which can change the support of $P_g$. The common practice of requiring the Lipschitz condition throughout $\mathbb{R}^n$ is thus reasonable from this viewpoint. This is therefore not the problem of the Wasserstein distance itself, but the problem regarding how the dual problem is implemented in learning of GANs. In this regard, the discussion in this section, as well as that in Section 4.4, is misleading.

On optimizing $k$, I do not agree with the authors's claim at the end of Section 3.1 that WGAN may not have zero gradient with respect to $f$ even when $P_g=P_r$. Indeed, when $P_g=P_r$, for any measurable function $f$ one trivially has $J_D[f]=E_{x\sim P_g}[f(x)]-E_{x\sim P_r}[f(x)]=0$, so that the functional derivative of $J_D$ with respect to $f$ does vanish identically. 

I do not understand the claim of Theorem 3. I think that the assumption is too strong. If one literally takes ""$\forall x \not= y$"", then one can exchange $x$ and $y$ in the condition $f(y)-f(x)=k\|x-y\|$ to obtain $f(x)-f(y)=k\|y-x\|$, which together would imply $k=0$, and consequently $f$ is constant. One would be able to prove that if there exists $(x,y)$ with $x \not= y$ such that $f(y)-f(x)=k\|x-y\|$ holds then the gradient of $f$ at $x_t$ is equal to $k(y-x)/\|x-y\|$ under the Lipschitz condition.

Appendix G: Some notations should be made more precise. For example, in the definition of J_D the variable of integration $x$ has been integrated out, so that $J_D$ no longer has $x$ as its variable. The expression $\partial J_D/\partial x$ does not make any sense. Also, $J_D^*(k)$ is defined as ""arg min"" of $J_D$, implying as if $J_D^*(k)$ were a $k$-Lipschitz function.

Page 5, line 36: $J_D(x)$ appears without explicit definition.

Page 23, lines 34 and 38: Cluttered expression $\frac{\partial [}{\partial 2}]$ makes the statements not understandable. It also appears on page 24 several times.","The review starts with some positive points about the general formulation and the use of the penalty term. However, it quickly shifts to negative points, criticizing the soundness of arguments and the originality of the proposed scheme. The sentiment score is therefore mixed but leans towards negative due to the significant criticisms. The language used is formal and polite, even when pointing out flaws, which indicates a high politeness score.",-30,80
"Summary: 
This paper expands on the work on 'emergent communication' with 2 innovations: 
- The architecture has a separate 'message channel' that processes the incoming and outgoing messages mostly independently of the hidden state of the agent. There are also dedicated architecture elements for the interaction between the hidden state and the message stream. 
- The outgoing message is gated with a 'speak' action: only when the agent takes the speak-action at time step t is a message sent out at timestep t+1. 

Comments for improvement: 
-The paper proposes a rather complicated architecture, with many moving part. In the paper's current form it is extremely hard to see which part of this architecture contribute to the success of the method. A set of ablation studies on the different components would indeed be very helpful. 
-Using the word 'thought' to describe the hidden state of the agent is rather distracting.
-Equation (1): This just seems to be the policy gradient term for a factorised action space across 'environment action' and 'communication action'. The only obvious difference is that the policy here is shown to condition on the state representation s_t, rather than on the input. Is that intended?
-The paper suffers from a lot of undefined notation, e.g. the s_t above. Please clarify.
-In Figure 2b) the MCU is shown to produce the action a_t as an output. That seems like a mistake. 
-Figure 4): The results seem to be extremely unstable, which is a well known issue for independent learning. Recent work (MADDPG, COMA) has shown that centralised critics can drastically avoid these instabilities and improve final performance. Did you compare against using a centralised critic, V(central state), rather the V(observation)? Also, using a single seed on this kind of unstable learning process renders the results highly non-conclusive. 
-In Figure (5), what are the red-arrows? Do these correspond to the actual actions taken by the agents or are they simply annotations? It would be good to see how far the communication range is by comparison. Also, why is there a blob of 'communicating' agents far from the enemy? 
-Are different methods in the large scale battle task trained in self-play and then pitched against other methods in a round-robin tournament after training has finished or are they trained against each other? 
-In Figure 6 (a), why are average rewards changing over the course of training? I would expect this to be a zero-sum setting in self-play. 
-I couldn't find any supplementary material referenced in the text for the details. Instead the paper seems to have another copy of the paper itself attached in the pdf. This makes it hard to evaluate the paper given that few details around training are provided in the main text. 

Overall I am concerned that the learning method used in the paper (independent baseline) is known to be unstable and to produce poor results in the multi-agent setting (see COMA and MADDPG). This raises the concern that the communication channel is mostly useful for overcoming the issues introduced from having a decentralised critic.","The sentiment of the review is mixed but leans towards negative. The reviewer acknowledges the innovations in the paper but raises several significant concerns about the complexity of the architecture, the lack of clarity, and the instability of the results. This is reflected in the detailed list of recommendations for improvement. Therefore, the sentiment score is -40. The politeness of the language used is generally respectful and constructive, with no rude or harsh language. The reviewer provides specific feedback and suggestions for improvement in a professional manner. Therefore, the politeness score is 80.",-40,80
"[Summary:]
This paper presents a meta-learning architecture where the slow learner is trained by SGD and the fast learner is trained according to what the meta-learner guides. CNN is split into two parts: (1) bottom conv layers devoted to learn meaningful representation, which is referred to as slow learner; (2) top-fully connected layers involving task-specific fast learners. As in [Andrychowicz et al., 2016], the meta-learner guides the training of task-specific learners. In addition, slow learners are trained by SGD. The motivation is that low-level features should be meaningful everywhere while high-level features should vary wildly. They introduce “miracle representations” and prove that fast/slow learning on a two-layer linear network should converge to somewhere near this miracle representation. They evaluate on few-shot classification benchmarks to evaluate how well this fast/slow meta-learning approach works.

[Strengths:]
The paper has a clear motivation. It is easy to read. Training slow/fast learners using different strategies is an interesting idea. 

[Weaknesses:]
- The technique used in this work is a mix of SGD and  [Andrychowicz et al., 2016].
- The analysis is limited to a simple two-layer linear network. It is not clear whether this analysis is carried over to the proposed deep nets. 
- Quantitative results did not compare to recent results such as Reptile[1] or MT-Nets[2].

[Specific comments:]
- The current work is an improvement over [Andrychowicz et al., 2016], claiming that training conv layers and fully-connected layers with different strategies improves the generalization. I am wondering why the comparison to [Andrychowicz et al., 2016] is missing. You can use (fully) pre-trained CNN (which already learns meaningful representation using a huge amount of data) in the framework of [Andrychowicz et al., 2016]. 
-As one of the points of the paper is that this meta-learning strategy enables life-long learning, it would have been nice to see an experiment using this, where the distribution of tasks changes as time goes on.
-The paper says SOA(State Of the Art); I think the term SOTA(State Of The Art) is more commonly used.
-The use of the term “miracle” keeps changing(miracle solution, miracle representation, miracle W, miracle knowledge); the paper would be clearer if only one “miracle X” was defined and used as these are all essentially saying the same thing.

References
[1]https://arxiv.org/abs/1803.02999
[2]https://arxiv.org/abs/1801.05558

","The sentiment of the review is moderately positive. The reviewer acknowledges the clear motivation, readability, and interesting idea of training slow/fast learners using different strategies. However, they also point out several weaknesses, such as the limited analysis and lack of comparison to recent results. Therefore, the sentiment score is 30. The politeness of the language is quite high. The reviewer uses polite language, provides constructive feedback, and suggests improvements without being harsh or dismissive. Therefore, the politeness score is 80.",30,80
"This paper presents an algorithm for finding a polytope of adversarial examples. This means that within a convex hull, you can move around freely and get a new adversarial example at each point, while still maintaining misclassification. It then couples this with a method of generating nearest neighbor patch-based images in an effort to create ""macro-level different"" examples. The premise is interesting, but the implications are questionable and I do not find the work in macro-level differences to be sound. This could be based in misunderstandings, so please let me know if you think that is the case.

Strengths:
- The notion of the polytope is interesting and the algorithm for finding such polytope seems perfectly reasonable.
- I think the goal of macro-level adversarial examples is interesting.

Weaknesses:
- First of all, the 5 corners of the polytope all look the same to me (for instance fig 2). This is not encouraging, because it means that every single point in the polytope will also look exactly like the corners. To be frank, this means the polytope is not that interesting and has only found an extremely small pocket of adversarial examples. If you use a regular method of finding a single adversarial example, I'm sure the outcome wouldn't change within some ball around the sample (perhaps with very small radius, but nonetheless). In fact, a comparison between that ball's volume and the volume of the polytope would be interesting.
- The implication of these polytopes is not at all clear if it doesn't really allow us to generate adversarial example of a new flavor. The investigation into macro-level differences does not help the case, as I will explain.
- I am not at all convinced that there is any meaning to the examples with ""macro-level differences.""  It's a bit unclear to me how many patches are used per image, but assuming that a patch is centered over each pixel,  it would mean that we have as many control parameters as we have pixels, which assuming the pixels each have three color values, is just 1/3 of the original degrees of freedoms. Now, the patches probably do constrain what we can paint a bit, but since the patches are applied with a pyramid, it means the center pixel will contribute more than any other for a given patch, so I'm not so sure. I'm not convinced that we can't come up with linear combinations of these patches that produce highly non-natural images with ""micro-level"" adversarial patterns. In fact, I think section 4.1 and figure 7 provide evidence to the contrary. Let me explain:
    - Section 4.1: Why do you need a total variation penalty at all if you have constructed a patch-based drawing method that is supposed to be unable to produce unnatural high-frequency patterns? If you only had a handful of patches and they were all non-overlapping, then this would be impressive and.
    - Figure 7: We can clearly see high-frequency patterns that create the shadow of an obelisk in 7(a). I think the same is true for ""erase"", although the pattern is not as recognizable. The examples actually look more suspicious than regular adversarial examples, since it looks like the original image has simply been blurred, which means the adversarial perturbations are more clear. I understand that these patterns were created using a complicated scheme of natural patches, but I think you made this method too powerful. The one interesting quality is the bottom right of the trimaran which looks like a shark - however, that is a singular occurrence in your examples and it certainly feels like the high-frequency patterns will contribute much more to class than the shark itself.
- Please let me know if I am misinterpreting the importance of the results in Figure 7, since this is an important culmination of this work.

Other comments:
- Some of notation is a bit confusing. In (1), why is p not bold but x and t are bold? They are all vectors. In Algorithm 1, x is not bold anymore.
- Algorithm 1 also seems quite unnecessary to include so explicitly.
- Isn't a bounded polytope called a ""simplex""? Perhaps there is a distinction that I'm not aware of, but the absence of the word ""simplex"" throughout the whole paper surprised me a bit. Perhaps this is a perfectly correct omission due to differences that I'm not aware of.

Minor comments:
- abstract, ""We propose a way to finding"" -> either ""to->""of"" or ""find""
- page 3, ""and we can generate new colliding example"" -> ""a new colliding example""
- page 3, ""taking arbitrary an convex combinations"" -> ""combination""
- page 3, ""Given a target x"", I think you mean ""Given a target t""
- page 5, ""As many gradient-based method"" -> ""methods""
- page 8, ""carton""? ""rubber""? Those are not in figure 7(b).
- page 10, ""are crucial to less non-robust"" ? This sentence (which is the final sentence of the conclusion and thus has a certain level of importance) is not something that is novel to your paper. The impact of non-linearities on adversarial examples have been well-studied.","The sentiment of the review is mixed but leans towards the negative side. The reviewer acknowledges some strengths of the paper, such as the interesting notion of the polytope and the goal of macro-level adversarial examples. However, the majority of the review focuses on the weaknesses and questionable implications of the work, indicating a sentiment score of -40. The politeness of the language used is generally respectful and constructive, with the reviewer asking for clarification and offering suggestions for improvement, resulting in a politeness score of 60.",-40,60
"The authors explore the possibility of using an end-to-end approach for predicting pharmacological assay outcome using fluorescence microscopy images from the public Cell Painting dataset. In my view, the primary contributions are the following: an interesting and relatively new application (predicting assay outcomes), enriching the CellPainting dataset with drug activity data, and a comparison of several relevant methods and architectures. The technical novelty is weak, and although the authors demonstrate that end-to-end holistic approaches outperform previous segmentation-and-feature-extraction approaches, this result is not surprising and has been previously reported in closely related contexts.


OVERVIEW

The authors evaluate the possibility of using and end-to-end deep learning approach to predict drug activity using only image data as input. The authors repurpose the CellPainting dataset for activity prediction by adding activity data from online ChEMBL databases. If made available as promised, the dataset will be a valuable resource to the community. The authors compare a number of previous approaches and state-of-the-art image classification network architectures to evaluate the use of CNNs instead of more classical image analysis pipelines. The comparison is a strong point of the paper, although some details are lacking. For example, the authors claim that GapNet is the quickest method to train, and while they report the number of hyperparameters and time per epoch, the number of epochs trained is never mentioned. 

The authors propose an architecture (GapNet) for the assay prediction task. While the way Global Average Pooling is used to extract features at different stages in the network might be new, it is a straightforward combination of GAP and skip connections. Little insight into why this approach is more efficient or evidence for its effectiveness is provided. Similarly, more explanation for why dilated convolutions and SELU activations would be appreciated. A comparison between GapNet and the same network without the GAP connections could possibly provide a more interesting comparison and might also provide a more pervasive argument as to why GapNet’s should be used. Ultimately, the benefit of using GapNet over the other architectures is not strongly motivated, as training time is less of a concern in this application than predictive power.


RELATED WORK

The authors present previous work in a clear and comprehensive manner. However, the reported finding that “CNNs operating on full images containing hundreds of cells can perform significantly better at assay prediction than networks operating on a single-cell level” is not surprising, and partial evidence of this can be found in the literature. In [1], it was shown that penultimate feature activations from pre-trained CNNs applied to whole-image fluorescence microscopy data (MOA prediction) outperform the baseline segmentation-then-feature extraction method (FNN). Similarly, in [2] (the paper proposing MIL-Net), it is shown that end-to-end whole-image CNN learning for protein localization outperforms the baseline (FNN). In [3] whole image end-to-end learning outperforms whole image extracted features for a phenotyping task. All of these references use fluorescence microscopy data similar to the dataset in this work.

[1] Pawlowski, Nick, et al. ""Automating morphological profiling with generic deep convolutional networks."" bioRxiv (2016): 085118.
[2] Kraus, Oren Z., Jimmy Lei Ba, and Brendan J. Frey. ""Classifying and segmenting microscopy images with deep multiple instance learning."" Bioinformatics 32.12 (2016): i52-i59
[3] Godinez, William J., et al. ""A multi-scale convolutional neural network for phenotyping high-content cellular images."" Bioinformatics 33.13 (2017): 2010-2019.


APPROACH

The authors compile enrich the CellPaining dataset with activity data from various drug discovery assays. In my view, the creation of this dataset is the strongest and most valuable contribution of the paper. The method used to collect the data is described clearly and the choices made when compiling the dataset, including the thresholds and combinations of activity measures seems like a well founded approach.

The authors then identify a number of approaches that are relevant for the problem at hand, binary prediction of drug activity based on image data. These include previous approaches used for cell images and modern image classification networks.


EXPERIMENTS

The different approaches/networks mentioned above were evaluated on a testset. The results indicate that end-to-end CNN approaches outperform all non-end-to-end with no significant difference between the individual end-to-end CNNs. The results are stated clearly and the presentation of different metrics is a nice addition to properly compare the results. It would however contribute valuable information if the authors stated how the confidence intervals of the F1 score are calculated (are the experiments based on several runs of each network or how is it done).


NOVELTY/IMPACT

+ Creation of a new dataset on a new and interesting problem 
+ Useful comparison of modern networks on the task
- GapNet - lacking technical novelty, insight, and performance is unconvincing
- Demonstrates that end-to-end learning outperforms cell centric approach - was this really surprising or even new information?


OTHER NOTES:
* Figure 3 is never mentioned in the main text
* Figure 3 (*’s) are confusing. Do they represent outliers? Statistical significance tests?
* Figure 5 which panel is which?
* Be clear what you mean when you refer to “upper layers” of a network
* An important point not mentioned: in practice, many assays use stains that are closely tied to the readout, unlike the dataset here which provides only landmark stains. The results found here do not necessarily apply in other cases.
","The review starts by acknowledging the interesting and relatively new application of the study, as well as the enrichment of the CellPainting dataset and the comparison of several methods. However, it also points out the weak technical novelty and the unsurprising results. The review provides constructive feedback and specific recommendations for improvement, indicating a balanced sentiment. The language used is polite and professional, offering suggestions without being dismissive or rude.",20,80
"The idea proposed in this paper is to improve classification accuracy by making use of the context.
E.g. on the north pole we will see polar bears but no penguins, on Antartica we have no polar bears but many penguins.
Hence, if we apply our imagenet-like classifier in the wild, we can improve accuracy by taking into account changes in the prior distribution.

The paper proposes a way to rescale the probabilities to do exactly this and reports improved results on modified versions of 
 CIFAR 10 and imagenet with artificial class skew. To achieve this, an additional trick is introduced where the re-scaling is only used when the model is not very certain of its prediction. And additional motivation for this work is that less compute resources are needed if the problem is simplified by utilizing class skew. 

The core idea of the paper is interesting. However, I am not able to understand what exactly is done and I am 100% confident I cannot re-implement it. The authors already improved upon this in our interactions prior to the review deadline. 
An additional issue is that the paper does not have a good baseline. 
I would not like to dismiss the approach based on its simplicity. An elegant solution is always preferred. However, all the tasks are quite artificial and this limits the ""impact"" of this work. If an ""natural"" application/evaluation where this approach would be possible, it would strengthen the paper greatly. 

For the reasons above I recommend rejection of the manuscript in the current state but I am confident that many of these issues can be resolved easily and if this is done I will update the review.

Missing information
----------------------------
- The original manuscript had a lot of information missing, but much of it has since been provided by the authors.
- In the static class skew experiment, were two passes over the data needed? Or was the Pt(i) pre-set? Would it also be possible to give details about LR, optimizer, LR schedule, batch size, .... for the transfer learning experiments. This would enhance reproducibility. 
- For the imagenet experiments how was Pt(i) set in the if I assume correctly, static setting.

Possible additional baselines:
-----------------------------------------

We could make a simpler rescaling by changing the prior distribution and assuming everything else remains constant.
While this is a simplifying assumption, it is very easy to implement and should take only a couple of minutes to run. 
P(i|x)=1/P(X)*P(X|i)*P(i)
Pt(i|x)=P(i|x)*Pt(i)/P(i)

One could also introduce another baseline where only the most probably classes are considered. Since this approach is clearly sub-optimal since it guarantees some mis-predictions it should serve as a lower bound on the performance that is to be expected. 
","The sentiment of the review is mixed. The reviewer acknowledges the interesting core idea of the paper but expresses significant concerns about the clarity, baseline, and practical impact of the work. The recommendation for rejection is tempered by the confidence that the issues can be resolved, indicating a sentiment score of -20. The politeness of the language is high, as the reviewer provides constructive feedback and suggests ways to improve the manuscript without being dismissive or harsh, resulting in a politeness score of 80.",-20,80
"This work aims to use formal languages to add a reward shaping signal in the form of a penalty on the system when constraints are violated. There is also an interesting notion of using an embedding based on the action history to aid the agent in avoiding violations. However, I do not believe this paper did a good enough job in situating this work in the context of prior work — in particular (Camacho 2017). There is a significant related work section that does an ok job of describing many other works, but to my knowledge (Camacho 2017) is the most similar to this one (minus the embedding), yet is not mentioned here. It is difficult to find all related work of course, so I would encourage revision with detailed description of the novelty of this work in comparison with that one. I would also encourage an more thoughtful examination of the theoretical ramifications of the reward shaping signal with respect to the optimal policy as (Camacho 2017) do and as is modeled in the (Ng 1999) paper. As of this revision, however, I'm not sure I would recommend it for publication. Additionally, I suggest that the authors describe the reward shaping mechanism a bit more formally, it was unclear whether it fits into Ng's potential function methodology at first pass.

Comments:

+ It would be nice to explain to the reader in intuitive terms what “no-1D-dithering” means near this text. I understand that later on this is explained, but for clarity it would be good to have a short explanation during the first mentioning of this term as well.
+ It would be good to clarify in Figure 1 what . * (lr)^2 is since in the main text near the figure is is just (lr)^2 and the .* is only explained several pages ahead
+ An interesting connection that might be made is that Ng et al.’s reward shaping mechanism, if the shaping function is based on a state-dependent potential then the optimal policy under the new MDP is still optimal for the old MDP. It would be interesting to see how well this holds under this holds under this schema. In fact, this seems like analysis that several other works have done for a very similar problem (see below).
+ I have concerns about the novelty of this method. It seems rather similar to 

Camacho, Alberto, Oscar Chen, Scott Sanner, and Sheila A. McIlraith. ""Decision-making with non-markovian rewards: From LTL to automata-based reward shaping."" In Proceedings of the Multi-disciplinary Conference on Reinforcement Learning and Decision Making (RLDM), pp. 279-283. 2017.
Camacho, Alberto, Oscar Chen, Scott Sanner, and Sheila A. McIlraith. ""Non-Markovian Rewards Expressed in LTL: Guiding Search Via Reward Shaping."" In Proceedings of the Tenth International Symposium on Combinatorial Search (SoCS), pp. 159-160. 2017.

However, that work proposes a similar framework in a much more formal way. In fact, in that work also a DFA is used as a reward shaping signal -- from what I can tell for the same purpose through a similar mechanism. It is possible, however, that I missed something which contrasts the two works.

Another work that can be referenced:

De Giacomo, Giuseppe, Luca Iocchi, Marco Favorito, and Fabio Patrizi. ""Reinforcement Learning for LTLf/LDLf Goals."" arXiv preprint arXiv:1807.06333 (2018).

I think it is particularly important to situate this work within the context of those others. 

+ General the structure of the paper was a bit all over the place, crucial details were spread throughout and it took me a couple of passes to put things together. For example, it wasn't quite clear what the reward shaping mechanism was until I saw the -1000 and then had to go back to figure out that basically -1000 is added to the reward if the constraint is violated. I would suggest putting relevant details all in one place. For example, ""Our reward shaping function F(x) was  { -1000, constraint violation, 0 otherwise}"". ","The sentiment of the review is mixed but leans towards the negative side. The reviewer acknowledges some interesting aspects of the work but expresses significant concerns about the novelty and the lack of situating the work within the context of prior research, particularly Camacho 2017. The reviewer also mentions that they are not sure they would recommend the paper for publication in its current form. Therefore, the sentiment score is -40. The politeness of the language is quite high. The reviewer uses polite language, such as 'I would encourage,' 'I suggest,' and 'It would be nice,' and provides constructive feedback without being rude or dismissive. Therefore, the politeness score is 80.",-40,80
"This paper presents an end to end rl approach for hierarchical text classification. The paper proposes a label assignment policy for determining the appropropriate positioning of a document in a hierarchy. It is based on capturing the global hierachical structure during training and prediction phases as against most methods which either exploit the local information or neural net approaches which ignore the hierarchical structure. It is demonstrated the method particularly works well compared to sota methods especially for macro-f1 measure which captures the label weighted performance. The approach seems original, and a detailed experimental analysis is carried out on various datasets. 

Some of the concerns that I have regarding this work are :
 - The problem of hierarchical text classification is too specific, and in this regard the impact of the work seems quite limited. 
 - The significance is further limited by the scale of the datasets of considered in this paper. The paper needs to evaluate against on much bigger datasets such as LSHTC datasets http://lshtc.iit.demokritos.gr/. For instance, the dataset available under LSHTC3 is in the raw format, and it would be really competitive to evaluate this method against other such as Flat SVM, and HRSVM[4] on this dataset, and those from the challenge.
- The experimental evaluation seems less convincing such as the results for HRSVM for RCV1 dataset are quite different in this paper, and that given HRSVM paper. It is 81.66/56.56 vs 72.8/38.6 reported in this paper. Given that  81.66/56.56 is not too far from that given by HiLAP, it remains a question if the extra computational complexity, and lack of scalability (?) of the proposed method is really a significant advantage over existing methods.
 - Some of the references related to taxonomy adaptation, such as [3] and reference therein,  which are also based on modifying the given taxonomy for better classification are missing.
 - Comparison with label embedding methods such as [1,2] are missing. For the scale of datasets discussed, where SVM based methods seem to be working well, it is possible that approaches [1,2] which can exploit label correlations can do even better.
[1] K. Bhatia, H. Jain, P. Kar, M. Varma, and P. Jain, Sparse Local Embeddings for Extreme Multi-label Classification, in NIPS, 2015.
[2]  H. Yu, P. Jain, P. Kar, and I. Dhillon, Large-scale Multi-label Learning with Missing Labels, in ICML, 2014.
[3] Learning Taxonomy Adaptation in Large-scale Classification, JMLR 2016.
[4] Recursive regularization for large-scale classification with hierarchical and graphical dependencies, https://dl.acm.org/citation.cfm?id=2487644","The sentiment of the review is moderately positive. The reviewer acknowledges the originality of the approach and the detailed experimental analysis, which indicates a positive sentiment. However, the reviewer also raises several concerns about the specificity of the problem, the scale of the datasets, and the experimental evaluation, which tempers the overall positivity. Therefore, the sentiment score is 30. The politeness of the language is high. The reviewer uses polite language throughout, such as 'Some of the concerns that I have' and 'it remains a question if,' which indicates a respectful and constructive tone. Therefore, the politeness score is 80.",30,80
"I feel like I am missing something about this paper, so rather than a review, this is just mainly a long question making sure I understand things properly.  Ignore the score for now, I'll change once I get a clearer picture of what's happening here.

The network you propose in this paper is motivated by solving PDEs where, as in (1), the actual solution as they are computed numerically depends on the current spatial field of the state, as well as difference operators over this field (e.g., both the gradients and the Laplacian terms).  So, I naturally was assuming that you'd be designing a network that actually represented state as a spatial field, and used these difference operators in computing the next state.  But instead, it seems like you reverted to the notion of ""because difference operators can be expressed as convolutions, we use a convolutional network"", and I don't really see anything specific to PDEs thereafter, just general statements about state-space models.

Am I understanding this correctly?  Why not just actually use the PDE-based terms in the dynamics model of an architecture?  Why bother with a generic ResNet? (And I presume you're using a fully convolutional ResNet here?)  Wouldn't the former work much better, and be a significantly more interesting contribution that just applying a ResNet and a generic U-Net as a state estimator?  I'm not understanding why the current proposed architecture (assuming I understand it correctly) could be seen as ""PDE guided"" in all but the loosest possible sense.  Can you correct me if I'm misunderstanding some element here?","The sentiment of the review appears to be neutral to slightly negative. The reviewer expresses confusion and seeks clarification rather than providing outright criticism or praise. This suggests a sentiment score around -20. The language used is polite and inquisitive, as the reviewer is asking questions and seeking understanding rather than making harsh judgments. Therefore, the politeness score is around 80.",-20,80
"## Strength

This paper explores ways of identifying prototypes with extensive qualitative and quantitative empirical attempts. 

## Weakness

### Not practical

The authors report that “removing individual training examples did not have a measurable impact on model performance”. However, this seems not to be supported by experiments.
First, it is not clear what exactly models do they use in Section 4, e.g. ResnetV2 with how many layers? Learning rate schedules? 
Second, why is the baseline models on CIFAR-10 perform so bad (<90%) even with 100% data?
Third, with `""adv"" metric, we need to perform adversarial-example attacks before training, which has little value in practice. 

### Datasets

They only conduct quantitative experiments (section 4) on relatively small datasets (i.e. MNIST, Fashion-MNIST and CIFAR-10). It is not clear how it will generalize to more realistic settings. 

## Most confusing typos

1. Section 4, paragraph 5, ""However, we find that training only on the most prototypical examples gives extremely high accuracy on the other prototypical examples."" Is there a missing ""than""? It's confused.
2. The description of Figure 6 is not clear enough. Especially there is no explanation to (d, e, f). 
","The sentiment of the review appears to be slightly negative. The reviewer acknowledges the strengths of the paper but focuses more on the weaknesses and areas that need improvement. The sentiment score is therefore -30. The politeness of the language used is relatively high. The reviewer provides constructive criticism without using harsh or rude language, and the feedback is presented in a professional manner. The politeness score is 80.",-30,80
"This paper considers adversarial attack and its defense to DQN. Specifically, the authors propose a poisoning attack that is able to fool DQN, and also propose a modification of DQN that enables the use of strong defense. Experimental results are provided to justify the proposed approach.

Detailed comments:

1.  Although the attack approach seems easy to implement, it would be interesting to see why it works. It might make this paper better if the intuition of the UQP is provided. FGSM is a well-known attack for deep learning models. What is the intuition of using the sign of the gradient of the cross-entropy? Since the argmax is a one-hot vector, this cross-entropy seems ill-defined. How to compute the gradient?

2. It would also be interesting to see why taking actions based on the student network enables better defense.  In DADQN, the authors seem to combine a few tricks proposed by existing works together. It might be better to highlight the contribution and novelty of this approach. ","The sentiment of the review is generally positive as it acknowledges the contributions of the paper and suggests improvements rather than criticizing the work. The reviewer appreciates the proposed approach and the experimental results provided. However, the review also points out areas where the paper could be improved, such as providing more intuition behind the attack approach and highlighting the novelty of the defense mechanism. The language used is polite and constructive, offering suggestions for improvement in a respectful manner.",50,80
"Summarization: This paper studies how to inject structured prior knowledge into the teaching model for machine learning. The authors propose a very general framework called `teach to teach’, in which 1) the knowledge is distilled via subset selection that matches the teacher decision distribution 2) the distilled knowledge is then transferred into the teacher model by reweighting the contributions of teacher objective and coherence constraints. Extensive experiments are conducted on image classification, unsupervised domain adaptation and sequence learning.
 
Questions: 1) Can the teacher models, like those in L2T, be successfully transferred? For example, the teacher model trained with task 1 (with prior knowledge 1) successfully applied to task 2?
2) I’m not that clear about the relationship with knowledge distillation (Hinton et. al 2015). Per my understanding, the authors seem to make the distribution of the knowledge (specified by the set prior function F) coherent with teacher model and let the both influence each other (in section 3.2). In that sense I do not know what is the `dark’ knowledge here.
 
Pros: In general I think this paper is a decent work that the structural prior knowledge is elegantly combined with teaching strategy (a.k.a. the teacher models in curriculum learning). The proposed method is intuitive and natural. The empirical verifications are deep and comprehensive to demonstrate the effectiveness of the `teaching to teach’ framework.
 
Cons: 1) I think the authors should compare with self-paced learning with diversity (SPLD) since you also take diversity as a form of structural knowledge.
2) The writing needs to be significantly polished. First, please simply the writing both in terms of general logic and language. I spent quite a few efforts in figuring out the meaning of some notations and complicated terms such as `curriculum-routed’ and `g_i’. Furthermore, I see no reason of putting so much fancy decorations on an essentially iterative algorithm (the bottom part of page 5 and all page 6). Second,  I suggest the authors give more intuitive and concrete examples towards what is the structural prior knowledge at the earlier phase of the paper, rather than putting most of them into appendix. Last but not least, please use more clear citation formats: currently quite a few citations are missing of publishing venues such as Fan et.al 2018 and Furlanello et.al 2018.","The sentiment of the review is generally positive, as indicated by phrases like 'decent work,' 'elegantly combined,' 'intuitive and natural,' and 'deep and comprehensive.' However, there are some criticisms and suggestions for improvement, which slightly temper the overall positivity. Therefore, the sentiment score is 60. The politeness of the language is quite high, as the reviewer uses polite phrases such as 'I think,' 'please,' and 'I suggest,' and provides constructive feedback without being harsh or rude. Therefore, the politeness score is 80.",60,80
"In this work, the authors tackle the problem of few-shot learning and open-set classification using a new type of NNs which they call alignment-based matching networks or ABM-Nets for short. They main idea is to benefit from binary maps between the query image and the support set (for the case of few-shot learning for the sake of discussion here) to guide the similarity measure. 

I have quite a few concerns;

- After reading the paper two times, I still couldn't find a clear explanation as how the binary map C is constructed. The paper says the cost of M,i,j,k = 1 is C. So what exactly happens given I_t and I_k. My understanding is that a vector representation of each image is obtained and then from those representations the matrix C is constructed (maybe an outer product or something). This does not come out clearly. 

- Nevertheless, I suspect if such a construction (based on my understanding) is the right approach. Firstly, I guess the algorithm should somehow encourage to match more points between the images. Right now the loss  does not have such a term so hypothetically you can match two images because they just share a red pixel which is obviously not right. 

- Aside from the above (so basically regularizing norm(C) somehow), one wonders why matching a point to several others (as done according to matrix C) is the right choice. 

- Related to the issues mentioned before, I may also complain that matching far away points might not be ideal. Currently I do not see how this can be avoided nor a solid statement as why this should not be a problem.  


- Another comment is how the alignment here differs from attention models? They surely resemble each-other though the alignment seems not that rich.


-  last but not least, I have found the language confusing. Some examples,
   -p2 bandwidth signal than the traditional label-only signal : I am very confused by how bandwidth comes to the picture and how this can be measured/justified

  - fig.1, what are \phi and \psi. paper never discussed these.

  - the authors say M is a tensor with 3dimensions. Then the marginalization before eq.1 uses M_{i,\cdot,\cdot} = 1 . What does this really mean?

    ","The sentiment of the review is generally negative, as the reviewer expresses multiple concerns and criticisms about the clarity, methodology, and language of the paper. The reviewer mentions having read the paper twice and still not understanding key aspects, which indicates a significant level of dissatisfaction. The politeness of the language, however, is relatively high. The reviewer uses phrases like 'I have quite a few concerns' and 'one wonders,' which are polite ways to express criticism. The reviewer also provides specific examples and suggestions, which is constructive and respectful.",-70,60
"The paper propose an end-to-end technique that applies both spatial and temporal attention. The spatial attention is done by training a mask-filter, while the temporal-attention use a soft-attention mechanism.  In addition the authors propose several regularization terms  to directly improve attention. The evaluated datasets are action recognition datasets, such as HMDB51, UCF10, Moments in Time, THUMOS’14. The paper reports SOTA on all three datasets. 



Strengths:

The paper is well written: easy to follow, and describe the importance of spatial-temporal attention. 

The model is simple, and propose novel attention regularization terms. 

The authors evaluates on several tasks, and shows good qualitative behavior. 


Weaknesses:

The reported number on UCF101 and HMDB51 are confusing/misleading.  Even with only RGB, the evaluation miss numbers of models like ActionVLAD with 50% on HMDB51 or Res3D with 88% on UCF101. I’ll also add that there are available models nowadays that achieve over 94% accuracy on UCF101, and over 72% on  HMDB51. The paper should at least have better discussion on those years of progress. The mis-information also continues in THUMOS14, for instance R-C3D beats the proposed model. 

In my opinion the paper should include a flow variant. It is a common setup in action recognition, and a good model should take advantage of these features. Especially for spatial-temporal attention, e.g., VideoLSTM paper by Li. 

In general spatial attention over each frame is extremely demanding. The original image features are now multiplied by 49 factor, this is more demanding in terms of memory consumption than the flow features they chose to ignore.  The authors reports on 15-frames datasets for those short videos. But it will be interesting to see if the model is still useable on longer videos, for instance on Charades dataset. 

Can you please explain why you chose a regularized making instead of Soft-attention for spatial attention? 

To conclude: 
The goal of spatial-temporal attention is important, and the proposed approach behaves well. Yet the model is an extension of known techniques for image attention, which are not trivial to apply on long-videos with many frames. Evaluating only on rgb features is not enough for an action recognition model. Importantly, even when considering only rgb models, the paper still missed many popular stronger baselines. 

","The sentiment of the review is mixed. The reviewer acknowledges the strengths of the paper, such as its well-written nature, simplicity, and novel attention regularization terms. However, the reviewer also points out significant weaknesses, including misleading reported numbers, lack of discussion on recent progress, and the omission of flow variants. The sentiment score is therefore slightly positive but tempered by the criticisms. The language used is polite and constructive, offering specific suggestions for improvement without being rude or dismissive.",20,80
"The authors propose a hierarchical model of symbolic music that takes explicit advantage of measures and chords to construct the hierarchy. Their model is very similar to SampleRNN (2-level RNN Autoregressive Model) but with an additional cross-entropy loss for chord labels at the higher level and a summarization connection passing back to the high level from the low-level at the end of each bar. They show that given monophonic music with chord labels their model is able to produce reasonably coherent chords and note samples, and improves the NLL over a low-level model alone. 

The core of their approach (using measures as a natural hierarchy for a multi-level RNN) is a good one, but not new in of itself as it was the basis for the prior work of Roberts et al. (http://proceedings.mlr.press/v80/roberts18a/roberts18a.pdf). The authors highlight in section 3.3 that their work is distinguished by the summarization connection, but do not provide any evidence in their results that the connection is useful. They find in Table 1 that connection hurts NLL on the note level, and do not compare summarized to non-summarized models in the listening tests. 

The area for most improvement in the paper is the evaluation, especially the listening tests. The authors compare samples from four models that generate different types of outputs and were trained on different datasets. Because of this, the notion of user preference is completely convoluted with external factors. In particular the comparisons to DeepBach and SequenceTutor are inappropriate and give little information about the quality of the model architecture itself. To be useful comparisons should be restricted to model architectures that are trained on the exact same data as HAPPIER, and output both chords and melodies like HAPPIER does. Given that the novelty of the paper rests on the summarization connections, and they were not shown to help NLL, it would be natural to try and compare the different model variants in the paper and see if the NLL misses some element of larger structure that listeners may care about. My rating is thus based on the lack of novelty and poor quality of evaluation justifying the actual novel aspects of the paper. 

Some minor comments that could also help improve the paper:

* Including NLL for chords is important to compare summarization (does it help in chord prediction?)
* The input representation could use further clarifying. What is the dictionary of chords to predict from? Are they just chord names or individual notes (the figures imply notes, but that doesn't seem what's happening). In Figure 2, clarify the meaning of tick, what 1, 0 means in terms of time progression.
* Provide quantitative evidence for the claims in 4.2 that the notes and chords belong to the same key. Compare real data and generated data for those statistics. 
* Provide explanation for why Note NLL is higher for Summarization.
* Minor notation problems: Eq 1, f should not be a function of n_i. Similar, in Eq 2, p(n_{ij}) should be a function of c_i. Eq 3 doesn't define what the hat represents. ","The sentiment of the review is mixed but leans towards the negative side. The reviewer acknowledges the core approach as good but criticizes the lack of novelty and poor quality of evaluation. The sentiment score is -40 because the review points out significant flaws and lacks enthusiasm for the work. The politeness score is 50 because the language is constructive and professional, offering specific recommendations for improvement without being harsh or rude.",-40,50
"This paper presents a method for learning the Q function from multi-agent demonstrations, such as trajectories of soccer players in a soccer game.  The basic ideas are:

-- The policy class pi(m|s) maps states s to actions m probabilistically.  In particular, the probability class is a mixture of Gaussians, with the mean/covariance functions and the mixture function all being instantiated via neural nets.  The behavior of the individual players are assumed to be conditionally independent given the current state.

-- A hand-designed reward function is used, such as the ball being in a ""strike zone"" of some kind for the team on offense. 

-- The Q function class is also a neural net.

-- The policy pi is learned through maximum likelihood of the state/action pairs of the the pre-collected demonstrations.  This is essentially probabilistic behavioral cloning over the policy class.

-- The Q function is learned via Q-fitting over the demonstration data, i.e., minimizing squared error on the Bellman residual.  This requires the pre-specified reward function, the policy pi, and some details regarding the state transitions (e.g., the non-deterministic transitions of the ball). 

-- The policy pi and the Q function are trained jointly by adding the two objectives.

-- The main empirical evaluation is comparing the quality of the Q function estimation against a naive baseline (random movements) and a non-mixture policy class (i.e., a single mode, rather than a mixture of Gaussians).  The results against the non-mixture policy class do not seem to show much difference in performance.  

-- Some qualitative evaluations are also presented, but it's unclear what insight one is expected to gain from looking at them.


**Clarity**
The paper is reasonably well written.  Some aspects of the logical flow could be improved, but overall it's fine.  Detailed comments are:

-- The proposed approach is essentially imitation learning and not, strictly speaking, reinforcement learning.  Yet the work is positioned as reinforcement learning.  Perhaps it's just a difference in norms of terminology usage.

--  There are more multi-agent RL & IL works than what was discussed in the related works section.  Of course, an exhaustive discussion is not expected, but the writing makes it sound like there hasn't been much work at all.  Examples include:
https://arxiv.org/abs/1807.09936
https://arxiv.org/abs/1706.02275
http://www.cs.ox.ac.uk/people/shimon.whiteson/pubs/rashidicml18.pdf

-- The authors comment in the related work that team sports cannot yet be simulated.  This statement is sort of true, although it's not clear how true. For instance, the physics engines of many sports video games are very realistic.  Moreover, it doesn't strike me as the most prominent point of contrast with [Hausknecht & Stone 2016].  For instance, a more prominent point of contrast is simply imitation learning vs reinforcement learning.  

-- It's not clear what the authors mean specifically when they refer to ""average performance"" contrasting with [Le et al. 2017b].  Also, desn't that work also condition on game context and situation? 

-- Section 4.1 is written in a somewhat disorganized way.  The exposition alternates between discussing baselines and the evaluation methodology in a way that is confusing. 

-- One of the baselines is described but then left to the appendix for analysis.  This doesn't make sense from a narrative perspective.


**Originality**
From a technical perspective, it's not clear that there's much novelty in this approach.  All the ingredients are pretty standard, and the ingredients are put together in a standard way. 

From an applications perspective, there might be some novelty here but it's not clear.  The application of imitation learning to sports data is not new, but also not saturated either.  The specific idea of learning a better rating system is interesting, but it's not clear how fleshed out this application is in the paper (more comments on this below).


**Significance**
For me, the key question for significance boils down to: ""Does this paper change the way people think about doing X?"" for the most interesting instantiation of X possible.  In this case, it seems X should be computing a rating system for multi-agent spatial multi-agent behavior (as suggested by the title).  But the limitations in the model and the experiments make this a questionable proposition.  

The model class all but ignores the multi-agent aspect (the agents are all independent conditioned on the states). Moreover there was no comparison against a model class that made a worse assumption on how to handle the multi-agent aspect.  So it's not clear how the multi-agent aspect is significant.

The evaluation of the rating approach is not convincing.  The approach essentially performed as well as a single-mixture baseline.  Furthermore, the direction V(s) learning approach is deferred to the appendix, and I'd like to see it included in the main paper.  Finally, it would be nice if there was some attempt to compare with a non-MDP based approach.  For instance, people directly compute things like P(goal|state).  I'd like to see evidence that this style of approach will outperform those approaches, which are the de facto standard in sports analytics right now.


**Overall Quality**
Based on the above comments, it is my opinion that this paper has too many holes in it to be ready for publication at a venue such as ICLR.  As a primarily applications paper, the burden is on the authors to demonstrate reasonably convincing evidence that this line of approach is valuable to the application.","The sentiment of the review is mixed, leaning towards negative. The reviewer acknowledges that the paper is reasonably well-written and has some interesting ideas, but they also point out several significant flaws and limitations in the methodology, originality, and significance of the work. The sentiment score is therefore -40. The politeness of the language is generally respectful and constructive, with the reviewer providing detailed feedback and suggestions for improvement without being rude or dismissive. The politeness score is 60.",-40,60
"This is a paper with scattered potentially interesting ideas. But the execution is limited and the writing poor with critical details lacking.  A major limitation of the paper is that it is not clear what contribution it makes. Some of the analyses are indeed interesting but 1) these analyses are mostly descriptive and 2) they are limited to one particular (outdated) architecture. How would batch norm or residual connections or any of the developments that have happened since AlexNet affect these results?

As a side note, the references/comparisons between AlexNet and recurrent nets (see abstract, etc) are misleading. This is based on the claim that Bowers et al (2014) qualitatively different results but this is for entirely different domains (words). Indeed what could have made potentially the work more relevant would have been to show some kind of benchmarking between AlexNet and alternative architectures (possibly RNNs). As such the current study does not contribute much except for comparing different semi-arbitrary measures of selectivity for one specific (outdated) network architecture trained on a particular problem (ILSVRC).

****
Minor points:

The study is limited to correctly classified images as stated on page 3. This seems like a major confound in a study aimed at understanding the visual representations learned. It seems to me that the conclusions of the paper could be heavily biased because of this (when computing any measure based on inter and intraclass responses).

In general, this is a relatively poorly written paper which would be hard to reproduce. For instance, the image generation for activating units (assuming it is novel) could be interesting but it is not even described with sufficient details so as to reproduce the results.","The sentiment score is derived from the overall tone and content of the review. The reviewer acknowledges some potentially interesting ideas but criticizes the execution, writing quality, and lack of clear contribution. This results in a sentiment score of -60, indicating a predominantly negative sentiment with some acknowledgment of positive aspects. The politeness score is based on the language used. While the review is critical, it avoids personal attacks and maintains a professional tone, resulting in a politeness score of 20, indicating a slightly polite but critical review.",-60,20
"This work provides theoretical insights on recent learning rate proposals such as Cyclical Learning Rates (Smith et al.). The authors focus on stochastic approximation i.e. how large is the SGD loss as a function of condition number and horizon. The critical contribution is the theoretical benefit of oscillating learning rates over more traditional learning rate schemes. Authors provide novel upper/lower bounds to establish benefit of oscillating LR, support their theory with experiments and provide insights on finite horizon learning rate selection. An important drawback is that results only apply to linear regression which is a fairly simple setup.

I have two important comments regarding this work:
1) I believe proof of Theorem 3 has a bug. In the proof, authors use the inequality
(1-gamma_t lambda^k)^2 < exp(-2lambda^k gamma_t).
Obviously this can only be correct for gamma_t lambda^k<1. However, checking the setup of the problem, it can be seen that for largest eigenvalue and gamma_0, ignoring log factors:

gamma_0L = L/(mu T_e)=kappa / T_e=kappa/T.

Since, no restriction is imposed on T, gamma_0L can be as large as O(kappa) and invalidates the above inequality. So T should be T>O(kappa). I am not sure if this affects the overall statement or the remaining argument.

2) The paper can benefit from more detailed experiments (e.g. Figs 1 and 2). Arguably the most obvious baseline is ""constant learning rate"". However, authors compare to 1/T or 1/sqrt(T) learning rates. It is not at all clear from current experiments, if the proposed approach beats a good constant LR choice.

I am happy to increase my score if the comments above are addressed.","The sentiment of the review is generally positive, as the reviewer acknowledges the theoretical insights and novel contributions of the paper. However, the reviewer also points out significant drawbacks and areas for improvement, particularly regarding the proof of Theorem 3 and the need for more detailed experiments. The politeness of the language is high, as the reviewer provides constructive feedback and expresses willingness to increase their score if the comments are addressed.",50,80
"This paper analyses the learning dynamics of GANs by formulating the problem as a primal-dual optimisation problem. This formulation assumes a limited class of models -- Wasserstein GANs with discriminators using linear combinations of base functions. Although this setting is limited, it advanced our understanding of a central problem related to GANs, and provides intuition for more general cases. The paper further shows the same analysis can be applied to multi-task learning and distributed learning.

Pros:

* The paper is well written and well motivated
* The theoretical analysis is solid and provide intuition for more complex problems

Cons:

* The primal-dual formulation assumes Wasserstein GANs using linear discriminator. This simplification is understandable, but it would be helpful to at least comment on more general cases.

* Experiments are limited: only results from GANs with LQG setting were presented. Since the assumption of linear discriminator (in basis) is already strong, it would be helpful to show the experimental results from this more general setting.

* The results on multi-task learning were interesting, but the advantage of optimising the mixing weights was unclear compared with the even mixture baseline. This weakens the analysis of the learning dynamics, since learning the mixing did not seem to be important.

It would also be helpful to comment on recently proposed stabilising methods. For example, would spectral normalisation bring learning dynamics closer to the assumed model?","The sentiment of the review is generally positive, as the reviewer acknowledges the paper's contributions to understanding GANs and its solid theoretical analysis. However, the reviewer also points out some limitations and areas for improvement, which slightly tempers the overall positivity. Therefore, the sentiment score is 60. The language used in the review is polite and constructive, offering specific suggestions for improvement without being harsh or dismissive. Therefore, the politeness score is 80.",60,80
"This paper is of high quality and clarity. I think it's originality is at least decent. Whether it is significant or not depends on how significant one thinks fully connected neural networks are as these are the models for which this explanation model makes sense.

Good things:
- It is a very elegant method. It is also very simple (in a good way).
- The paper is really well written.
- The experiments are carefully conducted and are indeed showing what the authors describe.
- I think the method is potentially of practical use.

Problems:
- I think qualifying this paper as a paper on representation learning is a small stretch. It would be perhaps more suitable to submit it to ICML or NIPS. I think it is close enough though.
- The font is too small in many figures. It is impossible to read it. 
- I am not sure whether model compression is actually necessary here. How good is the additive model if it is trained as a standalone model straight from the training data in comparison to the neural networks and to the additive model when trained with model compression? If the neural network and the additive model were similar in performance when trained from scratch, I would not see the point in explaining the neural network.
- Only makes sense to apply this to fully connected networks.","The sentiment of the review is generally positive, as indicated by phrases like 'high quality and clarity,' 'elegant method,' 'well written,' and 'carefully conducted experiments.' However, there are some critical points, such as the suitability of the paper for the current conference, readability issues with figures, and questions about the necessity of model compression. These critiques are constructive rather than negative. The politeness of the language is high, as the reviewer uses polite and respectful language throughout, even when pointing out issues.",70,90
"This paper presents an interesting way to reformulate intrinsic curiosity as a differentiable function. The authors compare the differentiable function against using prediction error via REINFORCE and DQN, showing that their intrinsic curiosity method results in more interactions with unseen objects than the other two methods. For DQN this is to be expected, but it shows that backprop through this function is more efficient than reinforce in getting to unseen state spaces. I think this is an interesting method/proposal and is a somewhat novel reformulation of intrinsic error, but I do have some concerns in comparisons/claims. 

In the introduction, the authors say that the intrinsic curiosity method proposed by Pathak et al. is sample inefficient and isn’t tested in robots. However, to my understanding the REINFORCE baseline isn’t really equivalent (though it may be possible that it is, it was unclear how exactly the loss was formulated in the baseline, did include the other components from Pathak et al.?). If the claim is that this method is more efficient, I think it should have compared against that method directly. 

Moreover, I think the description of the experiments doesn’t provide enough information. For example, the method says that different learning rates were used for the min-max game to stabilize it, but doesn’t say what they were. 
Also, for the DQN baseline what were the parameters? Was there an epsilon greedy policy on top of the exploration reward? Was this annealed as in other work? Generally, I think more detail is needed throughout (even if it just refers to a more detailed appendix).

Overall, I think this work needs to be revised to include more details on hyperaparameters, details on the baselines, and describing differences between Pathak et al.’s method and the REINFORCE baseline. Moreover, feedback from other comments on this work should be addressed which reflect in more detail my comments below on opinionated claims (e.g., https://openreview.net/forum?id=SkzeJ3A9F7&noteId=HJlFlZOa2X )


Comments/Thoughts:

+ I think in the introduction there are some statements that probably need citations. For example, “But the same formulation from an optimization viewpoint, it suffers from all the bad properties of extrinsic rewards. The reward is a function of environment behavior with respect to the performed action. Since the environment behavior function is unknown, it is treated as black-box and hence the gradients have to be computed using REINFORCE (Williams, 1992) which is quite sample inefficient.” —> Why is this true? Is there a citation that can back this? Do you prove it later in the paper? 
+ “Yes, 54 environments but no real-world physical robots” —> this and the intro seems like a blogpost at times. That can be fine (some would argue it’s a good thing), but there seem to be some opinions without citations/backing, I suggest trying to back up statements wherever possible and avoid opinions. For example in this statement, robots aren't a requirement for evaluating intrinsic motivation.
+ “Since the environment behavior function is unknown, it is treated as black-box and hence the gradients have to be computed using REINFORCE (Williams, 1992) which is quite sample inefficient.” —> citation/backing? it might be nice to point to the experiment section here to back it (e.g., ""As will be shown in Section X and in \citet{something}, REINFORCE can be quite sample inefficient"")
+ “In practice, the existing on-policy algorithms, e.g., A3C (Mnih et al., 2016), PPO (Schulman et al., 2017) etc. are deployed off-the shelf -> This is confusing, so is this using REINFORCE or PPO/A3C? what is this statement referring to?
+ “regress to rti to learn value estimates (i.e., off-policy) as discussed in the previous section” —> regress to \sum r_t{I} for a value estimate?? Value is the expected return so not sure if this is a typo or i missed something earlier
+ What is the actual loss function used for the baseline? Is it the same as Pathak et al.?
+ What are the hyper parameters for DQN exploration? What are all the hyper parameters for any/all the algorithms? 
+ Was a variance-reducing baseline used in REINFORCE?
+ What is the variance representing in the graphs, std across several trials? Maybe I missed it, but how many trials represent this standard deviation?
+ “Hence, we train the forward predictor slightly faster than the policy by keeping higher learning rate to stabilize the learning process. “ —> what were the learning rates?


Linguistic/Typos:

Also, some minor, but frequent, grammatical issues/typos that I’ve added below could be fixed. I would ask that the authors please have the submission proof-read for English style and grammar issues. There are many minor mistakes, some of which I’ve tried to point out below. 

+ “This leads to a significantly sample efficient exploration policy. “ —> significantly more (?) sample efficient ?

“Why is that? To understand the reason behind sample inefficiency of curiosity or intrinsic rewards, notice how the intrinsic rewards are given by agent” —> by the agent?

“Forward model fθF is trained to minimize its loss which amounts to minimizing rti with respect to θF” —> the forward model

“However, policy is optimized to maximize the objective” —> However, the policy

“We can also optimize  for policy parameters θP via differentiable loss function” —> We can also optimize for (the) policy parameters \theta via (a) differentiable loss function?

“To optimize policy to maximize a discounted sum “ —> To optimize the policy

“How good is Forward Prediction Model” —> How good is the forward prediction model

There are several other spots, but basically another pass over the paper might be worth it to check for these sorts of issues. 
","The review starts with a positive sentiment, appreciating the novelty and interest of the proposed method. However, it quickly transitions into a critical analysis, pointing out several concerns and areas for improvement. The sentiment score is therefore moderately positive but tempered by the critical feedback. The language used is polite and constructive, offering specific suggestions for improvement without being harsh or dismissive.",40,80
"i take reviewing very seriously, and it often takes hours per paper. this paper, however, has many typos, grammatical errors, and seems to have been submitted last minute.  therefore, i have read the paper quickly.
that said, i do not understand the results.
clearly, many discretization methods have previously been described, as alluded to by citing the taxonomy paper on the subject.  the authors state they have developed a better approach.  however, i do not see a comparison to the state of the art in the simulations, and i do not follow the results of Table 2, which columns correspond to which particular algorithms? in either case, the proposed approach does not seem to improve the empirical results, nor have theoretical guarantees, so i am not particularly impressed with the results either.","The sentiment of the review is quite negative, as the reviewer points out numerous issues with the paper, including typos, grammatical errors, lack of clarity in results, and a failure to compare with the state of the art. The reviewer explicitly states that they are not impressed with the results. Therefore, the sentiment score is -80. The politeness score is -20, as the language used is somewhat blunt and critical, though not overtly rude. The reviewer does not use any explicitly rude language but does express dissatisfaction in a straightforward manner.",-80,-20
"This paper studies the problem of robust policy optimization, motivated by the fact that policies that work in simulations do not transfer well to real world. The authors propose to use the diversity measure on the roll out trajectories to select a diverse set of simulator configurations and train policies that can work well for all of those selected configurations. 

Overall, I think the idea is interesting but it is not entirely clear why adding diverse configurations should result in good performance, and the experiments are very limited and not convincing enough. 

Pros:
- The paper is easy to follow.
- The idea of using a diverse summary to do robust policy optimization is interesting. 
- The diversity measure on the trajectories instead of the space of configuration parameters also intuitively makes sense since it takes into account that the similarity between two configuration parameters does not typically mean the similarity between their corresponding policies. 

Cons:
- The setting of this paper seems to only work for the fully observable case with state space being in R^d, deterministic dynamics and deterministic policy (otherwise the diversity measure would be stochastic?). It would be good to clarify these in Sec. 2. 
- For the example in Fig 2 and the first experiment, what I don't understand is why the initial state is not part of the policy.
- It is not clear if the reason that EP-OPT performed worse than the proposed approach is only because there are not enough rollouts for EP-OPT. This could be an unfair comparison.
- It would be good to show the comparison to EP-OPT for the second experiment as well. 
- Two experiments may not be enough to verify valid performance since there could be a lot of randomness in the results.
- In page 6, it would be good to clarify that the summary being optimal is only with respect to f(M_s), but not the original problem of finding optimal policy. ","The sentiment of the review is mixed, leaning slightly towards positive. The reviewer acknowledges the interesting idea and the clarity of the paper but raises significant concerns about the experimental validation and some methodological aspects. Therefore, the sentiment score is 10. The language used in the review is polite and constructive, offering specific suggestions for improvement without being harsh or dismissive. Thus, the politeness score is 80.",10,80
"
> Even though the paper details the underlying Markovian setup in Section 2, it is unclear to the reader how this knits with the FFNN architecture, for example what are the Markovian functions at hidden layer and output layer. Are they all conditional probabilities? How do you prove that this is what occurs within each node?

> Why is the functional form of f_\theta in Eq 1? 

> How many hidden layers are in place?

> What is the Stochastic dynamical process in Figure A and how is this tethered to DyMon? 

> The authors mention an nth-order Markovian process implemention but is this not the case with any fully connected neural network implementation? What the reader fails to see is why DyMoN is different to these already-existing architectures.

> In the teapot example, the authors mention a DyMoN architecture. (Page 8). Is this what is used throughout for all the experiments? If yes, why is it generalizable and if not, what is DyMoN’s architecture? You could open the DyMoN box in Figure 10 (1) and explain what DyMoN consists.
 

Section 2 is the crux of the paper and needs more work - explain the math in conjunction to the ‘deep’ architecture, what is the 'deep' architecture and why it is needed at all. Then go on to show/prove that the Markovian processes are indeed being realized. 

","The sentiment of the review is generally neutral to slightly negative. The reviewer points out several areas of the paper that are unclear or need further explanation, but does not provide any positive feedback or praise. The tone is critical but not overly harsh. The politeness of the language is neutral; the reviewer asks questions and makes suggestions without using rude or overly critical language, but also does not use particularly polite or encouraging language.",-20,0
"The paper proposes a data augmentation technique where the input image is sub-sampled by randomly sampling rows and columns without replacement, which the authors call ‘pseudosaccades’. Rather than multiple classifiers, the authors ensemble using multiple ‘pseudosaccades’ as input, with the same network.

Comments:
I think that the proposed augmentation is a neat trick. However, the inner-workings of the method are poorly presented (or not well understood). For eg. In section 3.5, while discussing the effects of the method on individual classes, the authors mention ‘different architectures do tend to be affected by the pseudosaccades differently’ and provide no further insights.

There are no experiments that compare this method with other standard data augmentation techniques. For instance, one could use a similar ensembling technique for transformations like shear, translation, rotation, etc. by randomly sampling their corresponding parameters. I would be interested in experimental results that compare the proposed ensemble with ensembles constructed using these common techniques.

Since there is no reason for this technique to be used in isolation (I found no such motivation in the paper), it would be insightful to have experimental results where this technique is combined with the aforementioned standard augmentation techniques. Will this method’s impact on the accuracy change with these other augmentations? (Ablation studies would be useful).

This is a a form of regularization and can be thought of reverse structured dropout. Also have the authors compared this with Cutout [1, 2]? Similar experiments and comparisons would be insightful.

[1] Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017.
[2] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data augmentation. arXiv preprint arXiv:1708.04896, 2017.

In summary:
The performance improvements are incremental. The paper lacks sufficient technical contribution. Further, it does not provide comparisons with standard techniques and similar augmentation methods to demonstrate the usefulness of the method.  ","The sentiment of the review is mixed but leans towards the negative side. The reviewer acknowledges the proposed augmentation as a 'neat trick,' which is a positive remark. However, the majority of the review focuses on the shortcomings of the paper, such as poor presentation of the method's inner workings, lack of comparative experiments, and insufficient technical contribution. Therefore, the sentiment score is -40. The politeness of the language is generally respectful and constructive, as the reviewer provides specific recommendations and does not use derogatory language. Thus, the politeness score is 60.",-40,60
"The paper proposes yet another variant of the celebrated Dropout algorithm. Specifically, the proposed method attempts to address the obvious drawbacks of Dropout: (i) the need to heuristically select the Dropout rate; and (ii) the universality of this selection across a layer. 

As the authors have admitted in the paper (Sec. 1.2), there is a variety of methods already addressing the same problem. They argue that contrary to some of these methods ""jumpout does not rely on additional trained models: it adjusts the dropout rate solely based on the ReLU activation patterns. Moreover, jumpout introduces negligible computation and memory overhead relative to the original dropout methods, and can be easily incorporated into existing model architectures.""

However, this is argument is certainly untrue and rather misleading. The works of Kingma et al. (2015) and Molchanov et al. (2017), that the authors cite, does not introduce additional trained models. In addition, there is additional related work that the authors do not cite, but ought to: 

[1] Yarin Gal, Jiri Hron, Alex Kendall, ""Concrete Dropout,"" Proc. NIPS 2017.
[2] Yingzhen Li, Yarin Gal, ""Dropout Inference in Bayesian Neural Networks with Alpha-divergences,"" Proc ICML 2017.
[3] Harris Partaourides, Sotirios Chatzis, “Deep Network Regularization via Bayesian Inference of Synaptic Connectivity,” J. Kim et al. (Eds.): PAKDD 2017, Part I, LNAI 10234, pp. 30–41, 2017. 

These methods also address a similar problem, without introducing extra networks or imposing extra costs art inference time. Thus, citing them, as well as COMPARING to them, is a necessity for this paper to be convincing.

These crucial shortcoming aside, there are various theoretical claims in this paper that are not sufficiently substantiated. To begin with, the arguments used in the last paragraph of page 4 seem at least speculative; then,  the authors proceed to propose a solution to the alleged problem in the beginning of page 5. They suggest sampling from a truncated Gaussian, but they do not elaborate on why this selection solves the problem; they limit themselves to noting that other selections, such as the Beta distribution, may also be considered in the future. We must also underline that [3] have suggested exactly that; sampling from a Beta. 

Finally, the last two modifications the authors propose seem reasonable, yet they are extremely heuristic. No one knows (which can be guaranteed through theoretical proofs or solid experimental evidence) that without these the method would not work. In addition, previous papers, e.g. [1-3] achieve similar goals in a principled fashion (ie by inferring proper posterior densities); without experimental comparisons, nobody knows which paradigm is best to adopt. 

","The sentiment of the review is predominantly negative. The reviewer criticizes the novelty of the proposed method, stating that it does not offer significant improvements over existing methods and even misrepresents the contributions of previous works. The reviewer also points out several theoretical and citation shortcomings. Therefore, the sentiment score is -80. The politeness of the language is relatively neutral to slightly negative. While the reviewer does not use overtly rude language, the tone is quite critical and dismissive, particularly when stating that the authors' arguments are 'certainly untrue and rather misleading.' Thus, the politeness score is -20.",-80,-20
"The authors present a simple algorithm based on the statistics of neural activations of deep networks to detect out-of-distribution samples. The idea is to use the existing running estimate of mean and variance within BatchNorm layers to construct feature representations that are later fed into a simple linear classifier. The authors demonstrate superior performance over the previous state-of-art in the standard evaluation setting and provide fascinating insights and empirical analysis of their method.

There are several aspects of this work that I admire.

- The authors evaluate the generalization of their OOD detection model through evaluation against unseen OOD samples. This critical evaluation strategy is not typical in this literature and is much needed.
- The organization of the material and the depth of the discussion is of high quality. They discuss and connect the previous work, they clearly explain the idea and provide empirical results to support the design decisions, and run several experiments to evaluate their method from different angles followed by interesting discussions.
- The proposed method is easy to implement and has a minimal runtime complexity with no adverse effect on the underlying classifier.
- The source code is already included in the submission.

My only concern is that the feature pooling strategy first averages the input spatially, then across the channels. This feature size reduction is necessary because we have to ensure the following OOD classifier does not overfit in the validation stage. However, this reduction also introduces a permutation invariance in the feature space that is not desirable in OOD sample detection. I think it would make the work more valuable if the authors also take a critical look at the possible failure cases -- a short discussion of the weaknesses and assumptions. 

Overall, the paper is technically sound and well-organized with sufficient coverage of the previous work. A thorough series of evaluations support the claims. It is a novel combination of existing techniques. The empirical evidence is strong and insightful. Given the simplicity of the method, I would expect a quick adoption by the community.
------
Rev. In light of the rebuttal and the following discussions I have updated my rating to 7.","The review is highly positive, praising the authors for their innovative approach, thorough evaluation, and clear presentation. The sentiment score is high because the reviewer admires several aspects of the work and expects quick adoption by the community. The politeness score is also high as the language used is respectful and constructive, even when pointing out a concern.",90,95
"This paper combines a number of ideas to train generative models with (deep) structured constraints. The general idea is similar to Flow-GAN, which learns a normalizing flow-based generator by optimizing the negative loglikelihood with an augmented GAN loss. However, It’s difficult to impose prior structure information in the GAN framework. To address this problem, the authors proposed to minimize a so-called Gibbs-regularized variational bound of Jeffery divergence, which is the summation of KL and reverse KL divergence. The authors provide some justification that the Jeffery divergence works by yielding good mass-covering and mode-seeing properties. 

It appears that the parameterization and adaptation of v throughout optimization is the key contribution of this work --- the technical details are not clear from the paper.

1.    Typo in the training objective (Eq .1):  the second (or the first) ""sup"" should be removed? 

2.    Section 2.3 is very confusing. Particularly, how is the parameter \phi introduced? What’s the detailed update of \phi? 
- ""We now observe that our methods can also be interpreted as a way of learning v as a Gibbs distribution approximating p."" If v_\phi(x) is a distribution, what’s the parametric form of v?  
- ""Generally, this is achieved by structuring the energy function V_\phi:=\log v_\phi."" It seems that V_\phi(x) is a scalar-valued function that represents the negative energy of the distribution v_\phi(x), however, why the distribution is self-normalized? Specifically, why \int \exp(V_\phi) dx = 1? Otherwise, how the authors deal with the partition function \int \exp(V_\phi(x)).  
- It is unclear to me why the inner loop optimization is connected with Itakura-Saito divergence minimization? The authors may consider including the detailed proofs?

3.    With the given description, the proposed algorithm is not easy to follow and implement by the reader. The paper would benefit from an Algorithm box with pseudocode.

If the authors can fully address the concerns above, I will consider changing the scores.  

Other comments:
1. The empirical results are fairly weak. Similar datasets are used, the authors may consider evaluating their approach on various different tasks. 

2. Duplicate citations – R2P2 [35] [36]

3. Other related papers:
 - Belanger et al., End-to-End Learning for Structured Prediction Energy Networks, ICML 17
- Tu et al., Learning Approximate Inference Networks for Structured Prediction, ICLR 18","The sentiment of the review appears to be neutral to slightly positive. The reviewer acknowledges the novelty and potential contribution of the paper but also points out significant areas of confusion and weakness, particularly in the technical details and empirical results. The sentiment score is thus around 10. The politeness of the language is quite high; the reviewer uses polite and constructive language throughout, even when pointing out flaws and making recommendations. The politeness score is therefore 80.",10,80
"The authors argue that graph neural networks based on the message passing frameworks are not able to infer the topological structure of graphs. Therefore, they propose to use the node embedding features from DeepWalk as (additional) input for the graph convolution. Moreover, a graph pooling operator is proposed, which clusters node pairs in a greedy fashion based on the l2-distances between feature vectors. The proposed methods are evaluated on seven common benchmark datasets and achieve better or comparable results to existing methods. Moreover, the method is evaluated using synthetic toy examples, showing that the proposed extensions help to infer topological structures.

A main point of criticism is that the authors claim that graph convolution is not able to infer the topological structure of a graph when no labels are present. In fact the graph convolution operator is closely related to the Weisfeiler-Lehman heuristic for graph isomorphism testing and can distinguish most graphs in many practical application. Therefore, it is not clear why DeepWalk features would increase the expressive power of graph convolution. It should be stated clearly which structural properties can be distinguished using DeepWalk features, but no with mere graph convolution.
The example on page 4 provides only a weak motivation for the approach: The nodes v_1 and v_2 should be indistinguishable since they are generated using the same generator. Thus, the problem is the mean/max pooling, and not the graph convolution. When using the sum-aggregation and global add pooling, graphs with two clusters and graphs with three clusters are distinguishable again. Further insights how DeepWalk helps to learn more ""meaningful"" topological features are required to justify its use.

Clustering nodes that are close in feature space for pooling is a reasonable idea. However, this contradicts the intuition of clustering neighboring nodes in the graph. A short discussion of this phenomenon would strengthen the paper in my opinion.

There are several other questions that not been answered adequately in the article.

* The 10-fold cross validation is usually performed using an additional validation set. What kind of stopping criteria has bee use? * It would be helpful to provide standard deviations on these small datasets (although a lot of papers sadly dismiss them).
* I really like the use of synthetic data to show superior expressive power, but I am unsure whether this can be contributed to DeepWalk or the use of the proposed pooling operator (or both). Please divide the results for these toy experiments in ""GEO-DEEP"" and ""GEO-deep no pooling"". As far as I understand, node features in different clusters should be indistinguishable from each other (even when using DeepWalk), so I contribute this success to the proposed pooling operator.
* A visualization of the graphs obtained by the proposed pooling operator would be helpful. How do the coarsened graphs look like? Given that any nodes can end up in the same cluster, and the neighborhood is defined to be the union of the neighboring nodes of the node pairs, I guess coarsened graphs are quite dense.
* DiffPool (NIPS 2018, Ying et al.) learns assignment matrices based on a simple GCN model (and thus infers topological structure from message passing). How is the proposed pooling approach related to DiffPool (except that its non-differentiable)? How does it perform when using only the features generated by a GCN? How does it compare to other pooling approaches commonly used, e.g., Graclus? At the moment, it is quite hard to judge the benefits of the proposed pooling operator in comparison to others.


In summary, the paper presents promising experimental results, but lacks a theoretical justification or convincing intuition for the proposed approach. Therefore, at this point I cannot recommend its acceptance.


Minor remarks:

* p2: The definition of ""neighbour set"" is needless in its current form.
* p2: The discussion of graph kernels neglects the fact that many graph kernels compute feature vectors that can be used with linear SVMs.

-----------
Update:
The comment of the authors clarified some misunderstandings. I now agree that the combination of DeepWalk features and GNNs can encode more/different topological information. I still think that the paper does not make this very clear and does not provide convincing examples. I have update my score accordingly.","The sentiment score is derived from the overall tone and content of the review. The reviewer acknowledges the promising experimental results and the potential of the proposed methods, but also highlights significant criticisms and concerns, particularly regarding the theoretical justification and clarity of the approach. This mixed feedback results in a sentiment score of -20, indicating a slightly negative sentiment. The politeness score is based on the language used throughout the review. The reviewer maintains a professional and respectful tone, even when pointing out flaws and making suggestions for improvement. This results in a politeness score of 80, indicating a high level of politeness.",-20,80
"The paper aims at a better understanding of the positive impacts of Batch Normalisation (BN) on network generalisation (mainly) and  convergence of learning. First, the authors propose a novel interpretation of the BN re-parametrisation. They show that an affine transform of the variables with their local variance (scale) and mean (shift) can be interpreted as a decomposition of the gradient of the objective function into a regressor assuming that the gradient is parallel to the variables (up to a shift) and the residual part which is the gradient w.r.t. to the new variables. In the second part of the paper, authors review various normalisation proposals (differing mainly in the subset of variables over which the normalisation statistics is computed) as well as the known empirical findings about the dependence of BN on the batch size. The paper presents an experiment that combines two normalisation variants. A further experiment strives at regularising BN for small batch sizes.

Unfortunately, it remains unclear what questions precisely the authors answer in the second part of the paper and, what is more important, how they are related to the novel interpretation of BN presented in the first part. This interpretation holds for any function and can be possibly seen as a gradient pre-conditioning. However, the authors do not ""extend"" it towards the gradients w.r.t. the network parameters and do not consider the specifics of the learning objectives (a sum of functions, each one depending on one training example only). The main presented experiment combines layer normalisation with standard batch normalisation for a convolutional network. The first one normalises using the statistics over channel and spatial dimensions, whereas the second one uses the statics over the batch and spatial dimensions. The improvements are rather marginal, but, what is more important, the authors do not explain how and why this proposal follows from their new interpretation of BN.

Overall, in my view, this paper is premature and not appropriate for publishing at ICLR in its present form.
","The sentiment score is determined by the overall tone and content of the review. The reviewer acknowledges the aim and some aspects of the paper but ultimately concludes that the paper is premature and not appropriate for publishing in its current form. This indicates a negative sentiment, though not extremely so, as the reviewer provides constructive criticism. Therefore, the sentiment score is -60. The politeness score is assessed based on the language used. The reviewer uses polite and professional language throughout, even when providing critical feedback. There are no rude or harsh words, and the critique is framed in a constructive manner. Thus, the politeness score is 80.",-60,80
"The paper casts the problems of value learning and policy optimization, which can be problematic the non-linear setting, into the bilevel optimization framework. It proposes two novel algorithms with convergence guarantees. Although other works with similar guarantees exist, these algorithms are very appealing for their simplicity. A limited empirical evaluation is provided for the value-based method in Acrobot and Mountain Car and in the Atari games Pong and Breakout for the proposed bilevel Actor Critic.

There are a few missing references to similar, recent work, including Dai et al’s saddle-point algorithm (https://arxiv.org/pdf/1712.10285.pdf). Also, the claim that this is “the first attempt to study the convergence of online reinforcement learning algorithms with nonlinear function approximation” can’t be true (even replacing ‘attempt’ by ‘successfully’, there is e.g. Maei et al.’s nonlinear GTD paper, see below).

Although certainly interesting, the claims relating bilevel optimization and the target network are not completely right. E.g. Equation 3.6 as given is a hard constraint on omega. More explicitly, there are no guarantees that either network is the minimizer of the RHS quantity in 3.6.

The two-timescale algorithm is closer in spirit to the use of a target network, but in DQN and variants the target network is periodically reset, as opposed to what the presented theory would suggest. A different breed of “soft target” networks, which is more closely related to bilevel optimization has been used to stabilize training in DDPG (https://arxiv.org/abs/1509.02971).

There was some confusion for me on the first pass that you define two algorithms called ‘online Q-learning’ and ‘actor-critic’. Neither algorithm is actually that, and they should be renamed accordingly (perhaps ‘bilevel Q-Learning’ and ‘bilevel actor-critic’?). In particular, standard Q-Learning is online; and the actor-critic method does not minimize the Bellman residual (i.e. I believe the RHS of 3.8 is novel within policy-gradient methods).

Once we’re operating on a bounded space with continuous operators, Theorem 4.2 is not altogether surprising – a case of Brouwer’s fixed point theorem, short of the result that theta* = omega*, which is explained in the few lines below the theorem. While I do think Theorem 4.2 is important, it would be good to contrast it to existing results from the GTD family of approaches. Also, requiring that |Q_theta(s,a)| <= Qmax is a significant issue -- effectively this test fails for most commonly used value-based algorithms.

The empirical evaluation lacks any comparison to baselines and serves for little more than as a sanity check of the developed theory. This is probably the biggest weakness of the paper, and is unfortunate given the claim of relevance to e.g. deep RL.



Questions

Throughout, the assumption of the data being sampled on-policy is made without a clear argument as to why. Would the relaxation of this assumption affect the convergence results?

Can the authors provide an intuitive explanation if/why bilevel optimization is necessary?

Can you contrast your work with Maei et al., “Convergent Temporal-Difference Learning with Arbitrary Smooth Function Approximation”?


Suggestions

The discussion surrounding the target network should be improved. In particular, claiming that the DQN target network can be viewed “as the parameter of the upper level optimization subproblem” is a stretch from what is actually shown.

The paper was sometimes hard to follow, in part because the claims are not crisply made. I strongly encourage the authors to more clearly relate their results to existing work, and ensure that the names they use match common usage.

I would have liked to know more about bilevel optimization, what it aims to solve, and the tools used to do it. Instead all I found was very standard two time-scale methods, which was a little disappointing – I don’t think these have been found to work particularly well in practice. This is particularly relevant in the context of e.g. the target network question.

A proper empirical comparison to existing algorithms would significantly improve the quality and relevancy of this work. There are tons of open-source baselines out there, in particular good state of the art implementations. Modifying a standard implementation to optimize its target network along the lines of bilevel optimization should be relatively easy.

Revision:
I thank the authors for their detailed feedback, but still think the work isn't quite ready for publication. After reading the other reviews, I will decrease my score from 6 to 5. Some sticking points/suggestions:
- Some of my concerns remain unanswered. E.g. the actor-critic method 3.8 is driven by the Bellman residual, which is not the same as e.g. the MSPBE used with linear function approximation. There is no harm in proposing variations on existing algorithms, and I'm not sure why the authors are reluctant to do. Also, Brouwer's fixed point theorem, unlike Banach's, does not require a contractive mapping.
- The paper over-claims in a number of places. I highly recommend that the authors make their results more concrete by demonstrating the implications of their method on e.g. linear function approximation. This will also help contrast with Dai et al., etc.","The sentiment of the review is mixed. The reviewer acknowledges the novelty and potential of the proposed algorithms but also points out several significant issues and areas for improvement. The sentiment score is therefore slightly negative. The language used is generally polite and constructive, with suggestions for improvement and specific questions for the authors, indicating a high level of politeness.",-20,80
"Quality is good, just a handful of typos.
Claritys above average in explaining the problem setting.
Originality: scan refs...
Significance: medium
Pros: the authors develop a novel GAN-based approach to denoising, demixing, and in the process train generators for the various components (not just inference). Further, for inference, the authors propose an explicit procedure. It seems like a noveel approach to demixing which is exciting.
Cons: The experiments do not push the limits of their method. It's difficult to judge the demixing 'power' of the method because it's difficult to tell how hard the problem is. Their method seems to easily solve it (super low MSE). The classification measure is clearly improved by denoising, which is totally unsurprising-- There should definitely be comparison with other denoising methods.

In general, they don't compare to any other methods. Actually in the appendix, comparisons are provided for a basic compressive sensing problem, but their only comparator is ""LASSO"" with a ""fixed regularization parameter"", and vanilla GAN. Since the authors ""main contribution"" (their words) is demixing, I'm surprised that they did not compare with other demixing approaches, or try on a harder problem. Could you  give some more details about the LASSO approach? How did you choose the L1 parameter?

I have another problem with the demixing experimental setting. On one hand, both the sinusoids and MNIST have ""similar characteristics"" in the sense that they are both pretty sparse, basically simple combinations of primary curves. This actually makes the problem harder for a dictionary learning approach like MCA (referenced in your paper). On the other hand, both signals are very simple to reconstruct. For example, what if you superimposed the grid of digits onto a natural image? Would you be able to train the higher resolution GAN to handle a more difficult setting? The other demixing setting of adding 1's and 2's has a similar problem.

The authors need to provide (R)MSE  results that show how well the method can reconstruct mixture components on average over the dataset. The only comparison is visual, and no comparators are provided.

Conclusions:
I'm actually torn on this paper. On one hand this paper seems novel and clearly contributes to the field. On the other hand, HOW MUCH contribution is not addressed experimentally, i.e. the method is not properly compared with other denoising or demixing methods, and definitely not pushed to its limits. It's hard to assess the difficulty of the denoising problem because their method does so well, and it's hard to assess the difficulty of demixing because of the lack of comparators.

Caveats:
I am knowledgeable about iterative optimization approaches to denoising and demixing, especially MCA (morphological component analysis), but *not knowledgeable about GAN-based approaches*, though I have familiarity with GANs.

*********************
Update after author response:
I think the Fashion-MNIST experiments and comparisons with ICA are many times more compelling than the original experiments. I think this is an exciting contribution to dually learning component manifolds for demixing.","The sentiment of the review is generally positive, as the reviewer acknowledges the novelty and potential contribution of the paper. However, there are significant concerns about the experimental validation and comparisons with other methods, which tempers the overall sentiment. Therefore, the sentiment score is 30. The language used is polite and constructive, with specific suggestions for improvement and no derogatory remarks, resulting in a politeness score of 80.",30,80
"The authors study the task of sample-based quantitative evaluation applied to GANs. The authors suggest multiple modifications to existing evaluation pipelines: (1) Instead of embedding the samples in the InceptionNet feature space, train a domain-specific encoder. If labeled data is available, add a cross-entropy loss to the encoder training objective so that the class can be predicted. (2) Instead of fitting a single Gaussian in the feature space, fit a GMM instead. This should allow for a more fine-grained “class-aware” distance between the (empirical) distributions. 

Pro: 
Attempt to attack a critical issue in generative modeling. Good overview of competing approaches.
Several ablation studies of evaluation measures and the behavior of FID with respect to the representation space.
The ideas make sense on a conceptual level, albeit suffering from major practical concerns.

Con:
- Clarity can be improved (e.g. use of double negatives as in the top of page 3), the same arguments repeated multiple (>3) times (i.e. deficiencies of FID and IS, etc.), Many statements which should be empirically tested are stated as folklore (last paragraph on page 3). In general the paper merits another polishing pass (mode != model, last paragraph in  section 3, “unmatch”, etc.).
- Why would a VAE capture a good feature space? It is known that the tradeoff between what is stored in the latent space versus the discriminator *completely* depends on the power of the discriminator -- if the discriminator is flexible enough it can just learn the marginal distribution and ignore the latent code. Hence, this subtle issue will likely undermine the entire model comparison.
- Using the predictive distribution as a soft label for CAFD. Interesting idea, but why would one have access to labels in the first place? Why wouldn't one use a conditional GAN if we already have labels? Secondly, why would the modes necessarily correspond to classes?
- Stated issues with FID: Why would you expect FID to be resistant to such drastic transformations as blocking out a significant proportion of pixels with “blocks”? This is a *major* change in the underlying distribution. The fact that humans can “fill in” this gap should have nothing to do with the quality of the underlying model. Arguably, you can also hide one eye, the nose and the mouth and still judge the sample as “good”.

The ideas presented in this paper are conceptually interesting. However, given the drawbacks discussed above I cannot recommend the acceptance of this work.
","The sentiment score is derived from the overall tone and content of the review. The reviewer acknowledges the conceptual interest and attempts to address critical issues in generative modeling, which is positive. However, the review also highlights significant practical concerns and ultimately does not recommend acceptance, leading to a mixed sentiment. Therefore, the sentiment score is slightly negative at -20. The politeness score is based on the language used throughout the review. The reviewer uses polite and professional language, even when pointing out flaws and making recommendations. There are no rude or harsh comments, so the politeness score is high at 80.",-20,80
"This paper proposes a curriculum that encourages training on easy examples first and postpones training on hard examples. However, contrary to common ideas, they propose to keep hard examples contribute to the loss and only forcing them to have internal representations similar to a nearby easy example. The proposed objective is hence biased at the beginning but they dampen it over time to converge to the true objective at the end.

Positives:
- There is not much work considering each example as an individual subtask.
- The observation that an under-fitted classifier can destroy a good feature extractor is good.

Negatives:
- In the intro it says “[update rule of gradient descent] assumes the top layer, F2, to be the right classifier.”. This seems like a fundamental misunderstanding of gradient descent and the chain rule. The term d output/d F1 takes into account the error in F2.
- The caption of figure 2 says the “... they cannot separate from its neighbors…”. If the loss of all examples in a cluster is high, all are being misclassified. A classifier then might have an easy job fixing them if all their labels are the same or have a difficult job if their labels are random. The second scenario is unlikely if based on the claim of this figure, the entropy has decreased during training. In short, the conclusion made in fig 2 does not necessarily hold given that figure.
- This method is supposed to speed up training, not necessarily improve the final generalization performance of the model. The figures show the opposite outcomes. It’s not clear why. The improvement might be due to not tuning the hyperparameters of the baselines.
- Figure 3 does not necessarily support the conclusion. The fluctuations might be caused by any curriculum that forces a fixed ordering across training epochs. Often on MNIST, the ordering of data according to the loss does not change significantly throughout training.","The sentiment of the review appears to be mixed, with both positive and negative points highlighted. The reviewer acknowledges some strengths of the paper, such as the novel approach and specific observations, but also points out several significant weaknesses and misunderstandings. Therefore, the sentiment score is slightly negative. The language used in the review is generally polite and professional, even when pointing out the negatives, which suggests a high politeness score.",-20,80
"*Summary:
This paper analyzes the convergence of ADAM and RMSProp to stationary points
in the non convex setting.
In the second part the authors experimantally compare the performance of these methods to Nesterov's Accelerated method.



*Comments:

-The paper does not tell a coherent story and the two parts of the paper are somewhat unrelated.

-The authors claim that they are the first to analyze adaptive methods in the non-convex setting, yet this was recently done in 
[Xiaoyu Li, Francesco Orabona; On the Convergence of Stochastic Gradient Descent with Adaptive Stepsizes]
The authors should cite this paper and compare their results to it.

-The above paper of [Li and Orabona] demonstrates a nice benefit of AdaGrad in the non-convex setting. Concretely they show that in the noisless setting adaptive methods give a faster rate of $O(1/T)$ compared to the standard rate of $O(1/\sqrt{T})$ of SGD.

Unfortunately, the results of the current paper do not illustrate the benefit of adaptive methods over SGD, since the authors provide similar rates to SGD or even worse rates in some situations.
I think that in light of [Li and Orabona] one should expect a $O(1/T)$ rate also for ADAM and RMSProp.


-The experimental part is not so related to the first part. And the experimental phenomena is only demonstrated for the MNIST dataset, which is not satisfying. 


*Summary:
The main contribution of this paper is to provide rates for approaching stationary points.
This is done for ADAM and RMSProp, two adaptive training methods.
The authors do not mention a very relevant reference, [Li and Orabona].
Also, the authors do not show if ADAM and RMSProp have any benefit compared to SGD in the non-convex setting, which is a bit disappointing. Especially since [Li and Orabona] do demonstrate the benefit of AdaGrad in their paper.
","The reviewer's sentiment is generally negative, as they point out several significant issues with the paper, such as the lack of coherence, failure to cite relevant work, and unsatisfactory experimental results. The sentiment score is -60 because the reviewer does acknowledge the main contribution but is largely critical. The politeness score is 20 because the language is direct and critical but not overtly rude. The reviewer uses phrases like 'unfortunately' and 'a bit disappointing,' which indicate a level of politeness despite the negative feedback.",-60,20
"This paper studies ReLU model, or equivalently, one-layer-one-neuron model, for the classification problem. This paper shows if the data is linearly separable, gradient descent may converge to either a global minimum or a sub-optimal local minimum, or diverges. This paper further studies the implicit bias induced by GD and SGD and shows if they converge, they can have a maximum margin solution. 

Comments:
1. Using ReLU model for linearly separable data doesn't make sense to me. When ReLU is used, I expect some more complicated separable condition. 
2. This paper only studies one-layer-one-neuron model, which is a very restricted setting. It's hard to see how this result can be generalized to the multiple-neuron case.
3. The analysis follows closely with previous work in studying the implicit bias for linear models.
","The sentiment of the review is generally negative, as the reviewer expresses several concerns about the relevance and generalizability of the study. The reviewer questions the use of the ReLU model for linearly separable data, the restricted setting of the one-layer-one-neuron model, and the originality of the analysis. The politeness of the language is relatively neutral; while the reviewer is critical, they do not use harsh or rude language.",-60,10
"The authors provided a training scheme that ensures network retains old performance as new data sets are encountered (e.g. (a) same class no drift, (b) same class with drift, (c) new class added). They do this by incrementally adding FC layers to the network, memory component that stores previous precomputed features, and the objective is a coupling between classification loss on lower level features and a feature-loss on retaining properties of older distributions. The results aren't very compelling and the approach looks like a good engineering solution without strong theoretical support or grounding. ",The sentiment of the review is slightly negative as it acknowledges the effort and the approach taken by the authors but criticizes the results and theoretical grounding. The sentiment score is -30. The politeness of the language is neutral; the reviewer does not use any rude or overly harsh language but is straightforward in their critique. The politeness score is 0.,-30,0
"Comments: 

The author(s) provide stability and generalization bounds for SGD with momentum for strongly convex, smooth, and Lipschitz losses. 

This paper basically follows and extends the results from (Hardt, Recht, and Singer, 2016). Section 2 is quite identical but without mentioning the overlap from Section 2 in (Hardt et al, 2016). The analysis closely follows the approach from there. 

The proof of Theorem 2 has some issues. The set of assumptions (smooth, Lipschitz and strongly convex) is not valid on the whole set R^d, for example quadratic function. In this case, your Lipschitz constant L would be arbitrarily large and could be damaged your theoretical result. To consider projected step is true, but the proof without projection (and then explaining in the end) should have troubles. 

From the theoretical results, it is not clear that momentum parameter affects positively or negatively. In Theorem 3, what is the advantage of this convergence compared to SGD? It seems that it is not better than SGD. Moreover, if \mu = 0 and \gamma > 0, it seems not able to recover the linear convergence to neighborhood of SGD. Please also notice that, in this situation, L also could be large. 

The topic could be interesting but the contributions are very incremental. At the current state, I do not support the publications of this paper. 
","The review starts by acknowledging the contributions of the paper, which is a positive note. However, it quickly transitions into pointing out significant overlaps with previous work and several critical issues with the theoretical results. The reviewer also mentions that the contributions are incremental and explicitly states that they do not support the publication of the paper. The language used is professional and constructive, but it is clear and firm in its criticism.",-40,50
"Summary:
The paper presents a method for ""learning an optimizer""(also in the literature Learning to Learn and a form of Meta-Learning) by using a Variational Optimization for the ""outer"" optimizer loss. The mean idea of the paper is to combine both the reparametrized gradient and the score-function estimator for the Variational Objective and weight them using a product of Gaussians formula for the mean. The method is simple and clearly presented. The paper also presents issues with the standard ""learning to learn"" optimizers, one being the short-horizon bias and as credited by the authors has been observed before in the literature, and the second one is what is termed the ""exponential explosion of gradients"" which I think lacks enough justification as currently presented (see below for details). The ideas are clearly stated, although the work is not groundbreaking, but more on combining several ideas into a single one. 

Experiments: 
The authors evaluate their method on a single task which consists of optimizing a 3-layer convolutional neural network on downsampled images from ImageNet. A key idea, not new to this work, is to optimize the meta-optimizer with respect to the validation dataset rather than the training, which seems to be crucial for any meaningful training to happen. Although the experiments do show so promising results, they seem to be somewhat limited (see below for details). There is also a small ablation study on how do different features presented to the optimizer affect its performance. Given the still small-scale experiments, I'm not sure this is a significant result for the community. 

Conclusion:
As a whole, I think the idea in the paper is a good one and worth investigating further. However, the objections I have on section 2.3 and the experiments seem to indicate that there needs to be more work into this paper to make it ready for publication. 


On section 2.3 and the explosion of gradients:

There is a mistake in the equation on page 4 regarding the ""gradient with respect to the learning rate"". Although the derivation in Appendix A is correct, the inner product in the equation starts wrongly from j=0, where it should in fact start at j = i + 1. To be more clear the actual enrolled equation for dw^T/dt for 3 steps back is:

dw^T/dt = (I - tH^{T-1})(I - tH^{T-2})(I - tH^{T-3}) dw^{T-3} - (I - tH^{T-1})(I - tH^{T-2}) g^{T-3} - (I - tH^{T-1}) g^{T-2} - g^{T-1} 

Hence the product must start at j = i + 1. 
It is correct that in this setting the equation is a polynomial of degree T of the Hessian, however, there are several important factors that the authors have not discussed. Namely, if the learning rate is chosen accordingly such that the spectral radius of the Hessian is less than 1/t then rather than the gradient exploding the higher order term will vanish. However, even if they do vanish for large T since the Hessian plays with smaller and smaller power to more recent gradients (after correcting the mistake in the equation) than the actual T-step gradient will never vanish (in fact even if tH = I then dw^T/dt = g^{T-1}). Hence the claims of exploding gradients made in this section coupled with the very limited theoretical analysis seem to unconvincing that this is nessacarily an issue and under what circumstances they are. 

The toy example with l(w) = (w - 4)(w - 3) w^2 is indeed interesting for visualizing a case where the gradient explosion does happen. However, surprisingly here the authors rather than optimizing the learning rate, which they analyzed in the previous part of the section, they are now optimizing the momentum. The observation that at high momentum the training is unstable are not really surprising as there are fundamental reasons why too high momentum leads to instabilities and these have been analyzed in the literature. Additionally, it is not mentioned what learning rate is used, which can also play a major role in the effects observed. 

As a whole, although the example in this section is interesting, the claims made by the authors and some of the conclusions seem to lack any significant justifications, in addition to the fact that usually large over-parameterized models behave differently than small models. 


Experiments:

I have a few key issues with the experimental setup, which I think need to be addressed:

1. The CNN being optimized is quite small - only 3 layers. This allows the authors to train everything on a CPU. The key issue here, as well with previous work on Learning to Learn, is that it is not clear how scalable is this method to very Deep Networks. 

2. Figure 1 - The setup is to optimize the problem for 10000 iterations, however, I think it is pretty clear even to the naked eye that the standard first-order optimizers (Adam/RMS/Mom) have not fully converged on this problem. Hence I think its slightly unfair to compare their ""final performance"" after this fixed period. Additionally using the curriculum the ""meta""-optimizer is trained explicitly for 10000 iterations. Hence, it is also unclear if it retains its stability after letting it run for longer. From the text it is also unclear whether the authors have optimized the parameters of the first-order methods with respect to their training or validation performance - I hope this is the latter as that is the only way to fairly compare the two approaches. 

3. Figure 6 - the results here seem to indicate that the learned optimizer transfers reasonably well, achieving similar performance to first-order methods (slightly faster validation reduction). Given however that these are plots for only 10000 iterations it is still unclear if this is scalable to larger problems. 

","The review starts with a balanced overview of the paper, acknowledging the clarity and simplicity of the method presented. However, it also points out that the work is not groundbreaking and combines existing ideas. The reviewer provides constructive criticism on specific sections, particularly on the 'explosion of gradients' and the experimental setup. The language used is polite and professional, aiming to help the authors improve their work rather than dismissing it outright. The sentiment is slightly positive as the reviewer sees potential in the paper but has significant reservations that need addressing. The politeness is high, as the reviewer maintains a respectful tone throughout.",20,80
"* The proposed SGLD-SA algorithm, together with its convergence properties, is very interesting. The introduction of step size $w^{k}$ is very similar to the ""convex combination rule"" in (Zhang & Brand 2017) to guarantee convergence.
  
* It seems that this paper only introduced Bayesian inference in the output layers. It would be more interesting to have a complete Bayesian model for the full network including the inner and activation layers.

* This paper imposed spike-and-slab prior on the weight vector which can yield sparse connectivity. Similar ideas have been explored to compress the model size of deep networks (Lobacheva, Chirkova and Vetrov 2017; Louizos, Ullrich and Welling 2017 ). It would make this paper stronger to compare the sparsification and compression properties with the above work.

* In equation (11) there is a summation from $\beta_{p+1}$ to $\beta_{p+u}$. I wonder where this term comes from, as I thought $\beta$ is a vector of dimension $p$.

Reference:
Zhang, Ziming, and Matthew Brand. ""Convergent block coordinate descent for training tikhonov regularized deep neural networks."" Advances in Neural Information Processing Systems. 2017.

Lobacheva, Ekaterina, Nadezhda Chirkova, and Dmitry Vetrov. ""Bayesian Sparsification of Recurrent Neural Networks."" arXiv preprint arXiv:1708.00077 (2017).

Louizos, Christos, Karen Ullrich, and Max Welling. ""Bayesian compression for deep learning."" Advances in Neural Information Processing Systems. 2017.

","The review starts with a positive remark about the proposed algorithm and its convergence properties, indicating an appreciation for the work. However, it also points out several areas for improvement, such as the limited scope of Bayesian inference and the need for comparisons with existing work. The language used is constructive and polite, offering suggestions for enhancement rather than outright criticism. The reviewer also raises a technical question about a specific equation, which is done in a neutral and inquisitive manner.",50,80
"The paper discusses clustering sparse sequences using some mixture model. It discusses results about clustering data obtained from a restaurant loyalty program.

It is not clear to me what the research contribution of the paper is. What I see is that some known techniques were used to cluster the loyalty program data and some properties of the experiments conducted noted down. No comparisons are made. I am not sure what to evaluate in this paper. ",The sentiment of the review is quite negative as the reviewer expresses confusion about the research contribution and indicates that the paper does not offer new insights or comparisons. The sentiment score is -80. The politeness of the language is neutral; the reviewer does not use rude or harsh language but is straightforward in their critique. The politeness score is 0.,-80,0
"The paper ""Neural Distribution Learning for generalized time-to-event prediction"" proposes HazardNet, a neural network framework for time-to-event prediction with right-censored data. 
 
First of all, this paper should be more clear from the begining of the kind of problems it aim to tackle. The tasks the proposal is able to consider is not easy to realize, at least before the experiments part. The problem should be clearly formalized in the begining of the paper (for instance in the introduction of section 3). It the current form, it is very hard to know what are the inputs, are they sequences of various kinds of events or only one type of event per sequence. It is either not clear to me wether the censoring time is constant or not and wether it is given as input (censoring time looks to be known from section 3.4 but in that case I do not really understand the contribution : does it not correspond to a very classical problem where events from outside of the observation window should be considered during training ? classical EM approaches can be developped for this). The problem of unevenly spaced sequences should also be more formally defined. 

Also, while the HazardNet framework looks convenient, by using hazard and survival functions as discusses by the authors, it is not clear to me what are the benefits from recent works in neural temporal point processes which also define a general framework for temporal predictions of events. Approaches such at least like ""Modeling the intensity function of point process via recurrent neural networks"" should be considered in the experiments, though they do not explicitely model censoring but  with slight adapations should be able to work well of experimental data. 


","The sentiment of the review is slightly negative, as it points out several areas where the paper lacks clarity and suggests improvements. The reviewer expresses difficulty in understanding the problem being addressed and questions the novelty of the contribution. The sentiment score is -40. The politeness of the language is neutral to slightly polite. The reviewer uses phrases like 'should be more clear' and 'it is not clear to me,' which are constructive but not overly polite. The politeness score is 20.",-40,20
"This paper while presenting interesting ideas, is very poorly written. It seems as though the authors were in a rush to submit a manuscript and did not even bother with basic typesetting.
Firstly, the paper spends too much time motivating and re-introducing the model of Arora et.al. Note to the authors here, they cite the same paper from Arora et.al for 2017 twice. The first time the model they refer to was introduced by the paper ""RAND-WALK: A latent variable model approach to word embeddings"", this is probably what the authors mean by the 2016 reference?

Now coming to the experiments, the results are presented in a table that is poorly formatted. The section partitions are not clearly delimited, making for a hard read. Even if we overcome that and look at the results, the presented numbers are incredibly confusing. On the STS 13 and 15 data sets, Ethayarajh 2018's numbers are much better at 66.1 and 79.0. Coming to STS14 Ethayarajh attain 78.4 while the proposed method achieves 78.1. If we discount this for the moment, and look at the results on STS12 where the proposed method achieves 71.4, this is the only data set where the proposed method does better than the other baselines.

So almost on 3 of the 4 datasets Ethayarajh 2018 does better. This makes me question what exactly is the proposed model improving?

Coupled with the fact that there is no motivation to explain results or future work, this makes for a very poorly written paper that is very challenging to read.

It is very likely that there is some merit to the proposed methods that introduce non linearity, but these points simply get lost in the mediocre presentation.","The sentiment of the review is quite negative, as the reviewer repeatedly points out flaws in the paper's writing, formatting, and experimental results. The reviewer uses phrases like 'very poorly written,' 'incredibly confusing,' and 'mediocre presentation,' which indicate a strong dissatisfaction with the manuscript. The politeness of the language is also low, as the reviewer uses direct and somewhat harsh language without much cushioning or constructive feedback. Phrases like 'did not even bother with basic typesetting' and 'very challenging to read' contribute to a tone that could be perceived as rude.",-80,-60
"This paper extends the definition of adversarial examples to the ones that are “far” from the training data, and provides two conditions that are sufficient to guarantee the non-existence of adversarial examples. The core idea of the paper is using the epistemic uncertainty, that is the mutual information measuring the reduction of the uncertainty given an observation of the data, to detect such faraway data. The authors provided simulation studies to support their arguments.

It is interesting to connect robustness with BNN. Using the mutual information to detect the “faraway” datapoint is also interesting. But I have some concerns about the significance of the paper:
1.  The investigation of this paper seems shallow and vague. 
    (1). Overall, I don’t see the investigation on the “typical” definition of adversarial examples. The focus of the paper is rather on detecting “faraway” data points. The nearby perturbation part is taken care by the concept of “all possible transformations” which is actually vague.
    (2). Theorem 1 is basically repeating the definition of adversarial examples. The conditions in the theorem hardly have practical guidance: while they are sufficient conditions, all transformations etc.. seem far from being necessary conditions, which raises the question of why this theory is useful? Also how practical for the notion of “idealized NN”?
    (3). What about the neighbourhood around the true data manifold? How would the model succeed to generalize to the true data manifold, yet fail to generalize to the neighbourhood of the manifold in the space?  Delta ball is not very relevant to the “typical” definition of adversarial examples, as we have no control on \delta at all.
2. While the simulations support the concepts in section 4, it is quite far from the real data with the “typical” adversarial examples. 

I also find it difficult to follow the exact trend of the paper, maybe due to my lack of background in bayesian models. 
1. In the second paragraph of section 3, how is the Gaussian processes and its relation to BNN contributing to the results of this paper?
2. What is the rigorous definition for \eta in definition 1?
3. What is the role of $\mathcal{T}$, all the transformations $T$ that introduce no ambiguity, in Theorem 1. Why this condition is important/essential here?
4. What is the D in the paragraph right after Definition 4? What is D’ in Theorem 1?
5. Section references need to be fixed. 



","The sentiment of the review is mixed, leaning towards negative. While the reviewer acknowledges the interesting aspects of the paper, such as connecting robustness with Bayesian Neural Networks (BNN) and using mutual information to detect faraway data points, they express significant concerns about the depth, clarity, and practical significance of the investigation. The reviewer points out several specific issues, such as the vagueness of the investigation, the practical utility of Theorem 1, and the relevance of the simulations to real data. Therefore, the sentiment score is -40. The politeness of the language is relatively high. The reviewer uses polite language, such as 'I have some concerns' and 'I also find it difficult to follow,' and provides constructive feedback without being rude or dismissive. Therefore, the politeness score is 80.",-40,80
"This paper provides a theoretical perspective of the dual learning tasks and proposes two generalizations (multipath/cycle dual learning) that utilize multiple language sets. Through experiments, the paper discusses the relationship between theoretical perspective and actual translation quality.

Overall, the paper is well written and discussed enough. My concern is about Theorem 1 that could be a critical problem.
In the proof of Theorem 1, it discussed that the dual learning can minimize Case 2. This assumption is reasonable if the vanilla translator is completely fixed (i.e., no longer updated) but this condition may not be assumed by the authors as far as I looked at Algorithm 2 and 3 that update the parameters of vanilla translators directly. The proof is constructed by only the effect against Case 2. However, if the vanilla translator is directly updated through dual training, there should exist some random changes in also Case 1 and this behavior should also be included in the theorem.

Correction and suggestions writing:
* It is better to introduce an additional $\alpha$, $\beta$ and $\gamma$ for the vanilla translation accuracy (e.g., $\alpha_0 := p_{ij}p_{ji}^r$) so that most formulations in Section 3 can be largely simplified.
* In Justification of Assumption1 ... ""the probability of each cluster is close to $p_max$"" ->  maybe ""greater than $p_max$"" to satisfy the next inequality.
* Eq. (3) ... $T_{ji}^d(T_{ij}^d(x^{(i)})$ -> $T_{ji}^d(x^{(j)})$ to adjust other equations.
* Section 3 Sentence 1: ""translatorby"" -> ""translator by""
* Section 4.2: ${\rm Pr}_{ X^{(3)} \sim T_{23}^d (X^{(1)}) }$ -> $... (X^{(2)}) }$","The sentiment of the review is generally positive, as the reviewer acknowledges that the paper is well written and discussed enough, but raises a critical concern about Theorem 1. The sentiment score is therefore 50, indicating a positive but cautious tone. The politeness of the language is high, as the reviewer uses polite language and constructive suggestions without any negative or rude remarks. The politeness score is 90.",50,90
"(Since the reviewer was unclear about the OpenReview process, this review was earlier posted as public comment)

Most claims of novelty can be clearly refuted such as the first sentence of the abstract ""...This work presents a new approach to active anomaly detection..."" and the paper does not give due credit to existing work. Current research such as Das et al. which is the most relevant has been deliberately not introduced upfront with other works (because it shows lack of the present paper's novelty) and instead deferred to later sections. The onus of a thorough literature review and laying down a proper context is on the authors, not the reviewers. Detailed comments are below.
      
      1. Related Works: ""...active anomaly detection remains an under-explored approach to this problem...""
          - Active learning in anomaly detection is well-researched (AI2, etc.). See related works section in Das et al. 2016 and:
            - K. Veeramachaneni, I. Arnaldo, A. Cuesta-Infante, V. Korrapati, C. Bassias, and K. Li, ""Ai2: Training a big data machine to defend,"" International Conference on Big Data Security, 2016.
        
      2. ""To deal with the cold start problem, for the first 10 calls of select_top..."":
          - No principled approach to deal with cold start and one-sided labels (i.e., the ability to use labels when instances from only one class are labeled.)
        
      3. Many arbitrary hyper parameters as compared to simpler techniques:
          - The number of layers, nodes in hidden layers.
            - The number of instances (k) per iteration
            - The number of pretraining iterations
            - The number of times the network is retrained (100) after each labeling call
            - Dealing with cold start (10 labeling iterations of 10 labels each, i.e. 100 labels).
        
      4. The paper mentions that s(x) might not be differentiable. However, the sigmoid form of s(x) is differentiable.
      
      5. Does not acknowledge the well-known result that mixture models are unidentifiable. The math in the paper is mostly redundant. Some references:
          - Identifiability  Of  Nonparametric  Mixture  Models And  Bayes  Optimal  Clustering (pradeepr/arxiv npmix v.pdf)"" target=""_blank"" rel=""nofollow"">https://www.cs.cmu.edu/ pradeepr/arxiv npmix v.pdf)
          - Semiparametric estimation of a two-component mixture model by Bordes, L., Kojadinovic, I., and Vandekerkhove, P., Annals of Statistics, 2006 (https://arxiv.org/pdf/math/0607812.pdf)
          - Inference for mixtures of symmetric distributions by David R. Hunter, Shaoli Wang, Thomas P. Hettmansperger, Annals of Statistics, 2007 (https://arxiv.org/pdf/0708.0499.pdf)
          - Inference on Mixtures Under Tail Restrictions by K. Jochmans, M. Henry, and B. Salanie, Econometric Theory, 2017 (http://econ.sciences-po.fr/sites/default/files/file/Inference.pdf)
          
      6. Does not acknowledge existing work that adds classifier over unsupervised detectors (such as AI2). This is very common.
        - This is another linear model (logistic) on top of transformed features. The difference is that the transformed features are from a neural network and optimization can be performed in a joint fashion. The novelty is marginal.
        
      7. While the paper argues that a prior needs to be assumed, it does not use any in the algorithm. There seems to be a disconnect. It also does not acknowledge that AAD (LODA/Tree) does use a prior. Priors for anomaly proportions in unsupervised algorithms are well-known (most AD algos support that such as OC-SVM, Isolation Forest, LOF, etc.).
        
      8. Does not compare against current state-of-the-art Tree-based AAD
          - Incorporating Expert Feedback into Tree-based Anomaly Detection by Das et al., KDD, 2017.
        
      9. The 'Generalized' in the title is incorrect and misleading. This is specific to deep-networks. Stacking supervised classifiers on unsupervised detectors is very common. See comments on related works.
      
      10. Does not propose any other query strategies than greedily selecting top.
      
      11. Question: Does this support streaming?
","The sentiment of the review is quite negative. The reviewer refutes the claims of novelty, criticizes the lack of proper literature review, and points out several flaws and omissions in the paper. The language used is direct and critical, but it is not rude. The reviewer provides detailed feedback and references to support their points, which indicates a professional and constructive approach despite the negative sentiment.",-80,20
"First of all, the paper cannot be accepted because it violates the double blind submission policy by including an acknowledgments section.

Nonetheless, I will give some brief comments:

 The paper proposes a probabilistic hierarchical approach to perform zero-shot learning.
Instead of directly optimizing the standard cross-entropy loss, the paper considers some soft probability scores that consider some class graph taxonomy.

 The experimental section of the paper is strong enough although more baselines could have been tested. The paper only compares the usual cross entropy loss with their proposed soft-classification framework. 
Nonetheless, different architectures of neural networks are tested on ImageNet and validate the fact that the soft probability strategy improves performance on the zero-shot learning task.

 
On the other hand, the theoretical aspect is weak. The proposed method seems to be a straightforward extension of Frome et al., NIPS 2013. The main contribution is that soft probability scores are used to perform classification instead of using only class membership information.

Some weighting strategy is proposed in Section 2.2 but the proposed steps seem very ad hoc with no theoretical justification. The first equation on page 8 has the same problem where some random definition is provided.
","The sentiment score is -40 because the review starts with a strong negative statement about the paper violating submission policies, which is a significant issue. However, the reviewer does acknowledge some strengths in the experimental section, which slightly mitigates the overall negativity. The politeness score is -20 because the language used is quite direct and lacks any mitigating phrases that soften the critique. The reviewer uses terms like 'cannot be accepted' and 'very ad hoc with no theoretical justification,' which come across as blunt and somewhat harsh.",-40,-20
"The paper presents an evaluation methodology for evaluating attacks on confidence thresholding methods and proposes a new kind of attack. In general I find the writing poor, as it is not exactly clear what the focus of the paper is - the evaluation or the new attack? The experiments lacking and the proposed evaluation methodology & theoretical guarantees trivial.
	
Major remarks:
- Linking the code and asking the reviewers not to look seems like bad practice and close to violating double blind, especially when considering that the cleavhans library is well known. Should have just removed the link and cleavhans name and state it will be released after review. 

- It is unclear what the focus of the paper is, is it the evaluation methodology or the new attack? While the evaluation methodology is presented as the main topic in title, abstract and introduction most of the paper is dedicated to the attack.

- The evaluation methodology is a good idea but is quiet trivial. Also, curves are nice visually but hard to compare between close competitors. A numeric value like area-under-the-curve should be better.

- The theoretical guarantees is also quiet trivial, more or less saying that if a confident adversarial attack exists then finding the most confident attack will be successful. Besides that the third part of the proof can be simplified significantly.

- The experiments are very lacking. The authors do not compare to any other attack so there is no way to evaluate the significance of their proposed method

- That being said, the max-confidence attack by itself sounds interesting, and might be useful even outside confidence thresholding.

- One interesting base-line experiment could be trying this attack on re-calibrated networks e.g. “On Calibration of Modern Neural Networks” 

- Another baseline for comparison could be doing just a targeted attack with highest probability wrong class.

- I found part 4.2 unclear 

- In the conclusion, the first and last claims are not supported by the text in my mind. 



Minor remarks:

- The abstract isn’t clear jumping from one topic to the next in the middle without any connection.

- Having Fig.1 and 2 right on the start is a bit annoying, would be better to put in the relevant spot and after the terms have been introduced.

-In 6.2 the periodically in third line from the end seems out of place.

","The sentiment of the review is generally negative, as the reviewer points out several major flaws in the paper, including poor writing, unclear focus, trivial evaluation methodology, and lacking experiments. The sentiment score is -70. The politeness of the language is relatively neutral to slightly polite. The reviewer provides constructive feedback and suggestions for improvement without using harsh or rude language. The politeness score is 20.",-70,20
"Pros:

This paper uses kernel mappings between any two layers for weight initialisation. Using the representer theorem, a proper distribution for weights is constructed in H_{k_i} instead of being learned by \phi_i, and then is formulated as a GP. 

Cons:

However, there are some key issues.
1. The so-called “infinite width” is just yielded by kernels in RKHS for weight initialization. For practical implementation, the authors use this scheme with random Fourier features to construct finite-width network. A key issue is that how to guarantee that the approximated weights are still in the same space? For example, weights can be in RKHS, but their approximation might be not in RKHS. See in [S1] for details.
 
[S1] Generalization Properties of Learning with Random Features, NIPS 2017.
 
2. Experimental part is not very convincing. First, the authors just compare different initialization schemes. The used architectures are simple and not representative. Second, the overall performance is not satisfactory, and the compared classification datasets are quite small. Overall, the experimental results are inadequate and unconvincing.

Summary:
The paper attempts to proposal a weight initialization scheme to enable infinite deep infinite-width networks. However, there are some key issues not address such as whether the approximated weights are still in the same space and the limited experimental results.

Response to rebuttal:
The authors have addressed my question about the weights being still in the same RKHS. I still think the motivation and experiments are not very satisfactory. 

Therefore the paper is very borderline. However, I would like to bump my rating a bit higher.

 
 
 ","The sentiment of the review is mixed but leans slightly positive. The reviewer acknowledges the innovative approach of using kernel mappings for weight initialization and the use of the representer theorem, which is a positive aspect. However, the reviewer also points out significant issues with the practical implementation and experimental validation, which are critical concerns. Therefore, the sentiment score is around 10. The politeness of the language is quite high; the reviewer uses formal and respectful language throughout, even when pointing out flaws. The reviewer also acknowledges improvements made in response to the rebuttal, which indicates a polite and constructive tone. Therefore, the politeness score is 80.",10,80
"- Does the paper present substantively new ideas or explore an under-explored or highly novel question? 

The paper claimed that there is limited work on the investigating the sensitivity of RL caused by the physics variations of the environment, such as object weight, surface friction, arm dynamics, etc. So the paper proposed learning a stochastic curriculum, guided by episodic reward signals (which is their contribution compared with previous related work) to develop policies robust to environmental perturbation.  Overall the combination of ideas is novel but the experimental results are limited in scope. 

- Does the results substantively advance the state of the art?

The results advance the state of the art, since they are compared against : 1) the best results observed via a grid search (oracle) on policies trained exclusively on specific individual environment settings; 2) Policies trained under a mixed training structure, where the environment settings are varied every episode during training, with the episode settings drawn uniformly at random from a list of values of interest. Their 3 experiment results are competitive with 1) and much better than 2).

- Will a substantial fraction of the ICLR attendees be interested in reading this paper? 

Yes, because the robustness of RL policies to changes in the physic parameters of the environment has not been well explored. Although previous investigations exist, and this paper’s algorithm is the combination of EXP3 and DDPG, it is still interesting to see them combined together to solve model uncertainty problem of RL with very good simulation results.

- Would I send this paper to one of my colleagues to read? 

  I would definitely send the paper to my colleagues to read. 


- In terms of quality:  

Clear motivation; substantiated literature review; but the algorithms proposed are not novel and the question of whether the method will scale to more unknown parameters is not answered. 

- I terms of clarity:  

Easy to read.–Experimental evaluation is clearly presented.

- Originality:  The problem of developing an automated curriculum for learning generalization over environment settings for a given RL task is formulated as a multi-armed bandit problem, and EXP3 algorithm is used to minimize regret and maximize the actor’s rewards. Itis a very interesting application of EXP3, although such inspiration is drawn from a former multi-task NLP paper Graves et al. (2017).

- In terms of significance:  

 The paper is definitely interesting and presents an  promising  direction. The significance is  limited because of the simplicity of the examples considered in the experimental session. It would be interesting to see how this method performs in problems with more states and more unknown parameters.   



","The sentiment of the review is generally positive, as the reviewer acknowledges the novelty of the combination of ideas and the advancement of the state of the art, despite some limitations in the experimental results. The reviewer also expresses interest in the paper and would recommend it to colleagues. Therefore, the sentiment score is 70. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, providing specific feedback without being harsh or dismissive. Therefore, the politeness score is 90.",70,90
"This work proposes a defence based on class-conditional feature distributions to turn deep neural networks into robust classifiers.

At present this work lacks even the most rudimentary evidence to support the claims of robustness, and I hence refrain from providing a full review. In brief, model robustness is only tested against adversarials crafted from a standard convolutional neural network (i.e. in a transfer setting, which is vastly different from what the abstract suggests). Unsurprisingly, the vanilla CNN is less robust than the density-based architecture introduced here, but that can be simply be explained by how close the substitute model and the vanilla CNN are. No direct attacks - neither gradient-based, score-based or decision-based attacks - have been used to evaluate robustness. Please check [1] for how a thorough robustness evaluation should be performed.

[1] Schott et al. “Towards the first adversarially robust neural network model on MNIST”.","The sentiment of the review is negative, as the reviewer explicitly states that the work lacks rudimentary evidence to support its claims and refrains from providing a full review. This indicates a strong disapproval of the current state of the work. The politeness of the language is neutral; while the reviewer is critical, they do not use rude or disrespectful language. They provide specific reasons for their critique and reference a source for further guidance, which is constructive.",-80,0
"This paper proposes a new approach for answering questions requiring multi-hop reasoning. The key idea is to introduce a sequence labeler to divide the question into at most 3 parts, each part corresponds to a relation-tuple. The labeler is trained with the whole KB-QA pipeline with REINFORCE in an end-to-end way.

The proposed approach was applied to a synthetic dataset and a new KB-QA dataset MetaQA, and achieves good results.

I like the proposed idea, which sounds a straightforward solution to compound question answering. I also like the clarification between ""compound questions"" instead of ""multi-hop questions"". In my opinion, ""multi-hop questions"" can also refer to the cases where the questions (can be simple questions) require multi-hop over evidence to answer.

My only concern is about the evaluation on MetaQA, which seems a not widely used dataset in our community. Therefore I am wondering whether the authors could address the following related questions in the rebuttal or revision:

(1) I was surprised that WebQuestions is not used in the experiments. Could you explain the reason? My guess is that WebQuestions contains compound questions that cannot be simply decomposed as sequence labeling, because that some parts of the question can participant in different relations. If this is not true, could you provide results on WebQuestions (or WebQSP).

(2) There were several previous methods proposed for decomposition of compound questions, although they are not proposed for KB-QA. Examples include ""Search-based Neural Structured Learning for Sequential Question Answering"" and ""ComplexWebQuestions"". I think the authors should compare their approach with previous work. One choice is to reimplement their methods. An easier option might be applying the proposed methods to some previous datasets, because the proposed method is not specific to KB-QA, as long as the simple question answerer is replaced to other components like a reader in the ComplexWebQuestions work.","The sentiment of the review is generally positive, as the reviewer expresses appreciation for the proposed idea and its straightforward solution to compound question answering. The reviewer also acknowledges the good results achieved by the approach on the datasets used. However, the sentiment is slightly tempered by concerns about the evaluation on a less widely used dataset and the lack of comparison with previous methods. The politeness of the language is high, as the reviewer uses polite phrases such as 'I like,' 'In my opinion,' and 'I was surprised,' and provides constructive feedback and suggestions for improvement without being dismissive or harsh.",70,90
"# Positive aspects of this submission

- This submission presents a really novel, creative, and useful way to achieve unsupervised abstractive multi-document summarization, which is quite an impressive feat.

- The alternative metrics in the absence of ground-truth summaries seem really useful and can be reused for other summarization problems where ground-truth summaries are missing. In particular, the prediction of review/summary score as a summarization metric is very well thought of.

- The model variations and experiments clearly demonstrate the usefulness of every aspect of the proposed model.

# Criticism

- The proposed model assumes that the output summary is similar in writing style and length to each of the inputs, which is not the case for most summarization tasks. This makes the proposed model hard to compare to the majority of previous works in supervised multi-document summarization like the ones evaluated on the DUC 2004 dataset.

- The lack of applicability to existing supervised summarization use cases leaves unanswered the question of how much correlation there is between the proposed unsupervised metrics and existing metrics like the ROUGE score, even if they seem intuitively correlated.

- This model suffers from the usual symptoms of other abstractive summarization models (fluency errors, factual inaccuracies). But this shouldn't overshadow the bigger contributions of this paper, since dealing with these specific issues is still an open research problem.","The sentiment of the review is quite positive, as evidenced by the praise for the novelty, creativity, and usefulness of the submission. The reviewer highlights several strengths before moving on to the criticisms, which are presented in a constructive manner. The sentiment score is therefore 80. The politeness of the language is also high, as the reviewer uses polite and respectful language throughout, even when pointing out the limitations of the work. The criticisms are framed in a way that acknowledges the challenges and contributions of the paper, leading to a politeness score of 90.",80,90
"The whole model can be simplified by this: using auto-encoders for X and Y's reconstruction, then use Triple GAN loss to match the joint distribution of (X, Y).  However, the deterministic model with GAN loss looks problematic to me.

questions:

1. Although the authors showed strong evidence in their experiment part, they still failed to compare models with Bicycle-GAN, i.e., how Bicycle GAN performs on these two dataset?

2. missing some comparison: why use simplified Triple-GAN loss (i.e. without two regularization terms)  instead of Triangle-GAN, which is addressed to be better? I think the authors need to discuss about this. Also, the authors need to use MMD and other methods mentioned in the original WAE paper.

3. In table 1, without triple-GAN loss, the whole model is deterministic, but the authors can still show the FID score for the generalization ability, which is better than all other cycle-GAN based models, why is that possible? Is this equivalent to claim that auto-encoder has the ability to generate realistic images just by sampling z? 
(If I understand the experiment correctly, the author's synthesized images is generated by $y_hat = G_2(E_1(X))$, no sampling z required)

4. Can the authors show the generalization ability of JWAE? For example, with input X, we can have different correct corresponding Ys, just like Bicycle-GAN did.

","The sentiment of the review is slightly negative, as the reviewer points out several issues and missing comparisons in the authors' work. The reviewer questions the validity of the model and requests additional comparisons and explanations. The sentiment score is -30. The politeness of the language is neutral to slightly polite. The reviewer uses phrases like 'I think the authors need to discuss' and 'Can the authors show,' which are polite requests for clarification and additional work. The politeness score is 20.",-30,20
"This paper describes a novel method to provide inference mapping for GAN networks. The idea is to reuse the discriminator network's feature vector (output of layer before last) and learn a direct mapping to the GAN's latent space. This can be done very efficiently since the dimensionality of both layers are relatively small. Also, the mapping does not interfere with the learning process of the GAN itself and thus can be applied on top of any GAN method without affecting its performance. 

Inference mapping is useful in the GAN context for several reasons that are well described in the paper. First it allows to more efficiently generate ""edited"" images as the mapping provides a good starting point in the latent space. Second it provides a sound way to evaluate GAN's performance as the reconstruction of a given image through the inference mapping and the generator provides auto-encoder-like capabilities. Comparison of GAN models have been difficult due to a lack of adequate evaluation technique. This paper proposes a novel evaluation scheme that is both fair and technically simple.

In the experimental part, the authors first compare their approach to the 'naive encoder' approach where the last layer of the discriminator is removed after training, a feature layer of the size of the encoder's latent space is added, and the rest of the discriminator's layers are frozen. The proposed approach outperforms the naive encoder approach on the CelebA dataset. The second set of experiments investigates reconstruction accuracy of various GAN models. Figure 2 shows reconstructed images for 7 GANs and 36 examples from 3 datasets. Unfortunately, no subjective comparison can be attempted since the examples are different for each GAN. In Figure 3, editing in performed on the CelebA dataset, but again, subjective comparison among the GAN's is precluded by the fact that different examples are chosen. This oversight does not affect the paper's relevance, since those comparison would be purely subjective, however it would add some visual interpretation to the quantitative comparison given in table 1. I also wish the authors would have provided the inception score for FashMNIST and CelebA and also provide the more recent FID (Frechet Inception Distance). Inception scores are trained on ImageNET and are too commonly applied to CIFAR-10 and CelebA. It would be good to compare them against the proposed method on those datasets to show that there are not good for datasets other than those on which they were trained.

The article is technically sound. The citations are adequate. The English is fine with some extraneous articles being the only issue. The article lacks a graphic for the architecture of the system and many of the figures are too small to interpret when printed out. Also there's a typo on table 1. where the inception score for WGAN-GP on CelebA should be 6.869 and not 0.6869.

Overall, I find this paper provides a simple, novel significant method for evaluating GAN models and making better use of their latent space arithmetic editing capabilities. Due to the algorithm's simplicity, most of the paper is devoted to experiments and discussions.
 
","The sentiment of the review is generally positive. The reviewer appreciates the novelty and simplicity of the proposed method, as well as its potential usefulness in evaluating GAN models. The review highlights the strengths of the paper and provides constructive feedback on areas for improvement, such as the need for additional visual comparisons and the inclusion of certain evaluation metrics. The language used is polite and professional, with suggestions framed in a helpful manner rather than as criticisms.",80,90
"The paper describes a differentiable expected BLEU objective which computes expected n-gram precision values by ignoring the brevity penalty. 

Clarity: 
Section 3 of the paper is very technical and hard to follow. Please rewrite this section to be more accessible to a wider audience by including diagrams and more explanation.

Originality/signifiance: the idea of making BLEU differentiable is a much researched topic and this paper provides a nice idea on how to make this work.

Evaluation: 
The evaluation is not very strong for the following reasons:

1) The IWSLT baselines are very weak. For example, current ICLR submissions, report cross-entropy baselines of >33 BLEU, whereas this paper starts from 23 BLEU on IWSTL14 de-en (e.g., https://openreview.net/pdf?id=r1gGpjActQ), even two years ago baselines were stronger: https://arxiv.org/abs/1606.02960

2) Why is policy gradient not better? You report a 0.26 BLEU improvement on IWSLT de-en, which is tiny compared to what other papers achieved, e.g., https://arxiv.org/abs/1606.02960, https://arxiv.org/abs/1711.04956

3) The experiments are on some of the smallest translation tasks. IWSLT is very small and given that the method is supposed to be lightweight, i.e., not much more costly than cross-entropy, it should be feasibile to run experiments on larger datasets.

This makes me wonder how significant any improvements would be with a good baseline and on a larger datasets.

Also, which test set are you using?

Finally, in Figure 3, why is cross-entropy getting worse after only ~2-4K updates? Are you overfitting? 
Please reference this figure in the text.","The sentiment of the review is mixed. The reviewer acknowledges the originality and significance of the idea, which is a positive aspect, but also points out several weaknesses in the evaluation, which are negative aspects. Therefore, the sentiment score is slightly negative. The politeness of the language is generally neutral to polite. The reviewer uses phrases like 'please rewrite' and 'please reference,' which are polite requests, but also includes direct criticisms without much cushioning, which brings the politeness score closer to neutral.",-20,20
"Summary
This paper proposes to formulate diverse segmentation problems as a guided segmentation, whose task is defined by the guiding annotations.
The main idea of this paper is using meta-learning to train a single neural network performing guidance segmentation.
Specifically, they encode S annotated support image into a task representation and use it to perform binary segmentation.
By performing episodic optimisation, the model's guidance to segmentation output is defined by the task distribution.

Strength
Learning a single segmentation algorithm to solve various segmentation problem is an interesting problem that worth exploring.
This paper tackles this problem and showed results on various segmentation problems.

Weakness
The proposed method, including the architecture and training strategy, is relatively simple and very closely related to existing approach. Especially, the only differences with the referenced paper (Shaban et al., 2017) is how the support is fused and how multiple guidance could be handled, which can be done by averaging. These differences are relatively minor, so I question the novelty of this paper.

This paper performs experiments on diverse tasks but the method is compared with relatively weak baselines absolute performance looks bad compared to existing algorithms exploiting prior knowledge for each of the tasks.
For example, the oracle performance in semantic segmentation (fully supervised method) is 0.45 IOU in PASCAL VOC dataset, while many existing algorithms could achieve more than 0.8 mean IOU in this dataset. 
In addition, I question whether foreground / background baseline is reasonable baseline for all these tasks, because a little domain knowledge might already give very strong result on various segmentation tasks.
For example, in terms of video segmentation, one trivial baseline might include propagating ground truth labels in the first frame with color and spatial location similarity, which might be already stronger than the foreground / background baseline.

There are some strong arguments that require further justification. 
- In 4.3, authors argue that the model is trained with S=1, but could operate with different (S, P).
However, it's suspicious whether this would be really true, because it requires generalisation to out-of-distribution examples, which is very difficult machine learning problem. The performance in Figure 5 (right) might support the difficulty of this generalisation, because increasing S does not necessarily increase the performance.
- In 5.3, this paper investigated whether the model trained with instances could be used for semantic segmentation. I think performing semantic segmentation with model trained for instance segmentation in the same dataset might show reasonable performance, but this might be just because there are many images with single instance in each image and because instance annotations in this dataset are based on semantic classes. So the argument that training with instance segmentation lead to semantic segmentation should be more carefully made.

Overall comment
I believe the method proposed in this paper is rather incremental and analysis is not supporting the main arguments of this paper and strength of the proposed method. 
Especially, simple performance comparison with weak baselines give no clues about the property of the method and advantage of using this method compared to other existing approaches.
","The sentiment of the review is mixed but leans towards the negative side. The reviewer acknowledges the interesting problem tackled by the paper and the results shown on various segmentation problems, which gives a slight positive sentiment. However, the majority of the review focuses on the weaknesses, questioning the novelty, the choice of baselines, and the justification of certain arguments. Therefore, the sentiment score is -40. The politeness of the language is quite high. The reviewer uses polite language throughout, providing constructive criticism without being rude or dismissive. Therefore, the politeness score is 80.",-40,80
"This paper considers the problem of Bayesian inference using particle optimization sampler. Similarly to SGLD, authors propose Stochastic Particle Optimization Sampler (SPOS), augmenting Stein Variational Gradient Descent (SVGD) with diminishing Gaussian noise, replacing the hard-to-compute term of the Chen et al. (2018) formulation. Various theoretical results are given.

This paper was a pleasant read until I decided to check the proof of Theorem 3. I was not able to understand transitions in some of the steps and certain statements in the proof seem wrong.

Theorem 3:
""Note that $\theta^i_t$ and $\hat \theta^i_t$ are initialized with the same initial distribution µ0 = ν0 and we can also set $\theta^i_0$ to be independent of $\hat \theta^i_0$, we can have $\gamma(0) = 0$. $\gamma(0) = E \|\theta^i_0 - \hat \theta^i_0 \|^2$."" - this doesn't seem right to me. Expectation of squared difference of two independent and identically distributed random variables is not 0, assuming expectation is with respect to their joint density.

""Then according to the Gronwall Lemma, we have"" - I don't see how the resulting inequality was obtained. When I tried applying Gronwall Lemma, it seems that authors forgot to multiply by $t$ and  $\lambda_1$. Could you please elaborate how exactly Gronwall Lemma was used in this case.

""... some positive constants c1 and c2 independent of (M, d)$ - in the proof authors introduce additional assumption ""We can tune the bandwidth of the RBF kernel to make ∇K ≤ H_∇K, which is omitted in the Assumption due to the space limit."" First, there is a missing norm, since ∇K is a vector and H_∇K is I believe a scalar constant. Second, c1 = H_∇K + H_F, which both bound norm of d-dimensional vector and hence depend on d. I also suggest that all assumptions are included in the theorem statements, especially since authors have another assumption requiring large bandwidth. Additionally, feasibility of these both assumptions being satisfied should be explored (it seems to me that they can hold together, but it doesn't mean that part of assumptions can be moved to the supplement).

I find using Wasserstein-1 metric misleading in the theorem statement . This is not what authors really bound - from the proof it can be seen that they bound W_1 with W_2 and then with just an expectation of l2 norm. Moreover I don't understand the meaning of this bound. Theorem is concerned with W_1 distance between two atomic measures. What is the expectation over? Note that atom locations are supposed to be fixed for the W_1 to make sense in this context (and the expectation is over the coupling of discrete measures defined by weights of the atoms, not atom locations).

""Note the first bullet indicates U to be a convex function and W to be ... "" I think it should be K, not W.

Theorems 3-6 could be lemmas, while there should be a unifying theorem for the bound.

Finally, I think notation should be changed - same letter is used for Wasserstein distance and Wiener process.

Other comments:

Example in Figure 1 is somewhat contrived - clearly gradient based particle sampler will never escape the mode since all modes are disconnected by regions with 0 density. Proposed method on the other hand will eventually jump out due to noise, but it doesn't necessarily mean it produces better posterior estimate. Something more realistic like a mixture of Gaussians, with density bounded away from zero across domain space, will be more informative.

It is not sufficient to report RMSE and test log likelihood for BNNs. One of the key motivating points is posterior uncertainty estimation. Hence important metric, when comparing to other posterior inference techniques, is to show high uncertainty for out of distribution samples and low for training/test data.
","The sentiment of the review is mixed. The reviewer starts by acknowledging that the paper was a pleasant read initially, which indicates a positive sentiment. However, the review quickly shifts to a critical tone, pointing out several issues with the proof of Theorem 3 and other aspects of the paper. This results in a sentiment score of -20. The politeness of the language is generally maintained, with the reviewer using phrases like 'could you please elaborate' and 'I suggest,' which indicates a polite tone. However, the critical nature of the comments slightly reduces the politeness score, resulting in a score of 60.",-20,60
"Post-rebuttal
------------------
I have read the rebuttal and I better understand the paper. Given that, I am going to raise my rating by one point for the following reason:
- The manuscript presents a novel solution to a general problem and it is a valid solution. However, the solution is somewhat obvious, which is not necessarily a bad thing, which is why I am raising my rating by a point. However, an easy solution like the one proposed in the manuscript means that OBFS considered in this manuscript is not as general as the authors let on -- there is an implicit assumption that f(x_i, q) is close to f(x_j, q) if x_i is close to x_j.
- While the authors answered a lot of my clarification questions, the manuscript seems still a little hard to parse and can be significantly improved for easier reading and understanding.

=========================================
Pros
-------
[Originality/Significance] The manuscript focuses on a very general and important problem and proposes a scheme to solve this general problem. The authors present some theoretical and empirical results to demonstrate the utility of the proposed scheme.

Limitations
----------------
[Clarity] While the problem being addressing is extremely important, and the proposed solution seems reasonable, the manuscript is really hard to follow. For example, Definition 3 and Theorem 1 are extremely hard to understand. 

[Clarity/Significance] Moreover, I feel that the authors should be more precise in pointing out why current graph based search algorithms are just not trivially applicable to OBFS. The nature of the approximate Delaunay graph is that it can be built for any given similarity function (the level of approximation obviously depends on the similarity function, but that is an existing issue with graph-based methods). Given the graph, I do not understand why the basic search algorithm on this similarity graph would not be an approximate solution to OBFS. Hence I believe the authors need to clarify why the existing graph based algorithms do not directly translate. 

[Significance] While Definition 1 considers topological spaces, SL2G is assuming that X and (maybe) Y are in R^d (for different values of d). So does that mean that SL2G does not solve the general OBFS?

[Significance/Correctness/Clarity] The assumptions in Theorem 2 (as well as the supporting Proposition 1 in Appendix B) seems quite unreasonable. In moderately high dimensional X, doesn't the curse of dimensionality imply that this condition will not hold in most case? In there any reason why/how this would be circumvented? Moreover, in Proposition 1 (in Appendix B), the quantity C_r needs to be precisely defined since it could in general be exponential in the number of dimensions. Also, the assumption in Proposition 1 where \lambda^* > 0 is fairly strong in high dimensional data since data gets really sparse in high dimensions. Finally, the last step in Proposition 1 (where the failure probability obtained from the union bound is connected to condition (b) in Theorem 2) is not clear at all -- it is not apparent how E and F related to S and how p relates to every ball containing a point in S. This is a very important step and needs better exposition. 

[Clarity/Significance] I am unable to understand the baseline HNSW-SBFG (or the motivation for it) in the empirical section. It would be good to clarify this. 


General comments
---------------------------
[Significance] Finally, I believe that it would be good to see a connection between the success of SL2G to relationship between |f(x1, q) - f(x2, q)| and ||x1 - x2 ||_2 since the author emphasize that the proposed scheme can be seen as ""gradient descent in Euclidean space"" (although the authors would need to also precisely explain what they mean by that statement).

[Originality] Some related work that the authors should position their proposed problem/solution against:
- There is some work on ""max-kernel search"" which can perform similarity search with general notions of similarity (than just Euclidean metrics).
- There is some work on search with Bregman divergences which handle asymmetric similarity functions and also incorporate notions of gradient descent over convex sets.

Minor comments/typos
---------------------------------
- The authors should present the precise SL2G algorithm given the graph in the manuscript.
- l^2 --> \ell_2
- gradient decent --> gradient descent
- Table 1, f(q, x) --> f(x, q)","The sentiment of the review is moderately positive. The reviewer acknowledges the novelty and validity of the solution presented in the manuscript, raising their rating by one point. However, they also point out several areas where the manuscript can be improved, particularly in terms of clarity and significance. Therefore, the sentiment score is 20. The politeness of the language is quite high. The reviewer uses polite language throughout, providing constructive feedback and suggestions for improvement without being harsh or dismissive. Therefore, the politeness score is 80.",20,80
"I am a researcher in NLP and know little about vision, so I cannot review this paper. I have contacted general chair about this situation. ","The reviewer clearly states their inability to review the paper due to a lack of expertise in the subject matter. This is a neutral statement as it neither praises nor criticizes the paper. The language used is polite and professional, as the reviewer explains the situation and mentions that they have contacted the general chair to address the issue.",0,100
"In this paper, the authors relate the architectures of recurrent neural
networks with ODEs and defines a way to categorize the RNN architectures by
looking at non-linearity order and temporal memory scale. They further
propose QUNN, a RNN architecture that is more stable and has less complexity
overhead in terms of input dimension while comparing with LSTM. 

Although this paper provides a new view point of RNN architectures and relates
RNNs with ODEs, it fails to provide useful insight using this view point.
Also, it is not clear what advantage the new proposed architecture QUNN has
over existing models like LSTM or GRU. 

The paper is well presented and the categorization method is well defined.
However, how the order of non-linearity or the length of temporal memory
affect the behavior and performance of RNN architectures are not studied.

It is proved that QUNN is guaranteed existence and its Jacobian eigen values
will always have zero real part. It would be easier to understand if the
authors could construct a simple example of QUNN and conduct at least some 
synthetic experiments.

In general I think this paper is interesting but could be extended in various
ways. 
","The sentiment of the review is mixed. The reviewer acknowledges the novelty and presentation of the paper but criticizes the lack of useful insights and unclear advantages of the proposed architecture. Therefore, the sentiment score is slightly negative. The language used is polite and constructive, offering specific recommendations for improvement without being harsh or dismissive.",-20,80
"This is an interesting paper in which the authors propose a shallow neural network, the architecture of which was optimized to maximize brain score, and show that it outperforms other shallow networks at imagenet classification. The goal of this paper — allowing brain data to improve our neural nets — is great. The paper is well written (except for a comment on clarity right below), and presents an interesting take on solving this problem. 

It is a little unclear how the authors made CORnet optimize brain score: “However, note that CORnet-S was developed using Brain-Score as a guiding benchmark and although it was never directly used in model search or optimization, testing CORnet-S on Brain-Score is not a completely independent test.” Making these steps clearer is crucial for evaluating better what the model means. In the discussion “We have tested hundreds of architectures before finding CORnet-S circuitry and thus it is possible that the proposed circuits could have a strong relation to biological implementations.” implies that the authors trained models with different architectures until the brain score was maximized after training. A hundred(s) times of training on 2760 + 2400 datapoints are probably plenty to overfit the brainscore datasets. The brain score is probably compromised after this, and it would be hard to make claims about the results on the brain modeling side. The author acknowledge this limitation, but perhaps a better thing is to add an additional dataset, perhaps one from different animal recordings, or from human fMRI? 

Arguably, the goal of the paper is to obtain a model that overpowers other simpler models, and not necessarily to make claims about the brain. The interesting part of the paper is that the shallow model does work better than other shallow models. The authors mention that brain score helps CORnet be better at generalizing to other datasets. Including these results would definitely strengthen the claim since both brain score and imagenet have been trained on hundreds of times so far. 

Another way to show that brain score helps is to show it generalizes above or differently from optimizing other models. What would have happened if the authors stuck to a simple, shallow model and instead of optimizing brain score optimized performance (hundreds of times) on some selected image dataset (this selection dataset is separate from imagenet, but the actual training is done on imagenet) and then tested performance on imagenet? Is the effect due to the brain or to the independent testing on another dataset?
","The sentiment of the review is generally positive, as indicated by phrases like 'interesting paper,' 'great goal,' and 'well written.' However, the reviewer also points out significant areas for improvement, particularly regarding the clarity of how the model optimizes brain score and the potential overfitting issue. The politeness of the language is high, as the reviewer uses constructive criticism and suggests specific ways to improve the paper without being harsh or dismissive.",60,80
"The main contribution of the paper are a set of new layers for improving the 1x1 convolutions used in the bottleneck of a ResNet block. The main idea is to remove channels of low “importance” and replace them by other ones which are in a similar fashion found to be important.  To this end the authors propose the so-called expected channel damage score (ECDS) which is used for channel selection. The authors have shown in their paper that the new layers improve performance mainly on CIFAR, while there’s also an experiment on ImageNet
It looks to me that the proposed method is overly complicated. It is also described in a complicated manner.  I don't see clear motivation for re-using the same features. Also I did not understand the usefulness of applying the spatial shifting of the so-called Channel Distributor. It is also not clear whether the proposed technique is applicable to only bottleneck layers.
The results show some improvement but not great and over results that as far as I know are not state-of-the-art (to my knowledge the presented results on CIFAR are not state-of-the-art). The results on ImageNet also show decent but not great improvement. Moreover, the gain in reducing the model parameters is not that great as the R parameters are only a small fraction of the total model parameters. Overall, the paper presents some interesting ideas but the proposed approach seems over-complicated","The sentiment of the review is mixed but leans towards the negative side. The reviewer acknowledges some interesting ideas but criticizes the complexity and the lack of clear motivation and applicability of the proposed method. The results are described as not state-of-the-art and the improvements are considered minor. Therefore, the sentiment score is -40. The politeness of the language is relatively neutral; the reviewer does not use harsh or rude language but provides constructive criticism. Therefore, the politeness score is 10.",-40,10
"- Summary
This paper presents a minor improvement over the previous Deli-GAN framework (Gurumurthy et al CVPR'17). Specifically, the work proposes to use the Fisher Integral Probability Metric as the divergence measure in our GAN model, instead of the Jensen-Shannon divergence. It shows little results (seems to be positive) using this new distance measure other than traditional ones.  Except that, I didn't see any other contribution from this paper.

- Suggestions
The paper is poorly written and seems to be a rush submission to ICLR. For example:
* a lot of grammatical errors throughout the paper
* only 6.5 papes out of 8 pages are utilized
* the introduction is not convincing -- what problem are you going to address? any summary of your methodology? why it is expected to outperform existing frameworks? what distinguishes your work from existing works? and what're your main results? I cannot conclude after reading the intro.
* the results are very minor and not convincing. It seems the authors conducted a very limited set of experiments and concluded that the proposed Deli-Fisher GAN is better. If you claim that the proposed framework can generate better images, at least the framework should be compared to the latest state-of-the-art GANs (e.g. spectral GANs, etc.)
* The writing is not polished. 

Overall, the paper is far from ready to be submitted to ICLR, not mentioning acceptance. I would recommend the authors to conduct more experiments and comparisons and do a better job before submitting it to future conferences.

","The sentiment of the review is quite negative, as the reviewer points out several major flaws in the paper, including poor writing, lack of convincing results, and insufficient experimentation. The sentiment score is therefore -80. The politeness of the language used is somewhat neutral to slightly negative. While the reviewer does not use explicitly rude language, the tone is quite harsh and critical, particularly with phrases like 'poorly written,' 'rush submission,' and 'far from ready.' The politeness score is -30.",-80,-30
"This submission proposes an energy-based method to correct mislabelled examples. Intuitively, the authors claim that contradictions between energy and noisy labels can be used to identify label noise. To make the idea reliable, the authors propose to compute the energy by using learned (commonly shared) features. Experiment results look good. The presentation is also clear. My concerns are as follows:

(1) By learning discriminative features and then correcting the label noise, the authors have implicitly assumed that the label noise strongly dependent on the discriminative features. This assumption may be strong as most labels are provided according to the original instance (features). In the experiment section, it is very unclear about how the noise is generated, e.g., how to select x_i according to variance? How to set the threshold? What is ""col"" in Tables 1 and 2? What are the overall label noise rates? Note that if the threshold for the variance is large. It means that the noise is only added to the discriminative features, making the experiment too ad-hoc. 

(2) The theory of why the residual energy can be used to identify label noise is elusive. How to set the threshold for identifying label noise with a theoretical guarantee is also unclear. Two recent papers on learning with instance-dependent label noise are surprisingly missed, e.g., Menon, Aditya Krishna, Brendan Van Rooyen, and Nagarajan Natarajan. ""Learning from binary labels with instance-dependent corruption."" arXiv preprint arXiv:1605.00751 (2016). and Cheng, Jiacheng, et al. ""Learning with Bounded Instance-and Label-dependent Label Noise."" arXiv preprint arXiv:1709.03768 (2017). The latter one proposes algorithms to identify label noise with theoretical guarantees. The authors should compare the proposed method with them.

(3) There are methods provided for choose the values of hyperparameters. Most of them are empirically set, which is not convincing.

(4) The authors reported that with discriminative features learned by employing noisy data, the proposed method also provide good performance. It would be interesting to see how corrected labels will recursively help better learn the discriminative features. Illustrating figures are preferred.","The sentiment of the review is moderately positive. The reviewer acknowledges the good experimental results and clear presentation, which suggests a positive sentiment. However, the review also contains several critical points and concerns, which temper the overall positivity. Therefore, the sentiment score is 40. The politeness of the language is high. The reviewer uses polite language, such as 'My concerns are as follows' and 'It would be interesting to see,' and provides constructive feedback without being rude or dismissive. Therefore, the politeness score is 80.",40,80
"This paper proposes to use n-ary representations for convolutional neural network model quantization. A novel strategy of nested-means clustering is developed to update weights. Batch normalization is also considered in the activation quantization. Experiments on both weight quantization and activation quantization are conducted and show effectiveness.

Strengths:
1.	The idea of nested-means clustering is interesting, which somehow shows its effectiveness.
2.	State-of-the-art experimental results.
3.	The representation is excellent, and it is easy to follow.

Concerns:
1.	Though the experiment study seems solid, an ablation study is still missing. For example, how important is the nested-means clustering technique? What is the effect if replacing it with the original one or with other clustering methods? What will happen if expanding the interval in the quantization of activation? All these kinds of questions are hard to answer without an ablation study.
2.	It is not clear how the weight and activation quantization are addressed together.
3.	If counting the first and last layers, what is the size of the model (the number of parameters)?
4.	Similarly, what are the FLOPs in different settings of experiments? This seems missing.
5.	When discussing the related work about model compression, there are important references missing. I just list two references in the latest vision and learning literature:
[Ref1] X. Lin et al. Towards accurate binary convolutional neural network. NIPS 2017
[Ref2] Z. Liu et al. Bi-Real Net: Enhancing the Performance of 1-bit CNNs with Improved Representational Capability and Advanced Training Algorithm. ECCV 2018.
","The sentiment of the review is generally positive, as indicated by the praise for the novel strategy of nested-means clustering, state-of-the-art experimental results, and excellent representation. However, the reviewer also raises several concerns and suggestions for improvement, which slightly temper the overall positive sentiment. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer uses polite and constructive language to provide feedback and suggestions. The reviewer acknowledges the strengths of the paper before listing concerns and does so in a respectful manner. Therefore, the politeness score is 90.",60,90
"The authors are providing an information theoretic viewpoint on the behavior of DNN based on the information bottleneck.  The clarity of the paper is my main concern.  It contains quite a number of typos and errors.  For example, in section 6, the results of MNIST in the first experiment was presented after introducing the second experiment.  Also, the results shown in Fig 1b seems to have nothing to do with Fig. 1a.  It makes use of some existing results from other literature but it is not clearly explained how and why the results are being used.   It might be a very good paper if the writing could be improved.   The paper also contains some experimental results.  But they are too brief and I do not consider the experiments as sufficient to justify the correctness of the bounds proved in the paper.","The review starts by acknowledging the authors' attempt to provide an information theoretic viewpoint on DNN behavior, which is a positive note. However, it quickly shifts to expressing concerns about the clarity of the paper, citing numerous typos, errors, and unclear explanations. The reviewer also mentions that the experimental results are insufficient to justify the correctness of the bounds proved in the paper. Despite these criticisms, the reviewer suggests that the paper could be very good if the writing is improved, indicating a willingness to see the paper succeed. The language used is constructive and polite, focusing on specific issues and offering a path for improvement.",-20,80
"This paper proposed a bio-inspired sparse coding algorithm where iterations
for dictionary updates take into account the past updates. It is argued
that time takes a crucial rule in learning.

The paper is quite well written and contains an extensive literature review
demonstrating a good understanding of previous literature in both ML/DL and biological
vision.

The idea of using a ""non-linear gain normalization"" to adjust atom selection
in sparse coding is interesting and as far as I know novel, while providing
interesting empirical results: The system learns in an unsupervised way faster.

Misc:

- Using < > for latex brakets is not ideal. I would recommend: $\langle\,,\rangle$

- ""derivable"" I guess you mean ""differentiable""

- Oliphant and Hunter are cited for Numpy/scipy and matplotlib but the
reference to Pedregosa et al. for sklearn is missing.","The sentiment of the review is positive, as indicated by phrases like 'quite well written,' 'good understanding,' 'interesting and novel,' and 'interesting empirical results.' These phrases suggest a favorable view of the paper's content and contributions. Therefore, the sentiment score is 80. The politeness of the language is also high, as the reviewer uses polite recommendations and constructive feedback without any negative or harsh language. The use of phrases like 'I would recommend' and 'I guess you mean' indicates a respectful and considerate tone. Thus, the politeness score is 90.",80,90
"This review will unfortunately be very short because I am afraid there is not much to say about this well written paper, which seems to have been sent to the wrong conference. The scientific problem is interesting, namely the detection of topological artifacts in images showing biological phenomena (which I don’t know much about). The relevant literature here is basically literature from this field, which is not machine learning and not even image processing. The contribution of the paper, in terms of machine learning, is to apply a well known neural model (YOLO) to detect bounding boxes of objects in images, which are very specific. The contribution here does not lie in machine learning, I am afraid.

This is thus a purely experimental paper on a single application, namely object detection in specific images. Unfortunately the experiments are not convincing. The results are validated against a “traditional method”, which has never been cited, so we do not know what it is.

The performance gain obtained with YOLO seems to be minor, although the difference in time complexity is quite enormous (to the advantage of YOLO).

The contribution is thus minor and for me does not justify publication at ICLR.

The grant number is mentioned in the acknowledgments, which seems to violate double blind policy.
","The sentiment of the review is generally negative, as the reviewer expresses that the paper does not justify publication at ICLR and points out several significant issues, such as the lack of convincing experiments and the minor contribution of the work. The sentiment score is -70 because the reviewer acknowledges the paper is well-written and the problem is interesting, but overall, the feedback is largely critical. The politeness score is 20 because the reviewer uses polite language and avoids being overly harsh, but the overall tone is still critical and somewhat dismissive.",-70,20
"The authors proposed a new method improving a previous Bayesian optimization approach for chemical design (Gomez-Bombarelli et al., 2016b) by addressing the problem that data points need to have valid molecular structures. The main contribution is a constrained Bayesian optimization approach that take into account the constraint on the probability of being valid.

My biggest concern of this paper is that it is not really using sequential evaluations to do automatic design of experiments on molecules. The experiments do not seem to fully support the proposed approach in terms of being able to adaptively do sequential evaluations. 

Detailed comments:
1. The term inside the expectation of the EI criterion in Sec 2.5 should be max(0, f(m)-\eta) rather than max(0, \eta - f(m)).
2. The EIC criterion the authors adopted uses Pr(C(m)) if the constraint is violated everywhere with high probability. It seems like Pr(C(m)) does not have the ability to explore regions with high uncertainty. How does this approach compare to Bayesian level set estimation approaches like 
B. Bryan, R. C. Nichol, C. R. Genovese, J. Schneider, C. J. Miller, and L. Wasserman, “Active learning for identifying function threshold boundaries,” in NIPS, 2006 
I. Bogunovic, J. Scarlett, A. Krause, and V. Cevher, “Truncated variance reduction: A unified approach to bayesian optimization and level-set estimation,” in NIPS, 2016.
3. It would be good to explain in more detail how a constraint is labeled to be valid or invalid. 
4. What is the space of m in Sec 2.3 and the space of m in Sec 2.4? It seems like there is a discrepancy: Sec 2.3 is talking about m as a molecule but Sec 2.4 describes f as a function on the latent variable? It would be good to be clear about it.
5. In the experiment, I think it is very necessary to show the effectiveness of the constrained BO approach in terms of how the performance varies as a function of the number of evaluations on the constraint and the function. The current empirical results only support the claim that the constrained BO approach is able to output more valid latent variables and the function values from constrained BO is higher than vanilla BO under the same number of training data. It is also strange why there is a set of test data. 


Typos/format:
1. citation format problems across the paper, e.g. 
""Gomez-Bombarelli et al. (Gomez-Bombarelli et al., 2016b) presented Automatic Chem""
""SMILES strings Weininger (1988) are a means of representing molecules as a character sequence.""
It's likely a problem of misuse of \cite, \citep.
2. no period at the end of Sec 2.4","The sentiment of the review is moderately negative, as the reviewer expresses significant concerns about the paper's methodology and the support provided by the experiments. The reviewer points out several issues that need to be addressed, indicating a lack of confidence in the current state of the paper. The politeness of the language is relatively high, as the reviewer provides constructive feedback and specific recommendations without using harsh or dismissive language.",-40,80
"This paper proposes an approach to introduce interpretability in NLP tasks involving text matching. However, the evaluation is not evaluated using human input, thus it is not clear whether the model is indeed meeting this important goal. Furthermore, there is no direct comparison against related work on the same topic, so it is not possible to assess the contributions over the state of the art on the topic. In more detail:

- There are versions of attention mechanisms that are spare and differentiable. See here: 
From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification
André F. T. Martins, Ramón Fernandez Astudillo 

- Why is ""rationalizing textual matching"" different than other approaches to explaining the predictions of a model? As far as I can tell, thresholding on existing attention would give the same output. I am not arguing that there is nothing different, but there should be a direct comparison, especially since eventually the method proposed is thresholded as well by limiting the number of highlights.

- A key assumption in the paper is that the method identifies rationales that humans would find useful as explanations. However there is no evaluation of this assumption. For a recent example of how such human evaluation could be done see:
D. Nguyen. Comparing automatic and human evaluation of local explanations for text classification. NAACL 2018
http://www.dongnguyen.nl/publications/nguyen-naacl2018.pdf

- I don't agree that explanations are sufficient if removing them doesn't degrade performance. While these two are related concepts, the quality of the explanation to a human is different to a system. In fact, more text can degrade performance when it is unrelated. See the experiments of this paper:
Adversarial Examples for Evaluating Reading Comprehension Systems.
Robin Jia and Percy Liang. EMNLP 2017: http://stanford.edu/~robinjia/pdf/emnlp2017-adversarial.pdf

- Reducing the selection of rationales to sequence tagging eventually done as classification is suboptimal compared to work on submodular optimization (cited in the introduction) if being concise is important. A comparison is needed.

- There is an argument that the training objective makes generated rationales corresponded and sufficient. This requires some evidence to support it.

- What is the ""certificate of exclusion of unselected parts"" that the proposed method has?

- An important argument is that the performance does not degrade. However there is no comparison against state of the art models to verfiy it.","The sentiment of the review is moderately negative. The reviewer points out several significant shortcomings in the paper, such as the lack of human evaluation, absence of direct comparison with related work, and insufficient evidence to support key assumptions. The politeness of the language is relatively high. The reviewer uses polite and professional language throughout the review, providing constructive criticism and suggesting specific improvements without being rude or dismissive.",-60,80
"I don't quite see how the proposed approach addresses non-linear canonical correlation analysis. In particular:

1) The main motivation is the minimization of the conditional mutual information I(X;Y|Z), where X and Y correspond to the two views and Z is latent. First of all, what uncertainty does this expression has when X and Y are observations and Z is given? My understanding is that the main objective of any CCA problem should be to find some transformations, say f(X) and g(Y), with some (to be defined) desirable properties. For example, these would correspond to linear transformations, say Ax and By, for classical CCA. Therefore, should not one be interested in minimizing something like I(f(X);g(Y)|Z)?

2) Assuming that the minimization of the conditional mutual information I(X;Y|Z) would be the goal, I don't quite see why the formulation in equation (6) would actually be equivalent (or be some reasonable approximation)? 

3) It is well known that differential entropy can be negative (e.g., Cover and Thomas, 2006). Why would the conditional mutual information in equation (4) be non-negative? Alternatively, what would negative values of I(X;Y|Z) mean in the CCA context? My understanding is that one should be interested in minimizing I(X;Y|Z), or its variants with transformations, in *absolute value* to ensure some closeness to conditional independence.

4) Expressions in equation (5)-(6) are general and hold with no assumptions whatsoever for any random variables X, Y, Z (given the expectations/integrals exist). It is therefore not clear what are the variables of this minimization problem? (parameters? but what is the parametric model?)

5) Assuming solving (6) is the goal, this formulation as mentioned by the authors is actually is quite a challenging problem involving latent variables. Some form of this approach explanation would 

I can not quite see how the proposed adversarial version would correct or supplement any of these questions.

Other comments:

1) It would be appropriate to cite the probabilistic CCA paper by Bach and Jordan (2005); a better citation for classical CCA would be Hotelling (1936).

2) I find the multiple mentioning of the *non-linear* (in-)dependence confusing. Is this in statistical sense? And how exactly is this related to CCA? Does it have anything to do with the fact that the third and higher order cumulants are zero only for independent variables unless they are Gaussian? Moreover, does this linear independence have any connection with the non-linearity of the proposed CCA approach?

3) What exactly is the *linear correlation criterion* and how does it enter the classical CCA or PCCA formulation (Introduction; bullet point 2)?

4) It would be helpful to introduce the original CCA problem emphasizing that each view, X and Y, are *different* linear transformation of *the same* latent codes z. Moreover, the full description of the models (classical CCA/ PCCA) wouldn't take more than one-two paragraphs and would help the readers to avoid any misunderstanding.

5) Are any assumptions necessary to ensure existence?
","The sentiment of the review is quite critical, focusing on the shortcomings and unclear aspects of the proposed approach. The reviewer raises several significant concerns and questions about the methodology and theoretical foundations, indicating a sentiment score of -60. The language used is formal and direct, but not impolite. The reviewer maintains a professional tone while pointing out the issues, which suggests a politeness score of 20.",-60,20
"The paper studies the problem of question generation from sparql queries. The motivation is to generate more training data for knowledge base question answering systems to be trained on. However, this task is an instance of natural language generation: given a meaning representation (quite often a database record), generate the natural language text correspoding to it. And previous work on this topic has proposed very similar ideas to the scratchpad proposed here in order to keep track of what the neural decoder has already generated, here are two of them:
- Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems
Tsung-Hsien Wen, Milica Gasic, Nikola Mrksic, Pei-Hao Su, David Vandyke, Steve Young, EMNLP 2015: https://arxiv.org/abs/1508.01745
- Globally Coherent Text Generation with Neural Checklist Models
Chloe Kiddon Luke Zettlemoyer Yejin Choi: https://aclweb.org/anthology/D16-1032
Thus the main novelty claim of the paper needs to be hedged appropriately. Also, to demonstrate the superiority of the proposed method an appropriate comparison against previous work is needed.

Some other points:
- How is the linearization of the inout done? It  typically matters
- Given the small size of the dataset, I would propose experimenting with non-neural approaches as well, which are also quite common in NLG.
- On the human evaluation: showing the gold standard reference to the judges introduces bias to the evaluation which is inappropriate as in language generation tasks there are multiple correct answers. See this paper for discussion in the context of machine translation: http://www.aclweb.org/anthology/P16-2013
- For the automatic evaluation measures there should be multiple references per SPARQL query since this is how BLEU et al are supposed to be used. Also, this would allow to compare the references against each other (filling in the missing number in Table 4) and this would allow an evaluation of the evaluation itself: while perfect scores are unlikely, the human references should be much better than the systems.
- In the outputs shown in Table 3, the questions generated by the scratchpad encoder often seem to be too general compared to the gold standard, or incorrect. E.g. ""what job did jefferson have"" is semntically related to his role in the declaration of independence but rather different. SImilarly, being married to someone is not the same as having a baby with someone. While I could imagine human judges preferring them as they are fluent, I think they are wrong as they express a different meaning than the SPARQL query they are supposed to express. What were the guidelines used?
","The review starts by acknowledging the relevance of the paper's topic, which is question generation from SPARQL queries, but quickly points out that the main novelty claim needs to be hedged due to similar previous work. The reviewer provides specific references to prior work and suggests necessary comparisons to demonstrate the proposed method's superiority. The review also includes several constructive recommendations for improving the paper, such as experimenting with non-neural approaches, addressing potential biases in human evaluation, and ensuring multiple references for automatic evaluation measures. The language used is professional and constructive, aiming to help the authors improve their work rather than dismissing it outright. Therefore, the sentiment score is slightly positive, and the politeness score is high.",20,80
"The paper proposes a modification of maximum likelihood estimation that encourages estimated predictive/conditional models p(z|x) to have low entropy and/or to maximize mutual information between z and x under the model p_{data}(x)p_{model}(z|x).

There is pre-existing literature on encouraging low entropy / high mutual information in predictive models, suggesting this can indeed be a good idea. The experiments in the current paper are preliminary but encouraging. However, the setting in which the paper presents this approach (section 2.1) does not make any sense. Also see my previous comments.

- Please reconsider your motivation for the proposed method. Why does it work? Also please try to make the connection with the existing literature on minimum entropy priors and maximizing mutual information.

- Please try to provide some guarantees for the method. Maximum likelihood estimation is consistent: given enough data and a powerful model it will eventually do the right thing. What will your estimator converge to for infinite data and infinitely powerful models?","The sentiment of the review is mixed. The reviewer acknowledges that the idea of encouraging low entropy and high mutual information in predictive models is supported by existing literature and finds the preliminary experiments encouraging. However, the reviewer also points out significant issues with the setting in which the approach is presented and asks for a reconsideration of the motivation and guarantees for the method. Therefore, the sentiment score is slightly positive at 20. The politeness of the language is generally respectful and constructive, with phrases like 'please reconsider' and 'please try to provide,' indicating a polite tone. Thus, the politeness score is 80.",20,80
"This paper tries to propose a so-called hybrid algorithm to eliminate the gradient delay of asynchronous methods. The authors propose algorithm 1 and a simplified version algorithm 2 and prove the convergence of algorithm 2 in the paper.  The paper is very hard to follow, especially the algorithm description part. What I can understand is that the authors want to let the fast workers do more local updates until the computation in the slowest worker is done. The idea is similar to EASGD except that it forces the workers to communicate the server once the slowest one has completed their job.

The following are my concerns:
1. Do you consider the overhead in constructing the communication between machines? in your method,  workers are keeping notifying servers that they are done with the computation. 
2. In Algorithm 1 line 9 and line 23, there are two assignments: x_init =x and x_init=ps.x, is there any conflict? 
3. In Algorithm 2,  at line 6 workers wait to receive ps.x, at line 20 server wait for updates. I think there is a bug, and nothing can be received at both ends.
4. The experiments are too weak. There is no comparison between other related methods, such as downpour, easgd.
5. The authors test resnet50 on cifar10,  however, there is no accuracy result. They show the result by using googlenet, why not resnet50? I am curious about the experimental settings.

Above all, the paper is hard to follow and the idea is very trivial. Experiments in the paper are also very weak. ","The sentiment of the review is quite negative. The reviewer describes the paper as 'very hard to follow' and 'trivial,' and criticizes the experiments as 'too weak.' These phrases indicate a strong dissatisfaction with the paper's quality and clarity. Therefore, the sentiment score is -80. The politeness of the language, however, is relatively neutral. While the reviewer is critical, they do not use offensive or rude language. They pose their concerns in the form of questions and statements without resorting to personal attacks or derogatory remarks. Thus, the politeness score is 0.",-80,0
"This paper presents a new semi-supervised method for bilingual dictionary induction and proposes a new metric to measure isometry between embedding spaces.

Pros:
- The paper proposes to use a new metric, the Gromov-Hausdorff distance to measure how isometric two word embedding spaces are.
- The toy example is useful for motivating the use case of the method.
- The approach achieves convincing results on the dataset.

Cons:
- Beyond the isometry metric, the main innovation as far as I can see seems to be the hubness filtering, which is incremental and not ablated, so it is not clear how much improvement it yields. The weak orthogonality constraint has already been used in [2].
- It is not clear to me what the proposed metric adds beyond the eigenvector similarity metric proposed in [1]. The authors should compare to this metric at least.
- The authors might want to add the results of [3] for an up-to-date comparison.

[1] Søgaard, A., Ruder, S., & Vulić, I. (2018). On the Limitations of Unsupervised Bilingual Dictionary Induction. In Proceedings of ACL 2018.
[2] Zhang, M., Liu, Y., Luan, H., & Sun, M. (2017). Adversarial Training for Unsupervised Bilingual Lexicon Induction. In Proceedings of ACL.
[3] Artetxe, M., Labaka, G., & Agirre, E. (2018). A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings. In Proceedings of ACL 2018.","The sentiment of the review is moderately positive. The reviewer acknowledges the novelty of the proposed metric and the usefulness of the toy example, as well as the convincing results achieved. However, the reviewer also points out several areas for improvement and comparisons that are missing, which tempers the overall positivity. Therefore, the sentiment score is 40. The politeness of the language is high. The reviewer uses polite and constructive language, providing specific references and suggestions for improvement without being dismissive or harsh. Therefore, the politeness score is 80.",40,80
"This paper introduces a technique using ensembles of models with MC-dropout to perform uncertainty sampling for active learning.

In active learning, there is generally a trade-off between data efficiency and computational cost. This paper proposes a combination of existing techniques, not just ensembling neural networks and not just doing MC dropout, but doing both. The improvements over basic ensembling are rather minimal, at the cost of extra computation. More specifically, the data efficiency (factor improvement in data to achieve some accuracy) of the proposed method over using a deterministic ensemble is around just 10% or so. On the other hand, the proposed algorithm requires 100x more forward passes when computing the uncertainty (which may be significant, unclear without runtime experiments). As a concrete experiment to determine the importance, what would be the accuracy and computational comparison of ensembling 4+ models without MC-dropout vs. 3 ensembled models with MC-dropout? At the point (number of extra ensembles) where the computational time is equivalent, is the learning curve still better?

The novelty of this method is minimal. The technique basically fills out the fourth entry in a Punnett square.

The paper is well-written, has good experiments, and has a comprehensive related work section.

Overall, this paper is good, but is not novel or important enough for acceptance.","The sentiment of the review is mixed. The reviewer acknowledges that the paper is well-written, has good experiments, and a comprehensive related work section, which are positive points. However, the reviewer also points out that the novelty of the method is minimal and that the improvements over existing techniques are not significant enough to warrant acceptance. Therefore, the sentiment score is slightly negative. The language used in the review is polite and constructive, offering specific suggestions for improvement and avoiding any harsh or rude comments.",-20,80
"Overview and contributions: The authors propose the ModifAE model that is based on an autoencoder neural network for continuous trait image modifications. ModifAE requires fewer parameters and less time to train than existing generative models. The authors also present experiments to show that ModifAE produces more convincing and more consistent continuous face trait modifications than the current baselines.

Strengths:
1. Nice presentation of the model.
2. Good experiments to justify improved running time and fewer number of parameters.

Weaknesses:
1. I am not completely convinced by the results in Figure 4. It doesn't seem like the model is able to pick up on subtle facial expressions and generate them in a flexible manner. In fact, the images look very very similar regardless of the value of the traits. Furthermore, the authors claim that ""In general, as she becomes more emotional, her smile increases, and as she is made more attractive, her smile increases as well, as smiling subjects are judged as more attractive"". I believe attractiveness and emotions are much more diverse and idiosyncratic than just the size of her smile...
2. From Figure 5 it seems like ModifAE generates images that are lower in quality as compared to StarGAN. Can the authors comment on this point? How can ModifAE be improved to generate higher-quality images?

Questions to authors:
1. Weakness points 1 and 2.
2. This did not affect my rating, but I am slightly concerned by the labelings as seen in Figure 1. Is it reasonable to infer traits like ""trustworthy"", ""attractive"", ""aggressive"", ""responsible"" from images? Are these traits really what we should be classifying people's faces as, and are there any possible undesirable/sensitive biases from the dataset that our models could learn? I would like to hear the author's opinions on the ethical implications of these datasets and models. 

Presentation improvements, typos, edits, style, missing references:
None","The sentiment of the review is generally positive, as the reviewer acknowledges the strengths of the model and the experiments conducted. However, the reviewer also points out significant weaknesses and raises important questions, which slightly lowers the overall sentiment. Therefore, the sentiment score is 30. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, even when pointing out weaknesses and asking critical questions. Therefore, the politeness score is 80.",30,80
"This paper introduces a domain adaptation approach for structured output data, with a focus here on semantic segmentation. The idea is to model the structure by exploiting image patches, but account for the fact that these patches may be misaligned, and thus not in exact correspondence. This is achieved by defining new patch classes via clustering the source patches according to the semantic information, and making use of an adversarial classifier on the predicted patch-class distributions.

Strengths:
- Modeling the structure via patches is an interesting idea.
- The proposed method achieves good results on standard benchmarks.

Weaknesses:

Method:
- The idea of relying on patches to model the structure is not new. This was achieved by Chen et al., CVPR 2018, ""ROAD: Reality Oriented Adaptation..."". In this work, however, the patches were assumed to be in correspondence, which leaves some novelty to this submission, although reduced.
- In essence, the patch-based adversarial alignment remains global; this can be thought of as working at a lower resolution and on a different set of classes, defined by the clusters, than the global alignment. The can be observed by comparing Eq. 3 and Eq. 6, which have essentially the same form. This is fine, but was not clear to me until I reached Eq. 6. In fact, what I understood from the beginning of the paper was an alternative formulation, where one would essentially assign each patch to a cluster and aim to align the distributions of the output (original classes) within each cluster. I suggest the authors to clarify this, and possibly discuss the relation with this alternative approach.
- I am not convinced by the claimed relationship to methods that learn disentangled representations. Here, in essence, the authors just perform clustering of the semantic information. This is fine, but I find the connection a bit far-fetched and would suggest dropping it.

Experiments:
- The comparison to the state of the art is fine, but I suggest adding the results of Chen et al, CVPR 2018, which achieves quite close accuracies, but still a bit lower. The work of Saito et al., CVPR 2018, ""Maximum Classifier Discrepancy..."" also reports results on semantic segmentation and should be mentioned here. I acknowledge however that their results are not as good as the one reported here.
- While I appreciate the ablation study of Section 4.2, it only provides a partial picture. It would be interesting to study the influence of the exact values of the hyper-parameters on the results. These hyper-parameters are not only the weights \lambda_d, \lambda^g_{adv} and \lambda^l_{adv}, but also the number of clusters and the size of the patches used. 

Summary:
I would rate this paper as borderline. There is some novelty in the proposed approach, but it is mitigated by the relation to the work of Chen et al., CVPR 2018. The experiments show good results, but a more thorough evaluation of the influence of the hyper-parameters would be useful.
","The sentiment of the review is mixed, with both positive and negative aspects highlighted. The reviewer acknowledges the strengths of the paper, such as the interesting idea of modeling structure via patches and the good results on standard benchmarks. However, the reviewer also points out several weaknesses, including the lack of novelty in the method and the need for further clarification and additional experiments. Therefore, the sentiment score is 0, indicating a neutral stance overall. The politeness of the language used in the review is quite high. The reviewer provides constructive criticism and suggestions for improvement in a respectful and professional manner, without any rude or harsh language. Thus, the politeness score is 80.",0,80
"The paper presents a variational inference approach for locally linear dynamical models. In particular,  the latent dynamics are drawn from a Gaussian approximation of the parent variational distribution,  enabled by Laplace approximations with fixed point updates, while the parameters are optimized the resulting stochastic ELBO. Experiments demonstrate the ability of the proposed approach to learning nonlinear dynamics, explaining data variability, forecasting and inferring latent dimensions.  

Quality: The experiments appear to be well designed and support the main claims of the paper. 

Clarity: The clarity is below average. In Section 2 the main method is introduced. However, the motivation and benefits of introducing a parent and child variational approximation are not discussed adequately. It would be helpful to move some of the stuff in the appendix to the main text, and present in a neat way. I also struggled a little to understand what is the difference between forward interpolate and filtering. 

Originality: Given the existing body of literature, I found the technical novelty of this paper rather weak. However, it seems the experiments are thoroughly conducted. In the tasks considered, the proposed method demonstrates convincing advantages over its competitors.  

Significance: The method shall be applicable to a wide variety of sequential data with nonlinear dynamics. 

Overall, this appears to be a board-line paper with weak novelty. On the positive side, the experimental validation seems well done. The clarity of this paper needs to be strengthened.  

Minor comments: 
- abstract: uncover nonlinear observation? -> maybe change ""observation"" to ""latent dynamics""?

","The sentiment of the review is moderately positive, as the reviewer acknowledges the well-designed experiments and the method's applicability to a wide variety of sequential data with nonlinear dynamics. However, the reviewer also points out significant weaknesses in clarity and technical novelty. Therefore, the sentiment score is 20. The politeness of the language is quite high, as the reviewer provides constructive feedback without being harsh or dismissive, and even offers specific suggestions for improvement. Thus, the politeness score is 80.",20,80
"This article presents a novel approach called Integral Pruning (IP) to reduce the computation cost of Deep neural networks (DNN) by integrating activation pruning along with weight pruning. The authors show that common techniques of exclusive weight pruning does compress the model size, but increases the number of non-zero activations after ReLU. This would counteract the advantage of DNN accelerator designs (Albericio et al., 2016; Reagen et al., 2016) that leverage activation sparsity to speed up the computations. IP starts with pruning the weights using an existing technique to mask out weights under a threshold and then fine-tune the network in an iterative fashion to maintain the accuracy. After weight pruning, IP further masks out the activations with smaller magnitude to reduce the computation cost. Unlike weight pruning techniques that use static masks, the authors propose to use dynamic activation masks for activation sparsity in order to account for various patterns that are being activated in DNN for different input samples. In order to do this, the 'winner rate' measure for every layer (or for a group of layers in deep networks like ResNet32) is defined, to dynamically set the threshold for the generation of activation masks which eventually controls the amount of non-zero activation entries. The article empirically analyzes the sensitivity of activation pruning on validation data by setting different winner rates at every layer in DNN and decides upon a set of winner rates accordingly followed by an iteration of fine-tuning the network to maintain its performance. The authors show that their technique produced lower number of non-zero activations in comparison with the intrinsic sparse ReLU activations and weight pruning techniques. 

The topic of reducing network complexity for embedded implementations of DNNs is highly relevant, in particular for the ICLR community.

The IP technique yields significantly reduced number of multiply-accumulate operations (MACs) across different models like MLP-3, ConvNet-5, ResNet32 and AlexNet and on different datasets like MNIST, CIFAR10 and ImageNet. They also depicted that pruning the activations with dynamic activation masks followed by fine-tuning the network yields more sparse activations and negligible loss in accuracy when compared against using static activation masks.
    

Strengths of the paper:
- The motivation to extend compression beyond the weights to activations in order to support the DNN accelerator designs and the technical details are clearly explained. 
- The proposed technique indeed produces sparser activations than intrinsic ReLu sparse activations and can also applied to any network regardless of the choice of activation function.
- The proposed technique is evaluated across different network architectures and datasets.
- The advantage of adapting dynamic activation masks over static ones is clearly demonstrated.

 Weaknesses of the paper:
- The originality of the approach is limited because it is a relatively straightforward combination of existing techniques for weight and activation pruning.
- The ""winner rate"" measure is defined for every layer and should be explored over different values in order to find the equilibrium to reduce the number of non-zero activations and maintain the accuracy. This search of winner rates will become inefficient as the depth of the network increases. However, the authors used a single winner rate for a group of layers in case of ResNet-32 to reduce the exploration of search space but this choice might lead to suboptimal results.
- The authors compare the resultant number of MAC operations against numbers from the weight pruning technique. However, there also exist different works on group pruning techniques like Liu et al. (2017), Huang & Wang (2017), Ye et al. (2018) to prune entire channels / feature maps and thus yield more compact networks. Since these approaches prune the channels, they show a direct impact on the computation complexity and greatly reduce the computation time. A proper and fair comparison would be to compare the numbers of IP against such group pruning techniques. This comparison is highly important to highlight the significance of the approach on speeding up the DNNs and it is missing from the paper.
- At several locations in Section 4, e.g. Sec. 4.1, 4.3, and 4.4. there is no precise statement about the incurred accuracy loss (or no statement at all). The connection to Figures 4 and 5 is not immediately clear and should be made explicit.
		
References:
- Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui Zhang. Learning efficient convolutional networks through network slimming.
- Jianbo Ye, Xin Lu, Zhe Lin, and James Z Wang. Rethinking the smaller-norm-less-informative assumption in channel pruning of convolution layers
- Zehao Huang and Naiyan Wang. Data-driven sparse structure selection for deep neural networks.
        
Overall Evaluation:
The authors integrate activation pruning along with the weight pruning and show that the number of MAC operations are greatly reduced by their technique when compared to the numbers of weight pruning alone. 	However, I am not convinced regarding the reported number of MAC operations since the number of MAC operations of sparse weight matrices and activations would remain the same as the original models unless some of the filters/activation maps are pruned from the network.  On the other hand, comparisons against group pruning techniques are highly necessary to evaluate the potential impact of the approach on speeding up of DNNs. My preliminary rating is a weak reject but I am open to revise my rating based on the authors response to the above stated major weaknesses.

Minor comments:
- Caption of Fig. 4 should mention the task on which the results were obtained.
- There are occasional grammar errors and typos that should be corrected.","The sentiment of the review is mixed. The reviewer acknowledges the relevance of the topic and the strengths of the paper, such as the clear explanation of the technique and its evaluation across different architectures and datasets. However, the reviewer also points out significant weaknesses, including the limited originality of the approach, inefficiency in the search for winner rates, and the lack of comparison with group pruning techniques. The overall evaluation leans towards a weak reject, indicating a sentiment score of -20. The language used in the review is polite and constructive, with suggestions for improvement and specific recommendations for the authors, resulting in a politeness score of 80.",-20,80
"Summary: This paper proposes three new techniques for improving Atari performance over APE (Horgan 2018).  Two of them are closely linked in that they deal with improving stability.  Another involves integrating the use of expert trajectories from DQfD.  

I will summarize each: 

Transformed Bellman: This applies a rescaling function (it's basically a monotonically increasing version of the sqrt(x) function) to the Q-function and applies the inverse of the function to the max Q-value of the next state (such that the contracting effect h-function is not ""applied"" multiple times when doing the TD backup).  

Temporal Consistency: This encourages the ""next state"" after where the TD-update is applied to not change too much.  This addresses a problem discussed in (Durugkar 2018).  I think the intuition here is that the state which follows the state with the TD update may be visually similar, but it does not impact the value in the past states, so its value function should not have a highly correlated change with the previous state's change in value function.    

DQfD: Storing an expert replay buffer and an actor replay buffer.  The expert replay buffer is fixed and the actor replay buffer stores the most recent ""actor processes"".  Train with both a supervised imitation loss (only for the highest return episode) and the original TD loss.  Additionally, the pre-training phase is removed and the ratio of expert-learned trajectories is fixed (both seem like steps in the right direction).  

Review: This paper proposes a few changes to DQN training, two of which are aimed at reducing instability, and one is aimed at improving exploration (expert trajectories).  Because all of these changes are well justified and the experiments are fairly thorough, I recommend acceptance.  My main reservation is that the ideas presented are not very strongly thematically linked.  The presence of ablation studies compensates for this to some extent.  

Strengths: 

  -The discussion of related work and comparison to baselines is pretty extensive.  For example I appreciated the ablation study removing ""transformed Q-learning"" and comparison to the pop-art method.  

  -The results, at least for Ape-X DQfD seem impressive to me in that the method works without reward clipping and with a much higher discount factor.  Additionally the results generally outperform DQfD (uses expert trajectories) and Rainbow (no human trajectories).  Additionally evidence was presented that the learned policies often exceed the performance of the human demonstrations (for example in time to achieve rewards).  

Weaknesses: 

  -Two of the techniques: ""transformed bellman"" and ""temporal consistency"" seem well-linked thematically, but the expert demonstration idea seems orthogonal.  I would have preferred splitting that idea out into a separate paper, given that the paper is already 20 pages.  

  -The motivation for temporal consistency just references (Durugkar 2018).  The readability of this paper would be improved if it were discussed more here as well.  I also feel like the analysis could be more thorough here, for example a result using the temporal consistency loss on Baird's counter example really should be shown (like figure 2 in Durugkar's paper).  

-It would be nice to see a visualization or a toy problem with the ""transformed bellman"".  

Questions: 

-Is the ""highest return episode"" idea (3.4) general or is it exploiting the fact that Atari is deterministic?  It seems like in general we'd want to use many high reward episodes, or the highest reward episodes that go into different parts of state space.  It seems like it could be a very bad idea on certain settings (for example if the reward has a lot of randomness).  

-""Proposition 3.1 shows that in the basic cases when either h is linear or the MDP is deterministic, Th
has the unique fixed point h ◦ Q∗"".  From 3.1, it looks if h is linear, then it distributes over r(x,a) + maxh^{-1}(Q) and then it also won't effect which is the max, so it would reduce to h*r(x,a) + max(Q) - which means it's just rescaling the original reward.  So then this result is trivial?  Please correct me if I misunderstood something here.  

-Could an MDP be constructed which causes the transformed bellman operator to perform badly?  I am imagining something where the MDP is just a single step, and there is a stochastic action which behaves like a lottery.  So perhaps there is a 1-in-1-million chance to win 1-billion dollars by taking an action.  If I understand correctly the transformed bellman operator will destroy the large reward here (because in a single step, there is just r(x,a) which h is applied to).  Which would make the action seem bad even though it's actually appealing.  

Notes: 

  -I did not read the proofs in the appendix.  ","The sentiment of the review is generally positive, as indicated by the recommendation for acceptance and the acknowledgment of the well-justified changes and thorough experiments. However, there are some reservations about the thematic linkage of the ideas and suggestions for improvement, which slightly temper the overall positivity. The politeness of the language is high, as the reviewer provides constructive feedback and poses questions in a respectful manner without any harsh or dismissive language.",70,90
"This paper puts forward a new schema for language modeling, especially for relationship between two parts far apart.

The experimental results on WikiText-103 are good, improving the STOA PPL by 9.0. On the other three datasets, however, there's little or no gain. The speed comparison should be carried out over more LM models, as Al-Rfou is not the fastest.

The writing is not very clear, especially around equations.

Overall the contribution of this paper is marginally incremental:
1. The major proposed idea is just to add one no-grad previous segment into the prediction for next segment. This is similar to Residual network idea but more simplified.
2. Using relative positional encoding is not a new idea, e.g. https://arxiv.org/pdf/1803.02155.pdf.
3. Reusing previous level/segment computation with gradient fixed is also not a big innovation.

typo:
1. end of page 3, and ""W."" denotes"".
2. The speed experiment should be put in the main text.","The sentiment of the review is mixed. The reviewer acknowledges the improvement on WikiText-103 but points out the lack of significant gains on other datasets and mentions that the contribution is marginally incremental. The sentiment score is therefore slightly negative. The politeness of the language is neutral to slightly polite, as the reviewer provides constructive feedback without using harsh or rude language.",-20,20
"The authors claim contributions in three areas:
1) Learning representations on physiological signals. The proposed approach  uses LSTMS with a loss function that aims at predicting the next five minutes of the physiological signals. Based on their experiments, using this criteria outperforms 
 LSTM autoencoder approaches that are tuned to reconstruct the original signals. The description of this work needs more details. It would be good to have clarity on these loss functions and also on the architecture of the LSTM autoencoder that is claimed here. Is it a standard seq2seq model? Is it something else?

2) They use the hidden state of the LSTMs as a representation of the inputs signals. From this representation, they have setup a set of supervised/predictive tasks to measure the efficacy of the representation. For this, they used gradient boosting machines. 

3) They propose a way to estimate interpretability by tracking the impact of the input data on the predictions using an model agnostic approach using Shapley values. I have found this part of the paper particularly obscure. I recommend shedding some light on the structure of this model that generates these Shapley values. 

The experimental result section also needs work in my opinion. First of all, the authors may want to better describe the data used. How many patients are in this set? How was the data partitioned for training, testing, validation? Any hyper-paremeter tuning? I have found the “transference” arguments a bit weak. First of all, the physical distance between hospital should not be mentioned as a way to compare “hospitals”. How did the authors select these features shown on Figure 2? MIMIC has more features than this. Why were these additional features discarded? Is the data coming from the same type of operating rooms in the case of hospital 0 and 1? I am somehow skeptical on the transfer of embeddings learned in an ICU setting to an OR setting. It would be great to provide details on the type of patients that are being monitored. 

It is quite hard to argue from what’s presented in 4.3.3 that the proposed approach is interpretable. Can the authors explain how a visual inspection of Figure 5 “makes sense” as stated in the paper? What is the point that’s being made here? Any reason why more conventional attention mechanisms have not been looked at for interpretability?

Overall, I have found the problem addressed here interesting. However, I think that the paper needs work, both on the presentation of the methodology and also on the presentation of more convincing experimental arguments. ","The sentiment of the review is mixed but leans towards the positive side. The reviewer acknowledges the interesting problem addressed by the authors and recognizes the potential contributions of the paper. However, the reviewer also points out several areas that need significant improvement, such as the need for more details on the methodology, better experimental results, and clearer explanations. Therefore, the sentiment score is 20. The politeness of the language is quite high. The reviewer uses polite language throughout the review, offering constructive criticism and suggestions for improvement without being rude or dismissive. Therefore, the politeness score is 80.",20,80
"This paper describes an approach for training conditional future frame prediction models, where the conditioning is with respect to the current frame and additional inputs - specifically actions performed in a reinforcement learning (RL) setting. 

The authors suggest that one can predict future frames from a vector comprised of an observation encoding and an action. To train the model, they suggest using a linear combination of three different losses: (1) an adversarial loss that encourages the generated sample to look similarly to training data, (2) an InfoGAN-inspired loss that is supposed to maximise mutual information between the conditioning (e.g. action) and the generated sample, and (3) a content loss, taken to be the mean-squared error of the prediction and ground-truth in the VGG feature space.

The major contribution of this work seems to be using these three losses in conjunction, while doing conditional frame prediction at the same time. While interesting, there exist very similar approaches that also use adversarial losses [1] as well as approaches using different means to reach the same goal [2, 3]. None of these are mentioned in the text, nor evaluated against. It is true that [1] is not action-conditional, but adding actions as conditioning could be a simple extension.

Experimental section consists of an ablation study, which evaluates importance of different components of the loss, and a qualitative study of model predictions. With no comparison to state of the art (e.g. [1, 3]), it is hard to gauge how valuable this particular approach is. 
The qualitative evaluation starts with §4.4¶1 “we follow the customary GAN literature to include some qualitative results for illustration”, as if there was no other reason for including samples than to follow the custom. Since the paper is about action-conditional prediction, it would be interesting to see predictions conditioned on the same initial sequence but different actions, which are not present, however. Moreover, this work is developed in the context of RL applications, and since prior art [4] has shown that better predictive models do not necessarily lead to better RL results, it would be interesting to evaluate the proposed approach against baselines in an RL setting.

The paper is clearly written, but some claims in the text are not supported by any citations (e.g. §1¶2 “More recently, several papers have shown that forward modelling…” without a citation).  Some claims are misleading (e.g. §1¶3 says that by using adversarial training we don’t need to use task-specific losses and it does not put constraints on input modality. While true, using MSE loss is equally general). Some other claims are not supported at all or may not be true (e.g. §3.2¶1 “ResNet … aims at compressing the information in the raw observation” - to the best of my knowledge, there is no evidence for this).

To conclude, the suggested approach is not novel, the experimental evaluation is lacking, and the text contains a number of unsupported statements. I recommend to reject this paper.

[1] Lee, A.X., Zhang, R., Ebert, F., Abbeel, P., Finn, C., & Levine, S. (2018). Stochastic Adversarial Video Prediction. CoRR, abs/1804.01523.
[2] Eslami, S.M., Rezende, D.J., Besse, F., Viola, F., Morcos, A.S., Garnelo, M., Ruderman, A., Rusu, A.A., Danihelka, I., Gregor, K., Reichert, D.P., Buesing, L., Weber, T., Vinyals, O., Rosenbaum, D., Rabinowitz, N.C., King, H., Hillier, C., Botvinick, M.M., Wierstra, D., Kavukcuoglu, K., & Hassabis, D. (2018). Neural scene representation and rendering. Science, 360, 1204-1210.
[3] Denton, E.L., & Fergus, R. (2018). Stochastic Video Generation with a Learned Prior. ICML.
[4] Buesing, L., Weber, T., Racanière, S., Eslami, S.M., Rezende, D.J., Reichert, D.P., Viola, F., Besse, F., Gregor, K., Hassabis, D., & Wierstra, D. (2018). Learning and Querying Fast Generative Models for Reinforcement Learning. CoRR, abs/1802.03006.","The sentiment score is -80 because the reviewer highlights several significant shortcomings of the paper, including lack of novelty, insufficient experimental evaluation, and unsupported claims, ultimately recommending rejection. The politeness score is 20 because, while the reviewer is critical, the language used is professional and avoids personal attacks, maintaining a level of respect towards the authors.",-80,20
"The paper aims at justifying the performance gain that is acquired by the use of ""composite"" neural networks (e.g., composed of a pre-trained neural network and additional layers that will be trained for the new task).

I found the paper lacking in terms of writing and in terms of clarity in expressing scientific/mathematical ideas especially for a theory paper.

Example from the Abstract:

""The advantages of adopting such a pre-trained model in a composite neural network are two folds. One is to benefit from other’s intelligence and diligence, and the other is saving the efforts in data preparation and resources
and time in training""

The main results of the paper (Theorem 1,2,3) are of the following nature: if you use more features (i.e., ""components"") in the input of a network then you have ""more information"", and this cannot be bad. Here are the corresponding claims in the Abstract:

""we prove that a composite neural network, with high probability, performs better than any of its pre-trained components under certain assumptions.""

""if an extra pre-trained component is added to a composite network, with high probability the overall performance will be improved.""

However, this argument seems to be just about expressiveness; adding more features can be statistically problematic. 

Furthermore, why is it specific to pre-trained components? Essentially the theorems are about adding any features.

Finally, the assumption that the pre-trained components are linearly independent is invalid and the makes the whole analysis somewhat simplistic.


The motivating Example 1 just shows that the convex hull of a class of hypotheses can include more hypotheses than the class itself. I don't see any connection between this and the use of pre-training.

Other examples unclear statements from the intro:

""One of distinctive features of the complicated applications is their applicable data sources are boundless. Consequently, their solutions need frequent revisions.""

""Although neural networks can approximate arbitrary functions as close as possible (Hornik, 1991), the major reason for not existing such competent neural networks for those complicated applications is their problems are hardly fully understood and their applicable data sources cannot be identified all at once.""

There are many typos in the paper including this one about X for the XOR function:
""Assume there is a set of locations indexed as X = {(0; 0); (0; 1); (1; 0); (1; 0)} with the corresponding values Y = (0; 1; 1; 0). Obviously, the observed function is the XOR""


","The reviewer's sentiment is largely negative, as they point out several significant issues with the paper, including lack of clarity, invalid assumptions, and numerous typos. The sentiment score is -80 because the reviewer does not find much merit in the paper and highlights fundamental flaws. The politeness score is 20 because, while the reviewer is critical, they do not use rude or offensive language. They provide specific examples and constructive criticism, which indicates a level of professionalism and politeness despite the negative feedback.",-80,20
"This is a well written paper which proposes to learn heteroscedastic noise models from data by optimizing the prediction likelihood end-to-end through differentiable Bayesian Filters. In addition to existing Bayesian filters, the paper also proposes two different versions of the [differentiable] Unscented Kalman Filter. Performance of the different filters and noise models is evaluated on two real-world robotic problems: Visual Odometry and visual tracking of an object pushed by the robot.
While the general idea of learning the noise variances through backpropagation are straightforward extensions of existing work on differential Bayesian filters, the questions that the paper explores are important to make end-to-end learning of Bayesian filter more common. The results will help future research select the correct differential filter for their use case, and insight in potential benefits (or lack thereof) by learning heteroscedastic or homoscedastic process noise, and/or observation noise.
A downside is that the paper does not further explore how to weigh different loss terms which are apparently important to successfully train such models. Also unfortunate is the footnote which states that the current results are incomplete and will be updated, hence as a reviewer I am not sure which results and conclusions are valid right now.


Pros:
+ clearly written
+ useful experiments for those seeking to select a differential Bayesian filter, and learning (heteroscedastic) noise from data.
+ experiments on real-world use cases rather than toy problems

Cons:
- Incomplete experiments according to footnote, thus results and conclusions might change after this review.
- Unclear what the effect of the selected process / observation model is on the learned noise


Below are more detailed comments and questions:
* p6. Footnote: ""due to time constraints, ..., results will be updated"" Is this acceptable? I have never seen such a notice when reviewing. So, are the current results on a single fold? Will the numbers in the tables, or the conclusions change after this review?
* If I understand correctly, the paper 'only' focuses on learning the heteroscedastic noise variance, but assumes that the deterministic non-linear parts of the process and observation models are fixed. I did not find this very clearly stated in the paper, though at least the Appendix explicitly states the used functions for the process models.
* I would have liked to see in the paper more explanation on how the process and observations models were selected and validated  in the experiments, since I expect that the validity of these functions affects the learned noise variances. Since the noise needs to account for the inaccuracies in the deterministic models, would the choice for these functions not impact your conclusions? And, would it or would it not be possible to learn both these deterministic models and the noise jointly from the training data?
* Is it possible to add priors on Q and R parameters for Bayesian treatment of learning model parameters? I can imagine that priors can guide the optimization to either adjust more of the Q or more of the R variance to improve the likelihood.

* Section 1:
	* ""Our experiments show that ... "" This may be a matter of taste, but I did not expect to see the main conclusions already in the introduction. They should appear in the abstract to help out the quick reader. In the introduction, it appears as if you are talking about some separate preliminary experiments, and which you base some conclusions that will be used in the remainder of this paper.

* Section 3:
	* So, mostly empirical study, since heteroscedastic noise models were already used?
	* ""Previous work evaluated ... "" please add citations

* Section 4.1:
	* ""train a discriminative neural network o with parameters wo to preprocess the raw sensory data D and thus create a more compact representation of the observations z = o(D;wo)."" At this point in the paper, I don't understand this. How is z learned, via supervised learning (what is the target value for z)? Or is z some latent representation that is jointly optimized with the filters? This only became somewhat clearer in Sec. 5.2 on p.8 where it states that ""We ... train a neural network to extract the position of the object, the contact point and normal as well as ..."". So if I understand correctly, the function o for z = o(D) is thus learned offline w.r.t. some designed observation variables for which GT is available (from manual annotations?).

* Section 4.2:
	* ""we predict a separate Qi for every sigma point and then compute Q as the weighted mean"" → So, separate parameters w_g for each sigma point i, or is a single learned non-linear function applied to all points?

* Section 4.3:
	* Equation 14: inconsistent use of boldface script: should use bold sigma_t, and bold l_t ?
	* ""In practice, we found that during learning ... by only increasing the predicted variance"" →  This is an interesting observation, which I would have liked to see explored more. I understand that term (ii) is needed to guide the learning processes, but in the end wouldn't we want to optimize the actual likelihood? So, could you (after the loss with (ii) converged) reduce \lambda_2 to zero to properly optimize only the log likelihood without guidance from a good initial state? Or is it not possible to reliably optimize the likelihood via back-propagation at all from some reason?

* Section 5.1.1
	* ""... of varying length (from 270 to over 4500 steps) ..."" it would be good to mention the fps, to get understand to what real-world time horizons 50 / 100 frames correspond.

* Section 5.1.2:
	* Table 1: How are the parameters of the filters in the ""no learning"" column obtained? Are these tuned in some other way, or taken form existing implementations? Also, can you clarify if the 'no learning' parameters served as the initial condition for the learning approaches?
	* Table 1, first row column Q+R: ""0.2"" → Is there a missing zero here, i.e. ""0.20""? Otherwise, the precision of reported results in this table is not consistent. Hard to say: is the mean of R+Q 0.2, and slightly lower than R+Qh, or could it be as high as 0.24 ?
	* ""learning a heteroscedastic process noise model leads to big improvements and makes the filters competitive with the EKF"". Results for EKF still appear significantly better than the novel UKF, and even the PF (especially rotational error).

* Section 6: 
	* ""Large outliers in the prediction of the preprocessing networks were not associated with higher observation noise."" I don't see on what presented results these conclusions were drawn, as this is the first time the word ""outlier"" is mentioned in the paper. Outliers seem indeed important, as they contradict the typical assumptions e.g. of Gaussian noise, so it would be useful to clarify how the proposed techniques handle such outliers.
","The sentiment of the review is generally positive, as indicated by phrases like 'well written paper' and 'useful experiments.' However, there are some critical points mentioned, such as the incomplete experiments and unclear effects of the selected models, which slightly temper the overall positivity. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer uses polite and constructive language throughout, even when pointing out weaknesses. Phrases like 'I would have liked to see' and 'Is it possible to add' indicate a polite and respectful tone. Therefore, the politeness score is 90.",60,90
"It has previously been observed that training deep networks using large batch-sizes leads to a larger generalization gap compared to the gap when training with a relatively small batch-size. This paper proposes to add noise sampled from diagonal ""empirical"" Fisher matrix to the large batch gradient as a method for closing the generalization gap. The authors motivate the use of empirical Fisher for sampling noise by arguing that the covariance of gradients from small batch-sizes can be seen as approximately equal to a scaled version of the Fisher matrix. It is then pointed out that using the Fisher matrix directly to sample noise could in principle close the generalization gap but would lead to slow converegence similar to SGD with a small batch-size. The authors then claim that the convergence speed is better when noise is sampled from the diagonal Fisher matrix instead of the full Fisher matrix. This claim is proven in theory for a convex quadratic loss surface and experiments are conducted to empirically verify this claim both in the quadratic setting are for realistic deep networks. Finally an efficient method for sampling noise from the diagonal empirical Fisher matrix is proposed.

Comments:
I think the paper is very well written and the results are presented clearly. In terms of novelty, I found the argument about convergence using diagonal Fisher being faster compared with full Fisher quite interesting, and its application for large batch training to be insightful. 

As a minor comment, for motivating theorem 3.1, it is pointed out by the authors that the diagonal Fisher acts as an approximation of the full Fisher and hence their regularization effects should be similar while convergence should be faster for diagonal Fisher. As a caveat, I think the authors should also point out that the convergence rate would be best when C is set to 0 in the result of the theorem. This implies no noise is used during SGD updates. However, this would imply the regularization effect from the noise will also vanish which would lead to poor generalization. 


However, there is a crucial detail that makes the main argument of the paper weak. In the main experiments in section 4.3, for the proposed large batch training method, the authors mention that they use a small batch-size of 128 for the first 50 epochs similar to Smith et al (2017) and then switch to the large batch-size of 4096, at which point, the learning rate is linearly scaled proportional to the large batch-size with a warmup scheme similar to Goyal et al (2017) and ghost batch normalization is used similar to Hoffer et al (2017). The former two tricks have individually been shown on their own to close the generalization gap for large batch-size training on large datasets like ImageNet. This paper combines these tricks and adds noise sampled from the diagonal Fisher matrix on top when switching to large batch-size after epoch 50 and reports experiments on smaller datasets-- MNIST, Fashion MNIST and the CIFAR datasets. Finally, the accuracy numbers for the proposed method is only marginally better than the baseline where isotropic noise is added to the large batch-size gradient. For these reasons, I do not consider the proposed method a significant improvement over existing techniques for closing the generalization gap for large batch training.

There is also a statement in the paper that is problematic but can be fixed by re-writing. In the paper, empirical Fisher matrix, as termed by the authors in the paper, refers to the Fisher matrix where the target values in the dataset is used as the output of the model rather than sampling it from the model itself as done for computing the true Fisher matrix. This empirical (diagonal) Fisher matrix is used to sample noise which is added to the large batch gradient in the proposed method. It is mentioned that the covariance of the noise in small batch SGD is exactly same as the empirical Fisher matrix. This claim is premised on the argument that the expected gradient (over dataset) is unconditionally roughly 0, i.e., throughout the training. This is absolutely false. If this was the case, gradient descent (using full dataset) should not be able to find minima and this is far from the truth. Even if we compare the scale of expected gradient to the mini-batch gradient (for small batch-size), the scale of these two gradients at any point during training (using say small batch-size SGD) is of the same order. I am saying the latter statement from my personal experience. The authors can verify this as well.

Overall, while I found the theoretical argument of the paper to be mostly interesting, I was dissapointed by the experimental details as they make the gains from the proposed method questionable when considered in isolation from the existing methods that close the generalization gap for large batch training.","The sentiment of the review is mixed. The reviewer appreciates the theoretical contributions and the clarity of the writing, which is reflected in positive comments about the novelty and presentation of the results. However, the reviewer also expresses significant concerns about the experimental details and the overall impact of the proposed method, which leads to a more critical tone in the latter part of the review. Therefore, the sentiment score is moderately positive but tempered by the critical points raised. The language used in the review is polite and constructive, even when pointing out flaws and suggesting improvements. The reviewer provides specific feedback and avoids any rude or dismissive language.",20,80
"This paper introduces an approach to compressing a trained neural network by looking at the correlation of the filter responses in each layer. Two strategies are proposed: one based on trying to preserve the energy of the original activations and one based on looking at the KL divergence between the normalized eigenvalues of the activation covariance matrix and the uniform distribution.

Strengths:
- The KL-divergence-based method is novel and has the advantage of not requiring to define any hyper-parameter.
- The results show the good behavior of the approach.

Weaknesses:

Method:
- One thing that bothers me is the spatial max pooling of the activations of convolutional layers. This means that is two filters have high responses on different regions of the input image, they will be treated as correlated. I do not understand the intuition behind this.
- In Section 2, the authors mention that other methods have also proposed to take the activation into account for pruning, but that they aim to minimize the reconstruction error of these activations. In fact, this is also what PFA-En does; for a given dimension, PCA gives the representation that minimizes the reconstruction error. Therefore, the connection between this method and previous works is stronger than claimed by the authors.
- While it is good that the KL-divergence-based method does not rely on any hyper-parameter, the function \psi used in Eq. 3 seems quite ad hoc. As such, there has also been some manual tuning of the method.

Experiments:
- In Table 1, there seems to be a confusion regarding how the results of FGA are reported. First, in (Peng et al., 2018), the %FLOPS is reported the other way around, i.e., the higher the better, whereas here the lower the better. Similarly, in (Peng et al., 2018), a negative \Delta in accuracy means an improved performance (as stated in the caption of their Table 2, where the numbers reported here were taken). As such, the numbers reported here, and directly taken from this work, are misinterpreted. 
- Furthermore, Peng et al., 2018 report much better compression results, with %FLOP compression going up to 88.58%. Why are these results not reported here? (To avoid any misunderstanding, I would like to mention that I am NOT an author of (Peng et al., 2018)).
- Many of the entries in Table 1 are empty due to the baselines not reporting results on these datasets or with the same network. This makes an actual comparison more difficult.
- Many compression methods report results on ImageNet. This would make this paper more convincing.
- While I appreciate the domain adaptation experiments, it would be nice to see a comparison with Masana et al., 2017, which also considers the problem of domain adaptation with network compression and, as mentioned in Section 2, also makes use of the activations to achieve compression.

Related work:
- It is not entirely clear to me why tensor factorization methods are considered being so different from the proposed approach. In essence, they also perform structured network pruning.
- The authors argue that performing compression after having trained the model is beneficial. This is in contrast with what was shown by Alvarez & Salzmann, NIPS 2017, where incorporating a low-rank prior during training led to higher compression rates.
- The authors list (Dai et al., 2018) as one of the methods that aim to minimize the reconstruction error of the activations. Dai et al., 2018 rely on the mutual information between the activations in different layers to perform compression. It is not entirely clear to me how this relates to reconstruction error.

Summary:
I do appreciate the idea of aiming for a hyper-parameter-free compression method. However, I feel that there are too many points to be corrected or clarified and too many missing experiments for this paper to be accepted to ICLR.

After Response:
I appreciate the authors' response, which clarified several of my concerns. I would rate this paper as borderline. My main concern now is that the current comparison with existing method still seems too incomplete, especially with ResNet architectures, to really draw conclusions. I would therefore encourage the authors to revise their paper and re-submit it to an upcoming venue.
","The sentiment of the review is mixed but leans towards the negative side. The reviewer acknowledges some strengths of the paper, such as the novelty of the KL-divergence-based method and its advantage of not requiring hyper-parameters. However, the review predominantly focuses on the weaknesses and shortcomings of the paper, including methodological concerns, misinterpretation of results, incomplete comparisons, and missing experiments. This leads to a sentiment score of -30. The politeness of the language used in the review is generally respectful and constructive. The reviewer provides detailed feedback and suggestions for improvement without being rude or dismissive, resulting in a politeness score of 70.",-30,70
"Thank you for an interesting read.

The paper proposes an approximate training technique for energy-based models (EBMs). More specifically, the samples used negative phase gradient in EBM training is approximated by samples from another generator. This ""approximate generator"" is a composition of a decoder (which, with a Gaussian prior on latent variable z, is trained to approximate the data distribution) and another EBM in latent space. The authors show connections to WGAN training, thus the name EnGAN. Experiments on natural image generation and anomaly detection show promising improvements, although not very significant.

From my understanding of the paper, the main contribution of the paper comes from section 4, which proposes a latent-space MCMC scheme to improve sample quality. I have seen several papers fusing EBMs and GAN training together and to the best of my knowledge section 4 is novel (but with problems, see below). Section 3's recipe is quite standard, e.g. as seen in Kim and Bengio (2017), and in principle contrastive divergence also uses the same idea. The idea of estimating of the entropy term for the implicit distribution p_G with adversarial mutual information estimation is something new, although quite straight-forward.

Although I do agree that MCMC mixing in x space can be much harder than MCMC mixing in z space, since I don't think the proposed latent-space MCMC scheme is exact (apart from finite-time simulation, rejection...), I don't see theoretically why the method works.

1. The MCMC method essentially samples z from another EBM, where that EBM(z) has energy function -E_{\theta}(G(z)), and then generate x = G(z). Note here EBM(z) != p(z). The key issue is, even when p_G(x) = p_{\theta}(x), there is no guarantee that the proposed latent-space MCMC method would return x samples according to distribution p_{\theta}(x). You can easily work out a counter example by considering G is an invertible transformation. Therefore I don't understand why doing MCMC on this latent-space EBM can help improve sample quality in x space.

2. Continuing point 1, with Algorithm 1 that only fits p_G(x) towards p_{\theta}(x), I am confident that the negative phase gradient is still quite biased. Why not just use the latent-space MCMC sampler composited with G as the generator, and use these MCMC samples to train both the decoder G and the mutual information estimator?

3. I am not exactly sure why the gradient norm regulariser in (3) make sense here? True that it would be helpful to correct the bias of the negative phase, but why this particular form? We are not doing WGAN here and in general we don't usually put a Lipschitz constraint on the energy function. I've seem several GAN papers arguing that gradient penalty helps in cases beyond WGAN, but most of them are just empirical observations...
Also the Omega regulariser is computed on which x? On data? Do you know whether the energy is guaranteed to be minimized at data locations? In this is that appropriate to call Omega a regulariser?

The presentation is overall clear, although I think there are a few typos and confusing equations:

1. There should be a negative sign on the LHS of equation 2.
2. Equation 3 is inconsistent with the energy update equation in Algorithm 1. The latter one makes more sense.
3. Where is the ratio between the transition kernels in the acceptance ratio equation? In general for Langevin dynamics the transition kernel is not symmetric.
","The sentiment of the review is moderately positive. The reviewer acknowledges the interesting nature of the paper and the novelty of section 4, but also points out several significant issues and areas for improvement. Therefore, the sentiment score is 20. The politeness of the language is quite high. The reviewer uses polite language, such as 'Thank you for an interesting read' and 'I do agree,' and provides constructive criticism without being rude or dismissive. Therefore, the politeness score is 80.",20,80
"Review: This paper deals with the issue of learning rotation invariant autoencoders and classifiers.  While this problem is well motivated, I found that this paper was fairly weak experimentally, and I also found it difficult to determine what the exact algorithm was.  For example, how the optimization was done is not discussed at all.  At the same time, I'm not an expert in group theory, so it's possible that the paper has technical novelty or significance which I did not appreciate.  

Strengths: 

 -The challenge of learning rotation equivariant representations is well motivated and the idea of learning representations which transfer between different scales also seems useful.  

Weaknesses: 
  
-I had a difficult time understanding how the preliminaries (section 2) were related to the experiments (section 3).  

-The reference (Kondor 2018) is used a lot but could refer to three different papers that are in the references.  

  -Only reported results are on rotated mnist, but the improvements seem reasonable, but unless I'm missing something are worse than the 1.62% error reported by harmonic nets (mentioned in the introduction of the paper).  In addition to rot-mnist, harmonic nets evaluated boundary detection on the berkeley segmentation dataset.  

  -It's interesting that the model learns to be somewhat invariant across scales, but I think that the baselines for this could be better.  For example, using a convolution network with mean pooling at the end, one could estimate how well the normal classifier handles evaluation at a different scale from that used during training (I imagine the invariance would be somewhat bad but it's important to confirm).  


Questions: 

-Section 3.1 makes reference to ""learning parameters"".  I assume that this is done in the usual way with backpropagation and then SGD/Adam or something?  

-How is it guaranteed that W is orthogonal in the learning procedure?  
","The sentiment of the review is mixed but leans towards the negative side. The reviewer acknowledges the motivation behind the problem and some strengths of the paper but points out several weaknesses and areas of confusion. Therefore, the sentiment score is -30. The politeness of the language is generally respectful and constructive, with the reviewer expressing their limitations in understanding certain technical aspects and providing specific feedback without being rude. Thus, the politeness score is 70.",-30,70
"This paper trains an information retrieval (IR) model by contrasting the joint query-document distributions, p(q, d) with negative samples drawn from a resampling of the product of marginals, p(q) x p(d). They use a second discriminator to provide the re-weighting (I believe picking to top negative sample from the other model) and train this other model in a way that mirrors the first. They also attempt to point out some theoretical problems with a competing model, IRGAN, which uses a generator that is trying to model the joint.

While I like the proposal idea, I think the paper has too many problems to warrant publication. First, the story is very disappointing. The authors phrase most of the paper as a critique of IRGAN, but this critique falls short. Really this is more of a paper about where to get negative samples when training a model of the joint (or the log-ratio in this case). Using negative samples from real data with noise contrastive estimation [1] is found in numerous works in NLP [2][3], and has gained some recent attention in the context of representation learning [4][5]. The first algorithm proposed is essentially doing a sort of ranking loss on negative samples, which mirrors similar works [6]. In fact, the generator in IRGAN could be viewed as just a parametric / adaptive negative sampling distribution in the context of NCE for the ultimate purpose of learning an estimate of the log-ratio. The most interesting thing I think of this work here is the co-training, i.e., using another model to help re-sample, and I think this idea should be explored in more detail.

Second, the paper spends far too much time revisiting prior work than addressing their own model, doing more analysis, providing more insight.

Third, the paper is just poorly written. The notation is confusing, some of the equations are unclear (I have no idea how ""r"" is used in any of this), and the arguments of the baseline in IRGAN don't really doesn't make any sense.

Notes:
P1
I don't really follow why IRGAN is so central to this work. Good ideas aren't difficult to motivate, especially if empirically everything works out.
P2
I'm having trouble with claims, especially more recently, about GAN instability, particularly since numerous approaches [7][8] seem to have more or less solved the problem.

The use of ""|"" in G is awfully confusing.
P3
Almost 2 pages of unnecessary background

P4
Why are we using ""|"" in functions? What's wrong with "",""?
theta = \theta
I don't understand the point of the quote (in italics).
What happened to ""r"" in all of this?
The last two equations and their relationship could be more clear.

You use italics, so is this supposed to be a quote? But then you have a section which attempts to show this.
P5
I have no idea what's supposed to be going on in 5). The samples from the real joint don't factor in the generator gradient, or at least it's absolutely not clear that this pops out of the baseline? Then you switch from log (1 - x) to - log x and there's some claim about this violating the adversarial objective?

It took me more than a few reads to figure out what the equation at the bottom of P5 is doing: is this resampling? It's fairly unclear.

[1] Gutmann, Michael U., and Aapo Hyvärinen. ""Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics.""
[2] Mnih, Andriy, and Koray Kavukcuoglu. ""Learning word embeddings efficiently with noise-contrastive estimation."" 
[3] Mikolov, Tomas, et al. ""Distributed representations of words and phrases and their compositionality.""
[4] Oord, Aaron van den, Yazhe Li, and Oriol Vinyals. ""Representation learning with contrastive predictive coding."" 
[5] Hjelm, R. Devon, et al. ""Learning deep representations by mutual information estimation and maximization.""
[6] Faghri, Fartash, et al. ""VSE++: Improving Visual-Semantic Embeddings with Hard Negatives.""
[7] Miyato, Takeru, et al. ""Spectral normalization for generative adversarial networks.""
[8] Mescheder, Lars, Andreas Geiger, and Sebastian Nowozin. ""Which Training Methods for GANs do actually Converge?."" ","The sentiment of the review is largely negative, as the reviewer points out several significant issues with the paper, including its disappointing story, excessive focus on prior work, poor writing, and unclear equations. The reviewer does acknowledge some positive aspects, such as the interesting idea of co-training, but these are overshadowed by the numerous criticisms. Therefore, the sentiment score is -70. The politeness of the language is relatively neutral. While the reviewer is critical, they do not use rude or disrespectful language. They provide specific feedback and suggestions for improvement, which indicates a constructive approach. Therefore, the politeness score is 0.",-70,0
"This paper shows that training of a 3 layer neural network with 2 hidden nodes in the first layer and one output node
is NP-complete. This is an extension of the result of Blum and Rivest'88. The original theorem was proved for 
threshold activation units and the current paper proves the same result for ReLU activations. The authors do this
by reducing the 2-affine separability problem to that of fitting a neural network to data. The reduction is well 
written and is clever. This is a reasonable contribution although it does not add significantly to the current state of the art. 
  ","The sentiment of the review is moderately positive. The reviewer acknowledges that the paper extends a previous result and describes the reduction as well-written and clever. However, the reviewer also notes that the contribution does not add significantly to the current state of the art. Therefore, the sentiment score is 30. The politeness of the language is high; the reviewer uses respectful and constructive language throughout the review, resulting in a politeness score of 80.",30,80
"Summary: 
This paper investigates the properties of minimizing cross-entropy of linear functions over separable data (looks like logistic loss). The authors show a simple example where the minimizer of the cross-entropy loss leads to maximum margin hyperplane where the bias term is regarded as an extra dimension, which is different from the standard max. margin solution of  SVMs with bias not regarded as an extra dimension. The authors then propose a method to obtain the latter solution by minimizing the cross-entropy loss.


Comments:

There is a previously known result quite related to this paper: 

Ishibashi, Hatano and Takeda: Online Learning of Approximate Maximum p-Norm Margin Classifiers with Bias, COLT2008. 

Theorem 2 of Ishibashi et al. shows that the hard margin optimization with linear classifier with bias is equivalent to those without bias over pairs of positive and negative instances. 

Combined with Theorem 3 of (Soudry et al., 2018)), I am afraid that the main result Theorem 5 can be readily derived. 

For this reason, I am afraid that the main technical result is quite weak.

After Rebuttal:
I read the authors' comments. I understand more the technical contribution of the paper and raised my score. But I also agree with Reviewer 3.
","The sentiment of the review appears to be mixed. Initially, the reviewer expresses concerns about the originality and strength of the main technical result, which suggests a negative sentiment. However, after reading the authors' rebuttal, the reviewer acknowledges a better understanding of the technical contribution and raises their score, indicating a shift towards a more positive sentiment. Therefore, the sentiment score is set to 10, reflecting a slightly positive but still cautious stance. The politeness of the language is generally respectful and professional, with phrases like 'I am afraid' and 'I understand more,' which soften the critique. Thus, the politeness score is set to 80.",10,80
"#update: I've read the authors comments but unfortunately my main concerns about the contributions and novelty of this work are not answered. As such, I cannot increase my score.

------------------ 

The authors provide a study on learning to refer to 3D objects. The authors collect a dataset of referential expressions and train several models by experimenting with a number of architectural choices.

This is an interesting study reporting results on the effect that several architectural choices have generating referential expressions. Overall, while I appreciate all the experiments and results, I don't really feel I've learned something from this paper. 

First and foremost, the paper, from the title already starts to build up expectations about the 3d nature of the study, however this is pretty much ignored at the rest of the paper. I would expect the paper to provide  some results and insights regarding the 3D nature of the dataset and how this affects referential expressions, however, there is no experiment that has used this 3d-ness in any way. Even the representations of the objects are stripped down to essentially 2D (a single-view of a 3D object used to derived VGG features is as 3D as any image dataset used for similar studies, right?). 
My major question is then: why should all this research take place in a 3D dataset? Is it to validate that research like this is at all possible with 3D objects? 

Moreover, all interesting aspects of referential expressions are stripped out since the authors experiment only with this geometric visual property (which has again nothing to do with 3d-ness, you could totally get that out of images). An interesting study would be to have all objects in the same image and have referential expressions that have to do with spatial expressions, something that the depth or a different view of the of the object could play a role.

Given the fact that there are no technical innovations, I can't vouch for accepting this paper, since there has been quite a lot of research on generating  referential expressions on image datasets (e.g., Kazemzadeh., 2014 and related papers). However, learning to refer to 3D objects is a very interesting topic, and of great importance given the growing interest of training agents in 3D virtual environments, and I would really encourage the authors to embrace the 3d-ness of objects and design studies that highlight the challenges and opportunities that the third dimension brings.


Kazemzadeh et al.: ReferIt Game: Referring to Objects in Photographs of Natural Scenes","The sentiment of the review is generally negative, as the reviewer expresses significant concerns about the contributions and novelty of the work, and ultimately cannot recommend acceptance. The sentiment score is -60. The politeness of the language is relatively high, as the reviewer provides constructive feedback and encourages the authors to improve their study, despite their criticisms. The politeness score is 60.",-60,60
"This paper proposed a new method for face completion using progressive GANs. The novelty seems very limited compared with previous methods. The results did not significantly outperform previous methods such as CTX in terms of visual quality. In addition, some of the features for the proposed method were not evaluated properly. 

1. The frequency attention module is not convincing. The visualization of the attention features look like normal feature in a neural network. Also, in Figure 8, the quality of results with and without FAM look very similar. These 4 images were selected from 3000 test images, but the difference is too small to show the benefit of FAM. 

2. In figure 8, it is unclear how the performance changes with each loss. Probably the results without L_bdy, L_rec, L_feat should be analyzed separately. 

3.  In figure 6, the results compared to CTX look similar. And the figure is too small to see the details. For example, from row 1, the result by CTX seems even better. 

4. How many images were used in the user study? Did each subjects evaluate the entire test set 3009 images? 
","The sentiment of the review is generally negative, as it highlights several shortcomings of the paper, such as limited novelty, lack of significant improvement over previous methods, and insufficient evaluation of proposed features. The reviewer also points out specific issues with the figures and the user study. The politeness of the language is neutral; the reviewer provides constructive criticism without using rude or overly harsh language.",-60,0
"This manuscript applies transfer learning for protein surface prediction. The problem is important and  the idea is novel and interesting. However, the  transfer learning model is unclear. 
Pros:  interesting and novel idea
Cons:  unclear transfer learning model, insufficient experiments. 

Detail: section 4 describes the transfer learning model used in the work, but the description is unclear. It is unknown the used model is a new model or existing model. Besides, in the experiments, the proposed method is not compared to other transfer learning methods.  Thus, the evidence of the experiments is not enough. ","The sentiment of the review is moderately positive as it acknowledges the importance and novelty of the idea but also points out significant issues with the clarity of the transfer learning model and the sufficiency of the experiments. Therefore, the sentiment score is 20. The language used in the review is polite and constructive, providing specific feedback without being harsh or rude. Hence, the politeness score is 80.",20,80
"This paper proposed a new method for image restoration based a task-discriminator in addition to the GAN network. It shows superior performance than the baseline methods without such task-discriminator on medical image restoration and image super-resolution. While the results are better, the idea seems straightforward and has limited novelty. Please see the following comments:

1. Adding an task-discriminator in a GAN network seems straightforward to improve the specific task. And this idea has already used in existing papers, e.g. Cycada.  

Hoffman, J., Tzeng, E., Park, T., Zhu, J.Y., Isola, P., Saenko, K., Efros, A.A. and Darrell, T., 2017. Cycada: Cycle-consistent adversarial domain adaptation. ICML, 2018

2. On the application side, the results are not very convincing because the baseline methods were not selected properly. For medical image reconstruction and image super-resolution, the proposed method was not compared with any of the state-of-the-art methods, but only with the same method without a task-discriminator as a baseline. For those tasks, there are many traditional methods and deep nets with different losses. For example, a simple L1/L2 or perceptual loss probably leads to better PSNR than the GAN loss, which is not compared at all. See the attached references. 


Ledig, C., Theis, L., Huszár, F., Caballero, J., Cunningham, A., Acosta, A., Aitken, A.P., Tejani, A., Totz, J., Wang, Z. and Shi, W., Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network. In CVPR 2017.

Johnson, J., Alahi, A. and Fei-Fei, L., Perceptual losses for real-time style transfer and super-resolution. In ECCV 2016.

Kim, J., Kwon Lee, J. and Mu Lee, K., Accurate image super-resolution using very deep convolutional networks. In CVPR 2016.

3. Some questions about medical image datasets. For the low-dose PET dataset, the input was randomly undersampled by a factor of 100. What is the random pattern? Is it uniform? In addition, why not acquire real low-dose data and show the quality results using the proposed model? For the multi-constast MRI data, how is the input generated and what is the ground-truth? 
",The sentiment of the review is moderately negative. The reviewer acknowledges the superior performance of the proposed method but criticizes its novelty and the selection of baseline methods. The politeness of the language is neutral to slightly polite. The reviewer provides constructive feedback and references to support their points without using harsh or rude language.,-30,20
"This paper solves Flappy bird by combining DQN and probabilistic programming. I think this is in general a good avenue to explore.

However I found the paper to be poorly written. For example, notation is not properly introduced, there are many mathematical mistakes and typos in the written text and citations. This makes it very hard to understand what is actually going on.

It is also not clear what is the probabilistic program and what are we conditioning on? What is the inference algorithm? Maybe it's useful to expand more on how this ties to the ""RL as inference"" framework (see e.g. Levine, 2018). It seems like we are doing rejection sampling where the condition is ""no collision"". As a result, I'm not sure whether sampling from prior is a competitive baseline.

For the DQN experiment, the learning curve seems very noisy in a way that it's unclear whether a fair conclusion can be drawn only from one run (as it appears to be done).

The experiments also feel a bit contrived to make a strong case for probabilistic programming + DQN.","The sentiment of the review is mixed but leans towards negative. The reviewer acknowledges that the paper explores a good avenue but criticizes the writing quality, clarity, and experimental design. Therefore, the sentiment score is -40. The politeness score is 20 because the reviewer uses polite language but is direct and critical in their feedback.",-40,20
"Summary.
The paper proposes a vehicle’s trajectory planner that iteratively predict next-step (longitudinal and latitudinal) position of an ego-vehicle. Instead of using a raw image, a set of handcrafted features (i.e., the status of traffic lights, route, roadmap, etc) are mapped onto a fixed-size of bird-eye view map, which is then fed into the recurrent neural network. Additional regularizing loss terms are explored for the robustness of the model. The effectiveness of the method is demonstrated in simulation and real-world experiment.

Strengths.
- Impressive demonstrations in simulation and real-world experiments.
- The paper is generally well-written and easy to follow.

vs. Existing motion planning approaches.
There exists a large volume of papers on vehicle motion planning, which has largely been explored for controlling self-driving vehicles. Some of them successfully demonstrated their effectiveness for navigating a vehicle in typical driving scenarios, including “slowing down for a slow car”.
A notable survey may include:

[1] Paden et al., “A survey of motion planning and control techniques for self-driving urban vehicles,” IEEE Transactions on intelligent vehicles, 2016. 

However, the paper provides neither any works of literature on existing motion planners nor any types of comparison with them. This makes hard to judge the proposed learning-based motion planner outperforms others including conventional optimization-based methods. 

Missing data collection details.
This work depends hugely on its own human-designated oracle-like map, which provides driving-related features, such as lane, the status of traffic lights, speed limits, desired route, dynamic objects, etc. Generating this map would not be a trivial task, but details are missing on (1) how this data collected and (2) how this data can be collected during the testing time (especially for dynamic objects/traffic light status). Section 6.2 should be explained more in detail.

A weak novelty of using intermediate-level input/output representation.
There exist similar approaches that utilized similar representations to determine a vehicle’s behaviour, examples may include:

[1] Lee et al., “Convolution Neural Network-based Lane Change Intention Prediction of Surrounding Vehicles for ACC,” IEEE ITSC 2017.
[2] We et al., “Modeling trajectories with recurrent neural networks,” IJCAI, 2017.

Missing evaluation details.
In Section 6.2, (though not mentioned) it seems that a training dataset is collected from 60-days of real-world driving (given the context). But, in the testing phase, it seems that the authors used a simulator to evaluate different driving scenarios with various initial condition (i.e., speed, heading angle, position, etc). Can authors clarify details of the evaluation environment?

Minor concerns.
A paragraph of contribution summary (in Introduction section) will help. 
Typos (e.g., Section 2 line 17: ‘off of’)","The sentiment of the review is moderately positive, as it acknowledges the impressive demonstrations and the well-written nature of the paper, but it also points out several significant shortcomings, such as the lack of comparison with existing methods, missing data collection details, and weak novelty. Therefore, the sentiment score is 30. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, providing specific recommendations without being harsh or dismissive. Therefore, the politeness score is 80.",30,80
"This paper studied the information bottleneck principle for deep learning. In the paper by (Schwatz-Ziv & Tishby 17'), it is empirically shown that the mutual information I(X;T) between input X and internal layers T decreases, which is called a compression phase. In this paper, the author found that the compression phase is not always happening and the shape of the curve of I(X;T) highly depends on the ""bining size"" which is used for estimating mutual information by (Schwatz-Ziv & Tishby 17'). Then the authors proposed to use a noisy DNN to make sure the map X->T is stochastic, then proposed a guaranteed mutual information estimator. Then some empirical results are shown.

I think the problem in (Schwatz-Ziv & Tishby 17') do exist and their result is highly questionable. However, I have some major question about this paper.

1. In this paper a noisy DNN was proposed. However, how do you choose the noise level \beta? If I understand correctly, the noise level plays a similar role of the bining size in (Schwatz-Ziv & Tishby 17'). Noise level goes to zero is similar to bining size goes to zero. I wish to see a figure about how different \beta affects the curve of I(X;T) (similar to Figure 1 but let \bet change). 

    In Figure 4(d) there is a plot showing different \beta will affect the mutual information, but the x-axis is ""weight"". I wonder that how the curve of mutual information change w.r.t \beta, if the x-axis is training epochs. Do your statement stable about \beta? 

2. I think Section 3 and Theorem 1 are interesting and insightful. But I notice that in Section 10 you mentioned that this will be a separate paper. Is it OK to put them together in this paper?

3. The paper by (Schwatz-Ziv & Tishby 17') has not pass a peer-review process and it is still a preprint. This paper is nothing but only saying some deficiencies of (Schwatz-Ziv & Tishby 17') (except Section 3 and Theorem 1 which I think should be an independent paper). I think such a paper should not be published as a conference paper before (Schwatz-Ziv & Tishby 17') pass a peer-review process.

So totally I think this paper should not be accepted by ICLR at this point. I think Section 3 and Theorem 1 should become an independent paper, and the DNN approach can be an application of the mutual information estimator.","The sentiment of the review is moderately negative. The reviewer acknowledges the existence of a problem in the referenced paper and finds some sections of the current paper interesting and insightful. However, the overall recommendation is to reject the paper for publication at this point, which indicates a negative sentiment. The politeness of the language is relatively high. The reviewer uses polite language, such as 'I think,' 'I wish to see,' and 'I wonder,' and provides constructive feedback without being rude or dismissive.",-40,80
" Summary: 
%%%%%%%%%%%%%%%
The paper explores ways to adapt the learning rate rule through a new minimax formulation.
The authors provide regret bounds for their method in the online convex optimization setting.

Comments:
%%%%%%%%%%%%%%%
-I found the motivation of the approach to be very lacking.
Concretely, it is not clear at all why the minimax formulation even makes sense, and the authors do not explain this issue.

-While the authors provide regret guarantees for their method, the theoretical analysis does not reflect when is their approach  beneficial compared to standard adaptive methods. Concretely, their bounds compare with the well known bounds of AdaGrad. 
It is nice that their approach enables to extract AdaGrad as a private case. But again, it is not clear what is the benefit of their extension.

-Finally, the experiments do not illustrate almost any benefit of the new approach compared to standard adaptive methods.


Summary
%%%%%%%%%%%%%%%
The paper suggests a different approach to adapt the learning rate.
Unfortunately, the reasoning behind the new approach is not very clear.
Also, nor theory neither experiments illustrate the benefit of this new approach over standard methods.
","The sentiment of the review is quite negative. The reviewer points out several significant issues with the paper, including a lack of clear motivation for the approach, insufficient theoretical analysis, and unconvincing experimental results. These criticisms suggest a sentiment score of -80. The politeness of the language is relatively neutral. While the reviewer is critical, they do not use harsh or rude language, and their comments are focused on the content rather than being personal. Therefore, the politeness score is 0.",-80,0
"
This paper consider the connections between SGD and distributionally robust optimization. There has long been observed a connection between robust optimization and generalization. Recently, this has been explored through the lens of distributionally robust optimization. e.g., in the papers of Namkoong and Duchi, but also many others, e.g., Farnia and Tse, etc. Primarily, this paper appears to build off the work of Namkoong. 

The key connection this paper tries to make is between SGD and DRO, since SGD in sampling a minibatch, can be considered a small perturbation to the distribution. Therefore the authors use this intuition to propose a weighted version of SGD (WSGD) whereby high variance weights are assigned to mini batch, thus making the training accomplish a higher level of distributional robustness. 

This idea is tested on a few data sets including CIFAR-10 and -100. The results compare WSGD with SGD, and they show that the WSGD-trained models have a lower robust loss, and also have a higher (testing) accuracy. 

This is an interesting paper. There has been much discussion of the role of batch size, and considering it from a different perspective seems to be of interest. But the connection of the empirical results to the theoretical results seems tenuous. It’s not clear how predictions of the theory match up. This would be useful to understand better. More generally, a simpler presentation of the key results would be useful, so as to allow the reader to better appreciate what are the main claims and if they are as substantial as claimed. Overall the writing needs significant polishing, though this is only at a local level, i.e, it doesn’t obscure the flow of the paper. ","The sentiment of the review is generally positive, as the reviewer acknowledges the paper's interesting approach and the relevance of the topic. However, there are some criticisms regarding the connection between empirical and theoretical results and the need for clearer presentation. Therefore, the sentiment score is 50. The politeness of the language is quite high, as the reviewer provides constructive feedback without being harsh or dismissive, and uses phrases like 'would be useful' and 'needs significant polishing' in a respectful manner. Thus, the politeness score is 80.",50,80
"This paper proposes a distributed policy gradient method for learning policies with large, collaborative, homogeneous swarms of agents. 

Formalism / objective: 
The setting is introduced as a ""collaborative Markov team"", so the objective is to maximise total team reward, as expressed in equation (3). This definition of the objective seems inconsistent with the one provided at line (14): Here the objective is stated as maximising the agent's return, L_n, after [k] steps of the agent updating their parameters with respect to L_n, assuming all other agents are static. I think the clearest presentation of the paper is to think about the algorithm in terms of meta-learning, so I will call this part the 'inner loop' from now on. 
Note (14) is a very different objective: It is maximising the return of an agent optimising 'selfishly' for [k] steps, rather than the ""collaborative objective"" mentioned above. This seems to break with the entire premise of collaborative optimisation, as it was stated above. 
My concern is that this also is reflected in the experimental results: In the food gathering game, since killing other agents incurs ""a small negative reward"", it is never in the interest of the team to kill other team-mates. However, when the return of individual agents is maximised both in the inner loop and the outer loop, it is unsurprising that this kind of behaviour can emerge. Please let me know if I am missing something here. 

Other comments: 
-The L_n(theta, theta_n) is defined and used inconsistently. Eg. compare line (9), L_n(theta_n, theta), with line below, L_n(theta, theta_n). This is rather confusing 
-In equation (10) please specific which function dependencies are assumed to be kept? My understanding is that \theata_n is treated as a function of theta including all the dependencies on the policies of other agents in the environment? 
-Related to above, log( pi_\theta_n ( \tan_n)) in line 16 is a function of all agents policies through the joint dependency on \theta. Doesn't that make this term extremely expensive to evaluate? 
-Why were the TRPO_kitchensink and A3C_kitchensink set up to operate on the minimum reward rather than the team reward as it is defined in the original objective? It is entirely possible that the minimum reward is much harder to optimise, since feedback will be sparse. 
-The survival game uses a discrete action space. I am entirely missing MARL baseline methods that are tailored to this setting, eg. VDN, QMIX, COMA etc to name a few. Even IQL has not been tried. Note that MADDPG assumes a continuous action space, with the gumble softmax being a common workaround for discrete action spaces which has not been shown to be competitive compared to the algorithms mentioned above. 
-Algorithmically the method looks a lot like ""Learning with Opponent Learning Awareness"", with the caveat that the return is optimised after one step of 'self-learning' by each agent rather than after a step of 'Opponent-learning'. Can you please elaborate on the similarity / difference? 
-Equation (6) and C1 are presented as contributions. This is the standard objective that's commonly optimised in MARL when using parameter sharing across agents.","The sentiment of the review is moderately negative. The reviewer points out several inconsistencies and issues with the paper's objectives, definitions, and experimental results. The review does not contain any positive remarks about the paper's contributions or strengths. The politeness of the language is relatively high. The reviewer uses polite phrases such as 'please let me know if I am missing something here' and 'can you please elaborate,' which indicate a respectful tone despite the critical feedback.",-60,80
"This problem of interest in this paper is Curriculum Learning (CL), in the context of deep learning in particular. CL refers to learning a non-random order of presenting the training examples to the learner, typically with easier examples presented before difficult ones, to guide learning more effectively. This has been shown to both speed up learning and lead to better generalization, especially for more challenging problems. In this paper, they claim that their contribution is to decompose the problem of CL into learning two functions: the scoring function and the pacing function, with the role of the former being to estimate the difficulty of each training example and the latter to moderate the schedule of presenting increasingly more challenging examples throughout training.

Overall, I found it hard to understand from reading the paper what exactly is new versus what is borrowed from previous work. In particular, after reading Weinshall et al, I realized that they have already proposed a number of things that are experimented with here: 1) they proposed the approach of transfer learning from a previously-trained network as a means of estimating the ‘scoring function’. 2) they also distinguish between learning to estimate the difficulty of examples, and learning the schedule of decreasing difficulty throughout learning, which is actually stated here as the contribution of this paper. In particular, in Section 3 of Weinshall et al, there is a sub-section named “scheduling the appearance of training examples” where they describe what in the terminology of this paper would be called their pacing function. They experiment with two variants: fixed, and adaptive, which are very similar to two of the pacing functions proposed here.

Bootstrapping:
A component of this work that didn’t appear in Weinshall et al, is the bootstrapping approach to estimating the scoring function. In general, this involves using the same network that is being trained on the task to estimate the difficulty of the training examples. The authors explain that there are two ways to do this: estimate how easy each training example is with respect to the ‘current hypothesis’ (the weights of the network at the current step), and with respect to the ‘final hypothesis’, which they estimate if I understand correctly as the network at the end of training. The latter would necessitate first training the network in the standard way, and then using it to estimate how easy or hard each example is, and using those estimates to re-train the network from scratch using that curriculum. They refer to the former as self-paced learning and to the latter as self-taught learning. I find these names confusing in that they don’t really convey what the difference is between the two. Further, while self-paced learning has been studied before (e.g. Kuman et al), I’m not sure about self-taught learning. Is this a term that the authors here coined? If not, it would be useful to add a reference. 

Using easy / hard examples as judged by the current / final hypothesis:
When using the current hypothesis, under some conditions, Weinshall et al showed that choosing harder examples is actually more beneficial than easy examples, similar in spirit to hard negative mining. On the other hand, when using the final hypothesis to estimate examples’ difficulty, using a schedule of increasing difficulty is beneficial. Based on this, I have two comments: 1) It would therefore be useful to implement a version that uses the current hypothesis to estimate how easy each example is (like the self-paced scoring function) but then invert these estimates, in effect choosing the most challenging instead of the easiest ones as is done for anti-curriculum learning. This would be a hybrid between the current self-paced scoring function and anti-curriculum scoring function that would essentially implement the hard negative mining technique in this context. 2) It would be useful to comment on the differences between the self-paced scoring function used here, and that in Kumar et al. In particular, in this case using a curriculum based on this scoring function seems to harm training but in Kumar et al, they showed it actually increased performance in a number of different cases. Why does one work but the other doesn’t?

Experiments:
The experiments are presented in a subset of 5 classes from CIFAR-10 (also used by Weinshall et al.), but also in the full CIFAR-10 and CIFAR-100 datasets. They used both a small CNN (same as in Weinshall et al) as well as a VGG architecture. Overall, their results are comparable to what was previously known: using a curriculum computed by transfer leads to improved learning speed and final performance (though sometimes very slightly) compared to the standard training, and the training with a random curriculum. Further, the benefit is larger when the task is harder (as measured by the final vanilla-trained performance). By computing the distances between the gradients obtained from using a curriculum (via the transfer scoring function) and no curriculum confirms that these two training setups indeed drive the learning in different directions; an analysis similar to Weinshall et al. Also, since, as was previously known and they also observe, the benefit of CL is larger at the beginning of training, they propose a single-step pacing function that performs similarly to other pacing functions while is simpler and more computationally effective. The idea is to decrease only once the proportion of easy examples used in mini-batches, via a step function. Therefore at the start many easy examples are used, and after this threshold is surpassed, few easy examples are used.
 
Overall, I don’t feel the contribution of this paper is large enough to recommend acceptance. The main points that guided this decision are: 
1) The relationship with previous work is not clear. In particular, Weinshall et al seems to have already proposed a few components that are claimed to be the contribution of this paper, as elaborated on above. The authors should mention that the transfer scoring function was borrowed from Weinshall et al, clarify the differences between their pacing functions from those in Weinshall et al., etc. 
2) The usefulness of using easy or hard experiments when consulting the current or final hypothesis is discussed but not explored sufficiently. An additional experiment is proposed above to add another ‘data point’ to this discussion. 
3) self-paced learning is presented as something that doesn’t work and wasn’t expected to work. However, in the past successes were shown with this method, so it would be useful to clarify the difference in setup, and justify this difference.
4) It seems that the experiments resulted to similar conclusions to what was already known. While it’s useful to confirm these findings on additional datasets, I didn’t feel that there was a significant insight gained from them.
","The sentiment of the review is moderately negative. The reviewer expresses difficulty in understanding the novelty of the paper and points out significant overlaps with previous work, particularly that of Weinshall et al. The reviewer also mentions that the contribution of the paper is not large enough to recommend acceptance. Therefore, the sentiment score is -60. The politeness of the language is quite high. Despite the critical feedback, the reviewer maintains a professional and respectful tone throughout the review, providing constructive criticism and suggestions for improvement. Thus, the politeness score is 80.",-60,80
"This manuscript introduces SEGEN, a model based on Evolutionary Computation for building deep models. Interestingly, the authors define deep models in a different way. Instead of stacking several hidden layers one after the other (as in traditional deep learning models), SEGEN uses the idea of generations in evolutionary models (Genetic Algorithms or GA) and puts the unit models in the successive generations into layers, i.e., “evolutionary layer”. Each layer then performs the validation, selection, crossover, and mutation operations, as in GA. Another interesting point of the proposed method is that the choice of unit models in SEGEN can be traditional machine learning or recent deep learning models.
The paper touches an interesting topic and proposes a sound method. However, there are several issues with the paper. There are several ungrounded and untested claims, as well as many unclear points in the method.
-	In page 5, Section 4.2.4, the authors introduce the loss function used to define the fitness for the evolutionary model. It is not clear why they use the difference between the latent representations of the autoencoders (z) from pairwise nodes to define the loss. There are no motivations or discussion for this. Two different representations of two nodes may both be good (e.g., in terms of classification of data), but they do not have to be necessarily identical. 
-	Given the loss defined in Section 4.2.4, it is not clear how the authors ran their model for MNIST and other datasets, for which they used CNN and MLP unit models. In CNN and MLP there is not latent representation z.
-	Based on the model descriptions in Section 4.2 (and its Subsections), the proposed method transfers the learned models in previous generations to the next ones. But there is no explanation if the new models are again fine-tuned on the data? For instance, take the autoencoders, for two different unit models, the cross-over operator defuses the variables (weights and bias) from the two selected models to create an offspring. There is no guarantee that the new autoencoder model works properly on the same dataset. As a naïve example, if there are correlated and redundant features in the data, different autoencoders may separately focus on one/some of these features. Defusing weights of the two autoencoders (built upon different aspects of the data) may most probably ruin the whole model. 
-	There are four claims in the paper on the advantages of the proposed model, compared to other deep learning algorithms. None of these claims are discussed in depth or at least illustrated experimentally. 
*** Less Data for Unit Model Learning. The authors could have reported the number of variables used in each model in the experiments. It is important to see with how many of a larger number of variables a traditional deep model can result in comparable results to SEGEN. 
*** Less Computational Resources. The model operates in several generations and in each generation, many unit models are built. It is not fair to say and not clear how it can occupy less space or time complexity than a regular GCNN or MLP.
*** Less Parameter Tuning. Again experiments could clarify this issue.
*** Sound Theoretic Explanation. The authors only refer to (Rudolph 1994) for the performance bounds of their model and claim that since they are using GA they are better than other deep learning models. However, performance bounds for GA models are very shallow and proximal. 
-	To calculate the computational complexity of the model, the authors analyzed the time for learning one unit model. However, in GA models, the complexity is calculated using the bounds on the number of times the fitness function is called since the fitness function is the most computationally intensive task (please see: Pelikan and Lobo 1999 ‘Parameterless Genetic Algorithm A Worst-case Time and Space Complexity Analysis’). 
-	One of the main fallacies of GAs and evolutionary algorithms is that they may lead to premature convergence. This is very common, especially at the presence of trap functions, such as non-convex functions that real-world problems deal with (please see: Goldberg et al. 1991 ‘Massive Multimodality, Deception, and Genetic Algorithms’). There are no discussions/experiments on how SEGEN may overcome the premature convergence, or even if it converges at all.
","The sentiment of the review is mixed. The reviewer acknowledges the interesting and sound aspects of the proposed method but also points out several significant issues and untested claims. This results in a sentiment score of around -20, indicating a slightly negative sentiment overall. The politeness of the language is quite high. The reviewer uses polite and constructive language throughout the review, providing detailed feedback without being rude or dismissive. This results in a politeness score of 80.",-20,80
"In the paper , the authors proposed the use of autoencoder for Model-X knockoffs. The authors proved that, if there exists latent factors, and if the encoders and the decoders can approximate conditional distributions well, the autoencoder can be used for approximating Model-X knockoff random variables: one can find relevant features while controlling FDR (Theorem 2.2).

I think the theoretical part is good, and the experimental results seem to be promising.

My concern is the gap between theory and practice. In the manuscript, the authors used VAE for approximating conditional distributions. The question is how we can confirm that the trained encoder and decoder satisfy the assumptions in Theorem 2.2. If the trained models violate the assumptions, the control of FDR is no longer guaranteed, which may lead to false discoveries. As long as this gap remains unfilled, we cannot use the procedure reliably: we always need to doubt whether the encoders and decoders are trained appropriately or not. I think this gap is unfavorable for scientific discovery where only rigid procedures are accepted.
How we can use the proposed procedure reliably, e.g. for scientific discovery? Is there any way to confirm that the encoders and decoders are appropriate? Or, is there any way to bypass the gap so that we can guarantee the FDR control even for inappropriate models?","The sentiment of the review is generally positive, as the reviewer acknowledges the theoretical soundness and promising experimental results of the paper. However, the reviewer expresses a significant concern about the practical applicability of the proposed method, specifically the gap between theory and practice. This concern is articulated in a constructive manner, asking for clarification and potential solutions. The language used is polite and professional, aiming to improve the work rather than dismiss it.",50,80
"## Summary ##

The authors apply policy gradients to combinatorial optimization problems. They suggest a surrogate reward function that mitigates the variance in the reward, and hence the update size. They demonstrate performance on a clique-finding problem.


## Assessment ##

I don't think Cakewalk is different enough from the cross-entropy method to warrant acceptance in ICLR. 

 I also have concerns about the independence assumption in their sampling distribution (Section 3.2), and the fact that their experiments use the same set of (untuned) hyperparameters for each method.

They both approximate the reward CDF from K samples and use this to construct a surrogate reward. The difference is that Cakewalk uses the CDF directly, while CE uses a threshold function on the CDF.


## Specific Comments and Questions ##

1. Cakewalk is *very* closely related to the cross-entropy method. The authors acknowledge this connection, but I think they should begin by introducing CE and then explain how Cakewalk generalizes it. Both Cakewalk and CE approximate the reward CDF from K samples and use this to construct a surrogate reward. The difference is that Cakewalk uses the CDF directly, while CE uses a threshold function on the CDF.
2. The distribution proposed in section 3.2 assumes independence between the elements $x_j$. This seems problematic for some relatively simple problems. Consider $x$ a binary vector and reward equal to the parity $S(x) = \sum{x_j} % 2$.
3. In the experiments, there are large discrepancies between different optimizers on Cakewalk (e.g. SGA vs AdaGrad, Table 4). Is there any explanation for this?
4. How were the hyperparameters (learning rate, AdaGrad $\delta$, Adam $\beta_1, \beta_2$) chosen? It seems like a large assumption that the same learning rate would work for different methods, especially when some of them are normalizing the objective function. I would suggest tuning these values for each method independently.
5. It would be nice to see experimental results on more than one problem. The authors discuss their results on k-medoids in the appendices, but it seems like these results aren't quite complete yet. 
6. In Table 3, the figure in bold is not the lowest (best) in the table. The reason for this is only given in a single sentence at the end of Section 6, so it is a little confusing. I would replace these values with N/A or something similar.","The sentiment score is determined by the overall tone and content of the review. The reviewer expresses a negative sentiment towards the paper's novelty and methodology, particularly in the assessment section where they state that the paper is not different enough from existing methods to warrant acceptance. This results in a sentiment score of -60. The politeness score is assessed based on the language used throughout the review. The reviewer uses polite language, providing constructive feedback and specific recommendations without being rude or dismissive. This results in a politeness score of 80.",-60,80
"In the manuscript entitled ""Neural Causal Discovery with Learnable Input Noise"" the authors describe a method for automated causal inference under the scenario of a stream of temporally structured random variables (with no missingness and a look-back window of given size).  The proposed approach combines a novel measure of the importance of fidelty in each variable to predictive accuracy of the future system state (""learnable noise risk"") with a flexible functional approximation (neural network).  Although the setting (informative temporal data) is relatively restricted with respect to the general problem of causal inference, this is not unreasonable given the proposed direction of application to automated reasoning in machine learning.  The simulation and real data experiments are interesting and seem well applied.

A concern I have is that the manuscript as it stands is positioned somewhere between two distinct fields (sparse learning/feature selection, and causal inference for counterfactual estimation/decision making), but doesn't entirely illustrate its relationship to either.  In particular, the derived criterion is comparable to other sparsity-inducing penalities on variable inclusion in machine learning models; although it has motivation in causality it is not exclusively derived from this position, so one might wonder how alternative sparsity penalities might perform on the same challenge.  Likewise, it is not well explained what is the value of the learnt relationships, and how uncertainty and errors in the causal learning are relevant to the downstream use of the learnt model.  In the ordinary feature selection regime one is concerned simply with improving the predictive capacity of models: e.g. a non-linear model might be fit using just the causal variables that might out-perform both a linear model and a non-linear model fit using all variables.  Here the end goal is less clear; this is understandable in the sense that the work is positioned as a piece in a grand objective, but it would seem valuable to nevertheless describe some concrete example(s) to elucidate this aspect of the algorithm (use case / error effects downstream).  ","The sentiment of the review is moderately positive, as the reviewer acknowledges the interesting and well-applied experiments and the reasonable restriction of the setting. However, the reviewer also raises concerns about the manuscript's positioning between two distinct fields and the clarity of its end goal. Therefore, the sentiment score is 40. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, providing specific recommendations without being harsh or dismissive. Therefore, the politeness score is 80.",40,80
"This paper studies the problem of optimizing non-decomposable metric in classification. This topic has been discussed in several recent works mainly under deterministic classifier context, the authors discuss the possibility of training a neural network and learn the model by gradient-based methods, which could result in randomized classifier; and conducted experiments to compare the performance with other existing methodologies. I have the following concerns after reading it.

1.The main idea of the paper has shown in other related works and the authors didn’t convince me why their work solves something that could not be solved in existing work. The related work section missed some relevant recent work including Ref[1], in which the method is also gradient-based and can be applied to neural networks. The well-behaved notion used in Definition 2 seems much weaker than the assumptions shown in Ref[1,2] to guarantee existence or uniqueness of the Bayes classifier, the authors could spend some effort to discuss why they require less assumptions.

2.For the theory part, all the convergence results are proved in an asymptotic way without further discussion in the sample complexity. This becomes problematic for this work because (as shown in eq (7)) mini batch size goes to infinity is an unrealistic assumption in neural network training. Also when the class is unbalanced, empirical mean converging to population also slows down significantly which is required in Eq (4) and other places. I would like to see more discussion on the sample complexity either theoretically or experimentally.

3.The experiments lack details for reproducing the results or generalizing the gain to other problems. For example, batch size, learning rate or how the size of the network influence the performance metrics. This information will be useful for others who want to apply the proposed method.
 
There are some minor formatting issues like the leading space in \citep. Please fix those.

Based on the above reasons, I’ll give this paper a 5.

[Ref 1] Yan, B., Koyejo, S., Zhong, K. & Ravikumar, P.. (2018). Binary Classification with Karmic, Threshold-Quasi-Concave Metrics. Proceedings of the 35th International Conference on Machine Learning, in PMLR 80:5531-5540
[Ref 2] Narasimhan, H., Kar, P., & Jain, P. (2015, June). Optimizing non-decomposable performance measures: a tale of two classes. In International Conference on Machine Learning (pp. 199-208).","The sentiment of the review is moderately negative. The reviewer expresses several concerns about the originality, theoretical rigor, and experimental details of the paper. The sentiment score is -40 because while the reviewer acknowledges the relevance of the topic, they are not convinced by the contributions and have significant criticisms. The politeness score is 50 because the reviewer uses polite language, such as 'I have the following concerns' and 'I would like to see more discussion,' and provides constructive feedback without being rude or dismissive.",-40,50
"The paper proposes a Siamese network model for mapping the space of natural compounds to a latent representation space; furthermore, it utilizes this representation to compute a similarity score between an already known compound and a newly discovered one. 

Some comments:
- Comparison with LDA requires more details: have you used the same number of topics for both small and large datasets?  How are you training it? 
- Page 4 the definition of micro-averaging is missing: “Micro-averaging for precision and recall is formally defined as: “ 
- Figure 3 does not show a well defined clusters of various compound families because it’s using same color for different families in some cases (according to the caption of Figure 3). I wonder if you can somehow show for which compound families the colors have been repeated or maybe show fewer compound families. 

The problem of finding similar compounds to a novel compound from NMR spectra is an interesting applied problem; however, technical novelty of the paper is not significant. Given the level of technical novelty, I believe the paper is more suitable for a more applied conference/journal in the fields of chemistry or bioengineering.","The sentiment of the review is moderately positive as it acknowledges the interesting applied problem the paper addresses but notes that the technical novelty is not significant. The sentiment score is therefore 20. The politeness of the language is high, as the reviewer provides constructive feedback and suggestions without being dismissive or rude. The politeness score is 80.",20,80
"The proposed method is too simplistic, the model being succinctly described in less than one page with many errors in the given math expressions. Only the model is given. The optimization problem, as given in (1) is not explained. the authors need to stud the optimization problem, to derive its resolution, and to describe the obtained algorithm.

The authors’ main motivation is to “maps the input convolutional features from the same class into tight clusters. In such a space, the clusters become compact and well-separated, …”. However, in the proposed method is operating in this way. The model is a simple transformation, and nothing ensures the compactness of the feature space, neither the separability of the classes.

It is difficult to understand the “arm CNNs with radial basis feature transformation”. There are two figure in the paper that seek to show this modification of CNN, but this is not enough because nothing is said in the text, which makes these images difficult to understand. Moreover, the figures have notations different than those in the  main body, such as F_{l-1} as opposed to F_{i,j,K}.

What is the transformation to be learned ? Is it T as given in the text before (1), or P as given in (3). In (1), it seems that it is a mix of both, namely T* = argmin_P ! Moreover, it is written “To enforce T’s positive semi-definiteness, using the eigenvalue decomposition, it can be decomposed into T ′T”.  Decomposing T as T’T, means that T is very very special.

Equation (4) is not correct. The summation is on i, which is not in the expressions, but in the result with F_{i,j,K}.

With the exception of Tables 3 and 4, most experiments are on comparing the conventional versus the proposed method. The authors need to compare to other methods available in the literature on defense against adversarial attacks. Moreover, it is not clear why the author compare the proposed method to ADVT (adversarial training) in Table 4, and not in Table 3.

Some references are incomplete. For example, the second reference is missing the publication type, volume, … 
","The review is critical of the proposed method, pointing out several flaws and areas that need improvement. The sentiment is negative due to the numerous issues highlighted, such as the simplistic nature of the model, errors in mathematical expressions, lack of clarity in the optimization problem, and insufficient experimental comparisons. The language used is direct and somewhat blunt, but it is not overtly rude. The reviewer provides specific feedback and suggestions for improvement, which indicates a professional tone despite the critical content.",-70,-20
"The paper proposed an interesting algorithm and direction, which tries fill the gap of NN in tabular data learning. My concern is, given this is an empirical work,  the number of datasets used in evolution is a bit small. 

Also, xgboost was the winning algorithm for many competitions for tabular data, would be good to compare the NN with properly optimised xgboost. 

In chapter 2, related work. The authors state that ""tree-based models still yield two obvious shortages: (1) Hard to be integrated into complex end-to-end frameworks... (2) Hard to learn from streaming data. 

To me these two reasoning statements are not particularly convincing. One could also say:

NN models yield two obvious shortages: (1) Hard to be integrated into complex end-to-end frameworks... (2) Hard to learn from streaming data...

Actually, tree ensemble based algorithms, eg Hoeffding tree ensembles, are among the best performed algorithms for data streaming tasks.","The review starts with a positive note, acknowledging the interesting algorithm and direction proposed by the paper. However, it quickly moves to concerns about the empirical work and the number of datasets used, as well as the comparison with xgboost. The language used is polite and constructive, offering specific recommendations and alternative viewpoints without being dismissive or rude.",20,80
"This paper studied an extension of the Information Bottleneck Principle called Information Bottleneck Hierarchy (IBH).  The goal of IBH is to extract meaningful information from a Markov Chain. Then the authors studied case of the Gaussian linear dynamic and proposed an algorithm for computing the IBH. Then an experiment was conducted to show the usage of IBH to practical problems.

Generally I like the idea of extending Information Bottleneck to dynamic systems and I think the experiment is interesting. But I have some major questions to the paper and these questions are important about the principle you are proposing.

1. About Figure 1, there is a link between X_{k-1} and B_k, but there are no link between X_k and B_{k+1}. I understand what you said --- B_k needs to compress X_{k-1} and delivers information to B_{k+1}. My question is ---- Figure 1 can not be generated to a longer Markov Chain. It seems that the principle you proposed only works for 3 random variables X_{k-1}-X_k-X_{k+1}, which weaken the principle a lot. Please draw a longer Markov Chain like Figure 1 to illustrate your principle.

2.  About the \epsilon_{1,2,3} in formula (3). \epsilon_1 is claimed to bound the accuracy of the prediction of X_k by B_{k-1}, but where not B_{k-1} appear in the formula (actually B_{k-1} is not even in Figure 1)? \epsilon_3 is claimed to define the closeness of prediction of X_{k+1} by B_{k+1}, but why does I(X_{k-1},X_{k+1}) need to be small? In the ""Markov chains are considered"" before formula (3), there are some typos, for example, X_{k+1}-B_k-B_{k+1} seems not a Markov Chain. Also why you are bounding the difference of two mutual informations, but not take the absolute value (I think the difference you are considered are not guaranteed to be non-negative)? I think formula (3) is the key to understand the IBH principle, but it is not well illustrated for the readers to understand.

3. I understand that you can only derive an algorithm for Gaussian linear dynamic, since non-Gaussian case might be difficult and Gaussian linear dynamic might be good enough for modeling real random processes. But I wonder what is the physical or practical meaning for the matrices \Psi and \Delta? Why \Delta can be used to predict sentiment intensity in your experiment? It seems that \Delta carries the information from B_k to B_{k+1}, so it is only one-hop information and the sentiment intensity involves multi-hop information. How do you combine the different \Delta for different hops to predict sentiment intensity? These questions are not well illustrated in the paper.

So I think the paper can be accepted if the author can provide some more insightful illustrations, especially for Figure 1, formula (3) and the experiment. But overall I think the idea in this paper is interesting, if well illustrated.","The sentiment of the review is generally positive, as indicated by phrases like 'Generally I like the idea' and 'I think the experiment is interesting.' However, the reviewer also expresses significant concerns and requests for clarification, which tempers the overall positivity. Therefore, the sentiment score is 30. The politeness of the language is quite high, as the reviewer uses polite phrases such as 'I understand,' 'please,' and 'I think,' and provides constructive feedback without being harsh or dismissive. Thus, the politeness score is 80.",30,80
"The paper proposes a modification of GAIL (Ho & Ermon, 2016) to make use of non-expert data. The non-expert data is used by training a classifier to classify between roll-outs of the current policy, expert demonstrations and non-expert demonstrations. Similar to GAIL, the policy is iteratively updated using TRPO with a cost that is given by the log probability of predicting the policy. The use of non-expert data acts as regularization in order to learn better features similar to universum prescription (Zhang & LeCun, 2017).  

The paper is well-written and very clear. The general problem setting is interesting, but I think it is of rather little significance, because I do not see many clear applications. The evaluation focuses on simulated robots, however gathering non-expert data on real robots would be very expensive, so the approach would not make a lot of sense here (even if we replace TRPO a more sample efficient rl method). The paper mentions the game of Go, but learning a policy on such large state spaces is not feasible without major modification and significant computational effort. However, the paper also mentions autonomous driving, which might be a more convincing application, because we can have a lot of demonstrations that we do not want to label as expert trajectories. I think the paper would profit a lot from having an experiment where the importance of making use of non-expert data becomes evident.

The approach seems sound, although I think that we can not expect much benefit from using the unlabelled data in the proposed way. By not making any assumptions on the non-expert data, they do not carry any information about the objective; the information that they carry about the system dynamics is not exploited for the RL update. Instead, the use of non-expert data is restricted to learning better features for discriminating between the agent and the expert. However, non-expert data is typically not cheaper than policy roll-outs and better features could also be learned by using more samples from the policy. The experiments also show only slight benefits, especially when comparing the final performance (instead of total returns) and accounting for the additional system interactions needed for generating the non-expert data. To make the comparison fairer, we could consider using a few more system interactions (variable K in the paper) per iteration for standard GAIL, so that the total number of function evaluations would match those of M-GAIL after a certain number of iterations. Especially if K is appropriately tuned, it is not clear whether we could still show an advantage of M-GAIL. It would also be interesting to show, whether we can benefit from using the policy roll-outs of previous iterations as non-expert data for the current iteration (in the traditional IL setting where no non-expert data is available a priori).

The main weakness of the paper is, that the novelty seems marginal. Instead of doing binary classification with cross-entropy loss, we're doing three-class classification with cross-entropy loss and use it for binary classification (by throwing away the auxiliary logit for predicting the non-expert class). Did I miss any other difference to GAIL? We can argue whether the policy objective is different (to me, H_\phi of M-GAIL corresponds to the discriminator of GAIL and the objective is exactly the same), however, even if we call it a minor modification, we would have very little novelty in the approach. As the paper does also not compensate for this with very good results or thorough theoretical analysis, I think that the contribution is too minor.

I do not see the point of section 4.4. and the related appendix A2. For all I understand, it proves that when using lambda=0 (standard GAIL, right?), the proof of Fu et al. (2018) for GAIL is valid (i.e. we learn a completely useless reward function that does not carry any additional information compared to the policy), and when using lambda!=0 we learn something different. I don't see the the purpose of this statement and I don't think that it needs to be proven. The paper argues, that for small lambda we can treat the discriminator logits as approximations of these (completely useless) reward functions--without providing any bound. As I do not see why this would be useful, I think the section should be removed. 

Minor:
Typo: ""[...]due to its dependent[sic] on the linearity of reward functions and good feature engineering"" 
","The sentiment of the review is mixed. The reviewer acknowledges that the paper is well-written and clear, and finds the general problem setting interesting. However, they also express significant concerns about the practical significance, novelty, and the effectiveness of the proposed approach. The sentiment score is therefore moderately negative. The politeness of the language is high; the reviewer provides constructive criticism and suggestions for improvement without being rude or dismissive.",-30,80
"The paper proposes a method MixFeat for regularizing deep neural networks models, aiming at avoiding overfitting in training. The MixFeat interpolates, based on a careful selected mixing ratio, the hidden states (feature maps) of two randomly selected examples. Unlike MixUp, the MixFeat does not interpolate the labels of the two selected examples and the feature interpolation processes are conducted in the hidden space. Experiments on both Cifar10 and Cifar100 show that the networks with MixFeat improve their predictive accuracy as well as outperform networks with Mixup as regularizer.   

The paper is well written and easy to follow, and the experimental results on both Cifar10 and Cifar100 show promising results. Nevertheless, the idea of interpolating pairs of latent features for network regularization is not very novel. Additional, the experimental section is a bit weak in its current form. 

Main Remarks:

1.	MixFeat is very similar to Manifold-Mixup (Verma et al., Manifold Mixup: Learning Better Representations by Interpolating Hidden States), where both feature maps and labels of a pair of examples are mixed, so Manifold-Mixup would be a valid comparison baseline to MixFeat. In addition, the proposed method is similar to SMOTE (where features are mixed in the input space). In this sense, performance of SMOTE may be a useful comparison baseline as well.
2.	In the experimental section, the choice of parameter for Mixup seems arbitrary to me and may not be the optimal one. For example, for the Cifar10 and Cifar100 datasets, the original paper highlights that Alpha equals to one is a better choice to obtain better accuracy for ResNet. Also, as highlighted from AdaMixup (Guo et al., MixUp as Locally Linear Out-Of-Manifold Regularization), MixUp is quite sensitive to the choice of Alpha and suboptimal Alpha value easily leads to underfitting. 
3.	Some claims are not well justified. For example, the authors claim that MixFeat can reduce overfitting even with datasets with small sample size, but did not provide any training cost or errors in Figure6 to support that claim. 
4.	MixFeat is closely related to MixUp, and I would like to see more experiments with MixUp as baseline in terms of regularization effect. For example, it would be useful to include MixUp in Figures 4 and 6.

Minor remarks: 

1.	What were the parameters for MixFeat used for Table 1?
2.	Is the proposed method robust to adversarial examples as shown in MixUp and ManiFold-Mixup?
3.	How the incorrect labels are generated in Section 3.2.1 is not very clear to me.
4.	Since MixFeat is similar to Mixup, I wonder if MixFeat has the problem of “manifold intrusion” as suggested in AdaMixUp when generating samples from image pairs?  How sensitive is MixFeat to the parameters Theta and Pi? Would learning mixing policies as suggested by AdaMixUp make sense here?

============after rebuttal============

I really appreciate the authors' rebuttal, which has addressed some of my concerns.
Nevertheless, I agree with the other reviewers about the main weakness of the paper. That is, why the proposed method works and what are its advantages over similar strategies, such as Mixup, AdaMixup and Manifold Mixup, are not clear.","The sentiment of the review is moderately positive, as the reviewer acknowledges that the paper is well-written and the experimental results are promising. However, the reviewer also points out several weaknesses, such as the lack of novelty and the need for more robust experimental comparisons. Therefore, the sentiment score is 30. The politeness of the language is quite high, as the reviewer uses polite phrases like 'I really appreciate' and 'I would like to see,' and provides constructive feedback without being harsh or dismissive. Therefore, the politeness score is 80.",30,80
"This paper study the model-based approach in deterministic low dimensional continuous control. As far as I am concerned and I understood, the main contribution of this paper is in substituting one-step-ahead prediction model with a multiple-step prediction model, resulting in a more accurate prediction model. I was not able to find points beyond this. I would be happy if the authors could clarify it. ","The sentiment of the review appears to be neutral to slightly positive. The reviewer acknowledges the main contribution of the paper but also indicates a lack of clarity on additional points. Therefore, the sentiment score is 10. The politeness of the language is quite high; the reviewer uses phrases like 'As far as I am concerned' and 'I would be happy if the authors could clarify it,' which are polite and respectful. Thus, the politeness score is 80.",10,80
"This paper empirically studies the robustness of equivariant CNNs to rotations as well as adversarial perturbations. It also studies their sample efficiency, parameter efficiency, and the effect of rotation- and adversarial augmentation during training and/or testing. 

The main findings are:
1) Rotation-equivariant networks are robust to small rotations, even if equivariance to small rotations is not directly built into the architecture
2) Applying rotational data augmentation increases robustness to rotations
3) Equivariant networks are more sample efficient than CNNs and outperform them for all dataset sizes.
4) Applying rotational data augmentation decreases robustness to adversarial perturbations, and this effect is more pronounced for GCNNs.

If true, this is a valuable addition to the literature. It is one of the first independent validations of claims regarding sample complexity and accuracy made by the authors of the various equivariant network papers, performed by a party that does not have their own method to promote. Many of the findings do not have an obvious explanation, so the data from this paper could conceivably prompt new theoretical questions and investigations.

The authors chose to highlight one finding in particular, namely that GCNNs become more sensitive to adversarial perturbations as they are trained on more heavily rotation-augmented data. However, this appears to be true for both CNNs and GCNNs, the difference being only in degree (see fig 4, 10, 11). This is not apparent from the text though, as e.g. the abstract notes that ""robustness to geometric transformations in these models [equivariant nets] comes at the cost of robustness to small pixel-wise perturbations"".

Since HNets, GCNNs and RotEqNets should be exactly equivariant to 90 degree rotations (and some others, perhaps), it is surprising that figure 1 shows a continuing decline in performance with bigger and bigger random rotations. If the network is made rotation invariant through some pooling layer at the end of the network, one would expect to see a decline in performance up to 45 degrees, followed by an increase back to baseline at 90 degrees, etc. 

Polar transformer networks achieve good results in fig. 1, but I wonder if this is still true for rotations around points other than the origin.

Since CNNs and GCNNs differ in terms of the number of channels at a certain number of parameters, and differ in terms of number of parameters at a certain number of channels, it could be that channel count or parameter count is the more relevant factor, rather than equivariance. So it would be good to make a scatterplot where each dot is a network (either CNN or GCNN, at various model sizes), the x-axis is parameter count (or in another plot, 2d channel count), and the y-axis corresponds to the accuracy. This can be done for various choices of augmentation / perturbation. The type of network (CNN or GCNN) could be color coded. If indeed the CNN/GCNN variable is relevant, that should be clearly visible in the plot, and similarly if the parameter count or channel count is relevant. One could also do a linear regression of accuracy or log-accuracy or something using CNN/GCCN, param-count, channel-count as covariates, and report the variance explained by each. 

In several plots, e.g. fig 4, 8, the y-axes do not have the same range, making it hard to compare results between subplots. 

The experiments have some weaknesses. For one thing, it seems like each accuracy value reported comes from a single training run. It would be much preferable to plot mean and standard deviation / error bars. Another weakness is that all experiments are performed on MNIST. Even just a simple validation of the main findings on CIFAR would significantly strengthen the paper.

Because of the limited scope of the experiments, it is not clear to me how generalizable and robust the experimental results are. With deep network performance it can be hard to know what the relevant hyperparameters are, and so we may well be reading tea leaves here.

It is also unfortunate that no explanation for the observed phenomena is available. However, it is conceivable that the findings presented in this paper could help researchers who are trying to understand adversarial attacks / robustness, so it is not a fatal flaw. I am certainly glad the authors did not make up some unsupported story to explain the findings, as is all too common in the literature these days.

Overall, I consider this a borderline paper, and am tending towards a reject. My main considerations are:
1. Uncertainty about generalizability
2. Uncertainty about usefulness to practitioners or theorists (admittedly, this is hard to predict, but no clear use-case is available at this point)
3. A lot of data, but no clear central finding of the paper","The sentiment of the review is mixed, with both positive and negative elements. The reviewer acknowledges the potential value of the paper's findings and appreciates the empirical validation of claims made by other authors. However, the reviewer also expresses significant concerns about the generalizability, robustness, and practical usefulness of the results. The overall sentiment score is therefore slightly negative. The politeness of the language is high; the reviewer provides constructive criticism and suggestions for improvement without being rude or dismissive. The reviewer also acknowledges the effort of the authors and refrains from making unsupported negative claims.",-20,80
"The main contribution of this paper is that it shows both theoretically and empirically that in linear graph embeddings, the generalization error is bounded by the norms of the embedding vectors rather than the dimension of the embedding vectors. 

The list of my concerns or cons of this paper is:

- For the main theorem, i.e., Theorem 1, 
a.) why is it intuitive that the size of the training dataset required for learning a norm constrained graph embedding is O(C|A|_2). This is not that intuitive to me. Later, the authors argue that graphs are usually sparse and average node degree is usually smaller than the embedding size, thus it is easily overfitting the training data. However, I would say, in practice, the positive training pairs are not restricted to 1-hop neighbors, but could also be 2 or more hops, in that case, it won't easily overfit. 
b.) the main result from the theorem is that the error gap of norm constrained embeddings scales as O(d^-0.5(lnn)0.5), but I did not see how this is related to the norms of the embedding vectors and how is this evidenced in the empirical studies? It might be better to show a plot of error gap vs. d and/or n. 
c.) how is this analysis related to the later claim that \lambda_r controls the model capacity of linear graph embedding?

- The linear graph embedding framework considered in this paper assumes that each node only has one set of embeddings, but in practice, one node usually has two sets of embeddings as context node or a center node. How would this affect the whole analysis and claims?

- How would the claims or analysis in this paper be generalized to non-linear graph embedding frameworks?

- For the experiments, 
a.) In Figure 3, the y label of (b) is missing, and the Average L2 norm of (c) cannot reflect the Generalization performance 
b.) In Figure 4(a), why after overfitting, we can still observe that the test accuracy increases?
c.) In Figure 5, why the test precision first increases and then decrease with more regularization?","The sentiment of the review is moderately positive as it acknowledges the main contribution of the paper in the first sentence. However, the review quickly shifts to a detailed list of concerns and questions, indicating that the reviewer has significant reservations about the paper's claims and methodology. Therefore, the sentiment score is 20. The politeness of the language is quite high; the reviewer uses polite and constructive language, such as 'it might be better' and 'how would this affect,' rather than using harsh or dismissive terms. Therefore, the politeness score is 80.",20,80
"The paper studies fatigue monitoring of EEG driving simulator experiments using various EEG analysis algorithms, one also based on ranking. The data used was from a prior experiment. 

The paper is written in a rather confusing manner, which makes the assessment of originality and significance a hard task for the reviewer. A novel algorithm Bdrank (based on raking is defined) and compared to 2 other algorithms; unclear why with these and not with others. The paper ignores a large portion of the literature, starting with Kohlmorgen et al 2007, Blankertz group, Lee group etc. 
The results  are only somewhat interesting, no understanding of the underlying physiological processes is given. 

Overall, I consider the paper somewhat preliminary. ","The sentiment of the review is somewhat negative, as the reviewer points out several issues with the paper, including its confusing writing, lack of clarity on the choice of algorithms, and omission of relevant literature. The reviewer also describes the results as only 'somewhat interesting' and considers the paper 'somewhat preliminary.' These comments indicate a sentiment score of -60. The politeness of the language is relatively neutral to slightly polite. While the reviewer is critical, they do not use harsh or rude language. They provide specific feedback without being overly negative or dismissive, which suggests a politeness score of 20.",-60,20
"The paper proposes a iterative approach at inference time to improve object detections. The work relies on updating the feature activations and perform new feed forward passes to obtain improved results. 

Pros:
(+) The idea of iterative inference is potentially effective
(+) The paper is well written and clear
(+) The authors show results on compelling benchmarks
Cons:
(-) Reported improvements are very small
(-) Important baselines are missing


First, while the authors state correctly that their updates have no memory cost and no new parameters are added, they do require more FLOPs at test time. For N-stages, the approach requires xN more operations for forward passes  and xN for backward passes. This is a serious shortcoming as it adds compute time per image for the inference stage and cannot be parallelized. 

The authors show small improvements for AP on COCO. From their analysis, it seems that the biggest gains come from N=1 stages, while the improvement added for N>1 are miniscule (Table 1). Note that the authors show results on COCO minival (5k images) and from my experience there, it's expected to see a +/- 0.2% AP between different trained models of the same architecture. The authors report a +0.46% gain. 

In addition, the authors do not provide results for other baseline approaches that have similar FLOPs at test time, such as iterative bounding box regression and input scale augmentation. Note that both these approaches do not add any parameters and require no additional memory, but add to the FLOPs at test time. From my personal experience, test time augmentations can add +1.5% to the final performance. Concretely, look at Mask R-CNN arXiv Table 8 last two rows. Test time augmentations add 1.5% on top of an already enhanced model. Empirically, the better the model the harder it is to get gains from inference tricks! And still test time augmentations boost performance significantly.

Given the small gains and the lack of competing baselines, it is hard to make a case for accepting the paper. ","The sentiment of the review is mixed but leans towards the negative side. While the reviewer acknowledges some positive aspects of the paper, such as the potential effectiveness of the iterative inference idea and the clarity of the writing, they also highlight significant shortcomings. These include the small reported improvements, the lack of important baselines, and the increased computational cost. Therefore, the sentiment score is -40. The politeness of the language is generally respectful and constructive, providing specific feedback without being harsh or dismissive. Thus, the politeness score is 60.",-40,60
"This paper gives a new algorithm for learning parameters of neural network under several assumptions: 1. the threshold for the first layer is very high; 2. the future layers of the neural network can be approximated by a polynomial. 3. The input distribution is Gaussian.

It is unclear why any of these assumptions are true. For 1, the thresholds in neural networks are certainly not as high as required in the algorithm (for the threshold in the paper after the first layer the neurons will be super sparse/often even just equal to 0, this is not really observed in real neural networks). For 2, there are no general results showing neural networks can be effectively approximated by low degree polynomials, and, if the future networks can be approximated, what prevents you from just assuming the entire neural network is a low degree polynomial? People have tried fitting polynomials and that does not perform nearly as well as neural networks.

The proof of the paper makes the problem even more clear because the paper shows that with this high threshold in the first layer, the future layers just behave linearly. This is again very far from true in any real neural networks.

Overall I'm OK with making some strong assumptions in order to prove some results for neural networks - after all it is a very difficult problem. However, this paper makes too many unrealistic assumptions. It's OK to make one of these assumptions, maybe 2, but 3 is too much for me.","The sentiment of the review is largely negative, as the reviewer points out several critical flaws in the assumptions and the proof provided in the paper. The reviewer does acknowledge the difficulty of the problem and is open to some strong assumptions, but ultimately finds the paper's assumptions to be too unrealistic. Therefore, the sentiment score is -60. The politeness of the language is relatively high; the reviewer uses phrases like 'it is unclear,' 'I'm OK with,' and 'maybe 2, but 3 is too much for me,' which indicate a polite and constructive tone despite the critical feedback. Therefore, the politeness score is 60.",-60,60
"The submission explores Breiman's dilemma: training margin is not always a good predictor of test error.

In particular, the authors show that:

- For under-parametrized CNNs, the training prediction margin is a good predictor of the test error.
- For over-parametrized CNNs, the training prediction margin is not a good predictor of the test error.

Throughout the submission, I suspect that the authors compute the ""functional margin"", that is, the difference between the largest label score and the second largest score, for correctly classified examples. Functional margins ignore the smoothness of the underlying function, a critical factor for generalization. For instance, the function f(x) = 1[x > 0] has large functional margin, but any perturbation around the x-origin would drastically change the prediction. For this reason, I think the authors should consider the ""geometrical margin"" instead, which is unfortunately difficult to compute for general neural networks. Their theory tries to reflect on this issue by using spectrally-normalized bounds, but the practice ignores this issue completely (as far as I can tell).

Therefore, we may be looking at the wrong statistic to predict generalization error. Is Breiman dilemma solved by re-defining margin properly? Geometrical margin can be computed in closed-form for linear classifiers, so perhaps this would be a first step in this investigation.

","The sentiment of the review is moderately positive. The reviewer acknowledges the significance of the problem addressed by the authors and the contributions made, but also points out a critical flaw in their approach. This results in a sentiment score of 20. The politeness of the language is high; the reviewer uses polite and constructive language throughout, offering suggestions for improvement without being dismissive or harsh. This results in a politeness score of 80.",20,80
"[Summary]
This paper proposes an extension of the dual learning framework, with a guider network and multiple languages included: (1) Each language $i$ has a guider network $GN_i$, that can be used to reconstruct the source sentence from either the output of the encoder or the output of the decoder. (2) Multiple languages are used in this framework, where each language also has a $GN_i$  for guiding the training according to the reconstruction error. The authors work on MultiUN dataset to verify their algorithms.
 
[Clarity]
This work is not easy to follow. My suggestions to revise the paper are shown as follows:
(1) Please use the \begin{equation}…\end{equation} environment to clearly describe your framework and training objectives, with each notation, function and hyper-parameter clearly defined. Actually, I do not find the training objective function in this paper.
Besides, currently, in this paper, there are many undefined notations and typos, for example, (1) in section 3.1, first paragraph, what is the $n$? Then in Eqn.(1) ,what is $N$ and $M$? Also, it is very confusing to use subscripts $i$ and $j$ to distinguish the hidden states from the encoder and decoder. (2) What is the mathematical definition of $ISE_i$? (3) In page 5, 3rd line, “then ISD_i is used to reconstruct Si = GNi(ISE_i , \theta)…” Should the ISE_i be ISD_i?
(2) Please use \begin{algorithm}…\end{algorithm} to tell the readers how your framework works.
 
[Details]
1. The first question is “why this problem”. In the 3rd paragraph of page 1, you mentioned that “However, the best direction to update parameters heavily relies on the quality of sampled translations ... which may be far from real translations Y due to inaccurate translations existing in the sampled ones……” But in practice, dual learning as well as back-translation [ref1] works well for many language pairs. In particular, the dual learning and back-translation works for the unsupervised NMT [ref2], where no labeled data is available. Therefore, I am not fully convinced by this claim and then, the motivation of this work. What’s more, this paper does not work on standard WMT dataset, while previous dual learning and back-translation work on that most commonly used dataset. Therefore, the comparison between the guider network and dual learning are not fair.
2. I am not sure how the BDE in Eqn. (1) is related to the NMT translation quality. Any reference or theoretical/empirical proofs? 
3. It is hard to reproduce such a complex NMT system with NMT, GN and an RL scheduler. Any open-source code or any simple solutions?
4. Do you use a single-layer LSTM or a deep LSTM? Transformer [ref3] is the state-of-the-art NMT system. Why don’t you choose this system? Also, you do not work on WMT dataset to verify your GLF-2L (Table 1). Therefore, I cannot justify whether the proposed algorithm is efficient compared to the current NMT algorithms. I am not convinced by the experimental results.
5. The connection/difference between this work and (Tu et al 2017) should be discussed clearly, and you should implement (Tu et al 2017) as your baseline.  Besides, for the 3-language setting, no multilingual baseline is implemented.
 
[Pros & Cons]
(+) This paper tries to extend dual learning from word level to hidden state level;
(+) Multiple languages are involved in this framework;
(-) Experiments are not convincing; the models are weak; many important baselines are missing; no results on widely used WMT datasets;
(-) The paper is not easy to follow. (See [clarify] part for details);
(-) Training process is a little complex; not easy to implement;
 
References
[ref1] Edunov, Sergey, et al. ""Understanding back-translation at scale."" EMNLP 2018
[ref2] Lample, Guillaume, et al. ""Phrase-Based & Neural Unsupervised Machine Translation."" EMNLP 2018
[ref3] Vaswani, Ashish, et al. ""Attention is all you need."" Advances in Neural Information Processing Systems. 2017.
 ","The sentiment of the review is mixed but leans towards the negative side. The reviewer acknowledges some positive aspects of the paper, such as the attempt to extend dual learning and the inclusion of multiple languages. However, the majority of the feedback is critical, pointing out issues with clarity, experimental validation, and the complexity of the training process. Therefore, the sentiment score is -40. The politeness of the language is generally maintained throughout the review. The reviewer uses polite phrases like 'My suggestions to revise the paper are shown as follows' and 'I am not fully convinced by this claim,' which indicates a respectful tone despite the critical feedback. Thus, the politeness score is 60.",-40,60
"Strength: 

Intuition that multiple sources of uncertainty are relevant to adversarial examples 

Weaknesses:

Threat model is unclear
No adaptive adversaries are considered
Attack parameters could be better justified

The intuition presented at the beginning of Section 4 is interesting. There are indeed multiple sources of uncertainty in machine learning, and the softmax probability only captures confidence partially. In particular, estimating the support of training data for a particular prediction and the density of that support is conceptually relevant to understanding and mitigating outlier test points like adversarial examples. 

Given that the approach is motivated as a defense (see Section 7 for instance), it needs to be evaluated in a realistic adversarial setting. In particular, it would greatly strengthen the paper if a clear threat model was specified. In your rebuttal, would you be able to formulate clearly what adversarial capabilities and goals were assumed when designing this defense? 

All experiments are performed on a binary variant of CIFAR-10. In addition, all pairs chosen for the experiments are well-separated: dogs are semantically further apart from airplanes than they are from horses. Would you be able to clarify in your rebuttal how the approach would generalize to multi-class classification? 

Perhaps the strongest limitation of the evaluation is that it does not consider adaptive adversaries. This goes back to the threat model point raised previously. Adaptive strategies will be put forward by adversaries aware of the defense being deployed (security should not be obtained through obscurity). For instance, the adversary could modify their attack to have it minimize the difference between logits on the training and adversarial data. This would help evading detection by the proposed scheme. However, results from Section 6 are shown for attacks that do not attempt to reduce the L1 difference between adversarial and training data. 

Some attack parameters could also be better justified. The naming convention for the perturbation sizes reads a bit imprecise and is perhaps more confusing than it is informative. Furthermore, could you explain in your rebuttal why epsilon is larger than 1.0 for the FGSM---when the inputs where normalized between 0 and 1?

Details: 

Page 1: Typo in “defence”
Page 2: Notation s_i is overloaded multiple times making it difficult to parse expressions
Page 2: Citation to Kull et al. should use \citep after “Beta calibration”
Page 3: Citation to Rozsa et al. should use \citep after “PASS score”
Page 5: Generally-speaking, it’s best to compute attacks at the logit layer rather than the probabilities to avoid numerical instabilities, which can then lead to gradient masking. However, the following sentence suggests the opposite: “The attacks were all white-box attacks and performed on the network which included a final softmax layer in its structure.”
","The review starts by acknowledging the strength of the intuition behind the paper, which is a positive note. However, it quickly transitions to listing several weaknesses and limitations, indicating a critical stance. The sentiment score is thus slightly negative. The language used is constructive and polite, with suggestions for improvement and specific questions for clarification, indicating a high level of politeness.",-20,80
"Summary:
The paper introduces a new approach for interpreting deep neural networks called step-wise sensitivity analysis. The approach is conceptually quite simple and involves some interesting ideas, but I have some serious concerns whether the output produced by this method carries any meaning at all. If the authors were able to refute my concerns detailed below, I would raise my score substantially.


Strengths:
+ Potentially interesting heuristic to identify groups of feature channels in DNNs that encode image features in a distributed way


Weaknesses:
- Using the magnitude of the gradient in intermediate layers of ReLU networks is not indicative of importance
- No verification of the method on a simple toy example


Details:


Main issue: Magnitude of the gradient as a measure of importance.

I have trouble with the use of the gradient to identify ""outliers,"" which are deemed important. Comparing the magnitude of activations across features does not make sense in a convnet with ReLUs, because the scale of activations in each feature map is arbitrary and meaningless. Consider a feature map h^l[i,x,y,f] (l=layer, i=images, x/y=pixels, f=feature channels), convolution kernels w^l[x,y,k,f] (k=input channels, f=output channels) and biases b^l[f]:

h^l[i,:,:,f] = ReLU(b^l[f] + \sum_k h^(l-1)[i,:,:,k] * w^l[:,:,k,f])

Assume, without loss of generality, the feature map h^l[:,:,:,f] has mean zero and unit variance, computed over all images (i) in the training set and all pixels (x,y). Let's multiply all ""incoming"" convolution kernels w^l[:,:,:,f] and biases b^l[f] by 10. As a result, this feature map will now have a variance of 100 (over images and pixels). Additionally, let's divide all ""outgoing"" kernels w^(l+1)[:,:,f,:] by 10.

Simple linear algebra suffices to verify that the next layer's features h^(l+1) -- and therefore the entire network output -- are unaffected by this manipulation. However, the gradient of all units in this feature map is 10x as high as that of the original network. Of course the gradient in layer l-1 will be unaltered once we backpropagate through w^l, but because of the authors' selection of ""outlier"" units, their graph will look vastly different.

In other words, it is unclear to me how any method based on gradients should be able to meaningfully assign ""importance"" to entire feature maps. One could potentially start with the assumption of equal importance when averaged over all images in the dataset and normalize the activations. For instance, ReLU networks with batch norm and without post-normalization scaling would satisfy this assumption. However, for VGG-16 studied here, this is not the case.

On a related note, the authors' observation in Fig. 4b that the same features are both strongly positive and strongly negative outliers for the same class suggests that this feature simply has a higher variance than the others in the same layer and is therefore picked most of the time. Similarly, the fact that vastly different classes such as shark and German Sheppard share the same subgraphs speaks to the same potential issue.


Secondary issue: No verification of the method on simple, understandable toy example.

As shown by Kindermans et al. [1], gradient-based attribution methods fail to produce the correct result even for the simplest possible linear examples. The authors do not seem to be aware of this work (at least it's not cited), so I suggest they have a look and discuss the implications w.r.t. their own work. In addition, I think the authors should demonstrate on a simple, controlled (e.g. linear) toy example that their method works as expected before jumping to a deep neural network. I suppose the issue discussed above will also surface in purely linear multi-layer networks, where the intermediate layers (and their gradients) can be rescaled arbitrarily without changing the network's function.


References:
[1] Kindermans P-J, Schütt KT, Alber M, Müller K-R, Erhan D, Kim B, Dähne S (2017) Learning how to explain neural networks: PatternNet and PatternAttribution. arXiv:170505598. Available at: http://arxiv.org/abs/1705.05598","The sentiment of the review is mixed but leans towards the negative side. The reviewer acknowledges the potential of the approach but expresses serious concerns about its validity, which suggests a sentiment score of -40. The language used is polite and constructive, as the reviewer provides detailed feedback and suggestions for improvement without being dismissive or rude, which suggests a politeness score of 80.",-40,80
"The paper adresses the problem of incremental learning when data from new classes are available as a stream and one wants to be able to update to learn new observed classes without forgetting the older ones. There is a budget issue here and one does not want to just keep the whole training set of all known previously observed classes but rather one wants to consider a maximum memory budget allowed to store what is necessary for an optimal incremental learning (typical examples, statistics etc). There is also a privacy issue preventing from storing original training samples.

This is a relevant problem that is has gain interest in the last few years. It is related to topics such as few shot learning and meta few shot learning (with respect ti the number of examples per class that are kept, which is limited) and somehow to budget learning . Yet these topics and associated references are surprisingly not evoked in the text.

The paper is rather well written but it strongly lacks precision about the proposed method. A description of the ICARL state of the art method is missing and would have been mandatory since the proposed work appears to build on iCARL method. Actually the description of the method is very short since the dedicated section (§4) is mainly used to describe a rather standard convolutional auto encoder architecture. At the end one tries to guess what the proposed method consists in. As far as i understand it is based on iCARL method where selected examples of past observed classes are not stored as is but in their encoded form (by the convolutional autoencoder). At the end my understanding of the proposed approach is that it consists in an incremental progress of a state of the art method, then an incremental work with limited innovation.

By the way i am not sure of the meaning of pseudo exemplar as used in the proposed method. Are these drawn following a distribution computed on training samples ? Or are these pseudo exemplar because you use reconstructed samples from encodings (by the CAE).  

When looking at experimental results the proposed method seem to bring some benefit but it does not look fully convincing. As written in the paper the proposed system outperforms iCARL in case the examples are encoded in the same dimension as original examples (hence no benefit on the storage side) but reaches similar performance when using less storage capacity. ","The sentiment of the review is mixed. The reviewer acknowledges the relevance of the problem and the general quality of the writing but criticizes the lack of precision and innovation in the proposed method. The sentiment score is therefore slightly negative. The language used is polite, with no rude or harsh words, and the criticisms are presented in a constructive manner.",-20,80
"This paper demonstrates that CNNs are more robust to class-relevant label noise. They argue that real-world noise should be class-relevant.

Pros:

1. The authors find a new angle to exploit robust learning with noisy labels.

2. The authors perform numerical experiments to demonstrate the effectiveness of their proposal. And their experimental result support their previous claims.

Cons:

We have two questions in the following.

1. Basic definition: in learning with noisy labels, there are two basic models. First, most research focuses on class-conditional noise (CCN) model [1]. Second, recent research explore a bit on instance-dependent noise (IDN) model [2, 3]. As far as I know, there is no class-irrelevant label noise and class-relevevant label noise. In CCN mode, people would like to use symmetric noise and asymmetric noise as a basic benchmark to conduct experiments.

2. Motivation: The authors want to claim CNNs are more robust to such realistic label noise than class-irrelevant label noise. However, they make one mistake. They do not have a clear definition about realistic label noise. In my mind, I believe Clothing1M [4] should be realistic label noise dataset.

By the way, in learning with noisy labels, there are two kinds of research. First, people propose new robust methods for CCN model. Second, people propose new robust methods for IDN models. Proposing new setting should be encouraged. However, the setting and conclusion should be reasonable.

References:

[1] D. Angluin and P. Laird. Learning from noisy examples. Machine Learning, 1988.

[2] A. Menon, B. Rooyen, and N. Natarajan. Learning from binary labels with instance-dependent corruption. Machine Learning, 2018.

[3] J. Cheng, T. Liu, K. Ramamohanarao, D. Tao. Learning with bounded instance-and label-dependent label noise. arxiv 1709.03768, 2017.

[4] T. Xiao, T. Xia, Y. Yang, C. Huang, and X. Wang. Learning from massive noisy labeled data for image classification. In CVPR, 2015.","The sentiment of the review is generally positive, as the reviewer acknowledges the novelty and effectiveness of the authors' approach. However, the reviewer also raises significant concerns and questions about the definitions and motivations used in the paper. Therefore, the sentiment score is not entirely positive but leans towards the positive side. The language used in the review is polite and constructive, as the reviewer provides specific feedback and references to support their points without using harsh or rude language.",30,80
"The paper presents some interesting observations related to the connection between the universal adversarial attacks on CNNs and spectral properties. While most of the results are empirical, the authors present two theorems to justify some of the observations. However, the paper is poorly written and very hard to read. Rather than providing too many plots/results in the main paper (maybe use supplementary matl.), the empirical results should be better explained to help the readers. Similarly, the implications of the theorems are not really clear and bit hand-wavy.    

xxxxxxxxxxxxxx

It seems that the authors provided a generic response to all the reviewers and I am not sure if they acknowledge the lack of clarity and lot of hand-wavy explanations in the paper. This issue has been raised by other reviewers too and is quite critical for becoming a good paper worthy for ICLR. Therefore, I am unable to update my score for this paper. However, I do appreciate the comparison with Moosavi-Dezfooli et al. (CVPR'17), this is a good addition as suggested by another reviewer. ","The sentiment score is derived from the overall tone of the review, which acknowledges some positive aspects such as interesting observations and the addition of a comparison with Moosavi-Dezfooli et al. However, it also highlights significant negative aspects, including poor writing, lack of clarity, and hand-wavy explanations. This mixed but predominantly critical tone results in a sentiment score of -40. The politeness score is based on the language used, which, while critical, remains professional and constructive. The reviewer uses phrases like 'I do appreciate' and 'this is a good addition,' which indicate a polite tone. Therefore, the politeness score is 40.",-40,40
"This paper presents a method for predicting future frames of a video (or unseen views of a 3D scene) in a ""jumpy"" way (you can query arbitrary viewpoints or timesteps) and ""consistent"" way (when you sample different views, the scene will be consistent). They use a VAE that encodes the input video in a permutation invariant way, which is achieved by summing the per-frame latent vectors. Then, they sample a latent vector using a DRAW prior. This latent vector can then be used to render the video/scene from different times/viewpoints via an LSTM decoder. They test the model on several toy datasets: they compare to video prediction methods on a dataset of moving shapes, and 3D viewpoint prediction on a 3D MNIST ""dice"" dataset.

Pros:
- The idea of developing new methods for viewpoint and video synthesis that allow for ""jumpy"" and ""consistent"" predictions is an important problem.

- The paper is fairly well written.

- The design of the model is reasonable (it is a natural extension of VAE viewpoint/future prediction methods).

Cons:
- All of the experiments were done on toy datasets. These are also not well-established toy datasets, and seem tailored to debugging the model, so it is not particularly surprising that the method worked. Since the main contribution is not very novel from a technical perspective (it is more about designing a model from existing, well-established components), this is a significant limitation. 

- The paper suggests with experiments that GQN generates predictions that are less consistent across samples, but it is not clear exactly which design decisions lead to this difference. Why is this model more jumpy and consistent than GQN?

- The paper claims that JUMP trains more reliably than several video prediction methods in Figure 5. Yet, in the 3D viewpoint synthesis task, they suggest that JUMP had trouble with convergence, i.e.: ""We ran 7 runs for each model, and picked the best 6/7 runs for each model (1 run for JUMP failed to converge)."" This is confusing for two reasons. First, why was this evaluation protocol chosen (i.e. running 7 times and picking the 6 best)? If it was a post-hoc decision to remove one training run, then this should be clarified, and the experiment should be redesigned and rerun. Second, is the implication that JUMP is more stable than video prediction methods, but not necessarily more stable than GQN for viewpoint prediction?

- The paper should consider citing older representation learning work that deals with synthesizing images from multiple viewpoints. For example: 

M. Tatarchenko, A. Dosovitskiy, T. Brox. ""Multi-view 3D Models from Single Images with a Convolutional Network"". ECCV 2016.

- There is insufficient explanation of the BADJ baseline. What architectural changes are different?

- The decision to use DRAW, instead of a normal VAE prior, is unusual and not explained in much detail. Why does this improve the visual fidelity of the samples?

Overall:
The paper does not present enough evidence that this model is better at jumpy/consistent predictions than other approaches. It is evaluated only on toy datasets: if the technical approach were more novel (and if it was clearer where the performance gains are coming from) then this could be OK, but it seems to be a fairly straightforward extension of existing models. ","The sentiment of the review is mixed but leans slightly positive. The reviewer acknowledges the importance of the problem addressed by the paper and appreciates the clarity of writing and the reasonable design of the model. However, the reviewer also points out several significant limitations, such as the use of toy datasets, lack of novelty, and insufficient explanations for certain design choices. Therefore, the sentiment score is 10. The politeness of the language is quite high; the reviewer provides constructive criticism and suggestions for improvement without being rude or dismissive. The language is respectful and professional, so the politeness score is 80.",10,80
"The paper uses a number of deep learning approaches to analyse sets of Traffic data. However, as these sets of traffic data are never explained it is difficult to follow or understand what is going on here.

Some major comments:
1) Many of the key concepts in the paper are not discussed. The primary one would be that of what the two data sets contain. Without knowledge of this it is difficult to ascertain what is going on. 

2) Many of the processes used are not described in enough detail to either understand what is going on or to re-produce the work. Without this it is difficult to make headway wit the work.

3) It is not clearly articulated what the experiments performed are doing. For example, how have you applied the other techniques to this data?

4) Key terms are not defined. Such as Traffic Flow.

5) The English structure of the paper is poor with many mistakes. A thorough proof-reading is essential.

Some more specific points:
- ""with the larger road network, the difficulty of flow forecasting grows."" - This seems to be a consequence of the other ones not a challenge in it's own right.

- What is ""superiority""?

- ""Spatiotemporal traffic flow forecasting task is currently under a heated discussion and has attracted a large research population."" - evidence to back up this statement.

- Your contributions aren't contributions, but rather a list of what you have done.

- How does your related work relate to what you have done?

- Hard to parse ""To extract temporal relationships within the history traffic flows, we model this process as a layering structure with autoencoder as cell""

- Appendices B and C should be in the main paper.

- What is in x^{(1)}?

- ""When take the sparsity constrains into consideration"" - what are the sparsity constraints?

- How do you obtain the weights?

- Figure 2 should come much sooner as it relates a lot of the concepts together.

- ""On both datasets, we slice traffic flow information into 15 minutes windows, where 70% of data is for training, 10% for validation and remaining 20% for testing."" - Is that each 15 mins is split 70:10:20?

- Proof by example is not a proof.
","The sentiment of the review is largely negative, as it highlights numerous significant issues with the paper, such as the lack of explanation of key concepts, insufficient detail in the processes used, unclear articulation of experiments, and poor English structure. The reviewer does not provide any positive feedback or commendations. Therefore, the sentiment score is -80. The politeness of the language used is relatively neutral. While the reviewer points out many flaws, they do so in a straightforward manner without using derogatory or overly harsh language. Thus, the politeness score is 0.",-80,0
"This paper provides a method to do explicit IB functional estimation for deep neural networks inspired from the recent mutual information estimation method (MINE).  By using the method, the authors 1) validate the IB theory of deep nets using weight decay, and 2) provides a layer-wise explicit IB functional training for DNN which is shown to have better prediction accuracy.

Pros:
- The paper carefully constructs a method to estimate the mutual information between high dimensional variables and address the infinite mutual information issue by adding noise to the output. This is novel and theoretical sounding. 
- The paper connects the IB theory of DNN with weight decay, which is a novel founding.

Cons:
- The paper claims no literature has been doing IB functional on a layer-by-layer objective, however, see [1, 2] for the total correlation explanation work which is closely related to IB functional and they have also verified the effectiveness of layer-by-layer objective. 
- The scope of the paper is unclear. It seems that the paper is trying to convince two things to the readers: 1) The compression phase in DNN does exist 2) Layer-wise training helps to improve the accuracy. Although these two things are close related to each other (because they all requires to estimate the IB functional), it seems that neither these two conclusions are convincing. First, the compression phase is achieved only through weight decay; without weight decay, as shown in the paper, the compression phase is gone. Does that verify the incorrectness of IB theory of deep nets? Second, for the layer-wise training, the paper only compares the layer-wise IB objective with the cross entropy loss. But if we really want to show the `effectiveness` of `layer-wise` training, one should compare the `layer-wise` training with `end-to-end` training while keeping the objective itself fixed. Otherwise, it is really difficult to draw conclusions about why the accuracy is improving, it is because of the objective changes or because of the `layer-wise` training.
- How does the beta (in IB objective) selected in the experiments for comparison? Do you use a validation dataset, and what is the final beta? If the paper fine-tune beta on the validation dateset, then the comparison of ""IB functional, only the first term"" and ""IB functional"" is unfair. 

[1]  Ver Steeg et al. Maximally Informative Hierarchical Representations of High-Dimensional Data. AISTATS 2015
[2]  Gao et al. Auto-Encoding Total Correlation Explanation. Arxiv 1802.05822.

[update] After carefully reading the response (also from other reviewers), I decide not to change my rating.","The sentiment of the review appears to be mixed, with both positive and negative aspects highlighted. The reviewer acknowledges the novelty and theoretical soundness of the method, as well as the connection between IB theory and weight decay. However, the reviewer also points out significant concerns regarding the scope of the paper, the validity of the conclusions, and the fairness of the comparisons made. Therefore, the sentiment score is slightly negative. The language used in the review is polite and constructive, providing specific feedback and suggestions for improvement without being rude or dismissive.",-20,80
"This paper proposes a joint embedding model that aligns video sequences with sentences describing the context (caption) in a shared embedding space. With the space, various tasks such as zero-shot activity recognition and unseen video captioning can be performed. The problem tackled in this paper is interesting. However, the approach proposed is limited in novelty and there are some serious flaws in the experimental settings. So overall, this paper is not yet ready for publication. 

Pros:

•	The overall bidirectional encoder-decoder architecture for learning a shared embedding space is sensible. It is also interesting that adversarial training is introduced so that unlabelled data can be utilized. 
•	Additional annotations are provided to two activity recognition datasets, creating new benchmarks.
Cons
•	My first concern is the limited novelty of the work. Although I am not aware of a joint embedding learning model that has exactly the same architecture and formulation, the model is closely related to many existing ones both in zero-shot learning and beyond. More specifically,
o	The overall framework is similar to “correlational neural networks”, Neural Computation, 2016 by Chandar et al. This should be acknowledged.
o	The connections to CyclyGan and its variants for image-to-image style transfer is obvious, as pointed out by the authors.
o	More importantly, there are quite a few closely related zero-shot learning (ZSL) papers published recently. Although they focus on static images and class name, rather that image sequences and sentences, I don’t see any reason why these models cannot be applied to solve the same problem tackled in this paper. In particular, the autoencoder architecture was first used in ZSL in E. Kodirov, T. Xiang and S. Gong, ""Semantic Autoencoder for Zero-Shot Learning"", in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Honolulu, Hawaii, July 2017. This work is further extended in Chen et al, “Zero-Shot Visual Recognition using Semantics-Preserving Adversarial Embedding Network”, cvpr18, now with adversarial learning. Similarly, variational autoencoder is adopted in Wang et al, Zero-Shot Learning via Class-Conditioned Deep Generative Models, AAAI 2018.  Note that the joint embedding spaces in these studies are the semantic spaces – attribute or word vector spaces representing the classes. In contrast, since the semantic modality is a variable-length word sequences, this is not possible, so a third space (other than the visual feature space or semantic space) is used as the embedding space. Beyond these autoencoder based models, there are also a number of recent ZSL works that use a conditional generative model with adversarial loss. Instead of learning a joint embedding space where the visual and text modalities are aligned and compared for recognition, these works use the text modality as condition to the generative model to synthesize visual features for the unseen classes followed by a conventional supervised classifier. The representative one of this line of work is Xian et al, “Feature Generating Networks for Zero-Shot Learning, cvpr18”.
o	In summary, there are too many existing works that are similar to the proposed one in one or more aspects. The authors failed to acknowledge most of them; moreover, it is not argued theoretically or demonstrated empirically, why combining different approaches together is necessary/making fundamental differences.

•	My second main concern is the experiment setting. This paper adopts a conventional ZSL setting in two aspects: (1) the visual features are obtained by a video CNN, I3D, which is pretrained on the large (400 or 600 classes depending on which version is used) Kinetics dataset. This dataset have classes overlapping with those in ActivityNet, HMDB and UCF101. So if these overlapped classes are used in the unseen class partition, then the ZSL assumption (the target classes are ‘unseen’) is violated. (2): The test data only contains unseen class samples. In practice, one will face a test set composed of a mix of seen and unseen classes. Under this more realistic setting (termed generalized ZSL in the ZSL community), a ZSL must avoid the bias towards the seen classes which provide the only visual data available during training. The two problems have been identified in the ZSl community when static images are considered. As a result, the conventional setting has been largely abandoned in the last two years and the ‘pure’ and ‘generalized’ settings become the norm; that is, there is no overlapping classes between the test classes and classes used to pretrain the visual feature extraction network; and both seen and unseen class samples are used for testing. The ZSL evaluation is only meaningful under this more rigorous and realistic setting. In summary, the experimental results presented in this paper are obtained under the wrong setting and the proposed model is not compared with a number of closely related ZSL models listed above, so it is not possible to judge how effective the proposed model is. 
","The sentiment of the review is mixed but leans towards negative. The reviewer acknowledges some positive aspects of the paper, such as the sensible architecture and the introduction of adversarial training. However, the reviewer also points out significant concerns regarding the novelty of the work and flaws in the experimental settings. Therefore, the sentiment score is -40. The politeness of the language is quite high. The reviewer uses polite and professional language throughout the review, even when pointing out flaws and concerns. Therefore, the politeness score is 80.",-40,80
"This paper investigates a number of techniques and neural network architectures for embedded acoustic modeling.  The goal is to reduce the memory access and make efficient computation, in the meantime, to sustain good ASR performance.  Overall, the paper is well motivated and well written.  However, I have following concerns.

1. It is not clear from the paper whether both the training and inference are conducted on embedded devices or only the inference?  I assume it is the latter but can't find it explicitly mentioned in the paper.  

2. The exploration carried out in the paper is more on the system level and the novelty is not overwhelmingly significant.

3. My major concern is that the reported WERs on WSJ and phoneme classification accuracy are quite off.  20%-30% WERs for WSJ  do not seem to be usable in real applications.  Honestly, I don't even think this performance is better than well-trained GMM-HMM acoustic models using a Viterbi decoder.  Furthermore, there is no clear winners across the investigated architectures  in terms of performance.  One question is if one wants to deploy such an on-device system, which architecture shall be chosen?  

4. A more general comment on the work explored  in the paper.  First of all, the on-device memory issue puts a heavy constraint on the capacity of acoustic models, which will significantly hurt the modeling capability for the DNN-based acoustic models.  Deep learning acoustic models can outperform GMM-HMM because they can use large model capacity with very deep and complex architectures when a large amount of training data is available.  Second, for CTC, when the training data is limited,  its performance is far worse than the hybrid DNN-HMM model, let alone a pure end-to-end fashion without using external LM and dictionary.  If WFST-based decoders (composition of WFSTs of LM, dictionary and deblank/repetition) are used, then the memory issue will surface again. 
","The sentiment of the review is mixed. The reviewer acknowledges that the paper is well-motivated and well-written, which is positive. However, the reviewer also expresses several significant concerns about the clarity, novelty, and performance of the work, which are negative points. Therefore, the sentiment score is slightly positive but not overwhelmingly so. The politeness of the language is quite high. The reviewer uses polite phrases such as 'I have following concerns,' 'It is not clear,' and 'A more general comment,' which indicate a respectful tone despite the critical feedback.",20,80
"The paper introduces a relatively simple method for imitation learning that seems to be successful despite its simplicity. The method, SQIL, assigns a constant positive reward (r) to the demonstrations and zero reward to generated trajectories. While I like the connections between SQIL and SQL and the simplicity of the idea, I think there are several issues which connections with GAIL that are not discussed; some ""standard"" environments (such as Mujoco) that SQIL has not compared against the baselines. I believe the paper would have a bigger impact after some addressing some of the issues.

(
Update: I am glad that the authors added updates to the experiments. I think the method could be practical due to the simplicity, therefore of interest to ICLR.

The Pong case is also quite interesting, although it seems slightly ""unfair"" since the true reward of Pong is also sparse and DQN could do well on it. I think the problem with GAIL is that the reward could be hard to learn in high-dimensional cases, so it is hard to find good hyperparameters for GAIL on the Pong case. This shows some potential of the idea behind using simple rewards.
)

1. The first issue is the similarity with GAIL in the ""realistic"" setting. Since we cannot have infinite expert demonstrations, there would always be some large enough network that could perfectly distinguish the demonstrations (assign reward to 1) and the generated policies (assign reward to 0). Therefore, it would seem to me that from this perspective SQIL is an instance of GAIL where the discriminator is powerful and expert demos are finite (and disjoint from generated trajectories, which is almost always the case for continuous control). In the finite capacity case, I am unsure whether the V and Q networks in SQIL does a similar job as the discriminator in GAIL / AIRL type algorithms, since both seem to extrapolate between demonstrations and generations?

2. Moreover, I don't think SQIL would always recover the expert policy even with infinite demonstrations. For example, lets think about the Reacher environment, where the agent controls a robotic arm to reach a target location. The expert demonstration is the fastest way of reaching the target (move minimum distance between joints). If we consider the MDP to have possibly very large / infinite horizon (only stops when it reaches the target), I could construct a hack policy that produces larger episodic reward compared to the expert. The policy would simply move back and forth between two expert demonstrated states, where it would receive 1 reward in the states for odd time and 0 reward for the states for even time. The reward would be something like 1 / (1 - \gamma^2) compared to the experts' reward which is \sum_{i=0..T} \gamma^{i} = (1 - \gamma^{T+1}) / (1 - \gamma). 

Some fix would be to set the reward for generated policies to be negative, or introduce some absorbing state where the expert will still receive the positive reward even after reaching the target (but that is not included in demonstrations). Nevertheless, a suitable reward prior seems to be crucial to the success of this SQIL, as with GAIL requiring reward augmentation.

3. Despite the above issues, I think this could be a very practical method due to its (perhaps surprising) simplicity compared to GAIL. However, the experiments only considered two environments that are not typically considered by GAIL; I believe SQIL would make a bigger impact if it is compared with GAIL in Mujoco environments -- seems not very implementation heavy because your code is based on OpenAI baselines anyway. Mujoco with image inputs would also be relevant (see ACKTR paper).

Minor points:
- What is the underlying RL algorithm for GAIL? It would seem weird if you use Q-learning for SQIL and TRPO for GAIL, which makes it impossible to identify whether Q-learning or SQIL contributed more to the performance. While GAIL used TRPO in the original paper, it would be relatively straightforward to come up with some version of GAIL that uses Q-learning. 
- Some more details in background for MaxEnt RL to make the paper more self contained.
- More details about the hyperparameters of SQIL in experiments -- e.g. what is \lambda?
- Did you pretrain the SQIL / GAIL policies? Either case, it would be important to mention that and be fair in terms of the comparison.
- Why does SQIL-11 perform worse than SQIL even though it is a tighter bound?
- wrt. math -- I think the anonymous comment addressed some of my concerns, but I have not read the updated version so cannot be sure.","The sentiment of the review is generally positive, as the reviewer acknowledges the simplicity and potential practicality of the method, despite pointing out several issues and areas for improvement. The sentiment score is therefore 40. The politeness of the language is quite high, as the reviewer uses polite and constructive language throughout the review, offering suggestions for improvement without being dismissive or rude. The politeness score is 80.",40,80
"TL;DR. a generalization of the mixup algorithm to any layer, improving generalization abilities.

* Summary

The manuscript generalizes the mixup algorithm (Zhang et al., 2017) which proposed to interpolate between inputs to yield better generalization. The present manuscript addresses a fairly more general setting as the mixup may occur at *any* layer of the network, not just the input layer. Once a layer is chosen, mixup occurs with a random proportion $\lambda\in (0,1)$ (sampled from a $\mathrm{Beta}(\alpha,\alpha)$ distribution).

A salient asset of the manuscript is that it avoids a pitfall of the original mixup algorithm: interpolating between inputs may result in underfitting (if inputs are far from each others: the interpolation may overlap with existing inputs). Interpolating deep layers of the networks makes it less prone to this phenomenon.

A sufficient condition for Manifold Mixup to avoid this underfitting phenomenon is that the dimension of the hidden layer exceeds the number of classes.

I found no flaw in the (two) proofs. Literature is well acknowledged. In my opinion, a clear accept.

* Major remarks

- There is little discussion in the manuscript about which layers should be eligible to mixup and how such layers get picked up by the algorithm. I would suggest elaborating on this.
- References: several preprints cited in the manuscript are in fact long-published. I strongly feel proper credit should be given to authors by replacing outdated preprints with correct citations.
- I find the manifold mixup idea to be closely related to several lines of work for generalization abilities in machine learning (not just for deep neural networks). In particular, I would like to read the authors' opinion on possible connection to the vicinal risk minimization (VRM) framework, in which training data is perturbed before learning, to improve generalization (see, among other references, Chapelle et al., 2000). I feel it would help improve supporting the case of the manuscript and reach a broader community.

* Minor issues

- Tables 1 and 3: no confidence interval / standard deviation provided, diminishing the usefulness of those tables.
- Footnote, page 4: I would suggest to add a reference to the consistency theorem, to improve readability.","The sentiment of the review is highly positive, as indicated by phrases like 'a clear accept' and 'I found no flaw in the (two) proofs.' The reviewer appreciates the manuscript's contribution and acknowledges its strengths. Therefore, the sentiment score is 90. The politeness of the language is also very high. The reviewer uses polite suggestions such as 'I would suggest elaborating on this' and 'I strongly feel proper credit should be given,' which indicates a respectful and constructive tone. Thus, the politeness score is 95.",90,95
"This paper introduces a ""behavior module"" (BM) which is a small network that encodes preferences over actions and runs parallel to the fully connected layers of a policy. The paper shows this architecture working in Atari games, where the same policy can be used to achieve different action preferences over a game while still playing well. It also includes a thorough recap of past modular approaches. 

The motivation for the BM is that we may want deep networks to be able to decompose ""strategy"" and ""behavior"", where behavior may influence decisions without affecting the performance. In this framework, the BM is trained on a reward of correctness + personalized “satisfaction”.

The experiments model behavior as preferences over how many actions to play simultaneously. The trained BMs can be transferred to new tasks without finetuning. The ideas here also have some similarity to the few shot learning literature.

Comments on the experiments:
1. The Table 2  do not show a smooth interpolation between reward scaling and AMSR vs BD. This is surprising because the performance on the game should be highest went it is weighted to the most. This indicates to me that the results are actually high variance, the 0.8 vs 0.88 in stage 2 of 0.25r vs 0.5r means that is probably at least +/- 0.08 standard deviation. Adding standard deviations to these numbers is important for scientific interpretability.
2. I expect some BMs should perform much better than others (as they have been defined by number of actions to play at once). I would like to see (maybe in the appendix) a table similar to table 2 for for individual BMs. I currently assume the numbers are averaged over all BMs.
3. Similarly, I would like to see the BD for BM0 (e.g., if a policy is not optimized for any behavior, how close does it get to the other behaviors on average). This is an important lower bound that we can compare the other BD to. 
4. An obvious baseline missing is to directly weight the Q values of the action outputs  (instead of having an additional network)  by the designed behavior rewards. There is an optimal way to do this because of experimental choices. 

Questions:
1.For BM2, you write "" Up and Down (or Right and Left)"" did you mean ""Up and Right""? How can Up and Down be played at the same time?

Overall, this paper uses neuroscience to motivate a behavior module. However, the particular application and problem settings falls short of these abstract ""behaviors"". Currently, the results are just showing that RL optimizes whatever reward function is provided, and that architectural decomposition allows for transfer, which was already showed in (Devin 2017). An experiment which would better highlight the behavior part of the BM architecture is the following:
1. Collect datasets of demonstrations (e.g. on atari) from different humans.
2. Train a policy to accomplish the task (with RL)
3. Train BMs on each human to accomplish the task in the style of each human.
This would show that the BMs can capture actual behavior. 

The dialog examples discussed in the abstract would also be very exciting.

In conclusion, I find the idea interesting, but the experiments do not show that this architecture can do anything new. The abstract and introduction discuss applications that would be much more convincing. I hope to see experiments with a more complex definition of ""behavior"" that cannot be handcoded into the Q function.","The sentiment of the review is mixed. The reviewer acknowledges the interesting idea and the thorough recap of past modular approaches, but also points out significant shortcomings in the experiments and their ability to demonstrate the novelty of the architecture. Therefore, the sentiment score is slightly positive at 20. The politeness of the language is high; the reviewer uses polite and constructive language throughout the review, providing specific recommendations and questions without being dismissive or rude. Thus, the politeness score is 80.",20,80
"The paper aims to address issues with Domain Adaptation by using RL approaches. Domain Adaptation is an actively studied area in NLP research and so this paper is relevant and timely. This paper proposes and algorithm that is in line with work that aims at selecting data smartly when performing Domain Adaptation. The proposed algorithm learns representations for text in the source and target domains jointly. The proposed algorithm has two components i) a selection distribution generator (SDG) and ii) a task specific prediction for tasks being POS tagging, Dependency parsing and Sentiment Analysis.

While the proposed algorithm is interesting from a RL perspective and make sense, there is no explanation provided as to why this algorithm should do better over non RL based approaches for tasks such as Sentiment Analysis.

Domain Adaptation is widely studied for Sentiment Analysis and a lot of current research focuses on the various aspects of domain data, such as word and sentence level semantics, when developing algorithms. For example the following papers all (saving the third) address the problem of Domain Adaptation for Sentiment Analysis through various approaches fairly similar to the authors' algorithm, that provide similar if not better results than those provided in the paper,

[1]Barnes, Jeremy, Roman Klinger, and Sabine Schulte im Walde. ""Projecting Embeddings for Domain Adaption: Joint Modeling of Sentiment Analysis in Diverse Domains."" arXiv preprint arXiv:1806.04381 (2018).
[2] Ziser, Yftah, and Roi Reichart. ""Pivot Based Language Modeling for Improved Neural Domain Adaptation."" In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), vol. 1, pp. 1241-1251. 2018.
[3]An, Jisun, Haewoon Kwak, and Yong-Yeol Ahn. ""SemAxis: A Lightweight Framework to Characterize Domain-Specific Word Semantics Beyond Sentiment."" arXiv preprint arXiv:1806.05521 (2018).

Particularly the second paper is a clear improvement over SCL (the earlier pivot based approach), a baseline that is considered by the authors in this work. There are no comparisons against this work in this paper, yet the authors compare against SCL alone.

Due to lack of comparisons against state-of-the-art in Sentiment Analysis/Domain Adaptation for Sentiment Analysis it is hard to accept the claims made by the authors on the superiority of their algorithm. Had their paper aimed at improving over other RL based approaches for Domain Adaptation for Sentiment Analysis, some experiments could be over looked. 

But, when making a claim that addresses the problem of Sentiment Analysis, comparisons against the state-of-the-art non RL based approaches is extremely important. Particularly, given the size of the data sets used, one could use lexical/dictionary based approaches [3] and improve upon the classification accuracies without having to train such an involved algorithm.

Furthermore there is no qualitative analysis provided to gain insights into the behavior of the embeddings spaces of the target and source domains that are learned jointly via the proposed algorithm. At least such an analysis would have provided some insight into why the authors' RL based solution is better than a non RL based solution.

The lack of reference or comparisons against relevant literature is future highlighted by the seemingly relevant, yet largely dated related works section.
","The sentiment of the review is mixed. The reviewer acknowledges the relevance and timeliness of the paper, as well as the interesting nature of the proposed algorithm from an RL perspective. However, the review also highlights significant shortcomings, such as the lack of comparisons with state-of-the-art approaches and the absence of qualitative analysis. Therefore, the sentiment score is slightly negative. The language used in the review is polite and constructive, offering specific recommendations and citing relevant literature without being dismissive or rude.",-20,80
"Edited: I raised the score by 1 point after the authors revised the paper significantly.

--------------------------------------------

This paper proposes a regularization approach for improving GCN when the training examples are very few. The regularization is the reconstruction loss of the node features under an autoencoder. The encoder is the usual GCN whereas the decoder is a transpose version of it.

The approach is reasonable because the unsupervised loss restrains GCN from being overfitted with very few unknown labels. However, this paper appears to be rushed in the last minute and more work is needed before it reaches an acceptable level.

1. Theorem 1 is dubious and the proof is not mathematical. The result is derived based on the ignorance of the nonlinearities of the network. The authors hide the assumption of linearity in the proof rather than stating it in the theorem. Moreover, the justification of why activation functions can be ignored is handwavy and not mathematical.

2. In Section 2.2 the authors write ""... framework is shown in Figure X"" without even showing the figure.

3. The current experimental results may be strengthened, based on Figures 1 and 2, through showing the accuracy distribution of GAT as well and thoroughly discussing the results.

4. There are numerous grammatical errors throughout the paper. Casual reading catches these typos: ""vertices which satisfies"", ""makes W be affected"", ""the some strong baseline methods"", ""a set research papers"", and ""in align with"". The authors are suggested to do a thorough proofreading.

","The sentiment of the review is moderately negative. The reviewer acknowledges the reasonableness of the approach but criticizes the paper for appearing rushed and needing more work. Specific issues are pointed out, such as the dubious nature of Theorem 1, missing figures, weak experimental results, and numerous grammatical errors. The sentiment score is -40 because the review contains more negative feedback than positive. The politeness score is 20 because, while the reviewer is critical, the language used is not rude and includes suggestions for improvement.",-40,20
"This paper proposes to apply MAML to a multi-agent setting. In this formulation each opponent corresponds to a task and two separate parts of the policy are learned via meta-learning: 
1) the opponent modelling network that predicts the value function for a given opponent based on past actions and states. 
2) the policy network which takes in the state and the predicted value function of the opponent. 
The main concern with this paper is the lack of technical detail and an important missing baseline. The paper also suffers from lacking clarity due to a large number of grammatical mistakes. 

Technical detail and concerns: 
The paper mentions Duelling DQN as the RL algorithm in the inner loop. This is very unusual and it's a priori unclear whether MAML with DQN in the inner loop is a sensible algorithm. For example, DQN relies both on a target network and an argmax operator which seem to violate the differentiability requirements needed for MAML regarding higher order gradients. The authors entirely miss this and fail to address possible concerns. 

The authors also fail to provide any details regarding the exploration scheme used. In fact, a value function is never mentioned, instead the authors talk about a policy pi^a_i, leaving it unclear how this policy is derived from the value function. When the Q-function takes as input the true opponent, there is no need for meta-learning of the policy: Given a known opponent, the tuple (s_t, opponent) defines a Markov state. As far as I could gather from the paper, the authors are missing a baseline which simply learns a single Q-function across all opponents (rather than meta-learning it per opponent) that takes as input the predicted opponent. 
My expectation is that this is more or less what is happening in the paper. The authors also fail to compare and contrast their method to a number of recent multi-agent algorithms, eg. MADDPG, COMA and LOLA. 

Furthermore, the results are extremely toy and seem to be for single runs , rendering them insignificant. 

While the idea itself is interesting, the above concerns render the paper unsuitable for publication in it's current form.


","The sentiment of the review is largely negative, as it highlights several significant concerns with the paper, including lack of technical detail, missing baselines, and unclear explanations due to grammatical mistakes. The reviewer does acknowledge that the idea itself is interesting, but this is overshadowed by the numerous criticisms. Therefore, the sentiment score is -80. The politeness of the language is relatively neutral to slightly polite. The reviewer uses formal and professional language without being overtly rude, but the tone is firm and critical. Therefore, the politeness score is 20.",-80,20
"This paper is about using ""neural stethoscopes"", small complementary neural networks that are added to a main network which with their auxilary loss functions can measure suitability of features or guide the learning process. The idea is incremental to multi-task learning and enable, in a single framework, to validate intermediate features for additional related tasks. Moreover it can promote or suppress the correlation of such features to the tasks related to the main one. The framework is applied to the task of visual stability prediction of block towers. The paper builds upon Groth et al. 2018, adding the concept of local stability as correlated secondary task, used with the proposed neural stethoscopes. Experiments with an extension of ShapeStacks (Groth et al. 2018) dataset where the local stability is added to the global stability class, show that it is possibile increase the performance using the additional task. Moreover, it is shown that neural stethoscopes can suppress nuisance information when using a biased training dataset where the local and global stability are purposely inversely correlated.

Strengths:
+ A very nice paper, well written and easy to read. Figures are helpful and the structure is clear.
+ The concept of neural stethoscope is interesting and simplify the concepts behind multitask learning.
+ Experiments are convincing, interesting and there is some novelty in vision stability prediction.

Weaknesses:
- The novelty is limited related to multitask learning, thus it is an incremental paper.
","The sentiment of the review is quite positive, as evidenced by phrases like 'A very nice paper, well written and easy to read,' 'The concept of neural stethoscope is interesting,' and 'Experiments are convincing, interesting.' The only negative point mentioned is that the novelty is limited, which is framed in a constructive manner. Therefore, the sentiment score is high. The politeness of the language is also very high, as the reviewer uses polite and respectful language throughout, even when pointing out the weaknesses.",80,90
"The paper explores unsupervised deep learning model for extractive telegraphic summaries, which extracts text fragments (e.g., fragments of a sentence) as summaries. The paper is in general well structured and is easy to follow. However, I think the submission does not have enough content to be accepted to the conference.

First, in term of methodology (as described in Section 3), the paper has little novelty. There has been intensive study using various deep learning models on summarization. The models described in the paper contain little novelty compared with previous work using autoencoder and LSTM for both extractive and abstractive summarization. 

Second, the paper claims contributions on using deep learning models on telegraphic summarization, but the advantage is not well demonstrated. For example, the advantage of the resulting summary is not compared with state-of-the-art sentence compression models with intrinsic evaluation or (probably better) with extrinsic evaluation. (By the way, it is interesting that the paper argues the advantage of using telegraphic summaries for fictional stories but actually gives an example which looks also very typical in news articles (the “earthquake Tokyo 12 dead” example).)

Third, there has been much work on speech summarization that summarizes with the “telegraphic” style (this is natural, considering speech transcripts are often non-grammatical, and “telegraphic” style summaries focusing on choosing informative fragments actually result in usable summaries.) The author(s) may consider discussing such work and compare the proposed methods to it.
","The sentiment of the review is mixed but leans towards the negative side. The reviewer acknowledges that the paper is well-structured and easy to follow, which is positive. However, the majority of the review focuses on the lack of novelty, insufficient demonstration of advantages, and the need for more comparisons with existing work, which are negative points. Therefore, the sentiment score is -40. The politeness of the language is quite high. The reviewer uses polite phrases such as 'I think,' 'may consider,' and 'it is interesting,' and provides constructive criticism without being rude or dismissive. Therefore, the politeness score is 80.",-40,80
"This paper provides a theoretical study of GANs in the following setting:

- The target distribution has a locally smooth density on a compact set [0, 1]^D. It might be supported only on M disjoint components, each of which has a smooth boundary, within that compact set.
- The latent noise dimension (inputs to the generator) is of the same dimension as the data.
- An IPM loss (1) is used.
- The discriminator functions of the IPM and the generator networks are both ReLU networks of at most L layers and at most S total nonzero weights, with all weights having magnitude at most B.
- The discriminator functions are Lipschitz continuous; this is implied by the previous assumption, but the bound is tighter if we have a tighter constraint here.
- We obtain the generator which exactly minimizes the IPM between the empirical distributions of m samples from the model and n samples from the target.
- Maybe: there is some g such that P* is produced by g. It's not clear in your statement of Theorem 1 whether this is necessary, or exactly what it means when you say that in Lemma 3, and I haven't fully checked what's used yet; it would help to explicitly say this is not assumed if it's not.

Pointing out that the kinds of distributions handled by GANs often have disjoint support, and analyzing this case, is certainly of interest.

The full-dimensional support assumption is not ideal, but of course just because the analysis doesn't apply to most practical GAN settings doesn't mean it's not an important step towards one that does.

Also note that the assumption about the structure of the networks eliminates the MMD GANs that you use in experiments -- which have a kernel function at the top of the critic network -- though it does allow for most GAN variants. Maybe the most interesting algorithm for this setting is the Coulomb GAN (ICLR 2018, https://arxiv.org/abs/1708.08819 ), which uses a neural network critic of the kind you study but estimates a distance which (unlike the Wasserstein and most other GAN objective functions) has good statistical convergence properties (kappa=2 as you mention).

My biggest concern by far, though, is Proposition 1. You present it as if it's a lower bound: establishing that there is some class of distributions for which Theorem 1 shows that GANs can account for local smoothness and standard methods are shown not to be able to. This isn't what you do; instead you exhibit a class of distributions for which Theorem 1 shows that GANs can account for local smoothness, and previous analyses of standard methods do not show that they are able to take advantage of it. This is not the same thing at all! Although the previous upper bounds have matching lower bounds, you don't demonstrate (and it is likely not the case) that the distribution you show fits into the class of distributions used by the previous lower bounds, and so it remains very possible that other methods are able to take advantage of local smoothness as well as GANs do.

Given that Proposition 1 is hence a very weak statement, your main contribution in terms of disconnected support becomes ""we can show that GANs can adapt to disconnected supports (under these other assumptions) that has not previously been shown for other methods."" Not only is this a weaker result, but the degree to which you show GANs can take advantage of local smoothness is somewhat limited: at least with the parameter choices in Corollary 1, the smoothness only improves m dependence, not n dependence.

But in the GAN setting, m is essentially a question of how long you optimize for (and the relative rates between generator and discriminator updates, and various other questions like that out of scope for this paper), not any kind of externally fixed limitation like n. It's perhaps not too surprising that you don't show that optimizing a WGAN is statistically easier than estimating Wasserstein the distance, but given that estimating Wasserstein is so hard in high dimensions, it's a little disappointing. (Maybe it's easier, so that kappa is smaller, with locally smooth densities as here, though I don't know of any results like that offhand.)

(It's interesting, then, that in Corollary 1 the generator complexity depends only on m, basically the amount of optimization you're willing to do, while the discriminator complexity depends only on the available number of target samples n.)

Another concern is that to me, it is not very clear exactly what the statements of the assumptions mean. For example, does Theorem 1 apply only if I search over all generators in the class \mathcal G = \Xi(S_g, B_g, L_g)? In particular, does this mean that I have to consider all possible architectures matching those constraints, including allowing for all possible depths up to L_g, and all possible ways of allocating widths of the various interior layers / which weight entries are fixed at zero? It seems so, but this could be more explicit in the statements, even just by replacing ""an existing \mathcal G"" with ""\mathcal G = \Xi(S_g, B_g, L_g)"" in the statement of (e.g.) Theorem 1.

In your numerical experiments: you don't make it at all clear enough that you're plotting *different loss functions* for the GANs and the other methods! (You say this, but only in the text where it should definitely also be in the figure caption.) What happens if you plot the L2 difference for all methods, and the MMD/Wasserstein for all methods? Looking at Figure 6, it's not obvious that the GAN would do so substantially better. (It does seem to perhaps have the overall scale of the two components better than the other methods, but it doesn't look like as enormous a difference as it seems from Figure 5.)

Overall: Theorem 1 is of interest, but the results and especially the comparison to classic methods are not as resounding as they're presented here.

(Note that I have not (yet) verified or even really read most of the proofs; I might come back and do that later.)

Smaller points:

- I don't think that f-GAN actually fits in the framework (1) as you claim, since it needs to use the conjugate of f on the samples from Q. Also, the original GAN does fit into (1) but not your assumptions about the network form of f, since it needs a log in its activations.

- Another class of generative models where disconnected supports are really important is normalizing flows, which often build ""bridges"" between separated modes because (like your Lemma 2) their generators are constrained to be smooth and invertible. See e.g. Figure 2 of https://arxiv.org/abs/1810.01367 (who propose a new normalizing flow less susceptible to these problems). 

- Remark 1 seems so obvious that it need not even be stated, since beta-smooth implies beta'-smooth for beta' < beta. It would only be interesting if you could actually take advantage of the smoother components somehow.

- Many papers in your bibliography are cited only as arXiv preprints when they were actually published in various places. For example, the first four papers were published at ICLR 2017, ICML 2017, ICML 2017, and ICLR 2018, respectively.

- There are many small typos and grammatical errors in the draft, including some that would be caught by a spell-checker (""methdos"" on page 8), and an undefined LaTeX reference at the top of page 2. It would benefit from a thorough proofread.
","The sentiment of the review is mixed. The reviewer acknowledges the interest and potential importance of the theoretical study of GANs presented in the paper, but also raises significant concerns about the strength and clarity of the results, particularly Proposition 1. The reviewer provides constructive feedback and suggestions for improvement, indicating a willingness to help the authors improve their work. Therefore, the sentiment score is slightly positive. The language used in the review is polite and professional, with the reviewer offering detailed and specific recommendations without being dismissive or rude. The reviewer maintains a respectful tone throughout, even when pointing out weaknesses and areas for improvement.",20,80
"Overview:
The authors aim at finding and investigating criteria that allow to determine whether a deep (convolutional) model overfits the training data without using a hold-out data set.  
Instead of using a hold-out set they propose to randomly flip the labels of certain amounts of training data and inspect the corresponding 'accuracy vs. randomization‘ curves. They propose three potential criteria based on the curves for determining when a model overfits and use those to determine the smallest l1-regularization parameter value that does not overfit. 
I have several issues with this work. Foremost, the presented criteria are actually not real criteria (expect maybe C1) but rather general guidelines to visually inspect 'accuracy over randomization‘ curves. The criteria remain very vague and seem be to applicable mainly to the evaluated data set (e.g. what defines a ’steep decrease’?). Because of that, the experimental evaluation remains vague as well, as the criteria are tested on one data set by visual inspection. Additionally, only one type of regularization was assumed, namely l1-regularization, though other types are arguably more common in the deep (convolutional) learning literature.  
Overall, I think this paper is not fit for publication, because the contributions of the paper seem very vague and are neither thoroughly defined nor tested.


Detailed remarks:

General:
A proper definition or at least a somewhat better notion of overfitting would have benefitted the paper. In the current version, you seem to define overfitting on-the-fly while defining your criteria. 

You mention complexity of data and model several times in the paper but never define what you mean by that.


Detailed:
Page 3, last paragraph: Why did you not use bias terms in your model?

Page 4, Assumption. 
- What do you mean by the data being independent? Independent and identically distributed?  
- ""As in that case correlation in the data can be destroyed by the introduction of randomness making the data easier to learn.“ What do you mean by ""easier to learn""? Better generalization? Better training error? 
- I don’t understand the assumptions. You state that the regularization parameter should decrease complexity of the model. Is that an assumption? And how do you use that later?
- What does ""similar scale“ mean? 

Page 4, Monotony. 
- You state two assumptions or claims, 'the accuracy curve is strictly monotonically decreasing for increasing randomness‘ and 'we also expect that accuracy drops if the regularization of the model is increased’, and then state that 'This shows that the accuracy is strictly monotonically decreasing as a function of randomness and regularization.‘ Although you didn’t show anything but only state assumptions or claims (which may be reasonable but are not backed up here). 
I actually don’t understand the purpose of this paragraph.

- Section 3.3 is confusing to me. What you actually do here is you present 3 different general criteria that could potentially detect overfitting on label-randomized  training sets. But you state it as if those measures are actually correct, which you didn’t show yet.

My main concern here, besides the motivations that I did not fully understand (s.b.), is the lack of measurable criteria. While for criterion 1 you define overfitting as 'above the diagonal line‘ and underfitting as ‚below the line‘, which is at least measurable depending on sample density of the randomization, such criteria are missing for C2 and C3.       Instead, you present vague of ’sharp drops’ and two modes but do not present rigorous definitions. You present a number for C2 in Section 5, but that is only applicable to the present data set (i.e. assuming that training accuracy is 1). 

Criterion 2 (b) is not clear.  
- I neither understand ""As the accuracy curve is also monotone decreasing with increasing regularization we will also detect the convexity by a steep drop in accuracy as depicted by the marked point in the Figure 1(b)"" 
nor do I understand ""accuracy over regularization curve (plotted in log-log space) is constant""?
Does that mean that you assume that whenever the training accuracy drops lower than that of the model without regularization, it starts to underfit?

Due to the lack of numerical measures, the experimental evaluation necessarily remains vague by showing some graphs that show that all criteria are roughly met by regularization parameter \lambda=0.00011 on the cifar data set.  In my view, this evaluation of the (vague) criteria is not fit for showing their possible merit.




","The sentiment of the review is quite negative, as the reviewer expresses several significant concerns about the paper's contributions, definitions, and experimental evaluations. The reviewer explicitly states that the paper is 'not fit for publication,' which strongly indicates a negative sentiment. However, the language used is not overly harsh or rude, and the reviewer provides detailed feedback and suggestions for improvement, which shows a degree of politeness. Therefore, the politeness score is slightly positive.",-60,20
"This work proposes an alternative loss function to train models robust to adversarial attacks. Specifically, instead of the common sparse, N-way softmax-crossentropy loss, they propose to minimize the MSE to the target column of a random, dense orthogonal matrix. I believe the high-level idea behind this work is that changing the target labelspace is a more effective means of defending against adversarial attacks than modifying the underlying architecture, as the loss-level gradients will be strongly correlated across all architectures in the latter scenario.

Pros:
-Paper was easy to follow
-Using orthogonal encodings to decorrelate gradients is an interesting idea
-Benchmark results appear promising compared to prior works

Cons:
-This work claims that their RO-formulation is fundamentally different from 1-of-K, but I'm not completely sure that's true. One could train a classification model where the final fully connected layer (C inputs K output logits) were a frozen matrix (updates disabled) of K orthogonal basis vector (ie, the same as the C_{RO}) codebook they propose. The inputs to this layer would probably have to be L2 normalized, and the output logits would then proceed through a softmax-crossentropy layer. Would this be any less effective than the proposed scheme?
-Another baseline/sanity test that should probably included is how does the 1-of-K softmax/cross-entropy compare with the proposed method where encoding length l = k and the C_{RO} codebook is just the identity matrix? 
-Some of the numbers in Table 4 are pretty close. Since the authors are replicating Kannan et al, it would be best to included error bars when possible to account for differences in random initializations.
-It is unclear the extent to which better classification performance on the clean input generalizes to datasets such as ImageNet

Overall, I think the results are promising, but I'm not fully convinced that similar results cannot be achieved using standard cross-entropy losses with 1-hot labels.","The sentiment of the review is generally positive, as the reviewer acknowledges the interesting idea and promising benchmark results. However, there are some concerns and suggestions for improvement, which slightly temper the overall positivity. Therefore, the sentiment score is 40. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, providing specific recommendations without being harsh or dismissive. Thus, the politeness score is 80.",40,80
"This paper proposes an algorithm for optimizing neural networks parametrized by Tensor Train (TT) decomposition based on the Riemannian optimization and rank adaptation, and designs a bidirectional TT LSTM architecture.

I like the topic chosen by the authors, using TT to parametrize layers of neural networks proved to be beneficial and it would be very nice to exploit the Riemannian manifold structure to speed up the optimization.

But, the paper needs to be improved in several aspects before being useful to the community. In particular, I found the several mathematical errors regarding basic definitions and algorithms (see below the list of problems) and I’m not happy with lack of baselines in the experimental comparison (again, see below).

The math problems
1) In equations (1), (2), (7), and (8) there is an error: one should sum out the rank dimensions instead of fixing them to the numbers r_i. At the moment, the left-hand side of the equations doesn’t depend on r and the right-hand side does.
2) In two places the manifold of d-dimensional low-rank tensors is called d-dimensional manifold which is not correct. The tensors are d-dimensional, but the dimensionality of the manifold is on the order of magnitude of the number of elements in the cores (slightly smaller actually).
3) The set of tensors with rank less than or equal to a fixed rank (or a vector of ranks) doesn’t form a Riemannian (or smooth for that matter) manifold. The set of tensors of rank equal to a fixed rank something does.
4) The function f() minimized in (5) is not defined (it should be!), but if it doesn’t have any rank regularizer, then there is no reason for the solution of (5) to have rank smaller then r (and thus I don’t get how the automatic rank reduction can be done).
5) When presenting a new retraction algorithm, it would be nice to prove that it is indeed a retraction. In this case, Algorithm 2 is almost certainly not a retraction, I don’t even see how can it reduce the ranks (it has step 6 that is supposed to do it, but what does it mean to reshape a tensor from one shape to a shape with fewer elements?).
6) I don’t get step 11 of Alg 1, but it seems that it also requires reshaping a tensor (core) to a shape with fewer elements.
7) The rounding algorithm (Alg 3) is not correct, it has to include orthogonalization (see Oseledets 2011, Alg 2).
8) Also, I don’t get what is r_max in the final optimization algorithm (is it set by hand?) and how the presented rounding algorithm can reduce the rank to be lower than r_max (because if it cannot, one would get the usual behavior of setting a single value of rank_max and no rank adaptivity).
9) Finally, I don’t get the proposition 1 nor it’s proof: how can it be that rounding to a fixed r_max won’t change the value of the objective function? What if I set r_max = 1? We should be explained in much greater detail.
10) I didn’t get this line: “From the RSGD algorithm (Algorithm 1), it is not hard to find the sub-gradient gx = ∇f(x) and Exp−1 x (y) = −η∇xf(x), and thus Theorem 3 can be derived.” What do you mean that it is not hard to find the subgradient (and what does it equal to?) and why is the inverse of the exponential map is negative gradient?
11) In general, it would be beneficial to explain how do you compute the projected gradient, especially in the advanced case. And what is the complexity of this projection?
12) How do you combine optimizing over several TT objects (like in the advanced RNN case) and plain tensors (biases)? Do you apply Riemannian updates independently to every TT objects and SGD updates to the non-TT objects? Something else?
13) What is E in Theorem 3? Expected value w.r.t. something? Since I don’t understand the statement, I was not able to check the proof.

The experimental problems:
1) There is no baselines, only the vanilla RNN optimized with SGD and TT RNN optimized with your methods. There should be optimization baseline, i.e. optimizing the same TT model with other techniques like Adam, and compression baselines, showing that the proposed bidirectional TT LSTM is better than some other compact architectures. Also, the non-tensor model should be optimized with something better than plain SGD (e.g. Adam).
2) The convergence plots are shown only in iteration (not in wall clock time) and it’s not-obvious how much overhead the Riemannian machinery impose.
3) In general, one can decompose your contributions into two things: an optimization algorithm and the bidirectional TT LSTM. The optimization algorithm in turn consist in two parts: Riemannian optimization and rank adaptation. There should be ablation studies showing how much of the benefits come from using Riemannian optimization, and how much from using the rank adaptation after each iteration.

And finally some typos / minor concerns:
1) The sentence describing the other tensor decomposition is a bit misleading, for example CANDECOMP can also be scaled to arbitrary high dimensions (but as a downside, it doesn’t allow for Riemannian optimization and can be harder to work with numerically).
2) It’s very hard to read the Riemannian section of the paper without good knowledge of the subject, for example concepts of tangent space, retraction, and exponential mapping are not introduced.
3) In Def 2 “different function” should probably be “differentiable function”.
4) How is W_c represented in eq (25), as TT or not? It doesn’t follow the notation of the rest of the paper. How is a_t used?
5) What is “score” in eq (27)?
6) Do you include bias parameters into the total number of parameters in figures?
7) The notation for tensors and matrices are confusingly similar (bold capital letters of slightly different font).
8) There is no Related Work section, and it would be nice to discuss the differences between this work and some relevant ones, e.g. how is the proposed advanced TT RNN different from the TT LSTMs proposed in Yang et al. 2017 (is it only the bidirectional part that is different?) and how is the Riemannian optimization part different from Novikov et al. 2017 (Exponential machines), and what are the pros and cons of your optimization method compared to the method proposed in Imaizumi et al. 2017 (On Tensor Train Rank Minimization: Statistical Efficiency and Scalable Algorithm).


Please, do take this as a constructive criticism, I would be happy to see you resubmitting the paper after fixing the raised concerns!
","The sentiment of the review is mixed. The reviewer starts with a positive note, appreciating the topic and the potential benefits of the proposed approach. However, the majority of the review is critical, pointing out numerous mathematical errors, lack of baselines in experiments, and other significant issues. Therefore, the sentiment score is slightly negative. The politeness score is high because the reviewer uses polite language throughout, even when pointing out errors and suggesting improvements. The reviewer also ends on a positive note, encouraging the authors to resubmit after addressing the concerns.",-20,80
"
This paper proposes an objective, M^2VAE, for multi-modal VAEs, which is supposed to learn a more meaningful latent space representation. To summarize my understanding of the proposed objective, in the bi-modal case, it combines both objectives of TELBO [1] and JMVAE-kl [2] with some hyperparameters to learn the uni-modal encoders. The terms of Eqns 7,8, and 9 are equivalent to TELBO and Eqns 9 and 10 are JMVAE-kl. It would be very beneficial for the readers if you could more clearly contrast your objective with the related work given how similar they are. 

Given these similarities between objectives, its unclear why JMVAE-Zero was chosen over JMVAE-kl as a baseline. Furthermore, the reasoning for the improvement of the ELBO of M^2VAE over the baselines in Section 5.3 is unclear, given the similarities between the objectives. 

The qualitative figures throughout the paper are hard to interpret. By looking at Fig 4., I cannot tell which latent space is best. 
“one can see from Fig. 4 that the most coherent latent space distribution was learned by the proposed M^2VAE” 
What is meant by ‘coherent latent space’? 

This paper was hard to follow and there are a number of typos throughout the paper. For instance, the labels within Fig 4 and the caption contradict themselves. If the clarity and quality of the writing could be improved then perhaps the contributions may become more evident.  

[1] R. Vedantam, I. Fischer, J. Huang, and K. Murphy. Generative Models of Visually GroundedImagination. ArXiv e-prints, May 2017.
[2] M. Suzuki, K. Nakayama, and Y. Matsuo. Improving Bi-directional Generation betweenDifferent Modalities with Variational Autoencoders. ArXiv e-prints, January 2018

","The sentiment of the review appears to be slightly negative. The reviewer points out several issues with the paper, such as the lack of clarity in the objectives, the choice of baselines, and the difficulty in interpreting qualitative figures. Additionally, the reviewer mentions that the paper is hard to follow and contains typos. However, the reviewer does not dismiss the paper entirely and suggests that improving the clarity and quality of writing could make the contributions more evident. Therefore, the sentiment score is -40. The politeness of the language used in the review is relatively high. The reviewer uses polite language and constructive criticism, offering specific recommendations for improvement without being rude or dismissive. Therefore, the politeness score is 80.",-40,80
"This paper proposes a new meta-learner for few-shot learning that conditions the parameters of the model on the given query image. The authors argue that this allows the model to focus on features particular to the query, thereby facilitating classification. The paper introduces a kernel generator as a meta-learner and report performance on two standard benchmarks, Omniglot and miniImagenet.

Several methods propose meta-learners that adapt the learner’s parameters to the task or each class in the task. This paper adapts to the query itself, which may provide other benefits, and provides a useful complement to prior work on parameter adaptation in few-shot classification.

While the core idea itself is clearly articulated, the reading is dense and many of the finer points are vaguely presented. This makes the paper hard to read and its contribution unclear. In particular, the meta objective itself is not defined, the second loss function contains an undefined (learnable?) functions whose role is not entirely clear. In the experimental section, the authors mention that they use Prototypical Networks (Snell et al., 2017) on top of their kernel generator. This puts their contribution in a different light, now as an extension of Snell et al., (2017). I’m also unclear about the novelty of kernel generator the authors supposedly introduce. The kernel generator appears identical to that of Han et at. (2018), in which case the contribution is its application to few-shot learning, not the kernel generator itself.

Since the main contribution of this paper is to condition the learner’s parameters on the query, as opposed to the task or the classes in the task, the relevant comparison is with respect to such alternative methods. Several such benchmarks are missing (below), and when considered, the reported results are relatively weak. 

For an up-to-date collection of benchmarks on miniImagenet, see Rusu et. al., (2018, https://arxiv.org/abs/1807.05960).

===

[1] Gidari and Komodakis. Dynamic few-shot visual learning without forgetting. 2018.
[2] Oreshkin et al.. TADAM: Task dependent adaptive metric for improved
few-shot learning. 2018.
[3] Qiao et al.. Few-shot image recognition by predicting
parameters from activations. 2017.
[4] Bauer et al.. Discriminative k-shot learning using probabilistic models. 2017.","The sentiment of the review is mixed. While the reviewer acknowledges the potential benefits of the proposed method and its complement to prior work, they also highlight significant issues with the clarity of the paper, the definition of the meta objective, and the novelty of the kernel generator. The sentiment score is therefore slightly negative at -20. The politeness of the language is generally respectful and constructive, with the reviewer providing specific feedback and suggestions for improvement without using harsh or dismissive language. The politeness score is 60.",-20,60
"This paper describes a model for cross lingual transfer with no target language information. This is a well written paper that makes a number of contributions:

1. It provides an interesting discussion of transfer form multiple source languages into a target language. This is a timely problem and the paper points out that adversarial networks may be too limiting in this setup.

2. It provides a modeling approach that deals with the limitations of adversarial networks as mentioned in (1).

3. It demonstrates the value of the proposed approach through an extensive experimental setup.

At the same time, I see two major limitations to the paper:

1. While the proposed approach is valid, it is not very original, at least in my subjective eyes. The authors integrate a classifier that combines the private, language-specific features so that not only features that are shared between all the involved languages can be used in the classification process. While this is a reasonable idea that works well in practice, IMO it is quite straight forward and builds on ideas that have been recently been proposed in many other works.

2. The authors claim that: ""To our best knowledge, this work is the first to propose
an unsupervised CLTL framework without depending on any cross-lingual resource""

This is, unfortunately, not true. I refer the authors to the paper:

Deep Pivot-Based Modeling for Cross-language Cross-domain Transfer with Minimal Guidance. Yftah Ziser and Roi Reichart. EMNLP 2018.

In their lazy setup, the EMNLP authors do exactly that. They address the more complicated cross-language, cross-domain setup, but their model can be easily employed within a single domain. Their experiments even use the multilingual sentiment dataset used in the current paper. The model in the EMNLP paper shows to outperform adversarial networks, so it can be competitive here as well.","The sentiment of the review is generally positive, as indicated by phrases like 'well written paper' and 'makes a number of contributions.' However, the reviewer also points out two major limitations, which slightly temper the overall positivity. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer uses polite phrases such as 'I see two major limitations' and 'I refer the authors to,' and provides constructive feedback without being harsh or dismissive. Thus, the politeness score is 90.",60,90
"This paper proposes training latent variable models (as in VAE decoders) by running HMC to approximate the posterior of the latents, and then estimating model parameters by maximizing the complete data log-likelihood. This is not a new idea by itself and is used e.g. as a baseline in Kingma and Welling's original VAE paper. The novelty in this paper is that it proposes tuning the parameters of the HMC inference algo by maximizing the likelihood achieved by the final sample in the MCMC chain. This seems to work well in practice and might be a useful method, but it is not clear under what conditions it should work.

The paper is written in an unnecessarily complicated and formal way. On first read it seems like the proposed method has much more formal justification than it really has. The discussion up to section 3.5 makes it seem as if there is some new kind of tractable variational bound (the ERLBO) that is optimized, but in practice the actual objective in equation 16 is simply the likelihood at the final sample of the MCMC chain, that is Monte Carlo EM as e.g. used by Kingma & Welling, 2013 as a baseline.  The propositions and theorems seem to apply to an idealized setting, but not to the actual algorithm that is used. They could have been put in an appendix, or even a reference to the exisiting literature on HMC would have sufficed.

The experiments do not clearly demonstrate that the method is much better than previous methods from the literature, although it is much more expensive. (The reported settings require 150 likelihood evaluations per example per minibatch update, versus 1 likelihood evaluation for a VAE). Also see my previous comments about evaluation in this paper's thread.

- Please explain why tuning the HMC algo by maximizing eq 16 should work. I don't think it is a method that generally would work, e.g. if the initial sample z0 ~ q(z|x) is drawn from a data dependent encoder as in HVI (Salimans et al) then I would expect the step size of the HMC to simply go to zero as the encoder gets good. However in your case this does not happen as the initial sample is unconditional from x. Are there general guidelines or guarantees we can conclude from this?

- The authors write ""Because MPFs are equivalent to ergodic Markov chains, the density obtained at the output of an MPF, that is, qL, will converge to the stationary distribution π as L increases.""

This is true for the idealized flow in continuous time, but HMC with finite step size does generally NOT converge to the correct distribution. This is why practical use of HMC includes a Metropolis-Hastings correction step. You omit this step in your algorithm, with the justification that we don't care about asymptotic convergence in this case. Fair enough, but then you should also omit all statements in the paper that claim that your method converges to the correct posterior in the limit. E.g. the writing makes it seem like Proposition 2 and Theorem 1 apply to your algorithm, but it in fact they do not apply for finite step size. Maybe the statements are still correct if we take the limit with L->inf and the stepsize delta->0 at a certain rate? This is not obvious to me.

In practice, you learn the stepsize delta. Do we have any guarantees this will make delta go to zero at the right rate as we increase the number of steps L? I.e. is this statement from your abstract true? -> ""we propose a novel method which is scalable, like mean-field VI, and, due to its theoretical foundation in ergodic theory, is also asymptotically accurate"". (convergence of uncorrected HMC only holds in the idealized case with step size -> 0)","The sentiment of the review is moderately negative. The reviewer acknowledges that the proposed method might be useful but criticizes the lack of clarity and formal justification in the paper. The reviewer also points out that the experiments do not clearly demonstrate the superiority of the method and that it is more computationally expensive. The politeness of the language is relatively high. The reviewer uses polite language and provides constructive criticism, offering specific recommendations for improvement without being rude or dismissive.",-40,60
"The paper present interactive parallel exploration (IPE), a reinforcement learning method based on an ensemble of policies and a shared experience pool. Periodically, the highest-return achieving policy is selected, towards which the other policies are updated in a sense of some distance metric. IPE is applicable to any off-policy reinforcement learning algorithm. The experiments demonstrate some improvement over TD3 on four MuJoCo benchmark tasks.

The method is motivated heuristically, and and it provides some benefits in terms of sample efficiency and lower variance between training trials. However, it is hard to justify the increased algorithmic complexity and additional hyperparameters just based on the presented results. The paper motivates IPE as an add-on that can increase the performance of any off-policy RL algorithm. As such, I would like to see IPE being applied to other algorithms (e.g., SAC or DQN) as a proof of generalizability, and compared to other similar ensemble based algorithms (e.g., bootstrapped DQN).

While the improvement in the sample complexity is quite marginal, what I find the most interesting is how IPE-TD3 reduces variance between training trials compared to vanilla TD3. Convergence to bad local optimum can be a big problem, and IPE could help mitigate it. I would suggest including environments where local optima can be a big problem, for example HumanoidStandup, or any sparse reward task. Also the paper does not include ablations, which, given the heuristic nature of the proposed method, seems important.","The review starts by summarizing the paper's contributions and acknowledges the improvements in sample efficiency and variance reduction. However, it also points out significant limitations, such as the increased algorithmic complexity and the need for additional hyperparameters. The reviewer suggests further experiments to demonstrate the generalizability of the method and to compare it with other ensemble-based algorithms. The review is constructive and provides specific recommendations for improvement, indicating a balanced sentiment. The language used is polite and professional, focusing on constructive criticism rather than negative remarks.",20,80
"This paper attempts to speed up convergence of deep neural networks by intelligently selecting batches. The experiments show this method works moderately well.

This paper appears quite similar to the recent work ""Active Bias"" [1].
The motivation for the technique and setting appear very similar, while the details of the techniques are different. Unfortunately, this is not mentioned in the related work, or even cited.

When introducing a new method, it is important that design choices are principled, have theoretical guidance, or are experimentally verified against similar design choices. Without one of these, the methods become arbitrary and it is unclear what causes better performance. Unfortunately, this paper makes several choices, about an uncertainty function, the probability distribution, the discretization, and the algorithm (when to update) that appear rather arbitrary. For instance, the uncertainty function is a signed standard deviation of the softmax output. While there are a variety of uncertainty functions, such as entropy and margin, a new seemingly arbitrary uncertainty function is introduced.

The experiments are good but could be designed a bit better. For instance, it is unclear if the gains are because of lower asymptotic error or because of faster convergence. The learning curves are stopped too early, while the test error is still dropping quickly.

In summary, it is not clear if this paper adds any insight beyond ""Active Bias"".

[1] Active Bias: Training More Accurate Neural Networks by Emphasizing High Variance Samples. 2017. Haw-Shiuan Chang, Erik Learned-Miller, Andrew McCallum.",The sentiment of the review is moderately negative. The reviewer acknowledges that the method works moderately well but raises significant concerns about the originality and the arbitrary nature of the design choices. The sentiment score is -40. The politeness of the language is relatively high; the reviewer uses formal and respectful language even while pointing out the flaws. The politeness score is 60.,-40,60
"This paper proposes a measure (“effective path”) of which units and weights were most important for classification of a particular input or input class. Using the effective path, the authors analyze the overlap between paths across classes for CNNs and between adversarially modified and unmodified images. Finally, the paper proposes an adversarial defense method based on effective path which detects adversarially manipulated images with high accuracy and generality to a variety of settings. 

Overall, this paper is interesting and provides several novel observations. The clarity of the exposition is generally good, but can be improved in several places (mentioned below). As for significance, effective path is likely to inform future analyses of neural networks, and the adversarial defense may prove impactful, though ultimately, its impact will depend on if and when the defense is broken. 

However, there are several important controls missing from the analysis, several claims which are unsubstantiated, and experimental details are lacking in a few places. As such, in its current form, I can only weakly recommend this paper for acceptance. If in the revision the controls requested below are included, additional evidence is provided for the unsubstantiated claims (or if those claims are toned down), and exposition of missing experimental details is included, I’d be happy to raise my score. 

Major points:

1) While the observation regarding path specialization is very interesting, one cannot gauge whether or not the degree of overlap observed between class-specific paths signals path specialization or simply high input-to-input path variance (which is similar both within and across classes). In order to distinguish between these possibilities, a measure of intra-class path similarity is necessary. In addition, an experiment similar to that in Figure 2 with CIFAR-10 would be quite helpful in evaluating whether this phenomenon exists in more natural datasets (the ImageNet results are difficult to interpret due to the large number of classes).

2) Several claims in the path specialization section are unsubstantiated. 

2a) In particular, the claim that ‘1’ has the highest degree of specialization “because of its unique shape” is made without evidence as is the similarity between ‘5’ and ‘8’. ‘6’ is also similar to ‘8’ and yet does not show the same similarity in the path specialization. These differences may very well simply be due to chance.

2b) The claim that the path specialization in ImageNet matches the class hierarchy is made only based on the rough non-linearity of Figure 3. Please either measure the overlap within and across class categories or soften this claim.

3) The similarity analysis for adversarial images is also very interesting, but a comparison between unmodified and randomly perturbed images with matched norms to the adversarially perturbed images is necessary to establish whether this effect is due to noise generally or adversarial noise.
It’s unclear how the effective path is calculated when negative weights are involved. Further exposition of this aspect would be helpful.

Minor points/typos: 

1) There are several places where confusing concepts are introduced in one paragraph but explained several paragraphs later. In particular, the distinction between synapses and weights is introduced halfway through page 2 but explained on page 3 and the fact that the coefficients for the defense metric are learned is unclear until page 4 even though they’re introduced on page 3.

2) Typos: 

2a) Section 1, fourth paragraph: “...and adversarial images, we uncover...” should be “...and adversarial images, and we uncover...”

2b) Section 1, fourth paragraph: “...by small perturbation, the network…” should be “...by small perturbations, the network…”

2c) Section 2, first paragraph: “...the black-boxed neural…” should be “...the black-box neural…”

2d) Section 2, first paragraph: “In the high level…” should be “At a high level…”

2e) Section 4, first paragraph: “...as it does no modify…” should be “...as it does not modify…”

2f) Title, should be ""Neural Network""? 
","The sentiment of the review is moderately positive, as the reviewer acknowledges the novelty and interest of the paper but also highlights several significant issues that need to be addressed. The sentiment score is therefore 30. The politeness of the language is high, as the reviewer provides constructive feedback and suggests improvements in a respectful manner. The politeness score is 80.",30,80
"This paper uses pruning and model distillation iteratively to reduce the model sizes. The pruning step is based on Molchanov et al. (2017). This is followed by a hints step that minimizes the feature map difference between student and teacher. Finally, a reconstruction step is used to restore original weights. Results are shown on CIFAR-10, Imagenet and COCO datasets for classification and pose estimation tasks where PWH reduces model costs with a small loss in accuracy.

The paper is interesting that it proposes a unique combination of existing methods iteratively to improve the compression rates in modern CNNs. However, given that these methods already exist, the novelty aspect of this paper is low. Furthermore, it is also hard to rebut these methods, since they have been published and extensively evaluated in the respective papers. Nevertheless, it is interesting to note that both methods assist one another in your set of experiments.

In order to improve the paper and for the reviewers to judge the papers more favorably, the authors can compute the time for reconstruction/hints and demonstrate that it is clearly superior to fine-tuning/re-training and offers a clear advantage. This should be emphasized and articulated in related work and introduction and will help the paper.

How did you arrive at the magic number of pruning just 256 channels in every step?","The sentiment of the review is mixed but leans towards positive. The reviewer acknowledges the interesting combination of existing methods and the potential of the proposed approach, but also points out the low novelty and the need for additional evidence to support the claims. Therefore, the sentiment score is 20. The language used is polite and constructive, offering specific recommendations for improvement without being dismissive or rude. Thus, the politeness score is 80.",20,80
"The submission proposes to increase the variety of generated samples from GANs by a) using an ensemble of discriminators, and b) tasking them with distinguishing not only fake from real samples, but also their fake samples from the fake samples given to the respective other discriminators. The cost function of the minimax GAN optimization problem is changed accordingly. Experimental results suggest that this approach leads to improved results, both visually and w.r.t. FID metric.

Improving on the well known problem of mode collapse in GAN training scenarios is without a doubt an important endeavor. Various methods have been proposed, as curtly summarized in Section 1.1 of the submission.

With respect to the proposed method, I am not completely clear on how it can increase sample variety, if it does. In understand the arguments brought forward in Appendix B, but:
Consider a minimax game involving one generator G and one discriminator D, where each batch of generated samples from G(Z), is split up into two parts, A and B, via selection without replacement using uniform sampling. Z is a matrix of noise inputs, where each column corresponds to one item of the batch. D is now tasked to differentiate whether a sample came from A or B. It seems intuitive to say that in this case, D can neither win, nor provide any useful signal to G, since the sets A and B were split randomly, and there is no influence on G during training. The variety of samples in A as well as in B will be identical to the variety in the set (A and B).

Yet this random microbatch splitting is what seems to be happening here, if I understood Section 3 correctly; just with an ensemble of discriminators, and not just with one.
While it is thus not completely clear to me *why* the proposed additional term seems to bring increased variety, experiments strongly suggest that it does.
As described in Section 3.1, choice of the weighting parameter alpha seems crucial, and additionally alpha needs to depend on the iteration index. Different schedules are demonstrated, but optimality of either is not guaranteed. This makes the actual influence of the additional loss term even harder to judge and evaluate.

Section 4.1 seems to confirm the increase in variety via the self-defined ""Intra FID"" measure. I would have liked to see this measure evaluated on conventionally trained GANs as a baseline, as well on the methods compared to in Section 5.
In Table 1, both min and mean FID are given over 50k iterations. Instead of reporting the minimum, it might be fairer to compare FIDs after a fixed number of iterations (i.e. 50k in this case).

The method comparison in Section 5 is generally appreciated, but I think some of its flaws are:
- Datasets, including ImageNet, are all downsampled to 32x32 pixels. We have seen generators in recent work that produce interesting high-resolution output in even megapixel size; the tiny size seems like a pessimization of overall approaches.
- The proposed method is compared to other methods using only 2 discriminators, although Section 4.3 suggest a larger number is better.
- MicroGAN does not compare favorably to many of the compared to methods in Table 2. This may not necessarily by a flaw of the MicroGAN contributions, but is rather a problem of an apples-to-oranges comparison, as the authors readily admit (""the use of more powerful architectures [...] plays a big role""). I question the value of such a comparison, if not only the method differ, but also implementation details such as network architectures.

Overall the submission is quite interesting, but not without the above-mentioned flaws.","The sentiment of the review is generally positive but with notable reservations. The reviewer acknowledges the importance of the problem being addressed and finds the submission interesting, but also points out several flaws and areas for improvement. Therefore, the sentiment score is moderately positive. The language used is polite and constructive, with the reviewer providing specific feedback and suggestions without being rude or dismissive. Hence, the politeness score is high.",40,80
"Paper overview: The paper extends the method proposed by Arora 2017 for sentence embeddings to longer document embeddings. The main idea is that, averaging word embedding vectors mixes all the different topics on the document, and therefore is not expressive enough. Instead they propose to estimate the topic of each word (using dictionary learning) through the $\alpha$ weights (see page 4).These weights give ""how much"" this word belongs to a certain topic. For every topic we compute the $\alpha$-weighted vector of the word and  concatenate them (see word topic vector formation). Finally, we apply SIF (Arora 2017) using these word embeddings on all the document.   

Questions and remarks:
     1) How sensitive is the method to a change in the number of topics (k)?
    2) Please provide also the std instead of just the average performance, so that we can understand if the differences between methods are significantly meaningful
 
Points in favor: 
   1) Good results and thorough tests 
    2) Paper is easy to read and follow 

Points against:
A very similar method was already proposed by Mekala 2017, as the authors acknowledge in section 7. The main difference between the two methods is that Mekala et al use GMM and the authors of the present paper use sparsity method K-SVD to define the topics. 


The novelty of the paper is not enough to justify its acceptance at the conference.","The sentiment of the review is slightly negative, as indicated by the final statement that the novelty of the paper is not enough to justify its acceptance at the conference. However, the reviewer does acknowledge some positive aspects, such as good results, thorough tests, and the paper being easy to read and follow. Therefore, the sentiment score is -30. The politeness of the language is quite high, as the reviewer provides constructive feedback and acknowledges the strengths of the paper without using harsh or rude language. Thus, the politeness score is 80.",-30,80
"This paper presents a view of sentence-level prediction tasks as statistical relation learning problems. In particular the paper argues that composition functions used in recent SRL techniques developed for entity-to-entity relationship detection can be applied to sentence-level relation prediction tasks. 

Suppose there is a prediction training task defined over pairs of sentences (x1, x2). This task requires some function 'f' that composes the sentence representations h1 and h2 into a single representation which is then used to 
make the relation prediction i.e., we have a model g(f(h1, h2)) that is used to predict some relation between R(x1, x2).  This paper aims to show that with a better 'f' we can hope for a better result in transfer tasks (in addition to doing better on the training task). 

The paper argues that this setting, at a high level, is similar to the composition function used in entity-entity relation prediction. There have been many such methods in the recent past (e.g., TransE, ComplEx, RESCAL). This paper asks whether these composition functions can work well for sentence-level tasks.

The paper then presents experiments which compare the performance of different composition functions against a basic composition function used in InferSent.

Strengths of the paper:

1. I like the main question of what can we learn from SRL. This seeks to bridge some independent research threads.
2. The evaluation considers a range of composition functions used in SRL and applies them to the sentence tasks. 
3. Points out that some of the composition functions used in existing models are not particularly strong.

Issues:

I like the starting point for this paper very much and agree that the existing composition functions for sentence relations are rather weak. However, I am struggling to see if there is (i) a convincing conceptual argument for why SRL view of compositions is necessarily the answer for sentence level tasks, or (ii) a convincing empirical case for the same.  Some details on these points:

1) The parallels between entity-entity relations and sentence-sentence relations seems a bit of a stretch to me. There is always some level of abstraction at which two problems might look similar, which can be advantageous for repurposing solutions. However, in this case I think the SRL view of the world hides the complexities in sentence-sentence relation tasks (e.g. aligning relevant pieces of information, requiring more complex composition functions to derive meaning etc.). 

2) I am not sure what knowledge we are getting from an SRL view of the problem that is not already known already to the communities that work on sentence embedding. The minimum requirements laid out can be met easily by existing methods for sentence representations. For instance that we need to allow for asymmetric relations (entailment order) is very well known. As the authors themselves point out there are solutions for this problem.  

3) The empirical results don't appear convincing. The average gain for any particular method over InferSent is 0.3 in macro average. There is no single SRL based composition method that works consistently clear gains across most tasks. 

Here are some suggestions that I think will improve the paper (or at least help me buy the motivation): 

1. One question that might be useful to make a conceptual argument is how much work should be done in 'f' and should it change for the different type of target tasks.

If the idea is to transfer h for single sentence target task, then a powerful 'f' can render h1 and h2 to be simple enough, such that bulk of the work in extracting task related information might be done by 'f' itself. Therefore, transferred h may not be as powerful as it could have been with a less powerful 'f'.

If the idea is to transfer f(h1, h2) for sentence-pair target tasks, then a powerful 'f' might be a good thing. 

2) Another useful discussion would be to discuss why more powerful alignment based sentence representations are not being considered at least for comparison purposes. 

The paper wants to go from a simple 'f' (i.e. concat(h1,h2), h1-h2) to some other choices for 'f' that are known functions from SRL. 

There are several sentence-level representation functions such as ESIM [Chen et al., 2016] which uses a combined representation of premise and hypothesis sentences using soft alignment to specifically address the issues in comparing sentences. A similar representation is computed in BiDAF [Seo et al., 2017] in the context of matching question representation with sentence representations. 

To summarize, I really like the basic starting point for the paper and would love to see a more compelling presentation of the conceptual argument and a stronger empirical comparison.
","The sentiment of the review is generally positive, as the reviewer expresses appreciation for the main question and the starting point of the paper. However, the reviewer also points out significant issues and areas for improvement, which tempers the overall positivity. Therefore, the sentiment score is 30. The politeness of the language is high, as the reviewer uses phrases like 'I like,' 'I agree,' and 'I would love to see,' and provides constructive feedback in a respectful manner. Therefore, the politeness score is 80.",30,80
"This paper studied learning unsupervised inductive node embeddings with an attention mechanism. For each positive edge, multiple different sets of neighborhoods are sampled for both the source and target nodes, and the similarity between the neighborhood are used as the attention functions. Experimental results prove the effectiveness of the proposed approach over GraphSAGE on a few networks. 

Strength:
- learning unsupervised inductive node embeddings is an important problem
- the proposed method seems to work

Weakness:
- the novelty of the proposed method seems to be very marginal
- the experiments are quite weak
- the complexity of the algorithm seems to be very high

Details:
- the complexity of the algorithm seems to be very high seem for each pair of nodes, multiple sets of neighborhoods must be sampled for each node.
- there are also other approaches for inductive unsupervised node embeddings, for example, the varitional graph autoencoder method (Kipf et al. 2017).
- I am wondering how the proposed method performs comparing with the methods of only selecting the nodes which form triangles with the given positive edges. 
","The sentiment of the review is mixed but leans slightly negative. The reviewer acknowledges the importance of the problem and the effectiveness of the proposed method, which gives a positive sentiment. However, the reviewer also points out significant weaknesses such as marginal novelty, weak experiments, and high algorithm complexity, which contribute to a negative sentiment. Therefore, the sentiment score is -20. The politeness of the language is quite high; the reviewer uses phrases like 'seems to be' and 'I am wondering,' which soften the critique and make it more constructive. Therefore, the politeness score is 80.",-20,80
"The paper studies the use of embedding techniques in recommender systems, and shows that item2vec (an item vectorization method) can be replaced by user2vec, as users and items are interchangeable.

This is a reasonable enough idea, though not sufficient for publication in ICLR. I'd suggest the authors address the following details:
-- The methodological contribution is too small, and fairly obvious. Not sufficient for this conference.
-- Only evaluated on one dataset, so unclear whether the results are really representative
-- Comparisons against a very limited set of similar methods, which are probably not state-of-the-art for this dataset
-- The results don't seem significant, all methods compared perform almost equally","The sentiment score is derived from the overall tone of the review, which is critical but not overly harsh. The reviewer acknowledges the idea as 'reasonable enough' but points out several significant shortcomings. This results in a sentiment score of -40, indicating a generally negative but not extremely harsh sentiment. The politeness score is based on the language used, which, while critical, remains professional and avoids rudeness. Phrases like 'I'd suggest the authors address' and 'unclear whether the results are really representative' are polite ways to offer criticism, leading to a politeness score of 40.",-40,40
"This paper investigates the use of techniques for improving neural network training (regularization, normalization of covariance, sparsity) in terms of their generalization properties, empirically and analytically. The claim is that most of these tools do not help improve performance, with the exception of mutual information.

Pros: It's interesting to investigate and compare these different ""regularization"" techniques and compare them on different tasks empirically.

Cons:
Many of the points made in the paper are not properly capturing the nuance in the ""conventional wisdom"", and although it's good to be reminded and the empirical results are interesting to look at, in fact these are not really new discoveries, and sometimes the conclusions are very misleading. 

1) There is test loss, and there is generalization loss, and it isn't exactly the same thing. For a hypothesis class H, we have

test loss <= train loss + generalization loss 

where train loss measures how well we've fit a particular sample, and generalization loss measures how well a model that is trained on one sample can fit a new sample. Note that if I apply L2 loss to my model and have a regularization parameter --> infty, my train loss is huge but my generalization loss is 0. In other words, for a large enough regularization parameter, most of the methods experimented here WILL limit effective capacity and minimize generalization loss; it just will not give you the best test error performance. So the distinction here should be made much clearer--conventional wisdom for regularization limiting generalization error is not wrong.

2) The point that is trying to be made in section 3.1 is somewhat well-known in the general optimization literature. Let's consider a much simpler example: linear regression 

min_x ||Ax-b||_2^2 + gamma ||x||_2^2

Let's consider first no regularization, gamma = 0. Assume that A has a huge nullspace. Then technically there are an infinite number of globally optimal solutions x, although if we solve this problem using SGD starting with x = 0, it is known that the minimum norm solution is always picked. You can also think of this as whitening, since large lambda smooths the spectrum of the Hessian. Now add in regularization. Now the solution is unique, even if A is ill-conditioned. It's true that it isn't super necessary to add this regularization, since SGD can get you a good solution, but now we can GUARANTEE that the generalization error is 0. In practice, also, regularization adds stability to the numerics. 

In deep learning, the hidden layer z also acts as a coefficient matrix for determining y. I assume that is why people pursue low correlation, since it affects the conditioning of z. 

3) Comments like  ""for scaling and permutation, their influences are rather insignificant"" seem a bit careless to me. In fact it is well known that scaling can affect training performance significantly. But of course, if I know the global solution for z which feeds into a softmax, then any scaling on z does not affect the output of the softmax. That, however, does not mean I don't care about the scaling of z when training. 

4) Sparsity and rank BOTH limit the degrees of freedom. In fact, sparsity makes more sense when optimizing a nonlinear objective, which is always the case in deep learning. The reason to limit rank is when you wish to ""learn your codewords"", e.g. the eigenvectors, whereas in sparsity, the ""codewords"" are already learned, and you just learn the weighting. But if the codewords span the space, they both have equal representability. 

4) It is not clear to me what the task is in 4.3. What is the ""accuracy"" in a data generation task? Is this the normal classification task? If so, is the accuracy reported train or test accuracy? How exactly is generalization error being measured? 


5)I am not clear as to what conclusion is being drawn in section 5, with the last sentence ""obviously, many of the statistical characteristics become meaningless for such a scalar representation, and it is high time to reconsider the so-called conventional wisdom on representation characteristics."" why is this conclusion drawn based on the observation that, if a scalar z perfectly correlates with y, in fact this is the most generalization neural network? 

6) Table 4: how did you choose your hyperparameters? (regularization performance is extremely sensitive to parameter choice.)

7) A major concern is that basically very little training is done in these comparisons, except in the very last section. As I previously mentioned, many of these regularization / normalization techniques are also meant to better condition the optimization itself, and thus this advantage should not be discarded. 


minor comments:
 - page 5 last sentence ""characteristics"" should be singular
 - page 8 first sentence ""to [a] deep network's performance""","The sentiment of the review is mixed but leans towards the negative side. The reviewer acknowledges the interesting aspects of the paper but criticizes the lack of novelty and misleading conclusions. Therefore, the sentiment score is -30. The politeness of the language is generally respectful and constructive, even though the reviewer points out several flaws. The reviewer uses phrases like 'it is not clear to me' and 'seem a bit careless to me,' which are polite ways to express criticism. Therefore, the politeness score is 50.",-30,50
"The authors present a novel method for generating images from sounds using a two parts model composed by a fusion network, aka. multi-modal layers, for learning sound and visual features in a common semantic space, and two conditional GANs for converting sound features into visual features and those into images. To validate their approach they created an ad-hoc dataset, based on Flickr-SoundNet dataset, which contains 104K pairs of sounds and images with matching scene content. Their model was trained as two separate models, the fusion network was trained to classify both images and sounds minimizing their cross-entropy and their L1 distance, while the two conditional GANs were trained until convergence penalizing the discriminator to prevent fast convergence.

Although the idea of generating images from sounds with the aid of Generative Adversarial Networks is quite novel and interesting, the paper exhibits several problems starting with the lack of clarity explaining the purpose of the proposed method and the contributions of the work itself. Overall, the idea is good but not well developed. Introduction should present more clearly the problem and framework.

In the related work section the authors omitted some relevant recent prior works such as “Look, Listen and Learn” paper by Arandjelović and Zisserman presented on ICCV’17, “Objects that Sound” by Arandjelović and Zisserman presented on ECCV’18, “Audio-Visual Scene Analysis with Self-Supervised Multisensory Features” by Owens and Efros presented on ECCV’18, and “Jointly Discovering Visual Objects and Spoken Words from Raw Sensory Input” by Harwath et al. also presented presented on ECCV’18. These works propose different methods for aligning visual and sound features.

There are also several concerns on the validity of the results: 1) none of the results achieved by training their multi-modal layers were validated against a baseline, e.g. evaluating the quality of the learned visual features against VGG or a simple GAN instead of two stacked conditional GANs, 2) it is not clear why they learned features minimizing L1 loss + Cross-Entropy while using L2 distance to address the quality of their learned features, a simple way of doing so would be evaluating their retrieval capabilities using any standard measure from the retrieval community, e.g. the normalized discriminative cumulative gain (nDCG) or the classical mean-average precision (mAP) as proposed in “Objects that Sound”, 3) the authors assume that using a conditional GAN is suitable for generating images from visual features, but they don’t provide any quantitative results supporting this claim, they only provide a few successful qualitative results and elaborate their model from there. 4) Ablation is completely missing: it would be interesting to prove the effective contribution for i) the multi-modal fusion ii) the two-steps of image generation iii) the L_ losses for the two GANs.

There are many missing citations throughout the paper, in particular: 1) the concatenation of visual and sound features followed by a fusion network for learning features in a common semantic space was already proposed on “Look, Listen and Learn”, 2) when the authors describe their strategy for sound features extraction in section four, they never mentioned that the idea of using pool5 layer features was already introduced by SoundNet authors, and 3) in section 5.3 when they mention that using a conditional GAN to convert between two different feature domains it might be that the discriminator may converge too rapidly while the generator does not learn sufficiently.

Finally although using an ad-hoc extremely simplified dataset with pairs of images and sounds matching scene content, the complete model is able to generate images which achieve only a 8,9% matching rate for the top 3 predicted classes. Given that the dataset was created with 100% matching on the top 3 scores for sound and images, the results are definitely  poor.
","The sentiment of the review is mixed but leans towards the negative side. The reviewer acknowledges the novelty and potential of the idea but points out several significant issues with the clarity, related work, validity of results, and missing citations. Therefore, the sentiment score is -40. The politeness of the language is relatively high; the reviewer uses polite and constructive language to point out the flaws and provide recommendations for improvement. Thus, the politeness score is 80.",-40,80
"Starting from a simple neural network with only one hidden layer and a single output, the basic idea of approximate empirical Bayes (AEB) method is proposed, defining a matrix-variate normal prior distribution with a Kronecker product structure, so as to capture correlations between the row and column vectors of the weight matrix. Then, a block coordinate descent algorithm for solving the optimization problem is proposed. It consists of alternating three steps to obtain the optimal solutions of model parameters, row and column covariance matrices.

The current method is investigated and tested on three data sets for both classification (MNIST & CIFAR10) and regression (SARCOS) tasks. Encouraging experimental results demonstrate that the correlation learning in the weight matrix significantly improves performance when the training set size is relatively small. It is also shown that the proposed AEB method does not seem sensitive to the size of mini-batches and its combination with other generalization methods can lead to better results in some cases.

 Strengths:

 This paper is mostly well written and overall is easy to follow. It clearly reveals that correlation in the weight matrix plays a crucial role in better generalizing on small training sets.

 Minor comments:

 * The authors state that it is straightforward to extend the proposed method to more sophisticated models with various structures, such as CNN. Perhaps a bit more detail should be given in the main text.
 * Fig. 6 on page 12 is not explicitly mentioned in the main text. It seems a bit confusing.","The sentiment of the review is generally positive. The reviewer highlights that the paper is mostly well-written, easy to follow, and demonstrates the importance of correlation in the weight matrix for better generalization on small training sets. This indicates a positive sentiment towards the paper's contributions and clarity. Therefore, the sentiment score is 80. The politeness of the language is also high. The reviewer uses polite and constructive language, providing minor comments and suggestions for improvement without any negative or harsh criticism. Hence, the politeness score is 90.",80,90
"Disclosure: I reviewed this paper for a different conference but have read the new manuscript and noted the changes.

Summary:
The paper considers a very novel (but important) RL context where the agent has a constrained amount of information for representing a policy.  The authors use techniques from rate-distortion theory to generate a clever Bellman loss function that can be used (1) in a context where V*(s) is already known, and more importantly (2) with an actor-critic architecture (CL-AC) where the value function is being learned online.  CL-AC is shown to actually achieve higher converged and cumulative rewards than AC in many grid world domains and is shown to be advantageous in a transfer learning setting as well.

Review:

The ideas in the paper are very well described and laid out.  The experiments are on grid worlds but for such a novel problem like this I think they are at the right level because they allow the reader to understand the results.  The empirical results are compelling, but I have a strong technical concern about the convergence issue noted by the authors (which was also communicated to the authors in a previous conference’s review session).  

My main concern is, as the authors noted, the required state occupation probability p(s) for RDT is approximated in a way that could lead to bad behavior in the RL algorithm.  What we’re seeing here is the application of an RDT procedure that was designed for a static distribution being applied to a dynamic distribution of states (that can change based on the policy).  In RL, there is no guarantee that the previous occupation probabilities have anything to do with the current policy’s induced distribution.  In a hallway world with a decent reward down the left and a bigger reward to the right, an algorithm might start off by going down the left side several times, making the probabilities of states on the right 0.  If I am reading the algorithm right, the states on the right are going to be essentially dismissed as unlikely, and the “go right” action (which is optimal) will likely be compressed out, since the states it should be used in are considered unlikely.  More succinctly, early trajectories will bias p(s) and cause the algorithm to essentially want to optimize the policy for that distribution, likely causing it to stay in that distribution.  Even more dangerously, there may be cases where this could cause the algorithm to thrash between policies as p(s) oscillates between different parts of the state space.  

In order to improve this paper and make it suitable for publication, the authors should at least empirically demonstrate how different state occupation probability approximations affect the algorithm.  A good example is the trace-decay probabilities mentioned (but not implemented) in the paper.  If the paper compared that approach to the current approach, and showed an environment where one or both approaches failed to act correctly, that would complete the scientific result. Right now, only one approximation is demonstrated, and as detailed above, its behavior is suspect. 

While most of the empirical results are well explained, the behavior in Figure 2B, where CL-AC is outperforming standard AC remain unclear. I understand that in 2A (avg. cumulative reward), CL-AC may be inducing a more efficient exploration policy and therefore the rewards during learning will be better.  But in 2B, we are just looking at the final policy.  Was standard AC not able to find the optimal policy after 100 episodes?  

The results in the transfer learning context (Figure 3) are well done and produce a very interesting curve.  

Reference 9 appears to only be available as an arXiv pre-print.  Papers that have not been properly vetted by peer review should not be cited in an ICLR paper unless they are extremely necessary, which this does not appear to be.


Typo: Page 6 – sate -> state

","The sentiment of the review is generally positive, as the reviewer acknowledges the novelty and importance of the research, as well as the clarity and compelling nature of the empirical results. However, the review also contains significant technical concerns and suggestions for improvement, which slightly temper the overall positive sentiment. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, even when pointing out concerns and suggesting improvements. Therefore, the politeness score is 90.",60,90
"This article compares different methods to train a two-layer spiking neural network (SNN) in a bio-plausible way on the MNIST dataset, showing that fixed localized random connections that form the hidden layer, in combination with a supervised local learning rule on the output layer can achieve close to state-of-the-art accuracy compared to other SNN architectures. The authors investigate three methods to train the first layer in an unsupervised way: principal component analysis (PCA) on the rates, sparse coding of activations, and fixed random local receptive fields.  Each of the methods is evaluated on the one hand in a time-stepped simulator, using LIF neurons and on the other hand using a rate-approximated model which allows for faster simulations. Results are compared between each other and as reference with  standard backpropagation and feedback alignment.  The main finding is that localized random projections outperform other unsupervised ways of computing first layer features, and with many hidden neurons approaches backpropagation results. These results are summarized in Table 8, which compares results of the paper and other state-of-the-art and bio-plausible SNNs. PCA and sparse coding work worse on MNIST than local random projections, regardless if the network is rate-based, spike-based or a regular ANN trained with the delta rule. Feedback Alignment, although only meant for comparison, performs best of the algorithms investigated in this paper.

In general the question how to train multi-layer spiking neural networks in a bio-plausible way is very relevant for computational neuroscience, and has attracted some attention from the machine learning community in recent years (e.g. Bengio et al. 2015, Scellier & Bengio 2016, Sacramento et al. 2018). It is therefore a suitable topic for ICLR. Of course the good performance of single-layer random projections is not surprising, because it is essentially the idea of the Extreme Learning Machine, and this concept has been well studied also for neuromorphic approaches (e.g. Yao & Basu, 2017), and versions with local receptive fields exist as well (Huang et al. 2015 ""Local Receptive Fields Based Extreme Learning Machine""). While the comparison of different unsupervised methods on MNIST is somehow interesting, it fails to show any deeper insights because MNIST is a particularly simple task, and already the CIFAR 10 results are far away from the state-of-the-art (which is >96% using CNNs). Another interesting comparison that is missing is with clustering weights, which has shown good performance for CNNs e.g. in (Coates & Ng, 2012) or (Dundar et al. 2015), and is also unsupervised.

The motivation is not 100% clear because the first experiment uses spikes, and shows a non-negligible difference to rate models (the authors claim it's almost the same, but for MNIST differences of 0.5% are significant). All later results are purely about rate models. The authors apparently did not explore e.g. conversion techniques as in (Diehl et al. 2015) to make the spiking results match the rate versions better e.g. by weight normalization.

I would rate the significance to the SNN community as average, and to the entire ICLR community as low. The significance would be higher if it was shown that this method scales to deeper networks or at least can be utilized in deeper architectures. Scrutinizing the possibilitites with random projections on the other hand could lead to more interesting results. But the best results here are obtained with 5000 neurons with 10x10 receptive fields on images of size 28x28, thus the representation is more than overcomplete, and of higher complexity than a convolution layer with 3x3 kernels and many input maps.

Because the results provide only limited insights beyond MNIST I can therefore not support acceptance at ICLR.

Pros:
+ interesting comparison of unsupervised feature learning techniques
+ interesting topic of bio-plausible deep learning

Cons:
- only MNIST, no indications if method will scale
- results are not better than state-of-the-art


Minor comments:

The paper is generally well-written and structured, although some of the design choices could have been explained in more detail. Generally, it is not discussed if random connections have any advantage over other spiking models in terms of accuracy, efficiency or speed, besides the obvious fact that one does not have to train this layer. 

The title is a bit confusing. While it's not wrong, I had to read it multiple times to understand what was meant.

The first sentence in the caption for Fig. 2 is also confusing, mixing the descriptions of panel A and B. Also, in A membrane potentials are shown, but the post-membrane potential seems to integrate a constant current instead of individual spikes. Is this already the rate approximation of Eq. 2? Or is it because of the statement in the caption that they both receive very high external inputs. In general, the figures in panel A and B do not make the dynamics of the network or the supervised STDP much clearer. 

Principal Component Analysis and Sparse Coding are done algorithmically instead of using a sort of nonlinear Hebbian Learning as in Lillicrap 2016. It would have been interesting to see if this changes the comparatively bad results for PCA and SC.

In Fig. 3, the curve in the random projections case is not saturated, maybe it would have been interesting to go above n_h = 5000. As there are 784 input neurons, a convolutional neural network with 7 filter banks already would have around 5000 neurons, but in this case each filter would be convolved over the whole image, while with random projections the filter only exists locally. 

In Eq. 1, the notation is a bit ambigous: The first delta-function seems to be the Dirac-delta for continuous t, while the second delta is a Kronecker-delta with discrete t.

In A.1 and A.4.2 it is stated that the output of a layer is u_{t+1} = W u_t + b but I think in both cases it should be W a_t + b where a_t = phi(u_t). Otherwise, you just have a linear model and no activations. 

In Table 3, a typo: ""eq. equation""   ","The review starts with a summary of the paper's contributions and findings, which is generally positive. However, it quickly transitions to pointing out significant limitations, such as the lack of deeper insights beyond MNIST and the absence of comparisons with other relevant methods. The reviewer acknowledges the relevance of the topic but ultimately does not support acceptance at ICLR due to the limited significance of the results. The language used is polite and constructive, offering specific suggestions for improvement and minor comments for clarity.",-20,80
"The paper addresses the problem of increasing and decreasing the number of hidden nodes (aka, dimensionality) in the network such that the optimization will not enter the plateaus of saddle points. The opening or closing of tunnels (filters) guarantee the existence of “new escape directions” and faster convergence. 

Strengths:
+ provide a new perspective of designing the shape/dimensionality of a network in a dynamic manner. 
+ provide theoretic proof of CNNs and FCs on the contribution to the gradient after cloning. 

Weakness:
- Experiments are very weak to verify the theory.

Detailed comments:

- Eqn. (6) seems to provide a unified evaluation on the contribution of two units to the gradients. How does it relate to the experiments? It gives me a sense that the manuscript is isolated between theory (Section 2 and 3) and verification (experiments).
- Why does the blue curve get stuck in a flat area? A better staring learning rate could alleviate the plateau bottleneck. 
- The experiment settings are a little bit simple, even for the most complicated one in Section 4.4, where there are five conv layers and the tunnel opening only involves one single filter. Do authors conduct more filters opening in more layers? How about the closing case? There is no result/analysis in the experiments. 
- Why authors claim the blue curve in the left figure 2, a “flat area”? It seems working as the orange one (loss decreases normally). 
- Another big concern is that the proposed method is supposed to prevent network from saddle points and faster convergence, which is verified. And yet, the ultimate goal is to improve the performance. I am surprised that there is no such result at all in the manuscript (for example, error rate goes down on cifar/mnist/etc). 

In summary, I do recognize the theoretical effort the paper has provided; however, the experiments seem not to verify the proposed method in a professional manner. 

","The sentiment of the review is mixed but leans towards the positive side. The reviewer acknowledges the strengths of the paper, such as providing a new perspective and theoretical proof, but also points out significant weaknesses in the experimental validation. Therefore, the sentiment score is slightly positive. The language used is polite and constructive, offering specific suggestions for improvement without being rude or dismissive.",20,80
"This work builds on the ICML paper from Saxe et al (2017) in which the compositionality property of LMDPs was exploited to solve multi-task hierarchies. The paper extends this work by proposing a method that learns incrementally such hierarchies instead of pre-defining them by design. Some experimental results illustrate the method on two toy problems, a 1D corridor and a corridor of rooms.

The paper deals with an interesting and hard problem. Learning hierarchies while solving an MDP is a much harder problem than solving the flat MDP or solving the hierarchical MDP. The authors leverage the compositionality of optimal controls of the LMDP framework to learn incrementally the hierarchies. Surprisingly, the proposed method not only learns those hierarchies, but also is more effective in terms of exploration.

On the positive side, the main idea is very interesting and has a lot of potential. The authors combine many techniques under the powerful framework of LMDPs such as hierarchical RL, low-rank factorization, and count-based exploration. The authors do a good job describing their approach (at a higher level).

On the negative side, the paper looks a bit incremental, given the prior existing work. I also found the paper unclear in many aspects lacking some relevant technical details (see below). The narrative is sometimes superficial or focused mainly on intuitions and analogies. Overall, it is difficult to assess the significance of this work and the results give the impression of limited applicability, beyond the described toy problem.

1- First, in order to combine optimal controls, the LMDPs need to be solved for each different boundary state, i.e., do you require to solve as many LMDPs as possible states? If that is the case, I don't think it makes sens to talk about exploration/exploration trade-off, since you really need to visit all states a priori.

2- I cannot understand what is learned and what is required a priori. the authors state that if ""the multi-scale structure of the domain is known a priori, the decomposition (...) explicitly specified"". What does exactly that mean? If what the method does is an incremental version of the low-rank factorization proposed in Earle at al (2018), I think the presentation can be better described in those terms.

2- Regarding exploration/exploitation tradeoff. From section 3, it seems that authors focus on a particular spatial problem and define already some exploration choices. But this means that the choice about when a state is integrated in the current MDP is already done, so no real trade-off exists?

3- The narrative in Section 3.2 is not very rigorous. The authors just mention the computational problem to keep consistency between layers and then just argue that ""in practice"" using count-based exploration everything works. I think a more principle approach is necessary.

4- Experiments I: what do the authors means by ""exploration""? Is it just Boltzmann exploration? I can think of an exploration strategy that would choose an unseen state with probability 1 and would bring the agent to the goal in one shot.

5- Experiments II: I like the benchmark but, how does the result depend on the structure of the problem? What happens if I the rooms have very different sizes?

6- I miss some references that are very relevant to this work:

- the ICAPS paper ""Hierarchical Linearly-Solvable Markov Decision Problems"" by Jonson et al. seems to be the first proposing a hierarchical embedding of LMDPs.

- other factorization techniques exist, e.g., ""Incremental Stochastic Factorization for Online Reinforcement Learning"", Barreto et al (AAAI' 16), to uncover an MDP structure.

There are also some minor grammar mistakes:

""passive dynamics then become"" -> ""passive dynamics then becomes""
""reward function r"" -> ""reward function R""
""Howver"" -> ""However""
""the room contains"" -> ""the room that contains""
""have simply add"" -> ""have simply added""
""spacial and temporal"" -> ""spatial and temporal""
...","The sentiment of the review is mixed, with both positive and negative aspects highlighted. The reviewer appreciates the interesting and hard problem tackled by the paper and acknowledges the potential of the main idea, giving credit to the authors for combining various techniques effectively. However, the reviewer also points out significant concerns regarding the incremental nature of the work, lack of clarity, and limited applicability of the results. Therefore, the sentiment score is moderately positive. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, even when pointing out weaknesses and making recommendations for improvement.",30,80
"The paper proposes contextual role representation which is an interesting point. 
The writing is clear and the idea is original.
Even with the interesting point, however, the performance improvement seems not enough compared to the baseline. The baseline might be carefully tuned as the authors said, but the proposed representation is supposed to improve the performance on top of the baseline.
The interpretation of the role representation is pros of the proposed model. However, it is somehow arguable, since it is subjective. 

- minor issues: 
There are typos in the notations right before Eq. (8). 
","The review starts with positive remarks about the originality and clarity of the paper, indicating a positive sentiment. However, it also points out a significant concern regarding the performance improvement, which tempers the initial positivity. Therefore, the sentiment score is moderately positive. The language used is polite and constructive, offering specific feedback without being harsh or dismissive.",40,80
"The primary contribution of this work is a dataset for action following through dialogue.  The authors collect a comparatively small dataset in terms of language but one which contains real images and dialogue. 

There are a number of aspects of the proposed approach which I found hard to follow/justify.  First off, I was unclear on the details of the collected data (e.g. average action sequence length, dialogue length, lexical types/tokens, etc).  There's a claim of 62 acts which sums both dialogue and actions with averages of 8 and 9 dialogue acts for tourist/guide implying 45 move actions?  on a 4x4 grid?  Is it safe therefore to assume that the example dialogue is therefore atypical? It's very hard to figure out based on the number of steps across the different tables what the model should be aiming for.  Also, in 2.3 does the claim that they ""successfully complete the task"" mean in the 76.74% of cases where they succeed or did they succeed in 100% of cases and then a new human eval was run afterwards which performed worse?

The primary modeling result appears to be the success of emergent language and the bold claim that humans are bad at localizing.  This doesn't feel intuitively true from the example dialogue, but the NLG system samples to appear to be quite bad which makes me worried that it's not so much that humans are bad localizers but that the model's NLU/NLG system is quite weak and maybe there's a problem with the data-collection procedure.  Additional justification and analysis would be appreciated.

As I understand the paper right now:
1. Humans talking to one another do very well on the task and achieve success very quickly. 
2. Emergent language can do better at the task though their approach is very sub-optimal (requiring 2-3x the number of steps).
3. The currently proposed NLU/NLG mechanisms are very weak and cannot produce or correctly interpret actual language.

There are many moving pieces in this paper (e.g. extracting text from images vs detections), there doesn't seem to be any pretraining of the decoder, etc which makes it very hard for me to understand what's going wrong.  The results in this paper, don't convince me that emergent language is better than natural language or that agents are better communicators than humans, but that the data-collection methodology was faulty leading to lots of failures. 

I haven't touched on the MASC aspect and how this compares to existing work on interpretable spatial relations and questions as to why various architectural choices were made though the paper would obviously benefit from that discussion as well.

I found this paper very confusing to read.  It relies heavily on 11 pages of appendices (where it puts all of the related work) and still fails to clearly explain its contributions or justify its claims.  

Minor: URLs intermittently anonymized page 12 vs 19","The sentiment of the review is generally negative, as the reviewer points out several issues with the paper, including unclear data details, weak modeling results, and a confusing presentation. The reviewer does not express any strong positive sentiments about the work. Therefore, the sentiment score is -70. The politeness of the language is relatively high, as the reviewer uses polite phrases such as 'I found hard to follow/justify,' 'I was unclear on the details,' and 'Additional justification and analysis would be appreciated.' The reviewer also provides constructive feedback without using harsh or rude language. Therefore, the politeness score is 70.",-70,70
"This paper suggests an exploration driven by uncertainty in the reward space.
In this way, the agent receives a bonus based on its squared error in reward estimation.
The resultant algorithm is then employed with DQN, where it outperforms an e-greedy baseline.

There are several things to like about this paper:
- The idea of exploration wrt uncertainty in the reward is good (well... actually it feels like it's uncertainty in *value* that is important).
- The resultant algorithm, which gives bonus to poorly-estimated rewards is a good idea.
- The algorithm does appear to outperform the basic DQN baseline on their experiments.

Unfortunately, there are several places where the paper falls down:
- The authors wrongly present prior work on efficient exploration as ""exploration in state space"" ... rather than ""reward space""... in fact prior work on provably-efficient exploration is dominated by ""exploration in value space""... and actually I think this is the one that makes sense. When you look at the analysis for something like UCRL2 this is clear, the reason we give bonus on rewards/transitions is to provide optimistic bounds on the *value function*... now for tabular methods this often degenerates to ""counts"" but for analysis with generalization this is not the case: https://arxiv.org/abs/1403.3741, https://arxiv.org/abs/1406.1853

- So this algorithm falls into a pretty common trope of algorithms of ""exploration bonus"" / UCB, except this time it is on the squared error of rewards (why not take the square root of this, so at least the algorithm is scale-invariant??)

- Once you do start looking at taking a square root as suggested (and incorporating a notion of transition uncertainty) I think this algorithm starts to fall back on something a lot more like lin-UCB *or* the sort of bonus that is naturally introduced by Thompson (posterior) sampling... for an extension of this type of idea to value-based learning maybe look at the line of work around ""randomized value functions""

- I don't think the experimental results are particularly illuminating when comparing this method to other alternatives for exploration. It might be helpful to distill the concept to simpler settings where the superiority of this method can be clearly demonstrated.

Overall, I do like the idea behind this paper... I just think that it's not fully thought through... and that actually there is better prior work in this area.
It could be that I am wrong, but in this case I think the authors need to include a comparison to existing work in the area that suggests ""exploration by uncertainty in value space""... e.g. ""deep exploration via randomized value functions""","The sentiment of the review is mixed but leans towards positive. The reviewer appreciates the idea behind the paper and acknowledges the algorithm's performance improvement over the baseline. However, they also point out several significant flaws and areas for improvement. Therefore, the sentiment score is 20. The politeness of the language is generally respectful and constructive, even when pointing out the paper's shortcomings. The reviewer uses phrases like 'I do like the idea behind this paper' and 'It could be that I am wrong,' which indicate a polite tone. Therefore, the politeness score is 60.",20,60
"-- Paper summary --

The primary goal of this paper is to investigate the suitability of BNNs for carrying out post-calibration on trained deep learning models. The results are compared to equivalent models calibrated using temperature scaling, and the proposed technique is shown to yield superior uncertainty calibration.

-- General Commentary --

The overall goal of this work is rather modest and the scope of the evaluation is limited. While not without challenges, carrying out offline calibration as a corrective measure is a simpler problem to tackle than developing well-calibrated models upfront, and limiting the comparison to just one other post-calibration method greatly narrows the overall vision such a paper should have. For instance, isn’t post-calibration more likely to result in overfitting than a model that is implicitly calibrated at training time?

I have plenty of concerns with the submission itself, listed below:

- First and foremost, the paper is full of typos and grammatical errors. I genuinely struggled to read the paper end-to-end without being continually distracted by these issues. While some mistakes may indeed be genuine, others are only there due to sheer negligence and because the authors didn’t properly check the paper before submission.

- While the overall objective of this work (i.e. improving calibration of deep models) is clearly established, the overall presentation of ideas is very muddled and I initially struggled to properly understand what’s being proposed.  A simple diagram or illustration would have clarified some of the notation at the very least.

- The sloppiness in the presentation is also manifested in other ways. For example, in Figure 1, the plots should be individually titled (‘uncalibrated’, ‘temp-scal' and ‘BNN') in order to immediately distinguish between them; instead, all this information is contained in the caption whereas it could just as easily have been added to the plot.

- As alluded to earlier, I am disappointed by the lack of scope in the paper. The experimental evaluation should have been widened to include direct comparisons against BNN models which one might expect to be slightly better-calibrated upfront. There has also been significant interest in improving the calibration of deep models by stacking different architectures in such a way that the model is implicitly calibrated at training time. Examples of such papers include ‘Adversarial Examples, Uncertainty, and Transfer Testing Robustness in Gaussian Process Hybrid Deep Networks’ (Bradshaw et al, 2017), and ‘Calibrating Deep Convolutional Gaussian Processes’ (Tran et al, 2018). The deep kernel learning schemes developed by Wilson et al. also discuss similar hybrid models. 

- With reference to the papers cited above, one possible extension the authors could consider is to use a Gaussian process for post-calibration instead of a BNN, although I suspect this may have already been investigated in the past. In any case, this warrants further discussion. 

- I can’t disentangle the two contributions listed at the bottom of Pg 2 and the top of Pg 3. There is no theoretical evaluation of the ‘alternative hypothesis’ being mentioned, and the investigation is entirely limited to the offline setting, so I’m not entirely sure what distinction the authors are trying to make here. 

- In the same section, the authors then remark that ‘Our results open new perspectives to improve the variational approximation…’ and ‘we believe our results might foster further research in…’, before proceeding to list a dozen or so papers which might be inspired by this work. However, I can’t really see how the single contribution being presented in this paper can have significant impact on the related work. I encourage the authors to substantiate their claims with more concrete examples rather than simply include vague mentions of other papers. 

- The structure and content of Section 3 is quite perplexing. Effectively, up until Equation 3, the authors are simply restating how to use VI for BNNs, with no mention whatsoever of how this fits in the storyline of model calibration. Whereas such a section should have contained novel methodology and/or intuition, the only reference to using BNNs for post-calibration is found in a small paragraph at the end of Pg 4, before immediately proceeding to the Experiments section. Once again, this makes any contributions of the paper unclear and inconclusive. Spurious comments such as the inconsequential connection to MDL further accentuate the paper’s lack of identity and focus.

- There are also some problematic technical details in this section, such as the definitive choice of using a two-layered BNN with no justification whatsoever. It is well known than plain BNNs also struggle to deliver well-calibrated outputs, and yet the authors immediately settle on a two-layered fully-connected network without stopping to consider whether some other network configuration or initialisation scheme might be more appropriate. Some introspection is later given in the experiment accompanied by Figure 2, but the analysis carried out there is just not sufficient. 

- There are some instances where the authors use text while in math mode, which gives poor formatting as exemplified by ‘conf’ in Equation 4. 

- Referring to ‘datasets’ as ‘databases’ in Section 4.1 is unusual. Some of the commentary in this subsection is also very difficult to interpret. For example, what is meant by ‘uses BNNs’? Does this mean that a BNN appears in the model being calibrated or is this referring to the BNN used to carry out calibration? The majority of these ambiguous statements could have been avoided had more care been given to checking the paper properly before submission.

- In their discussion of the results, the authors state that ‘We cannot conclude that BNNs are calibrating at the cost of losing accuracy’, which I consider to be an overly sunny view of the results. Even if minor, a dip in accuracy is observed in almost every example provided in the Experiments section, dropping as much as 3% for CIFAR-100. Given that calibration is the primary focus of this paper, it might also be worth including another metic for validating this criteria, such as the Brier score.

-- Recommendation --

Unfortunately, the material presented here is neither significant enough nor sufficiently explored to spark much interest. The overall scope of the paper is disappointingly limited, while novel ideas and design choices are poorly motivated and communicated throughout. This submission feels rushed and incomplete, and consequently well below the conference’s standards.

Pros/Cons summary:

+  The proposal yields good results in the provided experiments
-   Minor contributions that are not convincing enough
-   Muddled presentation of ideas
-   Dubious or weakly motivated design choices
-   Poorly written with plenty of typos
-   Difficult to follow","The sentiment of the review is largely negative, as evidenced by the numerous criticisms and concerns raised about the paper's scope, presentation, and technical details. The reviewer expresses disappointment and frustration with the paper's quality and completeness. Therefore, the sentiment score is -80. The politeness of the language, while critical, remains professional and avoids personal attacks. The reviewer uses phrases like 'I encourage the authors' and 'I consider' which indicate a polite tone despite the negative feedback. Thus, the politeness score is 20.",-80,20
"This paper proposes a neural network architecture PCnet for the prediction of intensive and extensive chemical properties of molecules and materials. The authors claim that the use of prior chemical knowledge such as Mulliken electronegativity, bond strength and orbital information improves prediction accuracy. While the idea of incorporating chemical domain knowledge in the interactions of an atomistic neural network is interesting in principle, this paper has severe issues ranging from presentation over the proposed approach to the results.

First and foremost, I would like to point out that the results in Table 1 are cherry-picked since the authors fail to cite neural network architectures that outperform their approach, e.g. for U0: 0.45 kcal/mol [MPNN, Gilmer et al 2017, ICML], 0.31 kcal/mol [SchNet, Schütt et al, NIPS 30, 2017], 0.26 kcal/mol [HIP-NN, Lubbers et al., JCP 148, 2018]. This is especially apparent since Figure 4 is obviously inspired by Fig. 1 in [SchNet, Schütt et al, JCP 148, 2018]. 

The authors use a variety of heuristics and approximations such as a ""charge transfer ability"", bond strength, exponential decay of distances and overlaps of atomic orbitals which are multiplied ""to mix all things up"", to arrive at the PixelChem representation which is then fed into an atomistic neural network (PCnet). Combining these chemical features in such a way is neither well-motivated, nor does it lead to an improvement in accuracy compared to state-of-the-art networks.
Even for the intensive properties (gap, HOMO, LUMO), where PCnet is supposed to have an advantage due to its use of orbital information, MPNN, SchNet and even GC and GG-NN in Table 1 outperform the proposed approach. Parameterization of the chemical features and training the PCnet end-to-end might have improved results and seems like a missed opportunity.

Further issues:
- The manuscript is riddled with typos, grammatical errors as well as confusing sentences.
- The authors claim that PCnet is applicable to periodic structures, however, this is never demonstrated. Beyond that their definition of periodic PixelChem does only include adjacent cells, while for a unique representation more cells might be required.
- The ""benefits"" listed in Section 2.3 compare selectively to previous work. E.g., invariances, uniqueness, asymmetric interactions are also fullfiled by the neural networks listed above. A comparison of the PixelChem representation to the Coulomb matrix is not sufficient here.
- The PCnet architecture uses PReLU nonlinearities. While this is fine for equilibrium predictions, for other configurations this prohibits the prediction of a smooth PES.

Overall, I believe that it is important to incorporate chemical knowledge into neural networks. However, neither the approach nor the results convince me that this has been achieved here.","The sentiment of the review is quite negative, as the reviewer points out several severe issues with the paper, including cherry-picking results, lack of motivation for the proposed approach, and numerous typos and grammatical errors. The reviewer also mentions that the results do not convincingly demonstrate the claimed improvements. Therefore, the sentiment score is -80. The politeness of the language is relatively neutral to slightly polite. While the reviewer is critical, they do not use rude or disrespectful language and acknowledge the importance of incorporating chemical knowledge into neural networks. Therefore, the politeness score is 10.",-80,10
"

The authors describe an anomaly/novelty detection method based on handcrafted features + VGG based features. 

I think the paper is out of the scope of the conference (the only part dealing with learned representations uses VG), plus it addresses a problem whose relevance is not correctly motivated. Finally, the method is quite basic, and is not compared to any state of the art method for novelty detectiobn.  

In ""... the detection of visual anomalies perceived by human observer is an open challenge… "" can you provide references of people working in this particular problem?

The review of related work seems obsolete, can you provide more recent references (in addition to ""historical"" ones). More importantly, please provide references of anomaly detection from textures
","The sentiment of the review is quite negative. The reviewer states that the paper is out of scope for the conference, questions the relevance of the problem addressed, and criticizes the method as basic and not compared to state-of-the-art methods. These points indicate a strong disapproval of the paper's content and contribution. The politeness of the language is relatively neutral. While the reviewer is critical, they do not use harsh or rude language. They provide specific feedback and ask for additional references in a straightforward manner.",-80,10
"The authors present a large synthetic dataset for 3D scenes with templated descriptions.  They then use the model of Eslami 2018 to this new domain.  The previous work appears to already introduce all the necessary mechanisms for 3D generalization from multiple viewpoints, though this work embeds language instead of a scene in the process.  Minor note: A bit more discussion on this distinction would be appreciated.  Also, it appears that the previous work includes many of the rendered scenes also present here, so the primary focus of this paper is on the use of a language encoder (not necessarily a trivial extension).

The model appears to perform well with synthetic data though very poorly with natural sentences.  This may be in part due to the very small dataset size.  It would be helpful to know how much of the performance gap is due to scaling issues (10M vs 5.6K) versus OOVs, new syntactic constructions, etc.  In particular, the results have ~two deltas of interest (NL vs SYN) and the gap in the upper bound from 0.66 to 0.91.  What do new models need to be able to handle to close these gaps?

Regarding the upper bound, there is some discussion that annotators might have had a strict definition of a perfect match.  Were annotators asked about this? The current examples (B.2), as the authors note, are more indicative of failings with the synthetic language than the human annotators.  This may again be motivation for collecting more natural language which would resolve some of the ambiguity and pragmatics of the synthetic dataset.

It would also be helpful to have some ablations included in this work. The most obvious being the role of $n$ (number of scene perspectives).  How crucial is it that the model has access to 9 of 10 perspectives?  One would hope that given the limited set of objects and colors, the model would perform well with far fewer examples per scene, learning to generalize across examples.

Since the primary contributions of the paper are a language dataset and a language encoder for the existing model of Eslami 2018, those should be discussed and ablated in the paper rather than relegated to the appendix.

Minor note:  the related work mentions grounding graphs which are core to work from Tellex and Roy, but omits existing fully neural end-to-end models in grounding (e.g. referring expressions work).
","The sentiment of the review appears to be neutral to slightly positive. The reviewer acknowledges the contributions of the paper, such as the introduction of a large synthetic dataset and the use of a language encoder, but also points out several areas for improvement and additional discussion. The sentiment score is therefore set at 20. The politeness of the language is quite high; the reviewer uses polite language and constructive criticism throughout the review, making suggestions for improvement without being harsh or dismissive. The politeness score is set at 80.",20,80
"The authors consider the few-shot / meta-learning scenario in which the test set of interest is drawn from a different distribution from the training set. This scenario is well-motivated by the ""researcher example"" given throughout the paper. The authors assume access to a large unlabelled set in test (target) domain, and a large labelled (few-shot) set in the source domain. Thus, the paper is concerned with unsupervised version of the meta-learning problem under domain shift (i.e., a large amount of data unlabelled are available from the target domain).

The key idea is to learn a mapping from the source domain to the target domain. This mapping is learned jointly with the meta-learner, who performs the meta-learning in the target domain, on examples from the labelled domain. In practice however, it appears from the experimental section that the domain mapping is learned offline, and then frozen for the meta-learning phase.  Thus, at test time, given examples from the target domain, the meta-learner can perform few-shot learning.

Pros:
- The paper addresses an important scenario which has not been addressed to this point: namely, meta-learning without the assumption that the train and test sets are drawn from the same domain/distribution.
- The authors propose a novel task and experimental framework for considering their method, and show (somewhat unsurprisingly) that their method outperforms standard meta-learning methods that do not properly account for domain shift.
- The paper reads well and is easy to follow.

Cons:
- My main concern is reproducibility: the authors employ a number of large architectures, complex loss functions, and regularizers / ""additional improvements"". Further, there a number of experimental details that need to be further elaborated upon. e.g., architectures and hyper-parameters used, and training procedures (I encourage the authors to utilize the appendices for this). It is unclear to me how difficult/easy these results would be to reproduce. Do the authors intend to release code for their implementations and experiments?
- Some assumptions are not explicitly stated. In particular, it is unclear what the assumption on the size of the unlabelled test set is. This is also lacking from the description of the experimental protocol, which does not address the data-splits (how many classes were used for each) and size of the unlabelled test set.
- While the method is presented as jointly learning all the components, in the experimental section it is stated that the embedding network (the meta-learner) and the GAN-based domain adaptation are done separately. Can the authors comment on this further? Is this different from first learning a image translation mapping (using the unlabelled data in the target domain), and then applying existing meta-learning models/algorithms to the labelled data in the target domain?
- The overall method seems to be not very principled, and requires a lot of ""tweaks and tunes"", with additional losses and regularizers, to work.

Overall, the paper proposes a method combining a number of existing useful works (prototypical networks for meta-learning and image-to-image translation for domain adaptation) to tackle an important problem setting that is not currently addressed in existing meta-learning research. Further, it establishes a useful experimental benchmark for this task, and provides what appear to be reasonable results (though this is somewhat difficult to judge due to the lack of baseline approaches). Hopefully, such a benchmark will inspire more researchers to explore this setting, and perhaps propose simpler, more principled approaches to perform this task. It is my impression that, if the authors elaborate on the experimental protocol and implementation details, this paper would be a good fit for the venue.
","The sentiment of the review is generally positive, as the reviewer acknowledges the importance of the problem addressed by the paper, the novelty of the task and experimental framework, and the clarity of the writing. However, the reviewer also raises several concerns, particularly about reproducibility, assumptions, and the principled nature of the method. These concerns are presented constructively, with suggestions for improvement. The politeness of the language is high, as the reviewer uses polite and respectful language throughout the review, even when pointing out the paper's weaknesses.",60,90
"The work does an analysis of impact of different pooling strategies on image classification with deformations. It shows different pooling strategies reach to similar levels of deformation stability after sufficient training. It also offers an alternative technique with smoothness filters with to CNNs more stable. 
Pros:
The paper considers a wide variety of pooling strategies and deformation techniques for evaluation.  Fair experiments with conclusion of similar stability of different pool layers after training is very evident.
Cons:
i) Results on CIFAR 10 show pooling has little effect but is it unnecessary for harder problems as well? What about human pose datasets where deformation is inherent?
iii) Although, the results presented on smoother filter initialization are interesting, but these are results are not compared in a one to one setting to different pooling methods, convolutions or residual networks. 

This paper tries to argue that pooling is unnecessary for deformation invariance, as title suggests, and proposes initialization based on smooth filters as an alternative. Results are presented on CIFAR 10 to show the same, albeit on a trained network. However, CIFAR 10 is not a difficult dataset and the level of cosine sensitivity (shown as same with and without pooling) could very well be a steady state for the specific classification task. Imagenet dataset doesn't seem to show ablative studies. So this little evidence is insufficient to conclude that pooling is unnecessary.  Also as mentioned in the conclusion of the paper, the effect of pooling through the course of training would add more weight. ","The sentiment of the review is mixed but leans towards the positive side. The reviewer acknowledges the strengths of the paper, such as the consideration of a wide variety of pooling strategies and fair experiments, which suggests a positive sentiment. However, the reviewer also raises significant concerns and questions about the applicability of the results to more complex datasets and the lack of one-to-one comparisons, which tempers the overall positivity. Therefore, the sentiment score is 20. The politeness of the language is quite high; the reviewer uses polite and constructive language throughout, even when pointing out the cons and areas for improvement. Therefore, the politeness score is 80.",20,80
"
This paper focuses on the reduction of training time by various mechanisms. By introducing a time gate during training, it controls when a neuron (weights?) can be updated during training. By introducing and additional budget term in the loss function, training costs (number of computations) are reduced by one order of magnitude. 
A major advantage of the newly introduced Gaussian-gated LSTM (g-LSTM; I suggest using a capital G for Gauss, e.g., GgLSTM).

Experiments are carried out on the adding-problem from 1997; the sequential MNIST and the sequential CIFAR-10 problem. In all experiments, g-LSTM converges faster. A few things would be of interest:
- clearly state the stopping criterium for training. Especially, I would still be interested to see, how Fig. 3d continues; it seems that the network begins to collapse (also a and be are interesting to see).
The ""This work"" in Table 2 is confusing; I would expect it to appear behind g-LSTM; 
It appears that in the budgeted g-LSTM some units are not used at all (Figure 5b); Please comment on that.

In general, the paper makes the impression that it is overselling the contribution a bit too much. It would be nice to question the outcomes more and investigate the g-LSTM for the existence of possible problems which might be introduced by the omission of computations.","The sentiment of the review appears to be slightly positive, as the reviewer acknowledges the advantages and faster convergence of the g-LSTM model. However, there are also several critical points and suggestions for improvement, indicating that the reviewer sees room for enhancement. Therefore, the sentiment score is 20. The politeness of the language is generally respectful and constructive, with suggestions and comments phrased in a way that aims to help the authors improve their work. Thus, the politeness score is 80.",20,80
"This paper presents ODD, a method that rejects incorrectly labeled / noisy examples from training on the fly. The motivation is sound, that with the capacity of modern neural networks, it's easy to memorize the mislabeled data and thus hurt generalization. If we could reject such mislabeled data, we may be able to get a more generalizable model. The authors made an observation that when training with large learning rate, examples with correct labeling and incorrect labeling exhibits different loss distributions. The authors further noticed that the loss distribution of incorrectly labeled examples can be simulated using eq.(1). Therefore, by setting a threshold that corresponds to a percentile of the incorrectly labeled loss distribution, the authors are able to reject incorrect examples.

Some comments:
1. Eq.(1) basically assumes all the noise is uniformly distributed among classes. What if only 2 classes are easily mislabeled while others are fine?
2. Section 4.1.3 and Section 4.4 Sensitivity to Noise are confusing. Please clarify the importance and rationale for such analysis.
3. Cosine schedule is used in the experiments. However, since the method does not work well with small learning rate, why not using a fixed large learning rate and decrease it after noise rejection? Also, in section 4.4 Sensitivity to E, the analysis of the sensitivity to the number of epochs is coupled with a changing learning rate. It would be better to see an experiment with the two decoupled.
4. The loss of an example is averaged over h epochs. It will better to clarify how the simulated distribution generated in such case since the distribution is dependent on fc(.), which is changed between two epochs.
5. Except for the first experiment, all other experiments are only compared with ERM, the vanilla algorithm. It would be better to show a comparison with other methods.
6. Please show a precision/recall of the examples that are marked as ""noise"" by the method.
7. I assume this method will remove a lot of hard examples. How does this affect training? Does this make the network more error-prone to harder instances?","The sentiment of the review is generally positive, as the reviewer acknowledges the sound motivation and the potential benefits of the proposed method. However, the review also includes several critical comments and suggestions for improvement, which slightly temper the overall positive sentiment. Therefore, the sentiment score is 40. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, making suggestions and asking clarifying questions without being rude or dismissive. Therefore, the politeness score is 80.",40,80
"Summary: The paper considers a variational inference strategy for learning neural networks with binary weights. In particular, the paper proposes using a structured recognition model to parameterise the variational distribution, which couples the weights in different layers/filters in a non-trivial way. The gradient of the expected likelihood term in the variational lower bound is estimated using the REINFORCE estimator. This paper adjusts this estimator to use the gradient of the log-likelihood wrt the samples. Experiments on several image classification tasks are provided.

evaluation:

pros:
- the idea of the proposed approach is interesting: using variational inference for binary weight neural networks. While recent work on VI for discrete variables only focused on discrete latent variable models, this work shows how VI can be used for binary neural networks.
 
cons:
- the writing, in my opinion, needs to be improved [see my comments below]. The VI presentation is cluttered and the justification of using the pseudo-reward for reinforce is not clear.
- the experimental results are mixed and it's not clear to me how to interpret them/compare to the baselines -- what is the goal here: computational efficiency, compression or accuracy?

Some specific questions/comments:

+ What is the input of the policy/recognition network? It's not clear from the paper whether this includes the inputs of the current batch or outputs or both? If so, how are variable batch sizes handled? What is the input to this network at test time? In contrast to generative models/VAEs, the weights here are global parameters and it's not clear to me these should be varied for different data batches.

+ related to the question above: how is prediction handled at test time? Say the parameters of the variational distribution over weights are generated using the recognition network, then 100 weights are sampled given these parameters which then give 100 predictions -- should these be then averaged out to get the final prediction? I'm not quite sure I understand why the paper chose to *pick the best one* out of 100 predictions and the justification/criterion for this procedure.  

+ The writing is not very clear at places, and it does not help that the references being merged with the text. I'm also not sure about some of the technical jargons/terms used in the papers:
- reinforcement learning: is this really a reinforcement learning problem? If you tackle this problem from a pure variational perspective, reinforce is used to obtain the gradient of the expected log-likelihood wrt the variational parameters. But instead of using the log likelihood, a learning signal that depends on the gradient of the log-likelihood is used.
- concrete weights -- what are these? I assume they are just binary weights sampled from the variational approximation.
- middle of page 3: p(w|X, Y) = p_\theta(w): this is not precise as p_\theta(w) is only an approximation to the exact posterior, which then allows us to lower bound the log marginal likelihood. ""common practice in modern variational approximation"": This is the standard way of deriving the lower bound and has been used for many years.

+ the reinforce estimator tends to have high variances since it does not make use of the gradient of the function in the expectation. This paper adjusts the vanilla estimator with a learning signal that involves the gradient. Could you comment on the bias/variance trade-off of the resulting estimator? Much of recent literature on learning discrete variables, as far as I understand, propose ways to not to have to use the vanilla reinforce, for example Concrete, Relax or rebar, albeit the focus on latent variable models.

+ model selection and uncertainty measure: the paper mentions these potential advantages of the proposed approach over deterministic binarisation schemes, but does not fully explore and test these.

","The sentiment of the review is mixed. The reviewer acknowledges the interesting idea behind the proposed approach and its novelty in applying variational inference to binary weight neural networks, which is a positive aspect. However, the reviewer also points out several significant issues, such as unclear writing, mixed experimental results, and lack of clarity in the methodology and technical terms used. Therefore, the sentiment score is slightly positive but not overwhelmingly so. The language used in the review is generally polite and constructive, offering specific suggestions for improvement without being rude or dismissive.",20,80
"This paper proposes a neural model for synthesizing instrument sounds, using an architecture based on the WaveNet and DeepVoice models. The model generates raw waveforms conditioned on a piano roll representation of aligned MIDI input.

My biggest gripe with this work is that the model is trained entirely on a synthetic dataset generated from a sample-based synthesizer using a sound font. I feel that this defeats the purpose, as it will never work better than just sampling the original sound library. One potential argument in favour would be to save storage space, but the sound font used for the work is only ~140 MB, which is not prohibitive these days (indeed, many neural models require a comparable amount of storage).

It would be much more interesting to train the model on real instrument recordings, because then it could capture all the nuances of the instruments that sample-based synthesizers cannot replicate. As it stands, all the model has to do is reproduce a fixed (and fairly small) set of audio samples. This is arguably a much simpler task, which could also explain why reducing the model size (SynthNet's depthwise convolutions have many fewer parameters than the regular convolutions used in WaveNet and DeepVoice) works so well here.

That said, I think the proposed architectural modifications for raw audio models could be interesting and should be tested for other, more challenging tasks. The proposed RMSE-CQT error measure is potentially quite valuable for music generation research, and its correlation with MOS scores is promising (but this should also be tested on more realistic audio).

The fact that the models were trained to convergence on only 9 minutes of data per instrument is also impressive, despite the limitations of the dataset. The use of dithering to reduce perceptual noise is also interesting and some comparison experiments there would have been interesting, especially to corroborate the claim that it is critical for the learning process.

I think the paper slightly overstates its contributions in terms of providing insight into the representations that are learned in generative convolutional models. The Gram matrix projections showing that the activations of different layers diverge for different input types as we advance through the model is not particularly surprising, and similar plots could probably be made for almost any residual model.

Overall, I feel the work has some fundamental flaws, mostly stemming from the dataset that was used.



Miscellany:

- In the abstract: ""is substantially better in quality"", compared to what?

- In the introduction, it is mentioned that words in a speech signal cannot overlap, but notes in a musical signal can. I would argue that these are not comparable abstractions though, words themselves are composed of a sequence of phonemes, which are probably a better point of comparison (and phonemes, while they don't tend to overlap, can affect neighbouring phonemes in various ways). That said, I appreciate that this is probably quite subjective.

- Overall, the formulation of paragraph 2 of the introduction is a bit unusual, I think the same things are said in a much better way in Section 3.

- ""Conditioning Deep Generative Raw Audio Models for Structured Automatic Music"" by Manzelli et al. (2018) also proposes a MIDI-conditional neural audio generation model, trained on real instrument recordings from the MusicNet dataset. I think this is a very relevant reference.

- In the contributions of the paper, it is stated that ""the generated audio is practically identical to ground truth as can be seen in Figure 4"" but the CQTs in this figure are visibly different.

- I don't think it is fair to directly compare this setup to Engel et al. (2017) and Mor et al. (2018) as is done in the last paragraph of Section 2, as these are simply different tasks (mapping from audio to audio as opposed to generating audio).

- At the start of Section 3.1 it would be good to explicitly mention whether 8-bit mu-law audio is used, to explain why the waveform is 256-valued.

- Why is the conditioning causal? It does not need to be, as the piano roll is fully available in advance of the audio generation. I guess one argument in favour would be to enable real-time generation, but it would still be good to compare causal and non-causal conditioning.

- Since the piano roll representation is binary, does that mean MIDI velocity is not captured in the conditioning signal? It would probably be useful for the model to provide this information, so it can capture the differences in timbre between different velocities.

- The use of a MIDI prediction loss to regularise the conditioning part of the model is interesting, but I would have liked to see a comparison experiment (with/without).

- In Section 4.3, specify the unit, i.e. ""Delta < 1 second"".

- For the task of recreating synthetic audio samples, the WaveNet models seem to be quite large. As far as I can tell the size hyperparameters were chosen based on the literature, but the inherited parameters were originally optimised for different tasks.

- In Section 4.3 under ""global conditioning"", the benchmark is said to be between DeepVoice L26 and SynthNet L24, but Table 4 lists DeepVoice L26 and SynthNet L26, which version was actually used?","The sentiment score is derived from the overall tone and content of the review. The reviewer acknowledges some positive aspects of the work, such as the architectural modifications and the RMSE-CQT error measure, but also highlights significant flaws, particularly with the dataset used. This mixed feedback results in a sentiment score of -20, indicating a slightly negative sentiment. The politeness score is based on the language used throughout the review. The reviewer uses polite language, even when pointing out flaws, and offers constructive criticism and suggestions for improvement. This results in a politeness score of 80.",-20,80
"Summary:
This paper proposes training VAEs with discrete latent variables by importance sampling the expected log likelihood (ELL) term in the ELBO, which is the problematic term since it is not amenable to reparametrization gradients.  For the importance sampling distribution, they choose the variational distribution itself, making the ELL gradient E[(d q(z|x) / d \theta) \log p(x|z) / q(z|x)].  Experiments are reported for MNIST and Fashion-MNIST using Bernoulli and categorical latent variables.

Critique:
The gradient estimator the paper proposes is the REINFORCE estimator [Williams, ML 1992] re-derived through importance sampling.  The equivalence can be seen just by expanding the derivative of log q in REINFORCE: E[log p(x|z) d log q(z|x)] = E[ (log p(x|z) / q(z|x)) d q(z|x) ], which is the exact estimator the paper proposes.  REINFORCE has been previously used for variational inference [Paisley et al., ICML 2012; Ranganath et al, AISTATS 2014] and deep generative models [Mnih & Gregor; ICML 2014] and recently extended for various control variates [Tucker et al., NIPS 2017].   The equivalence would not be exact if the authors chose the importance distribution to be different than the variational approximation q(z|x), so there still may be room for novelty in their proposal, but in the current draft only q(z|x) is considered.  

Conclusion: Due to lack of novelty, I recommend rejection.


Miscellaneous points:
“...there exist no simple solutions to circumvent this problem.”  The Gumbel-softmax trick is fairly simple (although an approximation) [Jang et al., ICLR 2017; Maddison et al., ICLR 2017]. 

“...after training q(z|x) is a very good approximation to the true posterior p(z|x).”  That’s not necessarily true.  

Equation #2 should be just equal to Equation #1.

“Kingma & Welling (2013) proposed to minimize L(\theta) using stochastic gradient descent on a training set...”. First uses of stochastic gradient for VI were [Sato, NC 2001; Platt et al., NIPS 2008; Hoffman et al., JMLR 2013].  Kingma & Welling [ICLR  2014] were the first to introduce reparameterized stochastic gradients.

Before Equation #11, the reference to Equation #4 should be to Equation #5.

“...the weighting...depends only on \theta_D and not on \theta_E” (p 4). D and E should be switched.","The sentiment of the review is negative, as the reviewer recommends rejection due to a lack of novelty. The critique points out that the proposed method is essentially a re-derivation of an existing method (REINFORCE) and highlights several inaccuracies and omissions in the paper. The politeness of the language is relatively high, as the reviewer uses formal and respectful language throughout the critique, even while pointing out flaws and suggesting corrections.",-80,80
"This paper aims for open-domain question answering with distant supervision. First, the authors proposed an aggregation-based openQA model with sentence discriminator and sentence reader. Second, they use a semantic labeler to handle distant supervision problem by utilizing other span supervision tasks, and propose two different denoising methods. They run experiments on 3 open-domain QA datasets and achieve SOTA.


Strengths

1) Their semantic labeler and exploration of two different denoising methods are interesting and meaningful.
2) They conducted experiments on 3 widely-used open-domain datasets, and the performance gain is impressive.


Weakness

Although there is an impressive performance gain, the contribution of the paper seems to be marginal.
1) First of all, it is hard to say there is a contribution to the idea of sentence discriminator and sentence reader — people have used this framework for large-scale QA a lot. Also, the architecture of the models in this paper are almost identical to Chen et al (ACL 2017) and Lin et al (ACL 2018).
2) Thus, the contribution is more on semantic labeler and denoising method. However, this contribution is marginal as well since its role is almost the same as sentence discriminator plus pretraining methods which have widely used already.


Questions

1) What exactly is the difference between semantic labeler and sentence discriminator? For me, it seems like both of them label each sentence `yes` or `no`. My thought is sentence discriminator is only trained on the target dataset (distant supervision dataset) while semantic labeler is also trained (either jointly or separately) trained on the source dataset (span supervision dataset). (If my thought is wrong, please let me know, I would like to update my score.)
2) Chen et al (ACL 2017) have shown that pretraining QA model on span supervision dataset (SQuAD) is effective to train the model on distant supervision dataset. Similarly, Min et al (ACL 2018) have pretrained both QA model and sentence selector on SQuAD. While I think pretraining sentence selector on SQuAD is almost identical to sentence labeler with SSL method, could you give exact comparison of these different methods? For example, remove sentence labeler, and pretrain both sentence discriminator and reader on SQuAD, or jointly train them on SQuAD & target dataset.


Marginal comments

1) At the beginning of Section 2.4.1, it says the semantic labeler is able to transfer knowledge from the span supervised data — however, the authors should be careful since people usually refers to `knowledge` as an external knowledge. This method is more like better learning of accurate sentence selection, not transferring knowledge.
2) Please mention the TriviaQA data you used is Wikipedia domain, since there are two different domains (Wikipedia and Web).
3) In References section, the conference venues in many papers are omitted.


Overall comments

The paper explored several different methods to deal with distant supervision via sentence labeling, and I really appreciate their efforts. While the result is impressive, the idea in the paper is similar to the methods that have widely used already.","The sentiment of the review is mixed. The reviewer acknowledges the strengths of the paper, such as the interesting and meaningful semantic labeler and denoising methods, as well as the impressive performance gains on three datasets. However, the reviewer also points out significant weaknesses, including the marginal contribution of the paper and the similarity to existing work. Therefore, the sentiment score is slightly positive at 20. The politeness of the language used in the review is high. The reviewer uses polite phrases such as 'I really appreciate their efforts' and 'please let me know,' and provides constructive feedback without being harsh or dismissive. Therefore, the politeness score is 80.",20,80
"This paper points out a important issue in current continual learning literature: Due to the different settings and different evaluation protocols of each method, comparison between methods are usually not fair, and lead to distinct conclusions.
The paper is in general easy to understand except a few drawbacks listed in the cons.

Pros:
1. This paper investigates an important problem, aka, how does the methods compare to each other with the same evaluation protocol.
2. Experiments are performed on the previous methods, which could be used as a baseline for future works in this field.
3. Proposes to combine discriminative model with generative model to save computation when using generative model to store rehearsal examples.

Cons:
1. Details of each experiments are missing. 
Different methods are evaluated under the ""incremental task learning"", ""incremental domain learning"",  ""incremental class learning"" settings. However, to my knowledge, some of the methods will not work under all of the three settings, as the author also suggest that XdG only works with task id. However, I think there are a few more. For example, the LwF methods has multiple sets of output neurons, which implicitly assumes the task id is known. It is not described in the paper how to evaluate it under ""incremental domain learning"", aka, how to decide which set of output to use if task id is not available during testing. Another example, the results in table 3 and 4 indicates that EWC with task id is better than without. However, original EWC does not take task id during testing, it is not described how to introduce dependency on the task id for EWC.
2. Using the term feedback connection is misleading to the reader since the described method is just using an encoder/decoder structure. In my opinion this is different from feedback connection in which higher layer is an input for lower layers. Autoencoder or encoder/decoder structure is more appropriate.
3. There is some contribution in the RtF part, namely the saved computation compared to DGR. However, subjectively, I think this contribution is not very significant. The same thing can be achieved with DGR by sharing the network between the discriminative model and the discriminator in GAN. In my opinion this is more a design bonus in using generative replay than a major methodology innovation.

Conclusion:
The first part that compares different methods is worth publishing given more details are provided. I'm more than happy to give a higher score if the authors are able to provide more details and the details are reasonable.","The sentiment of the review is generally positive, as the reviewer acknowledges the importance of the problem addressed by the paper and appreciates the efforts made in the experiments. However, the reviewer also points out several significant drawbacks and areas for improvement, which slightly tempers the overall positive sentiment. Therefore, the sentiment score is 40. The politeness of the language used is high, as the reviewer provides constructive criticism and offers suggestions for improvement without being harsh or dismissive. The reviewer also expresses willingness to give a higher score if the authors address the concerns, indicating a polite and supportive tone. Therefore, the politeness score is 80.",40,80
"This paper sets out to build good bilingual word alignments from the information in an NMT system (both Transformer and RNN), where the goal is to match human-generated word-alignments as measured by AER. At least that’s how it starts. They contribute two aligners: one supervised aligner that uses NMT source and target representations as features and is trained on silver data generated by FastAlign, and one interpretability-based aligner that scores the affinity of a source-target word-pair by deleting the source word (replacing its embedding with a 0-vector) and measuring the impact on the probability of the target word. These are both shown to outperform directly extracting alignments from attention matrices by large margins. Despite the supervised aligner getting better AER, the authors proceed to quickly discard it as they dive deep on the interpretability approach, applying it also to target-target word pairs, and drawing somewhat interesting conclusions about two classes of target words: those that depend most of source context and those that depend most on target context.

Ultimately, this paper’s main contribution is its subtraction-based method for doing model interpretation. Its secondary contributions are the idea of evaluating this interpretation method empirically using human-aligned sentence pairs, and the idea of using the subtraction method on target-target pairs. The conclusion does a good job of emphasizing these contributions, but the abstract and front-matter do not. Much of the rest of the paper feels like a distraction. Overall, I believe the contributions listed above are valuable, novel and worth publishing. I can imagine using this paper’s techniques and ideas in my own research.

Specific concerns:

The front-matter mentions ‘multiple attention layers’. It would probably be a good idea to define this term carefully, as there are lots of things that could fit: multiple decoder layers with distinct attentions, multi-headed attention, etc.

In contrast to what is said in the introduction, GNMT as described in the Wu et al. 2016 paper only calculates attention once, based on the top encoder layer and the bottom decoder layer, so it doesn’t fit any definition of multiple attention layers.

Equation (1) and the following text use the variable L without defining it.

‘dominative’ -> ‘dominant’

Is there any way to generate a null alignment with Equation 3? That is, a target word that has no aligned source words? If not, that is a major advantage for FastAlign.

Similarly, what exactly are you evaluating when you evaluate FastAlign? Are you doing the standard tricks from the phrase-based days, and generating source->target and target->source models, and combining their alignments with grow-diag-final? If so, you could apply the same tricks to the NMT system to help even the playing field. Maybe this isn’t that important since the paper didn’t win up being about how to build the best possible word aligner from NMT (which I think is for the best).

I found Equations (7) and (8) to be confusing and distracting. I understand that you were inspired by Zintgraf’s method, but the subtraction-based method you landed on doesn’t seem to have much to do with the original Zintgraf et al. approach (and your method is much easier to the understand in the context of NMT than theirs). Likewise, I do not understand why you state, “we take the uniform distribution as P(x) regarding equation 8 for simplicity” - equation 9 completely redefines the LHS of equation 8, with no sum over x and no uniform distribution in sight.

The Data section of 4.1 never describes the NIST 2005 hand-aligned dataset.

The conclusions drawn at the end of 4.4 based on ‘translation recall’ are too strong. What we see is that the Transformer outperforms Moses by 2.8 onCFS, and by 3.7 on CFT. This hardly seems to support a claim that CFT words are the reason why Transformer yields better translation.

4.5 paragraph 1: there is no way to sample 12000 datasets without replacement from NIST 2005 and have the samples be the same size as NIST 2005. You must mean “with replacement”?","The sentiment of the review is generally positive, as the reviewer acknowledges the valuable and novel contributions of the paper and expresses interest in using the techniques in their own research. However, there are also several critical points and suggestions for improvement, which slightly temper the overall positivity. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, even when pointing out issues and making recommendations. The reviewer avoids harsh or rude language and maintains a professional tone, resulting in a politeness score of 80.",60,80
"This paper unifies both classification and regression task based on the polar prototype network. For classification, the prototypes for all classes are chosen in advance based on a max-margin principle, while the embedding of all instances is then optimized to have small cosine distance to assigned prototypes. For the regression, the output value is interpolated between the two prototypes. Experiments on classification, regression, and combined tasks show the method can achieve good results.

The idea of using the prototype and the polar system is interesting, and the whole paper is well-written. However, there are still some problems and questions about this paper.
1. There are two problems with using the max-margin prototypes. First, to maximize the smallest distance between two prototypes, the authors use MC or evolutionary algorithms to do the optimization, which may be time-consuming, and it may be extremely difficult when the prototype space is high dimension. Second, the previous approach indeed obtains discriminative prototypes, but we lose the **class correlation**. In the extreme case, it is equal distance between all prototypes, but some similar classes will have a smaller prototype distance than others. For example, the prototype distance between ""cat"" and ""dog"" should not be the same as that between ""car1"" and ""car2"". The semantic consideration in the paper can solve this problem to some extent, but there needs more evidence.

Using the pre-defined prototype is also considered in the paper ""M. Perrot et al. Regressive Virtual Metric Learning. NIPS15"". 

2. For the unified output space
One main contribution is that based on the polar system, the method unifies both classification and regression tasks in the same space. We can also do this in basic embedding algorithms. In the embedding space, a method can do both classification and regression with the nearest neighbor rule (based on majority voting and average respectively). The authors should compare with such kinds of methods in the experiments.

3. Experiments
From the experiments, using semantic cannot improve a lot for the classification task. The authors can try more datasets to validate is this the common scenario. The reviewer strongly suggests the authors should compare with more methods. For example, in some papers the prototypes are learned simultaneously (Snell et al. Prototypical networks for few-shot learning. NIPS17; Wen et al. A discriminative feature learning approach
for deep face recognition. ECCV16); while in other cases, there are no prototypes as we optimize the triplet/contrastive loss directly. First, the authors can compare classification performance with these approaches; besides, some visualization results can also show the used prototypes or embeddings. 
The main advantage of the method is not stressed clearly in the experiments part. The authors can clarify it in later versions.

The final rating depends on the authors' response.","The sentiment of the review is generally positive, as the reviewer acknowledges the interesting idea and well-written nature of the paper. However, the reviewer also points out several significant issues and areas for improvement, which tempers the overall positivity. Therefore, the sentiment score is 40. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, offering suggestions and recommendations without being harsh or dismissive. Therefore, the politeness score is 80.",40,80
"# overview
In this work, Nesterov Accelerated gradient based updates are applied in a distributed fashion to scale SGD based training to multiple nodes without the introduction of further hyperparameters or having to adapt the learning rate schedule from that of single node training on the same data.

Evaluation is carried out on image classification workloads using ResNet model variants across CIFAR10,100, and ImageNet datasets, utilizing from 8-32 nodes. In contrasting test error relative to single node performance, the authors find their method degrades less than other synchronous and asynchronous SGD based approaches as node count increases.

Overall, this work is presented in a fairly clear and logical manner, and the writing is easy to follow.  However the approach described appears to be contingent on very specific worker communication patterns and timing which seem unrealistic for real-world settings (namely that each worker sends exactly one update per N sized block received).  Extrapolating from the curvature of the results shown it doesn't appear that DANA would continue to outperform other methods like ASGD once the worker count scales beyond the 32 node limit evaluated.

# pros
* no additional hyperparameter tuning required
* should be easy to drop into existing asynchronous SGD implementations, just need to modify the worker side.
* does appear to scale slightly better from an accuracy perspective in 16-32 node counts

# cons
* Biggest criticism is the assumption of block random or round-robin worker update scheduling. Presuming each worker will update master exactly once to determine future parameter position is far from realistic on real hardware (varying capacity, performance, system loads, dealing with stragglers) and should probably be considered a synchronous not asynchronous update.
* only evaluated on image classification tasks on cifar10, cifar100, imagenet on resnet-20 and resnet-50. Would have been better to evaluate on a more varied set of tasks/models/datasets

# other comments
* Figure 2 baseline performance reported is a bit misleading/confusing since it was only evaluated on a single worker. Would suggest restricting to a single point rather than some extrapolated line that seems to indicate being run on multiple-workers.
* Figure 3 should should also show multi-node speedups for the other methods compared for completeness. 
* Section 5.2 should report on percentage scaling efficiency rather than using speedup as it doesn't normalize for worker count.  For instance 16x could be interpreted as good or poor if it was achieved using 16 vs 160 nodes.
* Section 5.2 there's a small typo: GPUs -> GPU
* Consider https://arxiv.org/abs/1705.07176 in related work?","The sentiment of the review is moderately positive. The reviewer acknowledges the clarity and logical presentation of the work and highlights some strengths, such as the lack of additional hyperparameter tuning and the ease of integration into existing implementations. However, the reviewer also points out significant limitations, particularly the unrealistic assumptions about worker communication patterns and the limited evaluation scope. Therefore, the sentiment score is 30. The politeness of the language is quite high. The reviewer uses polite and constructive language throughout, even when pointing out weaknesses and making recommendations. Therefore, the politeness score is 80.",30,80
"This paper claims to propose a new iRDA method. Essentially, it is just dual averaging with \ell_1 penalty and an \ell_2 proximal term. The O(1/\sqrt{t}) rate is standard in literature. This is a clear rejection.","The sentiment score is -90 because the reviewer clearly expresses a very negative opinion about the paper, calling for a 'clear rejection' and dismissing the proposed method as standard and unoriginal. The politeness score is -70 because the language used is quite blunt and dismissive, lacking any form of constructive feedback or courteous language.",-90,-70
"In the paper, the authors proposed a new saliency map method, based on some empirical observations about the cause of noisy gradients.
Specifically, through experiments, the authors clarified that the noisy gradients are due to irrelevant information propagated in the forward pass in DNN. Because the backpropagation follows the same pass, irrelevant feature are conveyed back to the input, which results in noisy gradients.
To avoid noisy gradients, the authors proposed a new backpropagation named Rectified Gradient (RectGrad). In RectGrad, the backward pass is filtered out if the product of the forward signal and the backward signal are smaller than a threshold. The authors claim that, with this modification in backpropagation, the gradients get less noisy.
In some experiments, the authors presented that RectGrad can produce clear saliency maps.

I liked the first half of the paper: the observations that irrelevant forward passes are causing noisy gradients seem to be convincing. The experiments are designed well to support the claim.
Here, I would like to point out, that noisy gradients in occluded images may be because of the convolutional structures. Each filter in convolution layer is trained to respond to certain patterns. Because the same filter is used for each of subimages, some filters can be activated occasionally on occluded parts. I think this does not happen if the network is densely connected without convolutional structures. The trained dense connection will be optimized to remove the effects of occluded parts. Hence, for such networks, the gradient will be zeros for occluded parts.

The second half of the paper (Sec.4 and 5) are not very much convincing to me.
Below, I raise several concerns.

1. There is no justification on the definition of RectGrad: Why Rl = I(al * Rl > t) R(l+1)?
The authors presented Rl = I(al * Rl > t) R(l+1) as RectGrad, that can filter out irrelevant passes. However, there is no clear derivation of this formula: the definition suddenly appears. If the irrelevant forward passes are causes of noisy gradients, the modification Rl = I(al > t) R(l+1) seems to be more natural to me. It is also a natural extension to the ReLU backward pass Rl = I(al > 0) R(l+1). Why we need to filter out negative signals in backward pass?

2. The experimental results are less convincing: Is RectGrad truly good?
In Sec.5.2, the authors presented saliency maps only on a few images, and claimed that they look nicely. However, it is not clear that those ""nicely looking"" saliency map are truly good ones. I expect the authors to put much efforts on quantitative comparisons rather than qualitative comparisons, so that we can understand that those ""nicely looking"" saliency maps are truly good ones.
Sec.5.3 presents some quantitative comparisons, however, the reported Sensitivity and ROAR/KAR on RectGrad are not significant. The authors mentioned that this may be because of the sparsity of RectGrad. However, if the sparsity is the harm, the underlying observations of RectGrad may have some errors. I think the current manuscript has an inconsistency between the fundamental idea (based on empirical observations) and the performance of RectGrad.

[Minor Concern]
In Sec.5, the authors frequently refer to the figures in appendix. I think the main body of the paper should be self-contatined. I therefore think that some of the figures related to main results should appear in the main part.

","The sentiment of the review is mixed. The reviewer appreciates the first half of the paper, indicating that the observations and experiments are convincing, which suggests a positive sentiment. However, the second half of the paper is criticized for lacking justification and convincing experimental results, which brings the overall sentiment down. Therefore, the sentiment score is around -20. The politeness of the language is quite high. The reviewer uses polite language throughout, such as 'I liked,' 'I would like to point out,' and 'I expect the authors to,' which indicates a respectful and constructive tone. Therefore, the politeness score is around 80.",-20,80
"This paper presents a new method for fully- and semi-supervised few-shot classification that is based on learning a general embedding as usual, and then learning a sub-space of it for each class. A query point is then classified as the class whose sub-space is closest to it.

Pros: This is a neat idea and achieves competitive results. Learning a sub-space per class makes intuitive sense to me since it’s plausible that there is a lower-dimensional subspace of the overall embedding space that captures the properties that are common to only examples of a certain class. If this is indeed the case, it seems that indeed classifying query examples into classes based on their distances from the corresponding sub-spaces would lead to good discrimination. 

Cons: First, an inherent limitation is that this approach is not applicable to one-shot learning, and I have doubts in its merit for very low shot learning (explained below). Second, I’m missing the justification behind a key point used to motivate the approach, which requires clarification (explained below). Third, I feel that certain aspects of the approach were unclear (details to follow). Finally, I feel more analysis is needed to better understand the differences of this method from previous work (concrete suggestions follow). For semi-supervised learning, the novelty regarding how the unlabeled examples are incorporated is limited, as the approach used is previously-introduced in Ren et al, 2018.

Overall, even though I like the idea and the results are good, there are a few points, mentioned in the above section that I feel require additional work before I can strongly recommend acceptance. Most importantly, relating to getting more intuition about why and when this works best, and tying it in better with previous approaches. 

A key point requiring clarification.
There is a key fact that the authors used to motivate this approach which remains unclear to me: why is it the case that this approach is less sensitive to outliers than previous approaches? In Figure 1, an outlier is pictured in each of subfigures (a) and (b) corresponding to Matching and Prototypical Networks, but not in subfigure (c) which corresponds to PSN. No explanation is provided to justify this conjecture, other than empirical evaluation that is based on the overall accuracy only. In particular, since SVD is used to obtain the sub-spaces, instead of an end-to-end learned projector that directly optimizes the query set accuracy, it’s not clear why if a support point is an outlier it would not affect the sub-space creation. If I’m missing something, please clarify!

(A) Comments on the approach.
(1) Why define X_k as the support set examples minus the class prototype instead of just the support examples themselves? The latter seems simpler, and should have all the required information for shaping the class’ subspace.
(2) Note that if X_k is defined as [x_{k,1}, \dots, x_{k,K}] as proposed in the above point (ie. without subtracting the class mean from each support point) then this method would have been applicable to 1-shot too. How would it then compare to a 1-shot Prototypical Network? Notice that in this case the mean of the class is equal to this one example.
(3) In general, the truncated SVD decomposition for a class can be written using the matrices U, \Sigma and V^T with dimensions [D, n], [n, n] and [n, K] respectively, where D is the embedding dimensionality and K is the number of support points belonging to the given class. The middle matrix \Sigma in the non-truncated version would have dimensions [D, K]. Does this mean that when truncating, n is enforced to be smaller than each of D and K? This would mean that the dimensionality n of the sub-space is limited by the number of the support examples, which in some cases may be very small in few-shot learning. Can you comment on this?
(4) How to set n (the dimensionality of each subspace) is not obvious. What values were explored? Is there a sweet spot in the trade-off between the observed complexity and the final accuracy?

(B) Comparison with Prototypical Networks.
(1) In what situations do we expect learning a sub-space per class to do better than learning a  prototype per class? For example, Figure 4 shows the test-time performance as a function of the test ‘way’. A perhaps more interesting analysis would be to compare the models’ performance as a function of the test *shot*: if more examples are available it may be less appropriate to create a prototype and more beneficial to create a sub-space? 
(2) Can we recover Prototypical Networks as a special case of PSN? If so, how? It would be neat to show under which conditions these are equivalent.

(C) Clarifications regarding the semi-supervised setup.
(1) Are distractor classes sampled from a disjoint pool of classes, or is it that, for example, a class which is a distractor in an episode is a non-distractor in another episode.
(2) Similarly for labeled / unlabaled at training time. Can the same example appear as labeled in one episode but unlabaled in another? In Ren et al, 2018, this was prevented by creating an additional labeled/unlabeled split even for the training examples. Therefore they use strictly less overall information at meta-training time than if that split weren’t used. To be comparable with them, it’s important to apply this same setup.

(D) Additional minor comments.
(1) “To work at the presence of distractors, we propose to use a fake class with zero mean”. Note that this was already proposed in Ren et al, 2018. They used a zero-mean, high-variance additional cluster whose aim was to ‘soak up’ the distractor examples to prevent them for polluting legitimate clusters (this was the second model they proposed).
(2) In the introduction, regarding contribution iii. A more appropriate way to describe this is as exploring generalization to different numbers of classes, or ‘ways’ at test time than what was used at training time.
(3) Gidaris and Komodakis (2018) is described in the related work as using a more complicated pipeline. Note however that their pipeline is in place for solving a more challenging problem than standard few-shot classification: they study how a model can maintain the ability to remember training classes while rapidly learning about new ‘test’ classes.
(4) In the last line of section 5.3, use N-way instead of K-way since in the rest of the paper K was used to refer to the shot, not the way.
","The sentiment of the review is generally positive but with significant reservations. The reviewer appreciates the novelty and competitive results of the proposed method, which is reflected in statements like 'This is a neat idea and achieves competitive results.' However, the reviewer also lists several critical points that need clarification and improvement before they can strongly recommend acceptance. Therefore, the sentiment score is moderately positive. The politeness of the language is high; the reviewer uses polite and constructive language throughout the review, such as 'I feel that certain aspects of the approach were unclear' and 'Can you comment on this?' This indicates a respectful and professional tone.",40,80
"In the paper, author investigate the use of copy mechanisms for the question generation task. It evaluates on the SQuAD dataset. The model is a popular seq2seq/encoder-decoder model with copy mechanisms using pointer networks. 

Pros:
It is well motivated. For the question generation task, a word to be predicted can be from either a global vocabulary list or copied from the given documents (location vocabulary).  There are some overlap between these two vocabulary lists.  This paper mainly investigates this issue.

It is well written and easy to follow.

Interesting analysis of human/automatic metrics.

Cons:
The tricks here are a bit of ad hoc. It is better to have a systemic study.

Baseline results are too low. E.g., officially QANet results (from the paper) on SQuAD v1 is around 82.7 (my implementation obtains 83.1). However in the paper, its best result is 72.6 in terms of F1 score. 

The authors only evaluated on one dataset. It is hard to convincing.

It is lack of comparison results of question generation in literature. 
","The sentiment of the review is moderately positive, as the reviewer acknowledges the motivation, clarity, and interesting analysis of the paper, but also points out several significant drawbacks. Therefore, the sentiment score is 30. The politeness of the language is quite high, as the reviewer uses polite and constructive language throughout the review, even when pointing out the cons. Thus, the politeness score is 80.",30,80
"This paper presents a new adversarial defense method.  I found the paper difficult to understand, but as far as I understand, the method involves randomly perturbing the pixels of images in a dataset, and retraining the classifier to correctly classify these perturbed images as well.  The perturbations are done independently per pixel by quantizing the pixel value, and then using a perceptron model to generate the full pixel value from just this quantized value.  The perceptron is trained on randomly generated data mapping quantized pixel values to full pixel values (this random generation does not use the dataset statistics at all).  They use both partially converged perceptron models as well as fully converged perceptron models. 

Pros:
	1. The defense technique does not require knowledge of the attack method

Cons:
	1. The paper is incredibly difficult to understand due to the writing.
	2. The performed experiments are insufficient to determine  whether or not their technique works because:
		a. They don't compare against any other defense techniques.  In addition to at least adversarial training, I would like to also see comparisons to feature squeezing which I would expect to have a very similar effect.
		b. They do not show the results of an attack by an adversary that is aware of their technique.  (i.e. F(PR(A(F,PR,x))))  Many alternative defense techniques will work much better if we assume the adversary does not know about the technique.
		c. Their comparison against random noise is not an apples-to-apples comparison.  Instead of perturbing uniformly within a range, they perturb according to a normal distribution.  The random noise perturbations also have a much larger L2 distance than the perturbations from their technique.  To believe that their method is actually better than training with random noise I'd like to see an apples-to-apples comparison where these values are hyperparameter tuned as they presumably did for their method.
		d. They only show results for one value of epsilon and one value of ""# of colors"" for their technique.  Presumably if there is a mismatch between these values then the results will be much worse (i.e. if they choose a large ""# of colors""/small range per color and the attacker chooses a large epsilon).
	3. Their use of machine learning models is quite ad-hoc.  In particular they use a perceptron trained on algorithmically generated data.  And their justification for using a perceptron, instead of just using the known underlying generation algorithm is that they also use a partially converged version of the perceptron as part of their model.
	4. The authors partly deanonymize the paper through a github link
	5. The main paper is 10 pages long, and the quality of the paper does not justify the additional length.
","The sentiment of the review is generally negative. The reviewer highlights several significant issues with the paper, including its difficulty to understand, insufficient experiments, and ad-hoc use of machine learning models. The only positive point mentioned is that the defense technique does not require knowledge of the attack method. Therefore, the sentiment score is -70. The politeness of the language is relatively neutral to slightly polite. The reviewer uses formal language and provides detailed feedback without being overtly rude, but the tone is critical. Therefore, the politeness score is 10.",-70,10
"This paper tries to build a neural net to learn Nash equilibrium of games, even though it has been proved that no uncoupled algorithm can do that, except on specific games, as the ones considered in the example (0-sum, potentiel, solvable by iterated elimination of dominated strategies, etc.).

The algorithm proposed is a classical neural net without any insight (I believe its behavio must more or less be similar to regret matching).

In table 10, I do not think that the underline case is the NE. 

In table 12, the algorithm si conveniently initiated at the NE.","The sentiment of the review is quite negative, as the reviewer points out fundamental flaws in the paper's approach and questions the validity of the results. The language used is somewhat blunt and lacks politeness, as it directly criticizes the lack of insight in the algorithm and questions the correctness of the results without offering constructive feedback or suggestions for improvement.",-70,-30
" This paper proposed a so-called ISS-GAN framework for data hiding in images, which  integrates steganography and steganalysis processes in GAN. The discriminative model simulate the steganalysis process, and the steganography generative model is to generate stego image, and confuse steganalysis discriminative model. 

Overall the application seems interesting. My concern is its use in real secure information transmission systems: it can fool human eyes but what is its capacity against decoding algorithms; if the intent is to transmit some hidden information, how the receiver is supposed to decode it; is there something similar to the public key in encryption systems? These basic questions/concepts should be made clear to the reader to avoid confusion. 

The evaluation protocol should be clarified and especially on how the PSNR is calculated (i.e., using the reconstructed secret image and real one?) ","The sentiment of the review is moderately positive, as the reviewer finds the application interesting but raises some concerns about its practical use and clarity. The sentiment score is 30 because the reviewer acknowledges the potential of the work but points out significant issues that need to be addressed. The politeness score is 70 because the reviewer uses polite language and constructive criticism, asking questions and making suggestions without being rude or dismissive.",30,70
"Contributions:

The main contribution of this paper is the proposed DelibGAN for text generation. The framework introduces a coarse-to-fine generator, which contains a first-pass decoder and a second-pass decoder. Instead of using a binary classifier, the discriminator is a multiple instance discriminator. Two different variants of DelibGAN are proposed, with experiments showing that DelibGAN-II performs the best.

Strengths:

(1) Novelty: I think this paper contains some novelty inside. Using a coarse-to-fine generator is an interesting idea. However, as I noted below, the paper is not well-executed.

Weaknesses:

(1) Presentation: This paper is easy to follow, but poorly written.

First, the paper is too repetitive. For example, the two-pass decoding process has been repeatedly mentioned too many times in the paper, such as the paragraph above Eqn. (8). Please be concise. 

Second, when citing a paper, there should be a space between the word and the cited paper. For example, in the first paragraph of the introduction section, instead of ""(RNN)(Mikolov et al., 2011)"", it should be ""(RNN) (Mikolov et al., 2011)"". This should be corrected for the whole paper.

Third, in the first paragraph of the introduction section, I am not sure why (Wang & Wan, 2018a) is cited here. This is not the first paper that points out the problem. One should refer to [a], which is also not cited in the paper. 

Missing reference: I also encourage the authors citing [b] since it is directly related to this work, which is about using GAN for text generation.  

[a] Sequence Level Training with Recurrent Neural Networks, ICLR 2016
[b] Adversarial Feature Matching for Text Generation, ICML 2017

(2) Evaluation: My main concern lies in the experimental evaluation, with detailed comments listed below.  

Questions:

(1) In Algorithm 1, there exists the pretraining process of G_1 & G_2. However, it is not clear to me how this this pretraining is implemented, since the output of G_1 is not observed, but is hidden and imagined by the model. So, what is the training signal for pretraining G_1? Can the authors provide more details? Please clarify it. 

(2) In experiments, why the authors only compare with SeqGAN, SentiGAN & MaskGAN? One would naturally ask how the proposed model compare with RankGAN, TextGAN and LeakGAN. For example, what are the corresponding results of RankGAN, TextGAN & LeakGAN in Table 1 to 5? This should not be difficult to compare with, based on the availability of Texygen.

(3) Besides using the perplexity, the authors also use the novelty and diversity terms defined in (11) & (12) for evaluation. This is good. However, why not using some commonly used metrics in the literature, such as BLEU and self-BLEU? I agree these metrics are also not perfect, but it will be helpful to also report these results for benchmark purposes. 

(4) Instead of using datasets like HappyDB & SSTB, it would be helpful to also report results on some benchmark datasets such as COCO captions and EMNLP News as used in the LeakGAN paper.  What are the results looking like on these two datasets?

(5) The results in Table 1 & Figure 2 is misleading. They do not necessarily mean the proposed model is better, as the NLL value only measures how the generated sentence look like a real sentence, but it does not measure the diversity of generated sentences. For example, a model that only repeatedly produces one very realistic sentence would also achieve a very high NLL score.  

(6) Table 3 & 5 shows the human evaluation results. However, how this is performed is not clear at all. For example, is the human evaluation performed using AMT? How many samples are used for human evaluation? Also, how many workers has one sentence been evaluated by? Without all these details, how reliable this result is is questionable.

Minor issues:

(1) In Eqn. (3), since only two classes exist, there is no need to say ""softmax"", use ""sigmoid"" function is enough for illustration. 

(2) In the line below Eqn. (5), ""bias"" => ""baseline""

(3) In Eqn. (3), there is "")"" missing. 

(4) In Figure 2, there are two methods ""SentiGAN"" and ""DelibMLE"" with the same green color. Which one is which is hard to see. 

(5) In the first paragraph of related work, MaskMLE => MaskGAN.  
","The sentiment of the review is mixed, leaning towards negative. While the reviewer acknowledges the novelty of the paper, they also highlight significant weaknesses in presentation and evaluation. The sentiment score is -30 because the review contains more criticism than praise. The politeness score is 30 because the reviewer uses polite language but is direct and firm in pointing out the issues.",-30,30
"The authors of this paper compare the robustness of CNN and CapsNet to global translation on the MNIST dataset. Both models were trained on the standard training set of MNIST, and then tested on a set with digits shifting from the upper left corner to the lower right corner. The results of both models were poor. To improve it, the authors add some shifted digits to the training set, and the performances of both models were significantly enhanced. Moreover, the performance of CNN was better than that of CapsNet in the experiments. Generally speaking, the work presented in this paper is clear and straightforward. However, the work is not significant enough to publish as a ICLR paper. Below is my major comments.

1. There are lots of typos and grammatical errors everywhere in the paper. Thus, the manuscript was not well prepared.

2. It is unclear which CapsNet and what settings were used in the experiment.  

3. It is well-known that convolutional networks are good at capturing local patterns from the images, while capsule networks enhance it to consider global configurations of the local patterns, and robust to affine transformation. Obviously, the experiments presented in this manuscript is too simple. Lots of work should be done in the investigation. For example,o on the training set and shifted test set, the authors can enlarge the background and keep the digits in the original size to make it as a local pattern in the image. Will it be detected by CNNs with larger receptive fields for the images? How is it compared with CapsNets? 

4. How are both models compared on other (perhaps more complicated and larger) datasets?

In summary, the work presented here is interesting, but lots of work should be done in order to make it publishable.

","The sentiment of the review is mixed but leans towards the negative side. The reviewer acknowledges that the work is clear and straightforward, and interesting, but ultimately concludes that it is not significant enough to be published in its current form. This suggests a sentiment score of -30. The politeness of the language is generally respectful and constructive, with suggestions for improvement and no use of harsh or rude language, leading to a politeness score of 70.",-30,70
"This paper presents a GAN approach adapted for multi-modal distributions of single class data. The generator is trained to generate samples in the low density areas of the data distribution. The discriminator is training to distinguish between generated and real samples and hence is able to discriminate between normal data (=real) and anomalous data (=generated, in low density areas of the normal data). 

To force the model to map the different modes of the data, a categorical latent variable is used that represents the potential distribution modes. Both a one-hot code and a Gaussian mixture model are explored. This is not a novel approach, however, no citations are provided.

To force the generator to produce samples in the low density areas of the data distribution, a Complementary GAN is used. The authors cite OCAN [zheng18], which in turns cites [Dai, NIPS17]. This approach has the advantage that no threshold needs to be fine-tuned since the discriminator can directly be used for anomaly detection.

Constraints derived from both these goals are included in the loss function, which, in addition, includes terms to encourage diversity and similarity of the generated samples.

The model is tested on a proprietary dataset of real manufacturing product. The dimension of the data is 280 (after proprietary feature extraction). The authors compare their approach to 9 other anomaly detection methods. The reported performance is the highest. The OCAN method has similar performance. The authors specify that fine-tuning is need for all other methods (except OCAN). Fine-tuning is performed on the same data used for testing, hence providing a marked advantage. However, I do not understand why OCAN is listed in table 1 with both fine-tuning and no fine-tuning (raw). This is not explained and should be clarified. In any case, the combination of Complementary-GAN and the multi-modal latent variable seem to be very effective on this dataset. To understand whether this approach is really superior, other benchmark datasets should be tested.

The article is technically sound. The citations are generally ok, except for the missing citation related to the use of latent categorical codes for push the model into mapping multiple modes of the data. The math is reasonable, although some notations are a bit hard to follow. The English needs to be improved. There are many grammatical errors and the paper needs to be proof-read. Some errors make it hard to understand the text. In particular the adjective modal is used throughout the paper as a noun instead of 'mode'. There are also several LaTex formatting errors which lead to some gibberish and some of the figures are too small making them unusable when the paper is printed.

Overall, I think the paper is incremental, as it combines two previously published methods. It also lacks generality as only one (proprietary) dataset is used. English needs to be proof-read and formatting errors fixed.
","The sentiment of the review is mixed but leans slightly positive. The reviewer acknowledges the technical soundness of the paper and its high performance on the proprietary dataset, which suggests a sentiment score of around 20. However, the reviewer also points out several significant issues, such as the lack of novelty, missing citations, grammatical errors, and formatting problems. The politeness of the language is generally high. The reviewer uses polite and constructive language, even when pointing out flaws, which suggests a politeness score of around 80.",20,80
"In this work, the authors propose to use multiple generators to estimate the target distribution. Especially, it assumes the case that the range of generators is non-convex and the target distribution doesn't fall into it. To solve this issue, the multiple generators are convex combined to do better approximation and an incremental training process is proposed to train multiple generators one by one.

1) Using multiple generators seems reasonable based on the authors assumption (non-convex of the range of generators), but is  this assumption based on having a perfect discriminator? Could you assume a similar case for the discriminator?

2) In figure 3, it is shown each generator tries to improve the estimated target distribution. However, it is not clear what generator generates what samples. It would be better to use different colors for different generators. If I assume that the red samples are from the first generator, why the second image (top right) shows slightly shifted samples compared to the first image (top left). As far as I understand, the first generator is fixed after it is converged.

3) It is shown that the (convex) weights for generators are fixed to 1, is there any reason to fixed it?

4) On page 3, the equation in section 2.1 looks like missing $w_{n+1}$, could you confirm this?

5) is the Original GAN exactly the Ian's original GAN or WGAN?

6) Have you tried this approach using small sized generator (having  small number of parameters)?



","The sentiment of the review appears to be neutral to slightly positive. The reviewer acknowledges the reasonableness of using multiple generators but raises several questions and suggestions for improvement. This indicates a sentiment score of around 10. The language used in the review is polite and constructive, with phrases like 'could you confirm this?' and 'it would be better to,' which suggests a politeness score of 80.",10,80
"This work presents a recurrent attention model as part of an RNN-based RL framework. The attention over the visual input is conditioned on the the model's state representation at time t. Notably, this work incorporated multiple attention heads, each with differing behavior.

Pros:
-Paper was easy to understand
-Detailed analysis of model behavior. The breakdown analysis between ""what"" and ""where"" was particularly interesting.
-Attention results appear interpretable as claimed

Cons:
-Compared to the recurrent mechanism in MAC, both methods generate intermediate query vectors conditioned on previous model state information. I would not consider the fact that MAC expects a guiding question to initialize its reasoning steps constitute a major difference in the overall method.
-There should be an experiment demonstrating the effect of # of attention heads against model performance. How necessary is it to have multiple heads? At what point do we see diminishing returns?
-I would also recommend including a citation for :
Sukhbaatar, Sainbayar, Jason Weston, and Rob Fergus. ""End-to-end memory networks."" NIPS 2015.


General questions:
-Was there an effect of attention grid coarseness on performance?
-For the atari experiments, is a model action sampled after each RNN iteration? If so, would there be any benefit to trying multiple RNN iterations between action sampling?","The review starts with a neutral to positive sentiment, highlighting the strengths of the paper such as its clarity, detailed analysis, and interpretable results. The cons and recommendations are presented in a constructive manner, without harsh criticism. The language used is polite and professional, focusing on suggestions for improvement rather than pointing out flaws. Therefore, the sentiment score is 50, indicating a moderately positive sentiment, and the politeness score is 80, indicating a high level of politeness.",50,80
"The authors argue that they propose a method to find adversarial examples, when the data lie on a Riemannian manifold. In particular, they derive a perturbation, which is argued to be the worst perturbation for generating an adversarial example, compared to the classical Euclidean derivation.

I strongly disagree with the proposed (Riemannian) geometric analysis, because there are several technical mistakes, a lot of arbitrary considerations, and flawed assumptions. In particular, my understanding is that the proposed method is not related at all with Riemannian geometry. For justification I will comment some parts:

#1) In Section 1 paragraph 4, and in Section 2.3 after Eq. 14, the sentences about the gradient of a function that is defined on a manifold are strange and unclear. In general, the gradient of a function defined on a manifold, points to the ascent direction. Thus, if I understood correctly the sentences in the paper support that the gradient of such a function is meaningless, so I think that they are wrong.

#2) How the $\ell_2$-ball on a manifold is defined? Usually, we consider a ball on the tangent space, since this is the only Euclidean space related to the manifold. Here, my understanding is that the authors consider the ball directly on the manifold. This is clearly wrong and undefined.

#3) To find the geodesic you have to solve a system of 2nd order non-linear ODEs, and there are additional details which I will not include  here, but can be easily found in the Riemannian geometry literature. Also, I think that the Lemma 2.2. is wrong, since the correct quantity of Eq. 3 is $ds^2 = g_ij(t)d\theta^i d\theta^j dt^2$, where $dt\rightarrow 0$ based on the included proof. This is clearly not a sensible geodesic, it is just the infinitesimal length of a line segment when $t\rightarrow 0$, which means that the two points are infinitesimally close.

#4) If $x, y$ is on a Riemannian manifold then the $x+y$ operator does not make sense, so Eq. 7 is wrong. In particular, for operations on Riemannian manifolds you need to use the exponential and the logarithmic map.

#5) Continuing from #4). Even if we consider the perturbation to be sufficiently small, still the $x+\epsilon$ is not defined. In addition, the constraint in Eq. 8 is wrong, because the inner product related to the Riemannian metric has to be between tangent vectors. Here the $\epsilon$ is an arbitrary quantity, since it is not defined where it actually lies. In general, the derivation here is particularly confusing and not clear at all. In my understanding the constraint of Eq. 8 is a purely linear term, and $d$ is not the geodesic distance on a manifold. It just represents the Mahanalobis distance between the points $x$ and $x+\epsilon$, for a matrix $G$ defined for each $x$, so it is a linear quantity. So Eq. 9 just utilizes a precondiner matrix for the classical linear gradient.

#6) The Eq. 12 is very flawed, since it equalizes a distance with the Taylor approximation error. I think that this is an unrealistic assumption, since these terms measure totally different quantities. Especially, if $d$ is the geodesic distance.

#7) The upper bound in inequality Eq. 13 comes from Eq. 12, and it is basically an assumption for the largest absolute value of the Hessian's eigenvalues. However, this is not discussed in the text, which are the implications?

#8) I find the paper poorly written, and in general, it lacks of clarity. In addition, the technical inconsistencies makes the paper really hard to follow and to be understood. I mentioned above only some of them. Also, there are several places where the sentences do not make sense (see #1), and the assumptions made are really arbitrary (see #6). The algorithms are not easy to follow. Minor comments, you can reduce the white spaces by putting inline Eq. 3, 4, 5, 6, 9, 10, 14, and Figure 2. The notation is very inconsistent, since it is very unclear in which domain/space each quantity/variable lies. Also, in Section 2.5. the authors even change their notation.

In my opinion, the geometrical analysis and the interpretation as a Riemannian manifold is obviously misleading. Apart from the previously mentioned mistakes/comments, I think that the proposed approach is purely linear. Since actually the Eq. 14 implies that the linear gradient of the loss function, is just preconditioned with the Hessian matrix of the loss function with respect to the input $x$. Of course, if this function is convex around $x$, then this quantity is the steepest ascent direction of the loss function, simply on the Euclidean space where $x$ lies. However, when this function is not convex, I am not sure what is the behavior when all the eigenvalues of the Hessian are set to their absolute value. Also, the (arbitrary) constraint in Eq. 13 implicitly sets a bound to the eigenvalues of the Hessian, which in some sense regularizes the curvature of the loss function. To put it simple, I think that the proposed method, is just a way to find a preconditioner matrix for the linear gradient of the loss function, which points to the steepest direction. This preconditioner is based on the Hessian of the loss function, where the absolute values of the eigenvalues are used, and also, are constrained to be bounded from above based on a given value.

Generally, in my opinion the authors should definitely avoid the Riemannian manifold consideration. I believe that they should change their perspective, and consider the method simply as a local preconditioner, which is based on the Hessian and a bound to its (absolute) eigenvalues. They should also discuss what is the implication by setting the eigenvalues to their absolute values. However, since I am not an expert in the field of adversarial examples, I am not sure how novel is this approach.


","The sentiment of the review is highly negative, as the reviewer strongly disagrees with the proposed method and points out several technical mistakes, arbitrary considerations, and flawed assumptions. The reviewer also mentions that the paper is poorly written and lacks clarity. Therefore, the sentiment score is -90. The politeness of the language is relatively neutral to slightly negative. While the reviewer is critical and direct in their feedback, they do not use rude or offensive language. However, the tone is firm and uncompromising, which makes the politeness score -20.",-90,-20
"1. This papers leverages the concept of wavelet transform within a deep architecture to solve the classic problem (especially for wavelet analysis) of change point detection. The authors do a reasonably comprehensive job of demonstrating the efficacy of the proposed framework using various synthetic and real data sets with both gradual and abrupt changes

2. The concept of pyramid network idea is not really new, in the context of CNN it has been established quite well. The paper should highlight this fact by citing papers such as ""Lin, Tsung-Yi, et al. ""Feature Pyramid Networks for Object Detection."" CVPR. Vol. 1. No. 2. 2017."" 

3. Involving wavelet transforms in deep nets have been done before. This paper attempts to learn wavelet transform parameters by involving them as trainable layers. But even this kind of idea is also emerging in the community. Papers such as ""Fujieda, Shin, Kohei Takayama, and Toshiya Hachisuka. ""Wavelet Convolutional Neural Networks."" arXiv preprint arXiv:1805.08620 (2018)"" need to be discussed in this context. 

4. The biggest issue in my mind is that I feel ""Chung et al 2016"" is still a very similar framework as the proposed one. While authors argue that it uses more like CNN architecture and the proposed method may pick up the multi-scale features better, comparison with this seems to be most appropriate. This will also clearly identify the benefits of the wavelet structure to the filters and multi-resolution analysis approaches.

5. RCNN term has been used for CNN+RNN architecture. This may not be a good terminology to use since RCNN is a very popular term referring to Region based CNN for detection and localization purposes.

6. AUC metric, I believe is the - area under ROC curve, this needs to be spelled out, how it is computed? at least in the Appendix

xxxxxxxxxxxxxxxxxxx

Appreciate the authors' rebuttal, updated my score.","The sentiment of the review is moderately positive. The reviewer acknowledges the comprehensive job done by the authors in demonstrating the efficacy of the proposed framework, which suggests a sentiment score of around 40. The politeness of the language is quite high, as the reviewer uses polite phrases such as 'Appreciate the authors' rebuttal' and provides constructive feedback without being harsh, which suggests a politeness score of around 80.",40,80
"Authors extends stacked hourglass network with inception-resnet-A mudules and a multi-scale approach for human pose estimation in still RGB images. Given a RGB image, a pre-processing module generates feature maps in different scales which are fed into a set of serial stack hourglass modules each responsible for a different scale. Authors propose an incremental adaptive weighting formulation for each stack-scale-joint. They evaluate proposed architecture on LSP and MPII datasets.

positive:
- Having an adaptive weight strategy is a necessary procedure in multi-loss functions where cross-validation or manual tuning of fixed weights are expensive. While the weights are functions of the loss, it is not analyzed and evaluated thoroughly, e.g. evolution of weights for each joint-stack-scale. Even it is not given in the section 5.2.1. So it is hard to judge effectiveness of the proposed loss. 

negative:
- In general experiments section is the most weakness of the paper. I comment some points in the following:
a) Multi-scale image processing is not a novel idea in computer vision and specifically in human pose estimation. The authors have not compared their methods with most recent papers in the literature and I believe the results are not state-of-the-art (see [1] for instance which is a multi-scale approach).
b) What is the effect of each scale in the results and for each joint? This must be analyzed and shown visually or numerically.

- The number citations is not enough.

- The writing must be improved.

overall:
Regarding mentioned comments, I believe the paper needs a huge extra effort (both analytically and numerically) to be adequate for publication. Therefore, I recommend rejection.


[1] Yang, W., Li, S., Ouyang, W., Li, H., Wang, X.: Learning feature pyramids for human pose estimation. In: ICCV. (2017)","The sentiment score is derived from the overall tone and content of the review. The reviewer acknowledges some positive aspects, such as the necessity of an adaptive weight strategy, but the majority of the feedback is critical, pointing out significant weaknesses in the experiments, lack of novelty, insufficient citations, and poor writing quality. Therefore, the sentiment score is -60, indicating a predominantly negative sentiment. The politeness score is based on the language used in the review. While the reviewer is critical, the language is not rude or disrespectful. The reviewer provides constructive feedback and specific recommendations without using harsh or offensive language. Therefore, the politeness score is 20, indicating a slightly polite tone.",-60,20
"This paper proposes a method based on re-training the full-precision model and then optimizing the corresponding binary model. It consists of two phases: (1)  the full-precision model training where the quantization step is introduced through QSS to train the network, and (2) fine tuning of quantized networks, where  the trained network was converted into a binary model. In addition, using the skewed matrix for quantization improves the accuracy. Then a loss function based on the k means form is used to normalize the weight for reducing the quantization error. Quantization experiments for CNNs or LSTMs have been conducted on CIFAR10, CIFAR100, IMAGENET, and WikiText-2 dataset. 

This paper has been presented clearly. However, it can be improved by introducing the motivation of the tricks(e.g. skewed matrix and loss related to k-means ) used for quantization.

In the experiments, the precision improvement on the CIFAR and ImageNet dataset performs worse than some competitors. For example, the precision of the proposed method was significantly worse than Zhou et al, 2018 on ImageNet. It is better to  analyze the reason. 

In addition, as claimed from the introduction, the contribution of this paper was  to reduce the overhead of expensive quantization. However, no experimental results on computation time  and parameter size have been shown. ","The sentiment of the review is moderately positive, as the reviewer acknowledges that the paper is clearly presented and proposes a novel method. However, the sentiment is tempered by the mention of areas for improvement, such as the need for better motivation of certain techniques and the lack of experimental results on computation time and parameter size. Therefore, the sentiment score is 30. The politeness of the language is quite high, as the reviewer uses polite phrases like 'it can be improved' and 'it is better to analyze the reason,' without being overly critical or harsh. Thus, the politeness score is 80.",30,80
"The paper presents an extension of the MXFusion language that allows the use of probabilistic modules. These modules are defined as a set of random variables and a specific probabilistic distribution. The modules also contain dedicated inference methods. Using these modules, one can use probabilistic distributions with inference methods tailored to the distribution, which are usually more efficient than generic inference systems.
The paper presents several examples using Gaussian process models, evaluated by comparison with GPy and the standard spare gaussian process method implemented in MXFusion.

Overall, the paper is well written and clear, and all claims are justified. The idea of modularization is not really new (as other systems implement something similar) but this approach tries to be general, in order not to pose constraints on the specification of modules. The related work section provides a good positioning of the approach.
I have not found any specific problems in the paper, the quality is rather high. However, the actual content of the paper describes an extension of an existing system. Such an extension is certainly important, but the paper does not provide much more information.
Moreover, the results of the experimental test do not seem to me to be able to support the main objective of the extension, which is to give the possibility to exploit more specific probabilistic model and inference methods to achieve better results than an approach using general methods.
As far as the execution of the system is concerned, is this extension able to improve the scalability or reduce the walltime? Is this visible in the presented test (at least in terms of speed up)? Or is the convenience of this approach the simpler way to define distributions?

As for minor issues that I can point out, one concerns the definition of shape in the Variable of m.sigma2 (figures 1, 2, 3). I do not know the used in MXFusion, thus this might not be an error, but it seems that in the shape definition something is missing. It is written that shape=(1,), is it correct or is there an error? In case of absence of error, what does the empty argument mean?

The power benchmark is not described.

In references, Thomas V. Wiecki is mentioned with and without the first letter of middle name. I suggest to uniform the references.

Typos:
- Abstract: ""... but also sophisticated probabilistic model*s* such as ...""
- Sec. 1, first row of page 2: The sentence ""this would bring the a lot of benefits ..."". The ""the"" word should be deleted.
- Sec. 1 refers to a section after 4 which does not exist in the paper.
- Page 5: remove the full stop before the colon in the 4th row.
- Page 5: ""The log_pdf method of the SGPR module compute*s* the above variational lower bound""
- Sec. 6: the sentence ""MXFusion aims at closing the gap between having specialized, highly performant algorithms and generic, easily maintained generic algorithms by introducing probabilistic modules."" should be corrected.


Pros
- The extension allows the use of modules that define specific probabilistic distribution/inference methods
- It seems easy to extend the system with other modules
- Its a really useful extension...

Cons
- The performance presented in the paper is not entirely convincing
- ... but it is just an extension of an existing system","The sentiment of the review is generally positive, as the reviewer acknowledges that the paper is well-written, clear, and justified in its claims. However, the reviewer also points out that the paper is an extension of an existing system and questions the experimental results' ability to support the main objective. Therefore, the sentiment score is 50. The politeness of the language is high, as the reviewer provides constructive feedback and suggestions in a respectful manner, without any rude or harsh language. Therefore, the politeness score is 90.",50,90
"Overall, this is a interesting paper on an important topic: knowledge extraction from Neural Networks.
Even though the authors seem propose a novel approach to knowledge extraction, the paper would 
dramatically benefit from two additions:
- an empirical evaluation on at least two more datasets (as is, the paper uses a single dataset)
- an illustrative-but-realistic example of how at least one rule is extracted from each layer of the neural network 

Other comments:
- on page 4 (1st paragraph in 3.3), the authors talk about a ""test set"" that, it turns out, it is extracted from the actual training set (1st paragraph of 4.1); the authors should use a more careful terminology
- from the paper, it seems that the authors tried a single randomly chosen set 1000 random inputs in 4.1; they should most definitely try several such sets
- Figure 1 should have a legend in the image, rather than as a 2-line caption
 ","The sentiment of the review is generally positive, as indicated by the opening statement that the paper is interesting and addresses an important topic. However, the reviewer also points out significant areas for improvement, which slightly tempers the overall positivity. Therefore, the sentiment score is 50. The politeness of the language is high; the reviewer uses phrases like 'the paper would dramatically benefit' and 'the authors should use a more careful terminology,' which are constructive and respectful. Thus, the politeness score is 80.",50,80
"Presentation of the work is critically weak and I failed to understand the objective and contributions of the paper (despite a solid knowledge in Bayesian inference). They are many editing problems and the English is problematic, but most importantly the writing fails to properly introduce the problem, the objective and solutions.","The sentiment score is determined based on the overall negative tone of the review, which highlights critical weaknesses, lack of understanding, and multiple issues with the paper. The politeness score is based on the language used, which, while critical, is not overtly rude but rather straightforward and factual.",-80,-20
"*Update after discussion period*
I remain unconvinced. The authors failed to address my clearly articulated request for a more thorough analysis of additional networks trained on ImageNet (e.g. ResNet), which I don't think is asking for too much given a discussion period of three weeks.


Summary:
The authors find that (1) DNNs exhibit orientation selectivity in many of their hidden layers' units, (2) in the intermediate layers this selectivity emerges during training, concurrently with the network's ability to generalize, and (3) ablating orientation-selective units in the early layers impairs a network's generalization performance. 

Strengths:
+ Very straightforward and easy to follow 
+ Technically sound

Weaknesses:
- Feels trivial
- The claims seem to be too general

Conclusion:
I'm torn on the paper. On the one hand, it reports some potentially interesting observations (e.g. trajectory of emergence of orientation selectivity over training). On the other hand, I'm not really sure what we learn from the paper.


Specific comments:

- The result seems trivial. How should a network be able to recognize objects without detecting edges of certain orientation (which implies orientation selectivity)?

- Related to the previous point, pretty much every single (supervised or unsupervised) learning objective investigated so far has produced orientation selectivity, so it seems pretty well established that orientation selectivity is somehow useful. The interesting question is what needs to be done on top of it in order to get a representation useful for object recognition, but in this respect the paper does not contribute anything.

- The results are mostly on CIFAR-10 and only one network (VGG-16) trained on ImageNet is considered. Given the generality of the claims (""orientation selectivity [plays] a causally important role in object recognition"" – abstract), the authors would have to show that their results also hold for other high-performing networks on ImageNet, and not just VGG-16 (sort of, see next point). Otherwise, an appropriate conclusion would be that orientation selectivity plays a causal role in the functioning of VGG-16 and some networks trained on CIFAR-10.

- The analysis meant to establish causality (section 3.4) produces pretty mixed results on VGG-16 (Fig. A6b), where ablating the top 50% orientation-selective units in some layers has a *smaller* effect than ablating the rest. How do the authors explain this result?
","The sentiment of the review is mixed but leans towards the negative side. The reviewer acknowledges some strengths of the paper but expresses significant doubts about its contributions and generalizability. This is evident from phrases like 'feels trivial' and 'I'm torn on the paper.' Therefore, the sentiment score is -40. The politeness of the language is relatively neutral but slightly leaning towards polite. The reviewer uses phrases like 'I remain unconvinced' and 'I'm not really sure,' which are polite ways to express disagreement. Therefore, the politeness score is 20.",-40,20
"After rebuttal: The authors have nicely addressed my comments. I have increased my rating to 7.
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
This paper proposes a generalization error bound for DNNs (and their generalizations) based on the depth and width of the networks, as well as the spectral norm of weight matrices. The proposed work is shown to provide a tighter generalization error bounds compared with a few existing literatures. 

Pros: This paper makes theoretical contributions to the understanding of DNNs. This is an important but difficult task. As a theoretical paper, this one is relatively easy to follow. 

Cons: In spite of its theoretical contributions, this paper has a few major issues. 

Q1: This paper fails to fairly compare with the most recent work, Arora et al. (2018), Zhou and Feng (2018). For instance, Arora et al. (2018) uses error-resilience parameters instead of the norms of weight matrices to obtain a better generalization error. The authors claim that the error-resilience parameters are less interpretable than the norms of weight matrices. This claim could be subjective and is not convincing. 

Q2: The error bounds of Bartlett et al. (2017), Neyshabur et al. (2017) could be improved for low-rank weight matrices, in which case the proposed Theorem 1 is tighter only if $p \le D^2$. This holds only when DNN is very deep. Can theorem 1 be improved by similarly considering the low-rankness of weight matrices?

Q3: In Corollary 2, the error bound for CNN, the authors assume that the filters are orthogonal with unit norm. Can the authors provide some justification on the orthogonal filters? In addition, Zhou and Feng (2018) have achieved similar bound for CNN. Can the authors provide some justification why this latest result is not included in Table 2? 

","The sentiment of the review is generally positive, as indicated by the initial statement that the authors have 'nicely addressed' the comments and the increased rating. However, the review also includes several critical points, which slightly temper the overall positivity. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer uses polite and constructive language throughout, even when pointing out issues. Thus, the politeness score is 90.",60,90
"This paper introduces a new way to have more compressed (lower rank) representation of the data in a supervised fashion. The authors motivates their work by saying that such representation are more useful for transfer learning and are more robust to adversarial examples! In order to achieve this, the authors introduce a virtual LR layer and utilize Nystrom technique to make the process more efficient.  The idea introduced in this paper is interesting but the paper is poorly written and organized which makes its through evaluation difficult. Below, I provide more detailed comments.

I don't understand at what frequency the low-rank optimization as the  subproblem to Equation (OPT) is being done. Given that the DNN training  is being done using batched of examples, where do you put the low-rank  optimization, in the end of each epoch? It seems it is in the end of each epoch but it should be clearly stated.

I don't understand motivation for L_N(A). The authors justify it by  saying that a trivial solution for the optimization problem would be setting A+b=0 and they introduce this term to avoid it. However, this is not correct. Note that there are n examples in A and we can only make one of them  zero at a time. (Note that talking about A+b is not also accurate  because they are not of the same size).

In order to use Nystrom method for low-rank approximation, W needs to be  a symmetric postive semi-definite matrix. I am not sure if the heuristic  procedure introduced in Page 4 is well-justifed. and whether we are  still optimizing the objective function introduced in Page 3. Do we  still have a stable training?

What is ResNet N-LR in experiment? The authors introduced ResNet 1-LR and ResNet 2-LR but not ResNet N-LR! I found the description N-LR later but the naming is rather confusing. I would use LR instead of N-LR because it seems it has N LR layer. I would also explain this next to other methods. 

Not sure if I understand Bottle-LR. The description in the text is not clear and I don’t understand the motivation for this baseline! Again, this should be described next to other methods.

The authors do not mention what is their setting for r in the experiments (Table 1).

Page 5, Paragraph after Table 1: First CIFAR-100 should be CIFAR-10.

One way the authors defend their framework is to have a representation that can be used in transfer learning. Nonetheless, the results in Table 1.b shows that their framework is not doing good for transfer learning.

Not sure if I understand Figure 1. How do you change the number of singular values? I understand this is a hyper parameter for your framework but I am confused how it is being set for N-LR method. Similarly, I don’t understand Table 2 and how you change the embedding dimension. 


Unlike the claim made by the authors, it seems that VGG19 N-LR does better compared to VGG19 2-LR in Figure 2. ","The sentiment score is derived from the initial paragraph where the reviewer acknowledges the interesting idea but criticizes the poor writing and organization of the paper. This mixed feedback results in a sentiment score of -20. The politeness score is based on the language used throughout the review. While the reviewer points out several issues, the language remains professional and constructive, leading to a politeness score of 20.",-20,20
"In convolutional neural networks, a convolutional filter conducts a series of multiply-accumulate (MAC) operations, which is computationally heavy. To save computational cost, this manuscript proposes an algorithm to set a checkpoint in the MAC process to determine whether a filter could terminate early based on the intermediate result. The results empirically demonstrate that the proposed algorithm could save lots of MAC computations without losing too much accuracy.

Significance wise, the results look promising, though it is not always the best method to preserve accuracy while doing the pruning. There are a few experiments where CP is better at preserving accuracy. In addition, it would be great if the manuscript could also compare with other compression methods like low-rank factorization and knowledge distillation. In this manuscript, the best results are roughly 50% MAC savings (which is not equal to the saving of the whole networks) with some accuracy drop. It seems that knowledge distillation could often make the model half size without losing accuracy if properly tuned.

Quality wise, there seems to be some room to improve from a technical point of view. The proposed algorithm always starts from the center layers rather than shallow or deep layers based on the intuition that center layers have minimal influence on final output compared with other layers. This sounds about right, but lacks data support. In addition, even this intuition is correct, it might not be the optimal setting for pruning. We shall start from the layer where the ratio of accuracy drop by reducing one MAC operation is minimal. As a counterexample, if the shallow layer is the most computationally heavy, saving 10% MAC ops might save more computational cost than saving 5% in the center layer while preserving the same accuracy level. 

The above sub-optimality might be further magnified by the fact that the proposed algorithm is greedy because one layer might use up all the accuracy budget before moving on to the next candidate layer as shown in line 7 of Algorithm 1.

The method is quite original, and the manuscript is very well written and easy to follow.","The review starts with a positive sentiment towards the proposed algorithm, acknowledging its potential to save computational costs without significant accuracy loss. However, it also points out several areas for improvement, such as the need for comparison with other compression methods and the lack of data support for certain claims. The language used is polite and constructive, offering specific suggestions for improvement without being overly critical or dismissive.",30,80
"To improve the robustness of neural networks under various conditions, this paper proposes a new regularizer defined on the graph of the training examples, which penalizes the large similarities between representations belonging to different classes, thus increase the stability of the transformations defined by each layer of the network.

The paper is overall well written, and the idea involving the Laplacian of the similarity graph is interesting. I have reviewed this paper before. Compared to the previous version, this paper made a good improvement in its experimental results, by adding two different robustness settings in section 4.1 and section 4.3, and also include DeepFool as a strong attack method for testing adversarial robustness.

However, my main concern about the paper is still about its significance. 
1. It is still not clear why would this regularization help robustness especially when considering adversarial examples. Example 1 seems not obvious to me why maintaining the boundary margin (rather than expanding or shrinking) is preferred. As stated in the second paragraph in section 3.4, “lower value of \sigma^\ell(s) are indicative of better separation between classes”, what is the reason of not directly penalizing this value, rather than requesting a “stability” property on this value? How is this stability related to the robustness? This would request a deeper analysis and more empirical proofs in the paper.
2. Experimental results still seem not convincing to me. On one hand, based on the reported result, I am not very convincing that the proposed method outperforms Parseval, especially when considering the inconsistent behaviour of “Proposed + Parseval”. On the other hand, for adversarial robustness, the authors should have compared to the method of adversarial training as well. Beyond that, the authors should also be careful of the gradient masking effect of the proposed method. I am not sure if there is some other obvious benchmarks should be included for the other two robustness settings.

Other comments:
1. Descriptions in the last 3 paragraphs in section 3.2 are not very clear. It always took me a while to figure it out every time I read the paper. It would be very helpful if the computation process and the discussions can be separated here, maybe with a pseudo-code for computing the regularizer. 
2. On the other hand, while the proposed regularizer can be interpreted in a perspective of the Laplacian of the similarity graph, the third part in Equation (4), that expresses the smoothness as the sum of similarities between different classes, seems more intuitive to me. Emphasizing in this interpretation may also help convey the message.","The review starts with a positive sentiment, acknowledging that the paper is well-written and that the idea is interesting. It also notes improvements in the experimental results compared to a previous version. However, the review then shifts to a more critical tone, expressing significant concerns about the paper's significance and the convincingness of the experimental results. The language used is polite and constructive, offering specific suggestions for improvement without being rude or dismissive.",20,80
"# Summary

This work describes a shortcoming in existing dynamic batching strategies, namely that they operate only on the forward pass while some operations can be batched only in the backward pass. For example, the gradient of the transition matrix in a RNN consists of the sum of partial derivatives over each time step; the terms of this sum and the summation can be batched into a single matrix multiplication. The authors implement this batching strategy in DyNet and show empirically that it can lead to decent (0-25%) speedups.

# Quality

The proposed technique comes with a trade-off which is not discussed in the paper: Delaying computations until several can be batched together can increase peak memory usage. In particular, the memory requirements of a RNN would increase from O(T) to O(2T) since each forward and backward state must now be stored. (In fact, the authors use a separately allocated contiguous block of memory that they copy the states and gradients into, which would bring this to O(3T) or O(4T) memory complexity.)

A second observation that should have been made is that the potential for speedups depends on the batch size, hidden state size, and number of time steps (or tree depth). Given a small batch size and large hidden state, the batching method effectively replaces a series of outer products with a single matrix multiplication. One would expect good speedups in this scenario. On the other hand, for a large batch size with a small hidden state, the dynamic batching strategy effectively replaces a series of inner products with a single larger inner product, which would be far less beneficial. The experiments in this work use relatively small batch sizes (64), which gives little insight about whether the proposed method would lead to speedups in a wide range of models (for example, batches of 512 are common in some NLP applications).

Some smaller comments:

* Multi-threading on a multicore architecture does not necessarily imply that operations are performed sequentially.
* Input sequences in NLP are not always sentences given as sequences of words.
* The argument that padding always leads to unnecessary computation is overly simplistic; the added control flow and branching required to perform irregular computation can often make it slower than doing regular computation plus masking (additionally, sparse kernels are often memory bandwidth bound, leading to different performance properties).
* The authors say that operations of the same ""type"" can be batched together, but don't specify what ""type"" means. I assume the type is defined by both the operation as well as the shapes of its inputs and outputs?
* No distinction is made between different ways of batching and their performance characteristics. Two matrix-vector multiplications gemv(X, y1) and gemv(X, y2) can be efficiently batched as gemm(X, [y1 y2]') which reduces the number of times X needs to be loaded into working memory. This is not the case when batching distinct inputs such as gemv(X1, y1) and gemv(X2, y2). On the other hand, gemv(X1, y1) + gemv(X2, y2) can be efficiently batched as gemv([X1 X2], [y1', y2']'), reducing the number of memory accesses in the output buffer.
* Why perform 3 runs and report the fastest speed? Why not report the range, or better yet, perform more runs and report confidence intervals.

# Clarity

The writing in this paper needs significant improvement. In terms of structure, the introduction (section 1) and background (section 2) are very repetitive. The third, fourth, fifth and sixth paragraph of the introduction are effectively repeated in full in sections 2.1, 2.2, 2.3 and 3.1 respectively. On the other hand, the inclusion of table 1 at the beginning puts the reader on the wrong foot thinking that this paper will consider NMT models, whereas the paper only deals with POS tagging and sentiment analysis.

The text contains grammatical errors (""days even weeks"", ""The parallel computing helps""), tautological definitions (""batching [...] means organizing the same operations of computation graphs into batches"", ""padding, which is to pad the input sequences""), unclear use of language (""cooperating with the existing strategies""), and typographical mistakes (multiple citations are separately parenthesized). Overall, the lack of clarity inhibits the understanding of the paper.

# Originality and significance

The central contribution of this paper is relatively straightforward in retrospect, but can certainly be beneficial for the training of some particular models. I am no expert in the literature, but the authors' claim that they are the first ones to consider this technique seems justified. The paper has no reference to code, so it is hard to judge how easy it would be for practitioners to use the suggested technique.

# Summary

Pros:

  * Useful dynamic batching trick that can lead to speedups
  * Empirical evaluation compares to two existing techniques and breaks down individual components of runtime

Cons:

  * No critical look at the disadvantages of this technique such as applicability to larger batch sizes and memory usage
  * Some questionable statements and assumptions
  * Lack of formalization and clear definitions
  * Paper reads long-drawn-out, subpar writing hurts readability","The sentiment of the review is mixed, with both positive and negative aspects highlighted. The reviewer acknowledges the usefulness of the proposed technique and its potential benefits but also points out significant shortcomings and areas for improvement. Therefore, the sentiment score is slightly negative. The language used in the review is generally polite and constructive, with specific recommendations and observations provided in a respectful manner, even when pointing out flaws. Thus, the politeness score is high.",-20,80
"The authors proposed an unsupervised learning framework to learn multisensory binding, using visual and auditory domain from animal videos as example. First, the visual and auditory inputs are autoencoded, and these latent codes are binding using a recurrent self-organizing network (Gamma-GWR). Furthermore, the authors proposed the expectation learning idea, which is inspire by psychology literature. In short, after the first pass of training using the real data. The authors fine tuned the model to bind the real data from one domain and the reconstructed data from another domain. This could be a good idea, as the authors pointed out, human usually bind all kinds of yellow bird to a same mental 'chirping' sounds. So, this expectation learning could potentially group the representation to a canonical one. Also, the authors showed in Table 1 that with the expectation learning, the model's recognition accuracy is improved a bit. I think it would be interesting to show the reconstruction output example (as in Fig. 3) for both model with and without expectation learning. To see if it is as the authors claim, that the model with expectation learning is reconstructing the missing modality with more canonical images/sounds. (This may not be the goal in other practice, though I'm convinced it is a potentially good psychological model as it explain well the multisensory imagery effect (Spence & Deroy, 2013). 

I found this manuscript quite hard to follow though. The description seems sometime not flowing very smoothly. And there are some clear typos and mess up of math notations make the reading unpleasant. I have noted down several points below, and hope the authors could improve in the next iteration.

1. The description of variational autoencoder is not well written. The citation (Chen, 2016) is not the standard VAE paper people usually cite (unless the author is adopting something specific from the Chen's paper.). For example, the authors wrote ""the KL divergence between the encoded representation and a sample from the Gaussian distribution"" which sounds incorrect to me.

2. Why a Variational autoencoder is necessary for visual domain, but a regular autoencoder is used in auditory domain?

Typos:
1. page 2, 2nd line: a online --> an online
2. Use subscript I-1 to mean the winner neuron at t-1, I think this is not quite clear. I suggest to follow the notation in (Parisi & Wermter 2017), use I(t-1), which is easier to follow.
3. page 7, 2nd line: more than 17% for audio.  -> for vision.
4. page 8, 3rd line: not on the original high-abstraction data. Do the authors mean highly specific data? That seems make more sense.
5. Several notation mismatch here and there. for example, in formula 6 it is w_j^s, but in the text below it become w_{j,s}.
","The sentiment of the review is generally positive, as the reviewer acknowledges the potential of the proposed framework and its interesting aspects, such as the expectation learning idea and its psychological relevance. However, the reviewer also points out several issues with the manuscript, including difficulties in following the description, typos, and notation inconsistencies. Therefore, the sentiment score is not fully positive but leans towards the positive side. The politeness of the language is quite high, as the reviewer uses polite phrases like 'I think it would be interesting,' 'I found this manuscript quite hard to follow though,' and 'hope the authors could improve in the next iteration,' indicating a respectful and constructive tone.",40,80
"The paper addresses the problem of computational inefficiency in video surveillance understanding approaches. It suggests an approach called Dynamic Convolution consists of Frame differencing, Prediction, and Dyn-Convolution steps. The idea is to reuse some of the convolutional feature maps, and frame features particularly when there is a significant similarity among the frames. The paper evaluates the results on 4 public datasets. However, it just compares the approach to a baseline, which is indeed applying convnet on all frames. 

- State of the art is not well-studied in the paper. Video understanding approaches usually are not just applying convnet on all frames. Many of the approaches on video analysis, select a random set of frames (or just a single frame) [5], and extract the features for them. There is another set of work on attention, that try to extracts the most important spatio-temporal [1-4] information to solve a certain task. These approaches are usually computationally less expensive than applying convnet on all video frames. I suggest the authors compare their model with these approaches. 

[1] Spatially Adaptive Computation Time for Residual Networks., Figurnov et al. 
[2] Recurrent Models of Visual Attention, Mnih et al.
 [3] Action recognition using visual attention, Sharma et al.
 [4] End-to-end learning of action detection from frame glimpses in videos, Yeung et al.
 [5] Two-Stream Convolutional Networks for Action Recognition in Videos, Simonyan et al. 

- In addition, car and pedestrian detection performance is part of the evaluation process. In this case, the approach should be also compared to the state-of-the-art tracking approaches (that are cheaper to acquire) in terms of computational efficiency and performance. 
- The writing of the paper should also improve to make the paper more understandable and easier to follow. Some examples: 1. Unnecessary information can be summarized. For example, many details on the computational costs in abstract and the introduction can just simply be replaced by stating that “these approaches are computationally costly”.  2. Using present tense for the SoTA approaches is more common.“ShuffleNet (Zhang et al. (2017)) proposed two new strategies”.  3. Long sentences are difficult to follow: “In real surveillance video application, although the calculation reduction on convolution is the main concern of speeding up the overall processing time, the data transfer is another important factor which contributes to the time”
  + The problem of large-scale video understanding is an important and interesting problem to tackle.  ","The sentiment of the review is mixed but leans towards the positive side. The reviewer acknowledges the importance and interest of the problem tackled by the paper, which is a positive sentiment. However, the reviewer also points out several significant shortcomings and areas for improvement, which tempers the overall positivity. Therefore, the sentiment score is 20. The politeness of the language is quite high. The reviewer uses polite language, provides constructive feedback, and offers specific suggestions for improvement without being harsh or dismissive. Therefore, the politeness score is 80.",20,80
"This paper proposes new heuristics to prune and compactify neural networks. The heuristics try to consider 1) filter weight and gradient normalization by their size, 2) saliency normalization across layers, 3) saliency accumulation across batch. The author claims that these can address problems previous studies had and experimental results show that the proposed method achieve higher compression ration with less loss of accuracy.

This paper discusses how to determine the importance of filters. As cited in the paper, there have been various attempts to tackle the same problem and the paper contributes to the series of efforts. The paper introduces a new way to compute such importance values based on their observations. The method is tested on a few dataset and a various models and compared with some previous studies. I like the simple but yet effective method, however, I think it is not good enough for ICLR. 

1. What is effective is not very clear.

The paper pointed out issues of previous studies and proposed the new method based on the observations. However, only the final method is compared with other work and it did not examine which part of the method was essential. The paper needs more detailed analyses on the proposed method. For example, the readers would want to know if the normalization in Eq. (2) is really important or not. The readers would be also interested in a visualization like Fig. 2 without saliency normalization. 

2. The numbers of previous studies come only from their papers.

It is very difficult to know if the proposed method is actually better than the previous methods if the numbers just come from their papers. We want to compare the ideas, but not numbers. The essential ideas of other papers need to be abstracted and tested in the paper by itself. It relates to the first item above. ""Baseline"" should be a baseline method but not models without pruning.

Numbers from other papers are still useful to show that the numbers in the paper are good in an absolute manner.

3. Weak theoretical reasoning

Eq. (1) in the paper is not actually used for optimization while some previous methods do. If the proposed method is better than other methods which directly optimizes the loss, should we think that the formulation itself is bad? 

The paper discusses imbalanced pruned pruning results. It needs to show that it is actually bad.

* minor things

** Table 1: Should the first row of ""Wen et al. (2016)"" have ""5-19"" and ""1-5"" or ""4-19"" and ""1-4"" for ""Filters"" and ""Channels"", respectively?

** I'd recommend another proofreading.","The sentiment of the review is mixed but leans towards the positive side. The reviewer acknowledges the contribution of the paper and appreciates the simplicity and effectiveness of the proposed method. However, they also express concerns about the clarity of the effectiveness, the reliance on numbers from previous studies, and the weak theoretical reasoning. Therefore, the sentiment score is 20. The language used in the review is polite and constructive, with suggestions for improvement and minor corrections. The reviewer uses phrases like 'I like,' 'the readers would want to know,' and 'I'd recommend,' which indicate a polite tone. Therefore, the politeness score is 80.",20,80
"This paper introduces an approach to pruning while training a network. This is interesting and experiments show interesting results in several datasets including ResNet18

Here are a few comments:

 - Pruning or regularization for compression is not new. Alvarez and Wen have used group lasso types as suggested in the paper and some others such as Alvarez and Salzmann (Compression aware training NIPS 2017) and Wen (Coordinating filters ICCV2017) have used low-rank type of while training. How is this different to those? They also do not need any sort of fine tuning and more importantly, they show this can scale to large networks and datasets. 

- These last two works I mentioned promote redundancy, similarly to what is suggested in the paper. Would be good to get them cited and compared. Important from those is the training methodology to avoid relevant overheads. How is that happening in the current approach


- While I like the approach, would be nice to see how this scale.  All for methods above (and others related) do work on full imagenet to show performance.  For ResNet, cleaning the network is not really trivial (near the block), is that a limitation?
- Why limiting experiments to small networks and datasets? Time wise, how does this impact the training time?
- Why limiting the experiments to at most 4 layers? 
- I am certainly not impressed by results on fully connected layers in MNIST. While the experiment is interesting does not seem to be of value as most networks do not have those layers anymore.

- Main properties of this approach are selecting right filters while training without compromising accuracy or needing fine tuning. While that is of interest, i do not see the difference with other related works (such as those I cited above)

- As there is enough space, I would like to see top-1 results for comprehensive comparison. 

- I think tables need better captions for being self-contained. I do not really understand what i see in table 5 for instance. 
- Droping 13% of top5 accuracy does not seem negligible, what is the purpose there? Would also be interesting to compare then with any other network with that performance. 
- What about flops and forward time? Does this pruning strategy help there?

","The sentiment of the review is mixed but leans towards the negative side. The reviewer acknowledges the interesting approach and results but raises several critical points and comparisons to existing works, indicating skepticism about the novelty and effectiveness of the proposed method. Therefore, the sentiment score is -30. The politeness of the language is generally neutral to slightly polite. The reviewer uses phrases like 'would be good to get them cited' and 'would be nice to see,' which are polite, but there are also direct criticisms that are straightforward without being overly harsh. Thus, the politeness score is 20.",-30,20
"The paper proposes a new loss function which can be used in the reconstruction term of various auto-encoder architectures. The pixel-wise cost function \ell(X, X') = f(X - X'; a) is defined for pairs of two input images X and X' and has one positive real-valued hyperparameter a. For small values of t the function f(t; a) behaves like a quadratic function, while for large t it behaves like |t|. As a consequence, it is smooth, everywhere differentiable (like L2) while not penalizing outliers too hard (like L1). The authors present several experiments conducted on MNIST and Celeba datasets, demonstrating that a simple change of a conventional pixel-wise squared L2 distance with the proposed log-cosh cost function improves the FID scores of generated samples as well as the visual quality of reconstructions (including ""the sharpness""). 

I would say this is clearly an empirical study (even though the authors claim they provide ""theoretical justifications"", they are rather hand wavy), which is not a bad thing in this case. The message of the paper is very clear and I think the authors did a good job in selling their point. The main (and, perhaps, the only) contribution is the proposal to use the log-cosh function as the reconstruction cost. And this proposal is well justified by the set of experiments. 

However, there are several major issues:
(1.1) The objective functions reported in appendix A.1 corresponding to WAE have in fact nothing to do with WAE. In WAE the regularizer penalizes the divergence between the prior distribution p(a) and *the aggregated posterior* distribution \int_x q(z|x) p(x) dx. In other words, D_MMD(q(z|x) || p(z)) in Eq. 8 should be replaced with D_MMD(\int_x q(z|x) p(x) dx || p(z)) in order to result in the WAE model. In summary, if the authors indeed used objectives reported in Eq. 8 of Appendix A, they were actually not using WAE but rather some other sort of regularized auto-encoders, which in a way are quite similar to VAEs. 
(1.2)  I am surprised to see the reported FID scores for the Celeba data set. Having worked with this data set myself in combination with VAEs and WAEs, I am impressed with the extremely low FID scores: 46 for the vanilla L2 VAE and 30 for the L2 WAE. Note that while in the appendix the authors say they follow the architectural choices provided in [1] while performing the ""L2 WAE Celeba"" experiment, the authors arrive at FID=30 compared to FID=55 reported in the ""Wasserstein Autoencoders"" paper. Also, based on my experience, achieving FID=46 on CelebA with a vanilla VAE is very impressive. Note that the authors use 10^4 of samples to evaluate the FID scores, which is exactly the same as in [1]. This size is known to be large enough to reduce the variance of FID, so the difference (55 - 30) can not be explained by the fluctuations of FID. Therefore, I ask the authors to (anonymously) share the code and/or checkpoints of the 2 particular trained models: L2 VAE and L2 WAE trained on Celeba. 

Other comments:
(2.1) Note that the reconstruction cost function in VAE should be normalized for every value of the code Z, as it corresponds to the logarithm of the likelihood (density) function -log p(X|Z). L2 and L1 costs both correspond to the well known likelihood (decoder) models (Gaussian and Laplace). However, it is hard to say what decoder model (what type of conditional distribution p(X|Z) ) would give rise to the proposed log-cosh function. In particular, the normalizing constant is not known and may depend on Z. In other words, by exchanging the L2 cost with the log-cosh loss in the VAE one looses the theoretical guarantees supporting VAE, including the fact that the objective is the lower bound on the marginal log likelihood. While this is not necessarily a problem (unless one uses the value of the objective as the bound on the marginal log likelihood, which is not the case in this paper), I would suggest mentioning it. Notice that, for instance, in WAE this problem does not appear, as the reconstruction term there does not involve any likelihood functions and thus does not need to be normalized.
(2.2) In Figure 2 I don't see why the authors did not highlight bad samples in the second row corresponding to their proposed method? I see many badly looking images there. Say, (4, 9) in VAE (MLP) and (8, 9) in VAE (Conv) and (6, 1) in WAE (MLP) and (2, 10) in WAE (Conv), where (i, j) means i-th row, j-th column, indexing starting from 1. 
(2.3) How would the Huber loss perform and how does it compare to the proposed loss?

[1] Wasserstein Autoencoders. Tolstikhin et al., ICLR, 2018.","The sentiment of the review is generally positive, as the reviewer acknowledges the clear message of the paper and the well-justified proposal through experiments. However, the reviewer also points out several major issues and requests additional information, which slightly tempers the overall positive sentiment. The politeness of the language is high, as the reviewer uses polite phrases such as 'I would say,' 'I think,' and 'I would suggest,' and provides constructive criticism without being rude or dismissive.",60,80
"The authors propose modeling structural diversity of translations by conditioning the generation on both the source sentence and a latent encoding of the overall structure (captured by simplified part-of-speech tags). Specifically, they first train a conditional autoencoder to learn a latent code optimized towards reconstructing the tag sequence. They then prefix the inferred latent code to the target sentence before generation. A diversity metric which measures pairwise BLEU scores between beam items is also proposed. Experiments show that the latent codes lead to greater structural diversity as well as marginally improved translation results when combined with beam search.

Contributions
-----------------
A simple method for improving structural diversity.

The use of conditional autoencoding to capture structural ambiguity, while not in itself novel, could be interesting for other problems as well.

Experiments suggest that the method is rather effective (albeit only improving translation quality marginally)

I like the proposed discrepancy score based on pairwise BLEU scores.

Issues
---------
It is not clear if teacher forcing was used in the ""tag planning"" setting. If gold tag sequences were used during training there is a major train/test mismatch which would explain the dramatic drop in BLEU scores. If so, this is a major issue, since the authors claim that as the motivation for the use of discrete latent codes. To make the ""tag planning"" setting comparable to the latent code setting, you would need to train the tag prediction model first and then condition on predicted tags when training the translation model (potentially you would need to do jack-knifing to prevent overfitting as well).

It is unfortunate that there is no empirical comparison with the most closely related prior work, in particular Li et al. (2016) and Xu et al. (2018), which are both appropriately cited. As it stands it is not possible to tell which of these approaches is most useful in practice.

No details are provided on the tagset used and what system is used to predict it, or to what degree of accuracy.

Having a fixed number of codes regardless of sentence length seems like a major shortcoming. I would urge the authors to consider a variable coding length scheme, e.g., by generating codes autoregressively instead of with a fixed number of softmaxes. It would also be interesting to break down the numbers in table 1 with respect to sentence length.

Minor issues
-----------------
Citation for the Xavier method is missing.

Notation is somewhat hard to follow. Please add a few sentences describing it and make sure it is consistent.

There are many grammatical errors. Please make sure to proofread!

""Please note that the planning component can also be a continuous latent vector, which requires a discriminator to train the model in order that the latent cap."" What does this mean?","The sentiment of the review is generally positive, as the reviewer acknowledges the contributions and effectiveness of the proposed method, even though the improvements in translation quality are marginal. The reviewer also appreciates the proposed discrepancy score based on pairwise BLEU scores. However, the review also points out several significant issues and areas for improvement, which slightly tempers the overall positive sentiment. Therefore, the sentiment score is 40. The politeness of the language is high, as the reviewer uses polite language throughout the review, such as 'I like,' 'It is unfortunate,' and 'I would urge the authors.' The reviewer also provides constructive feedback and suggestions for improvement without being rude or dismissive. Therefore, the politeness score is 80.",40,80
"This paper reads like the first thoughts and experiments of a physicist or mathematician who has decided to look at word representations and hyponymy. I mean that in both the positive and negative ways that this remark could be construed. On the positive side, it provides an interesting read, with a fresh perspective, willing to poke holes in rather than accepting setups that several previous researchers have used.  On the negative side, though, this paper can have an undue, work-from-scratch mathiness that doesn't really contribute insight or understanding, and the current state of the work is too preliminary. I think another researcher interested in this area could benefit from reading this paper and hearing the perspective it presents. Nevertheless, there just isn't sufficient in the way of strong, non-trivial results in the current paper to justify conference acceptance. 

Quality:

 - Pro
   o Everything is presented in a precise formalized fashion. The paper has interesting remarks and perspectives. I appreciate that the authors not only did find most existing work on modeling hyponymy but provide a detailed and quite insightful discussion of it.  (A related paper from overlapping authors to papers you do cite that maybe should have been included is Chang et al. https://arxiv.org/abs/1710.00880 – which is a bit different in trying to learn hyponyms from text not WordNet, but still clearly related.)
  -Con
   o There just isn't enough here in the way of theoretical or experimental results. In the end, two ""methods"" of hyponymy modeling are presented: one is a simple logistic regression, which is estimated separately PER WORD for words with 10 or more hyponyms. This performs worse than the methods of several recent papers that the author cites. The other is a construction that shows that any tree can be embedded by representing nodes as ranges of the real line. This is true, but trivial. Why don't ML/NLP researchers do this? It's because they want a representation that doesn't only represent the ISA hierarchy but also other aspects of word meaning such as meaning similarity and dimensions of relatedness. Furthermore, in general they would like to learn these representations from data rather than hand-constructing it from an existing source like WordNet. For instance, simply doing that gives no clear way to add other words not in wordnet into the taxonomy. This representation mapping doesn't really give any clear advantage beyond just looking up hyponymy relationships in wordnet when you need them.

Clarity:
 - Pro
   o The paper is in most respects clearly written and enjoyable to read.
 - Con
   o The mathematical style and precision has it's uses, but sometime it just seemed to make things harder to follow. Referring to things throughout as ""Property k"" – even though some of those properties were given names when first introduced – left me repeatedly flicking up and down through the PDF to refresh myself on what claim was being referred to without any apparent need....

Originality:
 - Pro
   o There is certainly originality of perspective. The authors make some cogent observations on how other prior work has been naive about adopted assumptions and as to what it has achieved (e.g., in the discussion at the start of section 5.1).
 - Con
   o There is not really significant originality of method. The logistic regression model is nothing but straightforward. (It is also highly problematic in learning a separate model for each word with a bunch of hyponyms. This both doesn't give a model that would generalize to novel words or ones with few hyponyms.) Mapping a tree to an interval is fairly trivial, and besides this is just a mapping of representations, it isn't learning a good representation as ML people (or ICLR people) would like. The idea that you can improve recall by using a co-product (disjunction) of intervals is cute, though, I admit. Nice.

Significance 
 - Con
   o I think this work would clearly need more development, and more cognizance of the goals of generalizable representation learning before it would make a significant contribution to the literature. 

Other:
 - p.1: Saying about WordNet etc., ""these resources have a fundamentally symbolic representation, which can not be readily used as input to neural NLP models"" seems misplaced when there is now a lot of work on producing neural graph embeddings (including node2vec, skip-graphs, deepwalk, etc.). Fundamentally, it is just a bad argument: It is no different to saying that words have a fundamentally symbolic representation which cannot be readily used as input to neural NLP models, but the premise of the whole paper is already that we know how to do that and it isn't hard through the use of word embeddings.
 - p.2: The idea of words and phrases living in subset (and disjointness etc.) relationships according to denotation is the central idea of Natural Logic approaches, and these might be cited here. There are various works, some more philosophical. A good place to start might be: https://nlp.stanford.edu/pubs/natlog-iwcs09.pdf
 - p.2: The notions of Property 1 and 2 are just ""precision"" and ""recall"", terms the paper also uses. Do we gain from introducing the names ""Property 1"" and ""Property 2"" for them? I also felt that I wouldn't have lost anything if Property 3 was just the idea that hyponymy is represented as vector subspace inclusion.
 - p.2: fn.2: True, but it seems fair to more note that cosine similarity is very standard as a word/document similarity measure, not for modeling hyponymy, for this reason.
 - p.4: Below the equation, shouldn't it be Q(w', w) [not both w'] and then Q(w', w) and not the reverse? If not, I'm misunderstanding.","The sentiment of the review is mixed, with both positive and negative aspects highlighted. The reviewer appreciates the fresh perspective and interesting remarks but criticizes the lack of significant theoretical or experimental results. The sentiment score is therefore slightly negative at -20. The politeness of the language is generally high, as the reviewer provides constructive feedback and acknowledges the positive aspects of the paper, even though some criticisms are direct. The politeness score is 60.",-20,60
"Summary. The authors empirically investigate the influence of the architecture and the capacity of an NN-model on the transferability of adversarial examples. They also study the influence of the smoothness. From the obtained results, they propose the smoothed gradient attack showing improvements on the transferability of adversarial examples.

Pros.  
* Robustness of neural nets is a challenging problem of interest for ICLR
* The paper is well written
* The experimental study is convincing
* The experimental results for the smoothed gradient attacks are promising

Cons.
* The results of the experimental study are somehow expected
* the idea of smoothing gradients is not new

Evaluation.
The experimental study of the transferability of adversarial examples is well designed. Experimental protocol is convincing. The smoothed gradient attacks improve many previously proposed attacks. Therefore, my opinion is rather positive. But, as a non expert in the field, I am not completely convinced by the novelty of the approach.

Some details.
Typos: That l8 abstract; systems l9 intro; and l2 related work; directly evaluation l2 Section4, must has l-10 p4; 
* the choice \sigma = 15 in Section 6.2 should be justified by the following study
* \sigma is not given in Figure 3(a)","The sentiment of the review is generally positive, as indicated by phrases such as 'the paper is well written,' 'the experimental study is convincing,' and 'the experimental results for the smoothed gradient attacks are promising.' However, there are some reservations about the novelty of the approach, which slightly tempers the overall positivity. Therefore, the sentiment score is 60. The language used in the review is polite and constructive, with no harsh or rude comments. The reviewer provides specific feedback and suggestions in a respectful manner, leading to a politeness score of 80.",60,80
"Summary:

The paper presents a session-based recommendation approach by focusing on user purchases instead of clicks. The method is inspired from concepts of cognitive science which adds an imagination reconstruction network to an actor-critic RL framework in order to encourage exploration.


Comments:

The proposed architecture is an interesting inspiration from Neuroscience which fits into the sequential recommendation problem. However, the motivation of using RL is missing from the technical contribution. Considering a deterministic policy, using LSTMs which already encode sequentiality of states in addition to another component for planning, seem to undermine the role of RL. 

The motivation of creating imagined trajectories instead of actual user trajectories is unclear. On the other hand, there are many traditional planning approaches which are not mentioned such as Monte Carlo Tree Search that simultaneously trade-off exploration and exploitation. 

The literature review is incomplete and misses important contributions on session-based recommendation, particularly, MDP-based methods such as Shani et al., An MDP-based recommender system, 2005 and Tavakol and Brefeld, Factored MDPs for Detecting Topics of User Sessions, 2014 (also see references therein).

Empirically, the authors compare their method to several recent baselines. This renders the empirical part exceptionally strong. Nevertheless, the length of the trajectories is only 2 and instead should be varied empirically to show the usefulness of the reconstruction network. 


Questions:

-How are cold-start situations encountered if items are one-hot encoded?
-Why is there a strong focus on quick adaptation to user sessions? Usually, users tend to search quite a lot before converging; hence, longer sessions possibly better reflect user interests.


Minor:

-Proofreading is necessary
-Table 1 and 2 would be more readable if they were figures
-Figure 3 seems to be taken from Tensorflow runtime convergence plots, which could be dropped given the limited space","The sentiment of the review is moderately positive. The reviewer acknowledges the interesting inspiration from Neuroscience and the strong empirical comparison to recent baselines, which suggests a positive sentiment. However, the review also points out several significant areas for improvement, such as the unclear motivation for using RL, the incomplete literature review, and the need for empirical variation in trajectory length. Therefore, the sentiment score is 30. The politeness of the language is quite high. The reviewer uses polite and constructive language throughout the review, offering specific recommendations and questions without being dismissive or rude. Therefore, the politeness score is 80.",30,80
"The authors introduce a novel distance function between point sets, based on the ""permutation invariance"" of the zeros of a polynomial, calling it ""holographic"" distance, as it essentially depends on all the points of the sets being compared. They also consider two other permutation invariant distances, and apply these in an end-to-end object detection task. These distance functions have time-complexity O(N^2) unlike the previously proposed ""Hungarian distance"" based on the Hungarian algorithm which is O(N^3) in general. Moreover, they authors show that in two dimensions all local minima of the holographic loss are global minima.

Pros: The paper is well written, the ideas are clearly and succinctly presented. Exploiting the connection between 2D point sets and zeros of polynomials is an interesting idea.

Cons: The experimental section could be better. For example, the authors could do simple experiments to show how an optimization algorithm would explore the holographic loss surface (in terms of hitting global/local minima) in dimensions greater than two. Also, in the object detection example, no comparison is given with the Hungarian loss based algorithm of Stewart et al. (2016) (at the very least, the authors could train their neural nets using the Hungarian loss, choosing one optimal permutation at the ""transitioning points"") .","The sentiment of the review is generally positive, as the reviewer acknowledges the novelty and clarity of the paper, as well as the interesting idea of exploiting the connection between 2D point sets and zeros of polynomials. However, the reviewer also points out some areas for improvement, particularly in the experimental section. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer uses polite and constructive language to provide feedback and suggestions for improvement. Therefore, the politeness score is 80.",60,80
"The paper proposed a problem that most prior methods overlooked the underlying dependency of classes on domains, namely p (y|d) \= p(y).   Figure 1 is used to illustrate this issue. 

If the conditional probability of source domain and target domain is not equal (i.e., p(y|x_S) \= p(y|x_T)  ), the optimal invariance can lead the same generalization problem.   Unfortunately, a lot of works has been done [1,2] in matching domain classifier or conditional probability.  It is desirable to discuss the difference between these two problems and compared with the missing references in experiments. 

It is also suggested to conduct the analysis of why the datasets satisfy the assumption of the dependence of class and domains. 

Reference:
[1] Flexible Transfer Learning under Support and Model Shift, NIPS 2014.
[2]Conditional Adversarial Domain Adaptation, NIPS 2018","The review starts by acknowledging the problem proposed by the paper, indicating an understanding and recognition of the issue. However, it quickly moves to point out the shortcomings and suggests additional work that needs to be done, such as discussing differences between problems and conducting further analysis. The language used is neutral and professional, without any overtly positive or negative expressions. The suggestions are constructive and aimed at improving the paper, which indicates a polite tone.",0,50
"This paper proposes to jointly train a classifier with a domain and label-aware word embedding model and a variational Bayes model for sentiment domain adaptation. The model is evaluated on a standard multi-domain sentiment analysis dataset where it achieves convincing results against strong baselines. Extensive ablations are conducted.

Pros:
- The paper is clearly written. I particularly appreciated Figure 1 as it might otherwise be difficult to see the relation of the different components of the model.
- The model achieves convincing results and ablations and analyses are extensive.

Cons:
- None of the presented ideas are entirely novel. The model rather combines many existing ideas successfully.
- The framework consisting of many components (particularly the joint training with CBOW as indicated in the appendix) seems rather brittle and very task-specific. I am concerned if this framework will be able to work on other tasks. I would love to see an evaluation on another dataset. 
- The joint variational Bayes approach seems to be the most interesting aspect of the paper. Despite the ablations, it's not entirely clear to me, though, how useful this component is and if it can be applied beyond this particular model. I would like to see one of the baselines models or another model augmented with this component. 

Questions:
- Do you alternate updates between each of the components or use a more sophisticated multi-task learning strategy when training the word embedding model and the other components jointly? Did you try fine-tuning the trained word embeddings with the classifier?
- Why do you use this particular affine transformation for learning sentiment-specific word embeddings? Did you try, for instance, an MLP as used in [1]?
- You say that you use a classifier q_φ(z |D, c) in order to benefit from more freedom of design for Bayesian inference. Could you elaborate why you use this classifier in addition to the label classifier q_φ(y |D, c)? In Table 6, it seems that it doesn't help much. The use of the term ""classifier"" is confusing at times, as it seems to be both used to refer to the latent variable and the label classifier.

[1] Tang, D., Wei, F., Yang, N., Zhou, M., Liu, T., & Qin, B. (2014). Learning Sentiment-Specific Word Embedding. Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, 1, 1555–1565.","The sentiment of the review is generally positive, as indicated by the praise for the clarity of the writing and the convincing results of the model. However, there are some critical points regarding the novelty and generalizability of the approach. Therefore, the sentiment score is 50. The language used in the review is polite and constructive, with suggestions and questions framed in a respectful manner, leading to a politeness score of 80.",50,80
"The paper proposes to use the technique in VAT to generate adversarial complementary examples in the (K+1)-class semi-supervised learning framework described by the Bad GAN paper. This leads to a formulation that combines the VAT loss and the (K+1)-class classification loss. The paper also provides analysis regarding why VAT is useful for semi-supervised learning.

Pros
1. It is interesting to bridge two state-of-the-art semi-supervise learning methods in a meaningful.
2. Some positive results have been presented in Table 1 and Figure 4.

Cons and questions
1. I don't understand the authors' claim that FAT uses both pushing and pulling operations. It might be true that both Bad GAN and VAT encourage a decision boundary in the low-density region, but how are they different? Are pushing and pulling really different things here?
2. Unfortunately the proposed method does not give substantial improvement over Bad GAN or VAT in terms of accuracy.
3. If using VAT to generate bad samples is a reasonable approach, then based on the theory in Dai et al., the Bad GAN formulation would not need the additional VAT regularization term to guarantee generalization. On the other hand, based on the theory of Proposition 2, VAT itself should be sufficient. Why do we still need the (K+1)-class formulation. It seems that combination of Bad GAN and VAT objectives has not been well motivated or fully justified. Does this explain the fact that not much empirical gain was obtained by this method?
4. The authors try to use Proposition 1 to motivate the use of VAT for generating complementary examples. However, it seems that the authors misinterprets the concept of bad examples proposed in Dai et al. The original definition (which led to the theoretical guarantees in Dai et al) of bad examples is low-density data samples. In the current paper, the authors assume that data samples close to decision boundaries are bad examples. This is not sound because low-density samples are not equivalent to samples close to decision boundaries, especially when the classifier is less perfect. As a result, the theoretical justification of using VAT to sample complementary examples is a bit weak.
5. There is not ablation study of different terms in the objective function.
6. In Figure 4, you can compare your method with Bad GAN without a PixelCNN. Bad GAN does not need a PixelCNN to achieve the reported results in their paper, and their results are reproducible by running the commands given in the github repo. It would be good to add this comparison.","The sentiment of the review is mixed but leans slightly positive. The reviewer acknowledges the interesting aspect of bridging two state-of-the-art methods and notes some positive results. However, the review also contains several critical points and questions about the methodology and its justification, which brings down the overall sentiment. Therefore, the sentiment score is 10. The politeness of the language is quite high. The reviewer uses polite language, such as 'I don't understand,' 'It would be good to add,' and 'Unfortunately,' which indicates a respectful tone. Therefore, the politeness score is 80.",10,80
"This paper applied an object detection network, like SSD, for optical character detection and recognition. This paper doesn't give any new contributions and has no potential values.

weakness:
1. the paper is lack of novelty and the motivation is weak. I even can't find any contribution to OCR or object detection.

2. the paper is written badly so that I can't follow easily. In addition, the figures and tables are not always explained in the main body, which makes the experimental results confusing.

3. There are no titles in the figures and tables in this paper

4. the authors don't confirm the superiority of the proposed method to others.

minor comments
1. what's the meaning of Target mAP in the table?
2. It seems that Some figures are cropped from TensorBoard, with some extra shadows.","The sentiment of the review is clearly negative, as the reviewer states that the paper doesn't give any new contributions and has no potential values. The language used is quite direct and critical, pointing out several weaknesses without any positive remarks. The politeness score is low because the language is blunt and lacks any mitigating phrases or constructive feedback.",-90,-50
"\clarity & quality
The paper is easy to follow and self-contained. 
However, the motivation for minimizing the upper bound is not so clear for me. 
As far as I understood from the paper, changing the objective function to the upper bound of f-divergence have two merits compared to the existing methods. One is that by using the reverse KL, we can obtain sharper outputs, and the second one is that the optimization process will be stable compared to that of the lower bound. 
In the introduction, the author just mentioned that ""the f-divergence is generally computationally intractable for such complex models. The main contribution of our paper is the introduction of an upper bound on the f-divergence.""
For me, this seems that the author just introduced the new fancy objective. I think the motivation to introduce the new objective function should be stated clearly.

\originality & significance
Although the upper bound of the f-divergence is the trivial extension, the idea to optimize the upper bound for the latent model seems new and interesting.

However, it is hard to evaluate the usefulness of the proposed method from the current experiments.
It seems that there are two merits about the proposed method as above.
The only evidence that the learning tends to be stable is the Fig.8 in the appendix, but this is just the fitting of univariate Gaussian to a mixture of Gaussians, thus it is too weak as the evidence.
About the sharp output, there are already many methods to overcome the blurred output of the usual VAE. No comparison is done in the paper.
So I cannot tell whether the proposed objective is really useful to learn the deep generative models.
I think further experimental results are needed to validate the proposed method.

\Question
In page 4,  the variance of the p(y|x) and p_\theta(y|z) are set to be the same. What is the intuition behind this trick? 
Since this p(y|x) is used as the estimator for the log p(y) as the smoothed delta function whose Gaussian window width (the variance), and the Gaussian window width is crucial to this kind of estimator, I know why the author used this trick.","The sentiment of the review appears to be slightly positive, as the reviewer acknowledges the originality and potential significance of the proposed method, even though they express concerns about the clarity of the motivation and the sufficiency of the experimental validation. Therefore, the sentiment score is 20. The politeness of the language used in the review is quite high. The reviewer uses polite phrases such as 'I think,' 'it seems,' and 'I cannot tell,' which indicate a respectful and constructive tone. Therefore, the politeness score is 80.",20,80
"This paper provide a modification on the classical LSTM structure. Specifically, it reformulate the forget gate with a monotonically decreasing manner, using sinusoidal function as the activation function. 

However, both the motivation and experimental results on such modification are not convincing enough. 

1. While there are many heuristic guesses in sec3, important supports of these guesses are missed. For example, Figure 2 is designed to provide supports for the claim that we need controlled forget gates.  However, all the values of forget gates and input gates in Figure 2 are manually set as *conceptual observations*, which provides limited insight on what will happen in the real cases. While the reformulation in sec4 is based on the observations in Figure 2, it is important to plot the real cell propagation after the reformulation, and see whether the real observation meets the conceptual observations in Figure 2.
BTW, Plots in Figure 2 only account for LSTMs' propagation within 3 steps, but in real cases there are way more steps. 

2. The authors claim monotonic propagation in the constant forget gates is more interpretable than those of the vanilla-LSTM, as no abrupt shrinkage and sudden growth are observed. But it isn't straightforward to get the relations between abrupt shrinkage and sudden growth on forget gates and the expressive power of the vanilla-LSTM. Also, it's hard to say the monotonic propagation is more interpretable because we don't know what's the meaning of such propagation on the behaviors of LSTMs in applications. 

3. The reformulation in sec 4, especially the formula for the forget-polar input p_k, looks heavily hand-crafted, without experimental supports but statements such as ""we ran numerous simulations"", which is not convincing enough. 

4. Experiments are applied on MNIST and Fashion-MNIST. While both datasets are not designed in nature for sequential models like LSTMs. There are better datasets and tasks for testing the proposed reformulation.   e.g. sentence classification, text generation, etc.  No explanation on the choice of datasets.  In addition, the difference between vanilla-LSTM and DecayNet-LSTM is small and it's hard to say it isn't marginal. Maybe larger-scale datasets are needed. 

5. Lacking of explanation on specific experimental settings. E.g. training all methods for *only one epoch*, which is very different from the standard practice.  

6. More qualitative interpretations for real cell states in both vanilla LSTM  and DecayNet-LSTM are needed. Only conceptual demonstration is included in Figure 2. ","The sentiment of the review is moderately negative, as it highlights several significant issues with the paper, such as unconvincing motivation and experimental results, lack of important supports for heuristic guesses, and insufficient explanation of experimental settings. The politeness of the language used is relatively high, as the reviewer provides constructive criticism and specific recommendations without using harsh or rude language.",-60,80
"This paper describes a two-stage encoder-decoder model for semantic parsing. The model first decodes a cross-domain schema (CDS) representation from the input utterance, then decodes the final logial form from both the utterance and CDS. The model outperforms other multitask Seq2Seq models on the Snips (Goo et al., 2018) dataset, but is still behind the traditional slot-filling models (Goo et al., 2018).

My main concern is that it is unclear to me how CDS (cross-domain schema) can be generalized to the other semantic parsing datasets, e.g., the Overnight dataset (Wang et al., 2015), which also contains multiple domains. 

I think it would be nice to have some details about the CDS in the paper. For example, I’m wondering 1) how is this CDS designed? 2) how are the CDS annotations derived from the target output? 

There are other details missing regarding the comparisons and the evaluation metrics. In 4.2, the authors mentioned “We use accuracy as the evaluation metric’’, does “accuracy” mean full logical form accuracy or accuracy on execution results?

* More minor comments:
In the first paragraph of Section 3, “irrelevant to domain’’ -> “domain-general’’ or “domain-agnostic’’?

It will be nice to write something more specific than “explore more ways to make it work better” in the future work.

This paper has some grammatical errors and formatting issues (e.g. missing space before punctuations).

* Missing references:
Neural semantic parsing over multiple knowledge-bases, Herzig and Berant, ACL 2017 <- This paper explores shared encoder/decoder for multi-domain semantic parsing, which is very related.

(Concurrent) Decoupling Structure and Lexicon for Zero-Shot Semantic Parsing, Herzig and Berant, EMNLP 2018 
","The sentiment of the review is mixed. The reviewer acknowledges the model's performance but expresses significant concerns about the generalizability of the CDS to other datasets and the lack of details in the paper. Therefore, the sentiment score is around -20. The politeness of the language is generally high, as the reviewer uses phrases like 'I think it would be nice' and 'It will be nice,' and provides constructive feedback without being harsh or dismissive. Thus, the politeness score is 80.",-20,80
"This paper adds to a growing body of literature which suggests that modern CNNs use qualitatively different visual strategies for object recognition compared to human observers. More specifically, the authors create shapeless object features (by adding noise masks in various forms or single pixels that are predictive of categorization to object images) to study how much CNNs rely on shape information (as humans would) as opposed to shapeless arbitrary statistical dependencies between pixels. 

The hypotheses tested are straightforward and the experiments cleverly answer these questions. On the negative side, there is nothing groundbreaking in this study. As acknowledged by the authors, the results are not all that novel in light of recent work that has already shown that one could conduct adversarial attacks by corrupting a single pixel as well as work that has shown that CNNs do not generalize to noise degradations they have not seen. Still, there is value in the work presented as the empirical tests described address the role of shape in object recognition with CNNs.

In a sense, the present study offers a null result and obviously, the work would have been much more significant had the authors offered a mechanism to get CNNs to learn to prioritize ""shape"" features (then verifying that such network would work on CIFAR, but performed poorly on the shapeless images).

Additional analysis involving visualization methods to further explain why shape features were ignored would have been a plus– with bonus points for providing a heuristic to determine the ""shapelessness"" of a convolution kernel.","The sentiment of the review is mixed but leans slightly positive. The reviewer acknowledges the value and cleverness of the experiments but also points out that the findings are not groundbreaking and somewhat redundant with existing literature. Therefore, the sentiment score is 20. The politeness of the language is quite high; the reviewer provides constructive criticism and acknowledges the authors' efforts without being dismissive or rude. Hence, the politeness score is 80.",20,80
"This paper presents a method to train NNs as black box estimators of the commitor function for a physical, statistical mechanical, distribution. This training is performed using samples from the distribution. As the committor function is used to understand transitions between modes of the distribution, it is important that the training samples include points between modes, which are often extremely low probability. To address this concern, this paper draws MCMC samples at a high temperature, and then uses importance weights when training the committor function using these samples. Overall -- this seemed like a good application paper. It applies largely off-the-shelf machine learning techniques to a problem in physics. I don't have enough background to judge the quality of the experimental results.

I had one major concern: the approach in this paper is motivated as a solution to estimating commitor functions in high-d. The variance of importance sampling estimates typically increases exponentially in the dimensionality of the problem, so I suspect this technique as presented would fall apart quickly if pushed to higher dimensions. All experiments are on problems with either 9 or 10 (effective) degrees of freedom, which from the ML perspective at least is quite low dimensional, and which is consistent with this exponentially poor scaling. There are likely fixes to this problem -- e.g. the authors might want to look into annealed importance sampling*.

more specific comments:

""and dislocation dynamics"" -> ""dislocation dynamics""

""One can easily check"" -> ""One can check"" :P

eq 5 -- this is very sudden deep water! Especially for an ML audience. You should either give more context for the Kolmogorov backward equation, or just drop it. (The Kolmogorov formulation of the problem is not used later, and for an ML audience describing the task in terms of it will confuse rather than clarify.)
what is \Delta q? Does that indicate the Laplacian? Not standard ML notation -- define.

similarly, define what is intended by \partial A and \partial B (boundary of the respective regions?)

eq. 9 -- nit -- recommend using a symbol other than rho for regularization coefficient. visually resembles p, and is rarely used this way. lambda is very common.

eqs 10/11 -- include some text motivation for why the definition of chi explicitly excludes the regions inside A and B.

eq 14: cleverly formulated!

eq 14 / eq 20:
factor of 1000 is very fast! corresponds to an epsilon of O(1e-3). You need to make sure that training samples are generated in the epsilon width border around A and B, otherwise the effect of chi will be invisible when training q_theta. So it seems like epsilon should be chosen significantly larger than this. Might want to include some discussion of how to choose epsilon.

* Totally incidental to the current context, but fascinatingly, annealed importance sampling turns out to be equivalent to the Jarzynski equality in nonequilibrium physics.","The sentiment of the review is generally positive, as indicated by phrases like 'this seemed like a good application paper' and 'cleverly formulated.' However, the reviewer also expresses a significant concern about the scalability of the method to higher dimensions, which tempers the overall positivity. Therefore, the sentiment score is 50. The politeness of the language is high, as the reviewer uses polite suggestions and constructive feedback, such as 'recommend using a symbol other than rho' and 'might want to include some discussion.' The reviewer also uses polite language even when pointing out major concerns, so the politeness score is 80.",50,80
"The paper is extremely difficult to read. There are too many concepts introduced at once, casual comments mixed with semi-formal statements. The theorems sound interesting, the implications are grand and of interest to ICLR, but the proofs are impossible to follow. As such, I am not in a position to make a recommendation. 

I strongly recommend the authors to split the paper into multiple parts with clear-cut statements in each, with clear and detailed proofs, and submit to appropriate journals / conferences. 
","The sentiment of the review is quite negative, as it highlights significant issues with the readability and structure of the paper, making it difficult to follow and understand. The reviewer does acknowledge that the theorems sound interesting and the implications are grand, but the overall tone is critical. Therefore, the sentiment score is -60. The politeness of the language is relatively high, as the reviewer provides constructive feedback and recommendations without using harsh or disrespectful language. The reviewer uses phrases like 'I strongly recommend' and 'clear-cut statements,' which are polite and aimed at helping the authors improve their work. Therefore, the politeness score is 60.",-60,60
"This paper insists layer-level training speed is crucial for generalization ability. The layer-level training speed is measured by  angle between weights at different time stamps in this paper. To control the amount of weight rotation, which means the degree of angle movement, this paper proposes a new algorithm, Layca. This algorithm projects the gradient vector of SGD (or update vector of other variants) onto the space orthogonal to the current weight vector, and adjust the length of the update vector to achieve the desirable angle movement. This paper conducted several experiments to verify the helpfulness of Layca.

This paper have an impressive theme, the layer-level training speed is important to have a strong generalization power for CNNs. To verify this hypothesis, this paper proposes a simply SGD-variant to control the amount of weight rotation for showing its impact on generalization. This experimental study shows many insights about how the amount of weight rotation affect the generalization power of CNN family. However, the contribution of this paper is limited. I thought this paper lacks the discussion of how much the layer-level training speed is important. This paper shows the Figure 1 as one clue, but this figure shows the importance of each layer for generalization, not the importance of the layer-level training speed. It is better to show how and how much it is important to consider the layer-level training speed carefully, especially compared with the current state-of-the-art CNN optimization methods or plain SGD (like performance difference).

In addition, figures shown in this paper are quite hard to read. Too many figures, too many lines, no legends, and these lines are heavily overlapped. If this paper is accepted and will be published, I strongly recommend authors choose some important figures and lines to make these visible, and move others to supplementary material.","The sentiment of the review is moderately positive, as the reviewer acknowledges the impressive theme and insights provided by the paper but also points out significant limitations and areas for improvement. Therefore, the sentiment score is 30. The politeness of the language is quite high, as the reviewer uses polite and constructive language throughout, offering recommendations in a respectful manner. Thus, the politeness score is 80.",30,80
"This paper describes the method for performing self-training where the unlabeled datapoints are iteratively added to the training set only if their predictions by the classifier are confident enough. The contributions of this paper are to add datapoints based on the prediction of the confidence level by a separate selection network and a number of heuristics applied for better selection. On the experimental side, the contribution is to test the scenario where datapoints from irrelevant classes are included in the unlabeled dataset.
The paper is written in a way that makes following it a bit difficult, for example, the experimental setups. Also, the writing can be improved by making the writing more concise and formal (examples of informal: ""spoil the network"", ""model is spoiled"", ""problem of increased classes"", ""many recent researches have been conducted"", ""lots of things to consider for training"", ""supervised learning was trained"" etc.). The contributions of the method could also be underlined more clearly in the abstract and introduction. The description of consistency regularisation methods in section 2.2 is not very clear and I would like to get better understanding of temporal ensembling and SNTG methods here as they play an important role in the experiments. 
The idea of selective sampling for self-training is promising and the investigated questions are interesting. As far as I understand, the main contribution of this paper is the use of separate ""selection network"" to estimate the confidence of predictions by ""classification network"". However, as the ""selection network"" uses exactly the same input as ""classification network"", it is hard to imagine how it can learn additional information. For example, imagine the case of binary classification. If the selection network predicts 0 in come cases, it can be used to improve the result of ""classification network"" by flipping the corresponding label. How can you interpret such a thought experiment? One could understand the use of ""selection network"" as a way to automatically select a threshold of what to consider confident, however, in this case, the prediction of ""selection network"" should be thresholded at 0.5 (correct prediction or not), but the experiments use complex thresholds. Could you elaborate more on why the selection network is needed? How would it compare to a simple strategy of only including the datapoints whose top-1 prediction of ""classification network"" is greater than some threshold? Finally, could you show a plot of top-1 prediction of ""classification network"" vs score of ""selection network"" and elaborate on that?
Then, in sections 3.2 and 3.3 the authors introduce a few additional tricks for self-training: exclude datapoints whose predictions are changing and balance the classes. Intuitively, these criteria are well motivated, but unfortunately, the combination of all the intuitions (including ""selection network"" with threshold) is not very principled. Ablation study shows that the use of the ""selection network"" strategy does not improve the results without these heuristics. It would be interesting to see how these heuristics would do without ""selection network"", for example, either by doing simple self-training with thresholding on the score of the classifier or by applying only these heuristics in combination with TempEns+SNTG. In the current form of evaluation, it is hard to say if there is any benefit of using the ""selection network"" that is the main novelty of the paper.
It is very valuable that the experimental results include many recently proposed methods. Besides, the settings are described in details that could help for the reproducibility of the results. However, I have a few concerns about the results. First of all, the proposed SST algorithm alone only performs better than baselines in 1 case, equal to them in 1 case and worse in 1 (table 3). Besides, as the base classifier is different for various baselines, it is hard to compare the methods. Then, the important hyperparameter of the method---threshold---seems to be hard to select (both in sections 4.1 and 4.2). How did you chose the current values? How sensitive is it? Why various datasets need different settings? How the threshold value can be set in practice? Another important parameters is the number of iterations of the algorithm. How was it chosen? Concerning the experiments of section 4.2, how would the baseline methods of section 4.1 do in this case? Why did you select to study animal vs non-animals sets of classes? What would happen if you use random class splits or split animal classes (like in a more realistic scenario)? 
To conclude, while I find the studied problem quite interesting and intuitions behind the method very reasonable, the current methodology is not very principled and the experiment evaluation did not convince me that such an elaborate strategy is needed.

Some questions and comments:
- The setting of including unrelated classes in the unlabeled data resembles transfer learning setting. Could you explain why the ideas from transfer learning are not applicable in your case?
- In the training procedure of ""selection network"" of Sections 3.1, do you use the same datapoints to train a ""classification network"" and ""selection network""? If it is the case, how do you insure that the ""classification network"" does not learn to fit the data perfectly and thus all labels s_i are 1?
- In the last sentences of the first paragraph on p.2 you make a contrast between using softmax and sigmoid functions, however, normally the difference between them is their use in binary or multiclass classification. Is there anything special that you want to show in you case?
- What do you mean in section 3.3 by ""if one class dominates the dataset, the model tends to overfit""?
- I think parameters of training the networks from the beginning of section 4 could be moved to the supplementary materials.
- Figure 3: wouldn't the plot of accuracy vs amount of data be more suitable here?
- Synthetic experiments of supplementary materials: the gains of the methods seem to be small. What are the numerical results? What would happen if you allow to select starting point at random (a more realistic case)?
- Can you explain the sentence ""To prevent data being added suddenly, no data was added until 5 iterations""?
- How was it possible to improve the performance in experiment of section 4.2 with 100% of irrelevant classes?","The sentiment of the review is mixed but leans towards the negative side. The reviewer acknowledges the interesting and promising aspects of the paper but raises several significant concerns about the methodology, clarity, and experimental evaluation. This results in a sentiment score of -30. The politeness of the language is generally maintained throughout the review, with the reviewer providing constructive criticism and suggestions for improvement without being rude or dismissive. Therefore, the politeness score is 70.",-30,70
"This paper formulates feature attribution from a feature selection perspective, and compares EFS (Exclusive Feature Selection) and IFS (Exclusive Feature Selection), which shows IFS is a better fit for feature attribution.

[+] The paper is well-structured and the proposed approach is clearly presented.
[-] It would helpful if the author could discuss the time complexity of proposed methods and compare the running time with baseline methods in evaluation.
[-] My major concern on this paper is the significance, as the contribution of the paper seems to be very limited.
    1) Formalizing the feature attribution problem as a feature selection problem is straightforward. IFS and EFS are just Forward and Backward stepwise feature selection, which are classic feature selection schemes. Applying them to feature attribution/saliency map does not seem to have much technical contribution.
    2) One claimed contribution of this paper is that existing feature attribution methods can be viewed as approximation of IFS and EFS. However, this contribution also seems to be minor. As many feature selection methods are known to be approximation of backward or forward stepwise feature selection, it is straightforward to show the connection between other feature attribution methods and IFS/EFS.

In conclusion, I would recommend to reject this paper due to the limited novelty and technical contribution.
","The sentiment score is derived from the overall tone of the review. The reviewer acknowledges the clarity and structure of the paper (+10), but raises significant concerns about the novelty and technical contribution (-70). This results in a sentiment score of -60. The politeness score is based on the language used. The reviewer uses polite language, such as 'It would be helpful if' and 'I would recommend,' and avoids any rude or harsh terms, resulting in a politeness score of 80.",-60,80
"The paper proposed a RNN with skip-connection (external memory) to past hidden states, this is a slightly different version of the TARDIS network. The authors experimented on PTB and a temporal action detection method.

Novelty:

I dont see a lot of novelty to the method. The authors proposed a method very similar to TARDIS, the difference seems to be that MMARNN does not use extra usage vectors for reading from previous memory, but this is not a fundamental difference between MMARNN and Tardis.

Shortcomings of the paper:

1. The experiments seem rather weak. The authors experimented on PTB and temporal action detection method. It is not clear why authors experimented with PTB, this is not a task with long-term dependencies, I do not see how this task (compared to many other tasks) can benefit from using external memory (especially when only 1 past hidden state is used

2. The model uses a single past hidden state, it is not clear to me why this is better than using  a weighted sum of a few past hidden states, as many tasks requires long-term dependencies from multiple steps in the past. The authors should cite ""Sparse attentive backtracking"" (https://arxiv.org/abs/1809.03702) at NIPS 2018. SAB is very related in that it also propagate gradients to a few hidden states in the memory. The difference is that SAB used a few hidden states from the past/ memory instead of one; another difference is that it propagates gradients locally to the selected hidden states/ memory slots.

3. The paper only demonstrated experimental results on PTB and temporal action prediction. I think it would make the paper a lot stronger if the authors experimented with a variety of  different tasks. Tasks that requires long term dependencies can really demonstrate the strength of the model (copy and adding tasks).

4. If the authors could run the model on copy and adding tasks, I would be curious to see if the model is picking the ""correct"" timestep in the memory / past.

post rebuttal: I feel that the authors have addressed some of my concerns, in particular, in terms of additional experimental results. I have raised the score to reflect this changes.
","The sentiment score is derived from the overall tone of the review, which is critical but not overly negative. The reviewer acknowledges some improvements post-rebuttal, indicating a slightly positive shift. Therefore, the sentiment score is set at -20. The politeness score is based on the language used, which is professional and constructive, even when pointing out shortcomings. The reviewer provides specific recommendations and cites relevant work, which is indicative of a polite and respectful tone. Thus, the politeness score is set at 80.",-20,80
"This paper suggests a continuous-time framework consisting of two coupled processes in order to perform derivative-free optimization. The first process optimizes a surrogate function, while the second process updates the surrogate function. This continuous-time process is then discretized in order to be run on various machine learning datasets. Overall, I think this is an interesting idea as competing methods do have high computational complexity costs. However, I’m not satisfied with the current state of the paper that does not properly discuss notions of complexity of their own method compared to existing methods.

1) “The computational and storage complexity for (convex) surrogates is extremely high.” The discussion in this paragraph is too superficial and not precise enough.
a) First of all, the authors only discuss quadratic models but one can of course use linear models as well, see two references below (including work by Powell referenced there):
Chapter 9 in Nocedal, J., & Wright, S. J. (2006). Numerical optimization 2nd.
Conn, A. R., Scheinberg, K., & Vicente, L. N. (2009). Global convergence of general derivative-free trust-region algorithms to first-and second-order critical points. SIAM Journal on Optimization, 20(1), 387-415.
I think this discussion should also be more precise, the authors claim the cost is extremely high but I would really expect a discussion comparing the complexity of this method with the complexity of their own approach. As discussed in Nocedal (reference above) the cost of each iteration with a linear model is O(n^3) instead of O(n^4) where n is the number of interpolation points. Perhaps this can also be improved with more recent developments, the authors should do a more thorough literature review.
b) What is the complexity of the methods cited in the paper that rely on Gaussian processes?
(including (Wu et al., 2017) and mini-batch (Lyu et al., 2018)).


2) “The convergence of trust region methods cannot be guaranteed for high-dimensional nonconvex DFO”
Two remarks: a) This statement is incorrect as there are global convergence guarantees for derivative-free trust-region algorithms, see e.g.
Conn, A. R., Scheinberg, K., & Vicente, L. N. (2009). Global convergence of general derivative-free trust-region algorithms to first-and second-order critical points. SIAM Journal on Optimization, 20(1), 387-415.
In chapter 10, you will find global convergence guarantees for both first-order and second-order critical points.
b) The authors seem to emphasize high-dimensional problems although the convergence guarantees above still apply. For high-order models, the dimension does have an effect, please elaborate on what specific comment you would like to make. Finally, can you comment on whether the lower bounds derived by Jamieson mentioned depend on the dimension.

3) Quadratic loss function
The method developed by the authors rely on the use of a quadratic loss function. Can you comment on generalizing the results derived in the paper to more general loss functions? It seems that the computational complexity wouldn’t increase as much as existing DFO methods. Again, I think it would be interesting to give a more in-depth discussion of the complexity of your approach.

4) Convergence rate
The authors used a perturbed variant of the second-order ODE defined in Su et al. 2014. The noise added to the ODE implies that the analysis derived in Su et al. 2014 does not apply as is. In order to deal with the noise the authors show that unbiased noise does not affect the asymptotic convergence. I think the authors could get strong non-asymptotic convergence results. In a nutshell, one could use tools from Ito calculus in order to bound the effect of the noise in the derivative of the Hamiltonian used in Lemma 1. See following references:
Li, Q., Tai, C., et al. (2015). Stochastic modified equations and adaptive stochastic gradient algorithms. arXiv preprint arXiv:1511.06251.
Krichene, W., Bayen, A., and Bartlett, P. L. (2015). Accelerated mirror descent in continuous
and discrete time. In Advances in neural information processing systems, pages 2845–2853.
Of course, the above works rely on the use of derivatives but as mentioned earlier, one should be able to rely on existing DFO results to prove convergence. If you check Chapter 2 in the book of Conn et al. (see reference above), you will see that linear interpolation schemes already offer some simple bounds on the distance between the true gradient of the gradient of the model (assuming Lipschitz continuity and differentiability).

5) Noise
“The noise would help the system escape from an unstable stationary point in even shorter time”
Please add a relevant citation. For isotropic noise, see
Ge, R., Huang, F., Jin, C., and Yuan, Y. Escaping from saddle points-online stochastic gradient for tensor decomposition.
Jin, C., Netrapalli, P., and Jordan, M. I. Accelerated gradient descent escapes saddle points faster than gradient descent. arXiv preprint arXiv:1711.10456,

6) Figure 2
Instead of having 2 separate plots for iteration numbers and time per iteration, why don’t you combine them to show the loss vs time. This would make it easier for the reader to see the combined effect.

7) Empirical evaluation
a) There are not enough details provided to be able to reproduce the experiments. Reporting the range of the hyperparameters (Table 2 in the appendix) is not enough. How did you select the hyperparameters for each method? Especially step-size and batch-size which are critical for the performance of most algorithms. 
b) I have to admit that I am not extremely familiar with common experimental evaluations used for derivative-free methods but the datasets used in the paper seem to be rather small. Can you please justify the choice of these datasets, perhaps citing other recent papers that use similar datasets?

8) Connection to existing solutions
The text is quite unclear but the authors seem to claim they establish a rigorous connection between their approach and particle swarm (“In terms of contribution, our research made as yet an rigorous analysis for Particle Swarm”). This however is not **rigorously** established and needs further explanation. The reference cited in the text (Kennedy 2011) does not appear to make any connection between particle swarm and accelerated gradient descent. Please elaborate.

9) SGD results
Why are the results for SGD only reported in Table 1 and not in the figure? Some results for SGD are better than for P-SHE2 so why are you bolding the numbers for P-SHE2?
It also seem surprising that SGD would achieve better results than the accelerated SGD method. What are the possible explanations?

10) Minor comments
- Corollaries 1 and 2 should probably be named as theorems. They are not derived from any other theorem in the paper. They are also not Corollaries in Su et al. 2014.
- Corollary 2 uses both X and Z.
- Equation 5, the last equation with \dot{V}(t): there is a dot missing on top of the first X(t)
“SHE2 should enjoy the same convergence rate Ω(1/T) without addressing any further assumptions” => What do you mean by “should”?
- There are **many** typos in the text!! e.g. “the the”, “is to used”, “convergeable”,... please have someone else proofread your submission.
","The sentiment of the review is mixed. The reviewer acknowledges the interesting idea presented in the paper but expresses dissatisfaction with the current state of the paper, particularly regarding the discussion of complexity. The sentiment score is therefore slightly positive but tempered by significant criticism. The politeness of the language is high; the reviewer uses polite phrases such as 'I think,' 'please,' and 'I would really expect,' and provides constructive feedback without being rude or dismissive.",20,80
"The recent work of Schott et al (which the authors compare results to) proposed the use of Bayes rule inversion as a more robust mechanism for classification under different types of adversarial attacks. The probabilities are approximated with variational autoencoders. During training the inference network is used, but during testing optimization based inference is carried out to compute loglikelihoods.

This paper focuses on the second part, with a different model. Specifically, it proposes a specific Boltzmann machine to be used as a first layer of neural networks for MNIST classification. This Boltzmann machine is pre-trained in two-stages using mean field inference of the binary latent variables and gradient estimation of the parameters.  This pre-trained model is then incorporated into the neural net for MNIST classification.  The existence of couplings J_h among the hidden units means that we have to carry out mean field inference over several iterations to compute the output activations of the model. This is basically analogous to the optimization-based inference proposed by Schott et al. (As a detail, this optimization can be expressed as computation over several layers of a neural net.)

The authors compare to the work of Schott for one type of attack. It would be nice to see more detailed experiments as done in Schott.

Questions:
1- Why not use a single Boltzmann machine with 128 fully connected latent variables? Could you add this experiment please.
2- Why is a two-stage pre-training (Figure 2) process needed? Why not just a single stage?
3- Is the key that you used only 679 patches containing 98% of occurrences in the dataset as the first stage? What if we vary this percentage? How sensitive are the results? Such experiments could be useful to understand better why your method appears to work well.
4- Could you please add the found J_h's to the appendix. This architecture reminds me of the good old MRFs for image denoising. Could it be that what we are seeing is the attack being denoised?

I am puzzled and looking forward to answers to the above questions. I don't yet understand what is the thing that makes this approach appear to work, or why you were able to drop the Bayes inference inversion altogether as done by Schott. 

Thanks in advance. I will re-adjust the review rating following your reply.



","The sentiment of the review appears to be neutral to slightly positive. The reviewer acknowledges the work done by the authors and expresses interest in understanding the approach better, but also raises several questions and requests additional experiments. This indicates a balanced view with a slight inclination towards positive interest. The politeness of the language used is high. The reviewer uses polite phrases such as 'It would be nice to see,' 'Could you please,' and 'Thanks in advance,' which indicate a respectful and constructive tone.",20,90
"The main difficulty of neuroevolution---requiring a huge number of simulations for high dimensional problem---is addressed in this paper by introducing VAE to reduce the state space dimensionality and using a rather shallow controller network. This idea itself is very promising, however, it has been introduced in (Ha and Schmidhuber, 2018).  Still, there seems to be differences in how to gather histories and how to use them. Nevertheless, the differences are not well described in the text. The effect of the modification is not evaluated on experiments.","The review acknowledges the promise of the idea presented in the paper, which is a positive sentiment. However, it also points out that the idea has been previously introduced and criticizes the lack of detailed description and experimental evaluation of the differences. This mix of positive and negative feedback results in a sentiment score that is slightly positive. The language used in the review is neutral and factual, without any rude or overly critical remarks, indicating a neutral politeness score.",20,0
"The paper introduces a novel architecture for low-resource named entity tagging: a dual adversarial transfer network, in which fusion between high- and low-resource, or high- and low-noise data is achieved via also novel resource-adversarial discriminator.

The model is interesting, novel, clearly exposed in sufficient detail, and warrants publication as such. The idea to unify representation differences and data imbalance under one model is noteworthy.

I find that the description of related work, especially in the introduction, does not credit past contributions sufficiently. For one, large parallel corpora do exist for many languages, albeit some of them may not be sufficiently ample in named entities to facilitate cross-lingual NER. Yet, for the fortunate ones, such corpora do make for rather reasonable NER taggers via multi-source projection (cf. Enghoff et al., W-NUT 2018). Absent is the prominent work by Mayhew et al. (2017) in cross-lingual NER, as well as Pan et al. (2017) who engage with evaluation in 282 languages.

This unfair account of related work would not trouble me as much if it weren't coupled with an experiment in ""low-resource"" NER that features---Spanish and Dutch as target languages. Firstly, these languages are rich in resources, after all, they featured in CoNLL 2003, for one. Secondly, they are closely related to English as the source language, and any simulated low-resource scenario that features both the injection of target-language data *and* a very closely related source language is simply *not* representative of any true low-resource scenario.

This experiment setup troubles me, especially in light of real and synthetic NER data available to test the setup for true low-resource languages: from silver data by Al-Rfou et al. (2015) or Pan et al. (2017), via Mayhew et al. (2017) or Cotterrell and Duh (2017) who test on 10-15 gold datasets, etc., real low-resource NER data that is multilingual can be found. Any paper that in 2018 claims to do low-resource NER and then simulates a setup with Dutch and Spanish is poor scholarship in my submission, regardless of the clever model.

I do let the clever model upvote my review, but not beyond borderline.

Minor:
- the use of ""lingual"" as noun is rather off-putting, at least to me","The sentiment of the review is mixed. The reviewer acknowledges the novelty and clarity of the model, which warrants publication, but expresses significant concerns about the experimental setup and the representation of related work. Therefore, the sentiment score is 20, reflecting a slightly positive but cautious stance. The politeness of the language is generally respectful, though the reviewer does use strong language to express their concerns (e.g., 'poor scholarship'). However, the critique is framed constructively and professionally, so the politeness score is 50.",20,50
"The paper investigates different machine learning approaches to model and
predict the return on property investments, in particular with respect to eager
and lazy learning techniques. The authors evaluate those different techniques on
a dataset of properties in Virginia. They conclude that lazy techniques provide
better performance than eager ones.

The paper is a purely empirical study that does not introduce any novel
machine learning or evaluation techniques. The authors use the off-the-shelf
WEKA toolbox. The results are not clear, given that only a single data set was
used to evaluate the different approaches, and general recommendations cannot be
made.

The paper is not well written and the descriptions do not convey what the
authors have done very well. An example of this is Figure 3, which purports to
show the average rent (or rent distribution?) for different housing types. There
are multiple categories in there that are not valid housing types (""Make Me
Move"") and the rents shown are incorrect (e.g. more than a million for a town
house). It is also unclear why the average rent for a single family house is
approximately 4 times as much as for a town house.

The problems with Figure 3 are exemplary of the paper; the other issues are too
numerous to list.

In summary, this paper should be rejected.","The sentiment of the review is quite negative, as the reviewer points out several significant flaws in the paper, including the lack of novelty, unclear results, poor writing, and incorrect data representation. The reviewer concludes with a recommendation to reject the paper. Therefore, the sentiment score is -80. The politeness of the language is somewhat neutral to slightly negative. While the reviewer does not use overtly rude language, the tone is quite blunt and critical, especially in phrases like 'The paper is not well written' and 'the other issues are too numerous to list.' Thus, the politeness score is -20.",-80,-20
"This paper describes a meta-RL algorithm through imitation on RL policies. While the paper builds nicely up to the core part, I find essential details missing about the imitation setup. By glancing at previous BC papers (some of which are cited), the quantity for supervised imitations, etc., were clearly defined. 

It will be useful for this reviewer if the authors can provide more clarity in explaining the BC task involved in their algorithm.","The sentiment of the review is slightly negative, as the reviewer points out missing essential details and compares the paper unfavorably to previous work. However, the reviewer acknowledges that the paper builds nicely up to the core part, which prevents the sentiment from being entirely negative. Therefore, the sentiment score is -20. The politeness of the language is quite high; the reviewer uses polite phrases such as 'It will be useful for this reviewer if the authors can provide more clarity,' which indicates a respectful and constructive tone. Therefore, the politeness score is 80.",-20,80
"This paper proposes to learn a manifold of deep generative models using a pre-trained VAE. To generate samples with desired features, this paper proposes to learn an embedding of each feature in the hidden space using VAE. Then the learned hidden space is used to train a GAN.

However, the method in this paper and main contributions are not clearly represented. I can hardly understand the motivation of this paper. In the introduction part, this paper mentions “large scale of latent space” lots of times, but does not make it clear that why a large latent space hinders the deep generative models. In Fig.1, it demonstrates that for some manifold, L2 distance cannot be applied directly. However, for most DGMs, the hidden space is defined in Euclid Space, and L2 distance is a valid distance for them. 

In Sec. 3, the method is not presented clearly and the notation is confusing. In Sec. 3.2.1, Eqn (8) is not an objective function and it is confusing how to optimize the generator using it. In Sec. 3.2.2, the notation is really confusing and I can hardly understand the proof the Theorem 2.

The experimental results are not solid where no well-known metrics, such as Inception Score, FID, are used to evaluate the generated samples. For compression rate, the size of bottleneck has not been mentioned above, and the experimental setting of each baseline is not ignored which makes the experimental results incomparable.

Overall, this paper is not a qualified paper for ICLR.
","The sentiment of the review is quite negative, as the reviewer points out several major flaws in the paper, including unclear representation of the method and contributions, confusing notation, and lack of solid experimental results. The reviewer explicitly states that the paper is not qualified for ICLR, which further indicates a negative sentiment. The politeness of the language is relatively neutral; while the reviewer is critical, they do not use rude or overly harsh language. The comments are direct and focused on the issues without personal attacks or derogatory remarks.",-80,0
"The authors propose a method to generate predictions under fairness constraints. The main idea is to take linear fairness constraints, and replace them with weak squared penalties plugged into the objectives, which supposed to help in cases where the loss function is not convex. The penalty coefficients are chosen by cross-validation, and the effectiveness of this approach is demonstrated empirically.

In Sec. 3.1, the authors point out several shortcomings of using linear penalties (using Lagrange multipliers) for non-convex losses. These seem valid. Sec. 3.2, however, is not clear on why exactly replacing the linear penalties with quadratic penalties solves these issues. I'm hoping the authors can clarify the following points:

1) The authors note that, for quadratic penalties, \lambda->0 means no constraints, and \lambda->\infty means hard constraints. Isn't this also true for linear constraints?

2a) Why do linear penalties have unique \lambda_k for each constraint k, but the quadratic objective has only a single \lambda for all constraints?

2b) Why can CV over \lambda be used for quadratic constraints - what is the justification? And, more importantly, why *can't* it be used with linear constraints? If it can, then this should be one of the baselines compared to in the experiments.

3) What is the criterion optimized for by CV - accuracy or the constraints? Different parts of the paper give different answers to this question. For example, ""... may be easily determined via standard hyperparameter optimization methods"" vs. ""tuning \lambda to directly satisfy the desired fairness metrics"". Or even more unclear - ""choose \lambda ... so that the final solution gives the desired fairness-accuracy trade-off"". How is the desired trade-off defined?

4) If there is a trade-off between fairness and accuracy, and no clear-cut criterion for evaluation is pre-defined, then the evaluation procedure should compare methods across this trade-off (similarly to precision-recall analysis).

5) The authors differentiate between cases where the loss is either convex or non-convex. This is confusing - most losses are convex, and non-linearity appears when they are composed with non-linear predictors. Is this the case here? If so, the fairness constraints are no longer linear, and they're quadratic counterpart is no longer quadratic. It would be helpful if the authors specify where the non-linearity comes from, and what they assume about the loss and predictors.

6) Why is it important to show that the quadratic constraints can be written as an expectation? Isn't the square of an expectation always an expectation of pairs? How does the double summation/integral effect runtime?

7) It would be helpful if the authors differentiate between loss/constraints over the entire distribution vs. over a given sample set.


","The review starts with a neutral to slightly positive sentiment, acknowledging the proposal and its empirical demonstration. However, it quickly transitions into a series of critical questions and requests for clarification, indicating a more neutral overall sentiment. The language used is polite and constructive, aiming to help the authors improve their work rather than dismissing it outright.",-10,80
"This paper introduces two methods of adjusting the overconfidence error for predictions on novel data. The ensemble distillation approach is to penalize the distillation loss on a potentially unlabeled general dataset.  The second approach (NCR) detects the novelty first and reweigh the prediction based on the familiarity to training data. 

*stationarity*
From the statistical perspective, the overconfidence of extrapolation can kick in from two sources: a)the epistemic uncertainty.  The point estimation of softmax ignores the uncertainty of prediction at all.  A full Bayesian approach will remedy this though computationally impractical.  b) the generative distribution p(y|x) might not be identical on training data and test data.  To see the difference, if the training sample size goes to infinity, the uncertainty in a) will go to zero, but b) may still exist.  Section 3 assumes the invariant p(y|x) in novel data.  But theoretically, both methods do not require such invariance?

Slightly related here, there can be novel data for classification, and in principle, there can also be novel data for novelty detection?  That will make NCR fail.

*why the distillation helps uncertainty adjustment*
I am not convinced how the g-distillation works for this task.  In the extreme case if the ensemble model itself is totally wrong for novel data and the unlabeled general data used in training, how can I learn any extra uncertainty information from that noise? To be fair, when the temperature goes high enough, the ensemble will make uniform prediction and then the distillation loss is merely a loss function that enforces uniformity.  If I replace the ensemble softmax by a uniform prior for unlabeled general data, do I achieve the same effect?  That is essentially the same regularization as method 2, except g-distillation is on logit scale.  

*robustness-accuracy tradeoff*
The experiments do not reveal too much robustness-efficiency conflict, as the new methods still perform good enough on familiar dataset. Indeed they can be even better than the baseline in E99 loss. Does it suggest the over-confidence is even a concern for familiar data/ iid data?

In general, the paper is well-written and well-motivated. It would be more interesting to make some theoretical explanation why/when this simple approach works.   I would recommend a weak accept at this point.
","The sentiment of the review is generally positive, as indicated by the concluding statement recommending a 'weak accept' and the acknowledgment that the paper is 'well-written and well-motivated.' However, the reviewer does raise several critical points and questions about the methods and their theoretical underpinnings, which tempers the overall positivity. Therefore, the sentiment score is 40. The politeness of the language is quite high; the reviewer uses polite and constructive language throughout, even when pointing out potential issues or asking for clarifications. The use of phrases like 'I am not convinced,' 'To be fair,' and 'It would be more interesting' indicates a respectful and considerate tone. Therefore, the politeness score is 80.",40,80
"The authors proposed a generalized adaptive moment estimation method(Game). Compared to the existing methods AMSGrad and PAdam, the new method Game tracks only two parameters in iteration and hence saves memory. Besides, they introduced a additional tuning parameter $q$ to track the q-th moment of the gradient and allow more flexibility. The authors also provided the theoretical convergence analysis of Game for convex optimization and smooth nonconvex optimization. Their experiment shows Game may produce better performance than AMSGrad and PAdam with a little bit sacrifice of convergence speed. Game is a promising alternative method for training large-scale neural network.","The review is generally positive, highlighting the advantages of the proposed method (Game) over existing methods like AMSGrad and PAdam. It mentions the memory-saving aspect, the flexibility introduced by the additional tuning parameter, and the theoretical convergence analysis provided by the authors. The review also notes that the experimental results show better performance with a slight sacrifice in convergence speed. The language used is polite and constructive, focusing on the strengths of the work without any negative or rude remarks.",80,90
"The papers studies estimators of gradients taken from expectations with respect to the distribution parameters. The paper has studied two main types of estimators, Finite Difference and Continuous Relaxation. The paper made several improvements to existing estimators. 

My rating of the paper in different aspects (quality 6, clarity 8, originality 6, significance 4). 

Pros: 
1. The paper has made a nice introduction of FD and CR estimators. The improvements over previous estimators are concrete -- it is generally clear to see the benefit of these improvements. 

2. The first method reduces the running time of the RAM estimator. The second method (IGM) reduces the bias of GM estimator. The first improvement avoids many function evaluations when the probability is extreme. The second improvement helps to correct bias introduced by continuous approximation of \zeta_i itself. 

Cons: 
1. the paper content is a little disjointed: the improvement over RAM has not much relation with later improvements. It seems the paper is stacking different things into the paper. 

2. All these improvements are not very significant considering a few previous papers on this topic. Some arguments are not rigorous. (see details below)

3. A few important papers are not well discussed and omitted from the experiment section. 

Detailed comments

1. The REBAR estimator [Tucker et al., 2017] and the LAX estimator [Grathwohl et al., 2018] use continuous approximation and correct it to be unbiased. These papers in this thread are not well discussed in the paper. They are not compared in the experiment either.  

2. In the equation 7 and above: what does 4 mean? When beta \neq 4, do you still get unbiased estimation? My understanding is that the estimator is unbiased only when beta=4. (correct me if I'm wrong)

3. The paper argues that the variance of the estimator is mostly decided by the variance of q(zeta)^-1 when the function is smooth. I feel this argument is not very clear. First, what do you mean by saying the function is smooth? The derivative is near a constant in [0, 1]? 

4. In the PWL development, the paper argues that we can choose alpha_i \approx 1/(q_i(1-q_i)) to minimize the variance. However, my understanding is, the smaller alpha_i, the smaller variance.

","The sentiment of the review is moderately positive, as the reviewer acknowledges the improvements made by the paper and provides a balanced view of its strengths and weaknesses. The sentiment score is 20 because the reviewer appreciates the contributions but also points out significant areas for improvement. The politeness score is 80 because the reviewer uses polite language, such as 'nice introduction' and 'correct me if I'm wrong,' and provides constructive feedback without being harsh or dismissive.",20,80
"Given an MDP <S, A, T, R>, the paper suggests to learn both the optimal Q function of that MDP (denoted Q^+), but also that of the MDP <S, A, T, -R> (denoted Q^-). The basic idea is that min_a Q^-(s, a) could be a good action for the initial MDP. Based on this idea, the authors propose to combine Q^+ and Q^- with a linear combination in order to obtain what they call a hybrid policy. 

The proposed idea is indeed interesting and I find the experimental results surprising. It is not clear to me why the policy obtained from Q^- does better than Q^+. Theoretically, this should not happen: if we have the exact optimal Q function, Q^-, for <S, A, T, -R>, the policy defined by argmin_a Q^-(s, a) in every state s may be suboptimal in <S, A, T, R>. Is there a good conjecture/explanation for why the policy induced by Q^- works so well in 2(a) and (b)?

The authors chose to report the results using off-line training, which seem to favor their proposition. What are the results for on-line training?

In the experimental part, I think the authors should also report the results of the method that consists in learning two Q^+ and combining them with an average. This baseline would help understand if the good performance of hybrid policies really comes from learning Q^-.

Obtaining hybrid policies faces one important issue, which is the need to perform two actions in the environment in a given state, one for Q^+ and the other for Q^-. Therefore, the proposition seems to be doable only when one has access to a simulator.

The writing is generally clear, but the paper should be checked for typos.","The sentiment of the review is generally positive, as the reviewer finds the proposed idea interesting and the experimental results surprising. However, there are some critical points and questions raised, which slightly temper the overall positivity. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, making suggestions and asking questions in a respectful manner. Thus, the politeness score is 90.",60,90
" This paper proposes a novel idea of outputting a quantum state that represents a complete cost landscape of all parameters for a given binary neural network, by constructing a quantum binary neural network (QBNN). And then the landscape state is utilized to training the neural network by using the standard quantum amplitude amplification method.   

Although this idea is interesting, I trend to reject this submission as I think its presentation is unclear and the technical detail is a little difficult to follow. So, the correctness and soundness of this work is difficult to verify. I urge the authors to revise their draft to provide more and clearer technical details.

Detailed comments and questions:

Could the authors further point out that what the scope of the binary features are, {0, 1} or {-1, +1}? To my understanding, it should be {-1, +1}, or the corresponding variables are always +1. In addition, the construction of the “multiplying values by binary weights” module implies that the value should take -1 or +1, rather than 0 or 1. However, at the bottom of page 5, the authors claim that the binary values take +1 and 0. Could the authors clearly explain the term “parameter” and “value”?
 
Could the author further explain how to construct the majority activation function?
In the part of “calculating accuracy”, the authors mention that “running the QBNN with the weights in superposition for each point in the training set separately” and “there are N qubits containing the prediction of the QBNN”. To my understanding, there are 3 qubits representing the 8 weights, several qubits representing the input values, and N qubits representing the predictions. But how to construct the final landscape state to be optimized with these qubits?
 
Could the author explain intuitively the main idea of the amplitude amplification method? Specifically, what is the relation between the qubits representing parameters and the qubits presenting prediction results?
 
During the amplitude amplification process, the probabilities change periodically. How to select the best number of steps k in advanced if we do not known the best parameter? Or how to judge if the training is success?
 
Overall speaking, I think this paper is interesting. However, the presentation is unclear and I suggest the authors to revise their draft by providing more technical details.","The sentiment of the review is mixed but leans towards negative, as the reviewer expresses interest in the idea but ultimately recommends rejection due to unclear presentation and difficult-to-follow technical details. Therefore, the sentiment score is -30. The politeness of the language is quite high; the reviewer uses polite phrases such as 'I urge the authors' and 'I suggest the authors,' and provides constructive feedback without being rude or dismissive. Therefore, the politeness score is 80.",-30,80
"This paper proposes a novel parameter-efficient generative modeling approach that is based on the Monge-Ampere equation. In the proposal, a feed-forward neural network is trained as an ODE integrator which solves (2) and (3) for a fixed time interval $[0,T]$, so that the distribution $p(x,t)$ at time 0 is a simple base distribution such as a Gaussian, and that at time $T$ mimics the target distribution.

[pros]
- The proposal provides a parameter-efficient approach to generative modeling, via parameter sharing in the depth direction.
- I think that the idea itself is quite interesting and that it is worth pursuing this direction further.

[cons]
- The Applications section is not convincing enough to demonstrate usefulness of the proposal as an approach to generative modeling.
- How the gradient-based learning in the proposal behaves is not discussed in this paper.

[quality]
How the gradient-based learning in the proposal behaves is not discussed. I understand that the non-convex nature of the loss function poses problems already in the conventional back-propagation learning of a multilayer neural network. On the other hand, in the proposal, the loss function (e.g., (4)) is further indirectly parameterized via $\varphi$. It would be nice if the parameterization of the loss in terms of $\varphi$ is regular in some sence.

[clarity]
Description of this paper is basically clear. In the author-date citation style employed in this paper, both the author names and publication year are enclosed in parentheses, with exception being the author names incorporated in the text. This paper does not follow the above standard convention for citation and thus poses strong resistance to the reader. For example, in the first line of the Introduction section, ""Goodfellow et al. (2016)"" should read ""(Goodfellow et al., 2016)"".

[originality]
The idea of considering the Monge-Ampere equation in its linearized form to formulate generative modeling seems original.

[significance]
In the experiment described in Section 4.1, it is not clear at all from the description here whether the learned system is capable of successfully generating MNIST-like fake images, which would question the significance of the proposal as a framework for generative modeling. It is well known that the KL divergence $D(P\|Q)$ tends to put more penalty when $P$ is large and $Q$ is small than the opposite. One can then expect in this experiment that it tolerates the model, appearing as $Q$ in $D(P\|Q)$, to put weights on regions where the data are scarce, which might result in generation of low-quality fake images. It would be nice if the authors provide figures showing samples generated via mapping of Gaussian samples with the learned system.
Also, in the experiment described in Section 4.2, I do not see its significance. It is nice to observe in Figure 4 that the loss function approaches the true free energy as well as that the snapshots generated by the model seem more or less realistic. My main concern however is regarding what the potential utilities of the proposal are in elucidating statistical-physical properties of a system. For example, it would be nice if the proposal could estimate the phase-transition point more easily and/or more accurately compared with alternative conventional approaches, but there is no such comparison presented in this paper, making the significance of this paper obscure.

Minor points:

The reference entitled ""A proposal on machine learning via dynamical systems"" would be better cited not as ""E (2017)"" but rather as ""Weinan (2017)"".

Page 6, line 10: the likelihoods of these sample(s)

----Updated after author feedback----
Upon reading the author feedback, I have downgraded my rating from 7 to 6, because the author feedback is not satisfactory to me in some respects. In my initial review, my comment on the experiment on MNIST is not on correlation between the maximum likelihood estimation and visual quality of generated images, on which the author feedback was based, but regarding the well-known property of the KL divergence due to its asymmetry between the two arguments. Also, regarding the experiment on the Ising model, the proposal in this paper provides an approximate sampler, whereas for example the MCMC provides an exact sampler with exponential slowing down in mixing under multimodal distributions. In statistical physics, one is interested in studying physical properties of the system, such as phase transition, with samples obtained from a sampler. In this regard, important questions are how good the samples are and how efficiently they are generated. As for the quality, it would have been nice if results of evaluated free energy as a function of inverse temperature (that is K_ij in the case here) were provided. The author feedback was, on the other hand, mainly explanation of general variational approach, of which I am aware.
I still think that this paper contains interesting contributions, and accordingly have put my rating above the threshold.","The sentiment of the review is moderately positive. The reviewer acknowledges the novelty and potential of the proposed approach, stating that the idea is interesting and worth pursuing further. However, the review also highlights several significant concerns and areas for improvement, particularly regarding the demonstration of the proposal's usefulness and the behavior of gradient-based learning. This mixed feedback results in a sentiment score of 30. The politeness of the language used in the review is quite high. The reviewer provides constructive criticism and suggestions for improvement in a respectful and professional manner, without any rude or dismissive language. This results in a politeness score of 80.",30,80
"This paper proposes a new approach to sequential learning by introducing an adversarial memory unit for each new task and uses EWC as a regularizer for training other parts of the network on the new task.
The memory units are trained with Fast Gradient Sign Method to increase the loss, and they are connected to the next layer with weights trained to decrease the loss. 
It shows superior performance than EWC and the plain gradient descent baseline on disjoint MNIST/CIFAR10 and EMNIST. The authors also share their experience with EWC, which provides useful feedbacks to the community.

The proposed adversarial memory unit is novel to the best of my knowledge. However, its motivation is not quite intuitive to me, and the authors fail to provide persuading explanations. My major concern is whether it is better to take the adversarial direction rather than the direction that decrease the loss for the memory units.

To support their ideas, the authors mentioned the paper ""Adversarial Reprogramming of Neural Networks"" and said this paper's ""adversarial program"" is formed by choosing the ""intersection of adversarial subspaces"" as in their paper. However, they (Elsayed et al. 2018) are actually finding such adversarial programs in the direction of decreasing the loss, which is contrary to finding the ""intersection of adversarial subspaces"". 
The authors also want to support the pros of adversarial memory units by comparing against ""Gradient"" memory units that are trained to decrease the loss with the experiment shown in Figure 2. However, Figure 2a seems problematic to me, so I am not sure whether the authors are doing their experiments correctly. I think the experimental conditions for FGSD and Gradient are different, which makes the comparison meaningless. We can see that the network's accuracy with Adversarial memory unit on task 1 is a constant when the network is trained on task 2 and 3, because the network's weights (except memory units and their weights for task 2 and 3) and task 1's memory units are fixed, as described in the experimental setting for ""AD"". The accuracy on task 1 with Gradient memory units is changing when the network is trained on task 2 and 3, which means either the network's weights are changing or the memory unit is changing. 

As a result, I don't think this paper will be accepted until the authors provide further explanations and results to support the adversarial memory unit, or clarify my misunderstandings in the comments above.","The sentiment of the review is mixed but leans towards negative. The reviewer acknowledges the novelty of the proposed adversarial memory unit and its superior performance in certain tasks, which is positive. However, the reviewer expresses significant concerns about the motivation, explanation, and experimental validity of the approach, which are negative aspects. Therefore, the sentiment score is -30. The politeness of the language is generally respectful and constructive, as the reviewer provides specific feedback and suggestions for improvement without using harsh or rude language. Thus, the politeness score is 70.",-30,70
"The paper presents two variational inference frameworks for generative models of knowledge graphs. Such models are based respectively on latent fact model and latent information model.
The authors argue that with the presented framework the underlying probabilistic semantics can be discovered. Experiments show performances comparable with state-of-art approaches.

Unfortunately, the paper seems to me not clear and rather incomplete in its actual form.
Overall, the proposal is novel. I cannot decide about significance because results do not outperform those of other approaches. To this extent, the authors should better discuss the results, explaining in more detail why this approach should be used instead of others (scales better, is faster, etc.).

In the abstract, it is asserted that one can discover underlying probabilistic semantics, but in the corpus of the paper this aspect is not described or mentioned in detail.
Similar problem for the reference to von-Mises distribution. This distribution is just named, it is said that the framework can handle such a distribution, but a reference to a paper and/or a short paragraph to explain the sentence are missing. This statement now results to be just information disconnected by the rest of the paper.

In a similar way, many other points suffer from a poor organization in my opinion.
When describing LIM an error is introduced here that is then copied and pasted throughout the paper: in the productory on p, p is in R not in E. This is a simple typo, but the fact that it is repeated so many times, also in the proof, gives me the feeling that the paper was written at the last moment.
Figures 1 and 2 are never referred.

Formula 6 must be better explained. If I have not lost something, n is the number of labeled triples, s_c is undefined, b_c is the probability of s_c to be equal to 1, the index i is never used. The paper lacks information here.

As regards the experimental part, some results are shown in subsection 4.3 called link prediction, others in section 5 called link prediction analysis. This organization does not seem to me to be really optimal. I would suggest creating an experimental section.
Moreover, the tests should be better explained, the tables are shown without specifying how they are built and how the values are collected. Information is provided in the appendix but could be included in the paper as the maximum limit is of 10 pages (8 suggested but I think an extra half page can be used).
The knowledge bases used should be at least cited, I know that freebase and wordnet are well-known but somewhere, in the description of the test, the name should be included. Also to specify the characteristics of the versions (WN18 vs WN18RR). Moreover, what does the value -257 in column 1, row 4 means?
Then, it is said that Table 1 shows improvements for ComplEx, but such improvements are rather low, is there a way to prove their significance? Otherwise, I would say that the performance is the same for WN18.
Tables 1 and 2 contain cells with '-' value, what does it mean?

Discussion about table 3 is incomplete in my opinion. First of all, the ""proportion"" column should be described. Also, on one hand, it is true that the _member_meronym is the least accurate and prominent but the most problem may come from _hypernym, which is the most prominent and the accuracy is also low. This fact is highlighted for table 4 but not for table 3.

Minor issues
- sec 3: references to Miao et al. must be enclosed by brackets
- sec 4.3: ""We believe this *is* due to ...""
- sec 5.2: what is Model A? Also, the sentence seems incomplete.

Pros:
- Novel approach

Cons
- Test results are not convincing
- The paper is not mathematically sound
- The paper needs to be re-organized","The sentiment score is determined by the overall tone and content of the review. The reviewer acknowledges the novelty of the approach but expresses significant concerns about the clarity, completeness, and organization of the paper. The sentiment is mixed but leans towards the negative due to the numerous issues highlighted. Therefore, the sentiment score is -40. The politeness score is assessed based on the language used. The reviewer provides constructive criticism and suggestions for improvement without using harsh or rude language. The tone remains professional and courteous throughout, leading to a politeness score of 80.",-40,80
"This paper centers around efficient estimation of the kernel function for the Hawkes process and relaxation of the “linearity” assumption in the original Hawkes process. They rely on a classical sparse generalized linear model using the wavelet basis set and Hawkes loss function to estimate a shallow kernel function. This approach is opposite to the deep function estimation approach which does not rely on a predefined basis set [e.g. see [Du et al, 2016]]. However, it can have an advantage that the learned functions are interpretable, thought the authors never demonstrate it in the paper.

Given this view of WRNs (an unfortunate coincidence with WideResNets), we understand how LSTM2 outperforms LSTM1 in the results. However, the results tables do have peculiar numbers too. For example, why the Goodman-Kruskal gammas for H. Poisson are exactly -1? Why is it always pointing in the wrong direction? There are other observations in the results table that the authors have listed without much explanation. For example, in Section 5, what is the reason for “The WRN-PPL method excelled particularly in tasks with many target occurrences”?

Another example is the arguments in the discussion section about the use-case of rate functions. For example, the authors state: “ For example, the rate prediction for the individual denoted in green in Figure 5 (right) suggests that individual may have skipped, missed, or rescheduled 5 to 6 appointments over the last decade.” How did the authors conclude this claim? What is the clinical significance of missing or rescheduling 5-6 appointments in the context of A1c prediction?

Writing can be seriously improved (basically the paper is not ready in the current state). For example, only in Section 6, the authors have introduced the full name of WRN-PPL after using it many times before. 

The motivation for this paper is misleading. There have been several works on “Deep Cox” and “Deep Hawkes” models. I don’t see the novelty in the authors’ contribution in defining the clinical risk. Especially Fig. 3 (left) is already known and does not add much value.

Overall, on the positive side, this paper shows that in some datasets going back to the classical shallow models we might achieve better performance than the alternative deep models. Unfortunately, the authors do not clearly state how many training data points they have. They have a vague statement: “798,818 timestamped events in a study population of 4,732 individuals”, but it does not say exactly how many training examples they have.","The sentiment of the review is mixed but leans towards negative. The reviewer acknowledges some positive aspects, such as the potential for classical shallow models to outperform deep models in certain datasets. However, the majority of the review focuses on significant criticisms, including the lack of novelty, unclear explanations, and poor writing quality. Therefore, the sentiment score is -60. The politeness score is 20, as the reviewer uses polite language but is quite direct and critical in their feedback. The review is not rude, but it is firm and points out several areas for improvement without much cushioning.",-60,20
"This paper proposes an alternative search procedure for A* sampling that, in contrast to the original optimistic search, doesn't rely on (possibly difficult-to-find) bounds for the log-probability function.

The first major issue with this paper is clarity. The preliminary section describing the Gumbel process and A* sampling is very difficult to understand (despite my being quite familiar with A* sampling). The authors use undefined notation frequently throughout the introduction and refer to it in abstract terms. There are also numerous errors -- for example, when describing the ""bottom up"" approach to generating a Gumbel process, the authors suggest perturbing all points in the input space by independent Gumbel noise (which would result in Gp(S) = infty almost surely when omega is uncountable). 

The description of the main contributions in section 4 is equally unclear. This section starts by suggesting that sampling the next node to investigate according to the probability that it contains the maximum is reasonable, and then presents a lemma about regret in a bandit setting where the sampler never splits the subset. This lemma does not apply to the actual sampler proposed in the paper, so it is not clear why it is included. Section 4.2 is also very unclear -- I am not certain how both w and script Y are defined, nor why we need an ""unbiased estimator of Y"" (a random variable?) when we can simply sample from Y directly. As the definition of w is unclear, the purpose of 4.3 is unclear as well.

The other major issue is more fundamental -- I am not convinced the sampler is correct. The algorithm simply terminates after some finite horizon (rather than having a conclusive proof of termination via branch and bound as in the original A*). There is no proof or argument included in the paper regarding this. Any proposed sampling algorithm must be correct to be acceptable in ICLR.
","The sentiment of the review is quite negative, as it highlights major issues with the paper's clarity and the correctness of the proposed algorithm. The reviewer points out numerous errors and unclear sections, and expresses doubt about the fundamental correctness of the sampler. Therefore, the sentiment score is -80. The politeness score is relatively high, as the reviewer uses formal and respectful language despite the negative feedback. The reviewer does not use any rude or dismissive language, and the critique is presented in a constructive manner. Therefore, the politeness score is 60.",-80,60
"The underlying motivation for the paper is really interesting and cuts straight to the heart of Deep Learning and strives to unravel the key understanding that we are still to a large extent missing.

When it comes to clarity and organization I find the paper a bit ""messy"" in that it is a collection of quite a few findings on the very specific topic of binary classification with quite strong assumptions. Especially given the very specific nature of the topic I miss a strong and clear path through the paper. Unfortunately the paper leaves me with the distinct feeling that there are still a lot of work needed to be able to tell the story about the problem under study. Having said that the paper does contain several individual findings. Having said that I find the ideas leading up to what the authors refers to as ""gradient starvation"" to be really interesting and that would be a great clear idea to focus on.

A few concrete questions/comments:
Can you explain somewhere exactly what you mean when you say ""learning dynamics of deep learning""? Given the specific nature of the results presented in the paper it would be nice to be precise also when it comes to the overall topic under study.

Given the very specific nature of the topic treated in the paper I find the title of the paper largely misleading. The title claims way more than what is actually delivered in the paper, despite the fact that the authors have put in an ""On"" in the beginning of the title.

In Corollary 3.3. you characterize the convergence speed in a nice way, but I am missing the link to the behaviors observed empirically in e.g. Fig. 2. What am I missing?

The final sentence in Section 2 is highly speculative and I find this hard to believe without solid backing. The sentence reads ""... and helps develop intuitions about behaviors observed in more general settings."" Given the restrictive nature of your set-up I find it very hard to believe that this extends to more general settings.

Tiny detail: The axes of several of the plots given in the paper mis the lables which makes it hard to read. Straightforward to fix, but worth mentioning nevertheless.","The sentiment of the review is mixed. The reviewer acknowledges the interesting motivation and some valuable findings in the paper, but also points out significant issues with clarity, organization, and the misleading nature of the title. The sentiment score is therefore slightly negative. The politeness of the language is generally maintained, with the reviewer providing constructive criticism and specific suggestions for improvement without being rude or dismissive.",-20,60
"The paper applies multi-armed bandits for choosing the size of the minibatch to be used in each training epoch of a standard CNN. The loss of the bandit is binary: zero if the validation loss decreases and 1 otherwise. In the experiments, the Exp3 bandit algorithm is run with Adam and Adagrad on MNIST, CIFAR-10, and CIFAR-100. The results show that the bandit approach allows to obtain a test error better (although not significantly better) than the test error corresponding to the best minibatch size among those considered by the bandit.

The idea of viewing the choice of hyperparameters in a learning algorithm as a bandit problem is known and has been explored in different contexts, although the specific application to minibatch size is new as far as I know.

The paper could have gained strength if bandits had been considered in wider context of parameter/model selection in deep learning.

It is not clear how results scale with the number and choice of the grid values.

I would have liked to see a more thorough investigation of the impact of the bandit loss on the experiments. It is true that as far as the theory is concerned, any bounded loss is OK. But I practice I would expect that a graduated loss (e.g., signed percentage of change in validation loss), would be more informative.","The sentiment of the review appears to be slightly positive. The reviewer acknowledges that the idea of using multi-armed bandits for choosing minibatch sizes is novel and that the results show some improvement, even if not significant. However, the reviewer also points out several areas where the paper could be improved, such as considering bandits in a wider context and providing a more thorough investigation of the bandit loss. Therefore, the sentiment score is 20. The politeness of the language is quite high. The reviewer uses polite language and constructive criticism, suggesting improvements without being harsh or dismissive. Therefore, the politeness score is 80.",20,80
"This work proposes an approach for explicitly placing information in a subset of the latent variables. The approach is to construct an auxiliary generative model that takes as input the set of latent variables subtracted from the target subset, which is used to model modified data samples that do not contain the desired information. 

Experiments focus on learning global information. The auxiliary model is then given data that have their global information destroyed via random shuffling of image patches.

# Approach seems limited.
 - This approach seems very limited, as there must exist a known transformation that removes the desired information. Apart from global vs. local, can the authors provide more examples of what sort of information this approach can disentangle? (Even for global vs. local, is there a transformation that can remove local information as opposed to global information?)
 - Can this approach learn multiple factors as opposed to just two? 
 - What if the desired factors are not clearly disjoint and collectively exhaustive? (e.g. mustache vs. gender on human faces.)

# More ablations or experiments with comparable settings would be desirable.
 - What is the choice of beta in the beta-VAE training objective? Apart from 1.2, this isn't mentioned. My concern here is that beta might be affecting the result more than the proposed training algorithm. Can the proposed approach perform just as well without a modified objective? Ablation studies that show the proposed algorithm can improve upon the baseline in all settings would make this a stronger paper. (e.g. this approach with normal VAE objective, and normal VAE objective without auxiliary task for the clustering experiment.)
 - Why were 30 discrete categories used in the clustering experiment? Is this still comparable to the approaches that use 10, which would correspond to the number of classes?

# Related work.
There are some well-cited works that the authors may have missed. These are ultimately different approaches, but perhaps the authors can obtain some inspiration from these:
- Tranforming autoencoders [1] also apply a transformation to the image, but the goal is to learn the factor corresponding to the transformation, rather than the complement as in this work.
- An opposing approach for explicit information placement with a modified training procedure (where the target information is directly placed in the target subset and can handle multiple factors) is DC-IGN [2]. I believe the DC-IGN approach is more general and can handle a superset of the tasks of this approach, without requiring an auxiliary decoder. Comparing to this approach, I wonder if it would be better to provide samples that exhibit a particular factor, or samples that conceal the factor?

[1] Hinton, Geoffrey E., Alex Krizhevsky, and Sida D. Wang. ""Transforming auto-encoders."" International Conference on Artificial Neural Networks. Springer, Berlin, Heidelberg, 2011.
[2] Kulkarni, Tejas D., et al. ""Deep convolutional inverse graphics network."" Advances in neural information processing systems. 2015.

---- Update since rebuttal ----

I thank the authors for clarifying how this work fits in with related works and clarifying the hyperparameters. I maintain my concerns that the experiments are limited and do not showcase the individual benefit of using explicit information placement. More experiments based on different transformations that the authors have mentioned would make this a stronger contribution. The use of beta>1 is fine if it helps alongside the use of this approach, but it would have been better to see the effects of this approach and beta>1 (and other hyperparameters such as k in Table 1) in isolation. ","The sentiment of the review is mixed but leans towards the negative side. The reviewer acknowledges the approach but points out several limitations and areas for improvement. The sentiment score is -30 because the review contains more critical feedback than positive remarks. The politeness score is 50 because the reviewer uses polite language, such as 'can the authors provide' and 'I thank the authors,' but also includes direct critiques like 'this approach seems very limited.' The language is constructive and professional, but not overly courteous.",-30,50
"The paper claims to propose a computationally efficient algorithm for training deep CNNs by making assumptions about the distribution of data. The authors argue that (i) they don't make very simplistic assumptions about the data generating distribution as some other papers do, and (ii) their algorithm resembles the actual methods that are used for training deep models and shows some surprising properties of SGD.

Throughout the paper, the authors make a number of assumptions which seem arbitrary at times; not much justifications are provided. The authors claim that their assumptions are not as simplistic as assuming e.g., the inputs are sampled from Gaussian distribution. Unfortunately this is highly unclear: while the ""assumptions"" themselves are complex, the combination of those assumptions may make the problem solution trivial. While proving a lower bound to address this issue may be hard, at least the authors should try to illuminate more why the solution is not trivial (e.g., why a linear classifier doesn't work, etc.)

Despite the claims, I find the proposed algorithm very far from the usual SGD-based training methods; this is not a problem per se but I don't think that the result illuminates on the effectiveness of SGD (as the authors suggest). The proposed algorithm is a greedy layer-wise method that in each level does a clustering and also trains a ""linear"" CNN with SGD. So the hardness of end-to-end training of a deep network does not show up. Furthermore, it is not clear for training a linear CNN the SGD is even needed.

I suggest that the authors name each of the assumptions and clearly say which ones are assumed for which result. Here are some of the assumptions that the authors talk about.

1_ The data is generated by the following recursive procedure: First a small ""high-level image"" is generated from a distribution, G_0. The ""pixels"" of this high-level image are supposed to encode semantic classes, e.g., sky or ground. In the next step, each of these high-level pixels are turned into a small (lower-level) image. Therefore, we will have a more refined image after the second step. (each semantic class (e.g., sky) has a corresponding distribution that generates the smaller lower-level image (e.g., uniform over 4 possible types of skies)). This procedure continues recursively until we have the final image.

2_ G_0 is ""linearly separable"".

3_ Semantic classes defined in the model are different enough from each other

4_ {F_c} corresponding to semantic classes are linearly independent 

5_ Patch Orthonormality (apparently not assumed everywhere) 


it appears that if one assumes all of 1-5, then the problem becomes trivial (linearly separable). The authors then say that we don't want to make assumption 5 for this reason; still, the problem solution may be trivial (authors should at least intuitively justify why it isn't )

Here are some more uses of the word ""assumption"".

6_ ""For simplicity of analysis, we assume only the first layer of the network is trained"".

7_ ""We assume the algorithm [KMEANS++] returns a mapping [...] such that [...]"" 

The experiments do not seem conclusive. Only a few experiments have been done. I think the acquired results for CIFAR-10 are below the usual ones using CNNs, and the effects of various hyper-parameters may have interfered.

--
After reading the authors' response, I still think the way that the contributions are depicted (e.g., a justifying the effectiveness of SGD) are inaccurate/unsupported. 

Furthermore, although the authors' suggest that they have tested a linear classifier and observed that the data is not linearly separable, more explanations/intuitions are needed about the assumptions that are made throughout the paper.","The sentiment of the review is generally negative, as the reviewer points out several flaws and issues with the paper, such as arbitrary assumptions, unclear justifications, and inconclusive experiments. The reviewer also expresses skepticism about the claims made by the authors. Therefore, the sentiment score is -60. The politeness of the language is relatively high, as the reviewer uses polite phrases like 'I suggest,' 'unfortunately,' and 'at least,' and avoids harsh or rude language. Therefore, the politeness score is 60.",-60,60
"
UPDATE:

I have read the authors’ response and the other reviews.  While the authors have made some improvements, my core criticism remains – the paper does not produce concrete theoretical or empirical results that definitively address the problems described.  In addition, there are many confusing statements throughout the paper.  For instance, the discussion of positive and negative rewards in the introduction does not conform with the rest of the literature on exploration in RL.

The authors also seemed to have missed the point of the Kearns & Singh reference.  The authors are right that the older paper is a model-based approach, but the idea is that they too were solving infinite-horizon MDPs with finite trajectories and not introducing a bias.  


Summary:

The paper attempts to link the known mismatch between infinite horizon MDP values and finite trajectory sums to the problem of exploration.  Trajectories in environments requiring exploration (mountain car and a number-line walk) are shown and the effects of changing trajectory lengths and initial values are discussed.  Potential solutions to the problem are proposed though the authors did not deem any of the solutions satisfactory.


Review:

The paper brings up a number of important issues in empirical reinforcement learning and exploration, but fails to tackle them in a manner that convincingly isolates the problem nor proposes a solution that seems to adequately address the issue.  Specifically, several issues seem to be studied at once here (including finite-horizon MDPs, function approximation, and exploration), relevant work from the exploration and RL community is not cited, the early experiments do not reach a formal theoretical claim, and the proposed solutions do not appear to adequately address the problem).  These issues are detailed below.

First, the paper is considering many different issues and biases at once, including those introduced by initialization of the value function, exploration policies, function approximation, and finite/infinite length trajectories.  While the authors claim in several places that they show one bias is more important than another, no definitive experiment or theorem is given showing that finite-length trajectories are the cause of bad behavior.  While it is well known that infinite-horizon MDPs do not exactly match value functions for finite horizon MDPs, so many other factors are included in the current work (for instance the use of neural networks) that it remains unclear that the finite/infinite mismatch is an issue.

The paper also fails to cite much of the relevant work on these topics.  For instance, the use of infinite-horizon MDPs to study finite learning trajectories is often done under the guise of epsilon-optimal guarantees, with  epsilon derived from the discount factor (see “Near-Optimal Reinforcement Learning in Polynomial Time”).  In addition, the effects shown in mountain car when changing the values or the initialization function, mirror experiments with Q-learning that have shown that there is no one initialization scheme that guarantees optimal exploration (see Strehl’s thesis “Probably Approximate Correct Exploration in Reinforcement Learning” ).  Overall, the paper seems to confuse the problems of value initialization and trajectory length and does not show that they are particularly related.

In addition, the early sections covering theoretical models such as Wiener Processes and Random Walks lay out many equations but do not come to a specific formally proven point.  No theorem or proof is given that compactly describes which exact problem the authors have uncovered.  Therefore, when the solutions are presented, it remains unclear if any of them actually solve the problem.

Finally, several of the references are only ArXiv pre-prints.  Papers submitted to ICLR or other conferences and journals should only cite papers that have been peer-reviewed unless absolutely necessary (e.g. companion papers).
","The sentiment of the review is largely negative, as the reviewer repeatedly points out significant flaws in the paper, such as the lack of concrete theoretical or empirical results, confusing statements, and failure to adequately address the issues raised. The reviewer also mentions that the paper fails to cite relevant work and does not provide formal theoretical claims or proofs. However, the reviewer acknowledges that the paper brings up important issues, which prevents the sentiment from being extremely negative. The politeness of the language is relatively high, as the reviewer uses professional and respectful language throughout the review, even when pointing out major criticisms. The reviewer provides constructive feedback and specific examples to support their points, which is indicative of a polite and professional tone.",-60,80
"Authors present a new LSTM architecture, i.e. reduced gate convolutional LSTM. Authors use only one trainable gate, i.e. the forget gate which leads to less trainable parameters. They demonstrate the superiority of ti in two datasets, the moving MNIST and KITTI. Their results show that their architecture performs better than others in more accurately predicting next-frame. The paper is clearly written and the evaluation is based on other proposed similar architectures. For the moving MNIST they compared their model against the vanilla convLSTM with three gates and no peephole connections, based on previous work which has shown that it exceeds the accuracy performance of other LSTM based approaches. 
The results show that the training time is reduced and the standard error alike. The only limitations is that more databases could have been used to demonstrate the enhancement in performance.","The review is generally positive, highlighting the novelty of the LSTM architecture and its superior performance on two datasets. The reviewer appreciates the clear writing and thorough evaluation against similar architectures. The only critique is the limited number of datasets used, which is mentioned in a constructive manner. The language used is polite and professional throughout.",80,90
"The authors propose to study what they call weak contraction map. The idea may have its merits, but in the present form, it is not acceptable.

Notably, the key definition of the paper, that is that of the weak contraction mapping (starting with ""Then a mapping T : X → X is called weak"") is incomplete, because it uses a \mathcal{R}, which is never defined. This makes it hard to evaluate any of the results. 

Further, there is no clear separation between text and theorems. Worse, the theorems are not self-contained. E.g. what could perhaps be Theorem 2 (Starting with ""x* is a fixed-point of T in X0."") does not define x*.

While I cannot be certain because of the reasons stated above, the authors seem to have had in mind something like the Kakutani theorem:
https://en.wikipedia.org/wiki/Kakutani_fixed-point_theorem
which they don't cite. Their assumptions on the map are weaker than that of Kakutani (upper hemicontinuity), which makes me a bit doubtful as to whether the statements could be proven, even if made precise. ","The review starts by acknowledging that the idea may have merits, which is a positive note. However, it quickly transitions to pointing out significant flaws in the paper, such as incomplete definitions and lack of clarity in theorems. The sentiment is thus mixed but leans towards negative due to the critical nature of the comments. The language used is straightforward and critical but not rude, maintaining a professional tone throughout.",-40,20
"Strength: 

The proposed approach is architecture-independent: the attack is constructed only from the dataset.

Weaknesses: 

Paper is not sufficiently well written for a venue like ICLR. 
Attack has very low success rate.
To the exception of Figures 4 and 5, many experiments are conducted on MNIST.

Feedback: 

Experimental results show that the attack is able to degrade a classifier’s performance by inserting perturbations that are computed on the data only. However, there are no baselines included to compare adversarial evasion rates achieved here to prior efforts. This makes it difficult to justify the fairly low success rate. In your rebuttal, could you clarify why baselines were not used to offer comparison points in the experiments?

Furthermore, strong conclusions are made from the results despite the lack of supporting evidence. For instance, on P10, the attack is said to “also explains why adversarial examples can be universal.”. However, not only does the attack achieve less success than universal adversarial examples would (so it cannot explain all of them) but also does it not share any characteristics with the way universal adversarial examples are crafted. Drawing such a strong conclusion thus most likely needs a lot more supporting evidence. 

Several directions would improve the content of the paper: 

* Complete existing experimental results by being more systematic. For instance, in Section 3.1, measurements are only performed on one pair of MNIST images. Without studying a significant portion of the test set of two datasets, it is very difficult to draw any conclusions from the limited evidence.
* Perform a human study to have all perturbed images labeled again. Indeed, because of the ways images are perturbed here, it is unclear how much perturbation can be added without changing the label of the resulting input. 
* Study how existing adversarial example techniques modify internal representations. This would help support conclusions made (e.g., about universal perturbations---see above). 
* Rewrite the related work section to scope it better: for instance, Sabour et al. in Adversarial Manipulation of Deep Representations and Wicker et al. in Feature-Guided Black-Box Safety Testing of Deep Neural Networks explore adversaries operating in the feature space. This will also help build better baselines for the evaluation.

Additional details: 

TLDR: typo in the first word
P1: The following definition of adversarial examples is a bit restrictive, because they are not necessarily limited to vision applications (e.g., they could be found for text or speech as well). “Adversarial examples are modified samples that preserve original image structures” 
P1: The following statement is a bit vague (what is obvious impact referring to?): “Experiments show that simply by imitating distributions from a training set without any knowledge of the classifier can still lead to obvious impacts on classification results from deep networks.”
P1: References do not typeset properly (the parentheses are missing: perhaps, the \citep{} command was not used?)
P2: What is the motivation for including references to prior work in the realm of image segmentation and more generally-speaking multi-camera settings in the related work section? 
P2: Typo in “ linear vibration”
P2: It remains difficult to make a conclusion about humans being robust to the perturbations introduced by adversarial examples. For instance, Elsayed et al. at NIPS 2018 found that time-constrained humans were also misled by adversarial examples crafted to evade ML classifiers: see Adversarial Examples that Fool both Computer Vision and Time-Limited Humans.
P2: Prior work suggests that the following conclusion is not entirely true: “Most of these kinds of examples are generated by carefully designed algorithms and procedures. This complexity to some extent shows that adversarial examples may only occupy a small percentage for total image space we can imagine with pixel representations.” For instance, Tramer et al. in ICLR 2018 found that adversarial subspaces were often large: “Ensemble Adversarial Training: Attacks and Defenses”.
P2: Others have looked at internal representations of adversarial examples so the following statement would be best softened: “To the best of our knowledge, this paper should be the first one that discusses adversarial examples from the internal knowledge representation point of view.”. See for instance, Deep k-Nearest Neighbors: Towards Confident, Interpretable and Robust Deep Learning by Papernot and McDaniel.
P3: Could you add pointers to support the description of human abstraction and sparsity? It reads a bit speculative as is, and adding some pointers would help relate the arguments made to relevant pointers for readers that are less familiar with this topic. 
P3: What is the motivation for including the discussion of computations performed by a neural network layer-by-layer in Section 2?
P4: Given that saliency maps can be manipulated easily and are only applicable locally, it appears that Figure 1 is too limited to serve as sufficient evidence for the following conclusion: “This, in some way, proves the point that the knowledge storage and representation of current neural networks are not exactly sparse prototype based.”
P5: The error rate reported on MNIST is quite low (45%). Even using the Fast Gradient Method, one should be able to have the error rate be as high as 90% on a standard CNN.
P7: Would you be able to provide references to backup the following statement? “This is a network structure that is very common.”
P10: How does the discussion in Section 4.2 relate to the attack described in the submission?
","The sentiment of the review is generally negative, as it highlights several weaknesses and areas for improvement without much positive feedback. The reviewer points out that the paper is not well-written for the venue, the attack has a low success rate, and many experiments are conducted on a limited dataset. The politeness of the language is relatively high, as the reviewer provides constructive feedback and specific recommendations for improvement without being rude or dismissive. The reviewer uses polite language such as 'could you clarify' and 'would you be able to provide,' which indicates a respectful tone.",-60,80
"Summary:
This paper introduces a new dataset consisting of images of various objects placed on store shelves that are labeled with object boundaries and what are described as “ultrafine-grained” class labels. The accompanying task is to predict the labels of each object given the individual images as well as their spatial layout relative to each other. To solve this task, a deep structured model is used consisting of CNN features for each image which are fed into a linear-chain CRF. To better deal with the large number of classes, pairwise potentials are represented as the multiplication of two lower-rank matrices which represent a sort of “class embedding” for each potential label. Training efficiency is improved by considering an objective based on a form of piecewise pseudolikelihood, which allows for training-time inference to be conducted with linear complexity relative to the number of labels. This objective also allows for easy use of batch normalization for the input features to the CRF model. This model/training procedure are compared against a number of models/training procedures to demonstrate its utility.

Comments:
Arguably, the primary contribution of this paper is the introduction of a new “ultrafine-grained” classification dataset which additionally allows for context to be utilized during prediction. This an interesting task, and it’s clear where being able to make such classifications is useful. The task is somewhat limited in scope, however. It’s unclear to me how models developed for this specific task would contain insights or be useful for other tasks - the utility of any models developed for this task seem limited to this exact task. If you have any other examples where inputs might be structured in this way, this would be good to add to the paper.

The model introduced is interesting, but its novelty is limited. It’s mostly a synthesis of ideas from previous work - CNN-based features, using a CRF to model correlations among labels, and approximating the full likelihood with pseudolikelihood. The interesting additions to these ideas are the fact that an “embedding” is learned for each class and that using the pseudolikelihood during training allows for batch norm to be applied in an easy way. Neither of these is a ground-breaking insight, but they are interesting nonetheless. I am somewhat surprised that the use of batch norm during training but not during testing did not hurt performance - a discussion of why this is the case would be good to have. For the most part, I think the experimentation is sufficiently rigorous - comparisons are made against a variety of baselines, and the new model trained with the specified training procedure outperforms the other alternatives. The one additional comparison I would have liked to see would have been against a model that pairwise potentials from the input features using a neural network-based model (for example, the one used in [1] - this seems like a rather glaring omission.

Other Comments:
-Since you ran a cross-validation, you should add confidence intervals to your reported numbers
-One additional dataset detail I was hoping to see that you didn’t provide is the mean/standard deviation of the number of instances per class,
-Your appendix contains a number of interesting ablation studies - you really should report the numbers for these as well
-The title of your paper is somewhat misleading - it’s hard to argue that the form of class embedding you use is a “deep” class embedding since it’s just a matrix of parameters that are learned during training.

Overall, I’m not convinced the model/training procedure by themselves would be fully worthy of publication, but the fact that a new dataset is introduced with a challenging variant of standard classification tasks adds merit to this work.

[1] Ma, Xuezhe, and Eduard Hovy. ""End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF.""


REVISION:
The other reviewers raised some concerns that I had overlooked (especially regarding novelty of using matrix factorization to generate your potentials). Given these, I do not think that this paper is in a state where it is ready to be accepted. Proper citations and analysis of your approach will be needed first.
","The sentiment of the review is mixed, leaning slightly towards positive. The reviewer acknowledges the interesting aspects of the dataset and the model but also points out limitations in scope and novelty. The sentiment score is therefore 20. The politeness of the language is generally high, with the reviewer providing constructive feedback and suggestions for improvement without being harsh or dismissive. The politeness score is 80.",20,80
"# Summary of the paper

This paper proposes an embedding of directed graphs based on the SVD of a normalized adjacency matrix. This embedding is shown to be equivalent to the spectral embedding of a co-citation graph, which is more complex to calculate. Interestingly, the proposed approach does not require the *explicit* representation of this graph. Moreover, the paper also shows that distances of the embedded vectors are proportional to mean commute times of a forward--backward random walk in the original graph. A suite of experiments is run on graphs from KONECT.

# Review

This is a well-written paper, which I enjoyed reading. The extension of embeddings to the case of directed graphs is significant and warrants a detailed exploration.

The principal issues I see with this paper are as follows:

- The originality or scope of the contribution is not clear
- The experimental section is uncompelling
- Several relevant works appear to have been ignored

Overall, I like the way the paper treats the subject. In particular, I appreciate the fact that proofs are explained well; additionally, code is provided, which will increase reproducibility. This is uncommon and praiseworthy!

As for the originality of the paper, I find it hard to judge the scope of the contribution. The paper is extremely well written and employs a very pedagogical treatment of the subject, which I appreciate. Yet, it is hard for me to judge the utility and novelty of the proposed method in light of Section 8, where the paper shows that a spectral embedding of the undirected variant of the graph leads to essentially the *same* eigenvectors (up to renormalization and permutations). To prove that a new method is more effective, this point should be emphasized more:

1. In a sense, I would see the results from Section 8 as the equal to what 'Laplacian Eigenmaps' (LE) yields. This needs to be stressed, and analysed in an experimental section.

2. I understand that the order of the singular vectors is different, so embeddings that use only parts of them will be different. However, a convincing experiment should assess the differences. For example, in which regime for $d$ (number of used vectors) will the new method be surpassed by the old one? Is there such a regime? Ideally, this will be answered in the form of an asymptotic theorem; it could also be a larger experiment, though (to simulate the conditions in practice).

3. I understand that the new approach has a lower run-time, because the SVD is more efficient than eigendecomposition. However, what about a simple baseline algorithm that uses SVD for the *undirected* graph of the input data? This should be simple to accomplish, and would be a way to ascertain the benefit of using edge directions.

  To my understanding, LE should be this embedding, but from the table, I can see that its runtime is a lot worse than the novel method. What causes this? The fact that eigendecomposition is used instead of SVD?  If so, an additional SVD-based approach should be implemented.

This brings me to the experimental section. Here, the paper demonstrates the superiority of the new embedding based on evaluating modularity of a set of different clusterings of larger graphs, obtained using $k$-means. I have several concerns about this:

1. Modularity has problems with larger networks because only a small part of the network will be used in its configuration.

2. Since the embeddings cannot be easily compared due to missing ground truth information, other metrics should be employed. Here are a few, which are often used  by the community. See 'Is there a best quality metric for graph clusters?' by Almeida et al. for more details and a description of their shortcomings:
    - Silhouette coefficient
    - Coverage
    - Conductance

  Different ones should be evaluated here in order to show the behaviour of the new embedding. Do the embeddings differ if the modularities are similar?

3. How do the results change for different values of $d$? I find it hard to disentangle such a discussion from instabilities in $k$-means, but to my understanding of the method, tuning $d$ means that more or less information is used from the singular vectors.

   This could also be quantified in a proof (about asymptotic behaviour) but an experiment would be equally fine.

Concerning the bibliography, or the treatment of prior works, there are some issues:

- There appear to be some missing references of earlier works that used SVD or variants in order to cluster graphs or embed them:

  - Drineas et al.: 'Clustering Large Graphs via the Singular Value Decomposition'
  - Malliaros and Vazirgiannis: 'Clustering and Community Detection in Directed Networks: A Survey'

- Likewise, the use of pseudo-inverse Laplacians has a lot more papers attached to it (these are only a few that are relevant):

  - Ho and Dooren: 'On the pseudo-inverse of the Laplacian of a bipartite graph'
  - Gutman and Xia: 'Generalized inverse of the Laplacian matrix and some applications'

# Suggestions for improvement

- In some sense, this work can be seen as an extension of Laplacian eigenmaps to the directed case. The paper needs to be more clear about these extensions with respect to prior work. In Section 2, it is claimed that 'our main contribution is a proper normalization'. This strikes me as a rather small contribution in light of the experimental section, as outlined above.

- I am also hesitant to speak about a better interpretability of the mean commute time. I agree that it is nice to know that the distance permits such an interpretation in terms of random walks, but what is the impact of knowing the MCT? It is not only used in the embedding insofar as one obtains a vector representation.

- Section 5 is then the standard way of defining random walks based on a Laplacian matrix, and the correspondence to the pseudo-inverse of the Laplacian is shown. This is mathematically interesting, but appears to me to be in line with previous research.

- The section about co-citation graphs should make it more clear that 'successors' are to be taken in terms of the original graph and the directionality of edges. Since this is a standard definition in the domain of network analysis I would suggest citing a textbook here.

- In Section 6, the paper could give more details about random walk concepts such as 'stochastic', 'stationary distribution' etc., as it would make the paper more accessible (I am familiar with these concepts but since the writing of the paper is of high quality in the other sections, I am convinced this would improve its impact, and attract more readers).

Typos & grammar issues:

- 'in terms of random walk' --> 'in terms of random walks'
- 'equivalent to build' --> 'equivalent to building'
- 'with corresponding unitary matrix' --> 'with a corresponding unitary
  matrix'
- 'square Euclidean distance' --> 'squared Euclidean distance'
- 'equivalent to consider' --> 'equivalent to considering'
- 'irreductible' --> 'irreducible'
- 'and provide generally' --> 'and provides generally'
- 'in low dimension' --> 'in a lower dimension'

Furthermore, the bibliography should employ consistent capitalization and journal names for articles.","The review starts with a positive sentiment, appreciating the well-written nature of the paper and its significant contribution to the field. However, it also highlights several critical issues, such as the lack of clarity in the originality and scope of the contribution, an uncompelling experimental section, and missing references to relevant works. The reviewer provides detailed and constructive feedback, suggesting specific improvements and additional experiments to strengthen the paper. The language used is polite and professional, even when pointing out the shortcomings of the paper.",50,80
"This paper proposes an autoencoder architecture and training procedure for producing high-quality reconstructions and realistic interpolations. A ""generator"" autoencoder is trained to fool a ""discriminator"" autoencoder. The generator tries to minimize its own reconstruction error and minimize the reconstruction error of the discriminator when fed with interpolated latent vectors of real datapoints. The discriminator autoencoder has three losses, corresponding to minimizing reconstruction error on real datapoints and maximizing reconstruction error on the generator's output on both real datapoints and interpolated outputs. The authors also propose a loss which encourages the distances between real datapoints and their corresponding latent vectors to be similar, as well as a heuristic procedure for stabilizing GAN training. Qualitative results are shown on CelebA.

While the results look nice, the paper is not fit for publication in its current form. At a high level, the issues include a lack of convincing experimental verification of the method, a generally contradictory and confusing description of the methods, and frequent factual errors or mischaracterizations. Here I will try to describe many of the issues I found while reading the paper:
- Experimental results are only given on CelebA which is a dataset with a very strong and easy-to-model structure. The experimental results are completely qualitative. No effort is made to provide a quantitative proof of claims such as ""the reconstructions are less blurry"" or ""the interpolations are higher quality""; only a few examples are shown. The experiments are not even described in the text, and many of the figures are unreferenced. No ablation studies are done to determine the importance of different loss terms, such as L_dist. No mention is given to how hyperparameters like alpha should be chosen (and in fact, the value given for it ""1^{-4}/2"" is nonsense; 1^{-4} is just 1). No results for a baseline autoencoder (i.e., just optimizing reconstruction loss) are given.
- At a higher level, no effort is given to argue why interpolation is a useful characteristic to try to encourage. There are no downstream applications proposed or tested. Earlier models, such as VAEGAN, also give reasonable reconstructions and good interpolations. Why is GAIA better? On what problem would I use GAIA and achieve better results apart from making nice-looking interpolations of people's faces?
- Definitions are often unclear or contradictory. For example, the generator autoencoder is alternatingly treating as taking input X and taking input Z. I believe what is meant is that the generator consists of two networks which compute Z = encoder(X) and X = decoder(Z). Instead, the paper just switches between G(Z) and G(X) wherever convenient. Similarly, the equation for \delta_Disc is different in Algorithm 1 and in the equation in 2.2. Interpolation, arguably one of the core parts of the model, is described as ""interpolations are Euclidean interpolations between pairs of points in Z, sampled from a Gaussian distribution around the midpoint between Zg1en and Zg2en."" I assume the mean of this Gaussian is the midpoint; what is its covariance? Etc.
- All autoencoders are not generative models, and in particular GAIA is not a generative model. There is no generative process. It does not estimate a data distribution. A VAE is a generative model which an autoencoder-like structure, but this does not make all autoencoders generative models.
- GAIA is described as encouraging ""convex latent distributions"" and a convex set is defined in the text as ""A convex set of points is defined as a set in which the line connecting any pair of points will fall within the rest of the set."" A convex set is not defined in terms of lines; it's defined in terms of convex combinations of points within the set. In the paper, only lines between points are considered. Claiming that the latent space is ""convex"" in the sense of purple blobs in B is not done - you would need to take a convex combination of multiple latent vectors and decode the results.

This is an incomplete list of the issues with this paper. The paper would need significant changes before publication.","The sentiment of the review is generally negative, as the reviewer points out numerous significant issues with the paper, including lack of experimental verification, unclear definitions, and factual errors. The sentiment score is -80 because the reviewer acknowledges that the results look nice but states that the paper is not fit for publication in its current form. The politeness score is 20 because the reviewer uses polite language and phrases such as 'I believe' and 'I assume,' but the overall tone is critical and direct, which slightly reduces the politeness score.",-80,20
"The paper addresses the problem of pixel-wise segmentation of lanes from images taken from a vehicle-mounted camera. The proposed method uses multiple passes through encoders decoders convnets, thereby allowing extract global features to inform better local features, and vice versa. Only qualitative baseline comparisons are presented by manually comparing the output of the network to reported results of other methods in [Pan et al.2017].
It is unclear to me if the proposed multiple encoder-decoder network is a novel architecture, or a known architecture applied to a novel use case. In case of the former, more details should be given on the design of the network, how it is trained, etc. for reproducibility. The biggest problem however is the subjective manual comparison to existing methods, which the authors do in favor of a quantitative comparison using well-understand objective metrics. While they point out problems with evaluating segmentation with conventional accuracy metrics, no attempt is made to make a better objective measure. We are left to judge the results on only a few selected example frames. 
It is also unclear how the method and evaluation strategy compares to methods which predict lanes as splines or other parameterized functions. E.g. see surveys on existing approaches, and discussion of different evaluation strategies, e.g. ""Recent progress in road and lane detection: a survey"" [Hillel et al.2014] and ""Visual lane analysis and higher-order tasks: a concise review"" [Shin, 2014].
Throughout the paper, various fuzzy and unclear statements are made (see detailed comments below). The paper would be in a better shape if more time is spend to improve the writing, provide more details on the method, and extend the experiments.

Pros:
+ multiple encoder-decoder stages could be beneficial for lane segmentation

Cons:
- lacking evaluation and comparison to baseline methods
- missing details on proposed network architecture, making it hard to reproduce
- unclear what colors in figures for qualitative evaluation represent: are individual lanes also distinguished?

Below are more detailed comments and questions:
* Abstract
	* ""the capability has not been fully embodied for"" → Fuzzy statement, I don't understand what this means.
	* ""In especial"" → check grammar
* Sec 1.: Introduction
	* ""the local information of a lane such as sharp, edges, texture and color, can not provides distinctive features for lane detection"" local edges are not distinctive for lanes? Possibly local edges alone are not sufficient, but various lanes detection approaches rely on edge extraction as features. This statement therefore seems too strong.
	* ""End-to-end CNNs always give better results than systems relying on hand-crafted features."". It is not possible to say that one type of classifier categorically better than another. The 'best' classifier depends on the problem at hand, valid assumptions that can be made, and the amount of training data avaiable, among others. For instance, ""How Far are We from Solving Pedestrian Detection?"" [Zhang,CVPR16] demonstrates that CNNs do not always give better results than hand-crafted features for some tasks and datasets. The paper should be more careful with such strong statements.
	* ""Highly hand-craft features based methods can only deal with harsh scenarios."". I don't understand, is this statement intended as an argument against hand-crafted features? Isn't it good to deal especially with harsh scenarios?
	* ""but less explored on Semantic Image Segmentation due to strong prior information is needed."" CNNs are extensively used for semantic image segmentation, e.g. see the well-known Cityscapes benchmark.
	* ""recent methods have replaced the feature-based methods with model-based methods."". Not sure why the paper call CNNs ""model-driven methods"", but refer to the earlier classical methods with highly designed representations (Kalman filter, B-snakes, ...) as ""feature-based methods"". This seems diferent from what I typically see, where CNNs are referred to as 'data-driven methods', and the classical methods as ""model-driven"".
* Sec 1.2: Contributions
	* ""First, reduced localization accuracy due to the weak performance of combining the local information and global information effectively and efficiently"". Instead of presenting a first contribution, the paper presents a problem. Do the authors mean that they ""tackle the problem of reduced localization accuracy ..."" ? That would still not make this contribution very concrete though ...
	* ""We make our attempts to rethink these IoU based methods."" → Please argue in favor of your new method. An in-depth comparison of evaluation methods, and why some metrics fail or could be redesigned would be good. However, the paper currently fails to present a new metric, and convince that it tackles shortcomings of established metrics.

* Sec 2.: Multiple Encoder-Decoder Nets
	* Figure 2: Is this the first paper to propose this multiple encoder-decoder net? Or is the idea taken from other work, and is the novelty to apply it to this problem? If this general architecture was already proposed (for semantic segmentation?), please add citations and discuss it as related work. If this network design is completely novel, I would expect more details on how the network is constructed (e.g. dimensions of each layer, non-linear activation function used, batch normalization, strides, etc.). 
	* ""the following loss function:"". Since it is a binary classification problem, and not a regression problem, why not use a (binary) cross entropy loss instead of a mean squared error?

* Sec 3: Experiments
	* Figure 3: What is the ""Baseline"" method ? Where are the references to the other works, or is the reader required to read [Pan'2017] to understand your figures?
	* Figure 3: How are the colors in these figures determined? Is this also an instance segmentation problem? From your methodology section I though only binary classification was considered. Do you do some post-processing to separate individual lanes? I find this confusing, as I thought that the task was limited to binary segmentation.
	* ""Recent works evaluated ..."" please cite the works you refer to.
	* ""we have compared more than 500 probmaps of each level nets manually and count the accuracy of these probmaps as shown in figure 7."" So if I understand correctly, instead of using an objective evaluation metric, you have reverted to manual labor to visually judge lane detection quality. This is not really a metric, and not really a solution that 'rethinks IoU based methods.' Problems of your approach is that it is unclear on what criteria results are judged, your evaluation is not objectively reproducible by others, and does not scale well for novel future evaluations. Why is this even needed? E.g. why not use some chamfer distance or Gaussian smoothing of the edge map if you want to evaluate near coverage instead of hard boundaries? Or, fit a function through the boundary, and evaluate distance (in meters) to true lane. I find the proper discussion and motivation for manual evaluation over objective metric evaluation lacking.
	* Figure 6: What are the Ground Truth images of each row ? E.g. in the fourth row from the top, should the right-most yellow lane be present or not? As it stands, I can't interpret the columns and see which x times is visually 'better'.
* Sec 3.4:
	* ""To improve ability of the network, we propose a small quantity of channel to reduce overfitting by considering inter-dependencies among channels."" To improve relative to what? Where are the results comparing large amounts vs small amount of channels? Note that Figure 8 is not referred to in the text, and confusingly compares ""18 layers"" to ""1 layers"". Do you mean channels instead of layers? And, how many channels were to obtained the results in the preceding sections?
","The sentiment of the review is moderately negative. The reviewer points out several significant issues with the paper, such as the lack of quantitative evaluation, unclear novelty of the proposed architecture, and various unclear statements throughout the paper. The reviewer does acknowledge a potential benefit of the proposed method but emphasizes the shortcomings more heavily. Therefore, the sentiment score is -60. The politeness of the language is relatively high. The reviewer uses polite language, provides constructive criticism, and offers specific recommendations for improvement. There are no rude or harsh statements, so the politeness score is 80.",-60,80
"The author proposed a novel classification procedure of discerning, maximum response, and multiple check (DMM) to improve accuracy of mediocre network and enhance feedforward network. The author used a toy model with randomly scattered points around four centres to demonstrate the dynamics of decision boundary construction and show how and why the filter training had worked when a multiclass problem is reduced to a pseudo binary classification. The literature review is rather short, some discussion on how the proposed work advances the latest deep neural networks should be added. The experiments are carried out with a simple CNN, it will be better if the author demonstrate its integration to more state-of-art network and make a comparison to their performance.  ","The review starts by acknowledging the novelty of the proposed classification procedure, which indicates a positive sentiment. However, it quickly transitions to pointing out several areas for improvement, such as the short literature review and the need for more advanced experiments. This mix of positive and constructive feedback suggests a sentiment score that is slightly positive but not overwhelmingly so. The language used is constructive and suggests improvements without being harsh or rude, indicating a high level of politeness.",20,80
"
In this work, the authors propose a generalization of the batch normalization (BN) technique often used in training neural networks, and analyzed this convergence. In particular, a one hidden layer and one BN hidden layer fully connected network is considered, and a deterministic gradient descent algorithm with certain kind of BN has been considered in this work. The proposed “generalized” BN strategy is devised on the deterministic setting, but it is a slight generalization of the original BN by introducing a moving average operation. Classical results of Bertsekas is leveraged to show the asymptotic convergence of the algorithm. 

I have the following three main comments about the paper. 
1)	Only deterministic setting is considered, but in this case every time the entire data set will be used to perform the averaging, it appears to be much easier to analyze than the stochastic setting. Further the reviewer has doubt on whether the resulting deterministic algorithm has any practice value. 
2)	Because the authors have used the Bertsekas/Tsitsiklis (B/T) argument, only asymptotic convergence is shown. It is not clear, even in the deterministic case, whether some kind of sublinear convergence rate can be obtained. 
3)	Only one hidden layer of neural network with one BN operation is considered. It is not clear whether the analysis can be extended to multiple layers, despite the statement of the author saying that “the technique presented can be extended to more layers with additional notation”. In particular, when there are multiple layers, the BN layers will be further composite together across multiple nonlinear operations. 
4)	The authors have mentioned that the derivative is always taking w.r.t. theta. However, in (9) is appears that the derivative is taken with respect to lambda, in order to get the Lipschitz condition on \lambda. This is a bit confusing. Also it is not clear how the gradient in Assumption 5 is defined. 
5)	Assumption 5 does not make sense. Problem (1) is a constrained problem with both variables being confined in compact feasible sets. And this condition is important in Assumption2. Now the authors say that at stationary solution the gradient is zero? Please specify functions when this will happen. I will suggest that the authors use a proper definition of stationarity solution for constrained problems. 
6)	Follow up on the previous point. The analysis builds upon B/T argument for unconstrained optimization. However it is not suitable for the constrained problems that the authors started out at the beginning of the paper. The authors may consider develop new analysis tools to understand the problem at hand, rather than assuming away the difficulties. 
","The sentiment of the review is generally neutral to slightly negative. The reviewer acknowledges the work done by the authors but raises several significant concerns and doubts about the practical value, convergence rate, and applicability of the proposed method. The sentiment score is therefore -20. The politeness of the language used is relatively high. The reviewer uses formal and respectful language, even while pointing out several issues and making suggestions for improvement. The politeness score is 80.",-20,80
"In this paper, the authors present two methods, Sequential and Parallel-FEED for learning student networks that share architectures with their teacher.

Firstly, it would be a good idea to cite https://arxiv.org/abs/1312.6184, it precedes knowledge distillation and is basically the same thing minus a temperature parameter and a catchy name.

The paper could do with some further grammar/spell checks.

It isn't clear to me where the novelty lies in this work. Sequential-FEED appears to be identical to BANs (https://arxiv.org/abs/1805.04770) with an additional non-linear transformation on the network outputs as in https://arxiv.org/abs/1802.04977. Parallel-FEED is just an ensemble of teachers; please correct me if I'm wrong.

The experimental results aren't convincing. There aren't any fair comparisons. For instance, in table 6 a WRN-28-10(sFEED) after 5 whole training iterations is compared to a WRN-28-1(BAN) after 1. It would be good to run BAN for as many iterations. A comparison to attention transfer (https://arxiv.org/abs/1612.03928) would be ideal for the ImageNet experiments. Furthermore, if one isn't interested in compression, then Table 4 indicates that an ensemble is largely preferable.

This work would benefit from a CIFAR-10 experiment as it's so widely used (interestingly, BANs perform poorly on CIFAR-10), also a task that isn't image classification would be helpful to get a feel of how the method generalizes.

In summary I believe this paper should be rejected, as the method isn't very novel, and the experimental merits are unclear.

Pros:
- Simple method
- Largely written with clarity

Cons:
- Method is not very novel
- No compared thoroughly enough to other work","The sentiment of the review is largely negative, as the reviewer concludes that the paper should be rejected due to a lack of novelty and unclear experimental merits. The sentiment score is -80 because the reviewer acknowledges some positive aspects but predominantly focuses on the paper's shortcomings. The politeness score is 20, as the reviewer uses polite language but is direct and critical in their feedback. The reviewer provides constructive criticism and specific recommendations without being rude or dismissive.",-80,20
"This paper presents an analysis of the inverse invariance of ReLU networks. It makes the observation that one can describe the pre-image of an image point z = F(x) using linear algebra arguments. They provide necessary conditions for the pre-image to be a singleton or a finite volume polytope. They also provide upper-bounds on the singular values of a train network and measure those in standard CNNs.

The paper is well-written but the structure is a bit disconnected; most notably, I didn't see clearly how Section 2 and 3 fit together. The proofs seem correct and rely mostly on elementary linear algebra argument; this simplicity makes the analysis quite interesting. The argument about a different kind of adversarial examples is also very interesting; instead of looking for small perturbation that affect the mapping in drastic ways, find large perturbations that in invariant directions of the network. However, the experiments are overall not very useful to the comprehension of the paper and not that illustrative.

I have several questions for the authors:
- the conditions presented in Theorem 4, seem hard to check in practice; what is the time complexity of this operation? I believe that checking if A is omnidirectional is equivalent to an LP but how do you solve the combinatorial size of doing that over all set of indices?
- I understand the upper bounds on the singular values, but I am not sure how they relate to inverse stability. Maybe more explanation and quantitative analysis (e.g. relating the volume of the preimage of an epsilon ball around z to the singular values) could be helpful.
- Is there actionable consequences one could draw from your papers? The way the results are presented seem like they are only useful inspection after training; are your results able to derive methods to enforce conditions on the pre-images for example?

In conclusion, this paper does an interesting and original analysis which can help us understand better the polytopes composing the input space. The experiments are not very convincing or illustrative of the theoretical results in my opinion. It is not clear how those observations can affect practical algorithms and this is something I hope the author can address.","The sentiment of the review is generally positive, as the reviewer acknowledges the paper's interesting and original analysis, the correctness of the proofs, and the simplicity of the linear algebra arguments. However, there are some criticisms regarding the structure of the paper, the usefulness of the experiments, and the practical implications of the results. Therefore, the sentiment score is not fully positive but leans towards the positive side. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, asking questions and making suggestions in a respectful manner.",40,90
"The reinforcement learning tasks with sparse rewards are very important and challenging. The main idea of this work is to encourage intra-life novelty. The authors introduce the curiosity grid and the intrinsic reward term so that the agent can explore toward unvisited states at every episode. 

However, the results are not enough to be accepted to ICLR having a very high standard. In Section 3, the authors compare the game scores of DeepCS proposed in this paper only against to A2C. There are some RL algorithms reported to be better than A2C. For instance, I would like to see the comparison between DeepCS and SmartHash by Tang et al 2017. 

=================================================================================================
I've read the rebuttal. I updated my score but still not vote for accept. 

This paper is not my main research area. Very unfortunately, this paper was assigned to me. The main issue of this paper is the fair comparisons with other works. However, I don't have enough knowledges to judge this point.  So please assess this paper with other reviewers comments.
","The sentiment of the review is mixed. The reviewer acknowledges the importance and challenge of the topic, which is positive, but also expresses significant concerns about the adequacy of the results and the fairness of comparisons, which is negative. Therefore, the sentiment score is slightly negative. The politeness of the language is generally respectful, even when expressing dissatisfaction and limitations in their own expertise, so the politeness score is high.",-20,80
"Pros:
- The paper generalizes upon past observations by Ott et al. that NMT models might decode ""copies"" (of the source sentence) when using large beam widths, which results in degraded results. In particular, the present paper observes similar shortcomings in two additional tasks (summarization and captioning), where decoding with large beam widths results in ""training set predictions."" It's unclear if this observation is novel, but in any case the connection between these observations across NMT and summarization/captioning tasks is novel.
- The paper draws a connection between the observed degradation and ""label bias"", whereby prefixes with a low likelihood are selected merely because they lead to (nearly-)deterministic transitions later in decoding.
- The paper suggests two simple heuristics for mitigating the observed degradation with large beam widths, and evaluates these heuristics across three tasks. The results are convincing.
- The paper is very well written. The analysis throughout the paper is easy to follow and convincing.

Cons:
- Although the analysis is very valuable, the quantitive impact of the proposed heuristics is relatively minor.

Comments/questions:
- In Eq. 2, consider using $v$ or $w$ for the max instead of overloading $y$.
- To save space, you might compress Figure 1 into a single figure with three differently-styled bars per position that indicate the beam width (somewhat like how Figure 3 is presented). You can do this for Figure 2 as well, and these compressed figures could then be collapsed into a single row.
- In Section 5, when describing the ""Discrepancy gap"" constraint, you say that you ""modify Eq. 3 to include the constraint"", but I suspect you meant that you modify Eq. 1 to include this constraint.
- In Table 4, why didn't you tune $\mathcal{M}$ and $\mathcal{N}$ separately for each beam width?","The review starts with a list of positive aspects of the paper, highlighting its contributions and the quality of writing. This indicates a generally positive sentiment. The cons section is brief and acknowledges the value of the analysis while pointing out a minor limitation. The comments and questions are constructive and aimed at improving the paper, which reflects a polite tone. Therefore, the sentiment score is high, and the politeness score is also high.",80,90
"The manuscript proposes a method for unsupervised learning with unknown class number k. The problem is classical and important. The proposed method is interesting and novel, but the experiments are not convincing. In detail, it did not compare other methods in the experiments. 
Pros: clear description and novelty of the method
Cons: insufficient experiments. ","The sentiment of the review is mixed but leans slightly positive. The reviewer acknowledges the importance of the problem and the novelty of the method, which are positive points. However, the reviewer also points out a significant flaw in the experiments, which is a negative point. Therefore, the sentiment score is slightly positive at 20. The politeness of the language is quite high. The reviewer uses polite and constructive language, such as 'interesting and novel' and 'clear description,' even when pointing out the flaws. Therefore, the politeness score is 80.",20,80
"edit: the authors added several experiments (better evaluation of the predicted lambda, comparison with CodeSLAM), which address my concerns. I think the paper is much more convincing now. I am happy to increase my rating to clear accept.

I also agree with the introduction of the Chi vector, and with the use of the term of ""photometric BA"", since it was used before, even if it is unfortunate in my opinion. I thank the authors to replace reprojection by alignment, which is much clearer.

---------------


This paper presents a method for dense Structure-from-Motion using Deep Learning:
The input is a set of images; the output is the camera poses and the depth maps for all the images.
The approach is inspired by Levenberg-Marquardt optimization (LM): A pipeline extracting image features computes the Jacobian of an error function. This Jacobian is used to update an estimate of the camera poses. As in LM optimization, this update is done based on a factor lambda, weighting a gradient descent step and a Gauss-Newton step. In LM optimization, this lambda evolves with the improvement of the estimate. Here lambda is also predicted using a network based on the feature difference.

If I understand correctly, what is learned is how to compute image features that provide good updates, how to predict the depth maps from the features, and how to predict lambda.

The method is compared against DeMoN and other baselines with good results.

I like the fact that the method is based on LM optimization, which is the standard method in 'geometric bundle adjustment', while related works consider Gauss-Newton-like optimization steps. The key was to include a network to predict lambda as well.

However, I have several concerns:

* the ablation study designed to compare with a Gauss-Newton-like approach does not seem correct. The image features learned with the proposed method are re-used in an approach using a fixed lambda. If I understand correctly, there are 2 things wrong with that:
- for GN optimization, lambda should be set to 0 - not a constant value. Several constant values should also have been tried.
- the image features should be re-trained for the GN framework:  Since the features are learned for the LM iteration, they are adapted to the use of the predicted lambda, but they are not necessarily suitable to GN optimization.
Thus, the advantage of using a LM optimization scheme is not very convincing.

Since the LM-like approach is the main contribution, and the reported experiments do not show an advantage over GN-like approaches (already taken by previous work), this is my main reason for proposing rejection.

* CodeSLAM (best paper at CVPR'18) is referenced but there is no comparison with it, while a comparison on the EuRoC dataset should be possible.

Less critical concerns that still should be taken into account if the paper is accepted:

- the state vector Chi is not defined for the proposed method, only for the standard bundle adjustment approach. If I understand correctly is made of the camera poses.

- the name 'Bundle Adjustment' is actually not adapted to the proposed method.  'Bundle Adjustment' in 'geometric computer vision' comes from the optimization of several rays to intersect at the same 3D point, which is done by minimizing the reprojection errors. Here the objective function is based on image feature differences. I thus find the name misleading. The end of Section 3 also encourages the reader to think that the proposed method is based on the reprojection error. The proposed method is more about dense alignment for multiple images.


More minor points:

1st paragraph:  Marquet -> Marquardt
title of Section 3: revisitED
1st paragraph of Section 3: audience -> reader
caption of Fig 1: extractS
Eq (2) cannot have Delta Chi on the two sides. Typically, the left side should be \hat{\Delta \Chi}
before Eq (3): the 'photometric ..' -> a 'photometric ..'
1st paragraph of Section 4.3: difficulties -> reason
typo in absolute in caption of Fig 4
Eq (6): Is B the same for all scenes?  It would be interesting to visualize it.
Section 4.5: applies -> apply
","The sentiment of the review is positive, as indicated by the reviewer's statement that the paper is much more convincing now and their willingness to increase their rating to clear accept. This suggests a sentiment score of 80. The politeness of the language is also high, as the reviewer thanks the authors for their changes and provides constructive feedback in a respectful manner. This suggests a politeness score of 90.",80,90
"This paper proposes Leap, a meta-learning procedure that finds better initialization for new tasks. Leap is based on past training/optimization trajectories and updates the initialization to minimize the total trajectory lengths. Experiments show that Leap outperforms popular alternatives like MAML and Reptile.

Pros
- Novel idea
- Relatively well-written
- Sufficient experiment evidence

Cons
- There exist several gaps between the theory and the algorithm

I have several concerns.
1. The idea is clearly delivered, but there are several practical treatments that are questionable. The first special treatment is that on page 5, when the objective is increased instead of decreased, the sign of the f part is flipped, which is not theoretically sound. It is basically saying that when we move from psi^i to psi^{i+1} with increased objective, we lie to the meta-learner that it is decreasing. The optimization trajectory is what it is. It would be beneficial to see the effect of removing this trick, at least in the experiments. Second, replacing the Jacobian with the identity matrix is also questionable. Suppose we use a very small but constant learning rate alpha for a convex problem. Then J^i=(I-G)^i goes to the zero matrix as i increases (G is small positive). However, instead, the paper uses J^i=I for all i. This means that the contributions for all i are the same, which is unsubstantiated.

2. The proof of Thm1 in Appendix A is not complete. For example, ""By assumption, beta is sufficiently small to satisfy F"", which I do not understand the inequality. Is there a missing i superscript? Isn't this the exact inequality we are trying to prove for i=0? As another example, ""if the right-most term is positive in expectation, we are done"", how so? BTW, the right-most term is a vector so there must be something missing. It would be more understandable if the proof includes a high-level proof roadmap, and frequently reminds the reader where we are in the overall proof now.

3. The set \Theta is not very well-defined, and sometimes misleading. Above Eq.(6), \Theta is mathematically defined as the intersection of points whose final solutions are within a tolerance of the *global* optimum, which is in fact unknown. As a result, finding a good initialization in \Theta for all the tasks as in Eq.(5) is not well-defined.

4. About the experiments. What is the ""Finetuning"" in Table 1? Presumably it is multi-headed but it should be made explicit. What is the standard deviation for Fig.4? The claim that ""Leap learns faster than a random initialization"" for Breakout is not convincing at all.

Minors
- In Eq.(4), f is a scalar so abs should suffice. This also applies to subsequent formulations.
- \mu is introduced above Eq.(8) but never used in the gradient formula.
- On p6, there is a missing norm notation when introducing the Reptile algorithm.","The sentiment of the review is moderately positive, as the reviewer acknowledges the novelty of the idea, the quality of writing, and the sufficiency of experimental evidence. However, the review also points out several significant concerns and gaps in the theory and algorithm, which tempers the overall positivity. Therefore, the sentiment score is 30. The politeness of the language is quite high; the reviewer uses polite and constructive language throughout, even when pointing out flaws and making recommendations. Therefore, the politeness score is 80.",30,80
"(Score raised from 8 to 9 after rebuttal)
The paper examines the hypothesis that randomly initialized (feed-forward) neural networks contain sub-networks that train well in the sense that they converge equally fast or faster and reach the same or better classification accuracy. Interestingly, such sub-networks can be identified by simple, magnitude-based pruning. It is crucial that these sub-networks are initialized with their original initialization values, otherwise they typically fail to be trained, implying that it is not purely the structure of the sub-networks that matters. The paper thoroughly investigates the existence of such “winning-tickets” on MNIST and CIFAR-10 on both, fully connected but also convolutional neural networks. Winning-tickets are found across networks, various optimizers, at different pruning-levels and across various other hyper-parameters. The experiments also show that iterative pruning (with re-starts) is more effective at finding winning-tickets.

The paper adds a novel and interesting angle to the question of why neural networks apparently need to be heavily over-parameterized for training. This question is intriguing and of high importance to further the understanding of how neural networks train. Additionally, the findings might have practical relevance as they might help avoid unnecessary over-parameterization which, in turn, might save use of computational resources and energy. The main idea is simple (which is good) and can be tested with relatively simple experiments (also good). The experiments conducted in the paper are clean (averaging over multiple runs, controlling for a lot of factors) and should allow for easy reproduction but also for clean comparison against future experiments. The experimental section is well executed, the writing is clear and good and related work is taken into account to a sufficient degree. The paper touches upon a very intriguing “feature” of neural networks and, in my opinion, should be relevant to theorists and practitioners across many sub-fields of deep learning research. I therefore vote and argue for accepting the paper for presentation at the conference. The following comments are suggestions to the authors on how to further improve the paper. I do not expect all issues to be addressed in the camera-ready version.

1) The main “weakness” of the paper might be that, while the amount of experiments and controls is impressive, the generality of the lottery ticket hypothesis remains somewhat open. Even when restricting the statement to feed-forward networks only, the networks investigated in the paper are relatively “small” and MNIST and CIFAR-10 bear the risk of finding patterns that do not hold when scaling to larger-scale networks and tasks. I acknowledge and support the author’s decision to have thorough and clean experiments on these small models and tasks, rather than having half-baked results on ImageNet, etc. The downside of this is that the experiments are thus not sufficient to claim (with reasonable certainty) that the lottery ticket hypothesis holds “in general”. The paper would be stronger, if the existence of winning tickets on larger-scale experiments or tasks other than classification were shown - even if these experiments did not have a large number of control experiments/ablation studies.

2)  While the paper shows the existence of winning tickets robustly and convincingly on the networks/tasks investigated, the next important question would be how to systematically and reliably “break” the existence of lottery tickets. Can they be attributed to a few fundamental factors? Are they a consequence of batch-wise, gradient-based optimization, or an inherent feature of neural networks, or is it the loss functions commonly used, …? On page 2, second paragraph, the paper states: ”When randomly reinitialized, our winning tickets no longer match the performance of the original network, explaining the difficulty of training pruned networks from scratch”. I don’t fully agree - the paper certainly sheds some light on the issue, but an actual explanation would result in a testable hypothesis. My comment here is intended to be constructive criticism, I think that the paper has enough “juice” and novelty for being accepted - I am merely pointing out that the overall story is not yet conclusive (and I am aware that it might need several more publications to find these answers).

3) Do the winning tickets generalize across hyper-parameters or even tasks. I.e. if a winning ticket is found with one set of hyper-parameters, but then Optimizer/learning-rate/etc. are changed, does the winning-ticket still lead to improved convergence and accuracy? Same question for data-sets: do winning-tickets found on CIFAR-100 also work for CIFAR-10 and vice versa? If winning-tickets turn out to generalize well, in the extreme this could allow “shipping” each network architecture with a few good winning-tickets, thus making it unnecessary to apply expensive iterative pruning every time. I would not expect generalization across data-sets, but it would be highly interesting to see if winning tickets generalize in any way (after all I am still surprised by how well adversarial examples generalize and transfer).

4) Some things that would be interesting to try:
4a) Is there anything special about the pruned/non-pruned weights at the time of initialization? Did they start out with very small values already or are they all “behind” some (dead) downstream neuron? Is there anything that might essentially block gradient signal from updating the pruned neurons? This could perhaps be checked by recording weights’ “trajectories” during training to see if there is a correlation between the “distance weights traveled” and whether or not they end up in the winning ticket.
4b) Do ARD-style/Bayesian approaches or second-order methods to pruning identify (roughly) the same neurons for pruning?

5) Typo (should be through): “we find winning tickets though a principled search process”

6) For the standard ConvNets I assume you did not use batchnorm. Does batchnorm interfere in any way with the existence of winning tickets? (at least on ResNet they seem to exist with batchnorm as well)
","The sentiment of the review is highly positive. The reviewer praises the novelty, importance, and thoroughness of the paper, and explicitly states that they vote for its acceptance. The sentiment score is therefore 90. The politeness of the language is also very high. The reviewer uses constructive criticism and polite language throughout, even when pointing out weaknesses or areas for improvement. The politeness score is 100.",90,100
"The paper proposes a convolutional alternative to self-attention. To achieve this, the number of parameters of a typical convolution operation is first reduced by using a depth-wise approach (i.e. convolving only within each channel), and then further reduced by tying parameters across layers in a round-robin fashion. A softmax is applied to the filter weights, so that the operation computes weighted sums of its (local) input (LightConv).

Because the number of parameters is dramatically reduced now, they can be replaced by the output of an input-dependent linear layer (DynamicConv), which gives the resulting operation a ""local attention"" flavour. The weights depend only on the current position, as opposed to the attention weights in self-attention which depend on all positions. This implies that the operation is linear in the number of positions as opposed to quadratic, which is a significant advantage in terms of scaling and computation time.

In the paper, several NLP benchmarks (machine translation, language modeling) that were previously used to demonstrate the efficacy of self-attention models are tackled with models using LightConv and DynamicConv instead, and they are shown to be competitive across the board (with the number of model parameters kept approximately the same).

This paper is well-written and easy to follow. The proposed approach is explained and motivated well. The experiments are thorough and the results are convincing. I especially appreciated the ablation experiment for which results are shown in Table 3, which provides some useful insights beyond the main point of the paper. The fact that a linear time approach can match the performance of self-attention based models is a very promising and somewhat surprising result.

In section 5.3, I did not understand what ""head band, next band, last band"" refers to. I assume this is described in the anonymous paper that is cited, so I suppose this is an artifact of blind review. Still, even with the reference unmasked it might be useful to add some context here.","The review is highly positive, praising the paper for being well-written, easy to follow, and for providing thorough and convincing experiments. The reviewer also appreciates the ablation experiment and finds the results promising and somewhat surprising. The only minor critique is a request for clarification in section 5.3, which is presented politely and constructively. Therefore, the sentiment score is very high, and the politeness score is also high due to the courteous and constructive language used.",90,90
"There are several ingredients in this paper that I really liked. For example, (1) the notion that an agent should build a deterministic function of the past which implicitly captures the belief (the uncertainty or probability distribution about the state), by opposition for example to sampling trajectories to capture uncertainty, (2) modelling the world's dynamic in a learned encoded state-space (by opposition to the sensor space), (3) instead of modeling next-step probabilities p(z(t+1)|z(t)), model 'jumpy transitions' p(z(t+delta)|z(t)) to avoid unrolling at the finest time scale.

Now for the weak points:
(a) the justification for the training loss was not completely clear to me, although I can see that it has a variational flavor
(b) there is no discussion of the issue that we can't get a straightforward decomposition of the joint probability over the data sequence according to next-step probabilities via the chain rule of probabilities, so we don't have a clear way to compare the TD-VAE models with jumpy predictions against other more traditional models
(c) none of the experiments make comparisons against previously published models and quantitative results (admittedly because of (b) this may not be easy).

So I believe that the authors are onto a great direction of investigation, but the execution of the paper could be improved.","The review starts with a positive tone, highlighting several aspects of the paper that the reviewer liked. This indicates a generally positive sentiment towards the work. However, the reviewer also points out several weak points and areas for improvement, which slightly tempers the overall sentiment. Therefore, the sentiment score is moderately positive. The language used in the review is constructive and polite, even when pointing out weaknesses. The reviewer uses phrases like 'not completely clear to me' and 'could be improved,' which are polite ways of providing criticism.",60,80
"The paper is well written and flow well. The only thing I would like to see added is an elaboration of 
""run a semantic parsing module to translate a question into an executable program"". How to do semantic parsing is far from obvious. This topic needs at least a paragraph of its own. 

This is not a requirement but an opportunity, can you explain how counting work? I think you have it at the standard level of the magic of DNN but some digging into the mechanism would be appreciated. 

In concluding maybe you can speculate how far this method can go. Compositionality? Implicit relations inferred from words and behavior? Application to video with words?   ","The sentiment of the review is positive, as indicated by phrases like 'well written' and 'flow well.' The reviewer provides constructive feedback and suggestions for improvement without any negative remarks. Therefore, the sentiment score is 80. The politeness of the language is also high, as the reviewer uses polite phrases such as 'I would like to see,' 'This is not a requirement but an opportunity,' and 'maybe you can speculate.' The reviewer is respectful and considerate in their suggestions, so the politeness score is 90.",80,90
"Post-rebuttal revision: The authors have adressed my concerns sufficiently. The paper still has issues with presentation, and weak comparisons to earlier methods. However, the field is currently rapidly developing, and comparing to earlier works is often difficult. I believe the Langevin-based prediction is a significant and clever contribution. I'm raising my score to 6.

------

The paper proposes an end-to-end neural architecture for learning protein structures from sequences. The problem is highly important. The method proposes to use a Langevin simulator to fold the protein ‘in silico’ from some initial state, proposes numerous tricks for the optimisation, and proposes neural networks to extract information from both the sequence and the fold state (energy function). The system works on internal coordinates, which are conditioned and integrated on the fly. The method seems to perform very well, improving upon their baseline model considerably.

In spite of the paper being an outstanding work, I have two criticisms about the accessibility and impact of the paper on the broader ICLR audience. In its current form and complexity, the paper feels accessible mostly to a narrow audience.

First, the framework proposed in the paper is massive, containing a large amount of components, neural networks, simulators, integrators, optimisation tricks, alignments, profiles, stabilizations, etc. The amount of work done in the manuscript is staggering, but the method is also difficult to understand from reading the main manuscript alone. The 10+ page appendix is critical for understanding (for instance, the appendix reveals that MSA is used to generate more data), and even with it the method is difficult to grasp as a whole. This paper should be presented in a journal form with a presentation not hindered by page limits, while currently one needs to jump between the main text and appendix to get the whole picture. I also wonder if some parts of the system have already been published, and perhaps the presentation could be condensed that way. 

Second, the introduction lists numerous competing methods both on the protein modelling side and on the MCMC vs optimisation side. The paper does not compare to any of these, which is strange, and makes it difficult to assess how much this paper improves upon state-of-the-art. Right now its unclear what is state-of-the-art in general. No bigger context of protein folding is given either, for instance, how well the method fares against purely alignment based approaches, or against purely physics-based simulators. Finally, the experimental section poorly describes how all the pieces of the system affect the final predictions. The discussion on the exploding gradients and dampening is excellent however. The only baseline is one with the simulator replaced by an RNN. There does not seem to be any running time analyses. As such, it is hard to interpret the current system, and it feels like a black box.","The sentiment of the review is generally positive, as indicated by phrases like 'The authors have addressed my concerns sufficiently' and 'the Langevin-based prediction is a significant and clever contribution.' However, the reviewer also points out several issues with the paper, such as its presentation and lack of comparisons to earlier methods. This mixed feedback suggests a sentiment score of 50. The politeness of the language is high, with the reviewer using respectful and constructive language throughout, such as 'I believe' and 'I wonder if,' which suggests a politeness score of 80.",50,80
"This paper further explores the work of Chen et al. (2018) applied to reversible generative modelling. While section 1 and 2 focuses on framing the context of this work. The ODE solver architecture for continuous normalizing flow learn a density mapping using an instantaneous change of variable formula.
The contribution of this work seems to be enabling the use of deeper neural network than in Chen et al. (2018)  as part of the ODE solver flow. While the single-layer architecture in Chen et al. (2018) enable efficient exact computation of the Jacobian Trace, using a deeper architecture compromises that property. As a result, the authors propose to use the unbiased Hutchinson trace estimator of the Jacobian Trace. Furthermore, the authors observe that using a bottleneck architecture reduces the rank of the Jacobian and can therefore help reducing the variance of the estimator. 
The density estimation task in 2D is nice to see but lacks comparison with Chen et al. (2018), on which this paper improves. Moreover, is the Glow model used here only using additive coupling layers? If so, this might explain the difficulties of this Glow model. 
Although the model presented in this paper doesn't obtain state-of-the-art results on the larger problems, the work presented in this paper demonstrates the ability of ODE solvers as continuous normalizing flows to be competitive in the space of prescribed model.
Concerning discussions and analysis:
- given the lack of improvement using the bottleneck trick, is there an actual improvement in variance using this trick? or is this trick merely explaining why using a bottleneck architecture more suited for the Hutchinson trace estimator?
In algorithm 1, is \epsilon only one random vector that keeps being reused at every step of the solver algorithm? I would be surprised that the use of a single random vector across different steps did not significantly increased the variance of the estimator.","The sentiment of the review is moderately positive. The reviewer acknowledges the contributions of the paper, such as enabling the use of deeper neural networks and demonstrating the ability of ODE solvers as continuous normalizing flows to be competitive. However, the reviewer also points out several areas for improvement, such as the lack of comparison with Chen et al. (2018) and the need for clarification on certain points. The politeness of the language is quite high; the reviewer uses polite and constructive language throughout the review, asking questions and making suggestions in a respectful manner.",40,80
"This paper combines state of the art models for piano transcription, symbolic music synthesis, and waveform generation all using a shared piano-roll representation.  It also introduces a new dataset of 172 hours of aligned MIDI and audio from real performances recorded on Yamaha Disklavier pianos in the context of the piano-e-competition.  

By using this shared representation and this dataset, it is able to expand the amount of time that it can coherently model music from a few seconds to a minute, necessary for truly modeling entire musical pieces.

Training an existing state of the art transcription model on this data improves performance on a standard benchmark by several percentage points (depending on the specific metric used).

Listening test results show that people still prefer the real recordings a plurality of the time, but that the syntheses are selected over them a fair amount.  One thing that is clear from the audio examples is that the different systems produce output with different equalization levels, which may lead to some of the listening results.  If some sort of automatic mastering were done to the outputs this might be avoided.

While the novelty of the individual algorithms is relatively meager, their combination is very synergistic and makes a significant contribution to the field.  Piano music modeling is a long-standing problem that the current paper has made significant progress towards solving.

The paper is very well written, but there are a few minor issues:
* Eq (1) this is really the joint distribution between audio and notes, not the marginal of audio
* Table 4: What do precision, recall, and f1 score mean for notes with velocity?  How close does the system have to be to the velocity to get it right?
* Table 6: NLL presumably stands for Negative Log Likelihood, but this should be made explicity
* Figure 2: Are the error bars the standard deviation of the mean or the standard error of the mean?
","The review is generally positive about the paper, highlighting its significant contributions to the field and the synergistic combination of state-of-the-art models. The sentiment score is high because the reviewer acknowledges the progress made and the well-written nature of the paper, despite noting some minor issues. The politeness score is also high as the language used is constructive, respectful, and offers specific suggestions for improvement without being harsh or dismissive.",80,90
"This paper presents a very interesting investigation of the expressive capabilities of graph neural networks, in particular focusing on the discriminative power of such GNN models, i.e. the ability to tell that two inputs are different when they are actually different.  The analysis is based on the study of injective representation functions on multisets.  This perspective in particular allows the authors to distinguish different aggregation methods, sum, mean and max, as well as to distinguish one layer linear transformations from multi-layer MLPs.  Based on the analysis the authors proposed a variant of the GNN called Graph Isomorphism Networks (GINs) that use MLPs instead of linear transformations on each layer, and sum instead of mean or max as the aggregation method, which has the most discriminative power following the analysis.  Experiments were done on node classification benchmarks to support the claims.

Overall I quite liked this paper.  The study of the expressive capabilities of GNNs is a very important problem.  Given the popularity of this class of models recently, theoretical analysis for these models is largely missing.  Previous attempts at studying the capability of GNNs focus on the function approximation perspective (e.g. Mapping Images to Scene Graphs with Permutation-Invariant Structured Prediction by Hertiz et al. which is worth discussing).  This paper presents a very different angle focusing on discriminative capabilities.  Being able to tell two inputs apart when they are different is obviously just one aspect of representation power, but this paper showed that studying this aspect can already give us some interesting insights.

I do feel however that the authors should make it clear that discriminative power is not the only thing we care, and in most applications we are not doing graph isomorphism tests.  The ability to tell, for example, how far two inputs are, when they are not the same is also very (and maybe more) important, which such isomorphism / injective map based analysis does not capture at all.  In fact the assumption that each feature vector can be mapped to a unique label in {a, b, c, ...} (Section 3 first paragraph) is overly simplistic and only makes sense for analyzing injective maps.  If we want to reason anything about the continuity of the features and representations, this assumption does not apply, and the real set is not countable so such a mapping cannot exist.

In equation 4.1 describes the GIN update, which is proposed as “the most powerful GNN”.  However, such architecture is not really new, for example the Interaction Networks (Battaglia et al. 2016) already uses sum aggregation and MLP as the building blocks.  Also, it is said that in the first iteration a simple sum is enough to implement injective map, this is true for sum, but replacing that with mean and max can lose information very early on.  Another MLP on the input features at least for mean or max aggregation for the first iteration is therefore necessary.  This isn’t made very clear in the paper.

The training set results presented in section 6.1 is not very clear.  The plots show only one run for each model variant, which run was it?  As the purpose is to show that some variants fit well, and some others overfit, these runs should be chosen to optimize training set performance, rather than generalization.  Also the restrictions should be made clear that all models are given the same (small) amount of hidden units per node.  I imagine if the amount of hidden units are allowed to be much bigger, mean and max aggregators should also catch up.

As mentioned earlier I quite liked the paper despite some restrictions anc things to clarify.  I would vote for accepting this paper for publication at ICLR.

--------

Considering the counter-example given above, I'm lowering my scores a bit.  The proof of theorem 3 is less than clear.  The proof for the first half of theorem 3 (a) is quite obvious, but the proof for the second half is a bit hand-wavy.

In the worst case, the second half of theorem 3 (a) will be invalid.  The most general GNN will then have to use an update function in the form of the first half of 3(a), and all the other analysis still holds.  The experiments will need to be rerun.

--------

Update: the new revision resolved the counter-example issue and I'm mostly happy with it, so my rating was adjusted again.","The sentiment of the review is generally positive, as the reviewer expresses appreciation for the paper's interesting investigation and acknowledges its importance in the field. However, there are some critical points and suggestions for improvement, which slightly temper the overall positivity. Therefore, the sentiment score is 70. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, even when pointing out areas for improvement. The reviewer also acknowledges the authors' efforts and expresses a willingness to accept the paper with revisions. Thus, the politeness score is 90.",70,90
"General:
The paper tackles a problem of learning long-range dependencies in images in order to obtain high fidelity images. The authors propose to use a specific architecture that utilizes three main components: (i) a decoder for sliced small images, (ii) a size-upscaling decoder for large image generation, (iii) a depth-upscaling decoder for generating high-res image. The main idea of the approach is slicing a high-res original image and a new factorization of the joint distribution over pixels. In this model various well-known blocks are used like 1D Transformer and Gated PixelCNN. The obtained results are impressive, the generated images are large and contain realistic details.

In my opinion the paper would be interesting for the ICLR audience.

Pros:
+ The paper is very technical but well-written.
+ The obtained results constitute new state-of-the-art on HQ image datasets.
+ Modeling long-range dependencies among pixels is definitely one of the most important topics in image modeling. The proposed approach is a very interesting step towards this direction.

Cons:
- The authors claim that the proposed approach is more memory efficient than other methods. However, I wonder how many parameters the proposed approach requires comparing to others. It would be highly beneficial to have an additional column in Table 1 that would contain number of parameters for each model.
- All samples are take either at an extremely high temperature (i.e., 0.99) or at the temperature equal 1. How do the samples look for smaller temperatures? Sampling at very high temperature is a nice trick for generating nicely looking images, however, it could hide typical problems of generative models (e.g., see Rezende & Viola, “Taming VAEs”, 2018).

--REVISION--
I would like to thank the authors for their response. I highly appreciate their clear explanation of both issues raised by me. I am especially thankful for the second point (about the temperature) because indeed I interpreted it as in the GLOW paper. Since both my concerns have been answered, I decided to raise the final score (+2).","The sentiment of the review is positive, as indicated by phrases like 'the obtained results are impressive' and 'the paper would be interesting for the ICLR audience.' The reviewer also appreciates the technical quality and the state-of-the-art results. The politeness of the language is high, with the reviewer expressing gratitude for the authors' responses and providing constructive feedback in a respectful manner.",80,90
"This paper addresses questions about the representation of visual information in the retina. The authors create a deep neural network model of the visual system in which a single parameter (bandwidth between the “retina” and “visual cortex” parts) is sufficient to qualitatively reproduce retinal receptive fields observed across animals with different brain sizes, which have been hard to reconcile in the past. 

This work is an innovative application of deep neural networks to a long-standing question in visual neuroscience. While I have some questions about the analyses and conclusions, I think that the paper is interesting and of high quality.

My main concern is that the authors only show single examples, without quantification, for some main results (RF structure). For example, for Fig. 2A and 2B, an orientation selectivity index should be shown for all neurons. A similar population analysis should be devised for Fig 2C, e.g. like Fig 3 in [1]

Minor comments:
1. Page 4: “These results suggest that the key constraint ... might be the dimensionality bottleneck..”: The analyses only show that the bottleneck is *sufficient* to explain the differences, but “the key constraint” also implies *necessity*. Either soften the claim or provide control experiments showing that alternative hypotheses (constraint on firing rate etc.) cannot explain this result in your model.

2. I don’t understand most of the arguments about “cell types” (e.g. Fig. 2F and elsewhere). In neuroscience, “cell types” usually refers to cells with completely different connectivity constraints, e.g. excitatory vs. inhibitory cells or somatostatin vs. parvalbumin cells. But you refer to different CNN channels as different “types”. This seems very different than the neuroscience definition. CNN channels just represent different feature maps, i.e. different receptive field shapes, but not fundamentally different connectivity patterns. Therefore, I also don’t quite understand what you are trying to show with the weight-untying experiments (Fig. 2E/F).

3. It is not clear to me what Fig. 3B and the associated paragraph are trying to show. What are the implications of the nonlinearity being due to the first or second stage? 

4. Comment on Fig 3F: The center-surround RFs probably implement a whitening transform (which is linear). Whitened inputs can probably be represented more efficiently in a network trained with L2-regularization and/or SGD. This might explain why the “quasi-linear” retina improves separability later-on.

[1] Cossell, Lee, Maria Florencia Iacaruso, Dylan R. Muir, Rachael Houlton, Elie N. Sader, Ho Ko, Sonja B. Hofer, and Thomas D. Mrsic-Flogel. “Functional Organization of Excitatory Synaptic Strength in Primary Visual Cortex.” Nature 518, no. 7539 (February 19, 2015): 399–403. https://doi.org/10.1038/nature14182.","The sentiment of the review is generally positive, as the reviewer acknowledges the innovative application of deep neural networks and considers the paper interesting and of high quality. However, the reviewer also points out several concerns and suggestions for improvement, which slightly temper the overall positivity. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, offering specific recommendations without being harsh or dismissive. Therefore, the politeness score is 90.",60,90
"Revision 2: The new comparisons with CPC are very helpful.  Most of my other comments are addressed in the response and paper revision.  I am still uncomfortable with the sentence ""Our method ... compares favorably with fully-supervised learning on several classification tasks in the settings studied.""  This strongly suggests to me that you are claiming to be competitive with SOTA supervised methods.  The paper does not contain supervised results for the resnet-50 architecture.  I would recommend that this sentence should either be dropped from the abstract or have the phrase ""in the settings studied"" replaced by ""for an alexnet architecture"".  If you have supervised results for resnet-50 they should be added to table 3 and the abstract could be adjusted to that.  I apologize that this is coming after the update deadline (I have been traveling).  The authors should simply consider the reaction of the community to over-claiming.  Because of the new comparisons with CPC on resnet-50 I am upping my score.  My confidence is low only because the real significance can only be judged over time.

Revision 1: This is a revision of my earlier review.  My overly-excited earlier rating was based on tables 1 and 2 and the claim to have unsupervised features that are competitive with fully-supervised features. (I also am subject to an a-priori bias in favor of mutual information methods.)  I took the authors word for their claim and submitted the review without investigating existing results on CIFAR10.  It seems that tables 1 and 2 are presenting extremely weak fully supervised baselines.  If DIM(L) can indeed produce features that are competitive with state of the art fully supervised features, the result is extremely important.  But this claim seems misrepresented in the paper.

Original review:

There is a lot of material in this paper and I respect this groups
high research-to-publication ratio. However, it might be nice to have
the paper more focused on the subset of ideas that seem to matter.

My biggest comment is that the top level spin seems wrong.
Specifically, the paper focuses on the two bullets on page 3 ---
mutual information and statistical constraints.  Here mutual
information is interpreted as the information between the input and
output of a feature encoder.  Clearly this has a trivial solution
where the input equals the output so the second bullet --- statistical
constraints --- are required.  But the empirical content of the paper
strongly undermines these top level bullets.  Setting the training
objective to be the a balance of MI between input and output under a
statistical consrtraint leads to DIM(G) which, according the results in
the paper, is an empirical disaster.  DIM(L) is the main result and
something else seems to be going on there (more later).  Furthermore,
the empirical results suggest that the second bullet --- statistical
constraints --- is of very little value for DIM(L). The key ablation
study here seems to be missing from the paper.  Appendix A.4 states
that ""a small amount of the [statistical constraint] helps improve
classification results when used with the [local information
objective].  No quantitative ablation number is given.  Other measures
of the statistical constraint seem to simply measure to what extent
the constraint has been successfully enforced.  But the results
suggest that even successfully enforcing the constraint is of little,
if any, value for the ability of the features to be effective in
prediction.  So, it seems to me, the paper to really just about the
local information objective.

The real power house of the paper --- the local information objective
--- seems related to mutual information predictive coding as
formalized in the recent paper from deep mind by van den Oord et al
and also an earlier arxiv paper by McAllester on information-theoretic
co-training.  In these other papers one assumes a signal x_1, ... x_T
and tries to extract low dimensional features F(x_t) such that F(x_1),
..., F(x_t) carries large mutual information with F(x_{t+1}).  The
local objective of this paper takes a signal x1, ..., x_k (nXn
subimages) and extracts local features F(x_1), ... F(x_k) and a global
feature Y(F(x_1), ..., F(x_k)) such that Y carries large mutual
information with each of the features F(x_i).  These seem different
but related.  The first seems more ""on line"" while the second seems
more ""batch"" but both seem to be getting at the same thing, especially
when Y is low dimensional.

Another comment about top level spin involves the Donsker-Varadhan
representation of KL divergence (equation (2) in the paper).  The
paper states that this is not used in the experiments.  This suggests
that it was tried and failed.  If so, it would be good to report this.
Another contribution of the paper seems to be that the mutual
information estimators (4) and (5) dominate (2) in practice.  This
seems important.

","The sentiment of the review is mixed. The reviewer acknowledges the helpfulness of the new comparisons and addresses most of their previous comments, which is positive. However, they express discomfort with certain claims made in the paper and suggest modifications, indicating some level of dissatisfaction. Therefore, the sentiment score is around 20. The politeness of the language is quite high. The reviewer uses polite phrases such as 'I apologize,' 'I would recommend,' and 'The authors should simply consider,' which indicates a respectful and considerate tone. Therefore, the politeness score is 80.",20,80
The paper proposed a novel differentiable neural GLCM network which captures the high reference textural information and discard the lower-frequency semantic information so as to solve the domain generalisation challenge. The author also proposed an approach “HEX” to discard the superficial representations. Two synthetic datasets are created for demonstrating the methods advantages on scenarios where the domain-specific information is correlated with the semantic information. The proposal is well structured and written. The quality of the paper is excellent in terms of novelty and originality. The proposed methods are evaluated thoroughly through experiments with different types of dataset and has shown to achieve good performance. ,"The review is highly positive, praising the novelty, originality, and thorough evaluation of the proposed methods. The language used is very polite and respectful, with no negative or critical remarks. The reviewer appreciates the structure, writing quality, and performance of the paper, indicating a strong positive sentiment and high politeness.",100,100
"The paper describes unsupervised learning as a meta-learning problem: the observation is that unsupervised learning rules are effectively supervised by the quality of the representations that they yield relative to subsequent later semi-supervised (or RL) learning. The learning-to-learning algorithm allows for learning network architecture parameters, and also 'network-in-networks' that determine the unsupervised learning signal based on pre and post activations. 

Quality 
The proposed algorithm is well defined, and it is compared against relevant competing algorithms on relevant problems. 
The results show that the algorithm is competitive with other approaches like VAE (very slightly outperforms).

Clarity
The paper is well written and clearly structured. The section 5.4 is a bit hard to understand, with very very small images. 

Originality
There is an extensive literature on meta-learning, which is expanded upon in Appendix A. The main innovation in this work is the parametric update rule for outer loop updates, which does have some similarity to the old work by Bengio in 1990 and 1992. 

Significance
- pros clear and seemingly state-of-the-art results, intuitive approach, 
-cons only very modestly better than other methods. I would like to get a feel for why VAE is so good tbh (though the authors show that VAE has a problem with objective function mismatch).

One comment: the update rule takes as inputs pre and post activity and a backpropagated error; it seems natural to also use the local gradient of the neuron's transfer function here, as many three or four factor learning rules do. ","The sentiment of the review is generally positive, as the reviewer acknowledges the well-defined algorithm, its competitiveness, and the clear and structured writing of the paper. However, there are minor criticisms regarding the modest improvement over other methods and the difficulty in understanding section 5.4. Therefore, the sentiment score is 70. The politeness of the language is high, as the reviewer uses polite and constructive language throughout the review, even when pointing out areas for improvement. Therefore, the politeness score is 90.",70,90
"Language is hierarchically structured: smaller units (e.g., noun phrases) are nested within larger units (e.g., clauses). This is a strict hierarchy: when a larger constituent ends, all of the smaller constituents that are nested within it must also be closed. While the different units of an LSTM can learn to track information at different time scales, the standard architecture does not impose this sort of strict hierarchy. This paper proposes to add this constraint to the system by ordering the units; a vector of ""master"" input and forget gates ensures that when a given unit is reset all of the units that follow it in the ordering are also reset.

Strengths:
* The paper introduces an elegant way of adding a hierarchical inductive bias; the intuition behind this idea is explained clearly.
* The evaluation tasks are very sensible. It's good that the model is shown to obtain good perplexity and slightly improve over an LSTM baseline; it's not the state of the art, but that's not the point of the paper (in fact, I would emphasize that even more than the authors do). The unsupervised parse evaluation (Table 2) is the heart of the paper, in my opinion (and should probably be emphasized more) -- the results from the second layer are quite impressive.
* The (mildly) better performance than LSTMs on long-distance dependencies, and (mildly) worse performance on local dependencies, in the Marvin & Linzen dataset, is interesting (and merits additional analysis).

Weaknesses:
* The discussion of the motivation for unsupervised structure induction in the introduction is somewhat confused. I am not sure that neural networks with latent syntactic structures can really address the seemingly very fundamental question mentioned in the first paragraph (whether syntax is related to ""an underlying mechanism of human cognition"") - I would suggest eliminating this part. At the same time, the authors might want to add another motivation for studying architectures that discover latent structure (as opposed to being given that structure) - this setting corresponds more closely to human language acquisition, where children aren't given annotated parse trees.
* The authors discuss hierarchy in terms of syntactic structure alone, but it would seem to me that the hierarchy that the LSTM is inducing could just as well include topic shifts, speech acts and others, especially if the network is trained across sentences.
* There is limited analysis of the model. Why does the second layer show better unsupervised parsing performance than the third layer? (Could this be related to syntactic vs. semantic/discourse units I mention in the previous bullet?) Why is the model better at ADJP boundaries than NP boundaries? It would have been more useful to report less experiments but analyze the results of each experiment in greater depth.
* In this vein, I am not sure it's useful to include WSJ10 in Table 2, which is busy as it is. These sentences are clearly too easy, as the right branching baseline shows, and require additional POS tagging.
* I found it difficult to read Figure A.2: could you help us understand what we should take away from it? 
* It is not entirely clear why the model needs both unit-specific forget/input gates and the ""master"" forget/input gates, and there is no discussion of this issue. Have you tried using only the ""master"" gates?

Minor notes:
* RNNGs are described as having an explicit bias to model syntactic structure; this is an arguably confusing use of the word ""bias"", in that the architecture has a hard constraint enforcing syntactic structures (bias implies a soft constraint).
* There are some language issues: agreement errors (e.g. ""have"" in the sentence that starts with ""Developing"" in the introduction), typos (""A order should exist"", ""co-occurance""), determiner issues (""values in [the] master forget gate"", ""when the overlap exists"") - I would suggest going through and copy editing the paper.
* ""cummax"" seems like a better choice of name for cumulative maximum than ""cumax"".
* It may be helpful to remind the reader of the update equation for c_t in a standard LSTM.
* Did the language model have 1150 units in each layer or in total? Why did you use exactly three layers? Did you try one, two and four?
* It's not clear if the results in Table 2 reflect the best seed out of five (as the title of the column ""max"" indicates) or the average (as the caption says).
","The sentiment of the review is generally positive, as evidenced by the praise for the paper's elegant approach, clear explanation, sensible evaluation tasks, and impressive results in unsupervised parse evaluation. However, there are also several constructive criticisms and suggestions for improvement, which slightly temper the overall positivity. Therefore, the sentiment score is 60. The politeness of the language is high, as the reviewer uses polite and constructive language throughout, even when pointing out weaknesses and areas for improvement. The reviewer offers suggestions in a helpful manner and avoids any harsh or rude language. Therefore, the politeness score is 90.",60,90
"Review of ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness. 

In this submission, the authors provide evidence through clever image manipulations and psychophysical experiments that CNNs image recognition is strongly influenced by texture identification as opposed the global object shape (as opposed to humans). The authors attempt to address this problem by using image stylization to augment the training data. The resulting networks appear much more aligned with human judgements and less biased towards image textures.

If the authors address my major concerns, I would increasing my rating 1-2 points.

Major Comments:

The results of this paper are quite compelling and address some underlying challenges in the literature on how CNN's function. I particularly appreciated Figure 5 demonstrating how the resulting stylized-augmented networks more closely align with human judgements. Additionally, it is surprising to me how poor BagNet performs on Stylized-ImageNet (SIN) implying that ResNet-50 trained on Stylized ImageNet may be better perceptually aligned with global object structure. Very cool.

1. Please make sure to tone down the claims in your manuscript. Although I share enthusiasm for your results, please recognize that stating that your results are 'conclusive' is premature and not appropriate. (Conclusive requires more papers and much work by the larger scientific community for a hypothesis to become readily accepted). Some sentences of concern include:

  --> ""These experiments provide conclusive behavioural evidence in favour of the texture hypothesis""
  --> ""we conclude the following: Textures, not object shapes, are the most important cues for CNN object recognition.""

I would prefer to see language such as ""We provide evidence that textures provide a more powerful statistical signal then global object shape for CNNs."" or ""We provide evidence that CNNs are overly sensitive to textures in comparison to humans perceptual judgements"". This would be more measured and better reflect what has been accomplished in this study. Please do a thorough read of the rest of your manuscript and identify other text accordingly.

2. Domain shifts and data augmentation. I agree with your comment that domain shifts present the largest confound to Figure 2. The results of Geirhos et al, 2018 (Figure 4) indicate that individual image augmentations/distortions do not generalize well. Given these results, I would like to understand what image distortions were used in training each and all of your networks. Did you try a baseline with no image distortions (and/or just Stylized-ImageNet)?

Although the robustness in Figure 6 are great, how much of this can be attributed solely to Stylized-ImageNet versus the other types of image distortions/augmentations in each network. For instance, would contrast-insensitivity in Stylized-ImageNet diminish substantially if no contrast image distortion were used during training?

3. Semantics of 'object shape'. I suspect that others in the field of computer vision may take issue with your definition of 'object shape'. Please provide a crisp definition of what you test for as 'object shape' in each of your experiments (i.e. ""the convex outline of object segmentation"", etc.).

Minor Comments:

- Writing style in introduction. Rather then quoting phrases from individual papers, I would rather see you summarize their ideas in your own language and cite accordingly. This would demonstrate how you regard for their ideas and how these ideas fit together.

- Figure 2. Are people forced to select a choice or could they select 'I don't know'? Did you monitor response times to see if the manipulated images required longer times for individuals to pass decisions? I would expect that for some of the image manipulations that humans would have less confidence about their choices and that to be reflected in this study above and beyond an accuracy score.

- In your human studies, please provide some discussion about how you monitored performance to guard against human fatigue or lack of interest.

- Why did you use AdaIN instead of the original Gatys et al optimization method for image stylization? Was there some requirement/need for fast image stylization?

- Do you have any comment on the large variations in the results across class labels in Figure 4? Are there any easy explanations for this variation across class labels?

- Please use names of Shape-ResNet, etc. in Table 2.

- Are Pascal-VOC mAP results with fixed image features or did you fine-tune (back-propagate the errors to update the image features) during training? The latter would be particularly interesting as this would indicate that the resulting network features are better generic features as opposed to having used better data augmentation techniques.

- A.2. ""not not used in the experiment"" --> ""not used in the experiment""
","The sentiment of the review is generally positive, as indicated by phrases such as 'the results of this paper are quite compelling' and 'very cool.' The reviewer appreciates the authors' work and finds the results interesting and valuable. However, the reviewer also provides constructive criticism and suggestions for improvement, which is typical in a peer review process. The sentiment score is therefore 70. The politeness of the language used is high, as the reviewer uses polite language and phrases such as 'please' and 'I would prefer,' and provides constructive feedback in a respectful manner. The politeness score is 90.",70,90
"This paper present extensions of the Self-Attention Generative Adversarial Network approach SAGAN, leading to impressive images generations conditioned on imagenet classes. 
The key components of the approach are :
- increasing the batch size by a factor 8
- augmenting the width of the networks by 50% 
These first two elements result in an Inception score (IS) boost from 52 to 93.  
- the use of shared embeddings for the class conditioned batch norm layers, orthonormal regularization and hierarchical latent space bring an additional boost of IS 99.
The core novel element of the paper is the truncation trick: At train time, the input z is sampled from a normal distribution but at test time, a truncated normal distribution is used: when the magnitude of elements of z are above a certain threshold, they are re-sampled.
Variations of this threshold lead to variations in FD and IS, as shown in insightful experiments. The comments that more data helps (internal dataset experiments) is also informative. 
Very nice to have included negative results and detailed parameter sweeps.

This is a very nice work with impressive results, a great progress achievement in the field of image generation. 
Very well written.

Suggestions/questions: 
- it would be nice to also propose unconditioned experiments. 
It would be good to give an idea in the text of TPU-GPU equivalence in terms of feasibility of a standard GPU implementation - computation time it would involve. 
- I understand that no data augmentation was used during training?    
- clarification of the truncation trick: if the elements of z are re-sampled and are still above the threshold, are they re-sampled again and again until they are all below the given threshold?
- A sentence could be added to explain the truncation trick in the abstract directly since it is simple to understand and is key to the quality of the results.
- A reference to Appendix C could be given at the beginning of the Experiments section to help the reader find these details more easily.
- It would be nice to display more Nearest neighbors for the dog image.
- It would be nice to add a figure of random generations.
- make the bib uniform: remove unnecessary doi - url - cvpr page numbers
","The sentiment of the review is highly positive, as evidenced by phrases like 'impressive images generations,' 'insightful experiments,' 'very nice work with impressive results,' and 'a great progress achievement in the field of image generation.' The reviewer also praises the writing quality. Therefore, the sentiment score is 90. The politeness of the language is also very high, with the reviewer using polite suggestions and questions such as 'it would be nice to,' 'it would be good to,' and 'I understand that.' There are no rude or harsh comments, so the politeness score is 100.",90,100
"This paper introduces a novel feature selection method by utilizing GAN to learn the distributions. The novelty of this paper is to incorporate two recent works, i.e. knockoff for feature selection and W-GAN for generative models. Compared to the latest knockoff work which requires a known multivariate Gaussian distribution for the feature distribution, the proposed work is able to generate knockoffs for any distribution and without any prior knowledge of it.

Pros: This paper is very well written. I enjoyed reading this paper. It is novel and addresses an important problem. The numerical study clearly shows the advantage of the proposed work. 

Cons:

Q1: In the discriminator, instead of training with respect to the full loss, the authors consider to mask some information by using a multivariate Bernoulli random variable $B$ with success probability 0.9. Then the discriminator needs to predict only when $B_i = 0$. Can the authors provide some justification of such choice of the parameters? This choice is a little bit mysterious to me.

Q2: How sensitive are the hyper-parameters $\eta$ (set to 10 in the experiments), $\lambda$, and $\mu$ (set to 1 in the experiments)?

Q3: In the real data example, the feature selection performance is less justified as there is no truth. One suggestion is to evaluate the prediction errors using the selected features and compare with the benchmarks.



","The sentiment of the review is highly positive, as indicated by phrases like 'very well written,' 'enjoyed reading,' 'novel,' and 'addresses an important problem.' The reviewer also acknowledges the clear advantage shown in the numerical study. The politeness of the language is also very high, as the reviewer uses polite and constructive language throughout, even when pointing out areas for improvement. The questions are framed as requests for clarification or suggestions rather than criticisms.",90,95
"This paper deals with Memory Augmnted Neural Networks (MANN) and introduces an algorithm which allows full writes to the dense memory to be only exectued every L timesteps. The controller produces a hidden output at most timestps, whih is appended to a cache. Every L steps, soft attention is used to combine this cache of N hidden states to a single one, and then this is used as the input hidden state for the controller, with the outputs performing a write in the full memory M, along with clearing the cache.

The authors first derive ""Uniform Writing"" (UW) which updates the memory at regular intervals instead of every timestep. The derivation is based on the ""contribution"" which is norm of the gradient of some input timestep to some hidden state (potentially at a different timestep). I am not clear on whether this terminology for the quantity is novel, if this is the case maybe the authors should state this more clearly. UW says that if all timesteps are equally important, and only D writes can be made in a sequence of length T, then writes should be done every T/(D+1) steps. I have not checked the proof in detail but this seems reasonable that it would maximise the contribution quantity introduced. I am less clear on whether this is obviously the right thing to do - sometimes this value is referred to in relation to information, but that term does not strictly seem to be being used in the information theory sense (no mention of bits or nats anywhere). Regardless, as the authors point out, in real problems there are obviously timesteps which have less or no useful information, and clearly UW is mostly defined in order to build towards CUW.

CUW expands on UW by adding the cache of different hidden states, and using soft attention over them. This feels like a reasonable step, although I would presume there are times when the L hidden states were collected over timesteps with no information, and so the resulting write is not that useful, and times when all of hte L timesteps contain different useful information. In these circumstances it seems like the problem of getting the *useful* information into the memory is still present, as the single write done with the averaged hidden state will need to contain lots of information, which may be more ideal written with several timesteps.

The experiments are well described and overall the paper seems reproducable. The standard toy datasets of copy / reverse / sinusoid are used. The results are interesting - regular DNC with memory size 50 performs surprisingly badly on clean Sinusoid, my guess would be that with hyperparameter tuning this could be improved upon. I'm not sure that using exactly the same hyperparameters for a wide variety of models is appropriate - even with optimizers like Adam and RMSProp, I would want to see at least some sweeping for the best hyperparams, and then graphs like figure 3 should show error bars averaged across multiple runs with the best per-model hyperparameters. However, The DNC with CUW seems to perform well across all synthetic tasks.

There is no mention of Adaptive Computation Time/ACT (Graves, https://arxiv.org/abs/1603.08983) throughout the paper, which is surprising considering Alex Graves' models form two of the baselines used throughout the paper. ACT aims to execute an RNN a variable number of times, usually to do >1 timestep of processing for a single timestep of input. In the context of this paper, I believe it could be adapted to do either zero or one steps of computation per timestep, and that would yield a very comparable network where the LSTM controller always executes, and writes to the memory only happen sometimes. Given that it allows a learned process to decide whether to write, as opposed to having a fixed L which separates full writes, this should have the potential to outperform CUW, as it could learn that at certain times, writes must happen at every step. In my view ACT is attempting to solve essentially the same problem as this paper, so it should either be included as a baseline, or the manuscript should be updated to explain why this is not an appropriate comparison.


I think this is an interesting paper, trying to make progress on an important problem. The results look good, but I can only give a borderline score due to missing ACT numbers, and a few other unclear points. The addition of ACT experiments, and error bars on certain results, would change my mind here.


Notes:

""No solution has been proposed to help MANNs handle ultra long sequence"" - (Rae et al 2016) is an attempt to do this, by improving the complexity of reads / writes. This allows bigger memory and longer sequences to be processed.

""Current MANNS only support dense writing"" - presumably this means dense as in 'every timestep', but this terminology is overloaded - you could consider NTM / DNC as doing dense writing, and then work of Rae et al 2016 doing sparse writing.

In my experience training these kind of RNNs can have reasonably high variance across seeds - figures 2 & 3 should have error bars, and especially Table 4 as that contains the most important results. Getting 99 percent accuracy when previous SOTA is only 0.1% lower is only really meaningful if the standard deviation across seeds is very small.

Appendix A: the 'by induction' result - I believe there is an error, it should be:

h_t = \sigma_{i=1}^t U_{t-i}W x_i + C

As W is applied to inputs, before the repeated applications of U? I believe the rest of the derivation still holds the same, after the correction.

","The review starts with a neutral to slightly positive sentiment, acknowledging the paper's contribution and describing its content without strong praise or criticism. The reviewer expresses some confusion and raises several points for clarification, which slightly lowers the sentiment score. The review is constructive and provides detailed feedback, suggesting improvements and additional experiments. The language used is polite and professional throughout, with no instances of rudeness or harsh criticism.",20,80
"Summary:

This work is tackling two difficulties in current VB applied to DNNs (""Bayes by backprop""). First, MC approximations of intractable expectations are replaced by deterministic approximations. While this has been done before, the solution here is new and very interesting. Second, a Gaussian prior with length scales is learned by VB empirical Bayes alongside the normal training, which is also very useful.

The term ""fixing VB"" and some of the intro is not really supported by the rather weak experiments, done on small datasets and networks, where much older work like Barber&Bishop would apply without any problems. While interesting and potentially very useful novelties are presented, and the writing is excellent, both experiments and motivation can be improved.

- Quality: Extremely well written paper, I learned a lot from it. Approximations are
   tested, great figures to explain things. And the major technical novelty, the
   expression for <h_j h_l>, is really interesting and useful.
- Clarity: Excellent writing until it comes to the experiments. Here, important
   details are just missing, for example what q(w) is (fully factorized Gaussian?).
   Very nice literature review, also historical.
- Originality: The idea of matching Gaussian moments along the network graph is
   previously done in PBP (Lobato, Adams), as acknowledged here. Porting this from
   ADF to VB gives dDVI. PBP also has the property that a DL system gives you the
   gradients. Having said that, I think dDVI may be more useful than PBP.
   While Barber&BIshop 98 is cited, they miss the expression for <h_j h_l> in
   there. Now, what is done here, is more elegant, does not need 1D quadrature.
- Significance: Judging from the existing experiments, the significance may be
   rather small, *if one only looks at test log likelihood*. I'd still give this the
   benefit of the doubt, as in particular dDVI could be really interesting at large
   scale as well. But the authors may tone down their language a bit.
   To increase significance, I recommend to comment beyond just test log
   likelihood scores. For example:
   - Does the optimization become simpler, less tuning required, more automatic?
      Would one not expect so, given you make a big point out of reducing variance?
      Does it converge faster?
   - Can you do something with your posterior that normal DNN methods cannot
      do? Better decisions (bandits, active learning, HPO)? Continual learning?
      In the end, who really cares about test log likelihood?

Experiments:
- What is the q(w) family being used here? Fully factorized Gaussian? I
   suppose so for dDVI. But for DVI? Not said anywhere, in main paper or
   Appendix
- A bit disappointing. Why not evaluate at least dDVI with diagonal q(w) on
   some much larger models and datasets? Why not quote numbers on speed
   and robustness of learning, etc? Show what you really gain by reducing the
   variance.
- Experiments are OK, but on pretty small datasets, and for single hidden
   layer NNs. On such data and models, the Barber&Bishop 98 method could
   be run as well
- Was MCVI run with re-parameterization? This is really important. If not,
   this would be an important missing comparison. Please be clear in the main
   text
- Advantages over MCVI are not very large. At least, dDVI should be faster to
   converge than MCVI.
   Can you say something about robustness of training? Is it easier to train
   dDVI than MCVI?
- Why not show the PBP-1 results, comparing to dDVI, in the main text? Are they
   obtained with the same model? dDVI is doing better.

Other points:
- Please acknowledge the <h_j h_l> expression in Barber&Bishop 98. Yours is
   more elegant and faster (does not need 1D quadrature)
- Relation to PBP: Note that dDVI has an advantage in practice. With PBP, I need
   to compute gradients for every datapoint. In dDVI, I can do mini-batch
   updates.
- I just *love* the header ""Wild approximations"". I tend to refer to this kind of work
   as ""weak analogies"". Why do you not also compare against this, and show it really
   does not work?
","The review starts with a positive sentiment, appreciating the novelty and usefulness of the work. However, it also points out significant weaknesses in the experiments and motivation, suggesting areas for improvement. The language used is constructive and polite, offering specific recommendations without being harsh or dismissive. The reviewer acknowledges the strengths of the paper while providing detailed feedback on how it can be improved, which indicates a balanced and respectful approach.",50,80
"
Given the growing interest in building trust worthy and privacy protecting AI systems, this paper demonstrates a novel approach to achieve these important goals by allowing a trusted, but slow, computation engine to leverage a fast but untrusted computation engine. For the sake of protecting privacy, this is done by establishing an additive secret share such that evaluation on one part of the share is performed offline and the computation on the other part of the share is performed on the untrusted engine. To verify the correctness of the computation on the untrusted server, a randomized algorithm is used to sample the correctness of the results. Using these techniques, the authors demonstrate an order of magnitude speedup compared to running only on the trusted engine and 3-4 orders of magnitude speedup compared to software-based solutions.

Overall this is a strong paper which presents good ideas that have influence in ML and beyond. I appreciate the fact that the authors are planning to make their code publicly available which makes it more reproducible. Below are a few comments/questions/suggestions 

1.	This papers, and other papers too, propose mechanisms to protect the privacy of the data while outsourcing the computation on a prediction task. However, an alternative approach would be to bring the computation to the data, which means performing the prediction on the client side. In what sense is it better to outsource the computation? Note that outsourcing the computation requires both complexity on the server side and additional computation on the client side (encryption & decryption).
2.	You present the limitations of the trust model of SGX only in the appendix while in the paper you compare to other techniques such as Gazzelle which have a different trust model and assumption. It makes sense to, at least, hint the reader on these differences. 
3.	In section 2.2: “has to be processed with high throughput when available” is it high throughput that is required or low latency?
4.	In Section 4.3: in one of the VGG experiment you computed only the convolution layers which, as you say, are commonly used to generate features. In this case, however, doesn’t it make more sense that the feature generation will take place on the client side while only the upper layers (dense layers) will be outsourced?
5.	In section 4.3 “Private Inference” : do you include in the time reported also the offline preprocessing time? As far as I understand this should take the same amount of time as computing on the TEE.
","The sentiment of the review is positive, as indicated by phrases like 'Overall this is a strong paper' and 'good ideas that have influence in ML and beyond.' The reviewer appreciates the authors' intention to make their code publicly available, which adds to the positive sentiment. Therefore, the sentiment score is 80. The politeness of the language is also high, as the reviewer uses polite and constructive language throughout the review, such as 'I appreciate' and 'Below are a few comments/questions/suggestions.' The reviewer provides specific, constructive feedback without any negative or rude remarks, so the politeness score is 90.",80,90
"This paper studies auto-encoders under several assumptions: (a) the auto-encoder's layers are fully connected, with random weights, (b) the auto-encoder is weight-tied, (c) the dimensions of the layers go to infinity with fixed ratios. The main contribution of the paper is to point out that this model of random autoencoder can be elegantly and rigorously analysed with one-dimensional equations. The idea is original and will probably lead to new directions of research. Already the first applications that the paper suggests are exciting.

The paper does a good job in justifying assumptions (a), (b) and (c) in the introduction. It is convincing in the fact that this point of view may bring practical insights on training initialization for real-world autoencoders. Thus my opinion is that this paper brings original and significant ideas in the field.

One flaw of this paper is that the writing might be clearer. For instance when presenting the technical theorem (Theorem 1), it would be useful to have an intuitive explanation for the theorem and the state-evolution-like equations. However, I believe that there are some easy fixes that would greatly improve the clarity of the exposition. Here is a list of suggestions: 

- In Section 2.1, a large number of notations are introduced. It would help a lot if the authors made a graphical representation of these. For instance, a diagram where every linearity / non-linearity is a box, and the different variables $x_l$, $\hat{x}_l$ appear would help a lot. 

- Section 2.2 is rather technical. The authors could try to give some more intuition of what's happening. For instance, they could spend more time after the theorem explaining what $\tau_l, \gamma_l$ and $\rho_l$ mean. They could also introduce the notation S_sig and S_var early and this section (and not in Section 3), because it helps interpreting the parameters. It would also help if they could write a heuristic derivation of the state-evolution-like equations. From the paper, the only way the reader can understand the intuition behind those complicated equations is to look at the proof of Theorem 1 (which is rather technical). 

- In Section 3.1, I did not understand the difference between interpretations 1 and 2. Could the authors clarify? 

- In Section 3.4, I did not understand the sentence: ""In particular, near the phase transition of \gamma, S_sig/S_var = \Omega(\beta^{1.5}"". If one uses the \Omega notation, it means that some parameter is converging to something. What is the parameter? As a consequence, I did not understand this paragraph. 

- In Section 3.5, the authors should make clear from the beginning why they are running those specific simulations. What hypothesis are they trying to check? I finally concluded that they are running simulations to check if the hypothesis they make in the first paragraph are true. They also want to compare with some other criteria in the literature, named EOC, that also gives insights about the trainability of the network. However, they could explicitly say in the beginning of the second paragraph that this is the goal.

- In a similar spirit, the authors should end Section 3.5 with a clear conclusion on whether or not the framework enables us to predict the trainability of the autoencoder. 



Minor edits / remarks: 

- Typo: last but one paragraph of the introduction: ""whose analysis is typically more straighforwards"" -> ""straightforward"".

- At the end of Section 3.2: what can be proved about the behavior of \gamma / \sqrt{\rho}? It is obviously a central quantity and the authors do not say what happens in the phases where \gamma and \rho go to infinity for instance. Is it because it is hard to analyse?

","The sentiment of the review is quite positive, as the reviewer appreciates the originality and significance of the paper's contributions. They also find the suggested applications exciting and believe the paper brings valuable insights to the field. However, the reviewer points out that the writing could be clearer and provides specific suggestions for improvement. The politeness of the language is high, as the reviewer uses constructive and respectful language throughout the review, offering detailed and helpful feedback without being harsh or dismissive.",80,90
"Post-rebuttal revision: All my concerns were adressed by the authors. This is a great paper and should be accepted.

------

The paper presents smoothing probabilistic box embeddings with softplus functions, which make the optimization landscape continuous, while also presenting the theoretical background of the proposed method well. The paper presents the overall idea beautifully and is very easy to follow. The overall idea of smoothed sotfplus boxes is well-founded, elegant and practical. The results on standard WordNet do not improve upon state-of-the-art, however imbalanced WordNet with abundance of negative examples gain remarkable improvements. Similarly in Flickr and MovieLens the method performs well. This paper presents a novel, theoretically well-justified idea with excellent results, and is likely going to be a high-impact paper. 

An illustrating figure would still be nice to include, also for the convolutions of eq 2. The paper does not comment on running times, some kind of scalability comparison should be included since the paper claims that the model is easier to train.

The paper should clarify that the \prod in 3.3. meet and join definitions seems to refer to a set product, while the p(a) equation has a standard product (or does it?). What is the “a” in the p(a), should it be ""p(x)” ? 

I have trouble understanding eq 1: the difference inside the function is always negative, while the hinge function seems to clip negative values away. The definition of the m(x) is too clever, please clarify the function in more conventional notation.  ","The sentiment of the review is highly positive, as indicated by phrases such as 'great paper,' 'beautifully,' 'elegant and practical,' 'excellent results,' and 'high-impact paper.' The reviewer clearly appreciates the work and believes it should be accepted. Therefore, the sentiment score is 100. The politeness of the language is also very high. The reviewer uses polite and constructive language throughout, even when making suggestions for improvement, such as 'An illustrating figure would still be nice to include' and 'please clarify.' Therefore, the politeness score is 100.",100,100

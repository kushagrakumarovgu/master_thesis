title,decision,year,Authors,rating_score,rating_text,review,confidence_score,confidence_text,academic_age,current_age,total_num_pub,total_num_conference,total_num_informal,total_num_journal,reasoning,sentiment_score,politeness_score
A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks,Accept,2017,"['Dan Hendrycks', 'Kevin Gimpel']","[6, 6, 6]","['Marginally above acceptance threshold', 'Marginally above acceptance threshold', 'Marginally above acceptance threshold']","The paper address the problem of detecting if an example is misclassified or out-of-distribution. This is an very important topic and the study provides a good baseline. Although it misses strong novel methods for the task, the study contributes to the community.","['3', '3', '3']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[2, 10]","[8, 16]","[64, 161]","[25, 90]","[37, 66]","[2, 5]","The sentiment score is 50 (slightly positive) because the reviewer acknowledges the importance of the topic and states that the study provides a good baseline and contributes to the community. However, it's not overwhelmingly positive as the reviewer notes that it 'misses strong novel methods for the task'. The politeness score is 50 (moderately polite) because the language used is professional and respectful. The reviewer offers a balanced view, highlighting both strengths and limitations without using harsh or critical language. The tone is constructive and appreciative of the work's contribution, even while pointing out areas for improvement.",50,50
A Compare-Aggregate Model for Matching Text Sequences,Accept,2017,"['Shuohang Wang', 'Jing Jiang']","[6, 7, 8]","['Marginally above acceptance threshold', 'Good paper, accept', 'Top 50% of accepted papers, clear accept']","This paper proposed a compare-aggregate model for the NLP tasks that require semantically comparing the text sequences, such as question answering and textual entailment. The basic framework of this model is to apply a convolutional neural network (aggregation) after a element-wise operation (comparison) over the attentive outputs of the LSTMs. The highlighted part is the comparison, where this paper compares several different methods for matching text sequences, and the element-wise subtraction/multiplication operations are demonstrated to achieve generally better performance on four different datasets. While the weak point is that this is an incremental work and a bit lack of innovation. A qualitative evaluation about how subtraction, multiplication and other comparison functions perform on varied kinds of sentences would be more interesting.","['4', '5', '5']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[3, 13]","[9, 19]","[94, 145]","[43, 105]","[50, 29]","[1, 11]","The sentiment score is slightly positive (20) because the reviewer acknowledges the paper's contributions and its performance on multiple datasets. However, they also point out that it's an incremental work lacking innovation, which tempers the positivity. The politeness score is moderately positive (50) as the reviewer uses neutral language and offers constructive criticism without harsh words. They balance positive aspects ('better performance') with areas for improvement ('lack of innovation') in a professional manner. The reviewer also suggests additional analysis that would be 'interesting', which is a polite way of recommending improvements.",20,50
A Compositional Object-Based Approach to Learning Physical Dynamics,Accept,2017,"['Michael Chang', 'Tomer Ullman', 'Antonio Torralba', 'Joshua Tenenbaum']","[9, 7, 6]","['9', 'Good paper, accept', 'Marginally above acceptance threshold']","Summary === This paper proposes the Neural Physics Engine (NPE), a network architecture which simulates object interactions. While NPE decides to explicitly represent objects (rather than video frames), it incorporates knowledge of physics almost exclusively through training data. It is tested in a toy domain with bouncing 2d balls. The proposed architecture processes each object in a scene one at a time. Pairs of objects are embedded in a common space where the effect of the objects on each other can be represented. These embeddings are summed and combined with the focus object*s state to predict the focus object*s change in velocity. Alternative baselines are presented which either forego the pairwise embedding for a single object embedding or encode a focus object*s neighbors in a sequence of LSTM states. NPE outperforms the baselines dramatically, showing the importance of architecture choices in learning to do this object based simulation. The model is tested in multiple ways. Ability to predict object trajectory over long time spans is measured. Generalization to different numbers of objects is measured. Generalization to slightly altered environments (difference shaped walls) is measured. Finally, the NPE is also trained to predict object mass using only interactions with other objects, where it also outperforms baselines. Comments === * I have one more clarifying question. Are the inputs to the blue box in figure 3 (b)/(c) the concatenation of the summed embeddings and state vector of object 3? Or is the input to the blue module some other combination of the two vectors? * Section 2.1 begins with *First, because physics does not change across inertial frames, it suffices to separately predict the future state of each object conditioned on the past states of itself and the other objects in its neighborhood, similar to Fragkiadaki et al. (2015).* I think this is an argument to forego the visual representation used by previous work in favor of an object only representation. This would be more clear if there were contrast with a visual representation. * As addressed in the paper, this approach is novel, though less so after taking into consideration the concurrent work of Battaglia et. al. in NIPS 2016 titled *Interaction Networks for Learning about Objects, Relations and Physics.* This work offers a different network architecture and set of experiments, as well as great presentation, but the use of an object based representation for learning to predict physical behavior is shared. Overall Evaluation === This paper was a pleasure to read and provided many experiments that offered clear and interesting conclusions. It offers a novel approach (though less so compared to the concurrent work of Battaglia et. al. 2016) which represents a significant step forward in the current investigation of intuitive physics.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[2, 9, 19, 24]","[8, 15, 25, 30]","[19, 39, 391, 610]","[8, 28, 204, 353]","[11, 8, 145, 226]","[0, 3, 42, 31]","The sentiment score is 80 (positive) because the reviewer expresses clear appreciation for the paper, calling it 'a pleasure to read' and stating that it provides 'clear and interesting conclusions' and 'represents a significant step forward'. The reviewer also praises the paper's presentation and experiments. The only slight negative is the mention of concurrent work reducing the novelty somewhat, but this is presented as a minor point. The politeness score is 70 (polite) because the reviewer uses respectful and appreciative language throughout, such as 'This paper was a pleasure to read'. The reviewer also frames their comments and questions in a constructive manner, without harsh criticism. The tone is professional and courteous, though not excessively formal or deferential, hence the score of 70 rather than higher.",80,70
A Learned Representation For Artistic Style,Accept,2017,"['Vincent Dumoulin', 'Jonathon Shlens', 'Manjunath Kudlur']","[7, 8, 8]","['Good paper, accept', 'Top 50% of accepted papers, clear accept', 'Top 50% of accepted papers, clear accept']","This paper addresses the problem of efficient neural stylization. Instead of training a separate network for N different styles (as is done, e.g., in Johnson et al.), this paper extends the instance normalization work of Ulyanov et al. to train a single network and learn a smaller set “conditional instance normalization” parameters dependent on the desired output style. The conditional instance normalization applies a learnt affine transformation on normalized feature maps at each layer in the network. Qualitative results are shown. I have not worked in this area, but I’m generally aware of the main issues in transferring artistic style. The paper addresses a known challenge of incorporating different styles into the same net, which have a number of practical benefits. As far as I can tell the results look compelling. As I’m less confident in my expertise in this area, I’m happy to support another reviewer who is willing to champion this paper. My main comments are on the paper writing. As far as I understand, the main novelty of the approach starts in Section 2.2, and before that is review of prior art. If this is indeed the case, one suggestion is to remove the subsection heading for 2.1 so it’s grouped with the first part of Section 2, and to cite related work for the feedforward network (e.g., Johnson et al.) in the text and in Fig 2 so it’s clear. In fact, I’m wondering if Figs 2 and 3 can be combined somehow so that the contribution is clearer in the figures. I was at first confused by Eq (5) as x and z are not defined anywhere. Also, it may be helpful to write out everything explicitly as is done in the instance normalization paper. In Eq (4), perhaps you could write T_s to emphasize that there are separate networks for different styles. Fig 5 left: I’m assuming the different colors correspond to the different styles. If so, perhaps mention this in the caption. Also, this figure is hard to read. Maybe instead show single curves with error bars that are averages over the loss curves for N-styles and individual styles. Typos: Page 1: Shouldn’t it be “VGG-16” network (not *VGG-19”)? Page 2: “newtork” => “network”. Paragraph after Eq. (5): “much less” => “fewer”.","['3', '5', '5']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[5, 14]","[11, 15]","[41, 29]","[15, 23]","[24, 5]","[2, 1]","The sentiment score is 50 (moderately positive) because the reviewer expresses a generally favorable view of the paper, noting that it addresses a known challenge and that the results look compelling. However, they also mention not being an expert in the area and defer to other reviewers, which tempers the positivity somewhat. The politeness score is 80 (quite polite) because the reviewer uses respectful language throughout, offers constructive suggestions for improvement, and frames their comments in a helpful manner. They also acknowledge their own limitations in expertise, which is a polite way to qualify their feedback. The review provides specific, actionable feedback without harsh criticism, maintaining a professional and courteous tone throughout.",50,80
A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING,Accept,2017,"['Zhouhan Lin', 'Minwei Feng', 'Cicero Nogueira dos Santos', 'Mo Yu', 'Bing Xiang', 'Bowen Zhou', 'Yoshua Bengio']","[6, 5, 8]","['Marginally above acceptance threshold', '5', 'Top 50% of accepted papers, clear accept']","I like the idea in this paper that use not just one but multiple attentional vectors to extract multiple representations for a sentence. The authors have demonstrated consistent gains across three different tasks Age, Yelp, & SNLI. However, I*d like to see more analysis on the 2D representations (as concerned by another reviewer) to be convinced. Specifically, r=30 seems to be a pretty large value when applying to short sentences like tweets or those in the SNLI dataset. I*d like to see the effect of varying r from small to large value. With large r value, I suspect your models might have an advantage in having a much larger number of parameters (specifically in the supervised components) compare to other models. To make it transparent, the model sizes should be reported. I*d also like to see performances on the dev sets or learning curves. In the conclusion, the authors remark that *attention mechanism reliefs the burden of LSTM*. If the 2D representations are effective in that aspect, I*d expect that the authors might be able to train with a smaller LSTM. Testing the effect of LSTM dimension vs will be helpful. Lastly, there is a problem in the presentation of the paper in which there is no training objective defined. Readers have to read until the experimental sections to guess that the authors perform supervised learning and back-prop through the self-attention mechanism as well as the LSTM. * Minor comments: Typos: netowkrs, toghter, performd Missing year for the citation of (Margarit & Subramaniam) In figure 3, attention plotswith and without penalization look similar.","['5', '4', '4']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[6, 10, 13, 9, 19, 18, 30]","[12, 16, 18, 15, 25, 24, 36]","[59, 28, 90, 187, 154, 250, 975]","[24, 22, 53, 86, 84, 136, 405]","[31, 6, 33, 93, 65, 73, 454]","[4, 0, 4, 8, 5, 41, 116]","The sentiment score is slightly positive (20) because the reviewer expresses liking the core idea and acknowledges consistent gains across tasks, but also raises several concerns and requests for additional analysis. This mix of positive and critical feedback results in a mildly positive overall sentiment. The politeness score is moderately positive (50) as the reviewer uses polite language like 'I'd like to see' and 'I suspect', avoiding harsh criticism. They offer constructive suggestions rather than blunt criticisms. The tone is professional and respectful throughout, though not overly formal or deferential.",20,50
A Simple but Tough-to-Beat Baseline for Sentence Embeddings,Accept,2017,"['Sanjeev Arora', 'Yingyu Liang', 'Tengyu Ma']","[7, 7, 8]","['Good paper, accept', 'Good paper, accept', 'Top 50% of accepted papers, clear accept']","This is a good paper with an interesting probabilistic motivation for weighted bag of words models. The (hopefully soon) added comparison to Wang and Manning will make it stronger. Though it is sad that for sufficiently large datasets, NB-SVM still works better. In the second to last paragraph of the introduction you describe a problem of large cooccurrence counts which was already fixed by the Glove embeddings with their weighting function f. Minor comments: *The capturing the similarities* -- typo in line 2 of intro. *Recently, (Wieting et al.,2016) learned* -- use citet instead of parenthesized citation","['4', '4', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[28, 10, 7]","[34, 16, 13]","[217, 121, 181]","[103, 52, 76]","[76, 54, 97]","[38, 15, 8]","The sentiment score is 50 (moderately positive) because the reviewer describes it as a 'good paper with an interesting probabilistic motivation' and mentions that the comparison to Wang and Manning will make it stronger. However, they also express disappointment that NB-SVM still works better for large datasets, which tempers the positivity. The politeness score is 60 (fairly polite) because the reviewer uses respectful language throughout, offering constructive feedback and suggestions. They point out typos and citation issues in a helpful manner without being harsh. The use of phrases like 'hopefully soon' and describing issues as 'minor comments' contributes to the polite tone. The reviewer balances praise with constructive criticism in a professional manner.",50,60
A recurrent neural network without chaos,Accept,2017,"['Thomas Laurent', 'James von Brecht']","[8, 7, 7]","['Top 50% of accepted papers, clear accept', 'Good paper, accept', 'Good paper, accept']","The authors of the paper set out to answer the question whether chaotic behaviour is a necessary ingredient for RNNs to perform well on some tasks. For that question*s sake, they propose an architecture which is designed to not have chaos. The subsequent experiments validate the claim that chaos is not necessary. This paper is refreshing. Instead of proposing another incremental improvement, the authors start out with a clear hypothesis and test it. This might set the base for future design principles of RNNs. The only downside is that the experiments are only conducted on tasks which are known to be not that demanding from a dynamical systems perspective; it would have been nice if the authors had traversed the set of data sets more to find data where chaos is actually necessary.","['3', '4', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[12, 8]","[18, 14]","[39, 22]","[10, 7]","[23, 9]","[6, 6]","The sentiment score is 80 (positive) because the reviewer describes the paper as 'refreshing' and praises its clear hypothesis and testing approach. They mention it might set the base for future design principles, which is highly positive. The only criticism is minor, regarding the limited scope of experiments. The politeness score is 70 (polite) as the reviewer uses respectful language throughout, acknowledging the authors' efforts and contributions. They offer constructive feedback without harsh criticism. The phrasing 'it would have been nice' for the suggestion is particularly polite. The overall tone is professional and appreciative, though not excessively formal or deferential, hence not a perfect 100.",80,70
Adversarial Feature Learning,Accept,2017,"['Jeff Donahue', 'Philipp Krähenbühl', 'Trevor Darrell']","[7, 7, 7]","['Good paper, accept', 'Good paper, accept', 'Good paper, accept']","This paper provides an interesting idea, which extends GAN by taking into account bidirectional network. Totally, the paper is well-written, and easy to follow what is contribution of this paper. From the theoretical parts, the proposed method, BiGAN, inherits similar properties in GAN. The experimental results show that BiGAN is competitive with other methods. A drawback would a non-convex optimization problem in BiGAN, this paper is still suitable to be accepted in my opinion.","['3', '3', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[7, 17, 14, -1, 17]","[13, 23, 20, 5, 23]","[7, 216, 155, 12, 151]","[6, 123, 63, 6, 68]","[1, 37, 50, 4, 63]","[0, 56, 42, 2, 20]","The sentiment score is 70 (positive) because the reviewer describes the paper as 'interesting' and 'well-written', and states it is 'suitable to be accepted'. They mention positive aspects like the paper being easy to follow and the experimental results being competitive. The only negative point is a minor drawback mentioned. The politeness score is 60 (polite) as the reviewer uses respectful language throughout, acknowledging the paper's contributions and merits. They offer a balanced view, mentioning both strengths and a potential weakness, without using harsh or critical language. The tone is professional and constructive, which is indicative of polite academic discourse.",70,60
Adversarial Machine Learning at Scale,Accept,2017,"['Alexey Kurakin', 'Ian J. Goodfellow', 'Samy Bengio']","[7, 6, 6]","['Good paper, accept', 'Marginally above acceptance threshold', 'Marginally above acceptance threshold']","This paper has two main contributions: (1) Applying adversarial training to imagenet, a larger dataset than previously considered (2) Comparing different adversarial training approaches, focusing importantly on the transferability of different methods. The authors also uncover and explain the label leaking effect which is an important contribution. This paper is clear, well written and does a good job of assessing and comparing adversarial training methods and understanding their relation to one another. A wide range of empirical results are shown which helps elucidate the adversarial training procedure. This paper makes an important contribution towards understand adversarial training and believe ICLR is an appropriate venue for this work.","['3', '4', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[7, 9, 31]","[13, 12, 37]","[37, 107, 261]","[13, 48, 155]","[22, 55, 60]","[2, 4, 46]","The sentiment score is 90 because the review is overwhelmingly positive. The reviewer praises the paper's contributions, clarity, and importance. They use phrases like 'important contribution,' 'clear, well written,' and 'does a good job,' indicating strong approval. The reviewer also states that ICLR is an appropriate venue, further supporting their positive view. The politeness score is 70 because while the language is professional and respectful, it doesn't go out of its way to be overly polite. The reviewer focuses on the paper's merits without using excessively flattering language. The tone is constructive and appreciative, but maintains a professional distance typical of academic reviews.",90,70
Adversarial Training Methods for Semi-Supervised Text Classification,Accept,2017,"['Takeru Miyato', 'Andrew M. Dai', 'Ian Goodfellow']","[6, 7, 7]","['Marginally above acceptance threshold', 'Good paper, accept', 'Good paper, accept']","The authors propose to apply virtual adversarial training to semi-supervised classification. It is quite hard to assess the novelty on the algorithmic side at this stage: there is a huge available literature on semi-supervised learning (especially SVM-related literature, but some work were applied to neural networks too); unfortunately the authors do not mention it, nor relate their approach to it, and stick to the adversarial world. In terms of novelty on the adversarial side, the authors propose to add perturbations at the level of words embeddings, rather than the input itself (having in mind applications to NLP). Concerning the experimental section, authors focus on text classification methods. Again, comparison with the existing SVM-related literature is important to assess the viability of the proposed approach; for example (Wang et al, 2012) report 8.8% on IMBD with a very simple linear SVM (without transductive setup). Overall, the paper reads well and propose a semi-supervised learning algorithm which is shown to work in practice. Theoretical and experimental comparison with past work is missing.","['4', '3', '5']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[2, 7, 9]","[8, 13, 12]","[24, 74, 107]","[9, 28, 48]","[14, 42, 55]","[1, 4, 4]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges that the paper 'reads well' and proposes an algorithm 'shown to work in practice', they also point out significant shortcomings. The reviewer notes a lack of comparison with existing literature, missing theoretical and experimental comparisons, and difficulty in assessing novelty. These criticisms outweigh the positive aspects, resulting in a slightly negative overall sentiment. The politeness score is moderately positive (50) as the reviewer uses neutral, professional language throughout. They offer constructive criticism without harsh or rude phrasing, maintaining a respectful tone while pointing out areas for improvement. The reviewer balances critique with acknowledgment of the paper's strengths, which contributes to the polite tone.",-20,50
Adversarially Learned Inference,Accept,2017,"['Vincent Dumoulin', 'Ishmael Belghazi', 'Ben Poole', 'Alex Lamb', 'Martin Arjovsky', 'Olivier Mastropietro', 'Aaron Courville']","[7, 7, 8]","['Good paper, accept', 'Good paper, accept', 'Top 50% of accepted papers, clear accept']",This is a parallel work with BiGAN. The idea is using auto encoder to provide extra information for discriminator. This approach seems is promising from reported result.,"['3', '4', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[-3, 14]","[2, 20]","[2, 131]","[1, 53]","[1, 37]","[0, 41]","The sentiment score is 50 (moderately positive) because the reviewer describes the work as 'promising' and mentions that the reported results support this view. The tone is generally positive without being overly enthusiastic. The politeness score is 0 (neutral) because the language used is straightforward and factual, without any particularly polite or impolite phrasing. The reviewer simply states their observations about the work without using any courteous expressions or harsh criticisms.",50,0
An Actor-Critic Algorithm for Sequence Prediction,Accept,2017,"['Dzmitry Bahdanau', 'Philemon Brakel', 'Kelvin Xu', 'Anirudh Goyal', 'Ryan Lowe', 'Joelle Pineau', 'Aaron Courville', 'Yoshua Bengio']","[4, 8, 8]","['Ok but not good enough - rejection', 'Top 50% of accepted papers, clear accept', 'Top 50% of accepted papers, clear accept']","This paper proposes to use an actor-critic RL technique to train sequence to sequence tasks in natural language processing. In particular, experiments are shown in a synthetic denoising task as well as in machine translation. I like the idea of the paper, however, the experimental evaluation is not convincing. Why is the LL numbers in Ranzato et al. 2015 and your paper so different? Is the metric different? is it the scheduler? are the parameters different? If one extrapolates the numbers, it seems that MIXER will be much better than the proposed approach. I*d like to see a head-to-head comparison, either by reproducing the same setting or by running the mixer baseline. The authors should also compare their results to the state-of-the-art. How good is their machine translation system? Only comparing to a single baseline and without reproducing the numbers is not sufficient. While the idea makes sense, the authors needed to use many heuristics to make the model to work, e.g., using a delayed actor, update phi* with interpolation, penalize the variance, reducing the value of rare actions, etc. Furthermore, there is no in depth analysis of how much performance each of these heuristics brings. It seems that the authors need more work to make the model work without so many heuristics. The authors also mentioned several optimization difficulties (some of which are non-intuitive), 1) why does the critic assign very high value to actions with very low probability according to the actor? 2) why is a lower square error on Q resulting in much worst performance? The paper will benefit from a serious re-write. The technical part is not clearly written. The manuscript also assumes that the reader knows algorithms such as REINFORCE. I strongly suggest to include a brief description in the text. This will help the reader understand how to use the critic within this framework. Also the experimental section will benefit from dividing it by experiment. Right now is cumbersome to look at the details of each experiment as things are mixed up in the text. The paper criticizes the REINFORCE algorithm a lot, particularly for its high variance, however the best results in the real setting are achieved with this algorithm (+ the critic). How do you explain this? The text is also not consistent with what the results show. The discussion claims that using the critic on REINFORCE reduces the gap with the actor critic. However, it is better than the proposed approach. I*ll revise my score if the authors address my questions. In summary, an interesting idea, however many heuristics are used and the experimental evaluation is not sufficient.","['4', '5', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[4, 7, 3, 4, 6, 18, 17, 30]","[10, 12, 8, 10, 11, 24, 23, 36]","[55, 32, 29, 101, 44, 308, 309, 975]","[22, 15, 13, 40, 20, 151, 135, 405]","[33, 14, 15, 60, 19, 119, 160, 454]","[0, 3, 1, 1, 5, 38, 14, 116]","The sentiment score is -50 because while the reviewer likes the idea of the paper, they express significant concerns about the experimental evaluation, the use of heuristics, and the lack of comparison to state-of-the-art results. The review is more negative than positive, but not entirely dismissive. The politeness score is 20 because the reviewer uses generally polite language, acknowledging the interesting aspects of the work, and offering constructive criticism. They use phrases like 'I like the idea' and 'The paper will benefit from', which are polite ways to give feedback. However, the criticism is direct and the reviewer doesn't use many softening phrases, keeping the politeness score relatively low on the positive side.",-50,20
An Information-Theoretic Framework for Fast and Robust Unsupervised Learning via Neural Population Infomax,Accept,2017,"['Wentao Huang', 'Kechen Zhang']",[7],"['Good paper, accept']","This is an 18 page paper plus appendix which presents a mathematical derivation for infomax for an actual neural population with noise. The original Bell & Sejnowski infomax framework only considered the no noise case. Results are shown for natural image patches and the mnist dataset, which qualitatively resemble results obtained with other methods. This seems like an interesting and potentially more general approach to unsupervised learning. However the paper is quite long and it was difficult for me to follow all the twists and turns. For example the introduction of the hierarchical model was confusing and it took several iterations to understand where this was going. *Hierarchical* is probably not the right terminology here because it*s not like a deep net hierarchy, it*s just decomposing the tuning curve function into different parts. I would recommend that the authors try to condense the paper so that the central message and important steps are conveyed in short order, and then put the more complete mathematical development into a supplementary document. Also, the authors should look at the work of Karklin & Simoncelli 2011 which is highly related. They also use an infomax framework for a noisy neural population to derive on and off cells in the retina, and they show the conditions under which orientation selectivity emerges.",['3'],['The reviewer is fairly confident that the evaluation is correct'],"[15, 25]","[21, 30]","[131, 25]","[38, 4]","[17, 6]","[76, 15]","The sentiment score is 50 (slightly positive) because the reviewer acknowledges the paper as 'interesting and potentially more general approach to unsupervised learning'. However, they also express difficulties in following the paper and suggest condensing it, indicating mixed feelings. The politeness score is 70 (fairly polite) as the reviewer uses respectful language throughout, offering constructive criticism and suggestions rather than harsh critiques. They use phrases like 'I would recommend' and acknowledge the paper's potential value, while tactfully pointing out areas for improvement.",50,70
"Attend, Adapt and Transfer: Attentive Deep Architecture for Adaptive Transfer from multiple sources in the same domain",Accept,2017,"['Janarthanan Rajendran', 'Aravind Lakshminarayanan', 'Mitesh M. Khapra', 'Prasanna P', 'Balaraman Ravindran']","[7, 7, 7]","['Good paper, accept', 'Good paper, accept', 'Good paper, accept']","In this paper a well known soft mixture of experts model is adapted for, and applied to, a specific type of transfer learning problem in reinforcement learning (RL), namely transfer of action policies and value functions between similar tasks. Although not treated as such, the experimental setup is reminiscent of hierarchical RL works, an aspect which the paper does not consider at length, regrettably. One possible implication of this work is that architecture and even learning algorithm choices could simply be stated in terms of the objective of the target task, rather than being hand-engineered by the experimenter. This is clearly an interesting direction of future work which the paper illuminates. Pros: The paper diligently explains how the network architecture fits in with various widely used reinforcement learning setups, which does facilitate continuation of this work. The experiments are good proofs of concept, but do not go beyond that i.m.h.o. Even so, this work provides convincing clues that collections of deep networks, which were trained on not entirely different tasks, generalize better to related tasks when used together rather than through conventional transfer learning (e.g. fine-tuning). Cons: As the paper well recounts in the related work section, libraries of fixed policies have long been formally proposed for reuse while learning similar tasks. Indeed, it is well understood in hierarchical RL literature that it can be beneficial to reuse libraries of fixed (Fernandez & Veloso 2006) or jointly learned policies which may not apply to the entire state space, e.g. options (Pricop et. al). What is not well understood is how to build such libraries, and this paper does not convincingly shed light in that direction, as far as I can tell. The transfer tasks have been picked to effectively illustrate the potential of the proposed architecture, but the paper does not tackle negative transfer or compositional reuse in well known challenging situations outlined in previous work (e.g. Parisotto et. al 2015, Rusu el. al 2015, 2016). Since the main contributions are of an empirical nature, I am curious how the results shown in figures 6 & 7 look plotted against wall-clock time, since relatively low data efficiency is not a limitation for achieving perfect play in Pong (see Mnih. et al, 2015). It would be more illuminating to consider tasks where final performance is plausibly limited by data availability. It would also be interesting if the presented results were achieved with reduced amounts of computation, or reduced representation sizes compared to learning from scratch, especially when one of the useful source tasks is an actual policy trained on the target task. Finally, it is perhaps underwhelming that it takes a quarter of the data required for learning Pong from scratch just to figure out that a perfect Pong policy is already in the expert library. Simply evaluating each expert for 10 episodes and using an average-score-weighted majority vote to mix action choices would probably achieve the same final performance for a smaller fraction of the data.","['4', '3', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[3, 3, 9, 11, 20]","[9, 3, 15, 11, 26]","[31, 10, 144, 4, 253]","[11, 6, 71, 2, 142]","[18, 3, 64, 2, 86]","[2, 1, 9, 0, 25]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('interesting direction', 'diligently explains', 'good proofs of concept'), there are more criticisms and limitations pointed out. The reviewer expresses disappointment in certain aspects ('regrettably', 'does not convincingly shed light', 'underwhelming') and suggests several improvements or additional experiments that could have been done. The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, acknowledges the paper's strengths, and frames criticisms constructively ('I am curious', 'It would be more illuminating', 'It would also be interesting'). The reviewer also uses phrases like 'in my humble opinion' which adds to the politeness. However, it's not extremely high as the critique is still direct and substantial.",-20,60
Autoencoding Variational Inference For Topic Models,Accept,2017,"['Akash Srivastava', 'Charles Sutton']","[6, 7, 6, 5]","['Marginally above acceptance threshold', 'Good paper, accept', 'Marginally above acceptance threshold', '5']","The authors propose NVI for LDA variants. The authors compare NVI-LDA to standard inference schemes such as CGS and online SVI. The authors also evaluate NVI on a different model ProdLDA (not sure this model has been proposed before in the topic modeling literature though?) In general, I like the direction of this paper and NVI looks promising for LDA. The experimental results however confound model vs inference which makes it hard to understand the significance of the results. Furthermore, the authors don*t discuss hyper-parameter selection which is known to significantly impact performance of topic models. This makes it hard to understand when the proposed method can be expected to work. Can you maybe generate synthetic datasets with different Dirichlet distributions and assess when the proposed method recovers the true parameters? Figure 1: Is this prior or posterior? The text talks about sparsity whereas the y-axis reads *log p(topic proportions)* which is a bit confusing. Section 3.2: it is not clear what you mean by unimodal in softmax basis. Consider a Dirichlet on K-dimensional simplex with concentration parameter alpha/K where alpha<1 makes it multimodal. Isn*t the softmax basis still multimodal? None of the numbers include error bars. Are the results statistically significant? Minor comments: Last term in equation (3) is not *error*; reconstruction accuracy or negative reconstruction error perhaps? The idea of using an inference network is much older, cf. Helmholtz machine.","['4', '3', '5', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[8, 15]","[14, 20]","[39, 148]","[11, 76]","[23, 60]","[5, 12]","The sentiment score is slightly positive (20) because the reviewer expresses liking the direction of the paper and finding NVI promising, but also raises several concerns and criticisms. The overall tone is cautiously optimistic but with significant reservations. The politeness score is moderately positive (50) as the reviewer uses polite language like 'I like' and phrases criticisms as questions or suggestions rather than harsh statements. They also acknowledge positive aspects before raising concerns. However, the review is not overly effusive in its politeness, maintaining a professional tone throughout.",20,50
Automatic Rule Extraction from Long Short Term Memory Networks,Accept,2017,"['W. James Murdoch', 'Arthur Szlam']","[7, 7, 7]","['Good paper, accept', 'Good paper, accept', 'Good paper, accept']","EDIT: the revisions made to this paper are very thorough and address many of my concerns, and the paper is also easier to understand. i recommend the latest version of this paper for acceptance and have increased my score. This paper presents a way of interpreting LSTM models, which are notable for their opaqueness. In particular, the authors propose decomposing the LSTM*s predictions for a QA task into importance scores for words, which are then used to generate patterns that are used to find answers with a simple matching algorithm. On the WikiMovies dataset, the extracted pattern matching method achieves accuracies competitive with a normal LSTM, which shows the power of the proposed approach. I really like the motivation of the paper, as interpreting LSTMs is definitely still a work-in-progress, and the high performance of the pattern matching was surprising. However, several details of the pattern extraction process are not very clear, and the evaluation is conducted on a very specific task, where predictions are made at every word. As such, I recommend the paper in its current form as a weak accept but hope that the authors clarify their approach, as I believe the proposed method is potentially useful for NLP researchers. Comments: - Please introduce in more detail the specific QA tasks you are applying your models on before section 3.3, as it*s not clear at that point that the answer is an entity within the document. - 3.3: is the softmax predicting a 0/1 value (e.g., is this word the answer or not?) - 3.3: what are the P and Q vectors? do you just mean that you are transforming the hidden state into a 2-dimensional vector for binary prediction? - how does performance of the pattern matching change with different cutoff constant values? - 5.2: are there questions whose answers are not entities? - how could the proposed approach be used when predictions aren*t made at every word? is there any extension for, say, sentence-level sentiment classification?","['4', '3', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[1, 9]","[4, 15]","[10, 138]","[4, 54]","[6, 74]","[0, 10]","The sentiment score is 50 (moderately positive) because the reviewer starts by noting the thorough revisions and recommends acceptance, which is positive. However, they also mention some remaining concerns and areas for clarification, tempering the overall sentiment. The politeness score is 75 (quite polite) because the reviewer uses respectful language throughout, acknowledges improvements, and frames criticisms constructively as suggestions. They use phrases like 'I really like the motivation' and 'I hope that the authors clarify' which maintain a collegial tone. The reviewer balances praise with constructive feedback in a professional manner.",50,75
Bidirectional Attention Flow for Machine Comprehension,Accept,2017,"['Minjoon Seo', 'Aniruddha Kembhavi', 'Ali Farhadi', 'Hannaneh Hajishirzi']","[7, 8, 8]","['Good paper, accept', 'Top 50% of accepted papers, clear accept', 'Top 50% of accepted papers, clear accept']","This is a solid paper with good results. However, there aren*t many very interesting takeaways (most of the architecture seems a concatenation of standard elements to do well in the leaderboard) and some issues in the writing. The second paragraph of the introduction is very confusing. It*s clear the authors got really deep into their world and made no attempts to actually clearly explain their model, not even to an expert in the field, let a lone somebody who isn*t familiar with similar approaches. The authors keep referring to *previously popular attention paradigms* without any citation and then, I believe, incorrectly describe whatever those are supposed to be by writing that these unknown but popular approaches *summarize each modality into a single vector.* That*s one of the most incorrect descriptions I*ve yet seen for attention mechanisms. First, I don*t know what model works over several modalities in a single attention pass. Maybe the authors don*t know what a *modality* is? More importantly, the whole point of most attention mechanisms is that one does not simply summarize the whole input but instead can access all elements of it. So, this paper*s supposedly new way of using attention is pretty much exactly the standard way. Both modeling and modelling spellings are in the text. I understand the need to sometimes invent new terminology to describe a model but in this paragraph, the authors 3 times talk about a *modeling layer (RNN)*... It*s just an RNN, you don*t need to give an RNN another name, especially one that*s as nondescript as *modeling layer* all layers are part of a model? Typo: *let*s the modeling (RNN) layer to learn* This paragraph is supposed to give an overview of the model but just confuses readers. I would delete it. *Phrase embedding layer* -- terrible word choice as you are not embedding phrases here. It*s a standard bidirectional LSTM over words, not phrases. In all subsequent parts of the paper you just give examples of words embedded in context. No phrases. Please change this to *contextual word embedding layer* or something less incorrect. Your phrase layer embeddings only show single words, as expected in Table 2. Section 2: point 4. Second sentence needs citations for *popular* Typo: *from both *of* the context and query word* Typo: *aveaged* It seems like your output layer changes quite substantially so your claim in the abstract/intro of using the same model isn*t quite accurate. I*d say you*re changing one module or part of your model. Section 4: attention isn*t countable (no *a* in front of *huge attention*). Also, academic writing usually doesn*t include such adjectives in the first place.","['5', '4', '5']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[4, 10, 15, 11]","[10, 16, 21, 17]","[69, 93, 240, 237]","[26, 39, 117, 116]","[43, 49, 109, 118]","[0, 5, 14, 3]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges it as a 'solid paper with good results', they also point out numerous issues with the writing, explanations, and terminology. The reviewer expresses confusion about certain parts and suggests deletions and changes, indicating more negative than positive feedback overall. The politeness score is slightly positive (20) as the reviewer maintains a professional tone throughout, using phrases like 'I understand' and offering constructive suggestions. However, some direct criticisms like 'terrible word choice' and pointing out multiple typos prevent a higher politeness score. The reviewer balances critique with acknowledgment of the paper's strengths, maintaining a generally respectful tone while providing detailed feedback.",-20,20
Calibrating Energy-based Generative Adversarial Networks,Accept,2017,"['Zihang Dai', 'Amjad Almahairi', 'Philip Bachman', 'Eduard Hovy', 'Aaron Courville']","[8, 7, 8]","['Top 50% of accepted papers, clear accept', 'Good paper, accept', 'Top 50% of accepted papers, clear accept']","This paper addresses one of the major shortcomings of generative adversarial networks - their lack of mechanism for evaluating held-out data. While other work such as BiGANs/ALI address this by learning a separate inference network, here the authors propose to change the GAN objective function such that the optimal discriminator is also an energy function, rather than becoming uninformative at the optimal solution. Training this new objective requires gradients of the entropy of the generated data, which are difficult to approximate, and the authors propose two methods to do so, one based on nearest neighbors and one based on a variational lower bound. The results presented show that on toy data the learned discriminator/energy function closely approximates the log probability of the data, and on more complex data the discriminator give a good measure of quality for held out data. I would say the largest shortcomings of the paper are practical issues around the scalability of the nearest neighbors approximation and accuracy of the variational approximation, which the authors acknowledge. Also, since entropy estimation and density estimation are such closely linked problems, I wonder if any practical method for EGANs will end up being equivalent to some form of approximate density estimation, exactly the problem GANs were designed to circumvent. Nonetheless, the elegant mathematical exposition alone makes the paper a worthwhile contribution to the literature. Also, some quibbles about the writing - it seems that something is missing in the sentence at the top of pg. 5 *Finally, let*s whose discriminative power*. I*m not sure what the authors mean to say here. And the title undersells the paper - it makes it sound like they are making a small improvement to training an existing model rather than deriving an alternative training framework.","['4', '5', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[2, 3, 9, 33, 17]","[7, 9, 15, 39, 23]","[51, 24, 51, 517, 309]","[25, 8, 25, 350, 135]","[26, 16, 25, 123, 160]","[0, 0, 1, 44, 14]","The sentiment score is 70 (positive) because the reviewer acknowledges the paper's contribution to addressing a major shortcoming of GANs, praises the 'elegant mathematical exposition,' and calls it a 'worthwhile contribution to the literature.' While some shortcomings are mentioned, they are presented as minor issues. The politeness score is 60 (moderately polite) because the reviewer uses respectful language throughout, acknowledges the paper's strengths, and frames criticisms constructively as 'quibbles' or 'shortcomings.' The reviewer also uses phrases like 'I wonder' to soften potential criticisms. However, the tone remains professional rather than overtly deferential, hence not scoring higher on politeness.",70,60
Capacity and Trainability in Recurrent Neural Networks,Accept,2017,"['Jasmine Collins', 'Jascha Sohl-Dickstein', 'David Sussillo']","[8, 7, 8]","['Top 50% of accepted papers, clear accept', 'Good paper, accept', 'Top 50% of accepted papers, clear accept']","This paper performs a very important service: exploring in a clear and systematic way the performance and trainability characteristics of a set of neural network architectures -- in particular, the basic RNN motifs that have recently been popular. Pros: * This paper addresses an important question I and many others would have liked to know the answer to but didn*t have the computational resources to thoroughly attack it. This is a nice use of Google*s resources to help the community. * The work appears to have been done carefully so that the results can be believed. * The basic answer arrived at (that, in the *typical training environment* LSTMs are reliable but basically GRUs are the answer) seems fairly decisive and practically useful. Of course the real answer is more complicated than my little summary here, but the subtleties are discussed nicely in the paper. * The insistence on a strong distinction between capacity and trainability helps nicely clear up a misconception about the reasons why gated architectures work. In sum, they*re much more easily trainable but somewhat lower capacity than vanilla RNNs, and in hard tasks, the benefits of better trainability far outweigh the costs of mildly lower capacity. * The point about the near-equivalence of capacity at equal numbers of parameters is very useful. * The paper makes it clear the importance of HP tuning, something that has sometimes gotten lost in the vast flow of papers about new architectures. * The idea of quantifying the fraction of infeasible parameters (e.g. those that diverge) is nice, because it*s a practical problem that everyone working with these networks has but often isn*t addressed. * The paper text is very clearly written. Cons: * The work on the UGRNNs and the +RNNs seems a bit preliminary. I don*t think that the authors have clearly shown that the +RNN should be *recommended* with the same generality as the GRU. I*d at the least want some better statistics on the significance of differences between +RNN and GRU performances quantifying the results in Figure 4 (8-layer panel). In a way the high standards for declaring an architecture useful that are set in the paper make the UGRNNs and +RNN contributions seem less important. I don*t really mind having them in the paper though. I guess the point of this paper is not really to be novel in the first place -- which is totally fine with me, though I don*t know what the ICLR area chairs will think. * The paper gives short shrift to the details of the HP algorithm itself. They do say: *Our setting of the tuner’s internal parameters was such that it uses Batched GP Bandits with an expected improvement acquisition function and a Matern 5/2 Kernel with feature scaling and automatic relevance determination performed by optimizing over kernel HPs* and give some good references, but I expect that actually trying to replicate this involves a lot of missing details. * I found some of the figures a bit hard to read at first, esp. Fig 4, mostly due to the panels being small, having a lot of details, and bad choices for visual cleanliness. * The neuroscience reference (*4.7 bits per synapse*) seems a little bit of a throw-away to me, because the connection between these results and the experimental neuroscience is very tenuous, or at any rate, not well explained. I guess it*s just in the discussion, but it seems gratuitous. Maybe it should couched in slightly less strong terms (nothing is really strongly shown to be *in agreement* here between computational architectures and neuroscience, but perhaps they could say something like -- *We wonder if it is anything other than coincidence that our 5 bits result is numerically similar to the 4.7 bits measurement from neuroscience.*)","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[2, 9, 14]","[8, 15, 20]","[15, 130, 34]","[4, 52, 13]","[9, 74, 15]","[2, 4, 6]","The sentiment score is 80 (positive) because the review starts with a strong positive statement about the paper performing 'a very important service' and lists numerous pros, including the paper addressing an important question, being carefully done, arriving at decisive and useful conclusions, and being clearly written. The few cons mentioned are relatively minor and presented constructively. The politeness score is 70 (polite) because the reviewer uses respectful language throughout, acknowledges the value of the work, and frames criticisms in a constructive manner. The reviewer uses phrases like 'This paper addresses an important question' and 'The work appears to have been done carefully,' which demonstrate respect for the authors' efforts. Even when pointing out limitations, the reviewer uses polite language like 'I don't think that the authors have clearly shown' rather than making blunt criticisms.",80,70
Categorical Reparameterization with Gumbel-Softmax,Accept,2017,"['Eric Jang', 'Shixiang Gu', 'Ben Poole']","[7, 6, 6]","['Good paper, accept', 'Marginally above acceptance threshold', 'Marginally above acceptance threshold']","This paper introduces a continuous relaxation of categorical distribution, namely the the Gumbel-Softmax distribution, such that generative models with categorical random variables can be trained using reparameterization (path-derivative) gradients. The method is shown to improve upon other methods in terms of the achieved log-likelihoods of the resulting models. The main contribution, namely the method itself, is simple yet nontrivial and worth publishing, and seems effective in experiments. The paper is well-written, and I applaud the details provided in the appendix. The main application seems to be semi-supervised situations where you really want categorical variables. - P1: *differentiable sampling mechanism for softmax*. *sampling* => *approximate sampling*, since it*s technically sampling from the Gumbal-softmax. - P3: *backpropagtion* - Section 4.1: Interesting experiments. - It would be interesting to report whether there is any discrepancy between the relaxed and non-relaxed models in terms of log-likelihood. Currently, only the likelihoods under the non-relaxed models are reported. - It is slightly discouraging that the temperature (a nuisance parameter) is used differently across experiments. It would be nice to give more details on whether you were succesful in learning the temperature, instead of annealing it; it would be interesting if that hyper-parameter could be eliminated.","['5', '4', '3']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[2, 6, 7]","[7, 12, 13]","[35, 89, 61]","[17, 38, 24]","[18, 49, 36]","[0, 2, 1]","The sentiment score is 80 (positive) because the reviewer expresses strong approval of the paper's contribution, describing it as 'simple yet nontrivial and worth publishing,' and 'effective in experiments.' They also praise the writing quality and the detailed appendix. The politeness score is 70 (polite) due to the use of respectful language throughout, such as 'I applaud the details provided in the appendix.' The reviewer offers constructive suggestions without harsh criticism, maintaining a professional and courteous tone. They use phrases like 'It would be interesting' and 'It would be nice' when suggesting improvements, which contributes to the polite tone. The slightly lower politeness score compared to sentiment is due to the directness of some comments, which, while not impolite, are less overtly courteous.",80,70
Central Moment Discrepancy (CMD) for Domain-Invariant Representation Learning,Accept,2017,"['Werner Zellinger', 'Thomas Grubinger', 'Edwin Lughofer', 'Thomas Natschläger', 'Susanne Saminger-Platz']","[6, 7, 9]","['Marginally above acceptance threshold', 'Good paper, accept', '9']","The work introduces a new regularization for learning domain-invariant representations with neural networks. The regularization aims at matching the higher order central moments of the hidden activations of the NNs of the source and target domain. The authors compared the proposed method vs MMD and two state-of-art NN domain adaptation algorithms on the Amazon review and office datasets, and showed comparable performance. The idea proposed is simple and straightforward, and the empirical results suggest that it is quite effective. The biggest limitation I can see with the proposed method is the assumption that the hidden activations are independently distributed. For example, this assumption will clearly be violated for the hidden activations of convolutional layers, where neighboring activations are dependent. I guess this is why the authors start with the output of dense layers for the image dataset. Do the authors have insight on if it is beneficial to start adaptation from lower level? If so, do the authors have insight on how to relax the assumption? In these scenarios, if MMD has an advantage as it does not make this assumption? Figure 3 does not seems to clearly support the boost of performance shown in table 2. The only class where the new regularization brings the source and target domain closer seem to be the mouse class pointed by the authors. Is the performance improvement only coming from this single class?","['4', '4', '5']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[3, 8, 15, 22, 16]","[9, 11, 21, 26, 21]","[29, 11, 222, 48, 45]","[12, 4, 82, 27, 16]","[11, 3, 16, 5, 4]","[6, 4, 124, 16, 25]","The sentiment score is 50 (slightly positive) because the reviewer acknowledges the work's novelty and effectiveness, stating that the idea is 'simple and straightforward' and 'quite effective.' However, they also point out limitations and ask for clarifications, balancing the positive aspects. The politeness score is 75 (polite) because the reviewer uses respectful language throughout, asks questions instead of making accusations, and acknowledges the strengths of the work before discussing potential weaknesses. The reviewer's tone is constructive and inquisitive rather than critical, using phrases like 'Do the authors have insight' and 'Is the performance improvement,' which invite dialogue rather than demanding changes.",50,75
Combining policy gradient and Q-learning,Accept,2017,"[""Brendan O'Donoghue"", 'Remi Munos', 'Koray Kavukcuoglu', 'Volodymyr Mnih']","[9, 7, 7]","['9', 'Good paper, accept', 'Good paper, accept']","This is a very nicely written paper which unifies some value-based and policy-based (regularized policy gradient) methods, by pointing out connections between the value function and policy which have not been established before. The theoretical results are new and insightful, and will likely be useful in the RL field much beyond the specific algorithm being proposed in the paper. This being said, the paper does exploit the theory to produce a unified version of Q-learning and policy gradient, which proves to work on par or better than the state-of-art algorithms on the Atari suite. The empirical section is very well explained in terms of what optimization were done. One minor comment I had was related to the stationary distribution used for a policy - there are subtleties here between using a discounted vs non-discounted distribution which are not crucial in the tabular case, but will need to be addressed in the long run in the function approximation case. This being said, there is no major problem for the current version of the paper. Overall, the paper is definitely worthy of acceptance, and will likely influence a broad swath of RL, as it opens the door to further theoretical results as well as algorithm development.","['5', '3', '4']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[28, 3, 6, 34]","[34, 9, 12, 40]","[37, 22, 62, 550]","[12, 12, 24, 290]","[19, 7, 30, 94]","[6, 3, 8, 166]","The sentiment score is 90 because the review is overwhelmingly positive. The reviewer uses phrases like 'very nicely written', 'new and insightful', 'useful', 'works on par or better than the state-of-art algorithms', and 'definitely worthy of acceptance'. The only minor criticism is presented as a non-crucial point. The politeness score is 80 because the language is consistently respectful and appreciative. The reviewer uses polite phrases like 'very nicely written' and 'very well explained', and frames the minor criticism in a non-confrontational way. The tone is professional and constructive throughout, without any harsh or rude language.",90,80
DSD: Dense-Sparse-Dense Training for Deep Neural Networks,Accept,2017,"['Song Han', 'Jeff Pool', 'Sharan Narang', 'Huizi Mao', 'Enhao Gong', 'Shijian Tang', 'Erich Elsen', 'Peter Vajda', 'Manohar Paluri', 'John Tran', 'Bryan Catanzaro', 'William J. Dally']","[8, 8, 5]","['Top 50% of accepted papers, clear accept', 'Top 50% of accepted papers, clear accept', '5']","Training highly non-convex deep neural networks is a very important practical problem, and this paper provides a great exploration of an interesting new idea for more effective training. The empirical evaluation both in the paper itself and in the authors’ comments during discussion convincingly demonstrates that the method achieves consistent improvements in accuracy across multiple architectures, tasks and datasets. The algorithm is very simple (alternating between training the full dense network and a sparse version of it), which is actually a positive since that means it may get adapted in practice by the research community. The paper should be revised to incorporate the additional experiments and comments from the discussion, particularly the accuracy comparisons with the same number of epochs.","['3', '3', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[3, 10, 3, 3, 6, 6, 12, 12, 8, 14, 13, 33]","[9, 14, 9, 8, 11, 6, 17, 18, 11, 16, 19, 39]","[150, 24, 33, 24, 17, 10, 53, 73, 43, 23, 126, 274]","[66, 13, 8, 13, 9, 5, 24, 35, 24, 13, 52, 174]","[67, 10, 23, 10, 3, 2, 28, 31, 19, 4, 68, 23]","[17, 1, 2, 1, 5, 3, 1, 7, 0, 6, 6, 77]",The sentiment score is 90 (highly positive) because the review starts with emphasizing the importance of the problem and describes the paper as providing a 'great exploration of an interesting new idea'. It highlights the 'convincing' empirical evaluation and 'consistent improvements' achieved by the method. The simplicity of the algorithm is framed as a positive aspect. The politeness score is 70 (quite polite) due to the use of positive and respectful language throughout. The reviewer acknowledges the value of the work and offers constructive feedback for improvement without any harsh criticism. The suggestion for revision is presented as an opportunity to incorporate additional valuable information rather than as a criticism of the current work.,90,70
Data Noising as Smoothing in Neural Network Language Models,Accept,2017,"['Ziang Xie', 'Sida I. Wang', 'Jiwei Li', 'Daniel Lévy', 'Aiming Nie', 'Dan Jurafsky', 'Andrew Y. Ng']","[6, 8]","['Marginally above acceptance threshold', 'Top 50% of accepted papers, clear accept']","This paper proposes a data noising technique for language modeling. The main idea is to noise a word history by using a probabilistic distribution based on N-gram smoothing techniques. The paper is clearly written and shows that such simple techniques improve the performance in various tasks including language modeling and machine translation. May main concern is that the method is too simple and sounds ad hoc, e.g., there is no theoretical justification of why n-gram smoothing based data noising would be effective for recurrent neural network based language modeling. Comments: - p. 3 “can be seen has a way” -> “can be seen as a way” (?) - p. 3. In general, the explanation about blank noising should be improved. Why does it avoid overfitting on specific contexts? - p. 4. It would be better to provide more detailed derivations for a general form of unigram and blank noising equations. - p. 5, Section 3.6: Is there any discussions about noising either/both input and output sequences with some numbers? This would be helpful information.","['4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[6, 6, 4, 2, 1, 30, 23]","[7, 12, 10, 2, 5, 36, 29]","[9, 48, 125, 3, 4, 296, 284]","[5, 25, 46, 1, 2, 185, 192]","[3, 23, 77, 2, 1, 84, 58]","[1, 0, 2, 0, 1, 27, 34]","The sentiment score is 50 (slightly positive) because the reviewer acknowledges the paper's clear writing and the effectiveness of the proposed technique, but expresses concerns about its simplicity and lack of theoretical justification. The positive aspects (clear writing, improved performance) are balanced against the main concern, resulting in a slightly positive overall sentiment. The politeness score is 75 (polite) because the reviewer uses respectful language throughout, offering constructive criticism and suggestions for improvement without harsh or dismissive comments. The reviewer begins with positive observations before addressing concerns, and uses phrases like 'It would be better' and 'This would be helpful' when making suggestions, which contributes to a polite tone.",50,75
Decomposing Motion and Content for Natural Video Sequence Prediction,Accept,2017,"['Ruben Villegas', 'Jimei Yang', 'Seunghoon Hong', 'Xunyu Lin', 'Honglak Lee']","[7, 6, 7]","['Good paper, accept', 'Marginally above acceptance threshold', 'Good paper, accept']","This paper introduces an approach for future frame prediction in videos by decoupling motion and content to be encoded separately, and additionally using multi-scale residual connections. Qualitative and quantitative results are shown on KTH, Weizmann, and UCF-101 datasets. The idea of decoupling motion and content is interesting, and seems to work well for this task. However, the novelty is relatively incremental given previous cited work on multi-stream networks, and it is not clear that this particular decoupling works well or is of broader interest beyond the specific task of future frame prediction. While results on KTH and Weizmann are convincing and significantly outperform baselines, the results are less impressive on less constrained UCF-101 dataset. The qualitative examples for UCF-101 are not convincing, as discussed in the pre-review question. Overall this is a well-executed work with an interesting though not extremely novel idea. Given the limited novelty of decoupling motion and content and impact beyond the specific application, the paper would be strengthened if this could be shown to be of broader interest e.g. for other video tasks.","['5', '4', '4']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[4, 9, 5, 1, 13]","[10, 15, 11, 6, 19]","[33, 125, 64, 9, 273]","[16, 64, 36, 4, 145]","[17, 52, 26, 4, 123]","[0, 9, 2, 1, 5]","The sentiment score is slightly positive (20) because the reviewer acknowledges the paper's interesting idea and well-executed work, but also points out limitations in novelty and broader impact. The review begins with positive aspects ('interesting', 'works well') before discussing drawbacks, indicating a generally constructive tone. However, the limited novelty and impact beyond the specific application prevent a higher positive score. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, avoiding harsh criticism. They offer balanced feedback, acknowledging strengths ('well-executed work') while suggesting improvements ('paper would be strengthened if...'). The tone is professional and constructive, providing specific examples and recommendations without being overly critical or dismissive.",20,50
Deep Biaffine Attention for Neural Dependency Parsing,Accept,2017,"['Timothy Dozat', 'Christopher D. Manning']","[5, 5, 6]","['5', '5', 'Marginally above acceptance threshold']","The paper proposes a new function for computing arc score between two words in a sentence for dependency parsing. The proposed function is biaffine in the sense that it*s a combination of a bilinear score function and a bias term playing a role as prior. The paper reports new state-of-the-art dependency parsing performances on both English PTB and Chinese TB. The paper is very well written with impressive experimental results and analysis. However, the idea is hardly novel regarding to the theme of the conference: the framework that the paper uses is from Kiperwasser & Goldberg (2016), the use of bilinear score function for attention is from Luong et al (2015). Projecting BiLSTM outputs into different spaces using MLPs is a trivial step to make the model *deeper*, whereas adding linear bias terms isn*t confirmed to work in the experiments (table 2 shows that diag bilinear has a close performance to biaffine). I think that this paper is more proper for NLP conferences.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[5, 25]","[11, 31]","[15, 408]","[8, 284]","[7, 104]","[0, 20]","The sentiment score is 50 (slightly positive) because the reviewer acknowledges the paper is 'very well written with impressive experimental results and analysis' and reports 'new state-of-the-art dependency parsing performances'. However, the reviewer also expresses concerns about the novelty of the idea and its suitability for the conference theme, which balances out the positive aspects. The politeness score is 75 (quite polite) because the reviewer uses respectful language throughout, acknowledging the paper's strengths before presenting criticisms. The reviewer also uses phrases like 'I think' to soften their critique, maintaining a professional and courteous tone.",50,75
Deep Information Propagation,Accept,2017,"['Samuel S. Schoenholz', 'Justin Gilmer', 'Surya Ganguli', 'Jascha Sohl-Dickstein']","[9, 8]","['9', 'Top 50% of accepted papers, clear accept']","The paper expands a recent mean-field approximation of deep random neural networks to study depth-dependent information propagation, its phase-dependence and the influence of drop-out. The paper is extremely well written, the mathematical analysis is thorough and numerical experiments are included that underscore the theoretical results. Overall the paper stands out as one of the few papers that thoroughly analyses training and performance of deep nets.","['4', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[13, 0, 7, 3]","[19, 4, 13, 4]","[146, 3, 151, 8]","[57, 1, 37, 5]","[12, 0, 9, 0]","[77, 2, 105, 3]","The sentiment score is 95 out of 100 because the review is overwhelmingly positive. The reviewer describes the paper as 'extremely well written' with 'thorough' mathematical analysis and supporting numerical experiments. They also state that it 'stands out' among papers analyzing deep nets, indicating high praise. The politeness score is 80 out of 100 because the language is professional and respectful, using positive descriptors without being overly effusive. The reviewer provides clear, constructive feedback without any negative or critical comments, maintaining a polite and encouraging tone throughout.",95,80
Deep Learning with Dynamic Computation Graphs,Accept,2017,"['Moshe Looks', 'Marcello Herreshoff', 'DeLesley Hutchins', 'Peter Norvig']","[7, 8, 8]","['Good paper, accept', 'Top 50% of accepted papers, clear accept', 'Top 50% of accepted papers, clear accept']","Authors describe implementation of TensorFlow Fold which allows one to run various computations without modifying computation graph. They achieve this by creating a generic scheduler as a TensorFlow computation graph, which can accept graph description as input and execute it. They show clear benefits to this approach for tasks where computation changes for each datapoint, such as the case with TreeRNN. In the experiments, they compare against having static batch (same graph structure repeated many times) and batch size 1. The reason my score is 7 and not higher is because they do not provide comparison to the main alternative of their method -- someone could create a new TensorFlow graph for each dynamic batch. In other words, instead of using their graph as the scheduling algorithm, one could explicitly create each non-uniform batch as a TensorFlow graph, and run that using standard TensorFlow.","['5', '3', '3']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[15, 4, 15, 35]","[15, 4, 20, 41]","[20, 3, 11, 35]","[18, 1, 7, 16]","[2, 2, 3, 1]","[0, 0, 1, 18]","The sentiment score is 70 because the reviewer generally expresses a positive view of the work, highlighting clear benefits and praising the implementation. However, it's not 100 due to the criticism about lack of comparison to a main alternative method. The politeness score is 50 because the language is professional and neutral, without being overly formal or particularly warm. The reviewer provides constructive feedback and explains their scoring rationale without using harsh or overly critical language, but also doesn't use explicitly polite phrases or compliments.",70,50
Deep Multi-task Representation Learning: A Tensor Factorisation Approach,Accept,2017,"['Yongxin Yang', 'Timothy M. Hospedales']","[5, 7, 8]","['5', 'Good paper, accept', 'Top 50% of accepted papers, clear accept']","The paper proposed a nice framework leveraging Tucker and Tensor train low-rank tensor factorization to induce parameter sharing for multi-task learning. The framework is nice and appealing. However, MTL is a very well studied problem and the paper considers simple task for different classification, and it is not clear if we really need ``Deep Learning* for these simple datasets. A comparison with existing shallow MTL is necessary to show the benefits of the proposed methods (and in particular being deep) on the dataset. The authors ignore them on the basis of speculation and it is not clear if the proposed framework is really superior to simple regularizations like the nuclear norm. The idea of nuclear norm regularization can also be extended to deep learning as gradient descent are popular in all methods.","['3', '4', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[4, 11]","[10, 17]","[138, 305]","[63, 139]","[61, 131]","[14, 35]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the framework as 'nice and appealing', they express significant concerns about the necessity of deep learning for the simple datasets used and the lack of comparison with existing shallow MTL methods. The reviewer questions the superiority of the proposed method over simpler techniques. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, acknowledging the positive aspects of the work ('nice framework', 'appealing') before presenting their criticisms. The criticisms are presented as concerns and suggestions rather than harsh judgments, maintaining a professional and courteous tone.",-20,50
Deep Predictive Coding Networks for Video Prediction and Unsupervised Learning,Accept,2017,"['William Lotter', 'Gabriel Kreiman', 'David Cox']","[6, 8, 8]","['Marginally above acceptance threshold', 'Top 50% of accepted papers, clear accept', 'Top 50% of accepted papers, clear accept']","An interesting architecture that accumulates and continuously corrects mistakes as you see more and more of a video sequence. Clarity: The video you generated seems very helpful towards understanding the information flow in your network, it would be nice to link to it from the paper. *Our model with hyperparameters optimized for KITTI underperforms the model of Finn et al. (2016), but outperforms the previous state-of-the-art model by Mathieu et al. (2016).* It is not clear how different are the train and test sequences at the moment, since standard benchmarks do not really exist for video prediction and each author picks his/her favorite. Underperforming Finn et al 206 at the H3.6m Walking videos is a bit disappointing.","['3', '4', '5']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[3, 10, 15]","[6, 16, 21]","[13, 53, 92]","[3, 12, 44]","[9, 32, 35]","[1, 9, 13]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the architecture as 'interesting' and praises the helpful video, they also express disappointment about underperformance on certain benchmarks. The politeness score is moderately positive (50) as the reviewer uses polite language like 'it would be nice' and avoids harsh criticism, instead offering constructive feedback. The tone is professional and respectful throughout, even when pointing out areas for improvement.",-20,50
Deep Probabilistic Programming,Accept,2017,"['Dustin Tran', 'Matthew D. Hoffman', 'Rif A. Saurous', 'Eugene Brevdo', 'Kevin Murphy', 'David M. Blei']","[5, 8, 7]","['5', 'Top 50% of accepted papers, clear accept', 'Good paper, accept']","The authors propose a new software package for probabilistic programming, taking advantage of recent successful tools used in the deep learning community. The software looks very promising and has the potential to transform the way we work in the probabilistic modelling community, allowing us to perform rapid-prototyping to iterate through ideas quickly. The composability principles are used insightfully, and the extension of inference to HMC for example, going beyond VI inference (which is simple to implement using existing deep learning tools), makes the software even more compelling. However, the most important factor of any PPL is whether it is practical for real-world use cases. This was not demonstrated sufficiently in the submission. There are many example code snippets given in the paper, but most are not evaluated. The Dirichlet process mixture model example (Figure 12) is an important one: do the proposed black-box inference tools really work for this snippet? and will the GAN example (Figure 7) converge when optimised with real data? To convince the community of the practicality of the package it will be necessary to demonstrate these empirically. Currently the only evaluated model is a VAE with various inference techniques, which are not difficult to implement using pure TF. Presentation: * Paper presentation could be improved. For example the authors could use more signalling for what is about to be explained. On page 5 qbeta and qz are used without explanation - the authors could mention that an example will be given thereafter. * I would also suggest to the authors to explain in the preface how the layers are implemented, and how the KL is handled in VI for example. It will be useful to discuss what values are optimised and what values change as inference is performed (even before section 4.4). This was not clear for the majority of the paper. Experiments: * Why is the run time not reported in table 1? * What are the *difficulties around convergence* encountered with the analytical entropies? inference issues become more difficult to diagnose as inference is automated. Are there tools to diagnose these with the provided toolbox? * Did HMC give sensible results in the experiment at the bottom of page 8? only run time is reported. * How difficult is it to get the inference to work (eg HMC) when we don*t have full control over the computational graph structure and sampler? * It would be extremely insightful to give a table comparing the performance (run time, predictive log likelihood, etc) of the various inference tools on more models. * What benchmarks do you intend to use in the Model Zoo? the difficulty with probabilistic modelling is that there are no set benchmarks over which we can evaluate and compare many models. Model zoo is sensible for the Caffe ecosystem because there exist few benchmarks a large portion of the community was working on (ImageNet for example). What datasets would you use to compare the DPMM on for example? Minor comments: * Table 1: I would suggest to compare to Li & Turner with alpha=0.5 (the equivalent of Hellinger distance) as they concluded this value performs best. I*m not sure why alpha=-1 was chosen here. * How do you handle discrete distributions (eg Figure 5)? * x_real is not defined in Figure 7. * I would suggest highlighting M in Figure 8. * Comma instead of period after *rized), In* on page 8. In conclusion I would say that the software developments presented here are quite exciting, and I*m glad the authors are pushing towards practical and accessible *inference for all*. In its current form though I am forced to give the submission itself a score of 5.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[0, 8, 30, 15]","[5, 13, 35, 20]","[6, 15, 24, 22]","[5, 1, 7, 9]","[1, 4, 0, 0]","[0, 10, 17, 13]","The sentiment score is 50 (slightly positive) because the reviewer expresses enthusiasm about the software's potential and its promising features, but also raises significant concerns about the lack of practical demonstrations and empirical evidence. The review begins positively but becomes more critical as it progresses. The politeness score is 70 (fairly polite) because the reviewer uses respectful language throughout, offers constructive criticism, and provides specific suggestions for improvement. The reviewer acknowledges the exciting aspects of the work while clearly stating areas that need addressing, maintaining a professional and courteous tone throughout the review.",50,70
Deep Variational Bayes Filters: Unsupervised Learning of State Space Models from Raw Data,Accept,2017,"['Maximilian Karl', 'Maximilian Soelch', 'Justin Bayer', 'Patrick van der Smagt']","[6, 7, 6]","['Marginally above acceptance threshold', 'Good paper, accept', 'Marginally above acceptance threshold']","This is mainly a (well-written) toy application paper. It explains SGVB can be applied to state-space models. The main idea is to cast a state-space model as a deterministic temporal transformation, with innovation variables acting as latent variables. The prior over the innovation variables is not a function of time. Approximate inference is performed over these innovation variables, rather the states. This is a solution to a fairly specific problem (e.g. it doesn*t discuss how priors over the beta*s can depend on the past), but an interesting application nonetheless. The ideas could have been explained more compactly and more clearly; the paper dives into specifics fairly quickly, which seems a missed opportunity. My compliments for the amount of detail put in the paper and appendix. The experiments are on toy examples, but show promise. - Section 2.1: “In our notation, one would typically set beta_t = w_t, though other variants are possible” -> It’s probably better to clarify that if F_t and B_t and not in beta_t, they are not given a Bayesian treatment (but e.g. merely optimized). - Section 2.2 last paragraph: “A key contribution is […] forcing the latent space to fit the transition”. This seems rather trivial to achieve. - Eq 9: “This interpretation implies the factorization of the recognition model:..” The factorization is not implied anywhere: i.e. you could in principle use q(beta|x) = q(w|x,v)q(v)","['4', '3', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[5, 2, 9, 28]","[10, 6, 15, 34]","[17, 11, 51, 124]","[7, 6, 25, 58]","[9, 5, 24, 43]","[1, 0, 2, 23]","The sentiment score is slightly positive (20) because while the reviewer acknowledges the paper as 'well-written' and an 'interesting application', they also describe it as a 'toy application paper' with experiments on 'toy examples'. The reviewer offers compliments on the detail provided but suggests the ideas could have been explained more clearly and compactly. The overall tone is mildly positive with constructive criticism. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, offering compliments and framing criticisms constructively. They use phrases like 'My compliments for...' and provide specific suggestions for improvement without harsh language. The review maintains a professional and courteous tone while still providing honest feedback.",20,50
Deep Variational Information Bottleneck,Accept,2017,"['Alexander A. Alemi', 'Ian Fischer', 'Joshua V. Dillon', 'Kevin Murphy']","[7, 6, 6]","['Good paper, accept', 'Marginally above acceptance threshold', 'Marginally above acceptance threshold']","Update: raised the score, because I think the arguments about adversarial examples are compelling. I think that the paper convincingly proves that this method acts as a decent regularizer, but I*m not convinced that it*s a competitive regularizer. For example, I don*t believe that there is sufficient evidence that it gives a better regularizer than dropout/normalization/etc. I also think that it will be much harder to tune than these other methods (discussed in my rebuttal reply). ---- Summary: If I understand correctly, this paper proposes to take the *bottleneck* term from variational autoencoders which pulls the latent variable towards a noise prior (like N(0,1)) and apply it in a supervised learning context where the reconstruction term log(p(x|z)) is replaced with the usual supervised cross-entropy objective. The argument is that this is an effective regularizer and increases robustness to adversarial attacks. Pros: -The presentation is quite good and the paper is easy to follow. -The idea is reasonable and the relationship to previous work is well described. -The robustness to adversarial examples experiment seems convincing, though I*m not an expert in this area. Is there any way to compare to an external quantitative baseline on robustness to adversarial examples? This would help a lot, since I*m not sure how the method here compares with other regularizers in terms of combatting adversarial examples. For example, if one uses a very high dropout rate, does this confer a comparable robustness to adversarial examples (perhaps at the expense of accuracy)? Cons: -MNIST accuracy results don*t seem very strong, unless I*m missing something. The Maxout paper from ICML 2013 listed many permutation invariant MNIST results with error rates below 1%. So the 1.13% error rate listed here doesn*t necessarily prove that the method is a competitive regularizer. I also suspect that tuning this method to make it work well is harder than other regularizers like dropout. -There are many distinct architectural choices with this method, particularly in how many hidden layers come before and after z. For example, the output could directly follow z, or there could be several layers between z and the output. As far as I can tell the paper says that p(y | z) is a simple logistic regression (i.e. one weight matrix followed by softmax), but it*s not obvious why this choice was made. Did it work best empirically? Other: -I wonder what would happen if you *trained against* the discovered adversarial examples while also using the method from this paper. Would it learn to have a higher variance p(z | x) when presented with an adversarial example?","['4', '4', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[6, 5, 6, 15, 17]","[10, 11, 10, 20, 23]","[5, 113, 4, 20, 625]","[3, 41, 1, 9, 339]","[2, 65, 0, 3, 218]","[0, 7, 3, 8, 68]","The sentiment score is slightly positive (20) because while the reviewer acknowledges some strengths of the paper ('The presentation is quite good', 'The idea is reasonable'), they also express significant reservations ('MNIST accuracy results don't seem very strong', 'I'm not convinced that it's a competitive regularizer'). The overall tone suggests cautious approval with substantial critiques. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, acknowledges positive aspects, and frames criticisms as suggestions or questions rather than direct attacks. Phrases like 'If I understand correctly' and 'unless I'm missing something' show a willingness to consider alternative viewpoints. The reviewer also offers constructive suggestions for improvement, which is a polite way to address weaknesses.",20,60
DeepCoder: Learning to Write Programs,Accept,2017,"['Matej Balog', 'Alexander L. Gaunt', 'Marc Brockschmidt', 'Sebastian Nowozin', 'Daniel Tarlow']","[6, 6]","['Marginally above acceptance threshold', 'Marginally above acceptance threshold']","This is a good paper, well written, that presents a simple but effective approach to predict code properties from input output pairs. The experiments show superiority to the baseline, with speedup factors between one to two orders of magnitude. This is a solid gain! The domain of programs is limited, so there is more work to do in trying such ideas on more difficult tasks. Using neural nets to augment the search is a good starting point and a right approach, instead of generating full complex code. I see this paper as being above the threshold for acceptance.","['4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[3, 2, 8, 11, 12]","[8, 7, 14, 17, 18]","[10, 26, 75, 121, 83]","[4, 13, 41, 69, 46]","[5, 13, 29, 42, 35]","[1, 0, 5, 10, 2]","The sentiment score is 80 (positive) because the reviewer describes the paper as 'good' and 'well written', highlights its effectiveness and superiority to baselines, and explicitly states it's 'above the threshold for acceptance'. The positive tone is consistent throughout, with phrases like 'solid gain' and 'right approach'. The politeness score is 70 (polite) due to the respectful and constructive language used. The reviewer offers praise and acknowledges the paper's strengths without using overly effusive language. They also provide gentle suggestions for future work without being critical. The tone is professional and encouraging throughout.",80,70
DeepDSL: A Compilation-based Domain-Specific Language for Deep Learning,Accept,2017,"['Tian Zhao', 'Xiao Bing Huang', 'Yu Cao']","[7, 6, 8]","['Good paper, accept', 'Marginally above acceptance threshold', 'Top 50% of accepted papers, clear accept']","This paper presents and evaluates a Scala-based deep learning framework called “DeepDSL,” describing the language’s syntactic and performance benefits with respect to existing frameworks. Pros: The use of Scala is unique among deep learning frameworks, to my knowledge, making this framework interesting for Scala users. The fact that Scala compiles to Java and therefore cross-platform support comes for free is also nice. The ability to inspect memory information as shown in Figure 3 is interesting and potentially useful for large networks or situations where memory is limited. DeepDSL compares favorably with existing frameworks in terms of memory use and speed for many common convolutional network architectures. Cons: There appears to be special privileged handling of parameters, gradients, and updates in the compilation process itself (as in Caffe), rather than having gradients/updates as a normal part of the full user-defined computation graph (as in Theano + TensorFlow). This makes certain applications, such as RNNs (which require parameter sharing) and GANs (which require gradients wrt multiple objectives), impossible to implement in DeepDSL without further extension of the underlying API. (Note: I might be wrong about this -- and please correct me if I am -- but all the examples in the paper are nets trained by gradient descent on a single objective, and do not share parameters or access gradients directly.) The paper repeatedly refers to line counts from the verbose Protobuf-based low-level representation of networks in Caffe to demonstrate the compactness of its own syntax. This is misleading as Caffe has officially supported a compact network definition style called “NetSpec” for years -- see a ~15 line definition of AlexNet [1]. Given that, Protobuf is essentially an intermediate representation for Caffe (as with TensorFlow), which happens to have a human-readable text format. DeepDSL is not especially novel when compared with existing frameworks, which is not a problem in and of itself, but some statements misleadingly or incorrectly oversell the novelty of the framework. Some examples: “This separation between network definition and training is an unique advantage of DeepDSL comparing to other tools.” This separation is not unique -- it’s certainly a feature of Caffe where the network definition is its own file, and can be attained in TensorFlow as well (though it’s not the default workflow there). “The difference [between our framework and Theano, TensorFlow, etc.] is that we do not model deep networks as ‘networks’ but as abstract ‘functions’.” There is no notion of a “network” in Theano or TensorFlow (not sure about the others) either -- there are only functions, like in DeepDSL. I asked about this statement, and the response didn’t convince me otherwise. The counterexample given was that in TensorFlow the number of input channels needs to be specified separately for each convolution. This is only true using the low-level API and can easily be worked around with higher-level wrappers like TensorFlow Slim -- e.g., see the definition of AlexNet [2]. It may be true that DeepDSL is more “batteries included” for writing compact network definitions than these other frameworks, but the paper’s claims seem to go beyond this. Overall, the DeepDSL framework seems to have real value in its use of Scala and its memory/speed efficiency as demonstrated by the experiments, but the current version of the paper contains statements that overclaim novelty in ways that are misleading and unfair to existing frameworks. I will consider upgrading my rating if these statements are removed or amended to be more technically precise. [1] https://github.com/BVLC/caffe/blob/master/examples/pycaffe/caffenet.py#L24 [2] https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/nets/alexnet.py#L92 ===================== Update: the authors have revised their paper to address the concerns that I considered grounds for rejection in my review. I*ve upgraded my rating from 5 (below threshold) to 7 (good paper, accept).","['4', '4', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[14, 14, 20]","[20, 19, 26]","[66, 11, 233]","[25, 5, 122]","[4, 1, 28]","[37, 5, 83]","The sentiment score is 50 (slightly positive) because the reviewer initially points out both pros and cons of the paper, and ultimately upgrades their rating from 5 to 7, indicating a positive shift. However, they also highlight several criticisms, which prevents the score from being higher. The politeness score is 70 (fairly polite) because the reviewer uses respectful language throughout, acknowledges potential errors in their understanding ('Note: I might be wrong about this -- and please correct me if I am'), and provides specific examples and references to support their points. They also show willingness to reconsider their rating based on the authors' revisions. The tone is professional and constructive, even when pointing out flaws, which contributes to the politeness score.",50,70
Delving into Transferable Adversarial Examples and Black-box Attacks,Accept,2017,"['Yanpei Liu', 'Xinyun Chen', 'Chang Liu', 'Dawn Song']","[6, 7, 5]","['Marginally above acceptance threshold', 'Good paper, accept', '5']","The paper presents an interesting and very detailed study of targeted and non-targeted adversarial examples in CNNs. I’m on the fence about this paper but am leaning towards acceptance. Such detailed empirical explorations are difficult and time-consuming to construct yet can serve as important stepping stones for future work. I see the length of the paper as a strength since it allows for a very in-depth look into the effectiveness and transferability of different kinds of adversarial examples. There are, however, some concerns: 1) While the length of the paper is a strength in my mind, the key contributions should be made much more clear. As evidenced by my comment earlier, I got confused at some point between the ensemble/non-ensemble method, and about the contribution of the Clarifai evaluation and what I should be focusing on where. I’d strongly suggest a radical revision which more clearly focuses the story: - First, we demonstrate that non-targeted attacks are easy while targeted attacks are hard (evidenced by a key experiment comparing the two; we refer to appendix or later sections for the extensive exploration of e.g., current Section 3) - Thus, we propose an ensemble method that is able to handle targeted attacks much better (evidenced by experiments focusing on the comparison between ensemble and non-ensemble method, both in a controlled setting and on Clarifai) - Also, here are all the other details and explorations. 2) Instead of using ResNet-152, Res-Net-101 and ResNet-50 as three of the five models, it would*ve been better to use one ResNet architecture and the other two, say, AlexNet and Network-in-Network. This would make the ensemble results a lot more compelling.","['3', '3', '3']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[34, 8, 10, 19]","[39, 14, 14, 25]","[96, 93, 66, 440]","[11, 43, 40, 254]","[10, 43, 22, 154]","[75, 7, 4, 32]","The sentiment score is 50 (slightly positive) because the reviewer expresses interest in the paper and leans towards acceptance, acknowledging its strengths and potential impact. However, they also raise concerns, which prevents a higher score. The politeness score is 75 (quite polite) as the reviewer uses respectful language, acknowledges the paper's strengths, and frames criticisms constructively. They use phrases like 'I'd strongly suggest' rather than demanding changes, and the overall tone is professional and considerate. The reviewer balances positive feedback with constructive criticism, maintaining a courteous approach throughout.",50,75
Density estimation using Real NVP,Accept,2017,"['Laurent Dinh', 'Jascha Sohl-Dickstein', 'Samy Bengio']","[8, 8, 7]","['Top 50% of accepted papers, clear accept', 'Top 50% of accepted papers, clear accept', 'Good paper, accept']","This paper presents a clever way of training a generative model which allows for exact inference, sampling and log likelihood evaluation. The main idea here is to make the Jacobian that comes when using the change of variables formula (from data to latents) triangular - this makes the determinant easy to calculate and hence learning possible. The paper nicely presents this core idea and a way to achieve this - by choosing special *routings* between the latents and data such that part of the transformation is identity and part some complex function of the input (a deep net, for example) the resulting Jacobian has a tractable structure. This routing can be cascaded to achieve even more complex transformation. On the experimental side, the model is trained on several datasets and the results are quite convincing, both in sample quality and quantitive measures. I would be very happy to see if this model is useful with other types of tasks and if the resulting latent representation help with classification or inference such as image restoration. In summary - the paper is nicely written, results are quite good and the model is interesting - I*m happy to recommend acceptance.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[5, 9, 31]","[10, 15, 37]","[28, 130, 261]","[14, 52, 155]","[13, 74, 60]","[1, 4, 46]","The sentiment score is 90 because the reviewer expresses strong positive sentiment throughout the review. They describe the paper as presenting a 'clever way' of training a model, praise the 'nicely presents' core idea, and state that the results are 'quite convincing'. The reviewer concludes by saying they are 'happy to recommend acceptance', which is a clear positive endorsement. The politeness score is 80 because the reviewer uses respectful and encouraging language throughout. They acknowledge the authors' work positively without being overly effusive or informal. The reviewer offers constructive suggestions for future work in a polite manner, showing interest in the potential applications of the model. The tone is professional and courteous throughout, maintaining a balance between praise and objective assessment.",90,80
Designing Neural Network Architectures using Reinforcement Learning,Accept,2017,"['Bowen Baker', 'Otkrist Gupta', 'Nikhil Naik', 'Ramesh Raskar']","[6, 6, 6]","['Marginally above acceptance threshold', 'Marginally above acceptance threshold', 'Marginally above acceptance threshold']","This paper introduces a reinforcement learning framework for designing a neural network architecture. For each time-step, the agent picks a new layer type with corresponding layer parameters (e.g., #filters). In order to reduce the size of state-action space, they used a small set of design choices. Strengths: - A novel approach for automatic design of neural network architectures. - Shows quite promising results on several datasets (MNIST, CIFAR-10). Weakness: - Limited architecture design choices due to many prior assumptions (e.g., a set of possible number of convolution filters, at most 2 fully-connected layers, maximum depth, hard-coded dropout, etc.) - The method is demonstrated in tabular Q-learning setting, but it is unclear whether the proposed method would work in a large state-action space. Overall, this is an interesting and novel approach for neural network architecture design, and it seems to be worth publication despite some weaknesses.","['4', '3', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[2, 6, 9, 20]","[8, 11, 15, 26]","[16, 30, 41, 410]","[6, 9, 16, 223]","[9, 16, 20, 98]","[1, 5, 5, 89]","The sentiment score is 60 (positive) because the reviewer acknowledges the paper's novel approach and promising results, stating it 'seems to be worth publication despite some weaknesses.' The overall tone is supportive, with clear recognition of the paper's strengths. However, it's not extremely positive due to the mentioned weaknesses. The politeness score is 50 (slightly polite) because the reviewer uses neutral, professional language throughout. They present both strengths and weaknesses objectively without harsh criticism. The use of phrases like 'interesting and novel approach' and 'quite promising results' adds a polite, encouraging tone. However, the review doesn't go out of its way to be exceptionally polite, maintaining a balanced, professional demeanor.",60,50
Dialogue Learning With Human-in-the-Loop,Accept,2017,"['Jiwei Li', 'Alexander H. Miller', 'Sumit Chopra', ""Marc'Aurelio Ranzato"", 'Jason Weston']","[7, 6, 5]","['Good paper, accept', 'Marginally above acceptance threshold', '5']","As discussed, the there are multiple concurrent contributions in different packages/submission by the authors that are in parts difficult to disentangle. Despite this fact, it is impressive to see a system learning from natural feedback in an online fashion. To the best of my knowledge, this is a new quality of result that was achieved - in particular as close to full supervision results are reached in some cases in this less constraint setting. several points were raised that were in turn addressed by the authors: 1. formalisation of the task (learning dialogue) is not precise. when can we declare success? The answer of the authors is partially satisfying. For this particular work, it might make sense to more precisely set goals e.g. to be as good as full supervision. 2. (along the line of the previous question:) dialogue can be seen as a form of noisy supervision. can you please report the classic supervision baselines for the particular model used? this would give a sense what fraction of the best case performance is achieved via dialogue learning. The authors provided additional information along those lines - and I think this helps to understand how much of the overall goal was achieved and open challenges. 3. is there an understanding of how much more difficult the MT setting is? feedback could be hand labeled as positive or negative for an analysis (?). or a handcrafted baseline could be tested, that either extracts the reward via template matching … or maybe even uses the length of the feedback as a proxy/baseline. (it looks to me that short feedback is highly correlated with high reward / correct answer (?)) The authors replied - but it would have been clearer if they could have quantified such suggested baseline, in order to confirm that there is no simple handcrafted baseline that would do well on the data - but these concerns are marginal. 4. relation to prior work Weston’16 is not fully clear. I understand that this submission should be understood as an independent submission of the prior work Weston’16 - and not replacing it. In this case Weston’16 makes this submission appear more incremental. my understanding is that the punch line of this submission is the online part that leads in turn to more exploration. Is there any analysis on how much this aspect matters? I couldn’t find this in the experiments. The authors clarified the raised issues. The application of reinforcement learning and in particular FP is convincing. There is a incremental nature to the paper - and the impression is emphasised by multiple concurrent contributions of the authors on this research thread. Comparison to prior work (in particular Weston*16), should be made more explicit. Not only in text but also in the experiments - as the authors partially do in their reply to the reviewers question. Nevertheless, this particular contribution is assessed as significant and worth sharing and seems likely to have impact on how we can learn in these less constraint setting.","['4', '3', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[12, 2, 17, 12, 19]","[18, 7, 23, 18, 25]","[87, 24, 44, 106, 237]","[36, 11, 29, 57, 126]","[23, 13, 11, 44, 84]","[28, 0, 4, 5, 27]","The sentiment score is 60 (moderately positive) because the reviewer expresses admiration for the system's ability to learn from natural feedback online, calling it 'impressive' and a 'new quality of result'. They also note that the work is 'significant and worth sharing'. However, they raise several concerns and note the incremental nature of the work, which prevents a higher score. The politeness score is 70 (fairly polite) because the reviewer uses respectful language throughout, acknowledges the authors' responses positively (e.g., 'The authors clarified the raised issues'), and frames criticisms constructively (e.g., 'it would have been clearer if...'). The reviewer also uses phrases like 'to the best of my knowledge' and 'my understanding is', which show humility and openness to correction. The score is not higher because the review maintains a professional tone rather than being overtly friendly or deferential.",60,70
Diet Networks: Thin Parameters for Fat Genomics,Accept,2017,"['Adriana Romero', 'Pierre Luc Carrier', 'Akram Erraqabi', 'Tristan Sylvain', 'Alex Auvolat', 'Etienne Dejoie', 'Marc-André Legault', 'Marie-Pierre Dubé', 'Julie G. Hussin', 'Yoshua Bengio']","[6, 7, 8]","['Marginally above acceptance threshold', 'Good paper, accept', 'Top 50% of accepted papers, clear accept']","The paper presents an application of deep learning to genomic SNP data with a comparison of possible approaches for dealing with the very high data dimensionality. The approach looks very interesting but the experiments are too limited to draw firm conclusions about the strengths of different approaches. The presentation would benefit from more precise math. Quality: The basic idea of the paper is interesting and the applied deep learning methodology appears reasonable. The experimental evaluation is rather weak as it only covers a single data set and a very limited number of cross validation folds. Given the significant variation in the performances of all the methods, it seems the differences between the better-performing methods are probably not statistically significant. More comprehensive empirical validation could clearly strengthen the paper. Clarity: The writing is generally good both in terms of the biology and ML, but more mathematical rigour would make it easier to understand precisely what was done. The different architectures are explained on an intuitive level and might benefit from a clear mathematical definition. I was ultimately left unsure of what the *raw end2end* model is - given so few parameters it cannot work on raw 300k dimensional input but I could not figure out what kind of embedding was used. The results in Fig. 3 might be clearer if scaled so that maximum for each class is 1 to avoid confounding from different numbers of subjects in different classes. In the text, please use the standard italics math font for all symbols such as N, N_d, ... Originality: The application and the approach appear quite novel. Significance: There is clearly strong interest for deep learning in the genomics area and the paper seeks to address some of the major bottlenecks here. It is too early to tell whether the specific techniques proposed in the paper will be the ultimate solution, but at the very least the paper provides interesting new ideas for others to work on. Other comments: I think releasing the code as promised would be a must.","['4', '3', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[7, 6, 2, 2, 3, 2, 5, 12, 2, 30]","[12, 10, 7, 8, 9, 2, 11, 17, 2, 36]","[44, 15, 12, 22, 16, 2, 8, 10, 2, 975]","[20, 6, 6, 7, 6, 1, 1, 3, 1, 405]","[21, 6, 6, 14, 7, 1, 3, 2, 1, 454]","[3, 3, 0, 1, 3, 0, 4, 5, 0, 116]","The sentiment score is slightly positive (20) because the reviewer acknowledges the interesting aspects of the paper and its potential significance, but also points out several limitations and areas for improvement. The reviewer sees promise in the approach but feels the experiments are too limited to draw firm conclusions. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, offering constructive criticism and suggestions for improvement without harsh or dismissive comments. The reviewer balances positive remarks with areas of concern, maintaining a professional and courteous tone. The use of phrases like 'The approach looks very interesting' and 'The writing is generally good' contribute to the polite tone, while still clearly communicating the need for improvements in areas such as experimental evaluation and mathematical rigor.",20,60
Discrete Variational Autoencoders,Accept,2017,['Jason Tyler Rolfe'],"[9, 8]","['9', 'Top 50% of accepted papers, clear accept']","This paper presents a way of training deep generative models with discrete hidden variables using the reparameterization trick. It then applies it to a particular DBN-like architecture, and shows that this architecture achieves state-of-the-art density modeling performance on MNIST and similar datasets. The paper is well written, and the exposition is both thorough and precise. There are several appendices which justify various design decisions in detail. I wish more papers in our field would take this degree of care with the exposition! The log-likelihood results are quite strong, especially given that most of the competitive algorithms are based on continuous latent variables. Probably the main thing missing from the experiments is some way to separate out the contributions of the architecture and the inference algorithm. (E.g., what if a comparable architecture is trained with VIMCO, or if the algorithm is applied to a previously published discrete architecture?) I’m a bit concerned about the variance of the gradients in the general formulation of the algorithm. See my comment “variance of the derivatives of F^{-1}” below. I think the response is convincing, but the problem (as well as “engineering principles” for the smoothing distribution) are probably worth pointing out in the paper itself, since the problem seems likely to occur unless the user is aware of it. (E.g., my proposal of widely separated normals would be a natural distribution to consider until one actually works through the gradients — something not commonly done in the age of autodiff frameworks.) Another concern is how many sequential operations are needed for inference in the RBM model. (Note: is this actually an RBM, or a general Boltzmann machine?) The q distribution takes the form of an autoregressive model where the variables are processed one at a time. Section 3 mentions the possibility of grouping together variables in the q distribution, and this is elaborated in detail in Appendix A. But the solution requires decomposing the joint into a product of conditionals and applying the CDFs sequentially. So either way, it seems like we’re stuck handling all the variables sequentially, which might get expensive. Minor: the second paragraph of Section 3 needs a reference to Appendix A.","['4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[-3, 8]","[3, 14]","[5, 202]","[3, 109]","[2, 78]","[0, 15]","The sentiment score is 80 (positive) because the reviewer expresses strong approval of the paper's methodology, writing quality, and results. They use phrases like 'well written', 'thorough and precise', and praise the 'degree of care with the exposition'. The reviewer also notes 'strong' log-likelihood results. While they do raise some concerns, these are presented as areas for improvement rather than major flaws. The politeness score is 90 (very polite) due to the reviewer's consistently respectful and constructive tone. They use phrases like 'I wish more papers in our field would take this degree of care' and offer suggestions in a non-confrontational manner. The reviewer also acknowledges when their concerns are adequately addressed, showing a willingness to engage in productive dialogue.",80,90
Distributed Second-Order Optimization using Kronecker-Factored Approximations,Accept,2017,"['Jimmy Ba', 'Roger Grosse', 'James Martens']","[7, 6]","['Good paper, accept', 'Marginally above acceptance threshold']","In this paper, the authors present a partially asynchronous variant of the K-FAC method. The authors adapt/modify the K-FAC method in order to make it computationally tractable for optimizing deep neural networks. The method distributes the computation of the gradients and the other quantities required by the K-FAC method (2nd order statistics and Fisher Block inversion). The gradients are computed in synchronous manner by the ‘gradient workers’ and the quantities required by the K-FAC method are computed asynchronously by the ‘stats workers’ and ‘additional workers’. The method can be viewed as an augmented distributed Synchronous SGD method with additional computational nodes that update the approximate Fisher matrix and computes its inverse. The authors illustrate the performance of the method on the CIFAR-10 and ImageNet datasets using several models and compare with synchronous SGD. The main contributions of the paper are: 1) Distributed variant of K-FAC that is efficient for optimizing deep neural networks. The authors mitigate the computational bottlenecks of the method (second order statistic computation and Fisher Block inverses) by asynchronous updating. 2) The authors propose a “doubly-factored” Kronecker approximation for layers whose inputs are too large to be handled by the standard Kronecker-factored approximation. They also present (Appendix A) a cheaper Kronecker factored approximation for convolutional layers. 3) Empirically illustrate the performance of the method, and show: - Asynchronous Fisher Block inversions do not adversely affect the performance of the method (CIFAR-10) - K-FAC is faster than Synchronous SGD (with and without BN, and with momentum) (ImageNet) - Doubly-factored K-FAC method does not deteriorate the performance of the method (ImageNet and ResNet) - Favorable scaling properties of K-FAC with mini-batch size Pros: - Paper presents interesting ideas on how to make computationally demanding aspects of K-FAC tractable. - Experiments are well thought out and highlight the key advantages of the method over Synchronous SGD (with and without BN). Cons: - “…it should be possible to scale our implementation to a larger distributed system with hundreds of workers.” The authors mention that this should be possible, but fail to mention the potential issues with respect to communication, load balancing and node (worker) failure. That being said, as a proof-of-concept, the method seems to perform well and this is a good starting point. - Mini-batch size scaling experiments: the authors do not provide validation curves, which may be interesting for such an experiment. Keskar et. al. 2016 (On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima) provide empirical evidence that large-batch methods do not generalize as well as small batch methods. As a result, even if the method has favorable scaling properties (in terms of mini-batch sizes), this may not be effective. The paper is clearly written and easy to read, and the authors do a good job of communicating the motivation and main ideas of the method. There are a few minor typos and grammatical errors. Typos: - “updates that accounts for” — “updates that account for” - “Kronecker product of their inverse” — “Kronecker product of their inverses” - “where P is distribution over” — “where P is the distribution over” - “back-propagated loss derivativesas” — “back-propagated loss derivatives as” - “inverse of the Fisher” — “inverse of the Fisher Information matrix” - “which amounts of several matrix” — “which amounts to several matrix” - “The diagram illustrate the distributed” — “The diagram illustrates the distributed” - “Gradient workers computes” — “Gradient workers compute” - “Stat workers computes” — “Stat workers compute” - “occasionally and uses stale values” — “occasionally and using stale values” - “The factors of rank-1 approximations” — “The factors of the rank-1 approximations” - “be the first singular value and its left and right singular vectors” — “be the first singular value and the left and right singular vectors … , respectively.” - “Psi is captures” — “Psi captures” - “multiplying the inverses of the each smaller matrices” — “multiplying the inverses of each of the smaller matrices” - “which is a nested applications of the reshape” — “which is a nested application of the reshape” - “provides a computational feasible alternative” — “provides a computationally feasible alternative” - “according the geometric mean” — “according to the geometric mean” - “analogous to shrink” — “analogous to shrinking” - “applied to existing model-specification code” — “applied to the existing model-specification code” - “: that the alternative parametrization” — “: the alternative parameterization” Minor Issues: - In paragraph 2 (Introduction) the authors mention several methods that approximate the curvature matrix. However, several methods that have been developed are not mentioned. For example: 1) (AdaGrad) Adaptive Subgradient Methods for Online Learning and Stochastic Optimization (http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf) 2) Stochastic Quasi-Newton Methods for Nonconvex Stochastic Optimization (https://arxiv.org/abs/1607.01231) 3) adaQN: An Adaptive Quasi-Newton Algorithm for Training RNNs (http://link.springer.com/chapter/10.1007/978-3-319-46128-1_1) 4) A Self-Correcting Variable-Metric Algorithm for Stochastic Optimization (http://jmlr.org/proceedings/papers/v48/curtis16.html) 5) L-SR1: A Second Order Optimization Method for Deep Learning (https://openreview.net/pdf?id=By1snw5gl) - Page 2, equation s = WA, is there a dimension issue in this expression? - x-axis for top plots in Figures 3,4,5,7 (Updates x XXX) appear to be a headings for the lower plots. - “James Martens. Deep Learning via Hessian-Free Optimization” appears twice in References section.","['4', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[7, 11, 8]","[13, 17, 14]","[96, 123, 38]","[46, 60, 19]","[49, 61, 16]","[1, 2, 3]","The sentiment score is 70 (positive) because the reviewer begins by summarizing the paper's contributions in a neutral tone, then lists several 'Pros' that highlight the paper's strengths. The few 'Cons' mentioned are presented as constructive feedback rather than severe criticisms. The reviewer also compliments the paper as 'clearly written and easy to read'. The politeness score is 80 (quite polite) because the reviewer uses respectful language throughout, acknowledges the authors' efforts, and frames criticisms constructively. The reviewer also provides detailed, helpful feedback including a list of typos and minor issues, which shows engagement with the paper. The tone remains professional and courteous throughout, avoiding harsh or dismissive language.",70,80
Do Deep Convolutional Nets Really Need to be Deep and Convolutional?,Accept,2017,"['Gregor Urban', 'Krzysztof J. Geras', 'Samira Ebrahimi Kahou', 'Ozlem Aslan', 'Shengjie Wang', 'Abdelrahman Mohamed', 'Matthai Philipose', 'Matt Richardson', 'Rich Caruana']","[7, 7]","['Good paper, accept', 'Good paper, accept']","Description. This paper describes experiments testing whether deep convolutional networks can be replaced with shallow networks with the same number of parameters without loss of accuracy. The experiments are performed on he CIFAR 10 dataset where deep convolutional teacher networks are used to train shallow student networks using L2 regression on logit outputs. The results show that similar accuracy on the same parameter budget can be only obtained when multiple layers of convolution are used. Strong points. - The experiments are carefully done with thorough selection of hyperparameters. - The paper shows interesting results that go partially against conclusions from the previous work in this area (Ba and Caruana 2014). - The paper is well and clearly written. Weak points: - CIFAR is still somewhat toy dataset with only 10 classes. It would be interesting to see some results on a more challenging problem such as ImageNet. Would the results for a large number of classes be similar? Originality: - This is mainly an experimental paper, but the question it asks is interesting and worth investigation. The experimental results are solid and provide new insights. Quality: - The experiments are well done. Clarity: - The paper is well written and clear. Significance: - The results go against some of the conclusions from previous work, so should be published and discussed. Overall: Experimental paper with interesting results. Well written. Solid experiments.","['3', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[3, 6, 8, 9, 7, 8, 22, 22, 31]","[8, 11, 14, 13, 13, 14, 26, 27, 37]","[9, 50, 52, 10, 50, 112, 81, 56, 135]","[2, 16, 22, 7, 26, 50, 58, 40, 89]","[2, 27, 29, 3, 13, 54, 6, 8, 32]","[5, 7, 1, 0, 11, 8, 17, 8, 14]","The sentiment score is 80 (positive) because the review highlights several strong points and praises the paper's clarity, experimental design, and significance. The reviewer uses phrases like 'carefully done,' 'interesting results,' and 'well written,' indicating a positive overall impression. The politeness score is 70 (polite) as the reviewer maintains a professional and respectful tone throughout, balancing praise with constructive criticism. They use phrases like 'it would be interesting to see' when suggesting improvements, rather than demanding changes. The review concludes with a positive summary, further reinforcing the polite and encouraging tone.",80,70
Dropout with Expectation-linear Regularization,Accept,2017,"['Xuezhe Ma', 'Yingkai Gao', 'Zhiting Hu', 'Yaoliang Yu', 'Yuntian Deng', 'Eduard Hovy']","[8, 7, 8]","['Top 50% of accepted papers, clear accept', 'Good paper, accept', 'Top 50% of accepted papers, clear accept']","summary The paper explains dropout with a latent variable model where the dropout variable (0 or 1 depending on which units should be dropped) is not observed and is accordingly marginalised. Maximum likelihood under this model is not tractable but standard dropout then corresponds to a simple Monte Carlo approximation of ML for this model. The paper then introduces a theoretical framework for analysing the discrepancy (called inference gap) between the model at training (model ensemble, or here the latent variable model), and the model at testing (where usually what should be an expectation over the activations over many models becomes the activation of one model with averaged weights). This framework introduces several notions (e.g. expectation linearity) which allow the study of which transition functions (and more generally layers) can have a small inference gap. Theorem 3 gives a bound on the inference gap. Finally a new regularisation term is introduced to account for minimisation of the inference gap during learning. Experiments are performed on MNIST, CIFAR-10 and CIFAR-100 and show that the method has the potential to perform better than standard dropout and at the level of Monte Carlo Dropout (the standard method to compute the real dropout outputs consistently with the training assumption of an ensemble, of course quite expensive computationally) The study gives a very interesting theoretical model for dropout as a latent variable model where standard dropout is then a monte carlo approximation. This is very probably widely applicable to further studies of dropout. The framework for the study of the inference gap is interesting although maybe somewhat less widely applicable. The proposed model is convincing although 1. it is tested on simple datasets 2. the gains are relatively small and 3. there is an increased computational cost during training because a new hyper-parameter is introduced. p6 line 8 typo: expecatation","['3', '4', '3']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[8, 3, 5, 11, 3, 33]","[14, 4, 11, 17, 8, 39]","[79, 4, 112, 114, 54, 517]","[37, 2, 51, 64, 23, 350]","[42, 2, 54, 39, 29, 123]","[0, 0, 7, 11, 2, 44]","The sentiment score is 60 (positive) because the reviewer describes the paper as 'very interesting' and 'convincing', highlighting its theoretical contributions and potential applications. However, it's not extremely positive due to some limitations mentioned, such as simple datasets, small gains, and increased computational cost. The politeness score is 50 (somewhat polite) because the reviewer uses respectful language throughout, acknowledging the paper's strengths while also providing constructive criticism. The tone is professional and balanced, without any harsh or rude comments. The reviewer points out a typo politely at the end. The language is more neutral than overtly polite, hence the moderate positive score.",60,50
Dynamic Coattention Networks For Question Answering,Accept,2017,"['Caiming Xiong', 'Victor Zhong', 'Richard Socher']","[8, 8, 8]","['Top 50% of accepted papers, clear accept', 'Top 50% of accepted papers, clear accept', 'Top 50% of accepted papers, clear accept']","Paper Summary: The paper introduces a question answering model called Dynamic Coattention Network (DCN). It extracts co-dependent representations of the document and question, and then uses an iterative dynamic pointing decoder to predict an answer span. The proposed model achieves state-of-the-art performance, outperforming all published models. Paper Strengths: -- The proposed model introduces two new concepts to QA models -- 1) using attention in both directions, and 2) a dynamic decoder which iterates over multiple answer spans until convergence or maximum number of iterations. -- The paper also presents ablation study of the proposed model which shows the importance of their design choices. -- It is interesting to see the same idea of co-attention performing well in 2 different domains -- Visual Question Answering and machine reading comprehension. -- The performance breakdown over document and question lengths (Figure 6) strengthens the importance of attention for QA task. -- The proposed model achieves state-of-the-art result on SQuAD dataset. -- The model architecture has been clearly described. Paper Weaknesses / Future Thoughts: -- The paper provides model*s performance when the maximum number of iterations is 1 and 4. I would like to see how the performance of the model changes with the number of iterations, i.e., the model performance when that number is 2 and 3. Is there a clear trend? What type of questions is the model able to get correct with more iterations? -- As with many deep learning approaches, the overall architecture seems quite complex, and the design choices seem to be driven by performance numbers. As future work, authors might try to analyze qualitative advantages of different choices in the proposed model. What type of questions are correctly answered because of co-attention mechanism instead of attention in a single direction, when using Maxout Highway Network instead of a simple MLP, etc? Preliminary Evaluation: Novel and state-of-the-art question answering approach. Model is clearly described in detail. In my thoughts, a clear accept.","['4', '3', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[9, 3, 11]","[15, 8, 17]","[383, 37, 229]","[165, 18, 111]","[208, 19, 111]","[10, 0, 7]","The sentiment score is 80 because the review is overwhelmingly positive. The reviewer lists numerous strengths of the paper, including its novel concepts, state-of-the-art performance, and clear model description. The reviewer explicitly states 'a clear accept' in the preliminary evaluation. The only criticisms are framed as suggestions for future work rather than weaknesses. The politeness score is 70 because the reviewer uses respectful and constructive language throughout. They acknowledge the paper's strengths before offering suggestions, and frame their recommendations as 'future thoughts' rather than criticisms. The language is professional and objective, avoiding any harsh or dismissive tones.",80,70
EPOpt: Learning Robust Neural Network Policies Using Model Ensembles,Accept,2017,"['Aravind Rajeswaran', 'Sarvjeet Ghotra', 'Balaraman Ravindran', 'Sergey Levine']","[8, 7]","['Top 50% of accepted papers, clear accept', 'Good paper, accept']","This paper explores ensemble optimisation in the context of policy-gradient training. Ensemble training has been a low-hanging fruit for many years in the this space and this paper finally touches on this interesting subject. The paper is well written and accessible. In particular the questions posed in section 4 are well posed and interesting. That said the paper does have some very weak points, most obviously that all of its results are for a very particular choice of domain+parameters. I eagerly look forward to the journal version where these experiments are repeated for all sorts of source domain/target domain/parameter combinations. <rant Finally a stylistic comment that the authors can feel free to ignore. I don*t like the trend of every paper coming up with a new acronymy wEiRDLY cAsEd name. Especially here when the idea is so simple. Why not use words? English words from the dictionary. Instead of *EPOpt* and *EPOpt-e*, you can write *ensemble training* and *robust ensemble training*. Is that not clearer? />","['4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[3, 2, 20, 9]","[9, 2, 26, 15]","[60, 2, 253, 743]","[25, 1, 142, 326]","[33, 1, 86, 396]","[2, 0, 25, 21]","The sentiment score is 50 (slightly positive) because the reviewer expresses interest in the paper's topic and praises its accessibility and well-posed questions. However, they also point out 'very weak points,' particularly the limited scope of experiments. The politeness score is 70 (fairly polite) as the reviewer uses respectful language throughout, acknowledging the paper's strengths before offering criticisms. They even label their stylistic comment as a 'rant' that the authors can 'feel free to ignore,' showing consideration. The reviewer's tone is professional and constructive, offering suggestions for improvement rather than harsh criticism.",50,70
Efficient Representation of Low-Dimensional Manifolds using Deep Networks,Accept,2017,"['Ronen Basri', 'David W. Jacobs']","[6, 5, 7]","['Marginally above acceptance threshold', '5', 'Good paper, accept']","The paper presents an analysis of the ability of deep networks with ReLU functions to represent particular types of low-dimensional manifolds. Specifically, the paper focuses on what the authors call *monotonic chains of linear segments*, which are essentially sets of intersecting tangent planes. The paper presents a construction that efficiently models such manifolds in a deep net, and presents a basic error analysis of the resulting construction. While the presented results are novel to the best of my knowledge, they are hardly surprising (1) given what we already know about the representational power of deep networks and (2) given that the study selects a deep network architecture and a data structure that are very *compatible*. In particular, I have three main concerns with respect to the results presented in this paper: (1) In the last decade, there has been quite a bit of work on learning data representations from sets of local tangent planes. Examples that spring to mind are local tangent space analysis of Zhang & Zha (2002), manifold charting by Brand (2002) and alignment of local models by Verbeek, Roweis, and Vlassis (2003). None of this work is referred to in related work, even though it seems highly relevant to the analysis presented here. For instance, it would be interesting to see how these old techniques compare to the deep network trained to produce the embedding of Figure 6. This may provide some insight into the inductive biases the deep net introduces: does it learn better representations that non-parametric techniques because it has better inductive biases, or does it learn worse representations because the loss being optimized is non-convex? (2) It is difficult to see how the analysis generalizes to more complex data in which local linearity assumptions on the data manifold are vacuous given the sparsity of data in high-dimensional space, or how it generalizes to deep network architectures that are not pure ReLU networks. For instance, most modern networks use a variant of batch normalization; this already appears to break the presented analyses. (3) The error bound presented in Section 4 appears vacuous for any practical setting, as the upper bound on the error is exponential in the total curvature (a quantity that will be quite large in most practical settings). This is underlined by the analysis of the Swiss roll dataset, of which the authors state that the *bound for this case is very loose*. The fact that the bound is already so loose for this arguably very simple manifold makes that the error analysis may tell us very little about the representational power of deep nets. I would encourage the authors to address issue (1) in the revision of the paper. Issue (2) and (3) may be harder to address, but is essential that they are addressed for the line of work pioneered by this paper to have an impact on our understanding of deep learning. Minor comments: - In prior work, the authors only refer to fully supervised siamese network approaches. These approaches differ from that taken by the authors, as their approach is unsupervised. It should be noted that the authors are not the first to study unsupervised representation learners parametrized by deep networks: other important examples are deep autoencoders (Hinton & Salakhutdinov, 2006 and work on denoising autoencoders from Bengio*s group) and parametric t-SNE (van der Maaten, 2009). - What loss do the authors use in their experiments? Using *the difference between the ground truth distance ... and the distance computed by the network* seems odd, because it encourages the network to produce infinitely large distances (to get a loss of minus infinity). Is the difference squared?","['3', '3', '5']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[30, 27]","[35, 33]","[159, 186]","[77, 97]","[34, 46]","[48, 43]","The sentiment score is -50 because the reviewer expresses several significant concerns about the paper, including lack of reference to relevant prior work, limited generalizability, and potentially vacuous error bounds. However, they do acknowledge the novelty of the results, which prevents the score from being even lower. The politeness score is 20 because the reviewer uses generally respectful language and offers constructive criticism. They use phrases like 'I would encourage the authors' and 'It should be noted,' which are polite ways of suggesting improvements. However, the criticism is direct and doesn't employ overly deferential language, keeping the score from being higher.",-50,20
Efficient Vector Representation for Documents through Corruption,Accept,2017,['Minmin Chen'],"[7, 6, 7]","['Good paper, accept', 'Marginally above acceptance threshold', 'Good paper, accept']","This paper discusses a method for computing vector representations for documents by using a skip-gram style learning mechanism with an added regularizer in the form of a global context vector with various bits of drop out. While none of the individual components proposed in this paper are new, I believe that the combination in this fashion is. Further, I appreciated the detailed analysis of model behaviour in section 3. The main downside to this submission is in its relative weakness on the empirical front. Arguably there are more interesting tasks than sentiment analysis and k-way classification! Likewise, why waste 2/3 of a page on t-sne projections rather than use that space for further analysis? While I am a bit disappointed by this reduced evaluation and agree with the other reviewers concerning soft baselines, I think this paper should be accepted: it*s an interesting algorithm, nicely composed and very efficient, so it*s reasonable to assume that other readers might have use for some of the ideas presented here.","['3', '4', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']",[10],[16],[61],[37],[19],[5],"The sentiment score is 50 (mildly positive) because the reviewer expresses appreciation for aspects of the paper and recommends acceptance, despite noting some weaknesses. They use phrases like 'I appreciated' and 'it's an interesting algorithm, nicely composed' which indicate positive sentiment. However, they also mention disappointments and weaknesses, balancing out the overall sentiment to a moderately positive level. The politeness score is 75 (quite polite) because the reviewer uses respectful language throughout, acknowledging both strengths and weaknesses in a constructive manner. They use phrases like 'I believe' and 'I think' to soften criticisms, and offer specific suggestions for improvement rather than harsh critiques. The tone is professional and considerate, maintaining politeness even when discussing the paper's shortcomings.",50,75
Emergence of foveal image sampling from learning to attend in visual scenes,Accept,2017,"['Brian Cheung', 'Eric Weiss', 'Bruno Olshausen']","[6, 5, 6]","['Marginally above acceptance threshold', '5', 'Marginally above acceptance threshold']","This paper presents a succinct argument that the principle of optimizing receptive field location and size in a simulated eye that can make saccades with respect to a classification error of images of data whose labels depend on variable-size and variable-location subimages, explains the existence of a foveal area in e.g. the primate retina. The argument could be improved by using more-realistic image data and drawing more direct correspondence with the number, receptive field sizes and eccentricities of retinal cells in e.g. the macaque, but the authors would then face the challenge of identifying a loss function that is both biologically plausible and supportive of their claim. The argument could also be improved by commenting on the timescales involved. Presumably the density of the foveal center depends on the number of of saccades allowed by the inference process, as well as the size of the target sub-images, and also has an impact on the overall classification accuracy. Why does the classification error rate of dataset 2 remain stubbornly at 24%? This seems so high that the model may not be working the way we’d like it to. It seems that the overall argument of the paper pre-supposes that the model can be trained to be a good classifier. If there are other training strategies or other models that work better and differently, then it raises the question of why do our eyes and visual cortex not work more like *those ones* if evolutionary pressures are applying the same pressure as our training objective. Why does the model with zooming powers out-do the translation-only model on dataset 1 (where all target images are the same size) and tie the translation-only model dataset 2 (where the target images have different sizes, for which the zooming model should be tailor-made?). Between this strange tie and the high classification rate on Dataset 2, I wonder if maybe one or both models isn’t being trained to its potential, which would undermine the overall claim. Comparing this model to other attention models (e.g. spatial transformer networks, DRAW) would be irrelevant to what I take to be the main point of the paper, but it would address the potential concerns above that training just didn’t go very well, or there was some problem with the model parameterization that could be easily fixed.","['4', '4', '5']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[10, 66, 25]","[16, 66, 31]","[41, 5, 83]","[21, 3, 36]","[14, 1, 31]","[6, 1, 16]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the paper's succinct argument, they also point out several areas for improvement and raise concerns about the model's performance and the overall claim. The reviewer suggests that the argument could be strengthened with more realistic data, better correspondence with biological systems, and addressing issues with classification error rates and model comparisons. These critiques indicate a somewhat skeptical view of the paper's current state.

The politeness score is moderately positive (50) as the reviewer maintains a professional and constructive tone throughout. They begin by acknowledging the paper's main argument and use phrases like 'could be improved' rather than harsh criticisms. The reviewer asks questions to prompt further thought and explanation rather than making accusatory statements. However, the score is not higher because the review doesn't include explicitly positive or encouraging language, focusing mainly on areas for improvement.",-20,50
Energy-based Generative Adversarial Networks,Accept,2017,"['Junbo Zhao', 'Michael Mathieu', 'Yann LeCun']","[8, 7, 7]","['Top 50% of accepted papers, clear accept', 'Good paper, accept', 'Good paper, accept']","This paper is a parallel work to Improving Generative Adversarial Networks with Denoising Feature Matching. The main solution of both papers is introducing autoencoder into discriminator to improve the stability and quality of GAN. Different to Denoising Feature Matching, EBGAN uses encoder-decoder instead of denoising only, and use hingle loss to replace original loss function. The theoretical results are good, and empirical result of high resolution image is unique among all recent GAN advantages. I suggest to introduce Improving Generative Adversarial Networks with Denoising Feature Matching as related work.","['3', '5', '3']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is fairly confident that the evaluation is correct']","[2, 3, 9, 33, 17]","[7, 9, 15, 39, 23]","[51, 24, 51, 517, 309]","[25, 8, 25, 350, 135]","[26, 16, 25, 123, 160]","[0, 0, 1, 44, 14]","The sentiment score is 70 (positive) because the reviewer acknowledges the paper's good theoretical results and unique empirical results for high-resolution images. They also suggest including related work, which implies the paper is worthy of publication with minor revisions. The politeness score is 60 (moderately polite) as the reviewer uses respectful language throughout, acknowledging the paper's strengths and offering a constructive suggestion without any harsh criticism. The tone is professional and supportive, though not overly formal or effusive in its politeness.",70,60
Entropy-SGD: Biasing Gradient Descent Into Wide Valleys,Accept,2017,"['Pratik Chaudhari', 'Anna Choromanska', 'Stefano Soatto', 'Yann LeCun', 'Carlo Baldassi', 'Christian Borgs', 'Jennifer Chayes', 'Levent Sagun', 'Riccardo Zecchina']","[8, 6, 7, 9]","['Top 50% of accepted papers, clear accept', 'Marginally above acceptance threshold', 'Good paper, accept', '9']","Overview: This paper introduces a biasing term for SGD that, in theoretical results and a toy example, yields solutions with an approximately equal or lower generalization error. This comes at a computational cost of estimating the gradient of the biasing term for each iteration through stochastic gradient Langevin dynamics, approximating an MCMC sample of the log partition function of a modified Gibbs distribution. The cost is equivalent to adding an inner for-loop to the standard SGD algorithm for each minibatch. Pros: - Reviews and distills many results and theorems from past 2 decades that suggest a promising way forward for increasing the generalizability of deep neural networks - Generally very well written and well presented results, with interesting discussion of eigenvalues of Hessian as a way to characterize “flat” minima - Promising mathematical arguments suggest that E-SGD has generalization error bounded below by SGD, motivating further research in the area Cons / points suggested for a rebuttal: (1) One claim of the paper given in the abstract is ”experiments on competitive baselines demonstrate that Entropy-SGD leads to improved generalization and has the potential to accelerate training.“ This does not appear to be supported by the current set of experiments. As the authors comment in the discussion section, “In our experiments, Entropy-SGD results in a comparable generalization error as SGD, but always has a lower cross-entropy loss.” It*s not clear to me how to reconcile those two claims. (2) Similarly, the claim of accelerated training is not convincingly supported in the present version of the paper. Vanilla SGD requires a single forward pass through all M minibatches during one epoch for a parameter update, but the new method, E-SGD requires, L*M forward passes during one epoch where L is the number of Langevin updates, which require a minibatch sample each. This could in fact mean that E-SGD has worse computational complexity to reach the same point. In a remark on p.9, the authors note that a single epoch is defined to be “the number of parameter updates required to run through the dataset once.” It’s not clear to me how this answers the objection to a factor of L additional computations required for the inner-loop SGLD iterations. SGLD appears to introduces a potentially costly tradeoff that must be carefully managed by a user of E-SGD. (3) As the previous two points suggest, the paper could use some attention to the magnitude of the claims. For example, the introduction reads “Actively biasing towards wide valleys aids generalization, in fact, we can optimize solely the free energy term to obtain similar generalization error as SGD on the original loss function.“ According the the values reported on pp.9-10, only on MNIST is the generalization error, using only the free energy term (the log partition function of the modified Gibbs distribution), equivalent to using only the SGD loss function. This corresponds to setting rho to 0 in equation (6). On CIFAR-10, rho = 0.01 is used. (4) Another contribution of this paper, the characterization of the optimization landscape in terms of the eigenvalues of the Hessian and low generalization error being associated with flat local extrema, is helpful and interesting. I found the plots clear and useful. As another reviewer has already pointed out, there are high-level similarities to “Flat Minima” by Hochreiter and Schmidhuber (1997). The authors have responded already by adding a paragraph that helpfully explores some differences with H&S 1997. However, the similarities should also be carefully identified and mentioned. H&S 1997 includes detailed theoretical analysis that could be helpful for future work in this area, and has independently discovered a similar approach to training generalizable networks. (5) It*s not clear how the assumption about the eigenvalues that were made in section 4.4 / Appendix B affect the application of this result to real-world problems. What magnitude of c>0 needs to be chosen? Does this correspond to a measurable characteristic of the dataset? It*s a little mysterious in the current version of the paper.","['4', '4', '4', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[9, 6, 25, 30, 11, 19, 19, 3, 26]","[15, 11, 31, 36, 17, 25, 25, 8, 32]","[77, 70, 446, 315, 34, 111, 104, 35, 82]","[29, 31, 241, 162, 4, 51, 44, 12, 12]","[42, 32, 139, 113, 21, 34, 32, 23, 49]","[6, 7, 66, 40, 9, 26, 28, 0, 21]","The sentiment score is 50 (slightly positive) because the review begins with a balanced overview, listing both pros and cons. The reviewer acknowledges the paper's contributions and interesting aspects, but also points out several areas for improvement. The tone is generally constructive rather than dismissive. The politeness score is 75 (quite polite) because the reviewer uses respectful language throughout, framing criticisms as suggestions or points for consideration rather than direct attacks. Phrases like 'points suggested for a rebuttal' and 'could use some attention' indicate a considerate approach to feedback. The reviewer also acknowledges the authors' efforts and the paper's strengths before delving into criticisms.",50,75
Episodic Exploration for Deep Deterministic Policies for StarCraft Micromanagement,Accept,2017,"['Nicolas Usunier', 'Gabriel Synnaeve', 'Zeming Lin', 'Soumith Chintala']","[7, 7, 8]","['Good paper, accept', 'Good paper, accept', 'Top 50% of accepted papers, clear accept']","The paper presents a learning algorithm for micromanagement of battle scenarios in real-time strategy games. It focuses on a complex sub-problem of the full RTS problem. The assumptions and restrictions made (greedy MDP, distance-based action encoding, etc.) are clear and make sense for this problem. The main contribution of this paper is the zero-order optimization algorithm and how it is used for structured exploration. This is a nice new application of zero-order optimization meets deep learning for RL, quite well-motivated using similar arguments as DPG. The results show clear wins over vanilla Q-learning and REINFORCE, which is not hard to believe. Although RTS is a very interesting and challenging domain (certainly worthy as a domain of focused research!), it would have been nice to see results on other domains, mainly because it seems that this algorithm could be more generally applicable than just RTS games. Also, evaluation on such a complex domain makes it difficult to predict what other kinds of domains would benefit from this zero-order approach. Maybe the authors could add some text to clarify/motivate this. There are a few seemingly arbitrary choices that are justified only by *it worked in practice*. For example, using only the sign of w / Psi_{theta}(s^k, a^k). Again later: *Also we neglected the argmax operation that chooses the actions*. I suppose this and dividing by t could keep things nicely within or close to [-1,1] ? It might make sense to try truncating/normalizing w/Psi; it seems that much information must be lost when only taking the sign. Also lines such as *We did not extensively experiment with the structure of the network, but we found the maxpooling and tanh nonlinearity to be particularly important* and claiming the importance of adagrad over RMSprop without elaboration or providing any details feels somewhat unsatisfactory and leaves the reader wondering why.. e.g. could these only be true in the RTS setup in this paper? The presentation of the paper can be improved, as some ideas are presented without any context making it unnecessarily confusing. For example, when defining f(	ilde{s}, c) at the top of page 5, the w vector is not explained at all, so the reader is left wondering where it comes from or what its use is. This is explained later, of course, but one sentence on its role here would help contextualize its purpose (maybe refer later to the section where it is described fully). Also page 7: *because we neglected that a single u is sampled for an entire episode*; actually, no, you did mention this in the text above and it*s clear from the pseudo-code too. *perturbated* -> *perturbed* --- After response period: No rebuttal entered, therefore review remains unchanged.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[13, 8, 2, 8]","[19, 14, 7, 14]","[125, 132, 22, 39]","[72, 59, 11, 13]","[44, 67, 9, 23]","[9, 6, 2, 3]","The sentiment score is 50 (slightly positive) because the reviewer acknowledges the paper's contributions and clear wins over other methods, but also points out areas for improvement and some arbitrary choices. The reviewer sees value in the work but has reservations about its generalizability and presentation. The politeness score is 70 (fairly polite) as the reviewer uses respectful language throughout, offering constructive criticism and suggestions for improvement rather than harsh criticism. Phrases like 'it would have been nice to see' and 'Maybe the authors could add' indicate a polite approach to feedback. The reviewer also acknowledges the merits of the work, using phrases like 'nice new application' and 'clear wins', which contributes to the overall polite tone.",50,70
Exploring Sparsity in Recurrent Neural Networks,Accept,2017,"['Sharan Narang', 'Greg Diamos', 'Shubho Sengupta', 'Erich Elsen']","[6, 7]","['Marginally above acceptance threshold', 'Good paper, accept']","The paper proposes a method for pruning weights in neural networks during training to obtain sparse solutions. The approach is applied to an RNN-based system which is trained and evaluated on a speech recognition dataset. The results indicate that large savings in test-time computations can be obtained without affecting the task performance too much. In some cases the method can actually improve the evaluation performance. The experiments are done using a state-of-the-art RNN system and the methodology of those experiments seems sound. I like that the effect of the pruning is investigated for networks of very large sizes. The computational gains are clearly substantial. It is a bit unfortunate that all experiments are done using a private dataset. Even with private training data, it would have been nice to see an evaluation on a known test set like the HUB5 for conversational speech. It would also have been nice to see a comparison with some other pruning approaches given the similarity of the proposed method to the work by Han et al. [2] to verify the relative merit of the proposed pruning scheme. While single-stage training looks more elegant at first sight, it may not save much time if more experiments are needed to find good hyperparameter settings for the threshold adaptation scheme. Finally, the dense baseline would have been more convincing if it involved some model compression tricks like training on the soft targets provided by a bigger network. Overall, the paper is easy to read. The table and figure captions could be a bit more detailed but they are still clear enough. The discussion of potential future speed-ups of sparse recurrent neural networks and memory savings is interesting but not specific to the proposed pruning algorithm. The paper doesn’t motivate the details of the method very well. It’s not clear to me why the threshold has to ramp up after a certain period time for example. If this is based on preliminary findings, the paper should mention that. Sparse neural networks have been the subject of research for a long time and this includes recurrent neural networks (e.g., sparse recurrent weight matrices were standard for echo-state networks [1]). The proposed method is also very similar to the work by Han et al. [2], where a threshold is used to prune weights after training, followed by a retraining phase of the remaining weights. While I think that it is certainly more elegant to replace this three stage procedure with a single training phase, the proposed scheme still contains multiple regimes that resemble such a process by first training without pruning followed by pruning at two different rates and finally training without further pruning again. The main novelty of the work would be the application of such a scheme to RNNs, which are typically more tricky to train than feedforward nets. Improving scalability is an important driving force of the progress in neural network research. While I don’t think the paper presents much novelty in ideas or scientific insight, it does show that weight pruning can be successfully applied to large practical RNN systems without sacrificing much in performance. The fact that this is possible with such a simple heuristic is a result worth sharing. Pros: The proposed method is successful at reducing the number of parameters in RNNs substantially without sacrificing too much in performance. The experiments are done using a state-of-the-art system for a practical application. Cons: The proposed method is very similar to earlier work and barely novel. There is no comparison with other pruning methods. The data is private and this prevents others from replicating the results. [1] Jaeger, H. (2001). The “echo state” approach to analyzing and training recurrent neural networks-with an erratum note. Bonn, Germany: German National Research Center for Information Technology GMD Technical Report, 148, 34. [2] Han, Song, Pool, Jeff, Tran, John, and Dally, William J. Learning both weights and connections for efficient neural networks. In Advances in Neural Information Processing Systems, 2015.","['4', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[3, 10, 4, 12]","[9, 15, 10, 17]","[33, 49, 21, 53]","[8, 26, 9, 24]","[23, 20, 12, 28]","[2, 3, 0, 1]","The sentiment score is slightly positive (20) because the reviewer acknowledges the paper's merits, such as successful application of weight pruning to large RNN systems and substantial computational gains. However, they also point out several limitations, like lack of novelty and comparison with other methods. The overall tone is cautiously positive but with significant reservations. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, acknowledging the paper's strengths ('I like that...', 'The experiments... seem sound') and framing criticisms constructively ('It would have been nice to see...'). The reviewer maintains a professional tone, balancing praise with suggestions for improvement without using harsh or dismissive language.",20,60
FILTER SHAPING FOR CONVOLUTIONAL NEURAL NETWORKS,Accept,2017,"['Xingyi Li', 'Fuxin Li', 'Xiaoli Fern', 'Raviv Raich']","[6, 7, 7]","['Marginally above acceptance threshold', 'Good paper, accept', 'Good paper, accept']","Authors propose a mechanism for selecting the design of filters in convolutional layers. The basic idea is that convolution should be applied to input feature dimensions that are highly correlated in order to detect rare events. For example, adjacent pixels in images are correlated and edges are rare events of interest to be detected. Authors argue that square filters are therefore appropriate in images. However, in data such as bird songs high correlations might exist between non-adjacent harmonics and a convolution filter should take a weighted summation over these input feature dimensions. Such an operation can thus be thought of computing data-dependent dilated convolutions. Paper theoretically motivates this choice using the idea of Gaussian complexity of the learner (i.e. a CNN in this case). The main idea being that choosing convolution filters that sum over correlated features results in lower Gaussian complexity and thus the learner has higher ability to generalize. While I am no expert in theoretical analysis of learning algorithms – there are parts of proof that look sound, but there are parts that are rather hand wavy (for eg, extension to networks using max-pooling from average pooling). Also, the theory is not directly applicable to choosing filters when number of layers are more than 1. I am willing to overlook the paucity in rigor in some parts of the theoretical arguments because the empirical evidence looks convincing. The method of choosing the filter shape can be briefly summarized as: (a) The covariance matrix of the input features is computed. (b) Using the covariance matrix, feature dimensions with highest correlations are determined by solving equation (7). A hard limit on maximum number of filter dimensions is imposed (typically ~ 10-15). This leads to choice of a single design for all filters in the layer. (c) Authors extend the framework to work with multiple layers in the following way: A subset of feature dimensions cannot account for all variance in the inputs and there is some residual variance. The filter design of the next layer attempts to minimize this residual variance. This process is repeated iteratively by solving eq (8) to obtain filter designs for all the layers. Ideally for determining filter designs of different layers – one should have computed the covariance statistics of outputs of the previous layer. However this assumes that filters of the previous layer are already known and this is not computationally feasible to implement. Authors instead use the method described in (c). A question which comes to my mind is – a single feature design is chosen for each layer. Have the authors considered using the process in (c) to chose different filter designs for different filters in the same layer as opposed to using the same filter design for all the filters? Regarding baselines: B1. It would be great to see a comparison with randomly chosen filter designs. Two comparisons could be made – (1a) A single random design is chosen for each layer. (1b) The design of each filter is chosen randomly (i.e. allowing for different designs of filter within each layer). B2. Since the theory is not really applicable to CNNs with more that one layer – I wonder how much of the benefit is obtained by choosing the filter design just in a single layer v/s all the layers. A good comparison would be when filter design of the first layer are chosen using the described method and filters in higher layers are chosen to be square. B3. Authors mention the use L1 regularization in the baselines. Was the L1 penalty cross-validated? If so, then upto what range? Somethings which are unclear: - “exclude data that represent obvious noise” - DFMax mentioned in the supplementary materials Overall I think this is very interesting idea for filter design. The authors have done a fair set of experiments but I would really like to see results of B1, B2 and the answer to question in B3. I have currently set my rating to a weak reject, but I am happy to raise my ratings to – “Good paper, accept” if the authors provide results of experiments and answers to questions in my comments above.","['4', '3', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[2, 11, 15, 19]","[8, 17, 21, 25]","[32, 114, 117, 119]","[12, 63, 72, 79]","[7, 41, 27, 10]","[13, 10, 18, 30]","The sentiment score is slightly negative (-20) because while the reviewer finds the idea interesting, they have set their rating to a 'weak reject' and request additional experiments and clarifications. They express willingness to raise their rating if these requests are addressed, indicating a cautiously negative but open stance. The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, acknowledges their own limitations ('I am no expert...'), and frames criticisms constructively ('It would be great to see...'). They also express willingness to reconsider their rating, which is a polite gesture. The reviewer maintains a professional and courteous tone while providing detailed feedback.",-20,60
Faster CNNs with Direct Sparse Convolutions and Guided Pruning,Accept,2017,"['Jongsoo Park', 'Sheng Li', 'Wei Wen', 'Ping Tak Peter Tang', 'Hai Li', 'Yiran Chen', 'Pradeep Dubey']","[7, 6, 6]","['Good paper, accept', 'Marginally above acceptance threshold', 'Marginally above acceptance threshold']","The paper details an implementation of sparse-full convolutions and a model to work out the potential speed-up of various sparsity levels for CNNs. The first contribution is more about engineering, but the authors make the source code available which is greatly appreciated. The second contribution is perhaps more interesting, as so far pruning methods have focused on saving memory, with very modest speed gains. Imbuing knowledge of running speed into a pruning algorithm seems like the proper way to tackle this problem. The authors are very methodical in how they build the model and evaluate it very thoroughly. It seems that the same idea could be used not just for pruning existing models, but also when building new architectures: selecting layers and their parameters as to achieve an optimal throughput rate. This could make for a nice direction for future work. One point that is missing is some discussion of how transferable the performance model is to GPUs. This would make the technique easier to adopt broadly. Other areas for improvement: The points in Figure 4 are hard to distinguish (e.g. small red circle vs. small red square), and overall the figure could be made bigger; specifying whether the *base learning rate* in Section 3 is the start or end rate of the annealing schedule; typos: *punning* (p.4), *spares* (p.5).","['3', '3', '3']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[11, 2, 26, 29, 15, 16, 39]","[17, 2, 32, 34, 21, 22, 45]","[60, 3, 141, 62, 414, 540, 123]","[28, 1, 63, 27, 261, 311, 57]","[22, 2, 28, 15, 76, 94, 24]","[10, 0, 50, 20, 77, 135, 42]","The sentiment score is 80 (positive) because the reviewer expresses appreciation for the authors' work, describes the contributions as interesting and methodical, and suggests potential future directions, indicating overall approval. The politeness score is 70 (polite) as the reviewer uses respectful language, acknowledges the authors' efforts, and provides constructive feedback. The reviewer points out areas for improvement and minor issues in a tactful manner, without harsh criticism. The positive tone, use of phrases like 'greatly appreciated' and 'very thorough', and the balanced approach to feedback contribute to both the positive sentiment and polite tone of the review.",80,70
Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks,Accept,2017,"['Yossi Adi', 'Einat Kermany', 'Yonatan Belinkov', 'Ofer Lavi', 'Yoav Goldberg']","[8, 8, 8]","['Top 50% of accepted papers, clear accept', 'Top 50% of accepted papers, clear accept', 'Top 50% of accepted papers, clear accept']","The authors present a methodology for analyzing sentence embedding techniques by checking how much the embeddings preserve information about sentence length, word content, and word order. They examine several popular embedding methods including autoencoding LSTMs, averaged word vectors, and skip-thought vectors. The experiments are thorough and provide interesting insights into the representational power of common sentence embedding strategies, such as the fact that word ordering is surprisingly low-entropy conditioned on word content. Exploring what sort of information is encoded in representation learning methods for NLP is an important and under-researched area. For example, the tide of word-embeddings research was mostly stemmed after a thread of careful experimental results showing most embeddings to be essentially equivalent, culminating in *Improving Distributional Similarity with Lessons Learned from Word Embeddings* by Levy, Goldberg, and Dagan. As representation learning becomes even more important in NLP this sort of research will be even more important. While this paper makes a valuable contribution in setting out and exploring a methodology for evaluating sentence embeddings, the evaluations themselves are quite simple and do not necessarily correlate with real-world desiderata for sentence embeddings (as the authors note in other comments, performance on these tasks is not a normative measure of embedding quality). For example, as the authors note, the ability of the averaged vector to encode sentence length is trivially to be expected given the central limit theorem (or more accurately, concentration inequalities like Hoeffding*s inequality). The word-order experiments were interesting. A relevant citation for this sort of conditional ordering procedure is *Generating Text with Recurrent Neural Networks* by Sutskever, Martens, and Hinton, who refer to the conversion of a bag of words into a sentence as *debagging.* Although this is just a first step in better understanding of sentence embeddings, it is an important one and I recommend this paper for publication.","['5', '4', '4']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[3, 10, 5, 7, 12]","[9, 12, 11, 11, 18]","[95, 11, 128, 13, 279]","[36, 5, 55, 5, 130]","[54, 3, 64, 4, 125]","[5, 3, 9, 4, 24]","The sentiment score is 80 because the reviewer expresses a highly positive view of the paper, praising its thoroughness, interesting insights, and importance to the field. They recommend the paper for publication and describe it as making a 'valuable contribution'. However, it's not a perfect 100 as they do note some limitations, such as the simplicity of the evaluations. The politeness score is 90 because the reviewer uses respectful and professional language throughout, acknowledging the authors' work positively. They offer constructive feedback and suggestions for improvement in a courteous manner, without any harsh or critical tone. The high scores in both categories reflect the overall positive and respectful nature of the review.",80,90
FractalNet: Ultra-Deep Neural Networks without Residuals,Accept,2017,"['Gustav Larsson', 'Michael Maire', 'Gregory Shakhnarovich']","[7, 6, 5, 6]","['Good paper, accept', 'Marginally above acceptance threshold', '5', 'Marginally above acceptance threshold']","Looking through the comment section here, I agree to a large degree with the author*s standpoint on many issues discussed. Points (1) through (4) in the authors comment below are, in my opinion, a good summary of the contributions of the paper. While I don*t think those contributions are groundbreaking, I believe they are significant enough to merit acceptance. The reason I am commenting here is because, having looked at several comment sections for this ICLR, I am seeing a general trend that reviews have a strong focus on performance, i.e. reviews tend to be very short and judge papers, to a large degree, on whether they are a few percentage points better or worse than the reported baseline. E.g. see the comments *the experimental evaluation is not convincing, e.g. no improvement on SVHN* or *the effect of drop-path seems to vanish with data augmentation* below. I believe that papers should be judged more on their scientific contributions (see points (1), (2) and (4) below), especially when those papers themselves state that their focus is on those scientific contributions, not on amazing performance. Further, I believe the trend to focus excessively on performance is problematic for a number of reasons: - The Deep Learning community has focused very heavily on a few datasets (MNIST, ImageNet, CIFAR-10, CIFAR-100, SVHN). This means that at any time, a large chunk of the deep learning literature is battling for 5 SOTA titles. Hence, expecting any new model to attain one of those titles is a very high bar. - It is an arbitrary standard. Say the SOTA on ImageNet improves by 2% a year. Then a paper that outperforms by 1% in 2014 would underperform by 1% in 2015. By the performance standard, the same paper with the same ideas and the same scientific merit would have declined drastically in value over that one year. Is that really true? - How does one even draw a *fair comparison* on these standard datasets at this point? The bag of tricks for neural networks includes: drop-out, l2, l1, ensembling, various forms of data augmentation, various forms of normalization and initialization, various non-linearities, various learning rate schedules, various forms of pooling, label smoothing, gradient clipping etc. etc. There are a gazillion ways to eke out fractions of percentage points of performance. And - every single paper has a unique combination of tricks that they use for their model, even though the tricks themselves are unrelated to the model. Hence, the only truly fair comparison would be to compare against every reference model with the exact trick combination that the paper presenting the reference model used, which would take an exorbitant amount of time. What*s worse, many papers do not even report all of the tricks they used. One would have to get the authors code and reverse engineer the model, not to mention slight differences introduced by using e.g. TensorFlow vs. Torch vs. Caffe. In this light, the request from one of the reviewers to have a baseline *against which the improvements can be clearly demonstrated by making isolated changes* seems unrealistic to me. - The ML community should not make excessive fine-tuning of models mandatory for publication. By requiring models to beat SOTA, we force each author to fine-tune their model ad nauseum, which leads to an arms race. To get a publications, authors would spend ever more time fine-tuning their models. This can not only lead to *training on the test set*, but also wastes the time of researcher that could be better spent exploring new ideas. - It gives too much power to bad research. In science, there is always a certain background rate of *bad* results published: either the numbers are outright fake or the experimental protocol was invalid, e.g. someone used the test set as a validation set or someone did an exorbitant number of random reruns and only published the best single result. What*s worse, these *bad* results are far more likely to hold the SOTA title at any given time than a *good* result. By requiring new publications to beat SOTA, we give too much power to bad results. - It punishes authors for reporting many or strong baselines. In this paper, authors were careful to report many recent results. Table 1 is thorough. And now they are criticized for not beating all of those baselines. I have a feeling that if the authors of this paper had been more selective about which baselines they report, i.e. those that they can beat, they would have received higher scores on the paper. I have written an in-depth review for another paper at this conference that used, in my opinion, very weak baselines and ended up getting high reviewer marks. I don*t think that was a coincidence. The same arguments apply, though I think to a lesser degree, to judging models excessively on how many parameters they have or their runtime. However, I agree with reviewers that more information about how models compare in terms of those metrics would enhance this paper. I would like to see a discussion of that in the final version. In general, I think this paper would benefit from an appendix with more details on model and training procedure. I also agree with reviewers that 80 layers, which is the deepest that authors can go while improving test error (Table 3), is not ultra-deep. Hence putting *ultra-deep* in the paper title seems exaggerated and I would recommend scaling back the language. However, I don*t think being ultra-deep (~1000 layers) is necessary, because as Veit et al showed, networks that appear ultra deep might not be ultra deep in practice. Training an 80-layer net that functions at test time without residual connections seems to be enough of an achievement. In summary, I think if a paper makes scientific contribution (see points (1), (2) and (4) below) independent of performance, then competitive performance should be enough for publication, instead of requiring SOTA. I believe this paper achieves that mark.","['3', '4', '5', '5']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[2, 14, 17]","[3, 20, 23]","[10, 64, 123]","[5, 40, 66]","[5, 23, 47]","[0, 1, 10]","The sentiment score is 60 (moderately positive) because the reviewer agrees with the authors' standpoint and believes the paper's contributions merit acceptance, despite not being groundbreaking. They defend the paper against other reviewers' criticisms and argue for its value. The politeness score is 80 (quite polite) because the reviewer uses respectful language throughout, acknowledges other viewpoints, and offers constructive suggestions. They critique the review process in general rather than attacking specific reviewers. The reviewer also uses phrases like 'I believe' and 'in my opinion' to soften their statements.",60,80
Frustratingly Short Attention Spans in Neural Language Modeling,Accept,2017,"['Michał Daniluk', 'Tim Rocktäschel', 'Johannes Welbl', 'Sebastian Riedel']","[7, 7, 7]","['Good paper, accept', 'Good paper, accept', 'Good paper, accept']","This paper focusses on attention for neural language modeling and has two major contributions: 1. Authors propose to use separate key, value, and predict vectors for attention mechanism instead of a single vector doing all the 3 functions. This is an interesting extension to standard attention mechanism which can be used in other applications as well. 2. Authors report that very short attention span is sufficient for language models (which is not very surprising) and propose an n-gram RNN which exploits this fact. The paper has novel models for neural language modeling and some interesting messages. Authors have done a thorough experimental analysis of the proposed ideas on a language modeling task and CBT task. I am convinced with authors’ responses for my pre-review questions. Minor comment: Ba et al., Reed & de Freitas, and Gulcehre et al. should be added to the related work section as well.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[4, 6, 4, 12]","[8, 12, 9, 18]","[13, 129, 37, 233]","[7, 61, 14, 119]","[6, 63, 19, 101]","[0, 5, 4, 13]","The sentiment score is 80 (positive) because the reviewer describes the paper's contributions as 'interesting' and 'novel', and states they are 'convinced' by the authors' responses. The reviewer also mentions a 'thorough experimental analysis' and only includes a 'minor comment' at the end. The politeness score is 60 (moderately polite) because the reviewer uses respectful language throughout, acknowledging the authors' work positively. They offer constructive feedback and suggestions without harsh criticism. The tone is professional and courteous, though not overly formal or effusive in praise, hence the moderate positive score.",80,60
Generalizing Skills with Semi-Supervised Reinforcement Learning,Accept,2017,"['Chelsea Finn', 'Tianhe Yu', 'Justin Fu', 'Pieter Abbeel', 'Sergey Levine']","[6, 7, 8]","['Marginally above acceptance threshold', 'Good paper, accept', 'Top 50% of accepted papers, clear accept']","In supervised learning, a significant advance occurred when the framework of semi-supervised learning was adopted, which used the weaker approach of unsupervised learning to infer some property, such as a distance measure or a smoothness regularizer, which could then be used with a small number of labeled examples. The approach rested on the assumption of smoothness on the manifold, typically. This paper attempts to stretch this analogy to reinforcement learning, although the analogy is somewhat incoherent. Labels are not equivalent to reward functions, and positive or negative rewards do not mean the same as positive and negative labels. Still, the paper makes a worthwhile attempt to explore this notion of semi-supervised RL, which is clearly an important area that deserves more attention. The authors use the term *labeled MDP* to mean the typical MDP framework where the reward function is unknown. They use the confusing term *unlabeled MDP* to mean the situation where the reward is unknown, which is technically not an MDP (but a controlled Markov process). In the classical RL transfer learning setup, the agent is attempting to transfer learning from a source *labeled* MDP to a target *labeled* MDP (where both reward functions are known, but the learned policy is known only in the source MDP). In the semi-supervised RL setting, the target is an *unlabeled* CMP, and the source is both a *labeled* MDP and an *unlabeled* CMP. The basic approach is to use inverse RL to infer the unknown *labels* and then attempt to construct transfer. A further restriction is made to linearly solvable MDPs for technical reasons. Experiments are reported using three relatively complex domains using the Mujoco physics simulator. The work is interesting, but in the opinion of this reviewer, the work fails to provide a simple sufficiently general notion of semi-supervised RL that will be of sufficiently wide interest to the RL community. That remains to be done by a future paper, but in the interim, the work here is sufficiently interesting and the problem is certainly a worthwhile one to study.","['5', '3', '4']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[5, 2, 3, 16, 9]","[11, 8, 8, 22, 15]","[306, 50, 34, 608, 744]","[128, 22, 15, 291, 326]","[172, 24, 18, 291, 397]","[6, 4, 1, 26, 21]","The sentiment score is slightly positive (20) because while the reviewer acknowledges the paper's interesting attempt and the importance of the topic, they also express some reservations about the coherence of the analogy and the generalizability of the approach. The reviewer states that the work is 'interesting' and 'worthwhile,' but also mentions that it 'fails to provide a simple sufficiently general notion of semi-supervised RL.' This mixed feedback leans slightly positive overall. The politeness score is moderately positive (50) as the reviewer maintains a professional and respectful tone throughout. They acknowledge the paper's merits and the importance of the topic, and even when expressing criticisms, they do so in a constructive manner. The language used is not overly formal or deferential, but it avoids any harsh or rude expressions, striking a balance between honesty and courtesy.",20,50
Generative Models and Model Criticism via Optimized Maximum Mean Discrepancy,Accept,2017,"['Dougal J. Sutherland', 'Hsiao-Yu Tung', 'Heiko Strathmann', 'Soumyajit De', 'Aaditya Ramdas', 'Alex Smola', 'Arthur Gretton']","[6, 8, 7]","['Marginally above acceptance threshold', 'Top 50% of accepted papers, clear accept', 'Good paper, accept']","This paper provides an interesting idea to use the optimized MMD for generative model evaluation and learning. Starting from the test power, the authors justified the criterion. Moreover, they also provided an efficient implementation of perturbation tests for empirical MMD. Pros: 1) The criterion is principled which is derived from the test power. 2) The criterion can be used to detect the difference template by incorporating ARD technique. 3) By exploiting kernel in the objective, the generated algorithm, t-GMMN, training can be improved from the GMMN. Cons: 1) How to train the provided t objective is not clear. 2) The algorithm is only tested on MNIST dataset as model criticism and learning objective. Comprehensive empirical comparison to the state-of-the-art criteria, e.g., log-likelihood, and other learning objectives is missing.","['3', '3', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[6, 5, 6, 2, 6, 22, 16]","[12, 11, 12, 2, 12, 28, 22]","[59, 37, 24, 2, 122, 341, 201]","[24, 18, 11, 1, 45, 216, 102]","[33, 17, 12, 1, 64, 83, 74]","[2, 2, 1, 0, 13, 42, 25]","The sentiment score is 50 (slightly positive) because the reviewer starts by describing the paper as providing an 'interesting idea' and lists several pros, indicating a generally positive view. However, they also mention some cons, which balances out the sentiment to be only moderately positive. The politeness score is 75 (quite polite) because the reviewer uses respectful language throughout, acknowledging the strengths of the paper with phrases like 'interesting idea' and 'principled', and presents criticisms in a constructive manner without using harsh language. The reviewer maintains a professional tone, focusing on the content rather than making personal comments.",50,75
Generative Multi-Adversarial Networks,Accept,2017,"['Ishan Durugkar', 'Ian Gemp', 'Sridhar Mahadevan']","[6, 7, 7]","['Marginally above acceptance threshold', 'Good paper, accept', 'Good paper, accept']","This work brings multiple discriminators into GAN. From the result, multiple discriminators is useful for stabilizing. The main problem of stabilizing seems is from gradient signal from discriminator, the authors motivation is using multiple discriminators to reduce this effect. I think this work indicates the direction is promising, however I think the authors may consider to add more result vs approach which enforce discriminator gradient, such as GAN with DAE (Improving Generative Adversarial Networks with Denoising Feature Matching), to show advantages of multiple discriminators.","['4', '4', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[-5, 0, 12]","[1, 6, 18]","[1, 8, 39]","[1, 5, 26]","[0, 2, 4]","[0, 1, 9]","The sentiment score is 50 (slightly positive) because the reviewer acknowledges the usefulness and promise of the work, stating 'multiple discriminators is useful for stabilizing' and 'the direction is promising'. However, they also suggest improvements, which prevents a higher score. The politeness score is 60 (moderately polite) as the reviewer uses respectful language throughout, such as 'I think' to soften criticisms, and offers constructive suggestions rather than harsh critiques. The reviewer maintains a professional tone without being overly formal or informal.",50,60
Geometry of Polysemy,Accept,2017,"['Jiaqi Mu', 'Suma Bhat', 'Pramod Viswanath']","[7, 7, 7]","['Good paper, accept', 'Good paper, accept', 'Good paper, accept']","On the plus side, the paper proposes a mathematically interesting model for a context of a word (i.e., a Grassmanian manifold). On the minus side, the paper mostly ignores the long history of Word Sense Induction (WSI) and Word Sense Disambiguation (WSD), citing and comparing only some relatively recent papers. The experiments in this paper done on SemEval-2010 are not very persuasive. (It*s difficult to evaluate the experiments done on the 2016 data, since they are not directly comparable to published results). For example, going back to the SemEval-2010 WSI task in [1], the best system seems to be UoY [2]. The F-measure seems to be a poor metric: always assigning one sense to every word (*MFS*) yields the highest F-measure of 63.5%. The paper*s result with *2 clusters* (with an average of about 1.9) seems to be close to MFS. So I don*t think we can use F-measure to compare. The V-measure seems to be tilted towards systems that have high number of senses per word. UoY has V=15.7%, while the paper (with *5 clusters*) has 14.4%. That isn*t very convincing that the proposed method has captured the geometry of polysemy. In general, I have often wondered why people work on pure unsupervised WSI and WSD. The assessment is very difficult (as described above). More importantly, some very weakly supervised systems (with minimal labels) can work pretty well to bootstrap. See, e.g., the classic paper [3]. If the authors used the Grassmannian idea to solve higher-level NLP problems directly (such as analogies), that would be very persuasive. However, that*s a very different paper than what was submitted. For an example of application of Grassmannian manifolds to analogies, see [4]. References: 1. Manandhar, Suresh, et al. *SemEval-2010 task 14: Word sense induction & disambiguation.* Proceedings of the 5th international workshop on semantic evaluation. Association for Computational Linguistics, 2010. 2. Korkontzelos, Ioannis, and Suresh Manandhar. *Uoy: Graphs of unambiguous vertices for word sense induction and disambiguation.* Proceedings of the 5th international workshop on semantic evaluation. Association for Computational Linguistics, 2010. 3. Yarowsky, David. *Unsupervised word sense disambiguation rivaling supervised methods.* Proceedings of the 33rd annual meeting on Association for Computational Linguistics. Association for Computational Linguistics, 1995. 4. Mahadevan, Sridhar, and Sarath Chandar Reasoning about Linguistic Regularities in Word Embeddings using Matrix Manifolds https://arxiv.org/abs/1507.07636","['3', '4', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[3, 11, 22]","[4, 16, 28]","[10, 80, 262]","[5, 49, 104]","[4, 24, 99]","[1, 7, 59]","The sentiment score is -30 because while the reviewer acknowledges some positive aspects ('On the plus side...'), the majority of the review focuses on criticisms and shortcomings of the paper. The reviewer points out issues with the experimental results, lack of historical context, and questions the overall approach. However, it's not entirely negative as the reviewer does suggest alternative directions that could be more persuasive. The politeness score is 20 because the reviewer maintains a professional tone throughout, using phrases like 'On the plus side' and 'I have often wondered' rather than harsh language. The critique is presented in a constructive manner, offering specific examples and references to support their points. While critical, the reviewer avoids personal attacks or overly negative language, instead focusing on the content of the paper itself.",-30,20
Hadamard Product for Low-rank Bilinear Pooling,Accept,2017,"['Jin-Hwa Kim', 'Kyoung-Woon On', 'Woosang Lim', 'Jeonghee Kim', 'Jung-Woo Ha', 'Byoung-Tak Zhang']","[7, 6, 7]","['Good paper, accept', 'Marginally above acceptance threshold', 'Good paper, accept']","Summary: The paper presents low-rank bilinear pooling that uses Hadamard product (commonly known as element-wise multiplication). The paper implements low-rank bilinear pooling on an existing model (Kim et al., 2016b) and builds a model for Visual Question Answering (VQA) that outperforms the current state-of-art by 0.42%. The paper presents various ablation studies of the new VQA model they built. Strengths: 1. The paper presents new insights into element-wise multiplication operation which has been previously used in VQA literature (such as Antol et al., ICCV 2015) without insights on why it should work. 2. The paper presents a new model for the task of VQA that beats the current state-of-art by 0.42%. However, I have concerns about the statistical significance of the performance (see weaknesses below). 3. The various design choices made in model development have been experimentally verified. Weaknesses/Suggestions: 1. When authors explicitly (keeping rest of the model architecture same) compared low-rank bilinear pooling with compact bilinear pooling, they found that low-rank bilinear pooling performs worse. Hence, it could not be experimentally verified that low-rank bilinear pooling is better in performance than compact bilinear pooling (at least for the task of VQA). 2. The authors argue that low-rank bilinear pooling uses 25% less parameters than compact bilinear pooling. So, could the authors please explain how does the reduction in number of parameters help experimentally? Does the training time of the model reduce significantly? Can we train the model with less data? 3. One of the contributions of the paper is that the proposed model outperforms the current state-of-art on VQA by 0.42%. However, I am skeptical that the performance of the proposed model is statistically significantly better than the current state-of-art. 4. I would like the authors to explicitly mention the differences between MRN, MARN and MLB. It is not very clear from reading the paper. 5. In the caption for Table 1, fix the following: “have not” -> “have no” Review Summary: I like the insights about low-rank bilinear pooling using Hadamard product (element-wise multiplication) presented in the paper. However, it could not be justified that low-rank bilinear pooling leads to better performance than compact biliear pooling. It does lead to reduction in number of parameters but it is not clear how much that helps experimentally. So, to be more convinced I would like the authors to provide experimental justification of why low-rank bilinear pooling is better than other forms of pooling.","['3', '3', '5']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[14, 5, 4, 19, 11, 28]","[20, 5, 5, 24, 17, 34]","[43, 4, 8, 29, 102, 309]","[16, 3, 7, 17, 47, 190]","[26, 1, 1, 4, 46, 55]","[1, 0, 0, 8, 9, 64]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some strengths of the paper, they express several concerns and weaknesses that outweigh the positive aspects. The reviewer likes the insights presented but is skeptical about the statistical significance of the results and requests more experimental justification. The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, acknowledges the paper's strengths, and frames criticisms as suggestions or requests for clarification rather than harsh judgments. The reviewer maintains a professional tone, using phrases like 'I would like the authors to...' and 'Could the authors please explain...', which contribute to the polite tone.",-20,60
Hierarchical Multiscale Recurrent Neural Networks,Accept,2017,"['Junyoung Chung', 'Sungjin Ahn', 'Yoshua Bengio']","[8, 7, 8]","['Top 50% of accepted papers, clear accept', 'Good paper, accept', 'Top 50% of accepted papers, clear accept']","This paper proposes a new multiscale recurrent neural network, where each layer has different time scale, and the scale is not fixed but variable and determined by a neural network. The method is elegantly formulated within a recurrent neural network framework, and shows the state-of-the-art performance on several benchmarks. The paper is well written. Question) Can you extend it to bidirectional RNN?","['4', '3', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[6, 13, 30]","[11, 19, 36]","[21, 63, 975]","[10, 31, 405]","[10, 30, 454]","[1, 2, 116]","The sentiment score is 90 because the reviewer expresses a very positive view of the paper, describing the method as 'elegantly formulated' and noting that it shows 'state-of-the-art performance'. The reviewer also states that the paper is 'well written'. These are all strong positive indicators. The politeness score is 80 because the language used is professional and respectful throughout. The reviewer offers praise without being overly effusive, and poses a question in a neutral, curious manner rather than as a criticism. The tone is consistently courteous and appropriate for academic discourse.",90,80
Highway and Residual Networks learn Unrolled Iterative Estimation,Accept,2017,"['Klaus Greff', 'Rupesh K. Srivastava', 'Jürgen Schmidhuber']","[7, 8, 6]","['Good paper, accept', 'Top 50% of accepted papers, clear accept', 'Marginally above acceptance threshold']","Thank you for an interesting angle on highway and residual networks. This paper shows a new angle to how and what kind of representations are learnt at each layer in the aforementioned models. Due to residual information being provided at a periodic number of steps, each of the layers preserve feature identity which prevents lesioning unlike convolutional neural nets. Pros - the iterative unrolling view was extremely simple and intuitive, which was supported by theoretical results and reasonable assumptions. - Figure 3 gave a clear visualization for the iterative unrolling view Cons - Even though, the perspective is interesting few empirical results were shown to support the argument. The major experiments are image classification and language models trained on mutations of character-aware neural language models. - Figure 4 and 5 could be combined and enlarged to show the effects of batch normalization.","['4', '4', '5']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[6, 8, 28]","[12, 14, 34]","[45, 36, 440]","[16, 16, 232]","[26, 19, 132]","[3, 1, 76]","The sentiment score is 50 (moderately positive) because the reviewer begins by thanking the authors for an interesting perspective and highlights several pros of the paper, such as the 'extremely simple and intuitive' iterative unrolling view. However, they also mention some cons, which prevents the score from being higher. The politeness score is 80 (quite polite) because the reviewer uses respectful language throughout, starting with a thank you and using phrases like 'interesting angle' and 'clear visualization'. They present criticisms constructively, framing them as 'Cons' rather than using harsh language. The balance of positive and negative feedback, along with the polite phrasing, contributes to both the sentiment and politeness scores.",50,80
HolStep: A Machine Learning Dataset for Higher-order Logic Theorem Proving,Accept,2017,"['Cezary Kaliszyk', 'François Chollet', 'Christian Szegedy']","[6, 8, 7]","['Marginally above acceptance threshold', 'Top 50% of accepted papers, clear accept', 'Good paper, accept']","Use of ML in ITP is an interesting direction of research. Authors consider the problem of predicting whether a given statement would be useful in a proof of a conjecture or not. This is posed as a binary classification task and authors propose a dataset and some deep learning based baselines. I am not an expert on ITP or theorem proving, so I will present a review from more of a ML perspective. I feel one of the goals of the paper should be to present the problem to a ML audience in a way that is easy for them to grasp. While most of the paper is well written, there are some sections that are not clear (especially section 2): - Terms such as LCF, OCaml-top level, deBruijn indices have been used without explaining or any references. These terms might be trivial in ITP literature, but were hard for me to follow. - Section 2 describes how the data was splits into train and test set. One thing which is unclear is – can the examples in the train and test set be statements about the same conjecture or are they always statements about different conjectures? It also unclear how the deep learning models are applied. Let’s consider the leftmost architecture in Figure 1. Each character is embedded into 256-D vector – and processed until the global max-pooling layer. Does this layer take a max along each feature and across all characters in the input? My another concern is only deep learning methods are presented as baselines. It would be great to compare with standard NLP techniques such as Bag of Words followed by SVM. I am sure these would be outperformed by neural networks, but the numbers would give a sense of how easy/hard the current problem setup is. Did the authors look at the success and failure cases of the algorithm? Are there any insights that can be drawn from such analysis that can inform design of future models? Overall I think the research direction of using ML for theorem proving is an interesting one. However, I also feel the paper is quite opaque. Many parts of how the data is constructed is unclear (atleast to someone with little knowledge in ITPs). If authors can revise the text to make it clearer – it would be great. The baseline models seem to perform quite well, however there are no insights into what kind of ability the models are lacking. Authors mention that they are unable to perform logical reasoning – but that’s a very vague statement. Some examples of mistakes might help make the message clearer. Further, since I am not well versed with the ITP literature it’s not possible for me to judge how valuable is this dataset. From the references, it seems like it’s drawn from a set of benchmark conjectures/proofs used in the ITP community – so its possibly a good dataset. My current rating is a weak reject, but if the authors address my concerns I would change to an accept.","['3', '3', '3']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[14, 16, 16]","[20, 21, 22]","[155, 23, 57]","[89, 6, 28]","[42, 8, 23]","[24, 9, 6]","The sentiment score is slightly negative (-20) because while the reviewer finds the research direction interesting, they express several concerns and give a 'weak reject' rating. They mention issues with clarity, lack of non-deep learning baselines, and insufficient insights into model performance. However, they also note positive aspects like the potential value of the dataset and well-performing baselines. The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, acknowledges their own limitations in the field, and offers constructive feedback. They use phrases like 'it would be great' and 'if authors can revise' which maintain a polite tone. The reviewer also balances criticism with positive comments and expresses willingness to change their rating if concerns are addressed.",-20,60
HyperNetworks,Accept,2017,"['David Ha', 'Andrew M. Dai', 'Quoc V. Le']","[9, 8, 7, 6]","['9', 'Top 50% of accepted papers, clear accept', 'Good paper, accept', 'Marginally above acceptance threshold']","A well known limitation in deep neural networks is that the same parameters are typically used for all examples, even though different examples have very different characteristics. For example, recognizing animals will likely require different features than categorizing flowers. Using different parameters for different types of examples has the potential to greatly reduce underfitting. This can be seen in recent results with generative models, where image quality is much better for less diverse datasets. However, it is difficult to use different parameters for different examples because we typically train using minibatches, which relies on using the same parameters for all examples in a minibatch (i.e. doing matrix multiplies in a fully-connected network). The hypernetworks paper cleverly proposes to get around this problem by adapting different *parameters* for different time steps in recurrent networks and different. The basic insight is that a minibatch will always include many different examples from the same time step or spatial position, so there is no computational issue involved with using different *parameters*. In this paper, the *parameters* are modified for different positions based on the output from a hypernetwork which conditions on the time step. Hypothetically, this hypernetwork could also condition on other features that are shared by all sequences in the minibatch. I expect this method to become standard for training RNNs, especially where the length of the sequences is the same during the training and testing phases. Penn Treebank is a highly competitive baseline, so the SOTA result reported here is impressive. The experiments on convolutional networks are less experimentally impressive. I suspect that the authors were aiming to achieve state of the art results here but settled with achieving a reduction in the number of parameters. It might even be worthwhile to consider a synthetic experiment where two completely different types of image are appended (i.e. birds on the left and flowers on the right) and show that the hypernetwork helps in this situation. It may be the case that for convnets, the cases where hypernetworks help are very specific. For RNNs, it seems to be the case that explicitly changing the nature of the computation depending on the position in the sequence greatly improves generalization. While a usual RNN could learn to store a counter (indicating the position in the sequence), the hypernetwork could be a more efficient way to add capacity. Applications to time series forecasting and modeling could be an interesting area for future work.","['4', '4', '4', '5']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[-4, -1, 6, 10]","[2, 5, 12, 16]","[2, 9, 54, 181]","[1, 5, 33, 98]","[1, 3, 18, 52]","[0, 1, 3, 31]","The sentiment score is 80 (positive) because the reviewer expresses enthusiasm for the paper's approach, calling it 'clever' and expecting it to 'become standard for training RNNs'. They also praise the 'impressive' state-of-the-art results on Penn Treebank. While there are some critiques of the convolutional network experiments, the overall tone is very positive and encouraging. The politeness score is 70 (polite) as the reviewer uses respectful language throughout, acknowledging the paper's contributions and offering constructive suggestions for improvement. The reviewer avoids harsh criticism and instead frames potential weaknesses as opportunities for future work. The language is professional and objective, maintaining a courteous tone while providing a thorough analysis.",80,70
Hyperband: Bandit-Based Configuration Evaluation for Hyperparameter Optimization,Accept,2017,"['Lisha Li', 'Kevin Jamieson', 'Giulia DeSalvo', 'Afshin Rostamizadeh', 'Ameet Talwalkar']","[8, 7, 7]","['Top 50% of accepted papers, clear accept', 'Good paper, accept', 'Good paper, accept']","This was an interesting paper. The algorithm seems clear, the problem well-recognized, and the results are both strong and plausible. Approaches to hyperparameter optimization based on SMBO have struggled to make good use of convergence during training, and this paper presents a fresh look at a non-SMBO alternative (at least I thought it did, until one of the other reviewers pointed out how much overlap there is with the previously published successive halving algorithm - too bad!). Still, I*m excited to try it. I*m cautiously optimistic that this simple alternative to SMBO may be the first advance to model search for the skeptical practitioner since the case for random search > grid search (http://www.jmlr.org/papers/v13/bergstra12a.html, which this paper should probably cite in connection with their random search baseline.) I would suggest that the authors remove the (incorrect?) claim that this algorithm is *embarrassingly parallel* as it seems that there are number of synchronization barriers at which state must be shared in order to make the go-no-go decisions on whatever training runs are still in progress. As the authors themselves point out as future work, there are interesting questions around how to adapt this algorithm to make optimal use of a cluster (I*m optimistic that it should carry over, but it*s not trivial). For future work, the authors might be interested in Hutter et al*s work on Bayesian Optimization With Censored Response Data (https://arxiv.org/abs/1310.1947) for some ideas about how to use the dis-continued runs.","['4', '5', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[15, 9, 3, 11, 10]","[21, 15, 9, 17, 16]","[16, 99, 22, 70, 126]","[6, 49, 16, 37, 54]","[2, 47, 5, 30, 61]","[8, 3, 1, 3, 11]","The sentiment score is 60 (positive) because the reviewer expresses interest in the paper, praising its clarity, problem recognition, and strong results. They are 'excited to try it' and 'cautiously optimistic' about the approach. However, the score is not higher due to the mention of overlap with a previously published algorithm, which slightly dampens the enthusiasm. The politeness score is 70 (polite) because the reviewer uses respectful language throughout, acknowledging the paper's strengths and offering constructive suggestions. They use phrases like 'interesting paper' and 'I would suggest,' which maintain a courteous tone. The reviewer also provides helpful references and ideas for future work, demonstrating a supportive attitude. The language is consistently professional and considerate, without any rudeness or harsh criticism.",60,70
Identity Matters in Deep Learning,Accept,2017,"['Moritz Hardt', 'Tengyu Ma']","[8, 5, 6]","['Top 50% of accepted papers, clear accept', '5', 'Marginally above acceptance threshold']","This paper provides some theoretical guarantees for the identity parameterization by showing that 1) arbitrarily deep linear residual networks have no spurious local optima; and 2) residual networks with ReLu activations have universal finite-sample expressivity. This paper is well written and studied a fundamental problem in deep neural network. I am very positive on this paper overall and feel that this result is quite significant by essentially showing the stability of auto-encoder, given the fact that it is hard to provide concrete theoretical guarantees for deep neural networks. One of key questions is how to extent the result in this paper to the more general nonlinear actuation function case. Minors: one line before Eq. (3.1), U in R ? 	imes k","['3', '4', '5']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[12, 7]","[18, 13]","[130, 181]","[61, 76]","[62, 97]","[7, 8]","The sentiment score is 90 because the reviewer expresses a very positive overall view of the paper, describing it as 'well written' and stating that the result is 'quite significant'. The reviewer also mentions being 'very positive on this paper overall'. The score is not 100 as there is a minor question raised about extending the results to more general cases. The politeness score is 80 because the language used is consistently respectful and constructive. The reviewer offers praise and expresses their positive sentiment clearly, while also providing a suggestion for future work in a non-demanding way. The minor correction is pointed out without criticism. The tone throughout is professional and courteous, though not excessively formal or deferential, hence the score of 80 rather than 100.",90,80
Improving Generative Adversarial Networks with Denoising Feature Matching,Accept,2017,"['David Warde-Farley', 'Yoshua Bengio']","[7, 7]","['Good paper, accept', 'Good paper, accept']","This paper is well written, and well presented. This method is using denoise autoencoder to learn an implicit probability distribution helps reduce training difficulty, which is neat. In my view, joint training with an auto-encoder is providing extra auxiliary gradient information to improve generator. Providing auxiliary information may be a methodology to improve GAN. Extra comment: Please add more discussion with EBGAN in next version.","['4', '5']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[8, 30]","[13, 36]","[32, 975]","[14, 405]","[14, 454]","[4, 116]","The sentiment score is 80 (positive) because the review starts with praise for the paper being 'well written, and well presented.' It also describes the method as 'neat' and suggests that it may be a valuable methodology for improving GANs. The only slightly less positive element is the request for more discussion with EBGAN, but this is presented as a constructive suggestion rather than a criticism. The politeness score is 70 (polite) due to the use of positive language throughout and the constructive nature of the feedback. The reviewer uses phrases like 'in my view' to soften opinions and presents the suggestion for additional discussion as a polite request ('Please add...'). The overall tone is respectful and encouraging, without being overly formal or excessively deferential.",80,70
Improving Neural Language Models with a Continuous Cache,Accept,2017,"['Edouard Grave', 'Armand Joulin', 'Nicolas Usunier']","[7, 5, 9]","['Good paper, accept', '5', '9']","The authors present a simple method to affix a cache to neural language models, which provides in effect a copying mechanism from recently used words. Unlike much related work in neural networks with copying mechanisms, this mechanism need not be trained with long-term backpropagation, which makes it efficient and scalable to much larger cache sizes. They demonstrate good improvements on language modeling by adding this cache to RNN baselines. The main contribution of this paper is the observation that simply using the hidden states h_i as keys for words x_i, and h_t as the query vector, naturally gives a lookup mechanism that works fine without tuning by backprop. This is a simple observation and might already exist as folk knowledge among some people, but it has nice implications for scalability and the experiments are convincing. The basic idea of repurposing locally-learned representations for large-scale attention where backprop would normally be prohibitively expensive is an interesting one, and could probably be used to improve other types of memory networks. My main criticism of this work is its simplicity and incrementality when compared to previously existing literature. As a simple modification of existing NLP models, but with good empirical success, simplicity and practicality, it is probably more suitable for an NLP-specific conference. However, I think that approaches that distill recent work into a simple, efficient, applicable form should be rewarded and that this tool will be useful to a large enough portion of the ICLR community to recommend its publication.","['5', '4', '5']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[7, 8, 13]","[13, 14, 19]","[93, 130, 125]","[36, 59, 72]","[52, 67, 44]","[5, 4, 9]","The sentiment score is 70 (positive) because the reviewer expresses overall approval of the paper, highlighting its simplicity, efficiency, and practical implications. They recommend publication despite some minor criticisms. The politeness score is 80 (polite) as the reviewer uses respectful language throughout, acknowledging the paper's contributions and providing constructive feedback. They balance praise with gentle criticism, using phrases like 'My main criticism' rather than harsh language. The reviewer also recognizes the value of the work even while noting its simplicity, which demonstrates a considerate approach to feedback.",70,80
Improving Policy Gradient by Exploring Under-appreciated Rewards,Accept,2017,"['Ofir Nachum', 'Mohammad Norouzi', 'Dale Schuurmans']","[7, 8, 7]","['Good paper, accept', 'Top 50% of accepted papers, clear accept', 'Good paper, accept']","This paper proposes a novel exploration strategy that promotes exploration of under-appreciated reward regions. Proposed importance sampling based approach is a simple modification to REINFORCE and experiments in several algorithmic toy tasks show that the proposed model is performing better than REINFORCE and Q-learning. This paper shows promising results in automated algorithm discovery using reinforcement learning. However it is not very clear what is the main motivation of the paper. Is the main motivation better exploration for policy gradient methods? If so, authors should have benchmarked their algorithm with standard reinforcement learning tasks. While there is a huge body of literature on improving REINFORCE, authors have considered a simple version of REINFORCE on a non-standard task and say that UREX is better. If the main motivation is improving the performance in algorithm learning tasks, then the baselines are still weak. Authors should make it clear which is the main motivation. Also the action space is too small. In the beginning authors raise the concern that entropy regularization might not scale to larger action spaces. So a comparison of MENT and UREX in a large action space problem would give more insights on whether UREX is not affected by large action space. -------------------------- After rebuttal: I missed the action sequences argument when I pointed about small action space issue. For question regarding weak baseline, there are several tricks used in the literature to tackle high-variance issue for REINFORCE. For example, see Mnih & Gregor, 2014. I have increased my rating from 6 to 7. I still encourage the authors to improve their baseline.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[2, 9, 29]","[8, 15, 35]","[104, 137, 286]","[42, 59, 182]","[61, 73, 82]","[1, 5, 22]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('promising results', 'novel exploration strategy'), they express several concerns and criticisms. The reviewer questions the main motivation of the paper, suggests the baselines are weak, and points out limitations in the action space. However, they do increase their rating after the rebuttal, showing some positive shift. The politeness score is moderately positive (50) as the reviewer uses professional and respectful language throughout. They offer constructive criticism and suggestions for improvement rather than harsh criticism. The use of phrases like 'authors should' and 'I encourage the authors to' indicates a polite and collaborative tone, even when pointing out areas for improvement.",-20,50
Incorporating long-range consistency in CNN-based texture generation,Accept,2017,"['Guillaume Berger', 'Roland Memisevic']","[5, 7, 7]","['5', 'Good paper, accept', 'Good paper, accept']","The paper introduces a variation to the CNN-based texture synthesis procedure of Gatys et al. that matches correlations between spatially shifted feature responses in addition to the correlations between feature responses at the same position in the feature maps. The paper claims that this a) improves texture synthesis for textures with long-range regular structures, that are not preserved with the Gatys et al. method b) improves performance on texture inpainting tasks compared to the Gatys et al. method c) improves results in season transfer when combined with the style transfer method by Gatys et al. Furthermore the paper shows that d) by matching correlations between spatially flipped feature maps, symmetry properties around the flipping axis can be preserved. I agree with claim a). However, the generated textures still have some issues such as greyish regions so the problem is not solved. Additionally, the procedure proposed is very costly which makes an already slow texture synthesis method substantially slower. For example, in comparison, the concurrent work by Liu et al. (http://arxiv.org/abs/1605.01141) that tackles the same problem by adding constraints to the Fourier spectrum of the synthesised texture shows comparable or better results while being far more efficient. Also with b) the presented results constitute an improvement over the Gatys et al. method but again the results are not too exciting - one would not prefer this model to other inpainting algorithms. With c) I don’t see a clear advantage of the proposed method to the existing Gatys et al. algorithm. Finally, d) is a neat idea and the initial results look interesting but they don’t go much further than that. All in all I think it is decent work but neither its originality and technical complexity nor the quality of the results are convincing enough for acceptance. That said, I could imagine this to be a nice contribution to the workshop track though.","['5', '5', '5']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[2, 14]","[8, 20]","[8, 62]","[4, 35]","[4, 20]","[0, 7]","The sentiment score is -30 because while the reviewer acknowledges some positive aspects of the paper ('decent work', 'neat idea'), they ultimately recommend against acceptance, citing issues with the results and efficiency. The overall tone is more negative than positive, but not extremely negative. The politeness score is 20 because the reviewer uses respectful language throughout, acknowledging the paper's contributions and providing constructive feedback. They avoid harsh criticism and use phrases like 'I agree with...' and 'I could imagine this to be a nice contribution...', which contribute to a polite tone. However, the review is not overly effusive or excessively polite, maintaining a professional and somewhat neutral stance.",-30,20
Incremental Network Quantization: Towards Lossless CNNs with Low-precision Weights,Accept,2017,"['Aojun Zhou', 'Anbang Yao', 'Yiwen Guo', 'Lin Xu', 'Yurong Chen']","[8, 7, 7]","['Top 50% of accepted papers, clear accept', 'Good paper, accept', 'Good paper, accept']","There is a great deal of ongoing interest in compressing neural network models. One line of work has focused on using low-precision representations of the model weights, even down to 1 or 2 bits. However, so far these approaches have been accompanied by a significant impact on accuracy. The paper proposes an iterative quantization scheme, in which the network weights are quantized in stages---the largest weights (in absolute value) are quantized and fixed, while unquantized weights can adapt to compensate for any resulting error. The experimental results show this is extremely effective, yielding models with 4 bit or 3 bit weights with essentially no reduction in accuracy. While at 2 bits the accuracy decreases slightly, the results are substantially better than those achieved with other quantization approaches. Overall this paper is clear, the technique is as far as I am aware novel, the experiments are thorough and the results are very compelling, so I recommend acceptance. The paper could use another second pass for writing style and grammar. Also, the description of the pruning-inspired partitioning strategy could be clarified somewhat... e.g., the chosen splitting ratio of 50% only seems to be referenced in a figure caption and not the main text.","['4', '4', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[1, 10, 4, 27, 13]","[7, 16, 10, 33, 19]","[34, 65, 64, 258, 82]","[15, 32, 18, 120, 45]","[18, 25, 31, 25, 26]","[1, 8, 15, 113, 11]","The sentiment score is 90 because the reviewer expresses strong positive sentiment towards the paper. They describe the results as 'extremely effective' and 'very compelling', and recommend acceptance. The only minor criticisms are about writing style and clarity of a specific section. The politeness score is 70 because the reviewer uses respectful and professional language throughout. They offer constructive feedback and frame their suggestions politely. The score is not higher because while polite, the tone is primarily neutral and matter-of-fact rather than overtly courteous.",90,70
Inductive Bias of Deep Convolutional Networks through Pooling Geometry,Accept,2017,"['Nadav Cohen', 'Amnon Shashua']","[7, 7, 6]","['Good paper, accept', 'Good paper, accept', 'Marginally above acceptance threshold']","This paper investigates the fact why deep networks perform well in practice and how modifying the geometry of pooling can make the polynomially sized deep network to provide a function with exponentially high separation rank (for certain partitioning.) In the authors* previous works, they showed the superiority of deep networks over shallows when the activation function is ReLu and the pooling is max/mean pooling but in the current paper there is no activation function after conv and the pooling is just a multiplication of the node values. Although for the experimental results they*ve considered both scenarios. Actually, the general reasoning for this problem is hard, therefore, this drawback is not significant and the current contribution adds a reasonable amount of knowledge to the literature. This paper studies the convolutional arithmetic circuits and shows how this model can address the inductive biases and how pooling can adjust these biases. This interesting contribution gives an intuition about how deep network can capture the correlation between the input variables when its size is polynomial but and correlation is exponential. It worth to note that although the authors tried to express their notation and definitions carefully where they were very successful, it would be helpful if they elaborate a bit more on their definitions, expressions, and conclusions in the sense to make them more accessible.","['5', '3', '3']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[6, 30]","[12, 36]","[55, 142]","[23, 83]","[27, 39]","[5, 20]","The sentiment score is 60 (positive) because the reviewer acknowledges the paper's contribution as 'interesting' and 'reasonable', despite noting some drawbacks. They state that it 'adds a reasonable amount of knowledge to the literature' and gives 'intuition about how deep network can capture the correlation'. The politeness score is 70 (polite) as the reviewer uses respectful language throughout, acknowledging the authors' efforts ('the authors tried to express their notation and definitions carefully where they were very successful') and offering constructive suggestions ('it would be helpful if they elaborate a bit more'). The reviewer also balances critique with praise, softening negative points ('this drawback is not significant').",60,70
Introspection:Accelerating Neural Network Training By Learning Weight Evolution,Accept,2017,"['Abhishek Sinha', 'Aahitagni Mukherjee', 'Mausoom Sarkar', 'Balaji Krishnamurthy']","[7, 8, 9]","['Good paper, accept', 'Top 50% of accepted papers, clear accept', '9']","In this paper, the authors use a separate introspection neural network to predict the future value of the weights directly from their past history. The introspection network is trained on the parameter progressions collected from training separate set of meta learning models using a typical optimizer, e.g. SGD. Pros: + The organization is generally very clear + Novel meta-learning approach that is different than the previous learning to learn approach Cons: - The paper will benefit from more thorough experiments on other neural network architectures where the geometry of the parameter space are sufficiently different than CNNs such as fully connected and recurrent neural networks. - Neither MNIST nor CIFAR experimental section explained the architectural details - Mini-batch size for the experiments were not included in the paper - Comparison with different baseline optimizer such as Adam would be a strong addition or at least explain how the hyper-parameters, such as learning rate and momentum, are chosen for the baseline SGD method. Overall, due to the omission of the experimental details in the current revision, it is hard to draw any conclusive insight about the proposed method.","['4', '5', '5']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[7, 1, 12, 30]","[13, 1, 18, 36]","[102, 2, 28, 97]","[38, 1, 12, 45]","[48, 1, 16, 51]","[16, 0, 0, 1]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('Pros'), they also list several significant concerns ('Cons') and conclude that it's 'hard to draw any conclusive insight about the proposed method.' This indicates an overall skeptical view of the paper's current state. The politeness score is moderately positive (50) as the reviewer uses neutral, professional language throughout. They present both pros and cons objectively, and their criticisms are framed as suggestions for improvement rather than harsh critiques. The use of phrases like 'will benefit from' and 'would be a strong addition' contribute to the polite tone.",-20,50
LR-GAN: Layered Recursive Generative Adversarial Networks for Image Generation,Accept,2017,"['Jianwei Yang', 'Anitha Kannan', 'Dhruv Batra', 'Devi Parikh']","[6, 7]","['Marginally above acceptance threshold', 'Good paper, accept']","The paper proposes a model for image generation where the back-ground is generated first and then the foreground is pasted in by generating first a foregound mask and corresponding appearance, curving the appearance image using the mask and transforming the mask using predicted affine transform to paste it on top of the image. Using AMTurkers the authors verify their generated images are selected 68% of the time as being more naturally looking than corresponding images from a DC-GAN model that does not use a figure-ground aware image generator. The segmentations masks learn to depict objects in very constrained datasets (birds) only, thus the method appears limited for general shape datasets, as the authors also argue in the paper. Yet, the architectural contributions have potential merit. It would be nice to see if multiple layers of foreground (occluding foregrounds) are ever generated with this layered model or it is just figure-ground aware.","['3', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[15, 18, 10, 14]","[21, 24, 16, 20]","[184, 78, 293, 285]","[69, 48, 134, 145]","[37, 26, 143, 124]","[78, 4, 16, 16]","The sentiment score is 50 (slightly positive) because the reviewer acknowledges the potential merit of the architectural contributions and the model's effectiveness in constrained datasets. However, they also point out limitations, creating a balanced view. The politeness score is 75 (quite polite) due to the use of respectful language throughout, such as 'It would be nice to see' and 'the authors also argue,' which shows consideration for the authors' perspective. The reviewer offers constructive feedback and suggestions without harsh criticism, maintaining a professional and courteous tone.",50,75
Latent Sequence Decompositions,Accept,2017,"['William Chan', 'Yu Zhang', 'Quoc Le', 'Navdeep Jaitly']","[8, 7, 7]","['Top 50% of accepted papers, clear accept', 'Good paper, accept', 'Good paper, accept']","This paper proposes to learn decomposition of sequences (such as words) for speech recognition. It addresses an important issue and I forsee it being useful for other applications such as machine translation. While the approach is novel and well-motivated, I would very much like to see a comparison against byte pair encoding (BPE). BPE is a very natural (and important) baseline (i.e. dynamic vs fixed decomposition). The BPE performance should be obtained for various BPE vocab sizes. Minor points - Did the learned decompositions correspond to phonetically meaningful units? From the example in the appendix it*s hard to tell if the model is learning phonemes or just most frequent character n-grams. - Any thoughts on applications outside of speech recognition? If this is shown to be effective in other domains it would be a really strong contribution (but this is probably outside the scope for now).","['4', '4', '5']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[18, 5, 13, 11]","[24, 11, 19, 16]","[77, 174, 300, 78]","[33, 90, 143, 31]","[39, 81, 146, 38]","[5, 3, 11, 9]","The sentiment score is 50 (slightly positive) because the reviewer acknowledges the importance and potential usefulness of the paper's approach, calling it 'novel and well-motivated'. However, they also express a desire for additional comparisons and clarifications, indicating room for improvement. The politeness score is 80 (quite polite) due to the reviewer's constructive tone and use of phrases like 'I would very much like to see' and 'Any thoughts on...', which suggest recommendations rather than demands. The reviewer also acknowledges the potential impact of the work beyond its current scope, which is a polite way of encouraging further development.",50,80
Learning Curve Prediction with Bayesian Neural Networks,Accept,2017,"['Aaron Klein', 'Stefan Falkner', 'Jost Tobias Springenberg', 'Frank Hutter']","[7, 7, 7]","['Good paper, accept', 'Good paper, accept', 'Good paper, accept']","The paper addresses the problem of predicting learning curves. The key difference from prior work is that (1) the authors learn a neural network that generalizes across hyperparameter settings and (2) the authors use a Bayesian neural network with SGHMC. The authors demonstrate that the proposed approach is effective on extrapolating partially observed curves as well as predicting unobserved learning curves on various architectures (FC, CNN, LR and VAE). This seems very promising for Bayesian optimization, I*d love to see an experiment that evaluates the relative advantage of this proposed method :) Have you thought about ways to handle learning rate decays? Perhaps you could run the algorithm on a random subset of data and extrapolate from that? I was thinking of other evaluation measures in addition to MSE and LL. In practice, we care about the most promising run. Would it make sense to evaluate how accurately each method identified the best run? Minor comments: Fonts are too small and almost illegible on my hard copy. Please increase the font size for legends and axes in the figures. Fig 6: not all figures seem to have six lines. Are the lines overlapping in some cases?","['4', '4', '5']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[3, 3, 6, 16]","[9, 8, 12, 22]","[32, 19, 76, 252]","[16, 11, 37, 103]","[16, 7, 35, 120]","[0, 1, 4, 29]","The sentiment score is 70 (positive) because the reviewer describes the paper as 'very promising' and expresses enthusiasm about the approach. They use phrases like 'seems very promising' and 'I'd love to see' which indicate a positive reception. The score is not higher because the reviewer also includes suggestions for improvement and additional experiments. The politeness score is 80 (polite) because the reviewer uses respectful language throughout, phrases suggestions as questions or gentle recommendations ('Have you thought about...', 'Perhaps you could...'), and even includes a smiley face emoticon to maintain a friendly tone. The reviewer also balances critique with praise. The score is not 100 because there are some direct instructions ('Please increase the font size') which, while not impolite, are less deferential than the rest of the review.",70,80
Learning Features of Music From Scratch,Accept,2017,"['John Thickstun', 'Zaid Harchaoui', 'Sham Kakade']","[8, 6, 6]","['Top 50% of accepted papers, clear accept', 'Marginally above acceptance threshold', 'Marginally above acceptance threshold']","This paper introduces MusicNet, a new dataset. Application of ML techniques to music have been limited due to scarcity of exactly the kind of data that is provided here: meticulously annotated, carefully verified and organized, containing enough *hours* of music, and where genre has been well constrained in order to allow for sufficient homogeneity in the data to help ensure usefulness. This is great for the community. The description of the validation of the dataset is interesting, and indicates a careful process was followed. The authors provide just enough basic experiments to show that this dataset is big enough that good low-level features (i.e. expected sinusoidal variations) can indeed be learned in an end-to-end context. One might argue that in terms of learning representations, the work presented here contributes more in the dataset than in the experiments or techniques used. However, given the challenges of acquiring good datasets, and given the essential role such datasets play for the community in moving research forward and providing baseline reference points, I feel that this contribution carries substantial weight in terms of expected future rewards. (If research groups were making great new datasets available on a regular basis, that would place this in a different context. But so far, that is not the case.) In otherwords, while the experiments/techniques are not necessarily in the top 50% of accepted papers (per the review criteria), I am guessing that the dataset is in the top 15% or better.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[2, 14, 19]","[8, 20, 25]","[26, 126, 297]","[11, 62, 144]","[15, 46, 126]","[0, 18, 27]","The sentiment score is 80 (positive) because the reviewer expresses strong approval of the paper's contribution, particularly the dataset. They use phrases like 'This is great for the community' and describe the dataset as 'meticulously annotated' and 'carefully verified'. The reviewer also emphasizes the importance of the dataset for future research. The politeness score is 70 (polite) because the reviewer uses respectful and professional language throughout. They acknowledge the paper's strengths without being overly effusive, and even when mentioning potential limitations (e.g., 'One might argue...'), they do so in a constructive manner. The tone is consistently positive and supportive, showing appreciation for the authors' work.",80,70
Learning Invariant Feature Spaces to Transfer Skills with Reinforcement Learning,Accept,2017,"['Abhishek Gupta', 'Coline Devin', 'YuXuan Liu', 'Pieter Abbeel', 'Sergey Levine']","[7, 6, 6]","['Good paper, accept', 'Marginally above acceptance threshold', 'Marginally above acceptance threshold']","This paper presents an approach for skills transfer from one task to another in a control setting (trained by RL) by forcing the embeddings learned on two different tasks to be close (L2 penalty). The experiments are conducted in MuJoCo, with a set of experiments being from the state of the joints/links (5.2/5.3) and a set of experiments on the pixels (5.4). They exhibit transfer from arms with different number of links, and from a torque-driven arm to a tendon-driven arm. One limitation of the paper is that the authors suppose that time alignment is trivial, because the tasks are all episodic and in the same domain. Time alignment is one form of domain adaptation / transfer that is not dealt with in the paper, that could be dealt with through subsampling, dynamic time warping, or learning a matching function (e.g. neural network). General remarks: The approach is compared to CCA, which is a relevant baseline. However, as the paper is purely experimental, another baseline (worse than CCA) would be to just have the random projections for *f* and *g* (the embedding functions on the two domains), to check that the bad performance of the *no transfer* version of the model is due to over-specialisation of these embeddings. I would also add (for information) that the problem of learning invariant feature spaces is also linked to metric learning (e.g. [Xing et al. 2002]). More generally, no parallel is drawn with multi-task learning in ML. In the case of knowledge transfer (4.1.1), it may make sense to anneal alpha. The experiments feel a bit rushed. In particular, the performance of the baseline being always 0 (no transfer at all) is uninformative, at least a much bigger sample budget should be tested. Also, why does Figure 7.b contain no *CCA* nor *direct mapping* results? Another concern that I have with the experiments: (if/how) did the author control for the fact that the embeddings were trained with more iterations in the case of doing transfer? Overall, the study of transfer is most welcomed in RL. The experiments in this paper are interesting enough for publication, but the paper could have been more thorough.","['3', '5', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[3, 4, 1, 16, 9]","[9, 9, 7, 22, 15]","[78, 31, 12, 610, 744]","[35, 15, 5, 291, 326]","[43, 16, 7, 293, 397]","[0, 0, 0, 26, 21]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the interesting aspects of the paper and its potential for publication, they also point out several limitations and areas for improvement. The reviewer mentions that the experiments feel rushed, some baselines are missing, and the paper could have been more thorough. However, they do recognize the value of studying transfer in RL and the interesting nature of the experiments.

The politeness score is moderately positive (50) as the reviewer maintains a professional and constructive tone throughout. They use phrases like 'One limitation of the paper is...' and 'I would also add...' which are polite ways to offer criticism and suggestions. The reviewer also acknowledges the positive aspects of the work, such as the interesting experiments and the relevance of the approach to the field. There are no rude or harsh comments, and the criticism is presented in a respectful manner.",-20,50
Learning Recurrent Representations for Hierarchical Behavior Modeling,Accept,2017,"['Eyrun Eyjolfsdottir', 'Kristin Branson', 'Yisong Yue', 'Pietro Perona']","[7, 6, 7]","['Good paper, accept', 'Marginally above acceptance threshold', 'Good paper, accept']","While my above review title is too verbose, it would be a more accurate title for the paper than the current one (an overall better title would probably be somewhere in between). The overall approach is interesting: all three of the key techniques (aux. tasks, skip/diagonal connections, and the use of internal labels for the kind of data available) make a lot of sense. I found some of the results hard to understand/interpret. Some of the explanation in the discussion below has been helpful (e.g. see my earlier questions about Fig 4 and 5); the paper would benefit from including more such explanations. It may be worthwhile very briefly mentioning the relationship of *diagonal* connections to other emerging terms for similar ideas (e.g. skip connections, etc). *Skip* seems to me to be accurate regardless of how you draw the network, whereas *diagonal* only makes sense for certain visual layouts. In response to comment in the discussion below: *leading to less over-segmentation of action bouts* (and corresponding discussion in section 5.1 of the paper): I would be like to have a bit more about this in the paper. I have assumed that *per-bout* refers to *per-action event*, but now I am not certain that I have understood this correctly (i.e. can a *bout* last for a few minutes?): given the readership, I think it would not be inappropriate to define some of these things explicitly. In response to comment about fly behaviours that last minutes vs milliseconds: This is interesting, and I would be curious to know how classification accuracy relates to the time-scale of the behaviour (e.g. are most of the mistakes on long-term behaviours? i realize that this would only tell part of the story, e.g. if you have a behaviour that has both a long-term duration, but that also has very different short-term characteristics than many other behaviours, it should be easy to classify accuractely despite being *long-term*). If easy to investigate this, I would add a comment about it; if this is hard to investigate, it*s probably not worth it at this point, although it*s something you might want to look at in future. In response to comment about scaling to human behavior: I agree that in principle, adding conv layers directly above the sensory input would be the right thing to try, but seriously: there is usually a pretty big gap between what *should* work and what actually works, as I am sure the authors are aware. (Indeed, I am sure the authors have a much more experiential and detailed understanding of the limitations of their work than I do). What I see presented is a nice system that has been demonstrated to handle spatiotemporal trajectories. The claims made should correspond to this. I would consider adjusting my rating to a 7 depending on future revisions.","['3', '4', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[7, 13, 11, 29]","[7, 18, 17, 35]","[4, 21, 230, 298]","[3, 8, 101, 193]","[1, 11, 110, 56]","[0, 2, 19, 49]","The sentiment score is 50 (slightly positive) because the reviewer finds the overall approach interesting and acknowledges the sensibility of the key techniques used. However, they also express some concerns about the clarity of results and explanations. The politeness score is 70 (fairly polite) as the reviewer uses respectful language throughout, offers constructive feedback, and acknowledges the authors' expertise. They use phrases like 'I would be curious to know' and 'I agree that in principle,' which maintain a collegial tone. The reviewer also balances critique with positive comments and shows willingness to adjust their rating based on revisions.",50,70
Learning Visual Servoing with Deep Features and Fitted Q-Iteration,Accept,2017,"['Alex X. Lee', 'Sergey Levine', 'Pieter Abbeel']","[7, 8, 7]","['Good paper, accept', 'Top 50% of accepted papers, clear accept', 'Good paper, accept']","The paper proposes a novel approach for learning visual servoing based on Q-iteration. The main contributions of the paper are: 1. Bilinear dynamics model for predicting next frame (features) based on action and current frame 2. Formulation of servoing with a Q-function that learns weights for different feature channels 3. An elegant method for optimizing the Bellman error to learn the Q-function Pros: + The paper does a good job of exploring different ways to connect the action (u_t) and frame representation (y_t) to predict next frame features (y_{t+1}). They argue in favour of a locally connected bilinear model which strikes the balance between computation and expressive ability. Cons: - While, sec. 4 makes good arguments for different choices, I would have liked to see more experimental results comparing the 3 approaches: fully connected, convolutional and locally connected dynamics. Pros: + The idea of weighting different channels to capture the importance of obejcts in different channels seems more effective than treating errors across all channels equally. This is also validated experimentally, where unweighted performance suffers consistently. + Solving the Bellman error is a difficult problem in Q-learning approaches. The current paper presents a solid optimization scheme based on the key-observation that scaling Q-function parameters does not affect the best policy chosen. This enables a more elegant FQI approach as opposed to typical optimization schemes which (c_t + gamma min_u Q_{t+1}) fixed. Cons: - However, I would have liked to see the difference between FQI and such an iterative approach which holds the second term in Eq. 5 fixed. Experimental results: - Overall, I find the experimental results unsatisfying given the small scale and toy simulations. However, the lack of benchmarks in this domain needs to be recognized. - Also, as pointed out in pre-review section, the idea of modifying the VGG needs to be experimentally validated. In its current form, it is not clear whether the modified VGG would perform better than the original version. Overall, the contribution of the paper is solid in terms of technical novelty and problem formulations. However, the paper could use stronger experiments as suggested to earlier to bolster its claims.","['4', '3', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[5, 9, 16]","[10, 15, 22]","[24, 744, 610]","[15, 326, 291]","[8, 397, 293]","[1, 21, 26]","The sentiment score is 50 (slightly positive) because the reviewer presents a balanced view with both pros and cons. They acknowledge the paper's 'solid' contributions and 'good job' in certain areas, but also express concerns about experimental results and desire for more comparisons. The politeness score is 75 (quite polite) as the reviewer uses respectful language throughout, acknowledging the paper's strengths and framing criticisms constructively (e.g., 'I would have liked to see'). They also recognize challenges in the field ('lack of benchmarks'). The reviewer maintains a professional tone without using harsh or dismissive language.",50,75
Learning a Natural Language Interface with Neural Programmer,Accept,2017,"['Arvind Neelakantan', 'Quoc V. Le', 'Martin Abadi', 'Andrew McCallum', 'Dario Amodei']","[7, 6, 6]","['Good paper, accept', 'Marginally above acceptance threshold', 'Marginally above acceptance threshold']","The paper presents an end-to-end neural network model for the problem of designing natural language interfaces for database queries. The proposed approach uses only weak supervision signals to learn the parameters of the model. Unlike in traditional approaches, where the problem is solved by semantically parsing a natural language query into logical forms and executing those logical forms over the given data base, the proposed approach trains a neural network in an end-to-end manner which goes directly from the natural language query to the final answer obtained by processing the data base. This is achieved by formulating a collection of operations to be performed over the data base as continuous operations, the distributions over which is learnt using the now-standard soft attention mechanisms. The model is validated on the smallish WikiTableQuestions dataset, where the authors show that a single model performs worse than the approach which uses the traditional Semantic Parsing technique. However an ensemble of 15 models (trained in a variety of ways) results in comparable performance to the state of the art. I feel that the paper proposes an interesting solution to the hard problem of learning natural language interfaces for data bases. The model is an extension of the previously proposed models of Neelakantan 2016. The experimental section is rather weak though. The authors only show their model work on a single smallish dataset. Would love to see more ablation studies of their model and comparison against fancier version of memnns (i do not buy their initial response to not testing against memory networks). I do have a few objections though. -- The details of the model are rather convoluted and the Section 2.1 is not very clearly written. In particular with the absence of the accompanying code the model will be super hard to replicate. I wish the authors do a better job in explaining the details as to how exactly the discrete operations are modeled, what is the role of the *row selector*, the *scalar answer* and the *lookup answer* etc. -- The authors do a full attention over the entire database. Do they think this approach would scale when the data bases are huge (millions of rows)? Wish they experimented with larger datasets as well.","['3', '3', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[6, 13, 33, 28, 5]","[11, 19, 37, 34, 11]","[34, 300, 256, 393, 37]","[15, 143, 176, 258, 7]","[18, 146, 14, 114, 28]","[1, 11, 66, 21, 2]","The sentiment score is slightly positive (20) because the reviewer acknowledges the paper as 'interesting' and presenting a solution to a 'hard problem'. However, they also express concerns about the experimental section and model details, which tempers the positivity. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, expressing their opinions as personal views ('I feel', 'I wish') and using polite phrases like 'Would love to see'. They offer constructive criticism without harsh language. The reviewer maintains a professional tone, balancing praise with areas for improvement, which contributes to both the slightly positive sentiment and the polite tone.",20,60
Learning and Policy Search in Stochastic Dynamical Systems with Bayesian Neural Networks,Accept,2017,"['Stefan Depeweg', 'José Miguel Hernández-Lobato', 'Finale Doshi-Velez', 'Steffen Udluft']","[7, 6, 7]","['Good paper, accept', 'Marginally above acceptance threshold', 'Good paper, accept']","This paper introduces an approach for model-based control of stochastic dynamical systems with policy search, based on (1) learning the stochastic dynamics of the underlying system with a Bayesian deep neural network (BNN) that allows some of its inputs to be stochastic, and (2) a policy optimization method based on simulated rollouts from the learned dynamics. BNN training is carried out using alpha-divergence minimization, the specific form of which was introduced in previous work by the authors. Validation and comparison of the approach is undertaken on a simulated domain, as well as real-world scenarios. The paper is tightly written, and easy to follow. Its approach to fitting Bayesian neural networks with alpha divergence is interesting and appears novel in this context. The resulting application to model-based control appears to have significant practical impact, particularly in light of the explainability that a system model can bring to specific decisions made by the policy. As such, I think that the paper brings a valuable contribution to the literature. That said, I have a few questions and suggestions: 1) In section 2.2, it should be explained how the random z_n input is used by the neural network: is it just concatenated to the other inputs and used as-is, or is there a special treatment? 2) Moreover, much case is made for the need to have stochastic inputs, but only a scalar input seems to be provided throughout. Is this enough? How computationally difficult would providing stochastic inputs of higher dimensionality be? 3) How important is the normality assumption in z_n? How is the variance gamma established? 4) It is mentioned that the hidden layers of the neural network are made of rectifiers, but no further utilization of this fact is made in the paper. Is this assumption somehow important in the optimization of the alpha-divergence (beyond what we know about rectifiers to mitigate the vanishing gradient problem) ? 5) Equation (3), denominator mathbf{y} should be mathbf{Y} ? 6) Section 2.3: it would be helpful to have an overview or discussion of the computational complexity of training BNNs, to understand whether and when they can practicably be used. 7) Between eq (12) and (13), a citation to the statement of the time embedding theorem would be helpful, as well as an indication of how the embedding dimension should be chosen. 8) Figure 1: the subplots should have the letters by which they are referenced in the text on p. 7. 9) In section 4.2.1, it is not clear if the gas turbine data is publicly available, and if so where. In addition more details should be provided, such as the dimensionality of the variables E_t, N_t and A_t. 10) Perhaps the comparisons with Gaussian processes should include variants that support stochastic inputs, such as Girard et al. (2003), to provide some of the same modelling capabilities as what’s made use of here. At least, this strand of work should be mentioned in Section 5. References: Girard, A., Rasmussen, C. E., Quiñonero Candela, J., & Murray Smith, R. (2003). Gaussian process priors with uncertain inputs-application to multiple-step ahead time series forecasting. Advances in Neural Information Processing Systems, 545-552.","['3', '3', '3']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[2, 12, 11, 12]","[4, 18, 17, 18]","[8, 163, 200, 55]","[4, 82, 83, 30]","[4, 70, 96, 16]","[0, 11, 21, 9]","The sentiment score is 80 (positive) because the reviewer expresses a very positive view of the paper, stating it is 'tightly written, and easy to follow' and that it 'brings a valuable contribution to the literature'. The reviewer also praises the novelty and practical impact of the approach. The score is not 100 as the reviewer does have some questions and suggestions for improvement. The politeness score is 90 (very polite) because the reviewer uses respectful and constructive language throughout. They begin with positive comments, frame their suggestions as questions or recommendations rather than criticisms, and use phrases like 'it would be helpful' and 'perhaps'. The tone is professional and courteous throughout, without any harsh or rude language.",80,90
Learning through Dialogue Interactions by Asking Questions,Accept,2017,"['Jiwei Li', 'Alexander H. Miller', 'Sumit Chopra', ""Marc'Aurelio Ranzato"", 'Jason Weston']","[7, 7, 8]","['Good paper, accept', 'Good paper, accept', 'Top 50% of accepted papers, clear accept']","The paper introduces a simulator and a set of synthetic question answering tasks where interaction with the *teacher* via asking questions is desired. The motivation is that an intelligent agent can improve its performance by asking questions and getting corresponding feedback from users. The paper studies this problem in an offline supervised and an online reinforcement learning settings. The results show that the models improve by asking questions. -- The idea is novel, and is relatively unexplored in the research community. The paper serves as a good first step in that direction. -- The paper studies three different types of tasks where the agent can benefit from user feedback. -- The paper is well written and provides a clear and detailed description of the tasks, models and experimental settings. Other comments/questions: -- What is the motivation behind using both vanilla-MemN2N AND Cont-MemN2N? Is using both resulting in any conclusions which are adding to the paper*s contributions? -- In the Question Clarification setting, what is the distribution of misspelled words over question entity, answer entity, relation entity or none of these? If most of the misspelled words come from relation entities, it might be a much easier problem than it seems. -- The first point on Page 10 *The performance of TestModelAQ is worse than TestAQ but better than TestQA.* is not true for Task 2 from the numbers in Tables 2 and 4. -- What happens if the conversational history is smaller or none? -- Figure 5, Task 6, why does the accuracy for good student drop when it stops asking questions? It already knows the relevant facts, so asking questions is not providing any additional information to the good student. -- Figure 5, Task 2, the poor student is able to achieve almost 70% of the questions correct even without asking questions. I would expect this number to be quite low. Any explanation behind this? -- Figure 1, Task 2 AQ, last sentence should have a negative response *(-)* instead of positive as currently shown. Preliminary Evaluation: A good first step in the research direction of learning dialogue agents from unstructured user interaction.","['3', '3', '5']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[12, 2, 17, 12, 19]","[18, 7, 23, 18, 25]","[87, 24, 44, 106, 237]","[36, 11, 29, 57, 126]","[23, 13, 11, 44, 84]","[28, 0, 4, 5, 27]","The sentiment score is 70 (positive) because the reviewer expresses that the paper introduces a novel idea, is well-written, and provides clear descriptions. They state it's a 'good first step' in this research direction. The politeness score is 80 (polite) as the reviewer uses respectful language throughout, offering constructive feedback and asking questions rather than making harsh criticisms. They acknowledge the paper's strengths before presenting their questions and comments. The reviewer maintains a professional and courteous tone, even when pointing out potential issues or seeking clarification.",70,80
Learning to Compose Words into Sentences with Reinforcement Learning,Accept,2017,"['Dani Yogatama', 'Phil Blunsom', 'Chris Dyer', 'Edward Grefenstette', 'Wang Ling']","[7, 8, 6]","['Good paper, accept', 'Top 50% of accepted papers, clear accept', 'Marginally above acceptance threshold']","I have not much to add to my pre-review comments. It*s a very well written paper with an interesting idea. Lots of people currently want to combine RL with NLP. It is very en vogue. Nobody has gotten that to work yet in any really groundbreaking or influential way that results in actually superior performance on any highly relevant or competitive NLP task. Most people struggle with the fact that NLP requires very efficient methods on very large datasets and RL is super slow. Hence, I believe this direction hasn*t shown much promise yet and it*s not yet clear it ever will due to the slowness of RL. But many directions need to be explored and maybe eventually they will reach a point where they become relevant. It is interesting to learn the obviously inherent grammatical structure in language though sadly again, the trees here do not yet capture much of what our intuitions are. Regardless, it*s an interesting exploration, worthy of being discussed at the conference.","['5', '4', '3']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[9, 14, 19, 7, 8]","[14, 20, 24, 13, 13]","[74, 178, 268, 104, 68]","[33, 91, 161, 44, 41]","[34, 70, 90, 54, 21]","[7, 17, 17, 6, 6]","The sentiment score is 50 (slightly positive) because while the reviewer acknowledges the paper as 'very well written' with an 'interesting idea', they also express skepticism about the current effectiveness of combining RL with NLP. They note that this approach hasn't shown much promise yet, but still consider the paper 'worthy of being discussed at the conference'. The politeness score is 75 (quite polite) because the reviewer uses respectful language throughout, acknowledging the paper's merits and the importance of exploring different research directions. They offer constructive criticism without being harsh or dismissive, maintaining a professional and courteous tone.",50,75
Learning to Generate Samples from Noise through Infusion Training,Accept,2017,"['Florian Bordes', 'Sina Honari', 'Pascal Vincent']","[8, 7, 6]","['Top 50% of accepted papers, clear accept', 'Good paper, accept', 'Marginally above acceptance threshold']","This paper trains a generative model which transforms noise into model samples by a gradual denoising process. It is similar to a generative model based on diffusion. Unlike the diffusion approach: - It uses only a small number of denoising steps, and is thus far more computationally efficient. - Rather than consisting of a reverse trajectory, the conditional chain for the approximate posterior jumps to q(z(0) | x), and then runs in the same direction as the generative model. This allows the inference chain to behave like a perturbation around the generative model, that pulls it towards the data. (This also seems somewhat related to ladder networks.) - There is no tractable variational bound on the log likelihood. I liked the idea, and found the visual sample quality given a short chain impressive. The inpainting results were particularly nice, since one shot inpainting is not possible under most generative modeling frameworks. It would be much more convincing to have a log likelihood comparison that doesn*t depend on Parzen likelihoods. Detailed comments follow: Sec. 2: *theta(0) the* -> *theta(0) be the* *theta(t) the* -> *theta(t) be the* *what we will be using* -> *which we will be doing* I like that you infer q(z^0|x), and then run inference in the same order as the generative chain. This reminds me slightly of ladder networks. *q*. Having learned* -> *q*. [paragraph break] Having learned* Sec 3.3: *learn to inverse* -> *learn to reverse* Sec. 4: *For each experiments* -> *For each experiment* How sensitive are your results to infusion rate? Sec. 5: *appears to provide more accurate models* I don*t think you showed this -- there*s no direct comparison to the Sohl-Dickstein paper. Fig 4. -- neat!","['5', '5', '4']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[2, 10, 28]","[8, 16, 34]","[14, 47, 128]","[3, 19, 63]","[10, 22, 52]","[1, 6, 13]","The sentiment score is 60 (positive) because the reviewer expresses appreciation for the paper's idea and finds the visual sample quality impressive, particularly the inpainting results. They use phrases like 'I liked the idea' and 'impressive'. However, they also suggest improvements, which prevents the score from being higher. The politeness score is 70 (polite) because the reviewer uses respectful language throughout, offering constructive criticism and suggestions rather than harsh critiques. They use phrases like 'It would be much more convincing' instead of more direct criticisms. The reviewer also takes care to point out positive aspects before suggesting improvements. The presence of specific, helpful suggestions for improvements (like correcting typos) also contributes to the polite tone.",60,70
Learning to Navigate in Complex Environments,Accept,2017,"['Piotr Mirowski', 'Razvan Pascanu', 'Fabio Viola', 'Hubert Soyer', 'Andy Ballard', 'Andrea Banino', 'Misha Denil', 'Ross Goroshin', 'Laurent Sifre', 'Koray Kavukcuoglu', 'Dharshan Kumaran', 'Raia Hadsell']","[7, 5, 7]","['Good paper, accept', '5', 'Good paper, accept']","This paper shows that a deep RL approach augmented with auxiliary tasks improves performance on navigation in complex environments. Specifically, A3C is used for the RL problem, and the agent is simultaneously trained on an unsupervised depth prediction task and a self-supervised loop closure classification task. While the use of auxiliary tasks to improve training of models including RL agents is not new, the main contribution here is the use of tasks that encourage learning an intrinsic representation of space and movement that enables significant improvements on maze navigation tasks. The paper is well written, experiments are convincing, and the value of the auxiliary tasks for the problem are clear. However, the contribution is relatively incremental given previous work on RL for navigation and on auxiliary tasks. The work could become of greater interest provided broader analysis and insights on either optimal combinations of tasks for visual navigation (e.g. the value of other visual / geometry-based tasks), or on auxiliary tasks with RL in general. As it is, it is a useful demonstration of the benefit of geometry-based auxiliary tasks for navigation, but of relatively narrow interest.","['5', '4', '3']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[11, 8, 11, 7, 2, 2, 8, 9, 6, 9, 3, 15]","[17, 14, 17, 13, 7, 7, 14, 15, 12, 14, 8, 21]","[50, 156, 67, 22, 9, 17, 54, 24, 29, 101, 15, 101]","[25, 63, 34, 11, 2, 7, 25, 11, 10, 46, 3, 47]","[17, 87, 18, 10, 6, 9, 28, 12, 14, 46, 8, 44]","[8, 6, 15, 1, 1, 1, 1, 1, 5, 9, 4, 10]","The sentiment score is slightly positive (20) because the reviewer acknowledges the paper's strengths, such as being well-written, having convincing experiments, and demonstrating clear value of the auxiliary tasks. However, the reviewer also notes that the contribution is 'relatively incremental' and of 'relatively narrow interest,' which tempers the positive sentiment. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, acknowledging the paper's merits before offering constructive criticism. The reviewer avoids harsh language and presents their concerns in a professional manner, suggesting ways to improve the work's impact rather than dismissing it outright.",20,50
Learning to Optimize,Accept,2017,"['Ke Li', 'Jitendra Malik']","[6, 7, 7]","['Marginally above acceptance threshold', 'Good paper, accept', 'Good paper, accept']","This papers adds to the literature on learning optimizers/algorithms that has gained popularity recently. The authors choose to use the framework of guided policy search at the meta-level to train the optimizers. They also opt to train on random objectives and assess transfer to a few simple tasks. As pointed below, this is a useful addition. However, the argument of using RL vs gradients at the meta-level that appears below is not clear or convincing. I urge the authors to run an experiment comparing the two approaches and to present comparative results. This is a very important question, and the scalability of this approach could very well hinge on this fact. Indeed, demonstrating both scaling to large domains and transfer to those domains is the key challenge in this domain. In summary, the idea is a good one, but the experiments are weak.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[-5, 13, -5]","[1, 19, 1]","[2, 83, 2]","[1, 45, 1]","[1, 28, 1]","[0, 10, 0]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the paper as a 'useful addition' and describes the idea as 'a good one', they also state that 'the experiments are weak' and urge the authors to conduct additional experiments. The overall tone suggests that the paper has potential but falls short in its current state. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, such as 'I urge the authors' instead of making demands. They also acknowledge the positive aspects of the paper before providing criticism. However, the directness of some statements (e.g., 'the experiments are weak') prevents a higher politeness score.",-20,50
Learning to Perform Physics Experiments via Deep Reinforcement Learning,Accept,2017,"['Misha Denil', 'Pulkit Agrawal', 'Tejas D Kulkarni', 'Tom Erez', 'Peter Battaglia', 'Nando de Freitas']","[6, 7, 7, 7]","['Marginally above acceptance threshold', 'Good paper, accept', 'Good paper, accept', 'Good paper, accept']","This paper addresses the question of how to utilize physical interactions to answer questions about physical outcomes. This question falls into a popular stream in ML community -- understanding physics. The paper moved a step further and worked on experimental setups where there is no prior about the physical properties/rules and it uses a deep reinforcement learning (DRL) technique to address the problem. My overall opinion about this paper is: an interesting attempt and idea, yet without a clear contribution. The experimental setups are quite interesting. The goal is to figure out which blocks are heavier or which blocks are glued together -- only by pushing and pulling objects around without any prior. The paper also shows reasonable performances on each task with detailed scenarios. While these experiments and results are interesting, the contribution is unclear. My main question is: does this result bring us any new insight? While the scenarios are interesting and focused on physical experiments, this is not any more different (potentially easier) than learning from playing games (e.g. Atari). In other words, are the tasks really different from other typical popular DRL tasks? To this end, I would have been more excited if authors showed some more new insights or experiments on learned representations and etc. Currently, the paper only discusses the factual outcome. For example, it describes the experimental setup and how much performances an agent could achieve. The authors could probably dissect the learned representations further, or discuss how the experimental results are linked to the human behavior or physical properties/laws. I am very in-between for my overall rating. I think the paper could have a deeper analysis. I however recommend the acceptance because of its merit of the idea. The followings are some detailed questions (not directly impacting my overall rating): (1) Page 2 *we assume that the agent has no prior knowledge about the physical properties of objects, or the laws of physics, and hence must interact with the objects in order to learn to answer questions about these properties.*: why does one *must* interact with objects in order to learn about the properties? Can*t we also learn through observation? (2) Figure 1right is missing a Y-axis label. (3) Page 3: A relating to bandit is interesting, but the formal approach is all based on DRL. (4) Page 5 *which makes distinguishing between the two heaviest blocks very difficult*: I am a bit confused why having a small mass gap makes the task harder (unless it*s really close to 0). Shouldn*t a machine be possible to distinguish even a pixel difference of speed? If not, isn*t this just because of the network architecture? (5) Page 5 *Since the agents exhibit similar performance using pixels and features we conduct the remaining experiments in this section using feature observations, since these agents are substantially faster to train.*: How about at least showing a correlation of performances at the instance level (rather than average performances)? Even so, I think this is a bit of big conclusion. (6) Throughout the papers, I felt that many conclusions (e.g. difficulty and etc) are based on a particularly chosen training distribution. For example, how does an agent really know when the instance is any more difficult? Doesn*t this really depend on the empirically learned distribution of training samples (i.e. P(m_3 | m_1, m_2), where m_i indicates masses of object 1, 2, and 3)? In other words, does what*s hard/easy matter much unless this is more thoroughly tested over various types of distributions? (7) Any baseline approach?","['3', '3', '3', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[8, 7, 7, 13, 13, 18]","[14, 13, 9, 18, 19, 24]","[54, 100, 25, 39, 93, 199]","[25, 48, 12, 22, 38, 101]","[28, 51, 12, 14, 49, 84]","[1, 1, 1, 3, 6, 14]","The sentiment score is slightly negative (-20) because while the reviewer finds the paper interesting and recommends acceptance, they express significant reservations about the paper's contribution and depth of analysis. The reviewer states 'My overall opinion about this paper is: an interesting attempt and idea, yet without a clear contribution' and 'I think the paper could have a deeper analysis.' These criticisms outweigh the positive aspects, resulting in a slightly negative sentiment. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, acknowledges the paper's merits, and frames criticisms constructively. For example, they say 'I would have been more excited if authors showed some more new insights' rather than directly criticizing. The reviewer also provides detailed feedback to help improve the paper, which is a polite and helpful approach.",-20,50
Learning to Play in a Day: Faster Deep Reinforcement Learning by Optimality Tightening,Accept,2017,"['Frank S.He', 'Yang Liu', 'Alexander G. Schwing', 'Jian Peng']","[9, 4, 9]","['9', 'Ok but not good enough - rejection', '9']","In this paper, the authors proposed a extension to the DQN algorithm by introducing both an upper and lower bound to the optimal Q function. The authors show experimentally that this approach improves the data efficiency quite dramatically such that they can achieve or even supersede the performance of DQN that is trained in 8 days. The idea is novel to the best of my knowledge and the improvement over DQN seems very significant. Recently, Remi et al have introduced the Retrace algorithm which can make use of multi-step returns to estimate Q values. As I suspect, some of the improvements that comes from the bounds is due to the fact that multi-step returns is used effectively. Therefore, I was wondering whether the authors have tried any approach like Retrace or Tree backup by Precup et al. and if so how do these methods stack up against the proposed method. The author have very impressive results and the paper proposes a very promising direction for future research and as a result I would like to make a few suggestions: First, it would be great if the authors could include a discussion about deterministic vs stochastic MDPs. Second, it would be great if the authors could include some kind of theoretically analysis about the approach. Finally, I would like to apologize for the late review.","['3', '4', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[2, 2, 11, 9]","[2, 5, 17, 15]","[2, 14, 233, 166]","[1, 7, 129, 84]","[1, 5, 99, 62]","[0, 2, 5, 20]","The sentiment score is 80 (positive) because the reviewer expresses that the idea is novel, the improvement over DQN is significant, and the results are impressive. They describe the paper as proposing a 'very promising direction for future research'. The politeness score is 70 (polite) because the reviewer uses respectful language throughout, makes suggestions rather than demands, and even apologizes for the late review. The reviewer acknowledges the authors' work positively and offers constructive feedback. The slightly lower politeness score (compared to sentiment) is due to the directness of some suggestions, which is common in academic reviews but not overly effusive in politeness.",80,70
"Learning to Query, Reason, and Answer Questions On Ambiguous Texts",Accept,2017,"['Xiaoxiao Guo', 'Tim Klinger', 'Clemens Rosenbaum', 'Joseph P. Bigus', 'Murray Campbell', 'Ban Kawas', 'Kartik Talamadupula', 'Gerry Tesauro', 'Satinder   Singh']","[6, 7, 7]","['Marginally above acceptance threshold', 'Good paper, accept', 'Good paper, accept']","This paper proposed an integration of memory network with reinforcement learning. The experimental data is simple, but the model is very interesting and relatively novel. There are some questions about the model: 1. how does the model extend to the case with multiple variables in a single sentence? 2. If the answer is out of vocabulary, how would the model handle it? 3. I hope the authors can provide more analysis about the curriculum learning part, since it is very important for the RL model training. 4. In the training, in each iteration, how the data samples were selected, by random or from simple one depth to multiple depth?","['3', '4', '3']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[5, 14, 2, 18, 37, 7, 11, 31, 27]","[11, 20, 7, 18, 42, 11, 17, 37, 33]","[86, 38, 12, 12, 78, 19, 93, 118, 291]","[39, 22, 5, 8, 36, 10, 52, 67, 189]","[38, 15, 7, 0, 23, 3, 33, 28, 72]","[9, 1, 0, 4, 19, 6, 8, 23, 30]","The sentiment score is 50 (slightly positive) because the reviewer describes the model as 'very interesting and relatively novel', which is a positive assessment. However, they also raise several questions and request more analysis, indicating some reservations. The politeness score is 75 (quite polite) because the reviewer uses respectful language throughout, framing their comments as questions or suggestions rather than criticisms. They use phrases like 'I hope the authors can provide' which is a polite way to request additional information. The review maintains a professional and constructive tone without any harsh or rude language.",50,75
Learning to Remember Rare Events,Accept,2017,"['Lukasz Kaiser', 'Ofir Nachum', 'Aurko Roy', 'Samy Bengio']","[6, 8, 7]","['Marginally above acceptance threshold', 'Top 50% of accepted papers, clear accept', 'Good paper, accept']","This paper proposes a new memory module for large scale life-long and one-shot learning. The module is general enough that the authors apply the module to several neural network architectures and show improvements in performance. Using k-nearest neighbors for memory access is not completely new. This has been recently explored in Rae et al., 2016 and Chandar et al., 2016. K-nearest neighbors based memory for one-shot learning has also been explored in [R1]. This paper provides experimental evidence that such an approach can be applied to a variety of architectures. Authors have addressed all my pre-review questions and I am ok with their response. Are the authors willing to release the source code to reproduce the results? At least for omniglot experiments and synthetic task experiments? References: [R1] Charles Blundell, Benigno Uria, Alexander Pritzel, Yazhe Li, Avraham Ruderman, Joel Z. Leibo, Jack Rae, Daan Wierstra, Demis Hassabis: Model-Free Episodic Control. CoRR abs/1606.04460 (2016)","['5', '3', '4']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[13, 2, 4, 31]","[18, 8, 9, 37]","[84, 104, 28, 261]","[44, 42, 10, 155]","[32, 61, 14, 60]","[8, 1, 4, 46]","The sentiment score is 50 (slightly positive) because the reviewer acknowledges the paper's contributions and improvements in performance, while also noting that some aspects are not completely new. The reviewer seems satisfied with the authors' responses to pre-review questions, which indicates a positive sentiment. However, the lack of strong praise or enthusiasm keeps the score from being higher. The politeness score is 75 (quite polite) because the reviewer uses respectful language throughout, acknowledges the authors' work, and asks politely about code release. The tone is professional and constructive, without any harsh criticism. The reviewer also cites relevant work appropriately, showing respect for the field. The question about code release is framed as a polite inquiry rather than a demand, further contributing to the politeness score.",50,75
Learning to Repeat: Fine Grained Action Repetition for Deep Reinforcement Learning,Accept,2017,"['Sahil Sharma', 'Aravind S. Lakshminarayanan', 'Balaraman Ravindran']","[8, 7, 8]","['Top 50% of accepted papers, clear accept', 'Good paper, accept', 'Top 50% of accepted papers, clear accept']","This paper shows that extending deep RL algorithms to decide which action to take as well as how many times to repeat it leads to improved performance on a number of domains. The evaluation is very thorough and shows that this simple idea works well in both discrete and continuous actions spaces. A few comments/questions: - Table 1 could be easier to interpret as a figure of histograms. - Figure 3 could be easier to interpret as a table. - How was the subset of Atari games selected? - The Atari evaluation does show convincing improvements over A3C on games requiring extended exploration (e.g. Freeway and Seaquest), but it would be nice to see a full evaluation on 57 games. This has become quite standard and would make it possible to compare overall performance using mean and median scores. - It would also be nice to see a more direct comparison to the STRAW model of Vezhnevets et al., which aims to solve some of the same problems as FiGAR. - FiGAR currently discards frames between action decisions. There might be a tradeoff between repeating an action more times and throwing away more information. Have you thought about separating these effects? You could train a model that does process intermediate frames. Just a thought. Overall, this is a nice simple addition to deep RL algorithms that many people will probably start using. -------------------- I*m increasing my score to 8 based on the rebuttal and the revised paper.","['5', '3', '4']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[5, 3, 20]","[11, 3, 26]","[36, 10, 253]","[12, 6, 142]","[12, 3, 86]","[12, 1, 25]","The sentiment score is 70 (positive) because the reviewer starts by highlighting the paper's strengths, noting that it shows improved performance and has a thorough evaluation. The overall tone is positive, with phrases like 'nice simple addition' and the increase in score mentioned at the end. However, it's not 100 as there are some suggestions for improvement. The politeness score is 80 because the reviewer uses respectful language throughout, phrasing criticisms as suggestions ('could be easier to interpret', 'it would be nice to see') and ending on a positive note. The reviewer also asks questions rather than making demands, which is a polite approach. The score isn't 100 as the language, while polite, isn't excessively formal or deferential.",70,80
Learning to superoptimize programs,Accept,2017,"['Rudy Bunel', 'Alban Desmaison', 'M. Pawan Kumar', 'Philip H.S. Torr', 'Pushmeet Kohli']","[6, 8, 7]","['Marginally above acceptance threshold', 'Top 50% of accepted papers, clear accept', 'Good paper, accept']","This work builds on top of STOKE (Schkufza et al., 2013), which is a superoptimization engine for program binaries. It works by starting with an existing program, and proposing modifications to it according to a proposal distribution. Proposals are accepted according to the Metropolis-Hastings criteria. The acceptance criteria takes into account the correctness of the program, and performance of the new program. Thus, the MCMC process is likely to converge to correct programs with high performance. Typically, the proposal distribution is fixed. The contribution of this work is to learn the proposal distribution as a function of the features of the program (bag of words of all the opcodes in the program). The experiments compare with the baselines of uniform proposal distribution, and a baseline where one just learns the weights of the proposal distribution but without conditioning on the features of the program. The evaluation shows that the proposed method has slightly better performance than the compared baselines. The significance of this work at ICLR seems to be quite low., both because this is not a progress in learning representations, but a straightforward application of neural networks and REINFORCE to yet another task which has non-differentiable components. The task itself (superoptimization) is not of significant interest to ICLR readers/attendees. A conference like AAAI/UAI seem a better fit for this work. The proposed method is seemingly novel. Typical MCMC-based synthesis methods are lacking due to their being no learning components in them. However, to make this work compelling, the authors should consider demonstrating the proposed method in other synthesis tasks, or even more generally, other tasks where MH-MCMC is used, and a learnt proposal distribution can be beneficial. Superoptimization alone (esp with small improvements over baselines) is not compelling enough. It is also not clear if there is any significant representation learning is going on. Since a BoW feature is used to represent the programs, the neural network cannot possibly learn anything more than just correlations between presence of opcodes and good moves. Such a model cannot possibly understand the program semantics in any way. It would have been a more interesting contribution if the authors had used a model (such as Tree-LSTM) which attempts to learn the semantics the program. The quite naive method of learning makes this paper not a favorable candidate for acceptance.","['5', '4', '4']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[2, 4, 16, 27, 15]","[8, 10, 22, 33, 21]","[43, 25, 118, 624, 322]","[16, 9, 57, 299, 180]","[24, 15, 43, 242, 102]","[3, 1, 18, 83, 40]","The sentiment score is -60 because the reviewer expresses a generally negative view of the paper. They state that the significance of the work 'seems to be quite low' for ICLR, suggest it would be better suited for other conferences, and describe the method as 'quite naive' and 'not compelling enough'. However, they do acknowledge some positive aspects, such as the method being 'seemingly novel', which prevents the score from being even lower. The politeness score is 20 because while the reviewer is critical, they express their opinions in a professional and relatively respectful manner. They use phrases like 'The authors should consider' and 'It would have been more interesting if', which soften the criticism. However, the overall tone is more matter-of-fact than overtly polite, hence the relatively low positive score.",-60,20
Lie-Access Neural Turing Machines,Accept,2017,"['Greg Yang', 'Alexander Rush']","[6, 7, 6, 8]","['Marginally above acceptance threshold', 'Good paper, accept', 'Marginally above acceptance threshold', 'Top 50% of accepted papers, clear accept']","The paper introduces a novel memory mechanism for NTMs based on differentiable Lie groups. This allows to place memory elements as points on a manifold, while still allowing training with backpropagation. It*s a more general version of the NTM memory, and possibly allows for training a more efficient addressing schemes. Pros: - novel and interesting idea for memory access - nicely written Cons: - need to manually specify the Lie group to use (it would be better if network could learn the best way of accessing memory) - not clear if this really works better than standard NTM (compared only to simplified version) - not clear if this is useful in practice (no comparison on real tasks)","['4', '4', '3', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[4, 12]","[10, 18]","[46, 181]","[17, 89]","[27, 83]","[2, 9]","The sentiment score is slightly positive (20) because the reviewer acknowledges the paper's novel and interesting idea, and mentions it's nicely written. However, they also list several cons which balance out the positives, resulting in a mildly positive overall sentiment. The politeness score is moderately positive (50) as the reviewer uses neutral language and presents both pros and cons in a balanced manner without harsh criticism. They objectively state the limitations of the work without using negative or dismissive language. The review is constructive and professional in tone, which contributes to its politeness.",20,50
Loss-aware Binarization of Deep Networks,Accept,2017,"['Lu Hou', 'Quanming Yao', 'James T. Kwok']","[7, 7, 7]","['Good paper, accept', 'Good paper, accept', 'Good paper, accept']","This paper proposed a proximal (quasi-) Newton’s method to learn binary DNN. The main contribution is to combine pre-conditioning with binarization in a proximal framework. It is interesting to have a proximal Newton’s method to interpret the different DNN binarization schemes. This gives a new interpretation of existing approaches. However, the theoretical analysis is not very convincing or useful. The formulated optimization problem (3)-(4) is essentially a mixed integer programming. Even though the paper treats the integer part as a constraint and address it in proximal operators, the constraint set is still discrete and there is no guarantee that the proximal Newton algorithm could converge under practically useful conditions. In practice it is hard to verify the assumption [d_t^t]_k > _x0008_eta in Theorem 3.1. This relation could be hard to hold in DNN as the loss surface could be extremely complicated.","['3', '4', '3']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[3, 6, 25]","[9, 12, 31]","[70, 140, 257]","[31, 55, 136]","[22, 64, 46]","[17, 21, 75]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the paper's interesting aspects and new interpretations, they express significant concerns about the theoretical analysis and practical applicability. The reviewer states that the analysis is 'not very convincing or useful' and points out potential issues with the algorithm's convergence and assumptions. The politeness score is moderately positive (50) as the reviewer uses neutral language and acknowledges positive aspects before presenting criticisms. They avoid harsh or personal comments, instead focusing on specific technical issues. The reviewer maintains a professional tone throughout, balancing positive observations with constructive criticism.",-20,50
Lossy Image Compression with Compressive Autoencoders,Accept,2017,"['Lucas Theis', 'Wenzhe Shi', 'Andrew Cunningham', 'Ferenc Huszár']","[8, 7, 5]","['Top 50% of accepted papers, clear accept', 'Good paper, accept', '5']","This work proposes a new approach for image compression using auto encoders. The results are impressive, besting the state of the art in this field. Pros: + Very clear paper. It should be possible to replicate these results should one be inclined to do so. + The results, when compared to other work in this field are very promising. I need to emphasize, and I think the authors should have emphasized this fact as well: this is very new technology and it should not be surprising it*s not better than the state of the art in image compression. It*s definitely better than other neural network approaches to compression, though. Cons: - The training procedure seems clunky. It requires multiple training stages, freezing weights, etc. - The motivation behind Figure 1 is a bit strange, as it*s not clear what it*s trying to illustrate, and may confuse readers (it talks about effects on JPEG, but the paper discusses a neural network architecture, not DCT quantization)","['5', '3', '4']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[8, 8, 13, 7]","[14, 13, 19, 13]","[44, 77, 47, 36]","[18, 47, 38, 13]","[22, 17, 1, 20]","[4, 13, 8, 3]","The sentiment score is 70 (positive) because the reviewer starts by praising the work as 'impressive' and 'besting the state of the art'. They list several pros, including the clarity of the paper and promising results. While there are some cons mentioned, they are presented as minor issues and the overall tone remains positive. The politeness score is 80 (polite) because the reviewer uses respectful language throughout, acknowledging the authors' work positively. They even suggest that the authors should have emphasized their achievement more, which is a supportive comment. The cons are presented constructively without harsh criticism. The reviewer's language is professional and considerate, maintaining a polite tone even when pointing out areas for improvement.",70,80
Machine Comprehension Using Match-LSTM and Answer Pointer,Accept,2017,"['Shuohang Wang', 'Jing Jiang']","[6, 7, 6]","['Marginally above acceptance threshold', 'Good paper, accept', 'Marginally above acceptance threshold']","The paper looks at the problem of locating the answer to a question in a text (For this task the answer is always part of the input text). For this the paper proposes to combine two existing works: Match-LSTM to relate question and text representations and Pointer Net to predict the location of the answer in the text. Strength: - The suggested approach makes sense for the task and achieves good performance, (although as the authors mention, recent concurrent works achieve better results) - The paper is evaluated on the SQuAD dataset and achieves significant improvements over prior work. Weaknesses: 1. It is unclear from the paper how well it is applicable to other problem scenarios where the answer is not a subset of the input text. 2. Experimental evaluation 2.1. It is not clear why the Bi-Ans-Ptr in Table 2 is not used for the ensemble although it achieves the best performance. 2.2. It would be interested if this approach generalizes to other datasets. Other (minor/discussion points) - The task and approach seem to have some similarity of locating queries in images and visual question answering. The authors might want to consider pointing to related works in this direction. - I am wondering how much this task can be seen as a “guided extractive summarization”, i.e. where the question guides the summarization process. - Page 6, last paragraph: missing “.”: “… searching This…” Summary: While the paper presents an interesting combination of two approaches for the task of answer extraction, the novelty is moderate. While the experimental results are encouraging, it remains unclear how well this approach generalizes to other scenarios as it seems a rather artificial task.","['4', '3', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[3, 13]","[9, 19]","[94, 145]","[43, 105]","[50, 29]","[1, 11]","The sentiment score is slightly positive (20) because the reviewer acknowledges strengths of the paper, such as the approach making sense and achieving good performance. However, they also point out several weaknesses and describe the novelty as 'moderate'. The overall tone is balanced but leans slightly positive. The politeness score is moderately positive (50) as the reviewer uses professional and respectful language throughout. They offer constructive criticism and suggestions without harsh or rude phrasing. The use of phrases like 'The authors might want to consider...' and 'I am wondering...' contribute to the polite tone. The reviewer also provides a balanced summary, acknowledging both strengths and limitations of the work.",20,50
Maximum Entropy Flow Networks,Accept,2017,"['Gabriel Loaiza-Ganem *', 'Yuanjun Gao *', 'John P. Cunningham']","[6, 9, 6]","['Marginally above acceptance threshold', '9', 'Marginally above acceptance threshold']","This paper applies the idea of normalizing flows (NFs), which allows us to build complex densities with tractable likelihoods, to maximum entropy constrained optimization. The paper is clearly written and is easy to follow. Novelty is a weak factor in this paper. The main contributions come from (1) applying previous work on NFs to the problem of MaxEnt estimation and (2) addressing some of the optimization issues resulting from stochastic approximations to E[||T||] in combination with the annealing of Lagrange multipliers. Applying the NFs to MaxEnt is in itself not very novel as a framework. For instance, one could obtain a loss equivalent to the main loss in eq. (6) by minimizing the KLD between KL[p_{phi};f], where f is the unormalized likelihood f propto exp sum_k( - lambda_k T - c_k ||T_k||^2 ). This type of derivation is typical in all previous works using NFs for variational inference. A few experiments on more complex data would strengthen the paper*s results. The two experiments provided show good results but both of them are toy problems. Minor point: Although intuitive, it would be good to have a short discussion of step 8 of algorithm 1 as well.","['4', '5', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[1, 3, 12]","[7, 6, 18]","[29, 5, 96]","[11, 4, 50]","[17, 1, 31]","[1, 0, 15]","The sentiment score is slightly positive (20) because the reviewer acknowledges that the paper is clearly written and easy to follow, and the results are good. However, they also point out that novelty is a weak factor and the main contributions are applications of previous work. The politeness score is moderately positive (50) as the reviewer uses neutral language and provides constructive feedback without harsh criticism. They acknowledge the paper's strengths while also pointing out areas for improvement in a respectful manner. The reviewer suggests additional experiments and discussions, which is a polite way of indicating potential enhancements to the paper.",20,50
Metacontrol for Adaptive Imagination-Based Optimization,Accept,2017,"['Jessica B. Hamrick', 'Andrew J. Ballard', 'Razvan Pascanu', 'Oriol Vinyals', 'Nicolas Heess', 'Peter W. Battaglia']","[8, 8, 7, 8]","['Top 50% of accepted papers, clear accept', 'Top 50% of accepted papers, clear accept', 'Good paper, accept', 'Top 50% of accepted papers, clear accept']","A well written paper and an interesting construction - I thoroughly enjoyed reading it. I found the formalism a bit hard to follow without specific examples- that is, it wasn*t clear to me at first what the specific components in figure 1A were. What constitutes the controller, a control, the optimizer, what was being optimized, etc., in specific cases. Algorithm boxes may have been helpful, especially in the case of your experiments. A description of existing models that fall under your conceptual framework might help as well. In Practical Bayesian Optimization of Machine Learning Algorithms, Snoek, Larochelle and Adams propose optimizing with respect to expected improvement per second to balance computation cost and performance loss. It might be interesting to see how this falls into your framework. Experimental results were presented clearly and well illustrated the usefulness of the metacontroller. I*m curious to see the results of using more metaexperts.","['3', '3', '3', '3']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[7, 2, 8, 11, 9, 13]","[13, 7, 14, 17, 15, 19]","[43, 9, 156, 209, 200, 93]","[25, 2, 63, 101, 81, 38]","[18, 6, 87, 98, 111, 49]","[0, 1, 6, 10, 8, 6]","The sentiment score is 70 (positive) because the reviewer starts with praise, stating it's a 'well written paper' and that they 'thoroughly enjoyed reading it'. They also mention that the experimental results were 'presented clearly' and 'well illustrated'. However, it's not 100 as they do have some constructive criticism and suggestions for improvement. The politeness score is 80 (polite) because the reviewer uses respectful language throughout, offering suggestions rather than demands ('may have been helpful', 'might help'), and expressing curiosity about future work. They balance critique with praise and frame their feedback in a constructive manner. The language is consistently professional and courteous.",70,80
Mode Regularized Generative Adversarial Networks,Accept,2017,"['Tong Che', 'Yanran Li', 'Athul Jacob', 'Yoshua Bengio', 'Wenjie Li']","[6, 4, 7, 7]","['Marginally above acceptance threshold', 'Ok but not good enough - rejection', 'Good paper, accept', 'Good paper, accept']","The paper proposes two regularization approaches for training GAN, aiming to provide stronger gradient signal to move the generated distribution to data distribution and to avoid the generated distribution from getting trapped in only one or a few modes of the data distribution. The presented approaches are entirely based on some intuitive arguments. As such intuitions are interesting, likely useful, and deserve further exploration in a broader context, they stay as heuristics as this point. The paper will benefit from more rigorous theoretical justification of the presented approaches.","['5', '4', '4', '4']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[6, 4, 2, 30, 23]","[12, 10, 7, 36, 28]","[38, 62, 13, 977, 241]","[14, 27, 6, 405, 144]","[18, 23, 7, 456, 29]","[6, 12, 0, 116, 68]","The sentiment score is slightly positive (20) because the reviewer acknowledges the paper's interesting and potentially useful intuitions, and suggests that they deserve further exploration. However, the score is not higher due to the criticism that the approaches lack rigorous theoretical justification and are currently heuristic. The politeness score is moderately positive (50) as the reviewer uses respectful language, acknowledges the value of the work, and offers constructive criticism. The phrases 'interesting, likely useful, and deserve further exploration' indicate a polite and encouraging tone, while the suggestion for improvement is phrased diplomatically as 'The paper will benefit from...' rather than using more critical language.",20,50
Mollifying Networks,Accept,2017,"['Caglar Gulcehre', 'Marcin Moczulski', 'Francesco Visin', 'Yoshua Bengio']","[6, 6, 7]","['Marginally above acceptance threshold', 'Marginally above acceptance threshold', 'Good paper, accept']","The paper shows the relation between stochastically perturbing the parameter of a model at training time, and considering a mollified objective function for optimization. Aside from Eqs. 4-7 where I found hard to understand what the weak gradient g exactly represents, Eq. 8 is intuitive and the subsequent Section 2.3 clearly establishes for a given class of mollifiers the equivalence between minimizing the mollified loss and training under Gaussian parameter noise. The authors then introduce generalized mollifiers to achieve a more sophisticated annealing effect applicable to state-of-the-art neural network architectures (e.g. deep ReLU nets and LSTM recurrent networks). The resulting annealing effect can be counterintuitive: In Section 4, the Binomial (Bernoulli?) parameter grows from 0 (deterministic identity layers) to 1 (deterministic ReLU layers), meaning that the network goes initially through a phase of adding noise. This might effectively have the reverse effect of annealing. Annealing schemes used in practice seem very engineered (e.g. Algorithm 1 that determines how units are activated at a given layer consists of 9 successive steps). Due to the more conceptual nature of the authors contribution (various annealing schemes have been proposed, but the application of the mollifying framework is original), it could have been useful to reserve a portion of the paper to analyze simpler models with more basic (non-generalized) mollifiers. For example, I would have liked to see simple cases, where the perturbation schemes derived from the mollifier framework would be demonstrably more suitable for optimization than a standard heuristically defined perturbation scheme.","['4', '4', '5']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[5, 4, 3, 30]","[11, 7, 7, 36]","[74, 14, 20, 977]","[28, 7, 8, 405]","[38, 7, 12, 456]","[8, 0, 0, 116]","The sentiment score is 50 (slightly positive) because the reviewer acknowledges the paper's originality and clarity in certain sections, but also points out areas of confusion and suggests improvements. The reviewer appreciates the intuitive nature of some equations and the clear establishment of equivalence in Section 2.3. However, they also mention difficulties in understanding certain parts (Eqs. 4-7) and suggest that more analysis of simpler models would have been useful. The politeness score is 75 (quite polite) because the reviewer uses respectful language throughout, offering constructive criticism without harsh or dismissive statements. They use phrases like 'it could have been useful' and 'I would have liked to see' when suggesting improvements, which maintains a polite and professional tone.",50,75
Multi-view Recurrent Neural Acoustic Word Embeddings,Accept,2017,"['Wanjia He', 'Weiran Wang', 'Karen Livescu']","[5, 6, 6]","['5', 'Marginally above acceptance threshold', 'Marginally above acceptance threshold']","Pros: Interesting training criterion. Cons: Missing proper ASR technique based baselines. Comments: The dataset is quite small. ROC curves for detection, and more measurements, e.g. EER would probably be helpful besides AP. More detailed analysis of the results would be necessary, e.g. precision of words seen during training compared to the detection performance of out-of-vocabulary words. It would be interesting to show scatter plots for embedding vs. orthographic distances.","['4', '4', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[2, 8, 18]","[2, 14, 23]","[2, 107, 177]","[1, 54, 103]","[1, 49, 63]","[0, 4, 11]","The sentiment score is slightly negative (-20) because while the reviewer notes an 'interesting training criterion' as a pro, there are multiple cons and suggestions for improvement mentioned, indicating the review is more critical than positive overall. The reviewer points out missing baselines, a small dataset, and the need for more detailed analysis and measurements. The politeness score is neutral (0) as the language is direct and matter-of-fact without being particularly polite or rude. The reviewer uses neutral phrases like 'would be necessary' and 'would be interesting' rather than more forceful language, but also doesn't use overtly polite phrasing.",-20,0
Multilayer Recurrent Network Models of Primate Retinal Ganglion Cell Responses,Accept,2017,"['Eleanor Batty', 'Josh Merel', 'Nora Brackbill', 'Alexander Heitman', 'Alexander Sher', 'Alan Litke', 'E.J. Chichilnisky', 'Liam Paninski']","[8, 7, 4]","['Top 50% of accepted papers, clear accept', 'Good paper, accept', 'Ok but not good enough - rejection']","This paper explores the ability of nonlinear recurrent neural networks to account for neural response properties that have otherwise eluded the ability of other models. A multilayer rnn is trained to imitate the stimulus-response mapping measured from actual retinal ganglion cells in response to a sequence of natural images. The rnn performs significantly better, especially in accounting for transient responses, than conventional LN/GLM models. This work is an important step in understanding the nonlinear response properties of visual neurons. Recent results have shown that the responses of even retinal ganglion cells in response to natural movies are difficult to explain in terms of standard receptive field models. So this presents an important challenge to the field. If we even had *a* model that works, it would be a starting point. So this work should be seen in that light. The challenge now of course is to tease apart what the rnn is doing. Perhaps it could now be pruned and simplified to see what parts are critical to performance. It would have been nice to see such an analysis. Nevertheless this result is a good first start and I think important for people to know about. I am a bit confused about what is being called a *movie.* My understanding is that it is essentially a sequence of unrelated images shown for 1 sec. each. But then it is stated that the *frame rate* is 1/8.33 ms. I think this must refer to the refresh rate of the monitor, right? I would guess that the deviations from the LN model are even stronger when you show actual dynamic natural scenes - i.e., real movies. Here I would expect the rnn to have an even more profound effect, and potentially be much more informative.","['5', '4', '4']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[1, 6, 1, 1, 8, 8, 24, 17]","[5, 11, 6, 1, 14, 14, 30, 22]","[5, 49, 3, 1, 9, 11, 23, 108]","[4, 19, 2, 1, 7, 6, 16, 51]","[0, 25, 0, 0, 0, 0, 0, 8]","[1, 5, 1, 0, 2, 5, 7, 49]","The sentiment score is 70 (positive) because the reviewer expresses that the paper is 'an important step' and 'important for people to know about'. They describe the work as a 'good first start' and highlight its significance in addressing challenges in the field. The politeness score is 80 (polite) as the reviewer uses respectful language throughout, acknowledging the paper's contributions and suggesting improvements in a constructive manner. They use phrases like 'it would have been nice to see' rather than criticizing harshly. The reviewer also balances positive feedback with gentle suggestions for future work, maintaining a professional and courteous tone throughout the review.",70,80
Neural Photo Editing with Introspective Adversarial Networks,Accept,2017,"['Andrew Brock', 'Theodore Lim', 'J.M. Ritchie', 'Nick Weston']","[6, 5, 6]","['Marginally above acceptance threshold', '5', 'Marginally above acceptance threshold']","The paper presents two main contributions: (1) A novel model visualization and photo manipulation technique that allows to transform an image using a paintbrush, much like in an image editing software. (2) A hybridization of GANs and VAEs called Introspective Adversarial Network. The main problem I have with the paper is that it feels very much like two papers in one with a very loose story tying the two together. On one hand, the neural photo editing technique is presented in sufficient detail to be reproducible and it is shown to be effective. I personally find the idea exciting, but in order for it to be of interest to the ICLR community I think more emphasis should be put on what insights such a technique allows to gain on trained models. On the other hand, the IAF model is introduced, along with multiple network architecture modifications for improving its performance. One criticism that I have regarding the presentation is that it makes it hard to assign credit to individual ideas when they are presented in a *list of things to make it work* fashion. I would like to see more empirical results in that direction to help clear up things. Overall I think the paper proposes interesting ideas, but given its lack of focus on a single, cohesive story I think it is not yet ready for publication. UPDATE: The rating has been updated to a 6 following the authors* reply.","['4', '3', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[2, 17, 14, 2]","[8, 22, 18, 3]","[32, 62, 38, 6]","[12, 28, 12, 2]","[19, 9, 4, 4]","[1, 25, 22, 0]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some interesting ideas and contributions, they express significant concerns about the paper's structure and lack of focus. The reviewer states that the paper 'is not yet ready for publication' and feels like 'two papers in one with a very loose story.' However, the score is not deeply negative as the reviewer does find some aspects 'exciting' and 'interesting.' The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, acknowledging positive aspects alongside criticisms. They offer constructive feedback and suggestions for improvement rather than harsh criticism. The use of phrases like 'I personally find' and 'I would like to see' maintains a polite tone while expressing concerns.",-20,50
Neural Program Lattices,Accept,2017,"['Chengtao Li', 'Daniel Tarlow', 'Alexander L. Gaunt', 'Marc Brockschmidt', 'Nate Kushman']","[4, 7, 7]","['Ok but not good enough - rejection', 'Good paper, accept', 'Good paper, accept']","First I would like to apologize for the late review. This paper proposes an extension of the NPI model (Reed & de Freitas) by using an extension of the probabilistic stacks introduced in Mikolov et al.. This allows them to train their model with less supervision than Reed & de Freitas. Overall the model is a nice extension of NPI. While it requires less supervision than NPI, it still requires *sequences of elementary operations paired with environment observations, and [...] a couple of examples which include the full abstraction hierarchy*. This may limit the scope of this work. The paper claims that their *method is leverages stronger supervision in the form of elementary action sequences rather than just input-output examples (sic). Such sequences are relatively easy to gather in many natural settings*. It would be great if the authors clarify what they mean by *relatively easy to gather in many natural settings*. They also claim that *the additional supervision improves the data efficiency and allow our technique to scale to more complicated problems*. However, this paper only addresses two toy problems which are neither *natural settings* nor of a large scale (or at least not larger than those addressed in the related literature, see Zaremba et al. for addition). In the introduction, the author states that *Existing techniques, however, cannot be applied on data like this because it does not contain the abstraction hierarchy.* What are the *existing techniques*, they are referring to? This work only addresses the problem of long addition and puzzle solving in a block world. Afaik, Zaremba et al. has shown that with no supervision, it can solve the long addition problem and Sukhbaatar et al. (*Mazebase: A sandbox for learning from games*) shows that a memory network can solve puzzles in a blockworld with little supervision. In the conclusion, the author states that *remarkably, NPL achieves state-of-the-art performances with much less supervision compared to existing models, making itself more applicable to real-world applications where full program traces are hard to get.* However for all the experiments, they *include a small number of FULL samples* (FULL == *samples with full program traces*). Unfortunately even if this means that they need less FULL examples, they still need *full program traces*, contradicting their final claim. Moreover, as shown figure 7, their model does not use a *small number of FULL samples* but rather a significantly smaller amount of FULL examples than NPI, i.e., 16 vs 128. *All experiments were run with 10 different random seeds*: does the environment change as well between the runs, i.e. are the FULL examples different between the runs? If it is the case and since you select the best run (on a validation set), the NPL model does not consume 16 FULL examples but 160 FULL examples for nanoCraft. Concerning the NanoCraft example, it would be good to have more details about how the examples are generated: how do you make sure that the train/val/test sets are different? How the rectangular shape are generated? If I consider all possible rectangles in a 6x6 grid, there are (6x6)x(6x6)/2 = 648 possibilities, thus taking 256 examples sum up to ~40% of the total number of rectangles. This does not even account for the fact that from an initial state, many rectangles can be made, making my estimate probably lower than the real coverage of examples. Concerning the addition, it would interesting to show what an LSTM would do: Take a 2 layer LSTM that takes the 2 current digits as an input and produce the current output ( *123+45* would be input[0] = [3,5], input[1]=[2,4], input[2]=[1, 0] and output[0] = 8...). I would be curious to see how such baseline would work. It can be trained on input/output and it is barely different from a standard sequence model. Also, would it be possible to compare with Zaremba et al.? Finally, as discussed previously with the authors, it would be good if they discuss more in length the relation between their probabilistic stacks and Mikolov et al.. They have a lot of similarities and it is not addressed in the current version. It should be addressed in the section describing the approach. I believe the authors agreed on this and I will wait for the updated version. Overall, it is a nice extension of Reed & de Freitas, but I*m a bit surprised by the lack of discussion about the rest of the literature (beside Reed & de Freitas, most previous work are only lightly discussed in the related work). This would have been fine if this paper would not suffer from a relatively weak experiment section that does not support the claims made in this work or show results that were not obtained by others before. Missing references: *Learning simple arithmetic procedures*, Cottrell et al. *Neural gpus learn algorithms*, Kaiser & Sutskever *Mazebase: A sandbox for learning from games*, Sukhbaatar et al. *Learning simple algorithms from examples*, Zaremba et al.","['4', '4', '5']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[5, 12, 2, 8, 11]","[10, 18, 7, 14, 16]","[36, 83, 26, 75, 33]","[20, 46, 13, 41, 19]","[13, 35, 13, 29, 13]","[3, 2, 0, 5, 1]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('nice extension', 'allows them to train their model with less supervision'), they express several criticisms and doubts about the paper's claims and experimental support. The reviewer points out limitations, questions the authors' assertions, and suggests that the paper lacks sufficient discussion of related literature and comparison to existing work. The politeness score is moderately positive (50) as the reviewer maintains a professional tone throughout, uses polite language ('I would like to apologize', 'It would be great if', 'I believe the authors agreed'), and frames criticisms as suggestions or questions rather than direct attacks. The reviewer also acknowledges positive aspects of the work alongside the criticisms, which contributes to the overall polite tone.",-20,50
Neuro-Symbolic Program Synthesis,Accept,2017,"['Emilio Parisotto', 'Abdel-rahman Mohamed', 'Rishabh Singh', 'Lihong Li', 'Dengyong Zhou', 'Pushmeet Kohli']","[5, 7, 8]","['5', 'Good paper, accept', 'Top 50% of accepted papers, clear accept']","The paper presents a method to synthesize string manipulation programs based on a set of input output pairs. The paper focuses on a restricted class of programs based on a simple context free grammar sufficient to solve string manipulation tasks from the FlashFill benchmark. A probabilistic generative model called Recursive-Reverse-Recursive Neural Network (R3NN) is presented that assigns a probability to each program*s parse tree after a bottom-up and a top-down pass. Results are presented on a synthetic dataset and a Microsoft Excel benchmark called FlashFill. The problem of program synthesis is important with a lot of recent interest from the deep learning community. The approach taken in the paper based on parse trees and recursive neural networks seems interesting and promising. However, the model seems too complicated and unclear at several places (details below). On the negative side, the experiments are particularly weak, and the paper does not seem ready for publication based on its experimental results. I was positive about the paper until I realized that the method obtains an accuracy of 38% on FlashFill benchmark when presented with only 5 input-output examples but the performance degrades to 29% when 10 input-output examples are used. This was surprising to the authors too, and they came up with some hypothesis to explain this phenomenon. To me, this is a big problem indicating either a bug in the code or a severe shortcoming of the model. Any model useful for program synthesis needs to be applicable to many input-output examples because most complicated programs require many examples to disambiguate the details of the program. Given the shortcoming of the experiments, I am not convinced that the paper is ready for publication. Thus, I recommend weak reject. I encourage the authors to address the comments below and resubmit as the general idea seems promising. More comments: I am unclear about the model at several places: - How is the probability distribution normalized? Given the nature of bottom-up top-down evaluation of the potentials, should one enumerate over different completions of a program and the compare their exponentiated potentials? If so, does this restrict the applicability of the model to long programs as the enumeration of the completions gets prohibitively slow? - What if you only use 1 input-output pair for each program instead of 5? Do the results get better? - Section 5.1.2 is not clear to me. Can you elaborate by potentially including some examples? Does your input-output representation pre-supposes a fixed number of input-output examples across tasks (e.g. 5 or 10 for all of the tasks)? Regarding the experiments, - Could you present some baseline results on FlashFill benchmark based on previous work? - Is your method only applicable to short programs? (based on the choice of 13 for the number of instructions) - Does a program considered correct when it is identical to a test program, or is it considered correct when it succeeds on a set of held-out input-output pairs? - When using 100 or more program samples, do you report the accuracy of the best program out of 100 (i.e. recall) or do you first filter the programs based on training input-output pairs and then evaluate a program that is selected? Your paper is well beyond the recommended limit of 8 pages. please consider making it shorter.","['4', '3', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[3, 8, 9, 15, 17, 15]","[9, 14, 14, 20, 21, 21]","[32, 112, 133, 170, 62, 322]","[13, 50, 62, 89, 37, 180]","[17, 54, 50, 66, 17, 102]","[2, 8, 21, 15, 8, 40]","The sentiment score is -40 because while the reviewer initially found the approach interesting and promising, they ultimately recommended a 'weak reject' due to significant concerns about the experimental results and model performance. The reviewer expresses disappointment with the model's degrading performance when given more input-output examples, which they consider a 'big problem'. However, the score is not extremely negative as the reviewer still encourages the authors to address the comments and resubmit, seeing potential in the general idea. The politeness score is 60 because the reviewer maintains a professional and constructive tone throughout. They offer detailed feedback and suggestions for improvement, and use phrases like 'I encourage the authors' and 'the general idea seems promising'. The reviewer also explains their reasoning clearly and provides specific areas for improvement, which is helpful and considerate. However, the score is not extremely high as the criticism, while constructive, is still direct and pointed.",-40,60
Nonparametric Neural Networks,Accept,2017,"['George Philipp', 'Jaime G. Carbonell']","[7, 7, 5]","['Good paper, accept', 'Good paper, accept', '5']","This paper proposes a nonparametric neural network model, which automatically learns the size of the model during the training process. The key idea is to randomly add zero units and use sparse regularizer to automatically null out the weights that are irrelevant. The idea sounds to be a random search approach over discrete space with the help of sparse regularization to eliminate useless units. This is an important problem and the paper gives interesting results. My main comments are listed below: What is the additional computation complexity of the algorithm? The decomposition of each fan-in weights into a parallel component and an orthogonal component and the transformation into radial-angular coordinates may require a lot of extra computation time. The authors may need to discuss the extra amount of operations relative to the parametric neural network. Furthermore, it would be useful to show some running time experiments. It is observed that nonparametric networks return small networks on the convex dataset so that it is inferior to parametric networks. Any insight on this?","['3', '4', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[1, 25]","[7, 31]","[20, 350]","[9, 196]","[11, 79]","[0, 75]","The sentiment score is 50 (slightly positive) because the reviewer acknowledges the importance of the problem and the interesting results, stating 'This is an important problem and the paper gives interesting results.' However, they also raise some concerns and questions, which prevents the score from being higher. The politeness score is 75 (quite polite) because the reviewer uses respectful language throughout, framing their comments as questions or suggestions rather than criticisms. For example, they use phrases like 'The authors may need to discuss...' and 'It would be useful to show...', which are polite ways of requesting additional information. The reviewer also begins by acknowledging the paper's strengths before moving on to their comments, which is a courteous approach.",50,75
Normalizing the Normalizers: Comparing and Extending Network Normalization Schemes,Accept,2017,"['Mengye Ren', 'Renjie Liao', 'Raquel Urtasun', 'Fabian H. Sinz', 'Richard S. Zemel']","[7, 5, 9]","['Good paper, accept', '5', '9']","The authors present a unified framework for various divisive normalization schemes, and then show that a somewhat novel version of normalization does somewhat better on several tasks than some mid-strength baselines. Pros: * It has seemed for a while that there are a bunch of different normalization methods out there, of varying importance in varying applications, so having a standardized framework for them all, and evaluating them carefully and systematically, is a very useful contribution. * The paper is clearly written. * From an architectural standpoint, the actual comparisons seem well motivated. (For instance, I*m glad they tried DN* and BN* -- if they hadn*t tried those, I would have wanted them too.) Cons: * I*m not really sure what the difference is between their new DN method and standard cross-channel local contrast normalization. (Oh, actually -- looking at the other reviews, everyone else seems to have noticed this too. I*ll not beat a dead horse about this any further.) * I*m nervous that the conclusions that they state might not hold on larger, stronger tasks, like ImageNet, and with larger deeper models. I myself have found that while with smaller models on simpler tasks (e.g. Caltech 101), contrast normalization was really useful, that it became much less useful for larger architectures on larger tasks. In fact, if I recall correctly, the original AlexNet model had a type of cross-unit normalization in it, but this was dispensed with in more recent models (I think after Zeiler and Fergus 2013) largely because it didn*t contribute that much to performance but was somewhat expensive computationally. Of course, batch normalization methods have definitely been shown to contribute performance on large problems with large models, but I think it would be really important to show the same with the DN methods here, before any definite conclusion could be reached.","['4', '4', '5']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[3, 8, 17, 14, 29]","[8, 14, 22, 20, 35]","[65, 100, 375, 35, 243]","[29, 52, 223, 19, 136]","[36, 44, 137, 9, 83]","[0, 4, 15, 7, 24]","The sentiment score is slightly positive (20) because the reviewer acknowledges the paper's usefulness and clear writing, but also expresses significant concerns about the novelty and generalizability of the results. The pros are balanced against the cons, with a slight lean towards the positive. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, acknowledges the paper's strengths, and frames criticisms constructively. The reviewer uses phrases like 'I'm glad they tried' and 'I'm nervous that' which maintain a polite tone while expressing concerns. The overall language is professional and considerate, avoiding harsh criticism while still providing honest feedback.",20,60
"Offline bilingual word vectors, orthogonal transformations and the inverted softmax",Accept,2017,"['Samuel L. Smith', 'David H. P. Turban', 'Steven Hamblin', 'Nils Y. Hammerla']","[7, 8, 6]","['Good paper, accept', 'Top 50% of accepted papers, clear accept', 'Marginally above acceptance threshold']","This paper discusses aligning word vectors across language when those embeddings have been learned independently in monolingual settings. There are reasonable scenarios in which such a strategy could come in helpful, so I feel this paper addresses an interesting problem. The paper is mostly well executed but somewhat lacks in evaluation. It would have been nice if a stronger downstream task had been attempted. The inverted Softmax idea is very nice. A few minor issues that ought to be addressed in a published version of this paper: 1) There is no mention of Haghighi et al (2008) *Learning Bilingual Lexicons from Monolingual Corpora.*, which strikes me as a key piece of prior work regarding the use of CCA in learning bilingual alignment. This paper and links to the work here ought to be discussed. 2) Likewise, Hermann & Blunsom (2013) *Multilingual distributed representations without word alignment.* is probably the correct paper to cite for learning multilingual word embeddings from multilingual aligned data. 3) It would have been nicer if experiments had been performed with more divergent language pairs rather than just European/Romance languages 4) A lot of the argumentation around the orthogonality requirements feels related to the idea of using a Mahalanobis distance / covar matrix to learn such mappings. This might be worth including in the discussion 5) I don*t have a better suggestion, but is there an alternative to using the term *translation (performance/etc.)* when discussing word alignment across languages? Translation implies something more complex than this in my mind. 6) The Mikolov citation in the abstract is messed up","['5', '5', '3']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is fairly confident that the evaluation is correct']","[2, 1, 3, 7]","[8, 1, 8, 12]","[30, 2, 5, 57]","[10, 1, 2, 34]","[20, 1, 3, 19]","[0, 0, 0, 4]","The sentiment score is 50 (slightly positive) because the reviewer acknowledges the paper addresses an interesting problem and is 'mostly well executed,' but also points out some limitations like lacking in evaluation. The inverted Softmax idea is praised as 'very nice.' The politeness score is 70 (fairly polite) because the reviewer uses respectful language throughout, offering constructive criticism and suggestions for improvement. Phrases like 'It would have been nice if...' and 'This might be worth including...' indicate a polite tone. The reviewer also acknowledges the paper's strengths before pointing out areas for improvement, which is a polite approach to feedback.",50,70
On Detecting Adversarial Perturbations,Accept,2017,"['Jan Hendrik Metzen', 'Tim Genewein', 'Volker Fischer', 'Bastian Bischoff']","[7, 7, 5]","['Good paper, accept', 'Good paper, accept', '5']","This paper proposes a new idea to help defending adversarial examples by training a complementary classifier to detect them. The results of the paper show that adversarial examples in fact can be easily detected. Moreover, such detector generalizes well to other similar or weaker adversarial examples. The idea of this paper is simple but non-trivial. While no final scheme is proposed in the paper how this idea can help in building defensive systems, it actually provides a potential new direction. Based on its novelty, I suggest an acceptance. My main concern of this paper is about its completeness. No effective method is reported in the paper to defend the dynamic adversaries. It could be difficult to do so, but rather the paper doesn’t seem to put much effort to investigate this part. How difficult it is to defend the dynamic adversaries is an important and interesting question following the conclusions of this paper. Such investigation may essentially help improve our understanding of adversarial examples. That being said, the novelty of this paper is still significant. Minor comment: The paper needs to improve its clarity. Some important details are skipped in the paper. For example, the paper should provide more details about the dynamic adversaries and the dynamic adversary training method.","['4', '4', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[13, 6, 3, 7]","[19, 12, 8, 13]","[70, 31, 33, 16]","[33, 9, 18, 10]","[25, 18, 15, 2]","[12, 4, 0, 4]","The sentiment score is 60 (positive) because the reviewer suggests acceptance of the paper, praising its novelty and potential. They describe the idea as 'simple but non-trivial' and mention that it 'provides a potential new direction'. However, it's not extremely positive due to concerns about completeness and clarity. The politeness score is 70 (polite) because the reviewer uses respectful language throughout, offering constructive criticism. They frame their concerns as suggestions for improvement rather than harsh criticisms. Phrases like 'My main concern' and 'Minor comment' soften the critique. The reviewer also acknowledges the difficulty of addressing certain issues, showing empathy towards the authors.",60,70
On the Quantitative Analysis of Decoder-Based Generative Models,Accept,2017,"['Yuhuai Wu', 'Yuri Burda', 'Ruslan Salakhutdinov', 'Roger Grosse']","[7, 7, 6]","['Good paper, accept', 'Good paper, accept', 'Marginally above acceptance threshold']","# Review This paper proposes a quantitative evaluation for decoder-based generative models that use Annealed Importance Sampling (AIS) to estimate log-likelihoods. Quantitative evaluations are indeed much needed since for some models, like Generative Adversarial Networks (GANs) and Generative Moment Matching Networks (GMMNs), qualitative evaluation of samples is still frequently used to assess their generative capability. Even though, there exist quantitative evaluations like Kernel Density Estimation (KDE), the authors show how AIS is more accurate than KDE and how it can be used to perform fine-grained comparison between generative models (GAN, GMMs and Variational Autoencoders (VAE)). The authors report empirical results comparing two different decoder architectures that were both trained, on the continuous MNIST dataset, using the VAE, GAN and GMMN objectives. They also trained an Importance Weighted Autoencoder (IWAE) on binarized MNIST and show that, in this case, the IWAE bound underestimates the true log-likelihoods by at least 1 nat (which is significant for this dataset) according to the AIS evaluation of the same model. # Pros Their evaluation framework is public and is definitely a nice contribution to the community. This paper gives some insights about how GAN behaves from log-likelihood perspective. The authors disconfirm the commonly proposed hypothesis that GAN are memorizing training data. The authors also observed that GANs miss important modes of the data distribution. # Cons/Questions It is not clear for me why sometimes the experiments were done using different number of examples (100, 1000, 10000) coming from different sources (trainset, validset, testset or simulation/generated by the model). For instance, in Table 2 why results were not reported using all 10,000 examples of the testing set? It is not clear why in Figure 2c, AIS is slower than AIS+encoder? Is the number of intermediate distributions the same in both? 16 independent chains for AIS seems a bit low from what I saw in the literature (e.g. in [Salakhutdinov & Murray, 2008] or [Desjardins etal., 2011], they used 100 chains). Could it be that increasing the number of chains helps tighten the confidence interval reported in Table 2? I would have like the authors to give their intuitions as to why GAN50 has a BDMC gap of 10 nats, i.e. 1 order of magnitude compared to the others? # Minor comments Table 1 is not referenced in the text and lacks description of what the different columns represent. Figure 2(a), are the reported values represents the average log-likelihood of 100 (each or total?) training and validation examples of MNIST (as described in Section 5.3.2). Figure 2(c), I*m guessing it is on binarized MNIST? Also, why are there fewer points for AIS compared to IWAE and AIS+encoder? Are the BDMC gaps mentioned in Section 5.3.1 the same as the ones reported in Table2 ? Typo in caption of Figure 3: *(c) GMMN-10* but actually showing GMMN-50 according to the graph title and subcaption.","['4', '4', '5']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[2, 4, 15, 11]","[8, 10, 21, 17]","[70, 17, 419, 123]","[28, 8, 207, 60]","[39, 9, 201, 61]","[3, 0, 11, 2]","The sentiment score is 60 (moderately positive) because the reviewer begins by highlighting the importance and novelty of the paper's contribution, noting it addresses a need in the field. They list several pros, including the public availability of the evaluation framework and new insights provided. The cons are presented as questions rather than criticisms, suggesting a constructive approach. The politeness score is 70 (quite polite) because the reviewer uses respectful language throughout, frames criticisms as questions or suggestions for improvement, and acknowledges the value of the work. They use phrases like 'It is not clear for me' instead of more direct criticism. The review maintains a professional and courteous tone, even when pointing out potential issues or areas for improvement.",60,70
Online Bayesian Transfer Learning for Sequential Data Modeling,Accept,2017,"['Priyank Jaini', 'Zhitang Chen', 'Pablo Carbajal', 'Edith Law', 'Laura Middleton', 'Kayla Regan', 'Mike Schaekermann', 'George Trimponias', 'James Tung', 'Pascal Poupart']","[6, 7, 6]","['Marginally above acceptance threshold', 'Good paper, accept', 'Marginally above acceptance threshold']","This paper proposes an online inference algorithm by using online Bayesian moment matching for HMM-GMM. The method uses transfer learning by utilizing individual sequence estimators to predict a target sequence based on a weighted combination of individual HMM-GMM. Online Bayesian moment matching has a benefit of updating HMM-GMM parameters frame-by-frame, and fits to this problem. The authors compare the proposed method with the other sequential modeling methods including RNN and EM, and show the effectiveness of the proposed method. The paper is well written overall. Comments: 1) Could you provide the average performance in table? It is difficult to compare the performance only with individual performance. Also, it seems that the EM performance is sometimes good 2) I’m curious how initialization and hyper-parameter settings affect the final performance. If you provide some information about it, that is great. 3) It would be better to provide a figure of describing the transfer-learning-based proposed methods, since this is a unique and a little bit complicated setup.","['3', '3', '3']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[2, 7, 1, 18, 1, 1, 1, 5, 8, 18]","[8, 13, 1, 24, 6, 1, 7, 11, 14, 24]","[27, 75, 1, 75, 5, 1, 22, 23, 20, 195]","[12, 34, 1, 43, 5, 1, 10, 11, 13, 124]","[13, 29, 0, 18, 0, 0, 6, 6, 3, 57]","[2, 12, 0, 14, 0, 0, 6, 6, 4, 14]","The sentiment score is 70 (positive) because the reviewer starts with a detailed summary of the paper's content and methodology, indicating they have thoroughly read and understood the work. They state that 'The paper is well written overall,' which is a clear positive sentiment. The reviewer also mentions the 'effectiveness of the proposed method,' further indicating a positive view. However, it's not a perfect 100 as they do provide some suggestions for improvement.

The politeness score is 80 (polite) because the reviewer uses respectful and constructive language throughout. They frame their suggestions as questions or polite requests (e.g., 'Could you provide...', 'I'm curious...', 'It would be better...'). The tone is collaborative rather than critical, and there's no harsh or dismissive language. The reviewer also acknowledges the paper's strengths before offering suggestions, which is a polite approach to feedback.",70,80
Optimal Binary Autoencoding with Pairwise Correlations,Accept,2017,['Akshay Balsubramani'],"[7, 6]","['Good paper, accept', 'Marginally above acceptance threshold']","The paper presents a novel look at binary auto-encoders, formulating the objective function as a min-max reconstruction error over a training set given the observed intermediate representations. The author shows that this formulation leads to a bi-convex problem that can be solved by alternating minimisation methods; this part is non-trivial and is the main contribution of the paper. Proof-of-concept experiments are performed, showing improvements for 1-hidden layer auto-encoders with respect to a vanilla approach. The experimental section is fairly weak because the literature on auto-encoders is huge and many variants were shown to perform better than straightforward approaches without being more complicated (e.g., denoising auto-encoders). Yet, the paper presents an analysis that leads to a new learning algorithm for an old problem, and is likely worth discussing.","['3', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']",[5],[10],[30],[11],[18],[1],"The sentiment score is 50 (slightly positive) because the reviewer acknowledges the paper's novel approach and main contribution, stating it's 'likely worth discussing'. However, they also point out weaknesses in the experimental section, balancing the positive aspects. The politeness score is 75 (quite polite) as the reviewer uses respectful language throughout, acknowledging the paper's strengths while constructively pointing out areas for improvement. They avoid harsh criticism and use phrases like 'fairly weak' instead of more negative terms. The reviewer maintains a professional and objective tone throughout the review.",50,75
Paleo: A Performance Model for Deep Neural Networks,Accept,2017,"['Hang Qi', 'Evan R. Sparks', 'Ameet Talwalkar']","[7, 6, 6]","['Good paper, accept', 'Marginally above acceptance threshold', 'Marginally above acceptance threshold']","This paper introduces an analytical performance model to estimate the training and evaluation time of a given network for different software, hardware and communication strategies. The paper is very clear. The authors included many freedoms in the variables while calculating the run-time of a network such as the number of workers, bandwidth, platform, and parallelization strategy. Their results are consistent with the reported results from literature. Furthermore, their code is open-source and the live demo is looking good. The authors mentioned in their comment that they will allow users to upload customized networks and model splits in the coming releases of the interface, then the tool can become very useful. It would be interesting to see some newer network architectures with skip connections such as ResNet, and DenseNet.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[9, 10, 10]","[15, 13, 16]","[44, 21, 126]","[19, 9, 54]","[11, 10, 61]","[14, 2, 11]","The sentiment score is 80 (positive) because the reviewer expresses a very favorable opinion of the paper. They describe it as 'very clear' and praise the authors for including many variables in their calculations. The reviewer also commends the consistency of results, the open-source code, and the quality of the live demo. The only slight criticism is a suggestion to include newer network architectures, which is presented as an opportunity for improvement rather than a flaw. The politeness score is 70 (polite) because the reviewer uses respectful and professional language throughout. They offer praise without being overly effusive and make suggestions in a constructive manner. The tone is consistently positive and encouraging, with phrases like 'very clear,' 'consistent with reported results,' and 'can become very useful' indicating a polite and supportive approach to the review.",80,70
Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer,Accept,2017,"['Sergey Zagoruyko', 'Nikos Komodakis']","[6, 6, 6]","['Marginally above acceptance threshold', 'Marginally above acceptance threshold', 'Marginally above acceptance threshold']","The paper proposes a new way of transferring knowledge. I like the idea of transferring attention maps instead of activations. However, the experiments don’t show a big improvement compared with knowledge distillation alone and I think more experiments are required in IMAGENET section. I would consider updating the score if the authors extend the last section 4.2.2.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[3, 14]","[9, 19]","[29, 130]","[9, 71]","[15, 28]","[5, 31]","The sentiment score is slightly negative (-20) because while the reviewer likes the initial idea, they express concerns about the experimental results and request more experiments. This suggests the paper needs significant improvements. The politeness score is moderately positive (50) as the reviewer uses polite language, acknowledging positive aspects ('I like the idea') and offering constructive criticism without harsh words. They also suggest a path for improvement ('I would consider updating the score if...'). The tone is professional and respectful throughout, even when expressing concerns.",-20,50
PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications,Accept,2017,"['Tim Salimans', 'Andrej Karpathy', 'Xi Chen', 'Diederik P. Kingma']","[9, 6, 7, 7]","['9', 'Marginally above acceptance threshold', 'Good paper, accept', 'Good paper, accept']","It is refreshing that OpenAI has taken the time to resurrect classic heuristics like down-sampling and dropout into PixelCNN. Some sort of AR technique like PixelCNN probably holds the missing keys needed to eventually have decent originally-created images from CIFAR10 or other real-life data-sets. So any engineering streamlining, as in this paper, is welcome to the general public, especially when helping to avoid expensive clusters of GPUs, only DeepMind can afford. In this sense, OpenAI is fulfilling its mission and we are all very grateful! Thus the paper is a welcome addition and we hope it finds its way into what appears to be an experimental CS conference anyway. On a more conceptual level, our hope is that OpenAI, with so talented a team, will stop competing in these contrived contests to improve by basis points certain obscure log-likelihoods and instead focus on the bigger picture problems. Why for example, almost two years later, the class-conditional CIFAR10 samples, as on the left of Figure 4 in this paper (column 8 - class of horses), are still inferior to, say, the samples on Figure 4 of reference [2]? Forgive the pun, but aren*t we beating a dead horse here? Yes, resolution and sharpness have improved, due to good engineering but nobody in the general public will take these samples seriously! Despite the claims put forward by some on the DeepMind team, PixelCNN is not a *fully-generative* neural net (as rigorously defined in section 3 of reference [1]), but merely a perturbative net, in the vain of the Boltzmann machine. After the Procrustean experience of lost decades on Boltzmann machines, the time perhaps has come to think more about the fundamentals and less about the heuristics? [1] https://arxiv.org/pdf/1508.06585v5.pdf [2] https://arxiv.org/pdf/1511.02841v3.pdf","['4', '3', '4', '5']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[6, 7, 3, 8]","[12, 7, 8, 14]","[60, 19, 47, 46]","[21, 9, 21, 21]","[36, 6, 24, 24]","[3, 4, 2, 1]","The sentiment score is slightly positive (20) because the reviewer expresses appreciation for OpenAI's work and considers the paper a 'welcome addition'. However, this positivity is tempered by criticisms and suggestions for improvement. The politeness score is moderately positive (50) as the reviewer uses respectful language and expresses gratitude, but also includes direct criticisms. The reviewer begins with positive comments, acknowledging the value of the work and expressing gratitude. They use phrases like 'refreshing', 'welcome addition', and 'we are all very grateful'. However, the tone shifts to more critical in the latter half, questioning the focus on 'obscure log-likelihoods' and suggesting a need for more fundamental research. The language remains professional throughout, avoiding rudeness while still conveying critiques, hence the moderate politeness score.",20,50
PixelVAE: A Latent Variable Model for Natural Images,Accept,2017,"['Ishaan Gulrajani', 'Kundan Kumar', 'Faruk Ahmed', 'Adrien Ali Taiga', 'Francesco Visin', 'David Vazquez', 'Aaron Courville']","[7, 6, 7]","['Good paper, accept', 'Marginally above acceptance threshold', 'Good paper, accept']","UPDATE: The authors addressed all my concerns in the new version of the paper, so I raised my score and now recommend acceptance. -------------- This paper combines the recent progress in variational autoencoder and autoregressive density modeling in the proposed PixelVAE model. The paper shows that it can match the NLL performance of a PixelCNN with a PixelVAE that has a much shallower PixelCNN decoder. I think the idea of capturing the global structure with a VAE and modeling the local structure with a PixelCNN decoder makes a lot of sense and can prevent the blurry reconstruction/samples of VAE. I specially like the hierarchical image generation experiments. I have the following suggestions/concerns about the paper: 1) Is there any experiment showing that using the PixelCNN as the decoder of VAE will result in better disentangling of high-level factors of variations in the hidden code? For example, the authors can train a PixelVAE and VAE on MNIST with 2D hidden code and visualize the 2D hidden code for test images and color code each hidden code based on the digit and show that the digits have a better separation in the PixelVAE representation. A semi-supervised classification comparison between VAE and the PixelVAE will also significantly improve the quality of the paper. 2) A similar idea is also presented in a concurrent ICLR submission *Variational Lossy Autoencoder*. It would be interesting to have a discussion included in the paper and compare these works. 3) The answer to the pre-review questions made the architecture details of the paper much more clear, but I still ask the authors to include the exact architecture details of all the experiments in the paper and/or open source the code. The clarity of the presentation is not satisfying and the experiments are difficult to reproduce. 4) As pointed out in my pre-review question, it would be great to include two sets of MNIST samples maybe in an appendix section. One with PixelCNN and the other with PixelVAE with the same pixelcnn depth to illustrate the hidden code in PixelVAE actually captures the global structure. I will gladly raise the score if the authors address my concerns.","['4', '3', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[3, 12, 10, 2, 3, 8, 17]","[9, 18, 14, 6, 7, 14, 23]","[20, 57, 26, 12, 20, 116, 309]","[9, 29, 18, 5, 8, 47, 135]","[11, 17, 6, 7, 12, 52, 160]","[0, 11, 2, 0, 0, 17, 14]","The sentiment score is 50 (moderately positive) because the reviewer starts by acknowledging that their previous concerns have been addressed, leading to a recommendation for acceptance. They also express appreciation for certain aspects of the paper, using phrases like 'makes a lot of sense' and 'I specially like'. However, they still provide several suggestions for improvement, which prevents the score from being higher. The politeness score is 75 (quite polite) due to the reviewer's constructive tone throughout. They use phrases like 'I think', 'I have the following suggestions', and 'It would be interesting', which convey respect for the authors. The reviewer also expresses willingness to raise their score if concerns are addressed, showing a collaborative attitude. The language is professional and courteous throughout, without any rudeness or harsh criticism.",50,75
Pointer Sentinel Mixture Models,Accept,2017,"['Stephen Merity', 'Caiming Xiong', 'James Bradbury', 'Richard Socher']","[7, 8, 8]","['Good paper, accept', 'Top 50% of accepted papers, clear accept', 'Top 50% of accepted papers, clear accept']","This paper proposes augmenting RNN-based language models with a pointer network in order to deal better with rare words. The pointer network can point to words in the recent context, and hence the prediction for each time step is a mixture between the usual softmax output and the pointer distribution over the recent words. The paper also introduces a new language modelling dataset, which overcomes some of the shortcomings of previous datasets. The reason for the score I gave for this paper is that I find the proposed model a direct application of the previous work Gulcehre et al., which follows a similar approach but for machine translation and summarization. The main differences I find is that Gulcehre et al. use an encoder-decoder architecture, and use the attention weights of the encoder to point to locations of words in the input, while here an RNN is used and a pointer network produces a distribution over the full vocabulary (by summing the softmax probabilities of words in the recent context). The context (query) vector for the pointing network is also different, but this is also a direct consequence of having a different application. While the paper describes the differences between the proposed approach and Gulcehre et al.’s approach, I find some of the claims either wrong or not that significant. For example, quoting from Section 1: “Rather than relying on the RNN hidden state to decide when to use the pointer, as in the recent work of Gulcehre et al. (2016), we allow the pointer component itself to decide when to use the softmax vocabulary through a sentinel.” As far as I can tell, your model also uses the recent hidden state to form a query vector, which is matched by the pointer network to previous words. Can you please clarify what you mean here? In addition, quoting from section 3 which describes the model of Gulcehre et al.: “Rather than constructing a mixture model as in our work, they use a switching network to decide which component to use” This is not correct. The model of Gulcehre is also a mixture model, where an MLP with sigmoid output (switching network) is used to form a mixture between softmax prediction and locations of the input text. Finally, in the following quote, also from section 3: “The pointer network is not used as a source of information for the switching network as in our model.” It is not clear what the authors mean by “source of information” here. Is it the fact that the switching probability is part of the pointer softmax? I am wondering how significant this difference is. With regards to the proposed dataset, there are also other datasets typically used for language modelling, including The Hutter Prize Wikipedia (enwik8) dataset (Hutter, 2012) and e Text8 dataset (Mahoney, 2009). Can you please comment on the differences between your dataset and those as well? I would be happy to discuss with the authors the points I raised, and I am open to changing my vote if there is any misunderstanding on my part.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[7, 9, 3, 11]","[9, 15, 6, 17]","[14, 385, 18, 229]","[6, 165, 9, 111]","[8, 210, 9, 111]","[0, 10, 0, 7]","The sentiment score is -30 because the reviewer expresses several criticisms and doubts about the paper's novelty and claims. They point out similarities to previous work and question some of the authors' statements. However, it's not entirely negative as the reviewer shows interest in the new dataset and is open to discussion. The politeness score is 50 because the reviewer uses respectful language throughout, asks for clarifications politely, and expresses willingness to change their opinion. They use phrases like 'Can you please clarify' and 'I would be happy to discuss', which are quite courteous. The reviewer also acknowledges the possibility of misunderstanding on their part, which shows humility and respect for the authors.",-30,50
Predicting Medications from Diagnostic Codes with Recurrent Neural Networks,Accept,2017,"['Jacek M. Bajor', 'Thomas A. Lasko']","[7, 8, 6]","['Good paper, accept', 'Top 50% of accepted papers, clear accept', 'Marginally above acceptance threshold']","This is a well written, organized, and presented paper that I enjoyed reading. I commend the authors on their attention to the narrative and the explanations. While it did not present any new methodology or architecture, it instead addressed an important application of predicting the medications a patient is using, given the record of billing codes. The dataset they use is impressive and useful and, frankly, more interesting than the typical toy datasets in machine learning. That said, the investigation of those results was not as deep as I thought it should have been in an empirical/applications paper. Despite their focus on the application, I was encouraged to see the authors use cutting edge choices (eg Keras, adadelta, etc) in their architecture. A few points of criticism: -The numerical results are in my view too brief. Fig 4 is anecdotal, Fig 5 is essentially a negative result (tSNE is only in some places interpretable), so that leaves Table 1. I recognize there is only one dataset, but this does not offer a vast amount of empirical evidence and analysis that one might expect out of a paper with no major algorithmic/theoretical advances. To be clear I don*t think this is disqualifying or deeply concerning; I simply found it a bit underwhelming. - To be constructive, re the results I would recommend removing Fig 5 and replacing that with some more meaningful analysis of performance. I found Fig 5 to be mostly uninformative, other than as a negative result, which I think can be stated in a sentence rather than in a large figure. - There is a bit of jargon used and expertise required that may not be familiar to the typical ICLR reader. I saw that another reviewer suggested perhaps ICLR is not the right venue for this work. While I certainly see the reviewer*s point that a medical or healthcare venue may be more suitable, I do want to cast my vote of keeping this paper here... our community benefits from more thoughtful and in depth applications. Instead I think this can be addressed by tightening up those points of jargon and making the results more easy to evaluate by an ICLR reader (that is, as it stands now researchers without medical experience have to take your results after Table 1 on faith, rather than getting to apply their well-trained quantitative eye). Overall, a nice paper.","['5', '4', '3']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[1, 17]","[1, 23]","[1, 61]","[1, 26]","[0, 14]","[0, 21]","The sentiment score is 70 (positive) because the reviewer starts with strong praise, describing the paper as 'well written, organized, and presented' and stating they 'enjoyed reading' it. They commend the authors multiple times and express encouragement about their methods. While there are criticisms, they are presented as minor points and the overall tone remains positive. The politeness score is 80 (very polite) due to the reviewer's consistent use of respectful language. They begin with compliments, frame criticisms constructively, and use phrases like 'I commend the authors' and 'Overall, a nice paper.' Even when expressing concerns, the reviewer maintains a polite tone, using phrases like 'To be constructive' and 'I simply found it a bit underwhelming' rather than harsh criticism. The reviewer also acknowledges the paper's strengths while suggesting improvements, demonstrating a balanced and courteous approach.",70,80
Program Synthesis for Character Level Language Modeling,Accept,2017,"['Pavol Bielik', 'Veselin Raychev', 'Martin Vechev']","[5, 8]","['5', 'Top 50% of accepted papers, clear accept']","This paper proposes an approach to character language modeling (CLMs) based on developing a domain specific language to represent CLMs. The experiments show mixed performance versus neural CLM approaches to modeling linux kernel data and wikipedia text, however the proposed DSL models are slightly more compact and fast to query as compared with neural CLMs. The proposed approach is difficult to understand overall and perhaps is aimed towards the sub-community already working on this sort of approach but lacks sufficient explanation for the ICLR audience. Critically the paper glosses over the major issues of demonstrating the proposed DSL is a valid probabilistic model and how training is performed to fit the model to data (there is clearly not a gradient-based training approach used). FInally the experiments feel incomplete without showing samples drawn from the generative model or analyzing the learned model to determine what it has learned. Overall I feel this paper does not describe the approach in enough depth for readers to understand or re-implement it. Almost all of the model section is devoted to exposition of the DSL without specifying how probabilities are computed using this model and how training is performed. How are probabilities actually encoded? The DSL description seems to have only discrete decisions rather than probabilities. Training is perhaps covered in previous papers but there needs to be some discussion of how it works here. Section 2.5 does not do enough to explain how training works or how any measure of optimality is achieved. Given this model is quite a different hypothesis space from neural models or n-grams, looking and samples drawn from the model seems critical. The current experiments show it can score utterances relatively well but it would be very interesting if the model can sample more structured samples than neural approaches (for example long-range syntax constraints like brackets)","['3', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[6, 9, 15]","[10, 13, 21]","[26, 28, 204]","[18, 25, 137]","[6, 2, 51]","[2, 1, 16]","The sentiment score is -50 because the reviewer expresses several significant criticisms of the paper, including that it's difficult to understand, lacks sufficient explanation, glosses over major issues, and has incomplete experiments. However, they do note some positive aspects like the model being compact and fast to query, preventing the score from being more negative. The politeness score is 20 because the reviewer uses relatively neutral language and frames criticisms as observations or suggestions rather than harsh judgments. They use phrases like 'difficult to understand' and 'feels incomplete' rather than more confrontational language. The reviewer also acknowledges some positive aspects of the work, which contributes to a more polite tone overall.",-50,20
Pruning Convolutional Neural Networks for Resource Efficient Inference,Accept,2017,"['Pavlo Molchanov', 'Stephen Tyree', 'Tero Karras', 'Timo Aila', 'Jan Kautz']","[7, 9, 6]","['Good paper, accept', '9', 'Marginally above acceptance threshold']","Authors propose a strategy for pruning weights with the eventual goal of reducing GFLOP computations. The pruning strategy is well motivated using the taylor expansion of the neural network function with respect to the feature activations. The obtained strategy removes feature maps that have both a small activation and a small gradient (eqn 7). (A) Ideally the gradient of the output with respect to the activation functions should be 0 at the optimal, but as a result of stochastic gradient evaluations this would practically never be zero. Small variance in the gradient across mini-batches indicates that irrespective of input data the specific network parameter is unlikely to change - intuitively these are parameters that are closer to convergence. Parameters/weights that are close to convergence and also result in a small activation are intuitively good candidates for pruning. This is essentially what eqn 7 conveys and is likely to be reason why just removing weights that result in small activations is not as good of a pruning strategy (as shown by results in the paper). There are two kind of differences in weights that are removed by activation v/s taylor expansion: 1. Weights with high-activations but very low gradients will be removed by taylor expansion, but not by activation alone. 2. Weights with low-activation but high gradients will be removed by activation criterion, but not by taylor expansion. It will be interesting to analyze which of (1) or (2) contribute more to the differences in weights that are removed by the taylor expansion v/s activation criterion. Intuitively it seems that weight that satisfy (1) are important because they are converged and contribute significantly to network*s activation. It is possible that a modified criterion - eqn (7) + lambda feature activation, (where lambda needs to be found by cross-validation) may lead to even better results at the cost of more parameter tuning. (B) Another interesting comparison is with the with the optimal damage framework - where the first order gradients are assumed to be zero and pruning is performed using the second-order information (also discussed by authors in the appendix). Critically, only the diagonal of the Hessian is computed. There is no comparison with optimal damage as authors claim it is memory and computation inefficient. Back of envelope calculations suggest that this would result only in 50% increase in memory and computation during pruning, but no loss in efficiency during testing. Therefore from a standpoint of deployment, I don*t think this missing comparison is justified. (C) The eventual goal of the authors is to reduce GFLOPs. Some recent papers have proposed using lower precision computation for this. A comparison in GFLOPs with lower precision v/s pruning would be a great. While both these approaches are complementary and it is expected that combining both of them can lead to superior performance than either of the two - it is unclear when we are operating in the low-precision regime how much pruning can be performed. Any analysis on this tradeoff would be great (but not necessary). (D) On finetuning, authors report results of AlexNet and VGG on two different datasets - Flowers and Birds respectively. Why is this the case? It would be great to see the results of both the networks on both the datasets. (E) Authors report there is only a small drop in performance after pruning. Suppose the network was originally trained with N iterations, and then M finetuning iterations were performed during pruning. This means that pruned networks were trained for N + M iterations. The correct comparison in accuracies would be if we the original network was also trained for N + M iterations. In figure 4, does the performance at 100% parameters reports accuracy after N+M iterations or after N iterations? Overall I think the paper is technically and empirically sound, it proposes a new strategy for pruning: (1) Based on taylor expansion (2) Feature normalization to reduce parameter tuning efforts. (3) Iterative finetuning. However, I would like to see some comparisons mentioned in my comments above. If those comparisons are made I would change my ratings to an accept.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[3, 10, 8, 15, 20]","[9, 16, 14, 21, 26]","[68, 50, 55, 69, 397]","[33, 25, 26, 28, 194]","[34, 25, 20, 20, 141]","[1, 0, 9, 21, 62]","The sentiment score is 50 (slightly positive) because the reviewer acknowledges the paper's technical and empirical soundness, and proposes a new strategy for pruning. However, they also request additional comparisons and analyses, indicating room for improvement. The politeness score is 75 (quite polite) as the reviewer uses respectful language throughout, offers constructive criticism, and provides detailed explanations for their suggestions. They use phrases like 'it would be great to see' and 'I would like to see' rather than demanding changes. The reviewer also acknowledges the paper's strengths before suggesting improvements, which is a polite approach to feedback.",50,75
Pruning Filters for Efficient ConvNets,Accept,2017,"['Hao Li', 'Asim Kadav', 'Igor Durdanovic', 'Hanan Samet', 'Hans Peter Graf']","[7, 7, 6, 7]","['Good paper, accept', 'Good paper, accept', 'Marginally above acceptance threshold', 'Good paper, accept']","The idea of *pruning where it matters* is great. The authors do a very good job of thinking it through, and taking to the next level by studying pruning across different layers too. Extra points for clarity of the description and good pictures. Even more extra points for actually specifying what spaces are which layers are mapping into which (mathbb symbol - two thumbs up!). The experiments are well done and the results are encouraging. Of course, more experiments would be even nicer, but is it ever not the case? My question/issue - is the proposed pruning criterion proposed? Yes, pruning on the filter level is what in my opinion is the way to go, but I would be curious how the *min sum of weights* criterion compares to other approaches. How does it compare to other pruning criteria? Is it better than *pruning at random*? Overall, I liked the paper.","['4', '5', '5', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[3, 10, 28, 43, 38]","[7, 16, 28, 49, 44]","[11, 48, 13, 330, 69]","[7, 23, 10, 201, 46]","[4, 21, 1, 10, 10]","[0, 4, 2, 119, 13]","The sentiment score is 80 (positive) because the reviewer expresses strong approval of the paper's idea, execution, and clarity. They use phrases like 'great', 'very good job', and 'extra points', indicating a highly positive view. The only slight criticism is the desire for more experiments, which is softened by acknowledging this is a common request. The politeness score is 90 (very polite) due to the consistently respectful and encouraging tone. The reviewer uses phrases like 'good job', 'extra points', and 'I liked the paper', showing appreciation for the authors' work. They also frame their question/issue in a curious and non-confrontational manner, maintaining a polite discourse throughout the review.",80,90
Quasi-Recurrent Neural Networks,Accept,2017,"['James Bradbury', 'Stephen Merity', 'Caiming Xiong', 'Richard Socher']","[6, 7, 5, 7]","['Marginally above acceptance threshold', 'Good paper, accept', '5', 'Good paper, accept']","This paper introduces the Quasi-Recurrent Neural Network (QRNN) that dramatically limits the computational burden of the temporal transitions in sequence data. Briefly (and slightly inaccurately) model starts with the LSTM structure but removes all but the diagonal elements to the transition matrices. It also generalizes the connections from lower layers to upper layers to general convolutions in time (the standard LSTM can be though of as a convolution with a receptive field of 1 time-step). As discussed by the authors, the model is related to a number of other recent modifications of RNNs, in particular ByteNet and strongly-typed RNNs (T-RNN). In light of these existing models, the novelty of the QRNN is somewhat diminished, however in my opinion their is still sufficient novelty to justify publication. The authors present a reasonably solid set of empirical results that support the claims of the paper. It does indeed seem that this particular modification of the LSTM warrants attention from others. While I feel that the contribution is somewhat incremental, I recommend acceptance.","['4', '4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[-1, 22]","[4, 27]","[5, 77]","[5, 38]","[0, 1]","[0, 38]","The sentiment score is 60 (moderately positive) because the reviewer recommends acceptance and acknowledges the paper's contributions, despite noting some limitations. They use phrases like 'sufficient novelty to justify publication' and 'warrants attention from others', indicating a generally positive view. However, they also mention that the contribution is 'somewhat incremental', which prevents a higher score. The politeness score is 70 (fairly polite) because the reviewer uses respectful and professional language throughout. They acknowledge the authors' work and provide balanced feedback, using phrases like 'in my opinion' and 'reasonably solid set of empirical results'. The tone is constructive and avoids harsh criticism, maintaining a courteous approach even when discussing limitations.",60,70
Query-Reduction Networks for Question Answering,Accept,2017,"['Minjoon Seo', 'Sewon Min', 'Ali Farhadi', 'Hannaneh Hajishirzi']","[7, 7, 7]","['Good paper, accept', 'Good paper, accept', 'Good paper, accept']","The paper proposed a simple and effective model for QA. The paper is easy to read and result is impressive on the synthetic and real dataset. The one question is the paper is called query-reduction, but there is no place to show this reduction explicitly.","['3', '4', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[4, 1, 15, 11]","[10, 7, 21, 17]","[69, 51, 241, 238]","[26, 23, 117, 116]","[43, 28, 110, 119]","[0, 0, 14, 3]","The sentiment score is 70 (positive) because the reviewer praises the paper for being 'simple and effective', 'easy to read', and having 'impressive' results. The only criticism is a minor question about the paper's title, which slightly reduces the overall positive sentiment. The politeness score is 50 (somewhat polite) as the reviewer uses neutral language without any harsh criticism, and includes positive comments. However, it doesn't contain explicitly polite phrases, keeping it from scoring higher. The review is concise and straightforward, maintaining a professional tone throughout.",70,50
Reasoning with Memory Augmented Neural Networks for Language Comprehension,Accept,2017,"['Tsendsuren Munkhdalai', 'Hong Yu']","[6, 7]","['Marginally above acceptance threshold', 'Good paper, accept']","Thie paper proposed an iterative memory updating model for cloze-style question-answering task. The approach is interesting, and result is good. For the paper, I have some comments: 1. Actually the model in the paper is not single model, it proposed two models. One consists of *reading*, *writing*, *adaptive computation* and * Answer module 2*, the other one is *reading*, *composing*, *writing*, *gate querying* and *Answer module 1*. Based on the method section and the experiment, it seems the *adaptive computation* model is simpler and performs better. And without two time memory update in single iteration and composing module, the model is similar to neural turing machine. 2. What is the MLP setting in the composing module? 3. This paper tested different size of hidden state:[256, 368, 436, 512], I do not find any relation between those numbers, how could you find 436? Is there any tricks helping you find those numbers? 4. It needs more ablation study about using different T such as T=1,2.. 5. According to my understanding, for the adaptive computation, it would stop when the P_T <0. So what is the distribution of T in the testing data?","['3', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[7, 19]","[13, 25]","[52, 137]","[24, 69]","[17, 36]","[11, 32]","The sentiment score is 50 (slightly positive) because the reviewer starts by stating that the approach is interesting and the results are good, which indicates a generally positive view. However, they follow this with several comments and questions, suggesting there's room for improvement. The politeness score is 70 (fairly polite) because the reviewer uses respectful language throughout, framing their points as comments and questions rather than criticisms. They use phrases like 'I have some comments' and 'According to my understanding,' which maintain a courteous tone. The reviewer also acknowledges the paper's strengths before diving into their specific points, which is a polite approach to peer review.",50,70
Recurrent Batch Normalization,Accept,2017,"['Tim Cooijmans', 'Nicolas Ballas', 'César Laurent', 'Çağlar Gülçehre', 'Aaron Courville']","[7, 8, 7]","['Good paper, accept', 'Top 50% of accepted papers, clear accept', 'Good paper, accept']","The paper shows that BN, which does not work out of the box for RNNs, can be used with LSTM when the operator is applied to the hidden-to-hidden and the input-to-hidden contribution separately. Experiments are conducted to show that it leads to improved generalisation error and faster convergence. The paper is well written and the idea well presented. i) The data sets and consequently the statistical assumptions used are limited (e.g. no continuous data, only autoregressive generative modelling). ii) The hyper parameters are nearly constant over the experiments. It is ruled out that they have not been picked in favor of one of the methods. E.g. just judging from the text, a different learning rate could have lead to equally fast convergence for vanilla LSTM. Concluding, the experiments are flawed and do not sufficiently support the claim. An exhaustive search of the hyper parameter space could rule that out.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[20, 2, 15, 21]","[26, 7, 21, 27]","[973, 13, 231, 1196]","[440, 8, 140, 568]","[87, 1, 14, 44]","[446, 4, 77, 584]","The sentiment score is 20 (slightly positive) because the reviewer starts with positive comments about the paper being well-written and the idea well-presented. However, they then point out significant limitations in the experiments, which balances out the initial positivity. The politeness score is 50 (moderately polite) because the reviewer uses respectful language throughout, acknowledging the paper's strengths before presenting criticisms. They use phrases like 'The paper is well written' and present their concerns as observations rather than harsh criticisms. The reviewer maintains a professional tone, avoiding confrontational language while still clearly communicating the paper's shortcomings.",20,50
Recurrent Environment Simulators,Accept,2017,"['Silvia Chiappa', 'Sébastien Racaniere', 'Daan Wierstra', 'Shakir Mohamed']","[7, 5, 8]","['Good paper, accept', '5', 'Top 50% of accepted papers, clear accept']","[UPDATE] After going through the response from the author and the revision, I increased my review score for two reasons. 1. I thank the reviewers for further investigating the difference between yours and the other work (Scheduled sampling, Unsupervised learning using LSTM) and providing some insights about it. This paper at least shows empirically that 100%-Pred scheme is better for high-dimensional video and for long-term predictions. It would be good if the authors briefly discuss this in the final revision (either in the appendix or in the main text). 2. The revised paper contains more comprehensive results than before. The presented result and discussion in this paper will be quite useful to the research community as high-dimensional video prediction involves large-scale experiments that are computationally expensive. - Summary This paper presents a new RNN architecture for action-conditional future prediction. The proposed architecture combines actions into the recurrent connection of the LSTM core, which performs better than the previous state-of-the-art architecture [Oh et al.]. The paper also explores and compares different architectures such as frame-dependent/independent mode and observation/prediction-dependent architectures. The experimental result shows that the proposed architecture with fully prediction-dependent training scheme achieves the state-of-the-art performance on several complex visual domains. It is also shown that the proposed prediction architecture can be used to improve exploration in a 3D environment. - Novelty The novelty of the proposed architecture is not strong. The difference between [Oh et al.] and this work is that actions are combined into the LSTM in this paper, while actions are combined after LSTM in [Oh et al.]. The jumpy prediction was already introduced by [Srivastava et al.] in the deep learning area. - Experiment The experiments are well-designed and thorough. Specifically, the paper evaluates different training schemes and compares different architectures using several rich domains (Atari, 3D worlds). Besides, the proposed method achieves the state-of-the-art results on many domains and presents an application for model-based exploration. - Clarity The paper is well-written and easy to follow. - Overall Although the proposed architecture is not much novel, it achieves promising results on Atari games and 3D environments. In addition, the systematic evaluation of different architectures presented in the paper would be useful to the community. [Reference] Nitish Srivastava, Elman Mansimov, Ruslan Salakhutdinov. Unsupervised Learning with LSTMs. ICML 2016.","['5', '4', '4']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[14, 7, 14, 12]","[20, 13, 16, 17]","[38, 40, 70, 66]","[17, 11, 35, 27]","[18, 25, 27, 33]","[3, 4, 8, 6]","The sentiment score is 60 (positive) because the reviewer has increased their review score and acknowledges improvements in the revised paper. They appreciate the authors' further investigation and the more comprehensive results. However, it's not extremely positive as there are still some critiques about novelty. The politeness score is 80 (quite polite) because the reviewer uses respectful language throughout, thanks the authors for their efforts, and provides constructive feedback. They acknowledge the paper's strengths while also offering suggestions for improvement in a courteous manner. The reviewer's tone is professional and appreciative, especially when noting the usefulness of the work to the research community.",60,80
Recurrent Hidden Semi-Markov Model,Accept,2017,"['Hanjun Dai', 'Bo Dai', 'Yan-Ming Zhang', 'Shuang Li', 'Le Song']","[7, 7, 7]","['Good paper, accept', 'Good paper, accept', 'Good paper, accept']","This paper proposes a novel and interesting way to tackle the difficulties of performing inference atop HSMM. The idea of using an embedded bi-RNN to approximate the posterior is a reasonable and clever idea. That being said, I think two aspects may need further improvement: (1) An explanation as to why a bi-RNN can provide more accurate approximations than other modeling choices (e.g. structured mean field that uses a sequential model to formulate the variational distribution) is needed. I think it would make the paper stronger if the authors can explain in an intuitive way why this modeling choice is better than some other natural choices (in addition to empirical verification). (2) The real world datasets seem to be quite small (e.g. less than 100 sequences). Experimental results reported on larger datasets may also strengthen the paper.","['4', '3', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[15, 1, 2, 15, 13, 14]","[21, 7, 7, 21, 19, 20]","[564, 15, 31, 317, 105, 267]","[260, 5, 14, 147, 60, 107]","[66, 5, 12, 137, 23, 89]","[238, 5, 5, 33, 22, 71]","The sentiment score is 50 (slightly positive) because the reviewer starts by praising the paper as 'novel and interesting' and the idea as 'reasonable and clever'. However, they also point out two areas for improvement, balancing the positive with constructive criticism. The politeness score is 75 (quite polite) because the reviewer uses respectful language throughout, acknowledging the paper's strengths before offering suggestions. They use phrases like 'I think' and 'may need' to soften their critique, and frame their recommendations as ways to 'strengthen the paper' rather than pointing out flaws. The tone is professional and constructive throughout, without any harsh or rude language.",50,75
Recurrent Mixture Density Network for Spatiotemporal Visual Attention,Accept,2017,"['Loris Bazzani', 'Hugo Larochelle', 'Lorenzo Torresani']","[7, 6, 6]","['Good paper, accept', 'Marginally above acceptance threshold', 'Marginally above acceptance threshold']","The authors formulate a recurrent deep neural network to predict human fixation locations in videos as a mixture of Gaussians. They train the model using maximum likelihood with actual fixation data. Apart from evaluating how good the model performs at predicting fixations, they combine the saliency predictions with the C3D features for action recognition. quality: I am missing a more thorough evaluation of the fixation prediction performance. The center bias performance in Table 1 differs significantly from the on in Table 2. All the state-of-the-art models reported in Table 2 have a performance worse than the center bias performance reported in Table 1. Is there really no other model better than the center bias? Additionally I am missing details on how central bias and human performance are modelled. Is human performance cross-validated? You claim that your *results are very close to human performance (the difference is only 3.2%). This difference is actually larger than the difference between Central Bias and your model reported in Table 1. Apart from this, it is dangerous to compare AUC performance differences due to e.g. saturation issues. clarity: the explanation for Table 3 is a bit confusing, also it is not clear why the CONV5 and the FC6 models differ in how the saliency map is used. At least one should also evaluate the CONV5 model when multiplying the input with the saliency map to see how much of the difference comes from the different ways to use the saliency map and how much from the different features. Other issues: You cite Kümmerer et. al 2015 as a model which *learns ... indirectly rather than from explicit information of where humans look*, however the their model has been trained on fixation data using maximum-likelihood. Apart from these issues, I think the paper make a very interesting contribution to spatio-temporal fixation prediction. If the evaluation issues given above are sorted out, I will happily improve my rating.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[9, 13, 21]","[15, 19, 27]","[42, 162, 154]","[27, 64, 83]","[8, 74, 61]","[7, 24, 10]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the paper makes an 'interesting contribution', they express several concerns about the evaluation and clarity of the work. They state they are 'missing a more thorough evaluation' and point out inconsistencies in the results. However, they do offer to 'happily improve' their rating if issues are addressed, which prevents the score from being more negative. The politeness score is moderately positive (50) as the reviewer uses professional language throughout and offers constructive criticism. They use phrases like 'I am missing' rather than more accusatory language, and end on a positive note about the paper's potential. The reviewer maintains a respectful tone while clearly communicating their concerns.",-20,50
Regularizing CNNs with Locally Constrained Decorrelations,Accept,2017,"['Pau Rodríguez', 'Jordi Gonzàlez', 'Guillem Cucurull', 'Josep M. Gonfaus', 'Xavier Roca']","[7, 7, 7]","['Good paper, accept', 'Good paper, accept', 'Good paper, accept']","The author proposed a simple but yet effective technique in order to regularized neural networks. The results obtained are quite good and the technique shows to be effective when it it applied even on state of the art topologies, that is welcome because some regularization techniques used to be applied in easy task or on a initial configuration which results are still far from the best known results.","['3', '4', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[2, 18, 2, 8, 21]","[8, 24, 7, 13, 27]","[75, 138, 16, 16, 65]","[23, 71, 5, 5, 34]","[39, 11, 8, 4, 4]","[13, 56, 3, 7, 27]","The sentiment score is 80 (positive) because the reviewer describes the technique as 'simple but yet effective' and states that the results are 'quite good'. They also appreciate that the technique works on state-of-the-art topologies, which is 'welcome'. The overall tone is very positive towards the work. The politeness score is 50 (somewhat polite) because while the language is not overtly polite, it is professional and respectful. The reviewer uses neutral language to describe the work without any harsh criticism or overly effusive praise, maintaining a balanced and courteous tone throughout the paragraph.",80,50
Reinforcement Learning through Asynchronous Advantage Actor-Critic on a GPU,Accept,2017,"['Mohammad Babaeizadeh', 'Iuri Frosio', 'Stephen Tyree', 'Jason Clemons', 'Jan Kautz']","[5, 7, 8]","['5', 'Good paper, accept', 'Top 50% of accepted papers, clear accept']","The paper introduces GA3C, a GPU-based implementation of the A3C algorithm, which was originally designed for multi-core CPUs. The main innovation is the introduction of a system of queues. The queues are used for batching data for prediction and training in order to achieve high GPU occupancy. The system is compared to the authors* own implementation of A3C as well as to published reference scores. The paper introduces a very natural architecture for implementing A3C on GPUs. Batching requests for predictions and learning steps for multiple actors to maximize GPU occupancy seems like the right thing to do assuming that latency is not an issue. The automatic performance tuning strategy is also really nice to see. I appreciate the response showing that the throughput of GA3C is 20% higher than what is reported in the original A3C paper. What is still missing is a demonstration that the learning speed/data efficiency is in the right ballpark. Figure 3 of your paper is comparing scores under very different evaluation protocols. These numbers are just not comparable. The most convincing way to show that the learning speed is comparable would be time vs score plots or data vs score plots that show similar or improved speed to A3C. For example, this open source implementation seems to match the performance on Breakout: https://github.com/muupan/async-rl One or two plots like that would complete this paper very nicely. ----------------------------------- I appreciate the additional experiments included in the revised version of the paper. The learning speed comparison makes the paper more complete and I’m slightly revising my score to reflect that. Having said that, there is still no clear demonstration that the higher throughput of GA3C leads to consistently faster learning. With the exception of Pong, the training curves in Figure 6 seem to significantly underperform the original A3C results or even the open source implementation of A3C on Breakout and Space Invaders (https://github.com/muupan/async-rl).","['5', '5', '3']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is fairly confident that the evaluation is correct']","[6, 15, 10, 7, 20]","[11, 21, 16, 12, 26]","[25, 56, 50, 22, 397]","[11, 27, 25, 15, 194]","[14, 15, 25, 4, 141]","[0, 14, 0, 3, 62]","The sentiment score is 50 (slightly positive) because the reviewer acknowledges the paper's innovations and appreciates certain aspects, such as the automatic performance tuning strategy. However, they also point out missing elements and areas for improvement, indicating a balanced view. The politeness score is 80 (quite polite) due to the reviewer's use of respectful language throughout, such as 'I appreciate' and 'The paper introduces a very natural architecture.' They provide constructive criticism without harsh language, maintaining a professional and courteous tone even when suggesting improvements.",50,80
Revisiting Classifier Two-Sample Tests,Accept,2017,"['David Lopez-Paz', 'Maxime Oquab']","[7, 8, 7]","['Good paper, accept', 'Top 50% of accepted papers, clear accept', 'Good paper, accept']","I would like first to apologize for the delay. Summary: A framework for two-samples statistical test using binary classification is proposed. It allows multi-dimensional sample testing and an interpretability that other tests lack. A theoritical analysis is provided and various empirical tests reported. A very interesting approach. I have however two main concerns. The clarity of the presentation is obscured by too much content. It would be more interesting if the presentation could be somewhat self-contained. You could consider making 2 papers out of this paper. Seriously, you cram a lot of experiments in this paper. But the setting of the experiments is not really explained. We are supposed to have read Jitkrittum et al., 2016, Radford et al., 2016, Yu et al., 2015, etc. All this is okay but reduces your public to a very few. For example, if I am not mistaken, you never explained what SCF is, despite the fact that its performances are reported. As a second point, given also that the number of submissions to this conference are exploding, I would like to challenge you with the following question: Why is this work significant to the representation learning community?","['3', '5', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[7, 4]","[13, 10]","[60, 18]","[28, 8]","[27, 9]","[5, 1]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the approach as 'very interesting', they express 'two main concerns' about the paper's clarity and significance. The reviewer suggests splitting the paper into two and questions its relevance to the representation learning community, indicating some reservations about the work as presented. The politeness score is moderately positive (60) as the reviewer begins with an apology for the delay, uses polite language throughout (e.g., 'I would like to challenge you'), and frames criticisms as suggestions or questions rather than direct negative statements. The reviewer also acknowledges positive aspects of the work before presenting concerns, which contributes to the overall polite tone.",-20,60
SGDR: Stochastic Gradient Descent with Warm Restarts,Accept,2017,"['Ilya Loshchilov', 'Frank Hutter']","[7, 7, 7]","['Good paper, accept', 'Good paper, accept', 'Good paper, accept']","This heuristic to improve gradient descent in image classification is simple and effective, but this looks to me more like a workshop track paper. Demonstration of the algorithm is limited to one task (CIFAR) and there is no theory to support it, so we do not know how it will generalize on other tasks Working on DNNs for NLP, I find some observations in the paper opposite to my own experience. In particular, with architectures that combine a wide variety of layer types (embedding, RNN, CNN, gating), I found that ADAM-type techniques far outperform simple SGD with momentum, as they save searching for the right learning rate for each type of layer. But ADAM only works well combined with Poliak averaging, as it fluctuates a lot from one batch to another. Revision: - the authors substantially improved the contents of the paper, including experiments on another set than Cifar - the workshop track has been modified to breakthrough work, so my recommendation for it is not longer appropriate I have therefore improved my rating","['3', '4', '5']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[8, 16]","[10, 22]","[41, 252]","[23, 103]","[16, 120]","[2, 29]","The sentiment score is 50 (slightly positive) because the reviewer initially had reservations about the paper, suggesting it was more suitable for a workshop track. However, after revisions, the reviewer acknowledges substantial improvements and has upgraded their rating. The politeness score is 60 (moderately polite) because the reviewer uses professional language throughout, offers constructive criticism, and acknowledges the authors' efforts to improve the paper. The reviewer shares personal experiences and observations without being dismissive, and the tone remains respectful even when expressing initial doubts about the paper's suitability.",50,60
SampleRNN: An Unconditional End-to-End Neural Audio Generation Model,Accept,2017,"['Soroush Mehri', 'Kundan Kumar', 'Ishaan Gulrajani', 'Rithesh Kumar', 'Shubham Jain', 'Jose Sotelo', 'Aaron Courville', 'Yoshua Bengio']","[9, 8, 8]","['9', 'Top 50% of accepted papers, clear accept', 'Top 50% of accepted papers, clear accept']","The paper proposed a novel SampleRNN to directly model waveform signals and achieved better performance both in terms of objective test NLL and subjective A/B tests. As mentioned in the discussions, the current status of the paper lack plenty of details in describing their model. Hopefully, this will be addressed in the final version. The authors attempted to compare with wavenet model, but they didn*t manage to get a model better than the baseline LSTM-RNN, which makes all the comparisons to wavenets less convincing. Hence, instead of wasting time and space comparing to wavenet, detailing the proposed model would be better.","['4', '4', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[3, 12, 3, 2, 5, 2, 17, 30]","[7, 18, 9, 7, 11, 5, 23, 36]","[12, 57, 20, 10, 101, 11, 309, 977]","[7, 29, 9, 3, 63, 5, 135, 405]","[5, 17, 11, 7, 19, 6, 160, 456]","[0, 11, 0, 0, 19, 0, 14, 116]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the novel approach and better performance of SampleRNN, they express concerns about the lack of details in the model description and the unsuccessful comparison with wavenet. The reviewer suggests improvements, indicating that the paper has potential but needs work. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, acknowledging the paper's strengths and offering constructive criticism. They use phrases like 'Hopefully, this will be addressed' and suggest alternatives rather than harshly criticizing, maintaining a professional and courteous tone.",-20,50
Semi-Supervised Classification with Graph Convolutional Networks,Accept,2017,"['Thomas N. Kipf', 'Max Welling']","[7, 7, 7]","['Good paper, accept', 'Good paper, accept', 'Good paper, accept']","The paper develops a simple and reasonable algorithm for graph node prediction/classification. The formulations are very intuitive and lead to a simple CNN based training and can easily leverage existing GPU speedups. Experiments are thorough and compare with many reasonable baselines on large and real benchmark datasets. Although, I am not quite aware of the literature on other methods and there may be similar alternatives as link and node prediction is an old problem. I still think the approach is quite simple and reasonably supported by good evaluations.","['3', '4', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[2, 11, 24]","[7, 17, 30]","[11, 68, 236]","[3, 29, 104]","[6, 21, 63]","[2, 18, 69]","The sentiment score is 70 (positive) because the reviewer expresses a generally favorable view of the paper. They describe the algorithm as 'simple and reasonable', the formulations as 'very intuitive', and the experiments as 'thorough'. The reviewer also mentions that the approach is 'quite simple and reasonably supported by good evaluations'. However, it's not a perfect score due to some uncertainty expressed about the novelty of the method in the context of existing literature. The politeness score is 50 (slightly polite) because the language used is professional and respectful, without being overly formal or effusive. The reviewer offers balanced feedback, acknowledging both strengths and potential limitations of the work. They use phrases like 'I think' and 'I am not quite aware', which softens any potential criticism and shows a degree of humility. The tone is constructive and supportive overall, but doesn't go out of its way to be exceptionally polite.",70,50
Sigma Delta Quantized Networks,Accept,2017,"[""Peter O'Connor"", 'Max Welling']","[8, 6, 8]","['Top 50% of accepted papers, clear accept', 'Marginally above acceptance threshold', 'Top 50% of accepted papers, clear accept']","This paper presented a method of improving the efficiency of deep networks acting on a sequence of correlated inputs, by only performing the computations required to capture changes between adjacent inputs. The paper was clearly written, the approach is clever, and it*s neat to see a practical algorithm driven by what is essentially a spiking network. The benefits of this approach are still more theoretical than practical -- it seems unlikely to be worthwhile to do this on current hardware. I strongly suspect that if deep networks were trained with an appropriate sparse slowness penalty, the reduction in computation would be much larger.","['4', '3', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[21, 18]","[23, 24]","[32, 391]","[20, 190]","[5, 167]","[7, 34]","The sentiment score is 60 (positive) because the reviewer praises the paper as 'clearly written' and the approach as 'clever' and 'neat'. They acknowledge the theoretical benefits of the method. However, it's not extremely positive as they point out that the benefits are 'still more theoretical than practical' and suggest that other approaches might yield better results. The politeness score is 70 (polite) because the reviewer uses respectful language throughout, offering praise where due and constructive criticism without harsh words. They use phrases like 'it's neat to see' and 'I strongly suspect', which maintain a collegial tone. The review is objective and balanced, offering both positive feedback and areas for improvement in a professional manner.",60,70
"Snapshot Ensembles: Train 1, Get M for Free",Accept,2017,"['Gao Huang', 'Yixuan Li', 'Geoff Pleiss', 'Zhuang Liu', 'John E. Hopcroft', 'Kilian Q. Weinberger']","[9, 7, 8]","['9', 'Good paper, accept', 'Top 50% of accepted papers, clear accept']","This work develops a method to quickly produce an ensemble of deep networks that outperform a single network trained for an equivalent amount of time. The basis of this approach is to use a cyclic learning rate to quickly settle the model into a local minima and saving a model snapshot at this time before quickly raising the learning rate to escape towards a different minima*s well of attraction. The resulting snapshots can be collected throughout a single training run and achieve reasonable performance compared to baselines and have some of the gains of traditional ensembles (at a much lower cost). This paper is well written, has clear and informative figures/tables, and provides convincing results across a broad range of models and datasets. I especially liked the analysis in Section 4.4. The publicly available code to ensure reproducibility is also greatly appreciated. I would like to see more discussion of the accuracy and variability of each snapshot and further comparison with true ensembles. Preliminary rating: This is an interesting work with convincing experiments and clear writing. Minor note: Why is the axis for lambda from -1 to 2 in Figure 5 where lambda is naturally between 0 and 1.","['4', '5', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is fairly confident that the evaluation is correct']","[6, 3, 1, 2, 53, 14]","[12, 9, 7, 8, 59, 20]","[207, 55, 49, 40, 184, 199]","[71, 23, 22, 16, 78, 106]","[85, 29, 26, 20, 36, 82]","[51, 3, 1, 4, 70, 11]","The sentiment score is 80 (positive) because the reviewer expresses a very favorable view of the paper, describing it as 'well written', with 'clear and informative figures/tables', and 'convincing results'. They also mention appreciating specific aspects like the analysis in Section 4.4 and the publicly available code. The only slight criticism is a request for more discussion on certain points, which is a common and constructive element in positive reviews. The politeness score is 90 (very polite) due to the consistently respectful and appreciative tone. The reviewer uses phrases like 'I especially liked' and 'greatly appreciated', showing courtesy and respect for the authors' work. Even when suggesting improvements, the language is gentle and constructive, maintaining a polite tone throughout.",80,90
Soft Weight-Sharing for Neural Network Compression,Accept,2017,"['Karen Ullrich', 'Edward Meeds', 'Max Welling']","[7, 7, 7]","['Good paper, accept', 'Good paper, accept', 'Good paper, accept']","This paper proposes to use an empirical Bayesian approach to learn the parameters of a neural network, and their priors. A mixture model prior over the weights leads to a clustering effect in the weight posterior distributions (which are approximated with delta peaks). This clustering effect can exploited for parameter quantisation and compression of the network parameters. The authors show that this leads to compression rates and predictive accuracy comparable to related approaches. Earlier work [Han et al. 2015] is based on a three-stage process of pruning small magnitude weights, clustering the remaining ones, and updating the cluster centres to optimise performance. The current work provides a more principled approach that does not have such an ad-hoc multi-stage structure, but a single iterative optimisation process. A first experiment, described in section 6.1 shows that an empirical Bayes’ approach, without the use of hyper priors, already leads to a pronounced clustering effect and to setting many weights to zero. In particular a compression rate of 64.2 is obtained on the LeNet300-100 model. In section 6.1 the text refers to figure C, I suppose this should be figure 1. Section 6.2 describes an experiment where hyper-priors are used, and the parameters of these distributions, as well as other hyper-parameters such as the learning rates, are being optimised using Spearmint (Snoek et al., 2012). Figure 2 shows the performance of the different points in the hyper-parameter space that have been evaluated (each trained network gives an accuracy-compressionrate point in the graph). The text claims that best results lie on a line, this seems a little opportunistic interpretation given the limited data. Moreover, it would be useful to add a small discussion on whether such a linear relationship would be expected or not. Currently the results of this experiment lack interpretation. Section 6.3 describes results obtained for both CNN models and compares results to the recent results of (Han et al., 2015) and (Guo et al., 2016). Comparable results are obtained in terms of compression rate and accuracy. The authors state that their current algorithm is too slow to be useful for larger models such as VGG-19, but they do briefly report some results obtained for this model (but do not compare to related work). It would be useful here to explain what slows the training down with respect to standard training without the weight clustering approach, and how the proposed algorithm scales in terms of the relevant quantities of the data and the model. The contribution of this paper is mostly experimental, leveraging fairly standard ideas from empirical Bayesian learning to introduce weight clustering effects in CNN training. This being said, it is an interesting result that such a relatively straightforward approach leads to results that are on par with state-of-the-art, but more ad-hoc, network compression techniques. The paper could be improved by clearly describing the algorithm used for training, and how it scales to large networks and datasets. Another point that would deserve further discussion is how the hyper-parameter search is performed ( not using test data I assume), and how the compared methods dealt with the search over hyper-parameters to determine the accuracy-compression tradeoff. Ideally, I think, methods should be evaluated across different points on this trade-off.","['3', '4', '3']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[4, 13, 18]","[10, 19, 24]","[21, 21, 391]","[8, 10, 190]","[12, 9, 167]","[1, 2, 34]","The sentiment score is 50 (slightly positive) because the reviewer acknowledges the paper's contribution as 'interesting' and notes that it achieves results 'on par with state-of-the-art' methods. However, they also point out areas for improvement and some criticisms, balancing the positive aspects. The politeness score is 75 (quite polite) because the reviewer uses respectful language throughout, offers constructive criticism, and phrases suggestions in a considerate manner (e.g., 'It would be useful to...'). The reviewer maintains a professional tone without using harsh or dismissive language, even when pointing out potential issues or areas for improvement.",50,75
Sparsely-Connected Neural Networks: Towards Efficient VLSI Implementation of Deep Neural Networks,Accept,2017,"['Arash Ardakani', 'Carlo Condo', 'Warren J. Gross']","[7, 6, 6]","['Good paper, accept', 'Marginally above acceptance threshold', 'Marginally above acceptance threshold']","Experimental results look reasonable, validated on 3 tasks. References could be improved, for example I would rather see Rumelhart*s paper cited for back-propagation than the Deep Learning book.","['3', '4', '3']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[5, 7, 20]","[11, 13, 26]","[32, 98, 329]","[17, 38, 135]","[8, 32, 76]","[7, 28, 118]","The sentiment score is 50 (slightly positive) because the reviewer acknowledges that the experimental results 'look reasonable' and are 'validated on 3 tasks', which is a positive assessment. However, they also suggest improvements for references, indicating it's not entirely positive. The politeness score is 0 (neutral) because the language is direct and matter-of-fact, without being particularly polite or rude. The reviewer states their opinions and suggestions plainly, without using overly courteous language or harsh criticism.",50,0
Steerable CNNs,Accept,2017,"['Taco S. Cohen', 'Max Welling']","[6, 7, 8]","['Marginally above acceptance threshold', 'Good paper, accept', 'Top 50% of accepted papers, clear accept']","This paper presents a theoretical treatment of transformation groups applied to convnets, and presents some empirical results showing more efficient usage of network parameters. The basic idea of steerability makes huge sense and seems like a very important idea to develop. It is also a very old idea in image processing and goes back to Simoncelli, Freeman, Adelson, as well as Perona/Greenspan and others in the early 1990s. This paper approaches it through a formal treatment of group theory. But at the end of the day the idea seems pretty simple - the feature representation of a transformed image should be equivalent to a transformed feature representation of the original image. Given that the authors are limiting their analysis to discrete groups - for example rotations of 0, 90, 180, and 270 deg. - the formalities brought in from the group theoretic analysis seem a bit overkill. I*m not sure what this buys us in the end. it seems the real challenge lies in implementing continuous transformations, so if the theory could guide us in that direction it would be immensely helpful. Also the description of the experiments is fairly opaque. I would have a hard time replicating what exactly the authors did here in terms of implementing capsules or transformation groups.","['3', '4', '3']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[-1, -1, 1]","[4, 4, 6]","[8, 6, 21]","[4, 3, 9]","[4, 3, 12]","[0, 0, 0]","The sentiment score is slightly positive (20) because the reviewer acknowledges the importance and potential of the paper's main idea, calling it 'very important' and saying it 'makes huge sense'. However, they also express several criticisms and doubts, which temper the positive sentiment. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, acknowledging the paper's merits before presenting criticisms. They avoid harsh or dismissive language, instead using phrases like 'I'm not sure' and 'it seems' to soften their critiques. The reviewer also offers constructive suggestions for improvement, which is a polite approach to criticism.",20,50
Stick-Breaking Variational Autoencoders,Accept,2017,"['Eric Nalisnick', 'Padhraic Smyth']","[8, 4, 8]","['Top 50% of accepted papers, clear accept', 'Ok but not good enough - rejection', 'Top 50% of accepted papers, clear accept']","This paper presents an approach which modifies the variational auto-encoder (VAE) framework so as to use stochastic latent dimensionality. This is achieved by using an inherently infinite prior, the stick-breaking process. This is coupled with inference tailored to this model, specifically the Kumaraswamy distribution as an approximate variational posterior. The resulting model is named the SB-VAE which also has a semi-supervised extension, in similar vein to the original VAE paper. There*s a lot of interest in VAEs these days; many lines of work seek to achieve automatic *black-box* inference in these models. For example, the authors themselves mention parallel work by Blei*s lab (also others) towards this direction. However, there*s a lot of merit in investigating more bespoke solutions to new models, which is what the authors are doing in this paper. Indeed, a (useful) side-effect of providing efficient inference for the SB-VAE is drawing attention to the use of the Kumaraswamy distribution which hasn*t been popular in ML. Although the paper is in general well structured, I found it confusing at parts. I think the major source of confusion comes from the fact that the model specification and model inference are discussed in a somehow mixed manner. The pre-review questions clarified most parts. I have two main concerns regarding the methodology and motivation of this paper. Firstly, conditioning the model directly on the stick-breaking weights seems a little odd. I initially thought that there was some mixture probabilistic model involved, but this is not the case. To be fair, the authors discuss about this issue (which became clearer to me after the pre-review questions), and explain that they*re investigating the apparently challenging problem of using a base distribution G_0. The question is whether their relaxation is still useful. From the experiments it seems that the method is at least competitive, so the answer is yes. Hopefully an extension will come in the future, as the authors mention. The second concern is about the motivation of this method. It seems that the paper fails to clearly explain in a convincing way why it is beneficial to reformulate the VAE as a SB-VAE. I understand that the non-parametric property induced by the prior might result in better capacity control, however I feel that this advantage (and potentially others which are still unclear to me) is not sufficiently explained and demonstrated. Perhaps some comparison with a dropout approach or a more thorough discussion related to dropout would make this clearer. Overall, I found this to be an interesting paper, it would be a good fit for ICLR.","['4', '4', '5']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[5, 30]","[10, 36]","[38, 216]","[19, 129]","[18, 29]","[1, 58]","The sentiment score is 60 (positive) because the reviewer expresses interest in the paper's approach, acknowledges its merits, and states it would be a good fit for ICLR. However, they also raise some concerns and points of confusion, which prevents a higher score. The politeness score is 70 (polite) as the reviewer uses respectful language throughout, acknowledges the paper's strengths, and frames criticisms constructively. They use phrases like 'I found it confusing' rather than directly criticizing the authors, and offer suggestions for improvement. The overall tone is professional and courteous, though not excessively formal or deferential.",60,70
Stochastic Neural Networks for Hierarchical Reinforcement Learning,Accept,2017,"['Carlos Florensa', 'Yan Duan', 'Pieter Abbeel']","[7, 7, 8]","['Good paper, accept', 'Good paper, accept', 'Top 50% of accepted papers, clear accept']","Interesting work on hierarchical control, similar to the work of Heess et al. Experiments are strong and manage to complete benchmarks that previous work could not. Analysis of the experiments is a bit on the weaker side. (1) Like other reviewers, I find the use of the term ‘intrinsic’ motivation somewhat inappropriate (mostly because of its current meaning in RL). Pre-training robots with locomotion by rewarding speed (or rewarding grasping for a manipulating arm) is very geared towards the tasks they will later accomplish. The pre-training tasks from Heess et al., while not identical, are similar. (2) The Mutual Information regularization is elegant and works generally well, but does not seem to help in the more complex mazes 1,2 and 3. The authors note this - is there any interpretation or analysis for this result? (3) The factorization between S_agent and S_rest should be clearly detailed in the paper. Duan et al specify S_agent, but for replicability, S_rest should be clearly specified as well - did I miss it? (4) It would be interesting to provide some analysis of the switching behavior of the agent. More generally, some further analysis of the policies (failure modes, effects of switching time on performance) would have been welcome.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[1, 10, 16]","[5, 15, 22]","[18, 52, 610]","[8, 28, 291]","[9, 19, 293]","[1, 5, 26]","The sentiment score is 50 (slightly positive) because the reviewer starts by calling the work 'interesting' and praising the strong experiments. They mention that the analysis is 'a bit on the weaker side,' but overall the tone is more positive than negative. The reviewer provides constructive criticism and suggestions for improvement, which indicates a generally positive sentiment towards the work.

The politeness score is 60 (moderately polite) because the reviewer uses respectful language throughout. They begin with a positive comment and frame their criticisms as suggestions or questions rather than direct attacks. Phrases like 'I find,' 'It would be interesting,' and 'did I miss it?' show a considerate approach to giving feedback. The reviewer also acknowledges the authors' own observations, which demonstrates respect for their work. While not overly formal or effusive, the language is consistently professional and courteous.",50,60
Structured Attention Networks,Accept,2017,"['Yoon Kim', 'Carl Denton', 'Luong Hoang', 'Alexander M. Rush']","[8, 8, 8]","['Top 50% of accepted papers, clear accept', 'Top 50% of accepted papers, clear accept', 'Top 50% of accepted papers, clear accept']","This is a very nice paper. The writing of the paper is clear. It starts from the traditional attention mechanism case. By interpreting the attention variable z as a distribution conditioned on the input x and query q, the proposed method naturally treat them as latent variables in graphical models. The potentials are computed using the neural network. Under this view, the paper shows traditional dependencies between variables (i.e. structures) can be modeled explicitly into attentions. This enables the use of classical graphical models such as CRF and semi-markov CRF in the attention mechanism to capture the dependencies naturally inherit in the linguistic structures. The experiments of the paper prove the usefulness of the model in various level — seq2seq and tree structure etc. I think it’s solid and the experiments are carefully done. It also includes careful engineering such as normalizing the marginals in the model. In sum, I think this is a solid contribution and the approach will benefit the research in other problems.","['4', '5', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is fairly confident that the evaluation is correct']","[-1, 26, 11, -3, -1, -3]","[4, 32, 17, 2, 4, 2]","[3, 148, 82, 3, 2, 2]","[3, 127, 69, 3, 2, 2]","[0, 4, 2, 0, 0, 0]","[0, 17, 11, 0, 0, 0]","The sentiment score is 90 because the review is overwhelmingly positive. The reviewer uses phrases like 'very nice paper', 'clear writing', 'solid contribution', and states that the approach will benefit research in other problems. They also praise the experiments as 'carefully done' and mention the paper's usefulness at various levels. The politeness score is 80 because the reviewer maintains a professional and respectful tone throughout. They use polite language such as 'I think' to soften their assertions and provide constructive feedback. The review focuses on the paper's strengths without any harsh criticism, indicating a courteous approach to the review process.",90,80
Support Regularized Sparse Coding and Its Fast Encoder,Accept,2017,"['Yingzhen Yang', 'Jiahui Yu', 'Pushmeet Kohli', 'Jianchao Yang', 'Thomas S. Huang']","[7, 6, 7]","['Good paper, accept', 'Marginally above acceptance threshold', 'Good paper, accept']","The work proposes to use the geometry of data (that is considered to be known a priori) in order to have more consistent sparse coding. Namely, two data samples that are similar or neighbours, should have a sparse code that is similar (in terms of support). The general idea is not unique, but it is an interesting one (if one admits that the adjacency matrix A is known a priori), and the novelty mostly lies on the definition of the regularisation term that is an l1-norm (while other techniques would mostly use l2 regularisation). Based on this idea, the authors develop a new SRSC algorithm, which is analysed in detail and shown to perform better than its competitors based on l2 sparse coding regularisation and other schemes in terms of clustering performance. Inspired by LISTA, the authors then propose an approximate solution to the SRSC problem, called Deep-SRSC, that acts as a sort of fast encoder. Here too, the idea is interesting and seems to be quite efficient from experiments on USPS data, even if the framework seems to be strongly inspired from LISTA. That scheme should however be better motivated, by the limitations of SRSC that should be presented more clearly. Overall, the paper is well written, and pretty complete. It is not extremely original in its main ideas though, but the actual algorithm and implementation seem new and effective.","['4', '3', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[10, 2, 15, 10, 53]","[16, 8, 21, 15, 59]","[60, 114, 322, 154, 1126]","[32, 52, 180, 80, 740]","[23, 40, 102, 39, 101]","[5, 22, 40, 35, 285]","The sentiment score is 50 (slightly positive) because the reviewer acknowledges the work as interesting and effective, despite noting that it's not extremely original. They mention that the paper is well-written and complete, and that the algorithm and implementation seem new and effective. However, they also point out some limitations and areas for improvement. The politeness score is 75 (quite polite) because the reviewer uses respectful and professional language throughout. They offer constructive criticism and balance positive comments with areas for improvement. The tone is objective and academic, avoiding harsh or dismissive language. The reviewer acknowledges the merits of the work while also providing suggestions for enhancement, which is a hallmark of polite academic discourse.",50,75
Temporal Ensembling for Semi-Supervised Learning,Accept,2017,"['Samuli Laine', 'Timo Aila']","[7, 9, 8]","['Good paper, accept', '9', 'Top 50% of accepted papers, clear accept']","This paper presents a model for semi-supervised learning by encouraging feature invariance to stochastic perturbations of the network and/or inputs. Two models are described: One where an invariance term is applied between different instantiations of the model/input a single training step, and a second where invariance is applied to features for the same input point across training steps via a cumulative exponential averaging of the features. These models evaluated using CIFAR-10 and SVHN, finding decent gains of similar amounts in each case. An additional application is also explored at the end, showing some tolerance to corrupted labels as well. The authors also discuss recent work by Sajjadi &al that is very similar in spirit, which I think helps corroborate the findings here. My largest critique is it would have been nice to see applications on larger datasets as well. CIFAR and SVHN are fairly small test cases, though adequate for demonstration of the idea. For cases of unlabelled data especially, it would be good to see tests with on the order of 1M+ data samples, with 1K-10K labeled, as this is a common case when labels are missing. On a similar note, data augmentations are restricted to only translations and (for CIFAR) horizontal flips. While *standard,* as the paper notes, more augmentations would have been interesting to see --- particularly since the model is designed explicitly to take advantage of random sampling. Some more details might also pop up, such as the one the paper mentions about handling horizontal flips in different ways between the two model variants. Rather than restrict the system to a particular set of augmentations, I think it would be interesting to push it further, and see how its performance behaves over a larger array of augmentations and (even fewer) numbers of labels. Overall, this seems like a simple approach that is getting decent results, though I would have liked to see more and larger experiments to get a better sense for its performance characteristics. Smaller comment: the paper mentions *dark knowledge* a couple times in explaining results, e.g. bottom of p.6. This is OK for a motivation, but in analyzing the results I think it may be possible to have something more concrete. For instance, the consistency term encourages feature invariance to the stochastic sampling more strongly than would a classification loss alone.","['4', '4', '5']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[14, 15]","[20, 21]","[67, 69]","[30, 28]","[18, 20]","[19, 21]","The sentiment score is 50 (slightly positive) because the reviewer acknowledges the paper's contributions and decent results, but also expresses some critiques and suggestions for improvement. The review starts with a neutral summary and then balances positive aspects ('decent gains', 'simple approach that is getting decent results') with constructive criticism ('largest critique', 'would have liked to see more and larger experiments'). The politeness score is 75 (quite polite) because the reviewer uses respectful language throughout, offers constructive feedback, and frames criticisms as suggestions for improvement rather than harsh judgments. Phrases like 'it would have been nice to see' and 'I think it would be interesting' maintain a polite tone while expressing areas for potential enhancement.",50,75
The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables,Accept,2017,"['Chris J. Maddison', 'Andriy Mnih', 'Yee Whye Teh']","[7, 9, 8]","['Good paper, accept', '9', 'Top 50% of accepted papers, clear accept']","The authors of the paper present a novel distribution for discrete variables called the *concrete distribution*. The distribution can be seen as a continuous relaxation for a distribution over discrete random variables. The main motivation for introduction of the concrete distribution is the possibility to compute the gradient of discrete stochastic nodes in Stochastic Computational Graphs. I think the paper is well written and sound, definitely of interest for the conference program. As to the experimental part, the authors have results which support some kind of consistent superior performance for VIMCO for linear models and for concrete relaxations for non-linear models. Any explanation for that? Is this confirmed over different models and maybe datasets? Similarly, it looks like VIMCO outperforms (in Figure 4) Concrete for large m, on the test NLL. I would encourage to try with other values of m to see if this dependence on large m is confirmed or not. I believe the paper should be accepted to the conference, however please consider that I*m not an expert in this field. Some minor observations/comments/issues: -Section 2.1: there is a repetition *be be* in the first paragraph. -Section 2.4: I would add a reference for the *multi-sample variational objective* -Section 3.1, just before Section 3.2: *the Gumbel is a crucial 1*. Why 1 and not *one*? -Section 3.3, last paragraph: *Thus, in addition to relaxing the sampling pass of a SCG the log...* I would add a comma after *SCG*. More in general, the second part of the paragraph is very dense and not easy to *absorb*. I don*t think it*s an issue with the presentation: the concepts themselves are just dense. However, maybe the authors could find a way to make the paragraph easier to assimilate for a less experienced reader. -Section 5.1, second paragraph: *All our models are neural networks with layers of n-ary discrete stochastic nodes with log_2(n)-dimensional states on the corners of the hypercube {-1,1}^log_2(n). The distribution of the nodes are parametrized by n real values log alpha_k*. It is not clear to me, where does the log_2(n) come from. Similarly for the {-1,1}. -Section 5.2: After *this distribution.* and *We will* there is an extra space. -If a compare the last formula in Section 5.3 with Eq. 8, I don*t see exactly why the former is a special case of the latter. Is it because q(Z^i | x) is always one?","['3', '5', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[5, 15, 20]","[10, 20, 26]","[57, 55, 267]","[28, 31, 139]","[27, 22, 107]","[2, 2, 21]","The sentiment score is 70 (positive) because the reviewer states the paper is 'well written and sound, definitely of interest for the conference program' and recommends acceptance. They provide constructive feedback and suggestions for improvement, indicating overall positive sentiment. The politeness score is 80 (quite polite) due to the reviewer's respectful tone, use of phrases like 'I think' and 'I believe', and acknowledgment of their own potential limitations ('I'm not an expert in this field'). They offer suggestions politely and highlight both strengths and areas for improvement. The reviewer maintains a professional and courteous tone throughout, even when pointing out minor issues.",70,80
The Neural Noisy Channel,Accept,2017,"['Lei Yu', 'Phil Blunsom', 'Chris Dyer', 'Edward Grefenstette', 'Tomas Kocisky']","[7, 7, 6]","['Good paper, accept', 'Good paper, accept', 'Marginally above acceptance threshold']","This paper proposes to use an SSNT model of p(x|y) to allow for a noisy channel model of conditional generation that (still) allows for incremental generation of y. The authors also propose an approximate search strategy for decoding, and do an extensive empirical evaluation. PROs: This paper is generally well written, and the SSNT model is quite interesting and its application here well motivated. Furthermore, the empirical evaluation is very well done, and the authors obtain good results. CONs: One might be concerned about whether the additional training and decoding complexity is warranted. For instance, one might plausibly obtain the benefits of the proposed approach by reranking (full) outputs from a standard seq2seq model with a score combining p(y|x), p(x|y), and p(y). (It*s worth noting that Li et al. (NAACL 2016) do something similar for conversation modeling). At the same time, being able to rerank during search may be helpful, and so it might be nice to see some experiments addressing this. Other Comments: - Given that the main thrust of the paper is to provide a model for p(x|y), the paper might be slightly clearer if Section 2 were presented from the perspective of modeling p(x|y) instead of switching back to p(y|x) as in the original Yu et al. paper. - It initially seems strange to suggest a noisy-channel model as a way of addressing the *explaining away* problem, since now you have an explicit, uncalibrated p(y) term. However, since seq2seq models appear to naturally do a lot of target-side language modeling, incorporating an explicit p(x|y) term seems quite clever.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[18, 8]","[22, 14]","[29, 397]","[17, 154]","[9, 216]","[3, 27]","The sentiment score is 60 (moderately positive) because the reviewer begins by highlighting several pros of the paper, including that it is well-written, has an interesting model, good motivation, and extensive empirical evaluation with good results. However, they also mention some cons and suggestions for improvement, which prevents the score from being higher. The politeness score is 70 (fairly polite) because the reviewer uses respectful language throughout, acknowledges the paper's strengths, and frames criticisms constructively as suggestions or areas for potential improvement rather than harsh criticisms. They use phrases like 'One might be concerned' and 'it might be nice to see' when suggesting changes, which maintains a polite tone.",60,70
Third Person Imitation Learning,Accept,2017,"['Bradly C Stadie', 'Pieter Abbeel', 'Ilya Sutskever']","[5, 6, 6]","['5', 'Marginally above acceptance threshold', 'Marginally above acceptance threshold']","This paper proposed a novel adversarial framework to train a model from demonstrations in a third-person perspective, to perform the task in the first-person view. Here the adversarial training is used to extract a novice-expert (or third-person/first-person) independent feature so that the agent can use to perform the same policy in a different view point. While the idea is quite elegant and novel (I enjoy reading it), more experiments are needed to justify the approach. Probably the most important issue is that there is no baseline, e.g., what if we train the model with the image from the same viewpoint? It should be better than the proposed approach but how close are they? How the performance changes when we gradually change the viewpoint from third-person to first-person? Another important question is that maybe the network just blindly remembers the policy, in this case, the extracted feature could be artifacts of the input image that implicitly counts the time tick in some way (and thus domain-agonistic), but can still perform reasonable policy. Since the experiments are conduct in a synthetic environment, this might happen. An easy check is to run the algorithm on multiple viewpoint and/or with blurred/differently rendered images, and/or with random initial conditions. Other ablation analysis is also needed. For example, I am not fully convinced by the gradient flipping trick used in Eqn. 5, and in the experiments there is no ablation analysis for that (GAN/EM style training versus gradient flipping trick). For the experiments, Fig. 4,5,6 does not have error bars and is not very convincing.","['3', '4', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[1, 13, 17, 7]","[7, 19, 23, 13]","[42, 76, 481, 13]","[17, 34, 227, 4]","[11, 14, 57, 2]","[14, 28, 197, 7]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the paper's novelty and elegance, they express significant concerns about the lack of baselines, potential issues with the approach, and the need for more experiments and ablation analyses. The overall tone suggests that substantial revisions are needed. The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, acknowledging the paper's strengths ('elegant and novel', 'I enjoy reading it') before presenting criticisms. They phrase their concerns as suggestions and questions rather than direct criticisms, maintaining a constructive tone. The reviewer also uses polite hedging language like 'Probably' and 'maybe' when presenting potential issues, which softens the critique.",-20,60
Tighter bounds lead to improved classifiers,Accept,2017,['Nicolas Le Roux'],"[8, 6, 4]","['Top 50% of accepted papers, clear accept', 'Marginally above acceptance threshold', 'Ok but not good enough - rejection']","The paper analyses the misclassification error of discriminators and highlights the fact that while uniform probability prior of the classes makes sense early in the optimization, the distribution deviates from this prior significantly as the parameters move away from the initial values. Consequently, the optimized upper bound (log-loss) gets looser. As a fix, an optimization procedure based on recomputing the bound is proposed. The paper is well written. While the main observation made in this paper is a well-known fact, it is presented in a clear and refreshing way that may make it useful to a wide audience at this venue. I would like to draw the author*s attention to the close connections of this framework with curriculum learning. More on this can be found in [1] (which is a relevant reference that should be cited). A discussion on this could enrich the quality of the paper. There is a large body of work on directly optimizing task losses[2][3] and the references therein. These should also be discussed and related particularly to section 3 (optimizing the ROC curve). [1] Training Highly Multiclass Classifiers, Gupta et al. 2014. [2] Direct Loss Minimization for Structured Prediction, McAllester et al. [3] Generalization Bounds and Consistency for Latent Structural Probit and Ramp Loss, McAllester and Keshet. Final comment: I believe the material presented in this paper is of interest to a wide audience at ICLR. The problem studied is interesting and the proposed approach is sound. I recommend to accept the paper and increase my score (from 7 to 8).","['5', '4', '4']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']",[15],[21],[70],[34],[30],[6],"The sentiment score is 80 (positive) because the reviewer expresses a generally favorable view of the paper. They describe it as 'well written' and presenting ideas in a 'clear and refreshing way'. The reviewer recommends accepting the paper and even increases their score from 7 to 8. However, it's not a perfect 100 as they do suggest some improvements and additional references to include. The politeness score is 90 (very polite) because the reviewer uses respectful language throughout, offers constructive feedback, and frames their suggestions as recommendations rather than demands. They use phrases like 'I would like to draw the author*s attention' and 'These should also be discussed', which are polite ways of suggesting improvements. The overall tone is professional and courteous, showing respect for the authors' work while providing valuable feedback.",80,90
TopicRNN: A Recurrent Neural Network with Long-Range Semantic Dependency,Accept,2017,"['Adji B. Dieng', 'Chong Wang', 'Jianfeng Gao', 'John Paisley']","[6, 7, 8]","['Marginally above acceptance threshold', 'Good paper, accept', 'Top 50% of accepted papers, clear accept']",This paper introduces a model that blends ideas from generative topic models with those from recurrent neural network language models. The authors evaluate the proposed approach on a document level classification benchmark as well as a language modeling benchmark and it seems to work well. There is also some analysis as to topics learned by the model and its ability to generate text. Overall the paper is clearly written and with the code promised by the authors others should be able to re-implement the approach. I have 2 potentially major questions I would ask the authors to address: 1 - LDA topic models make an exchangability (bag of words) assumption. The discussion of the generative story for TopicRNN should explicitly discuss whether this assumption is also made. On the surface it appears it is since y_t is sampled using only the document topic vector and h_t but we know that in practice h_t comes from a recurrent model that observes y_t-1. Not clear how this clean exposition of the generative model relates to what is actually done. In the Generating sequential text section it’s clear the topic model can’t generate words without using y_1 - t-1 but this seems inconsistent with the generative model specification. This needs to be shown in the paper and made clear to have a complete paper. 2 - The topic model only allows for linear interactions of the topic vector theta. It seems like this might be required to keep the generative model tractable but seems like a very poor assumption. We would expect the topic representation to have rich interactions with a language model to create nonlinear adjustments to word probabilities for a document. Please add discussion as to why this modeling choice exists and if possible how future work could modify that assumption (or explain why it’s not such a bad assumption as one might imagine) Figure 2 colors very difficult to distinguish.,"['3', '4', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[2, 14, 18, 11]","[7, 20, 24, 17]","[22, 80, 545, 117]","[7, 41, 263, 49]","[14, 30, 250, 38]","[1, 9, 32, 30]","The sentiment score is 50 (slightly positive) because the reviewer acknowledges the paper's clear writing, promising results, and the authors' commitment to sharing code. However, they also raise two 'potentially major questions,' indicating some significant concerns. The politeness score is 75 (quite polite) as the reviewer uses respectful language throughout, acknowledging the paper's strengths before presenting their questions, and framing their concerns as requests for clarification or discussion rather than outright criticism. The reviewer also offers constructive suggestions for improvement and future work, which contributes to the polite tone.",50,75
Topology and Geometry of Half-Rectified Network Optimization,Accept,2017,"['C. Daniel Freeman', 'Joan Bruna']","[2, 7, 8]","['Strong rejection', 'Good paper, accept', 'Top 50% of accepted papers, clear accept']","This is an incremental result (several related results that the authors of the paper mentioned here were already published). The authors claim that they can get rid of the technical assumptions from the previous papers but the results they propose are significantly weaker and also quite technical. The main theoretical result - Theorem 2.4 is not convincing at all. Furthermore, the paper is badly written. No theoretical intuition is given, the experimental section is weak and in some places the formatting is wrong.","['5', '3', '3']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[2, 8]","[7, 14]","[19, 162]","[5, 64]","[14, 88]","[0, 10]","The sentiment score is -80 because the review is overwhelmingly negative. The reviewer describes the paper as an 'incremental result' with 'significantly weaker' findings compared to previous work. They state that the main theoretical result is 'not convincing at all' and criticize the paper as 'badly written' with a 'weak' experimental section. There are no positive comments to balance these criticisms. The politeness score is -50 because while the reviewer doesn't use explicitly rude language, their tone is quite harsh and dismissive. They bluntly state negative opinions without softening the language or offering constructive feedback. Phrases like 'not convincing at all' and 'badly written' are particularly impolite in academic discourse. A more polite review would offer specific suggestions for improvement or acknowledge any positive aspects of the work.",-80,-50
Towards Deep Interpretability (MUS-ROVER II): Learning Hierarchical Representations of Tonal Music,Accept,2017,"['Haizi Yu', 'Lav R. Varshney']","[6, 8, 6]","['Marginally above acceptance threshold', 'Top 50% of accepted papers, clear accept', 'Marginally above acceptance threshold']","After the discussion below, I looked at previous work by the authors (MUS-ROVER) on which this paper was based. On one hand, this was very helpful for me to better understand the current paper. On the other hand, this was very needed for me to better understand the current paper. Overall, while I think that I like this work, and while I am familiar with the JSB chorales, with probabilistic approaches, with n-grams, etc, I did find the paper quite hard to follow at various parts. The extensive use of notation did not help the clarity. I think the ideas and approaches are good, and certainly worth publishing and worth pursuing. I am not sure that, in the paper*s current form, ICLR is an appropriate venue. (Incidentally, the issue is not the application as I think that music applications can be very appropriate, nor is the problem necessarily with the approach... see my next suggestion..). I get the sense that a long-form journal publication would actually give the authors the space necessary to fully explain these ideas, provide clearer running examples where needed, provide the necessary background for the appropriate readership, provide the necessary background on the previous system, perhaps demonstrating results on a second dataset to show generality of the approach, etc. A short conference paper just seems to me to be too dense a format for giving this project the description it merits. If it were possible to focus on just one aspect of this system, then that might work, but I do not have good suggestions for exactly how to do that. If the paper were revised substantially (though I cannot suggest details for how to do this within the appropriate page count), I would consider raising my score. I do think that the effort would be better invested in turning this into a long (and clearer) journal submission. [Addendum: based on discussions here & revisions, I have revised my score]","['3', '4', '3']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[3, 12]","[8, 18]","[15, 294]","[7, 132]","[7, 98]","[1, 64]","The sentiment score is slightly negative (-20) because while the reviewer expresses some positive aspects ('I think that I like this work', 'ideas and approaches are good'), there are significant criticisms and suggestions for major revisions. The reviewer finds the paper hard to follow, suggests it may not be suitable for the current venue, and recommends considering a journal publication instead. The politeness score is moderately positive (60) as the reviewer uses polite and constructive language throughout. They acknowledge positive aspects, offer explanations for their criticisms, and provide suggestions for improvement. The tone is professional and respectful, avoiding harsh or rude language even when expressing concerns.",-20,60
Towards a Neural Statistician,Accept,2017,"['Harrison Edwards', 'Amos Storkey']","[8, 6]","['Top 50% of accepted papers, clear accept', 'Marginally above acceptance threshold']","Sorry for the late review -- I*ve been having technical problems with OpenReview which prevented me from posting. This paper presents a method for learning to predict things from sets of data points. The method is a hierarchical version of the VAE, where the top layer consists of an abstract context unit that summarizes a dataset. Experiments show that the method is able to *learn to learn* by acquiring the ability to learn distributions from small numbers of examples. Overall, this paper is a nice addition to the literature on one- or few-shot learning. The method is conceptually simple and elegant, and seems to perform well. Compared to other recent papers on one-shot learning, the proposed method is simpler, and is based on unsupervised representation learning. The paper is clearly written and a pleasure to read. The name of the paper is overly grandiose relative to what was done; the proposed method doesn’t seem to have much in common with a statistician, unless one means by that *someone who thinks up statistics*. The experiments are well chosen, and the few-shot learning results seem pretty solid given the simplicity of the method. The spatial MNIST dataset is interesting and might make a good toy benchmark. The inputs in Figure 4 seem pretty dense, though; shouldn’t the method be able to recognize the distribution with fewer samples? (Nitpick: the red points in Figure 4 don’t seem to correspond to meaningful points as was claimed in the text.) Will the authors release the code?","['4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[2, 22]","[8, 28]","[22, 135]","[9, 62]","[12, 54]","[1, 19]","The sentiment score is 80 (positive) because the reviewer describes the paper as 'a nice addition to the literature', 'conceptually simple and elegant', and 'clearly written and a pleasure to read'. They also mention that the method 'seems to perform well' and the experiments are 'well chosen'. The few criticisms (overly grandiose title, dense inputs in Figure 4) are minor and presented constructively. The politeness score is 90 (very polite) due to the reviewer's apologetic opening for the late review, the use of positive language throughout, and the constructive framing of criticisms. The reviewer also asks a polite question about code release, showing engagement with the work. The overall tone is respectful and encouraging, with criticisms presented as suggestions rather than demands.",80,90
Towards the Limit of Network Quantization,Accept,2017,"['Yoojin Choi', 'Mostafa El-Khamy', 'Jungwon Lee']","[7, 7, 7]","['Good paper, accept', 'Good paper, accept', 'Good paper, accept']","This paper proposes a network quantization method for compressing the parameters of neural networks, therefore, compressing the amount of storage needed for the parameters. The authors assume that the network is already pruned and aim for compressing the non-pruned parameters. The problem of network compression is a well-motivated problem and of interest to the ICLR community. The main drawback of the paper is its novelty. The paper is heavily built on the results of Han 2015 and only marginally extends Han 2015 to overcome its drawbacks. It should be noted that the proposed method in this paper has not been proposed before. The paper is well-structured and easy to follow. Although it heavily builds on Han 2015, it is still much longer than Han 2015. I believe that there is still some redundancy in the paper. The experiments section starts on Page 12 whereas for Han 2015 the experiments start on page 5. Therefore, I believe much of the introductory text is redundant and can be efficiently cut. Experimental results in the paper show good compression performance compared to Han 2015 while losing very little accuracy. Can the authors mention why there is no comparison with Hang 2015 on ResNet in Table 1? Some comments: 1) It is not clear whether the procedure depicted in figure 1 is the authors’ contribution or has been in the literature. 2) In section 4.1 the authors approximate the hessian matrix with a diagonal matrix. Can the authors please explain how this approximation affects the final compression? Also how much does one lose by making such an approximation? minor typos (These are for the revised version of the paper): 1) Page 2, Parag 3, 3rd line from the end: fined-tuned -> fine-tuned 2) Page 2, one para to the end, last line: assigned for -> assigned to 3) Page 5, line 2, same as above 4) Page 8, Section 5, Line 3: explore -> explored","['3', '3', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[12, 15, 18]","[17, 20, 24]","[29, 122, 213]","[14, 60, 109]","[11, 43, 48]","[4, 19, 56]","The sentiment score is slightly positive (20) because the reviewer acknowledges the paper's relevance and good structure, while also noting some drawbacks. The reviewer mentions that the problem is well-motivated and of interest to the community, and that the paper is well-structured and easy to follow. However, they also point out issues with novelty and redundancy, which prevents a higher positive score. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, offers constructive criticism, and phrases their concerns as questions or suggestions rather than harsh criticisms. They also provide specific examples and minor typo corrections, which is helpful and considerate. The reviewer maintains a professional tone without using overly negative language, even when pointing out areas for improvement.",20,60
Tracking the World State with Recurrent Entity Networks,Accept,2017,"['Mikael Henaff', 'Jason Weston', 'Arthur Szlam', 'Antoine Bordes', 'Yann LeCun']","[7, 7, 7]","['Good paper, accept', 'Good paper, accept', 'Good paper, accept']",The paper proposed a multi-memory mechanism that memorizes different information into different components/entities. It could be considered as a mixture model in RNN. This is a very interesting model and result is convincing. A limitation is that we do not know how to generalize to some unseen entities and how to visualize what entities the model learned.,"['3', '5', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[7, 19, 9, 13, 30]","[12, 25, 15, 18, 36]","[24, 237, 138, 86, 315]","[11, 126, 54, 48, 162]","[12, 84, 74, 29, 113]","[1, 27, 10, 9, 40]","The sentiment score is 70 (positive) because the reviewer describes the paper as 'very interesting' and states that the 'result is convincing'. They also mention a limitation, which slightly reduces the overall positive sentiment. The politeness score is 50 (somewhat polite) because the language is professional and respectful, without being overly formal or effusive. The reviewer provides a balanced perspective, acknowledging both strengths and limitations of the work, which contributes to a polite tone. However, the review is quite brief and direct, which prevents it from scoring higher on the politeness scale.",70,50
Trained Ternary Quantization,Accept,2017,"['Chenzhuo Zhu', 'Song Han', 'Huizi Mao', 'William J. Dally']","[3, 7, 7, 3, 8]","['Clear rejection', 'Good paper, accept', 'Good paper, accept', 'Clear rejection', 'Top 50% of accepted papers, clear accept']","The paper shows a different approach to a ternary quantization of weights. Strengths: 1. The paper shows performance improvements over existing solutions 2. The idea of learning the quantization instead of using pre-defined human-made algorithm is nice and very much in the spirit of modern machine learning. Weaknesses: 1. The paper is very incremental. 2. The paper is addressed to a very narrow audience. The paper very clearly assumes that the reader is familiar with the previous work on the ternary quantization. It is *what is new in the topic* update, not really a standalone paper. The description of the main algorithm is very concise, to say the least, and is probably clear to those who read some of the previous work on this narrow subject, but is unsuitable for a broader deep learning audience. 3. There is no convincing motivation for the work. What is presented is an engineering gimmick, that would be cool and valuable if it really is used in production, but is that really needed for anything? Are there any practical applications that require this refinement? I do not find the motivation *it is related to mobile, therefore it is cool* sufficient. This paper is a small step further in a niche research, as long as the authors do not provide a sufficient practical motivation for pursuing this particular topic with the next step on a long list of small refinements, I do not think it belongs in ICLR with a broad and diversified audience. Also - the code was not released is my understanding.","['3', '5', '3', '5', '5']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[2, 3, 3, 33]","[8, 9, 8, 39]","[7, 150, 24, 274]","[2, 66, 13, 174]","[3, 67, 10, 23]","[2, 17, 1, 77]","The sentiment score is -50 because while the reviewer acknowledges some strengths of the paper (performance improvements and a novel approach), they express significant concerns about its incremental nature, narrow audience, lack of motivation, and insufficient explanation. The overall tone suggests the reviewer does not recommend the paper for publication in its current form. The politeness score is 20 because the reviewer uses professional language and provides both positive and negative feedback. They avoid harsh criticism and use phrases like 'The paper shows' and 'The idea... is nice' when discussing strengths. However, the politeness is somewhat limited by the direct nature of the criticisms, particularly in stating that the paper doesn't belong in ICLR.",-50,20
Training Agent for First-Person Shooter Game with Actor-Critic Curriculum Learning,Accept,2017,"['Yuxin Wu', 'Yuandong Tian']","[6, 4, 7]","['Marginally above acceptance threshold', 'Ok but not good enough - rejection', 'Good paper, accept']","The paper describes approaches taken to train learning agents for the 3D game Doom. The authors propose a number of performance enhancements (curriculum learning, attention (zoomed-in centered) frames, reward shaping, game variables, post-training rules) inspired by domain knowledge. The enhancements together lead to a clear win as demonstrated by the competition results. From Fig 4, the curriculum learning clearly helps with learning over increasingly difficult settings. A nice result is that there is no overfitting to the harder classes once they have learned (probably because the curriculum is health and speed). The authors conclude from Fig 5 that the adaptive curriculum is better and more stable that pure A3C; however, this is a bit of a stretch given that graph. They go on to say that Pure A3C doesn*t learn at all in the harder map but then show no result/graph to back this claim. Tbl 5 shows a clear benefit of the post-training rules. If the goal is to solve problems like these (3D shooters), then this paper makes a significant contribution in that it shows which techniques are practical for solving the problem and ultimately improving performance in these kinds of tasks. Still, I am just not excited about this paper, mainly because it relies so heavily of many sources of domain knowledge, it is quite far from the pure reinforcement learning problem. The results are relatively unsurprising. Maybe they are novel for this problem, though. I*m not sure we can realistically draw any conclusions about Figure 6 in the paper*s current form. I recommend the authors increase the resolution or run some actual metrics to determine the fuzziness/clarity of each row/image: something more concrete than an arrow of already low-resolution images. --- Added after rebuttal: I still do not see any high-res images for Figure 6 or any link to them, but I trust that the authors will add them if accepted.","['4', '5', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[8, 12]","[14, 18]","[49, 150]","[10, 69]","[16, 73]","[23, 8]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects of the paper (e.g., 'clear win', 'nice result', 'significant contribution'), they express overall lack of excitement about the paper ('I am just not excited about this paper'). They also point out several criticisms and areas for improvement. The politeness score is moderately positive (50) as the reviewer uses professional language throughout, offers constructive criticism, and acknowledges positive aspects alongside the negative ones. They also provide specific recommendations for improvement without using harsh or dismissive language. The reviewer maintains a respectful tone even when expressing concerns or disagreements with the authors' conclusions.",-20,50
Training Compressed Fully-Connected Networks with a Density-Diversity Penalty,Accept,2017,"['Shengjie Wang', 'Haoran Cai', 'Jeff Bilmes', 'William Noble']","[9, 6]","['9', 'Marginally above acceptance threshold']","The method proposes to compress the weight matrices of deep networks using a new density-diversity penalty together with a computing trick (sorting weights) to make computation affordable and a strategy of tying weights. This density-diversity penalty consists of an added cost corresponding to the l2-norm of the weights (density) and the l1-norm of all the pairwise differences in a layer. Regularly, the most frequent value in the weight matrix is set to zero to encourage sparsity. As weights collapse to the same values with the diversity penalty, they are tied together and then updated using the averaged gradient. The training process then alternates between training with 1. the density-diversity penalty and untied weights, and 2. training without this penalty but with tied weights. The experiments on two datasets (MNIST for vision and TIMIT for speech) shows that the method achieves very good compression rates without loss of performance. The paper is presented very clearly, presents very interesting ideas and seems to be state of the art for compression. The approach opens many new avenues of research and the strategy of weight-tying may be of great interest outside of the compression domain to learn regularities in data. The result tables are a bit confusing unfortunately. minor issues: p1 english mistake: “while networks *that* consist of convolutional layers”. p6-p7 Table 1,2,3 are confusing. Compared to the baseline (DC), your method (DP) seems to perform worse: In Table 1 overall, Table 2 overall FC, Table 3 overall, DP is less sparse and more diverse than the DC baseline. This would suggest a worse compression rate for DP and is inconsistent with the text which says they should be similar or better. I assume the sparsity value is inverted and that you in fact report the number of non-modal values as a fraction of the total.","['4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[7, 2, 29, 22]","[13, 7, 35, 28]","[50, 18, 299, 157]","[26, 14, 211, 62]","[13, 0, 49, 6]","[11, 4, 39, 89]","The sentiment score is 80 (positive) because the reviewer expresses a very favorable opinion of the paper, describing it as 'very clearly' presented, with 'very interesting ideas' and 'state of the art for compression'. They also mention that the approach 'opens many new avenues of research'. The only negative points are minor issues with tables and an English mistake, which don't significantly detract from the overall positive sentiment. The politeness score is 70 (polite) because the reviewer uses respectful and constructive language throughout. They acknowledge the paper's strengths and offer specific, helpful feedback for improvement. The tone is professional and encouraging, without any harsh criticism. The reviewer also uses phrases like 'I assume' when pointing out potential inconsistencies, showing consideration for the authors' perspective.",80,70
Training deep neural-networks using a noise adaptation layer,Accept,2017,"['Jacob Goldberger', 'Ehud Ben-Reuven']","[5, 7, 5]","['5', 'Good paper, accept', '5']","This paper looks at how to train if there are significant label noise present. This is a good paper where two main methods are proposed, the first one is a latent variable model and training would require the EM algorithm, alternating between estimating the true label and maximizing the parameters given a true label. The second directly integrates out the true label and simply optimizes the p(z|x). Pros: the paper examines a training scenario which is a real concern for big dataset which are not carefully annotated. Cons: the results on mnist is all synthetic and it*s hard to tell if this would translate to a win on real datasets. - comments: Equation 11 should be expensive, what happens if you are training on imagenet with 1000 classes? It would be nice to see how well you can recover the corrupting distribution parameter using either the EM or the integration method. Overall, this is an OK paper. However, the ideas are not novel as previous cited papers have tried to handle noise in the labels. I think the authors can make the paper better by either demonstrating state-of-the-art results on a dataset known to have label noise, or demonstrate that a method can reliably estimate the true label corrupting probabilities.","['4', '5', '5']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[21, 16]","[27, 17]","[196, 5]","[116, 4]","[35, 1]","[45, 0]","The sentiment score is slightly positive (20) because the reviewer describes the paper as 'good' and 'OK', acknowledging its pros and relevance to real-world concerns. However, they also point out cons and suggest improvements, indicating a mixed but generally positive sentiment. The politeness score is moderately positive (50) as the reviewer uses professional and respectful language throughout, offering constructive criticism without harsh or rude comments. They balance positive aspects with areas for improvement, maintaining a courteous tone while providing honest feedback.",20,50
Transfer Learning for Sequence Tagging with Hierarchical Recurrent Networks,Accept,2017,"['Zhilin Yang', 'Ruslan Salakhutdinov', 'William W. Cohen']","[5, 7, 8]","['5', 'Good paper, accept', 'Top 50% of accepted papers, clear accept']","The authors propose transfer learning variants for neural-net-based models, applied to a bunch of NLP tagging tasks. The field of multi-tasking is huge, and the approaches proposed here do not seem to be very novel in terms of machine learning: parts of a general architecture for NLP are shared, the amount of shared *layers* being dependent of the task of interest. The novelty lies in the type of architecture which is used in the particular setup of NLP tagging tasks. The experimental results show that the approach seems to work well when there is not much labeled data available (Figure 2). Table 3 show some limited improvement at full scale. Figure 2 results are debatable though: it seems the authors fixed the architecture size while varying the amount of labeled data; it is very likely that tuning the architecture for each size would have led to better results. Overall, while the paper reads well, the novelty seems a bit limited and the experimental section seems a bit disappointing.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[15, 15, 33]","[21, 21, 39]","[82, 419, 345]","[34, 207, 218]","[35, 201, 80]","[13, 11, 47]","The sentiment score is -30 because the reviewer expresses mixed feelings, leaning towards negative. They acknowledge that the paper 'reads well' but criticize the novelty as 'a bit limited' and the experimental section as 'a bit disappointing'. They also question the methodology in Figure 2. The overall tone suggests more criticism than praise.

The politeness score is 20 because the reviewer uses relatively neutral language and avoids harsh criticism. They use phrases like 'seems to work well' and 'limited improvement' rather than outright negative statements. The critique is presented in a professional manner, without personal attacks or overly blunt language. However, it's not overtly polite either, maintaining a fairly neutral, academic tone throughout.",-30,20
Transfer of View-manifold Learning to Similarity Perception of Novel Objects,Accept,2017,"['Xingyu Lin', 'Hao Wang', 'Zhihao Li', 'Yimeng Zhang', 'Alan Yuille', 'Tai Sing Lee']","[5, 6, 7]","['5', 'Marginally above acceptance threshold', 'Good paper, accept']","This paper proposes a model to learn across different views of objects. The key insight is to use a triplet loss that encourages two different views of the same object to be closer than an image of a different object. The approach is evaluated on object instance and category retrieval and compared against baseline CNNs (untrained AlexNet and AlexNet fine-tuned for category classification) using fc7 features with cosine distance. Furthermore, a comparison against human perception on the *Tenenbaum objects” is shown. Positives: Leveraging a triplet loss for this problem may have some novelty (although it may be somewhat limited given some concurrent work; see below). The paper is reasonably written. Negatives: The paper is missing relevant references of related work in this space and should compare against an existing approach. More details: The “image purification” paper is very related to this work: [A] Joint Embeddings of Shapes and Images via CNN Image Purification. Hao Su*, Yangyan Li*, Charles Qi, Noa Fish, Daniel Cohen-Or, Leonidas Guibas. SIGGRAPH Asia 2015. There they learn to map CNN features to (hand-designed) light field descriptors of 3D shapes for view-invariant object retrieval. If possible, it would be good to compare directly against this approach (e.g., the cross-view retrieval experiment in Table 1 of [A]). It appears that code and data is available online (http://shapenet.github.io/JointEmbedding/). Somewhat related to the proposed method is recent work on multi-view 3D object retrieval: [B] Multi-View 3D Object Retrieval With Deep Embedding Network. Haiyun Guo, Jinqiao Wang, Yue Gao, Jianqiang Li, and Hanqing Lu. IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 25, NO. 12, DECEMBER 2016. There they developed a triplet loss as well, but for multi-view retrieval (given multiple images of the same object). Given the similarity of the developed approach, it somewhat limits the novelty of the proposed approach in my view. Also related are approaches that predict a volumetric representation of an input 2D image (going from image to canonical orientation of 3D shape): [C] R. Girdhar, D. Fouhey, M. Rodriguez, A. Gupta. Learning a Predictable and Generative Vector Representation for Objects. ECCV 2016. [D] Learning a Probabilistic Latent Space of Object Shapes via 3D Generative-Adversarial Modeling. Jiajun Wu*, Chengkai Zhang*, Tianfan Xue, William T. Freeman, and Joshua B. Tenenbaum. NIPS 2016. For the experiments, I would like to see a comparison using different feature layers (e.g., conv4, conv5, pool4, pool5) and feature comparison (dot product, Eucllidean). It has been shown that different layers and feature comparisons perform differently for a given task, e.g., [E] Deep Exemplar 2D-3D Detection by Adapting from Real to Rendered Views. Francisco Massa, Bryan C. Russell, Mathieu Aubry. Conference on Computer Vision and Pattern Recognition (CVPR), 2016. [F] Understanding Deep Features with Computer-Generated Imagery. Mathieu Aubry and Bryan C. Russell. IEEE International Conference on Computer Vision (ICCV), 2015.","['5', '4', '3']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[1, 27, 14, 10, 35, 26]","[7, 33, 20, 16, 41, 31]","[30, 1335, 94, 64, 704, 43]","[11, 627, 49, 33, 337, 20]","[16, 199, 18, 9, 265, 8]","[3, 509, 27, 22, 102, 15]","The sentiment score is slightly negative (-20) because while the reviewer notes some positives ('Leveraging a triplet loss for this problem may have some novelty', 'The paper is reasonably written'), there are more substantial criticisms. The reviewer points out that the paper is missing relevant references, should compare against existing approaches, and that the novelty may be limited due to similar concurrent work. The politeness score is moderately positive (50) as the reviewer uses professional and respectful language throughout, balancing positive and negative feedback, and offering constructive suggestions for improvement rather than harsh criticism. The reviewer uses phrases like 'It would be good to' and 'I would like to see' which maintain a polite tone while still conveying areas for improvement.",-20,50
Tree-structured decoding with doubly-recurrent neural networks,Accept,2017,"['David Alvarez-Melis', 'Tommi S. Jaakkola']","[6, 6, 7]","['Marginally above acceptance threshold', 'Marginally above acceptance threshold', 'Good paper, accept']","This paper proposes a variant of a recurrent neural network that has two orthogonal temporal dimensions that can be used as a decoder to generate tree structures (including the topology) in an encoder-decoder setting. The architecture is well motivated and I can see several applications (in addition to what*s presented in the paper) that need to generate tree structures given an unstructured data. One weakness of the paper is the limitation of experiments. IFTTT dataset seems to be an interesting appropriate application, and there is also a synthetic dataset, however it would be more interesting to see more natural language applications with syntactic tree structures. Still, I consider the experiments sufficient as a first step to showcase a novel architecture. A strength is that the authors experiment with different design decisions when building the topology predictor components of the architecture, about when / how to decide to terminate, as opposed to making a single arbitrary choice. I see future applications of this architecture and it seems to have interesting directions for future work so I suggest its acceptance as a conference contribution.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[3, 25]","[9, 31]","[46, 294]","[17, 149]","[27, 112]","[2, 33]","The sentiment score is 70 (positive) because the reviewer expresses clear approval of the paper, highlighting its strengths and potential applications. They suggest acceptance and see future potential, although they do note a weakness in the limited experiments. The politeness score is 80 (quite polite) as the reviewer uses respectful language throughout, acknowledging both strengths and weaknesses in a constructive manner. They use phrases like 'I can see', 'it would be more interesting', and 'I suggest' which maintain a polite and professional tone. The reviewer also balances critique with praise, showing consideration for the authors' work.",70,80
Trusting SVM for Piecewise Linear CNNs,Accept,2017,"['Leonard Berrada', 'Andrew Zisserman', 'M. Pawan Kumar']","[5, 4, 6]","['5', 'Ok but not good enough - rejection', 'Marginally above acceptance threshold']","This paper presents a novel layer-wise optimization approach for learning CNN with piecewise linear nonlinearities. The proposed approach trains piecewise linear CNNs layer by layer and reduces the sub-problem into latent structured SVM, which has been well-studied in the literature. In addition, the paper presents improvements of the BCFW algorithm used in the inner procedure. Overall, this paper is interesting. However, unfortunately, the experiment is not convincing. Pros: - To my best knowledge, the proposed approach is novel, and the authors provide nice theoretical analysis. - The paper is well-written and easy to follow. Cons: - Although the proposed approach can be applied in general structured prediction problem, the experiments only conduct on a simple multi-class classification task. This makes this work less compelling. - The test accuracy performance on CIFAR-10 reported in the paper doesn*t look right. The accuracy of the best model reported in this paper is 70.2% while existing work often reports 90+%. For example, https://arxiv.org/pdf/1412.6806.pdf showed an accuracy of 91% without data augmentation. Also, CIFAR-10 is a relatively small dataset, Other comments: - If I understand correctly, BCFW only guarantees monotonically increasing in the dual objective and does not have guarantees on the primal objective. Especially, in practice, the inner optimization process often stops pretty early (i.e., stops when duality gap is still large). Therefore, when putting them together, the CCCP procedure may not monotonically decrease as the inner procedure is only solved approximately. The authors should add this note when they discuss the properties of their algorithm.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[2, 33, 16]","[8, 39, 22]","[15, 746, 118]","[5, 455, 57]","[9, 188, 43]","[1, 103, 18]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('interesting', 'novel', 'well-written'), they express significant concerns about the experimental results ('not convincing', 'doesn't look right'). The overall tone suggests the paper needs substantial improvements. The politeness score is moderately positive (60) as the reviewer uses polite language throughout, acknowledging both pros and cons, and phrases criticisms constructively ('Unfortunately...', 'If I understand correctly...'). They maintain a professional tone even when pointing out issues, avoiding harsh or rude language.",-20,60
Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling,Accept,2017,"['Hakan Inan', 'Khashayar Khosravi', 'Richard Socher']","[6, 7, 8]","['Marginally above acceptance threshold', 'Good paper, accept', 'Top 50% of accepted papers, clear accept']","This work offers a theoretical justification for reusing the input word embedding in the output projection layer. It does by proposing an additional loss that is designed to minimize the distance between the predictive distribution and an estimate of the true data distribution. This is a nice setup since it can effectively smooth over the labels given as input. However, the construction of the estimate of the true data distribution seems engineered to provide the weight tying justification in Eqs. 3.6 and 3.7. It is not obvious why the projection matrix L in Eq 3.6 (let*s rename it to L*) should be the same as that in Eq. 2.1. For example, L* could be obtained through word2vec embeddings trained on a large dataset or it could be learned as an additional set of parameters. In the case that L* is a new learned matrix, it seems the result in Eq 4.5 is to use an independent matrix for the output projection layer, as is usually done. The experimental results are good and provide support for the approximate derivation done in section 4, particularly the distance plots in figure 1. Minor comments: Third line in abstract: where model -> where the model Second line in section 7: into space -> into the space Shouldn*t the RHS in Eq 3.5 be sum 	ilde{y_{t,i}}(frac{hat{y}_t}{	ilde{y_{t,i}}} - e_i) ?","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[4, 4, 11]","[9, 9, 17]","[10, 14, 229]","[4, 5, 111]","[5, 6, 111]","[1, 3, 7]","The sentiment score is 50 (slightly positive) because the reviewer acknowledges the work as offering a 'nice setup' and providing 'good' experimental results that support the derivation. However, they also raise questions about certain aspects of the methodology, indicating a balanced view. The politeness score is 70 (fairly polite) as the reviewer uses respectful language throughout, offering constructive criticism and suggestions without harsh or dismissive statements. They use phrases like 'This is a nice setup' and provide specific, helpful feedback, including minor comments for improvement. The tone is professional and courteous throughout.",50,70
Understanding Trainable Sparse Coding with Matrix Factorization,Accept,2017,"['Thomas Moreau', 'Joan Bruna']","[6, 8]","['Marginally above acceptance threshold', 'Top 50% of accepted papers, clear accept']","This paper performs theoretical analysis to understand how sparse coding could be accelerated by neural networks. The neural networks are generated by unfolding the ISTA/FISTA iterations. Based on the results, the authors proposed a reparametrization approach for the neural network architecture to enforce the factorization property and recovered the original gain of LISTA, which justified the theoretical analysis. My comments are listed below. It is not clear about the purpose of Section 2.3.2. Adapting the factorization to the input distribution based on (15) would be time consuming because the overhead of solving (15) may not save the total time. In fact, the approach does not use (15) but back propagation to learn the factorization parameters. Minor comments: - E(z_k) in (3) and (4) are not defined. - E_x in (19) is not defined. - Forward referencing (“Equation (20) defines…”) in the paragraph above Theorem 2.2. needs to be corrected.","['3', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']",['skipped'],['skipped'],"[48, 162]","[19, 64]","[22, 88]","[7, 10]","The sentiment score is slightly positive (20) because the reviewer acknowledges the paper's theoretical analysis and proposed approach, which 'justified the theoretical analysis'. However, they also raise some concerns and point out minor issues, which prevents a higher positive score. The politeness score is moderately positive (50) as the reviewer uses neutral language and presents their comments as suggestions rather than harsh criticisms. They use phrases like 'It is not clear' and 'needs to be corrected' instead of more confrontational language. The reviewer also balances positive aspects with constructive criticism, maintaining a professional and respectful tone throughout.",20,50
Unrolled Generative Adversarial Networks,Accept,2017,"['Luke Metz', 'Ben Poole', 'David Pfau', 'Jascha Sohl-Dickstein']","[7, 7, 9]","['Good paper, accept', 'Good paper, accept', '9']","This work introduces a novel method for training GANs by displacing simultaneous SGD, and unrolling the inner optimization in the minmax game as a computational graph. The paper is very clearly written, and explains the justification very well. The problem being attacked is very significant and important. The approach is novel, however, similar ideas have been tried to solve problems unrelated to GANs. The first quantitative experiment is section 3.3.1, where the authors attempt to find the best z which can generate training examples. This is done by using L-BFGS on |G(z) - x|. The claim is that if we*re able to find such a z, then the generator can generate this particular training example. It*s demonstrated that 0-step GANs are not able to generate many training examples, while unrolled GANs do. However, I find this experiment unreasonable. Being able to find a certain z, which generates a certain sample does not guarantee that this particular mode is high probability. In fact, an identity function can potentially beat all the GAN models in the proposed metric. And due to Cantor*s proof of equivalence between all powers of real spaces, this applies to smaller dimension of z as well. More realistically, it should be possible to generate *any* image from a generator by finding a very specific z. That a certain z exists which can generate a sample does not prove that the generator is not missing modes. It just proves that the generator is similar enough to an identity function to be able to generate any possible image. This metric is thus measuring something potentially tangential to diversity or mode-dropping. Another problem with this metric is that that showing that the optimization is not able to find a z for a specific training examples does not prove that such a z does not exist, only that it*s harder to find. So, this comparison might just be showing that unrolled GANs have a smoother function than 0-step GANs, and thus easier to optimize for z. The second quantitative experiment considers mean pairwise distance between generated samples, and between data samples. The first number is likely to be small in the case of a mode-dropping GAN. The authors argue that the two numbers being closer to each other is an indication of the generated samples being as diverse as the data. Once again, this metric is not convincing. 1. The distances are being measured in pixel-space. 2. A GAN model could be generating garbage, and yet still perform very well in this metric. There are no other quantitative results in the paper. Even though the method is optimizing diversity, for a sanity check, scores for quality such as Inception scores or SSL performance would have been useful. Another metric that the authors can consider is training GAN using this approach on the tri-MNIST dataset (concatenation of 3 MNIST digits), which results in 1000 easily-identifiable modes. Then, demonstrate that the GAN is able to generate all the 1000 modes with equal probability. This is not a perfect metric either, but arguably much better than the metrics in this paper. This metric is used in this ICLR submission: https://openreview.net/pdf?id=HJKkY35le Whether this paper is accepted or not, I encourage the authors to investigate this approach further, since the method is promising and interesting. # Post-rebuttal review The authors have incorporated changed in the paper by adding more experiments. These experiments now demonstrate the claims of the paper better. The paper was already well-written and introduced a novel idea and addressed an important problem. The only thing holding this paper back was unconvincing experiments, which now has been corrected. Thus, I would increase my score by 2 points, and recommend accepting the paper.","['5', '5', '5']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[2, 7, 8, 9]","[8, 13, 14, 15]","[48, 62, 27, 130]","[19, 24, 11, 52]","[29, 37, 14, 74]","[0, 1, 2, 4]","The sentiment score is 50 (slightly positive) because the reviewer acknowledges the paper's clear writing, novel approach, and importance of the problem being addressed. However, they express significant concerns about the experimental methods and metrics used. The reviewer encourages further investigation, which indicates a generally positive outlook despite the criticisms. The politeness score is 80 (quite polite) because the reviewer uses respectful language throughout, offers constructive criticism, and provides specific suggestions for improvement. They acknowledge the paper's strengths before delving into concerns, and use phrases like 'I encourage the authors' which maintain a supportive tone. The post-rebuttal review also shows a willingness to reconsider their evaluation based on the authors' responses, which is indicative of a polite and fair review process.",50,80
Unsupervised Cross-Domain Image Generation,Accept,2017,"['Yaniv Taigman', 'Adam Polyak', 'Lior Wolf']","[7, 6, 7]","['Good paper, accept', 'Marginally above acceptance threshold', 'Good paper, accept']","This paper presents an unsupervised image transformation method that maps a sample from source domain to target domain. The major contribution lies in that it does not require aligned training pairs from two domains. The model is based on GANs. To make it work in the unsupervised setting, this paper decomposes the generation function into two modules: an encoder that identify a common feature space between two domains and an decoder that generates samples in the target domain. To avoid trivial solutions, this paper proposed two additional losses that penalize 1) the feature difference between a source sample and its transformed sample and 2) the pixel difference between a target sample and its re-generated sample. This paper presents extensive experiments on transferring SVHN digit images to MNIST style and transferring face images to emoji style. +The proposed learning method enables unsupervised domain transfer that could be impactful in broad problem contexts. +This paper presents careful ablation studies to analyze the effects of different components of the system, which is helpful for understanding the paper. +The transferred images are visually impressive and quantitative results also show the image identities are preserved across domains to some degree. -It will be more interesting to show results in other domains such as texts and images. -In addition to the face identities, it is also of great interest to analyze how well the facial attributes are preserved when mapping to target domain.","['4', '3', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[9, 3, 18]","[15, 9, 24]","[42, 41, 416]","[19, 16, 213]","[21, 23, 168]","[2, 2, 35]","The sentiment score is 70 (positive) because the review begins with a neutral description of the paper's content, followed by several positive points indicated by '+' symbols. These highlight the paper's contributions, careful analysis, and impressive results. There are only two minor criticisms at the end, prefaced with '-' symbols, suggesting areas for improvement rather than major flaws. The politeness score is 50 (slightly polite) because the reviewer uses professional and objective language throughout, without any harsh criticism or personal attacks. They acknowledge the paper's strengths and offer constructive suggestions for improvement. The tone is respectful but not overly formal or effusive, maintaining a balanced and academic approach.",70,50
Variable Computation in Recurrent Neural Networks,Accept,2017,"['Yacine Jernite', 'Edouard Grave', 'Armand Joulin', 'Tomas Mikolov']","[4, 7, 7]","['Ok but not good enough - rejection', 'Good paper, accept', 'Good paper, accept']","TLDR: The authors present Variable Computation in Recurrent Neural Networks (VCRNN). VCRNN is similar in nature to Adaptive Computation Time (Graves et al., 2016). Imagine a vanilla RNN, at each timestep only a subset (i.e., *variable computation*) of the state is updated. Experimental results are not convincing, there is limited comparison to other cited work and basic LSTM baseline. === Gating Mechanism === At each timestep, VCRNN generates a m_t vector which can be seen as a gating mechanism. Based off this m_t vector, a D-first (D-first as in literally the first D RNN states) subset of the vanilla RNN state is gated to be updated or not. Extra hyperparams epsilon and _x0008_ar{m} are needed -- authors did not give us a value or explain how this was selected or how sensitive and critical these hyperparms are. This mechanism while novel, feels a bit clunky and awkward. It does not feel well principled that only the D-first states get updated, rather than a generalized solution where any subset of the state can be updated. A short section in the text comparing to the soft-gating mechanisms of GRUs/LSTMs/Multiplicative RNNs (Wu et al., 2016) would be nice as well. === Variable Computation === One of the arguments made is that their VCRNN model can save computation versus vanilla RNNs. While this may be technically true, in practice this is probably not the case. The size of the RNNs they compare to do not saturate any modern GPU cores. In theory computation might be saved, but in practice there will probably be no difference in wallclock time. The authors also did not report any wallclock numbers, which makes this argument hard to sell. === Evaluation === This reviewer wished there was more citations to other work for comparison and a stronger baseline (than just a vanilla RNN). First, LSTMs are very simple and quite standard nowadays -- there is a lack of comparison to any basic stacked LSTM architecture in all the experiments. The PTB BPC numbers are quite discouraging as well (compared to state-of-the-art). The VCRNN does not beat the basic vanilla RNN baseline. The authors also only cite/compare to a basic RNN architecture, however there has been many contributions since a basic RNN architecture that performs vastly better. Please see Chung et al., 2016 Table 1. Chung et al., 2016 also experimented w/ PTB BPC and they cite and compare to a large number of other (important) contributions. One cool experiment the authors did is graph the per-character computation of VCRNN (i.e., see Figure 2). It shows after a space/word boundary, we use more computation! Cool! However, this makes me wonder what a GRU/LSTM does as well? What is the magnitude of the of the change in the state vector after a space in GRU/LSTM -- I suspect them to do something similar. === Minor === * Please add Equations numbers to the paper, hard to refer to in a review and discussion! References Chung et al., *Hierarchical Multiscale Recurrent Neural Networks,* in 2016. Graves et al., *Adaptive Computation Time for Recurrent Neural Networks,* in 2016. Wu et al., *On Multiplicative Integration with Recurrent Neural Networks,* in 2016.","['5', '4', '4']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[5, 7, 8, 10]","[11, 13, 14, 15]","[49, 93, 130, 83]","[14, 36, 59, 46]","[33, 52, 67, 33]","[2, 5, 4, 4]","The sentiment score is -50 because the review is generally critical of the paper, pointing out several limitations and shortcomings. The reviewer states that 'Experimental results are not convincing' and that the proposed model 'does not beat the basic vanilla RNN baseline.' However, it's not entirely negative as the reviewer acknowledges some positive aspects, such as the 'cool experiment' showing per-character computation. The politeness score is 20 because while the reviewer is critical, they maintain a professional and constructive tone throughout. They use phrases like 'This reviewer wished' and 'Please add' which are polite ways of making suggestions. The reviewer also acknowledges positive aspects and provides specific recommendations for improvement, which is a polite approach to criticism. However, some phrases like 'feels a bit clunky and awkward' are slightly less polite, preventing a higher score.",-50,20
Variational Lossy Autoencoder,Accept,2017,"['Xi Chen', 'Diederik P. Kingma', 'Tim Salimans', 'Yan Duan', 'Prafulla Dhariwal', 'John Schulman', 'Ilya Sutskever', 'Pieter Abbeel']","[9, 7, 7, 6]","['9', 'Good paper, accept', 'Good paper, accept', 'Marginally above acceptance threshold']","The AR prior and its equivalent - the inverse AR posterior - is one of the more elegant ways to improve the unfortunately poor generative qualities of VAE-s. It is only an incremental but important step. Incremental, because, judging by the lack of, say, CIFAR10 pictures of the VLAE in its *creative* regime ( i.e., when sampling from prior), it will not answer many of the questions hanging over. We hope to see the paper accepted: in relative terms, the paper shines in the landscape of the other papers which are rich on engineering hacks but lacking on theoretical insights. Some disagreements with the theoretical suppositions in the paper: i) The VAE-s posterior converges to the prior faster than we would like because the gradients of the *generative* error (the KL divergence of prior and posterior) w.r.t. mu & sigma are simple, inf differentiable functions and their magnitude far exceeds the magnitude of the resp. gradients of the reconstruction error. Especially when more *hairy* decoders like pixelCNN are used. We always considered this obvious and certainly not worthy of one page of CS mumbo-jumbo to explain. Dumbing-down the decoder via variations of dropout or *complexifying* the sampler as in here, or slapping the generative error with a *DeepMind* constant (beta_VAE), are the natural band-aids, but seem to fail in the *creative* regime, for real-life sets like CIFAR10 or more complex ones. Other conceptual solutions are needed, some are discussed in [2]. ii) The claim near the end of section 2.2 that *the extra coding cost a.k.a. variational error will exist and will not be negligible* is a speculation, which, in our empirical experience at least, is incorrect. The variational error is quantifiable for the Gibbs/exponential family of priors/posteriors, as described in [1], section 3.8, and as Tim Salimans knows from his own previous work. In the case of CIFAR10 for example, the variational error is negligible, even for simple sampling families like Gaussian, Laplacian, etc. Moreover, in hindsight, using the closed-form for generative error (the KL divergence of prior and posterior) in the pioneering VAE papers, was likely a mistake inherited by the unnecessary Bayseanism which inspired them (beautiful but a mistake nonetheless): The combo of generative and variational error should together be approximated by the same naive Monte Carlo used for the reconstruction error (easily follows from equation (3.13) in [1]) i.e. arithmetic average over observations. On the lighter side, guys, please do not recycle ridiculous terms like *optimizationally challenged*, as in section 2.2! The English language has recently acquired *mentally-challenged*, *emotionally-challenged*, etc, and now political correctness has sadly found its way to machines? [1] https://arxiv.org/pdf/1508.06585v5.pdf [2] https://arxiv.org/pdf/1511.02841v3.pdf","['4', '4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[3, 8, 6, 10, 2, 7, 11, 16]","[8, 14, 12, 15, 8, 13, 17, 22]","[47, 46, 60, 52, 22, 66, 99, 610]","[21, 21, 21, 28, 8, 27, 49, 291]","[24, 24, 36, 19, 14, 37, 45, 293]","[2, 1, 3, 5, 0, 2, 5, 26]","The sentiment score is 50 (slightly positive) because the reviewer expresses hope for the paper's acceptance and praises it for providing theoretical insights, which is relatively rare. However, they also point out limitations and disagree with some theoretical suppositions, balancing the positive aspects. The politeness score is 20 (slightly polite) because the reviewer uses generally respectful language and acknowledges the paper's merits. However, there are instances of direct criticism and some informal language (e.g., 'mumbo-jumbo', 'band-aids') that prevent a higher politeness score. The reviewer also advises against using certain terms, which is constructive but could be perceived as slightly critical.",50,20
Variational Recurrent Adversarial Deep Domain Adaptation,Accept,2017,"['Sanjay Purushotham', 'Wilka Carvalho', 'Tanachat Nilanon', 'Yan Liu']","[5, 6, 6]","['5', 'Marginally above acceptance threshold', 'Marginally above acceptance threshold']","The work combines variational recurrent neural networks, and adversarial neural networks to handle domain adaptation for time series data. The proposed method, along with several competing algorithms are compared on two healthcare datasets constructed from MIMIC-III in domain adaptation settings. The new contribution of the work is relatively small. It extends VRNN with adversarial training for learning domain agnostic representations. From the experimental results, the proposed method clearly out-performs competing algorithms. However, it is not clear where the advantage is coming from. The only difference between the proposed method and R-DANN is using variational RNN vs RNN. Little insights were provided on how this could bring such a big difference in terms of performance and the drastic difference in the temporal dependencies captured by these two methods in Figure 4. Detailed comments: 1. Please provide more details on what is plotted in Figure 1. Is 1 (b) is the t-sne projection of representations learned by DANN or R-DANN? The text in section 4.4 suggests it’s the later case. It is surprising to see such a regular plot for VRADA. What do you think are the two dominant latent factors encoded in figure 1 (c)? 2. In Table 2, the two baselines have quite significant difference in performance testing on the entire target (including validation set) vs on the test set only. VRADA, on the other hand, performs almost identical in these two settings. Could you please offer some explanation on this? 3. Please explain figure 3 and 4 in more details. how to interpret the x-axis of figure 3, and the x and y axes of figure 4. Again the right two plots in figure 4 are extremely regular comparing to the ones on the left.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[9, 1, 2, 16]","[15, 7, 2, 22]","[43, 9, 2, 219]","[25, 4, 2, 132]","[14, 5, 0, 57]","[4, 0, 0, 30]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects of the work (e.g., 'clearly out-performs competing algorithms'), they also express concerns about the small contribution and lack of clarity in explaining the advantages of the proposed method. The overall tone suggests that the reviewer is not fully convinced by the work.

The politeness score is moderately positive (50) as the reviewer maintains a professional and respectful tone throughout. They use neutral language to express their concerns and provide constructive feedback. The reviewer asks questions and requests clarifications rather than making harsh criticisms.

The reasoning for these scores is based on the balance of positive and negative comments, the professional tone, and the constructive nature of the feedback. The reviewer's language is not overly enthusiastic or praising, but it's also not rude or dismissive, leading to a slightly negative sentiment score but a positive politeness score.",-20,50
Visualizing Deep Neural Network Decisions: Prediction Difference Analysis,Accept,2017,"['Luisa M Zintgraf', 'Taco S Cohen', 'Tameem Adel', 'Max Welling']","[6, 9, 6]","['Marginally above acceptance threshold', '9', 'Marginally above acceptance threshold']","The paper presents a theoretically well motivated for visualizing what parts of the input feature map are responsible for the output decision. The key insight is that features that maximally change the output and are simultaneously more unpredictable from other features are the most important ones. Most previous work has focused on finding features that maximally change the output without accounting for their predictability from other features. Authors build upon ideas presented in the work of Robnik-Šikonja & Kononenko (2008). The results indicate that the proposed visualization mechanism based on modeling conditional distribution identifies more salient regions as compared to a mechanism based on modeling marginal distribution. I like that authors have presented visualization results for a single image across multiple networks and multiple classes. There results show that the proposed method indeed picks up on class-discriminative features. Authors have provided a link to visualizations for a random sample of images in a comment – I encourage the authors to include this in the appendix of the paper. My one concern with the paper is – Zeiler et al., proposed a visualization method by greying small square regions in the image. This is similar to computing the visualization using the marginal distribution. Authors compute the marginal visualization using 10 samples, however in the limit of infinite samples the image region would be gray. The conditional distribution is computed using a normal distribution that provides some regularization and therefore estimating the conditional and marginal distributions using 10 samples each is not justified. I would like to see the comparison when grey image patches (akin to Zeiler et al.) are used for visualization against the approach based on the conditional distribution.","['4', '4', '5']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[2, 4, 8, 18]","[8, 10, 12, 24]","[36, 86, 25, 391]","[15, 32, 18, 190]","[18, 49, 7, 167]","[3, 5, 0, 34]","The sentiment score is 60 (moderately positive) because the reviewer expresses appreciation for the paper's theoretical motivation, key insights, and results. They use phrases like 'I like that authors have presented...' and note that the results show the method 'indeed picks up on class-discriminative features'. However, they also express a concern about the comparison methodology, which prevents a higher positive score. The politeness score is 70 (fairly polite) because the reviewer uses respectful language throughout, offers constructive feedback, and encourages the authors to include additional information. They phrase their concern as 'My one concern...' rather than using more critical language. The reviewer also uses phrases like 'I encourage the authors...' and 'I would like to see...', which are polite ways of making suggestions.",60,70
What does it take to generate natural textures?,Accept,2017,"['Ivan Ustyuzhaninov *', 'Wieland Brendel *', 'Leon Gatys', 'Matthias Bethge']","[7, 8, 8]","['Good paper, accept', 'Top 50% of accepted papers, clear accept', 'Top 50% of accepted papers, clear accept']","This work proposed a simple but strong baseline for parametric texture synthesis. In empirical experiments, samples generated by the baseline composed by multi-scale and random filters sometime rival the VGG-based model which has multi-layer and pre-trained filters. The authors concluded that texture synthesis does not necessarily depend on deep hierarchical representations or the learned feature maps. This work is indeed interesting and insightful. However, the conclusions are needed to be further testified (especially for deep hierarchical representations). Firstly, all of generated samples by both VGG and single layer model are not perfect and much worse than the results from non-parametric methods. Besides VGG-based model seems to do better in inpainting task in Figure 7. Last but not least, would a hierarchical model (instead of lots of filters with different size) handle multi-scale more efficiently?","['4', '3', '5']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[2, 7, 3, 19]","[7, 13, 7, 25]","[15, 63, 19, 142]","[6, 22, 8, 52]","[9, 34, 10, 58]","[0, 7, 1, 32]","The sentiment score is 50 (slightly positive) because the reviewer acknowledges the work as 'interesting and insightful' and praises the baseline's performance. However, they also express concerns about the conclusions and suggest further investigation, balancing the positive aspects with constructive criticism. The politeness score is 75 (quite polite) as the reviewer uses respectful language throughout, acknowledging the work's merits before presenting their concerns. They phrase their criticisms as suggestions ('needed to be further testified') and questions ('would a hierarchical model...?') rather than direct criticisms, maintaining a courteous tone.",50,75
Why Deep Neural Networks for Function Approximation?,Accept,2017,"['Shiyu Liang', 'R. Srikant']","[7, 7, 7]","['Good paper, accept', 'Good paper, accept', 'Good paper, accept']","SUMMARY This paper contributes to the description and comparison of the representational power of deep vs shallow neural networks with ReLU and threshold units. The main contribution of the paper is to show that approximating a strongly convex differentiable function is possible with much less units when using a network with one more hidden layer. PROS The paper presents an interesting combination of tools and arrives at a nice result on the exponential superiority of depth. CONS The main result appears to address only strongly convex univariate functions. SPECIFIC COMMENTS - Thanks for the comments on L. Still it would be a good idea to clarify this point as far as possible in the main part. Also, I would suggest to advertise the main result more prominently. I still have not read the revision and maybe you have already addressed some of these points there. - The problem statement is close to that from [Montufar, Pascanu, Cho, Bengio NIPS 2014], which specifically arrives at exponential gaps between deep and shallow ReLU networks, albeit from a different angle. I would suggest to include that paper it in the overview. - In Lemma 3, there is an i that should be x - In Theorem 4, ``	ilde f** is missing the (x). - Theorem 11, the lower bound always increases with L ? - In Theorem 11, _x0008_f xin [0,1]^d?","['4', '4', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[4, 28]","[10, 34]","[20, 378]","[9, 165]","[7, 75]","[4, 138]","The sentiment score is 50 (slightly positive) because the reviewer acknowledges the paper's interesting contribution and 'nice result' in the pros section, but also points out some cons and areas for improvement. This indicates a balanced but generally favorable view. The politeness score is 70 (fairly polite) because the reviewer uses respectful language throughout, such as 'Thanks for the comments' and 'I would suggest,' which shows consideration for the authors. The reviewer also acknowledges potential updates in a revision they haven't read yet, showing fairness. The specific comments are presented as constructive suggestions rather than harsh criticisms, further contributing to the polite tone.",50,70
Words or Characters? Fine-grained Gating for Reading Comprehension,Accept,2017,"['Zhilin Yang', 'Bhuwan Dhingra', 'Ye Yuan', 'Junjie Hu', 'William W. Cohen', 'Ruslan Salakhutdinov']","[7, 6, 7]","['Good paper, accept', 'Marginally above acceptance threshold', 'Good paper, accept']","This paper proposes a new gating mechanism to combine word and character representations. The proposed model sets a new state-of-the-art on the CBT dataset; the new gating mechanism also improves over scalar gates without linguistic features on SQuAD and a twitter classification task. Intuitively, the vector-based gate working better than the scalar gate is unsurprising, as it is more similar to LSTM and GRU gates. The real contribution of the paper for me is that using features such as POS tags and NER help learn better gates. The visualization in Figure 3 and examples in Table 4 effectively confirm the utility of these features, very nice! In sum, while the proposed gate is nothing technically groundbreaking, the paper presents a very focused contribution that I think will be useful to the NLP community. Thus, I hope it is accepted.","['4', '4', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[15, 2, 14, 6, 33, 15]","[21, 8, 20, 12, 39, 21]","[82, 71, 245, 63, 345, 419]","[34, 30, 99, 31, 218, 207]","[35, 39, 50, 30, 80, 201]","[13, 2, 96, 2, 47, 11]","The sentiment score is 80 (positive) because the reviewer expresses a favorable view of the paper, highlighting its contributions and stating 'I hope it is accepted.' They praise specific aspects like the visualization and examples, and note that the work will be 'useful to the NLP community.' The politeness score is 70 (polite) due to the reviewer's constructive and respectful tone. They use phrases like 'very nice!' and acknowledge the paper's merits without harsh criticism. The reviewer balances positive feedback with honest assessment, maintaining a professional and courteous demeanor throughout.",80,70
Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations,Accept,2017,"['David Krueger', 'Tegan Maharaj', 'Janos Kramar', 'Mohammad Pezeshki', 'Nicolas Ballas', 'Nan Rosemary Ke', 'Anirudh  Goyal', 'Yoshua Bengio', 'Aaron Courville', 'Christopher Pal']","[7, 8, 8]","['Good paper, accept', 'Top 50% of accepted papers, clear accept', 'Top 50% of accepted papers, clear accept']","The authors propose a conceptually simple method for regularisation of recurrent neural networks. The idea is related to dropout, but instead of zeroing out units, they are instead set to their respective values at the preceding time step element-wise with a certain probability. Overall, the paper is well written. The method is clearly represented up to issues raised by reviewers during the pre-review question phase. The related work is complete and probably the best currently available on the matter of regularising RNNs. The experimental section focuses on comparing the method with the current SOTA on a set of NLP benchmarks and a synthetic problem. All of the experiments focus on sequences over discrete values. An additional experiment also shows that the sequential Jacobian is far higher for long-term dependencies than in the dropout case. Overall, the paper bears great potential. However, I do see some points. 1) As raised during the pre-review questions, I would like to see the results of experiments that feature a complete hyper parameter search. I.e. a proper model selection process,as it should be standard in the community. I do not see why this was not done, especially as the author count seems to indicate that the necessary resources are available. I want to repeat at this point that Table 2 of the paper shows that validation error is not a reliable estimator for testing error in the respective data set. Thus, overfitting the model selection process is a serious concern here. Zoneout does not seem to improve that much in the other tasks. 2) Zoneout is not investigated well mathematically. E.g. an analysis of the of the form of gradients from unit K at time step T to unit K’ at time step T-R would have been interesting, especially as these are not necessarily non-zero for dropout. Also, the question whether zoneout has a variational interpretation in the spirit of Yarin Gal’s work is an obvious one. I can see that it is if we treat zoneout in a resnet framework and dropout on the incremental parts. Overall, little effort is done answering the question *why* zoneout works well, even though the literature bears plenty of starting points for such analysis. 3) The data sets used are only symbolic. It would have been great if more ground was covered, i.e. continuous data such as from dynamical systems. To me it is not obvious whether it will transfer right away. An extreme amount of “tricks” is being published currently for improved RNN training. How does zoneout stand out? It is a nice idea, and simple to implement. However, the paper under delivers: the experiments do not convince me (see 1) and 3)). There authors do not provide convincing theoretical insights either. (2) Consequently, the paper reduces to a “epsilon improvement, great text, mediocre experimental evaluation, little theoretical insight”.","['4', '4', '5']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[3, 2, 8, 5, 6, 7, 4, 30, 17, 21]","[9, 8, 14, 10, 12, 13, 10, 36, 23, 27]","[39, 22, 21, 19, 75, 61, 102, 977, 309, 231]","[11, 6, 10, 8, 39, 22, 40, 405, 135, 98]","[28, 15, 11, 11, 35, 38, 61, 456, 160, 110]","[0, 1, 0, 0, 1, 1, 1, 116, 14, 23]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the paper's potential and praises aspects like the writing and related work section, they express significant concerns about the experimental methodology, lack of theoretical analysis, and limited scope of datasets. The reviewer concludes that the paper 'under delivers' and reduces to a 'mediocre experimental evaluation' with 'little theoretical insight'. The politeness score is moderately positive (50) as the reviewer maintains a professional tone throughout, balancing criticism with praise where appropriate. They use phrases like 'I would like to see' and 'It would have been great if' to soften their critiques, and acknowledge the paper's strengths before detailing its weaknesses. The language is constructive rather than harsh, even when pointing out significant shortcomings.",-20,50
beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework,Accept,2017,"['Irina Higgins', 'Loic Matthey', 'Arka Pal', 'Christopher Burgess', 'Xavier Glorot', 'Matthew Botvinick', 'Shakir Mohamed', 'Alexander Lerchner']","[7, 5, 7, 6]","['Good paper, accept', '5', 'Good paper, accept', 'Marginally above acceptance threshold']","The paper is attacking a classical and very hard problem: non-linear PCA i.e. learning the principal components a.k.a independent factors, a.k.a. orthogonal geodesics in the highly non-linear latent manifold of a given data-set. Moreover, the paper is hoping for these factors to be humanly-interpretable. The problem is so hard that is likely unsolvable in unsupervised fashion for real-life datasets. After all, even we humans are able to zero-in the the type of, say, legs of chairs (Fig 3 in the paper), only after we have identified the object as a chair and have rotated and translated it. The importance of this problem is hard to overstate, so we hope the paper is accepted, on the grounds of asking so fundamental a question. The paper correctly points out the inability of VAE to disentangle important factors even on toy data-sets. This of course has been known for awhile, e.g., Fig 4. on reference [1], from almost 2 years ago. On the back of so much hope and expectation built-up in the introduction, the solution put forward in the paper strikes us as a curious but a hardly useful toy. Slapping a large multiplicative factor on the generative error term of a VAE is not going to work for any real-life datasets. Generation without reconstruction leads to schizophrenia, as every respectable psychiatrist will testify. Physicists have attacked this problem considerably earlier than DeepMind and their scalable and conceptual solutions have been discussed at length in references [1], section 1.6, section 4, and most of reference [2]. In short, for spatial, color, time symmetries, symmetry statistics are produced by smaller specialized nets like spatial transformers and used to augment the latent variables, to aid the decoder. As demonstrated in reference [2], Figures 3,4, this is indispensable in order to handle distortion, e.g., spatial transformation. Note that this approach is completely unsupervised. This of course does not handle complex factors like *type of legs* but that is a hell of an ambitious goal, and while we hope to be proven wrong, probably way beyond reach of machines and even many humans (for real-life data-sets that is!) By complete luck, the authors may have hit upon something else of fundamental importance: the letter *beta* for their multiplicative coefficient is of course reserved in statistical physics for the inverse of the temperature. This requires the separation of generative *energy* and *entropy* and is partially addressed in section 2.9 of reference [1]. The correct definition of *generative temperature* is not published yet, but used extensively in experiments and can be privately communicated upon request. When worked out correctly, the beta does not multiply the generative error, as in this paper, so it would be very interesting to repeat the toy experiments here with the *correct* physical model instead. A question to the authors: creating a new metric (*disentanglement*) as u do is commendable but after hundred years of PCA, ICA, etc , is there no some proxy that can be used instead? Also, could you not for example simply distort the dataset along different factors and then look for the quality of the reconstructed images (as in the Introduction of reference [2])? [1] https://arxiv.org/pdf/1508.06585v5.pdf [2] https://arxiv.org/pdf/1511.02841v3.pdf","['5', '4', '4', '4']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[6, 10, 2, 1, 8, 17, 12, 15]","[11, 14, 3, 5, 13, 23, 17, 20]","[36, 28, 7, 22, 24, 110, 66, 30]","[11, 12, 3, 9, 14, 40, 27, 11]","[20, 15, 4, 13, 9, 54, 33, 16]","[5, 1, 0, 0, 1, 16, 6, 3]","The sentiment score is -30 because while the reviewer acknowledges the importance of the problem and hopes for the paper's acceptance, they express significant skepticism about the proposed solution, calling it a 'curious but hardly useful toy'. They also point out limitations and suggest that the authors' approach is unlikely to work for real-life datasets. The politeness score is 20 because the reviewer uses generally respectful language and offers constructive criticism. They acknowledge the difficulty of the problem and the paper's contributions, but also firmly point out perceived shortcomings. The reviewer offers alternative approaches and references, which is helpful, but the overall tone suggests more criticism than praise.",-30,20
A Context-aware Attention Network for Interactive Question Answering,Reject,2017,"['Huayu Li', 'Martin Renqiang Min', 'Yong Ge', 'Asim Kadav']","[5, 4, 4]","['5', 'Ok but not good enough - rejection', 'Ok but not good enough - rejection']","1. the QA model is not novel, very similar to the existing model. 2. The IQA model is very confusing. If it needs human interactive in the training process, how could it be practical to ask human to join the training in each iteration? It sounds impractical. If the human interactive questions are predefined, then it is not interactive at all, since it is not based on the current state of model output.","['3', '4', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[10, 11, 17, 10]","[16, 17, 23, 16]","[59, 79, 232, 48]","[31, 37, 104, 23]","[13, 32, 15, 21]","[15, 10, 113, 4]","The sentiment score is -80 because the review is highly critical of the work, stating that the QA model is not novel and the IQA model is confusing and impractical. There are no positive comments, indicating a strongly negative sentiment. The politeness score is -20 because while the language is not overtly rude, it is quite blunt and dismissive. The reviewer uses phrases like 'not novel, very similar to existing model' and 'very confusing', which are direct criticisms without any softening language or acknowledgment of potential merits. The tone is more matter-of-fact than polite, but stops short of being explicitly impolite.",-80,-20
A Convolutional Encoder Model for Neural Machine Translation,Reject,2017,"['Jonas Gehring', 'Michael Auli', 'David Grangier', 'Yann N. Dauphin']","[7, 6, 6]","['Good paper, accept', 'Marginally above acceptance threshold', 'Marginally above acceptance threshold']","The system described works comparably to bi-directional LSTM baseline for NMT, and CNN*s are naturally parallelizable. Key ideas include the use of two stacked CNN*s (one for each of encoding and decoding) for translation, with res connections and position embeddings. The use of CNN*s for translation has been attempted previously (as described by the authors), but presumably it is the authors* combination of various architectural choices (attention, position embeddings, etc) that make the present system competitive with RNN*s, whereas earlier attempts were not. They describe system*s sensitivity to some of these choices (e.g. experiments to choose appropriate number of layers in each of the CNN*s). The experimental results are well reported in detail. One or two figures would definitely be required to help clarify the architecture. This paper is less about new ways of learning representations than about the combination of choices made (over the set of existing techniques) in order to get the good results that they do on the reported NMT tasks. In this respect, while I am fairly confident that the paper represents good work in machine learning, I am not quite as confident about its fit for this particular conference.","['3', '5', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[5, 9, 13, 7]","[11, 15, 18, 13]","[23, 137, 94, 67]","[15, 68, 44, 32]","[7, 67, 41, 32]","[1, 2, 9, 3]","The sentiment score is 50 (slightly positive) because the reviewer acknowledges the system's comparable performance to LSTM baselines and highlights its advantages, such as parallelizability. They also praise the detailed experimental results. However, they express some uncertainty about the paper's fit for the specific conference, which prevents a higher score. The politeness score is 75 (quite polite) as the reviewer uses respectful language throughout, acknowledging the authors' work and contributions. They provide constructive feedback and express their opinions diplomatically, using phrases like 'I am fairly confident' and 'I am not quite as confident' instead of more direct criticism. The reviewer also suggests improvements (e.g., adding figures) in a helpful manner.",50,75
A Deep Learning Approach for Joint Video Frame and Reward Prediction in Atari Games,Reject,2017,"['Felix Leibfried', 'Nate Kushman', 'Katja Hofmann']","[4, 4, 4]","['Ok but not good enough - rejection', 'Ok but not good enough - rejection', 'Ok but not good enough - rejection']","This paper introduces an additional reward-predicting head to an existing NN architecture for video frame prediction. In Atari game playing scenarios, the authors show that this model can successfully predict both reward and next frames. Pros: - Paper is well written and easy to follow. - Model is clear to understand. Cons: - The model is incrementally different than the baseline. The authors state that their purpose is to establish a pre-condition, which they achieve. But this makes the paper quite limited in scope. This paper reads like the start of a really good long paper, or a good short paper. Following through on the future work proposed by the authors would make a great paper. As it stands, the paper is a bit thin on new contributions.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[3, 11, 13]","[8, 16, 19]","[23, 33, 146]","[8, 19, 80]","[13, 13, 53]","[2, 1, 13]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('well written', 'easy to follow', 'clear to understand'), they also express significant criticisms. The reviewer states that the paper is 'limited in scope', 'thin on new contributions', and 'reads like the start of a really good long paper'. These criticisms outweigh the positive comments, resulting in a slightly negative overall sentiment. The politeness score is moderately positive (60) because the reviewer uses respectful language throughout, balancing positive and negative feedback. They start with positive points before moving to criticisms, and use phrases like 'the authors state' and 'as it stands' which maintain a professional tone. The reviewer also offers constructive suggestions for improvement, which is a polite way to express criticism.",-20,60
A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks,Reject,2017,"['Kazuma Hashimoto', 'Caiming Xiong', 'Yoshimasa Tsuruoka', 'Richard Socher']","[6, 3, 5]","['Marginally above acceptance threshold', 'Clear rejection', '5']","The authors propose a transfer learning approach applied to a number of NLP tasks; the set of tasks appear to have an order in terms of complexity (from easy syntactic tasks to somewhat harder semantic tasks). Novelty: the way the authors propose to do transfer learning is by plugging models corresponding to each task, in a way that respects the known hierarchy (in terms of NLP *complexity*) of those tasks. In that respect, the overall architecture looks more like a cascaded architecture than a transfer learning one. There are some existing literature in the area (first two Google results found: https://arxiv.org/pdf/1512.04412v1.pdf, (computer vision) and https://www.aclweb.org/anthology/P/P16/P16-1147.pdf (NLP)). In addition to the architecture, the authors propose a regularization technique they call *successive regularization*. Experiments: - The authors performed a number of experimental analysis to clarify what parts of their architecture are important, which is very valuable; - The information *transferred* from one task to the next one is represented both using a smooth label embedding and the hidden representation of the previous task. At this point there is no analysis of which one is actually important, or if they are redundant (update: the authors mentioned they would add something there). Also, it is likely one would have tried first to feed label scores from one task to the next one, instead of using the trick of the label embedding -- it is unclear what the latter is actually bringing. - The successive regularization does not appear to be important in Table 8; a variance analysis would help to conclude.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[15, 9, 17, 11]","[21, 15, 23, 17]","[66, 383, 139, 229]","[31, 165, 81, 111]","[31, 208, 35, 111]","[4, 10, 23, 7]","The sentiment score is slightly positive (20) because the reviewer acknowledges the value of the experimental analysis and the novelty of the approach, but also raises several concerns and points out areas needing clarification or improvement. The overall tone is more constructive than critical. The politeness score is moderately positive (50) as the reviewer uses neutral language and offers suggestions rather than harsh criticisms. They acknowledge the authors' efforts ('which is very valuable') and mention future additions ('the authors mentioned they would add something there'). The reviewer maintains a professional tone throughout, avoiding personal attacks or overly negative language.",20,50
A Neural Knowledge Language Model,Reject,2017,"['Sungjin Ahn', 'Heeyoul Choi', 'Tanel Parnamaa', 'Yoshua Bengio']","[6, 6, 6]","['Marginally above acceptance threshold', 'Marginally above acceptance threshold', 'Marginally above acceptance threshold']","This paper proposes to incorporate knowledge base facts into language modeling, thus at each time step, a word is either generated from the full vocabulary or relevant KB entities. The authors demonstrate the effectiveness on a new generated dataset WikiFacts which aligns Wikipedia articles with Freebase facts. The authors also suggest a modified perplexity metric which penalizes the likelihood of unknown words. At a high level, I do like the motivation of this paper -- named entity words are usually important for downstream tasks, but difficult to learn solely based on statistical co-occurrences. The facts encoded in KB could be a great supply for this. However, I find it difficult to follow the details of the paper (mainly Section 3) and think the paper writing needs to be much improved. - I cannot find where f_{symbkey} / f_{voca} / f_{copy} are defined - w^v, w^s are confusing. - e_k seems to be the average of all previous fact embeddings? It is necessary to make it clear enough. - (h_t, c_t) = f_LSTM(x_{t?1}, h_{t?1}) c_t is not used? - The notion of “fact embeddings” is also not that clear (I understand that they are taken as the concatenation of relation and entity (object) entities in the end). For the anchor / “topic-itself” facts, do you learn the embedding for the special relations and use the entity embeddings from TransE? On generating words from KB entities (fact description), it sounds a bit strange to me to generate a symbol position first. Most entities are multiple words, and it is necessary to keep that order. Also it might be helpful to incorporate some prior information, for example, it is common to only mention “Obama” for the entity “Barack Obama”?","['4', '4', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[25, -1, 15, 10, 14, 8]","[31, 5, 21, 16, 20, 14]","[106, 27, 215, 168, 275, 184]","[55, 6, 129, 82, 162, 73]","[7, 9, 40, 54, 58, 54]","[44, 12, 46, 32, 55, 57]","The sentiment score is slightly negative (-20) because while the reviewer likes the motivation of the paper, they express significant concerns about the clarity and details of the paper, particularly in Section 3. They state that the paper writing 'needs to be much improved' and list several specific issues. The politeness score is mildly positive (30) as the reviewer begins with a positive note about liking the motivation, uses polite language throughout (e.g., 'I find it difficult to follow' rather than more harsh criticism), and phrases their concerns as questions or suggestions rather than direct criticisms. The reviewer also provides specific, constructive feedback, which is a polite approach to peer review.",-20,30
A Neural Stochastic Volatility Model,Reject,2017,"['Rui Luo', 'Xiaojun Xu', 'Weinan Zhang', 'Jun Wang']","[6, 5, 5]","['Marginally above acceptance threshold', '5', '5']","The authors propose a recurrent variational neural network approach to modelling volatility in financial time series. This model consists of an application of Chung et al.’s (2015) VRNN model to volatility forecasting, wherein a Variational Autoencoder (VAE) structure is repeated at each time step of the series. The paper is well written and easy to follow (although this reviewer suggests applying a spelling checking, since the paper contains a number of harmless typos). The paper’s main technical contribution is to stack two levels of recurrence, one for the latent process and one for the observables. This appears to be a novel, if minor contribution. The larger contribution is methodological, in areas of time series modelling that are both of great practical importance and have hitherto been dominated by rigid functional forms. The demonstration of the applicability and usefulness of general-purpose non-linear models for volatility forecasting would be extremely impactful. I have a few comments and reservations with the paper: 1) Although not mentioned explicitly, the authors’ framework are couched in terms of carrying out one-timestep-ahead forecasts of volatility. However, many applications of volatility models, for instance for derivative pricing, require longer-horizon forecasts. It would be interesting to discuss how this model could be extended to forecast at longer horizons. 2) In Section 4.4, there’s a mention that a GARCH(1,1) is conditionally deterministic. This is true only when forecasting 1 time-step in the future. At longer horizons, the GARCH(1,1) volatility forecast is not deterministic. 3) I was initially unhappy with the limitations of the experimental validation, limited to comparison with a baseline GARCH model. However, the authors provided more comparisons in the revision, which adds to the quality of the results, although the models compared against cannot be considered state of the art. It would be well advised to look into R packages such as `stochvol’ and ‘fGarch’ to get implementations of a variety of models that can serve as useful baselines, and provide convincing evidence that the modelled volatility is indeed substantially better than approaches currently entertained by the finance literature. 4) In Section 5.2, more details should be given on the network, e.g. number of hidden units, as well as the embedding dimension D_E (section 5.4) 5) In Section 5.3, more details should be given on the data generating process for the synthetic data experiments. 6) Some results in the appendix are very puzzling: around jumps in the price series, which are places where the volatility should spike, the model reacts instead by huge drops in the volatility (Figure 4(b) and (c), respectively around time steps 1300 and 1600). This should be explained and discussed. All in all, I think that the paper provides a nice contribution to the art of volatility modelling. In spite of some flaws, it provides a starting point for the broader impact of neural time series processing in the financial community.","['4', '4', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[2, 7, 10, 15]","[6, 13, 16, 21]","[17, 375, 78, 359]","[8, 185, 30, 163]","[9, 163, 19, 155]","[0, 27, 29, 41]","The sentiment score is 60 (positive) because the reviewer generally praises the paper, calling it 'well written and easy to follow' and stating that it provides 'a nice contribution to the art of volatility modelling'. They mention that the paper's contribution would be 'extremely impactful'. However, they also list several reservations and areas for improvement, which prevents the score from being higher. The politeness score is 80 (quite polite) because the reviewer uses respectful language throughout, offering constructive criticism and suggestions rather than harsh critiques. They use phrases like 'It would be interesting to discuss' and 'It would be well advised to look into', which are polite ways of suggesting improvements. The reviewer also acknowledges the authors' efforts in the revision, showing respect for their work. The overall tone is professional and courteous, though not excessively formal or deferential.",60,80
A Way out of the Odyssey: Analyzing and Combining Recent Insights for LSTMs,Reject,2017,"['Shayne Longpre', 'Sabeek Pradhan', 'Caiming Xiong', 'Richard Socher']","[5, 5, 5]","['5', '5', '5']","The paper proposes and analyses three methods applied to traditional LSTMs: Monte Carlo test-time model averaging, average pooling, and residual connections. It shows that those methods help to enhance traditional LSTMs on sentiment analysis. Although the paper is well written, the experiment section is definitely its dead point. Firstly, although it shows some improvements over traditional LSTMs, those results are not on par with the state of the art. Secondly, if the purpose is to take those extensions as strong baselines for further research, the experiments are not adequate: the both two datasets which were used are quite similar (though they have different statistics). I thus suggest to carry out more experiments on more diverse tasks, like those in *LSTM: A Search Space Odyssey*). Besides, those extensions are not really novel.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[2, 2, 9, 11]","[8, 7, 15, 17]","[33, 5, 385, 229]","[11, 2, 165, 111]","[21, 3, 210, 111]","[1, 0, 10, 7]","The sentiment score is -40 because while the reviewer acknowledges that the paper is well-written and shows some improvements, they express significant concerns about the experiment section, calling it the paper's 'dead point'. They criticize the lack of state-of-the-art results, inadequate experiments, and lack of novelty in the proposed methods. The politeness score is 20 because the reviewer uses generally polite language, acknowledging positive aspects before critiquing, and uses phrases like 'I suggest' rather than making demands. However, the use of 'dead point' is somewhat harsh, preventing a higher politeness score.",-40,20
Adding Gradient Noise Improves Learning for Very Deep Networks,Reject,2017,"['Arvind Neelakantan', 'Luke Vilnis', 'Quoc V. Le', 'Lukasz Kaiser', 'Karol Kurach', 'Ilya Sutskever', 'James Martens']","[4, 4, 7]","['Ok but not good enough - rejection', 'Ok but not good enough - rejection', 'Good paper, accept']","The authors consider a simple optimization technique consisting of adding gradient noise with a specific schedule. They test their method on a number of recently proposed neural networks for simulating computer logic (end-to-end memory network, neural programmer, neural random access machines). On these networks, the question of optimization has so far not been studied as extensively as for more standard networks. A study specific to this class of models is therefore welcome. Results consistently show better optimization properties from adding noise in the training procedure. One issue with the paper is that it is not clear whether the proposed optimization strategy permits to learn actually good models, or simply better than those that do not use noise. A comparison to results obtained in the literature would be desirable. For example, in the MNIST experiments of Section 4.1, the optimization procedure reaches in the most favorable scenario an average accuracy level of approximately 92%, which is still far from having actually learned an interesting problem representation (a linear model would probably reach similar accuracy). I understand that the architecture is specially designed to be difficult to optimize (20 layers of 50 HUs), but it would have been more interesting to consider a scenario where depth is actually beneficial for solving the problem.","['4', '5', '5']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[6, 5, 13, 11, 13, 6, 8]","[11, 10, 19, 17, 18, 9, 14]","[34, 32, 300, 99, 84, 28, 38]","[15, 16, 143, 49, 44, 12, 19]","[18, 16, 146, 45, 32, 14, 16]","[1, 0, 11, 5, 8, 2, 3]","The sentiment score is slightly positive (20) because the reviewer acknowledges the value of the study, stating it is 'welcome' and that results 'consistently show better optimization properties'. However, they also express concerns about the actual effectiveness of the method and suggest improvements, which tempers the positivity. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, acknowledging the authors' work and offering constructive criticism. They use phrases like 'I understand' and 'it would have been more interesting', which maintain a polite tone while suggesting improvements. The reviewer avoids harsh or dismissive language, instead offering specific, actionable feedback in a professional manner.",20,50
Adversarial examples for generative models,Reject,2017,"['Jernej Kos', 'Ian Fischer', 'Dawn Song']","[6, 5, 5]","['Marginally above acceptance threshold', '5', '5']","Comments: *This contrasts to adversarial attacks on classifiers, where any inspection of the inputs will reveal the original bytes the adversary supplied, which often have telltale noise* Is this really true? If it were the case, wouldn*t it imply that training *against* adversarial examples should easily make a classifier robust to adversarial examples (if they all have a telltale noise)? Pros: -The question of whether adversarial examples exist in generative models, and indeed how the definition of *adversarial example* carries over is an interesting one. -Finding that a certain type of generative model *doesn*t have* adversarial examples would be a really significant result, finding that generative models have adversarial examples would also be a worth negative result. -The adversarial examples in figures 5 and 6 seem convincing, though they seem much more overt and noisy than the adversarial examples on MNIST shown in (Szegedy 2014). Is this because it*s actually harder to find adversarial examples in these types of generative models? Issues: -Paper is significantly over length at 13 pages. -The beginning of the paper should more clearly motivate its purpose. -Paper has *generative models* in the title but as far as I can tell the whole paper is concerned with autoencoder-type models. This is kind of annoying because if someone wanted to consider adversarial attacks on, say, autoregressive models, they might be unreasonably burdened by having to explain how they*re distinct from a paper called *adversarial examples for generative models*. -I think that the introduction contains too much background information - it could be tightened.","['3', '4', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[23, 9, 16]","[29, 14, 22]","[111, 41, 126]","[52, 37, 65]","[1, 0, 25]","[58, 4, 36]","The sentiment score is slightly positive (20) because the reviewer acknowledges the interesting nature of the research question and the potential significance of the results. They mention some pros of the paper, such as convincing adversarial examples. However, they also point out several issues, which tempers the overall positive sentiment. The politeness score is moderately positive (50) as the reviewer uses neutral language and frames criticisms constructively as 'issues' rather than harsh judgments. They also balance negative points with positive observations. The reviewer doesn't use overly formal or polite language, but maintains a professional and respectful tone throughout.",20,50
Alternating Direction Method of Multipliers for Sparse Convolutional Neural Networks,Reject,2017,"['Farkhondeh Kiaee', 'Christian Gagné', 'and Mahdieh Abbasi']","[7, 7, 6, 5]","['Good paper, accept', 'Good paper, accept', 'Marginally above acceptance threshold', '5']","The authors use the Alternating Direction Method of Multipliers (ADMM) algorithm for the first time on CNN models, allowing them to perform model compression without any appreciable loss on the CIFAR-10, CIFAR-100, and SVHN tasks. The algorithmic details and the intuition behind their algorithm are generally well presented, (although there are occasional typos). Pros: 1) Put an old algorithm to good use in a new setting 2) The algorithm has the nice property that it is partially analytically solvable (due to the separability property the authors mention). This contributes to the efficient trainability of the model 3) Seems to dovetail nicely with other results to encourage sparsity--that is, it can be used simultaneously--and is quite generalizable. Cons: 1) It would be nice to see a more thorough analysis of the performance gains for using this method, beyond raw data about % sparsity--some sort of comparison involving training time would be great, and is currently lacking. EDIT: Authors addressed this by addition of results in Appendix B. 2) I would very much like to see some discussion about why the sparsity seems to *improve* the test performance, as mentioned in my previous comment. Is this a general feature? Is this a statistical fluke? etc. Even if the answer is *it is not obvious, and determining why goes outside the scope of this work*, I would like to know it! EDIT: Authors addressed this by addition of statistical significance tests in the new Appendix A. 3) Based on the current text, and some of the other reviewer comments, I would appreciate an expanded discussion on how this work compares with other methods in the field. I don*t think a full numerical comparison is necessary, but some additional text discussing some of the other papers mentioned in the other reviews would greatly benefit the paper. EDIT: Authors addressed this by followup to question and additional text in the paper. Additional comments: If my Cons are addressed, I would definitely raise my score to a 6 or even a 7. The core of this paper is quite solid, it just needs a little bit more polishing. EDIT: Score has been updated. Note: the authors probably meant *In order to verify* in the first sentence of Appendix A.","['3', '5', '4', '3']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[2, 17, 3]","[6, 23, 8]","[6, 128, 16]","[2, 60, 5]","[1, 44, 9]","[3, 24, 2]","The sentiment score is 70 (positive) because the reviewer starts by highlighting the novelty and effectiveness of the authors' approach, mentioning several pros. The reviewer does list some cons, but these are presented as suggestions for improvement rather than major flaws. The reviewer also notes that if these cons are addressed, they would raise their score significantly, indicating an overall positive view of the paper. The politeness score is 80 (polite) because the reviewer uses respectful language throughout, acknowledging the authors' work positively and framing criticisms as constructive suggestions. The reviewer also appreciates the authors' efforts to address previous comments, as indicated by the 'EDIT' notes. The use of phrases like 'I would appreciate' and 'I would like to see' further contribute to the polite tone.",70,80
An Analysis of Deep Neural Network Models for Practical Applications,Reject,2017,"['Alfredo Canziani', 'Adam Paszke', 'Eugenio Culurciello']","[4, 4, 5]","['Ok but not good enough - rejection', 'Ok but not good enough - rejection', '5']","The paper evaluates recent development in competitive ILSVRC CNN architectures from the perspective of resource utilization. It is clear that a lot of work has been put into the evaluations. The findings are well presented and the topic itself is important. However, most of the results are not surprising to people working with CNNs on a regular basis. And even if they are, I am not convinced about their practical value. It is hard to tell what we actually learn from these findings when approaching new problems with computational constraints or when in production settings. In my opinion, this is mainly because the paper does not discuss realistic circumstances. Main concerns: 1) The evaluation does not tell me much for realistic scenarios, that mostly involve fine-tuning networks, as ILSVRC is just a starting point in most cases. VGG for instance really shines for fine-tuning, but it is cumbersome to train from scratch. And VGG works well for compression, too. So possibly it is a very good choice if these by now standard steps are taken into account. Such questions are of high practical relevance! 2) Compressed networks have a much higher acc/parameter density, so comparison how well models can be compressed is important, or at least comparing to some of the most well-known and publicly available compressed networks. 3) There is no analysis on the actual topology of the networks and where the bottlenecks lie. This would be very useful to have as well. Minor concern: 1) Why did the authors choose to use batch normalization in NiN and AlexNet?","['3', '3', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[3, 2, 18]","[8, 8, 24]","[10, 20, 116]","[5, 4, 80]","[5, 13, 17]","[0, 3, 19]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the effort put into the paper and its clear presentation, they express significant concerns about the practical value and relevance of the findings. The reviewer states that the results are 'not surprising' and questions what can be learned from them in realistic scenarios. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, acknowledging the work done and the importance of the topic before presenting their concerns. They phrase their criticisms as 'concerns' rather than outright flaws, and use phrases like 'in my opinion' to soften their critique. The reviewer also provides specific suggestions for improvement, which is a constructive approach.",-20,50
An Empirical Analysis of Deep Network Loss Surfaces,Reject,2017,"['Daniel Jiwoong Im', 'Michael Tao', 'Kristin Branson']","[4, 4, 4, 6]","['Ok but not good enough - rejection', 'Ok but not good enough - rejection', 'Ok but not good enough - rejection', 'Marginally above acceptance threshold']","First of all, I would like to thank the authors for putting this much work into a necessary but somewhat tedious topic. While I think the paper is somewhat below the standard of a conference paper (see detailed comments below), I would definitely love to see a version of this paper published with some of the issues ironed out. I also agree with many of the points raised by other reviewers and will not repeat them here. Major points: -- *As we saw in the previous section, the minima of deep network loss functions are for the most part decent.* All you said in the previous section was that theory shows that there are no bad minima under *strong assumptions*. There is no practical proof that minima do not vary in quality. -- *This implies that we probably do not need to take many precautions to avoid bad minima in practice. If all minima are decent, then the task of finding a *decent minima quickly* is reduced to the task of finding any minima quickly.* First of all, as one of the reviewers pointed out, we are never guaranteed in practice to actually reach a local minimum. We could always hit a region of the objective function where the algorithm makes essentially no further progress. The final error level, in practice, actually does depend significantly on many factors such as (i) optimization algorithm (ii) learning rate schedule (iii) initialization of weights (iv) presence of unsupervised pretraining (v) whether neurons are added or eliminated during training etc. etc. Therefore, the task of optimizing neural networks is far from being *reduced to finding any minima quickly*. -- Figure 1 I don*t like Figure 1, because it suggests to me that you diagnosed exactly where the transition between the two phases happened, which I don*t think you did. Also, the concept of having a fast-decaying error followed by a slow-decaying error is simple enough for readers to understand without a dedicated graph. Minor point on presentation: The red brace is positioned lower in the figure than the blue brace and the braces don*t join up horizontally. Please be more careful. -- Misuse of the transient phase / minimization phase concept In section 4.3, you talk about the transient and minimization phase of optimization. However, you have no way of diagnosing when or if your algorithm reaches the minimization phase. You seem to think that the minimization phase is simply the part of the optimization process where the error decreases slowly. AFAIK, this is not the case. The minimization phase is where the optimization algorithm enters the vicinity of the local minimum that can be approximated by the second-order Taylor expansion. For this to even occur, one would have to verify, for example, that the learning rate is small enough. You change the algorithm after 25%, 50% and 75% of training, but these points seem arbitrary. What is the minimization phase was reached at 99%, or 10 epochs after you decided to stop training? -- Only 1 dataset You run most experiments on only 1 dataset (CIFAR). Please replicate with at least one more dataset. -- Many figures are unclear For each figure, the following information are relevant: network used; dataset used; learning rate used; batch norm yes / no; whether figure shows train, test, or validation error. It should be easy for the reader to ascertain this information for all figures, not just for some. -- You say at the beginning of section 4.1 that each algorithm finds a different minimum as if this is a significant finding. However, this is obvious because the updates taken by these algorithms vary wildly. Keep in mind that there is an exponentially large number of minima. The probability of different algorithms choosing the same minimum is essentially zero because of their sheer number. The same would be true if you even shift the learning rate slightly or use a different random seed for minibatch generation etc. etc. -- Lack of confidence intervals The value of Figures 1, 2, 3 and 6 is limited is because it is unclear how these plots would change if the random seed were changed. We only get information for a single weight initialization and a single minibatch sequence. While figures 5 and 7 can be used to try and infer what confidence intervals around plots in figures 1, 2, 3 and 6 might look like, I think those confidence intervals should still be shown for at least a subset of the configurations presented. -- Lack of information regarding learning rate There is big question mark left open regarding how all your results would change if different learning rates were used. You don*t even tell us how you chose the learning rates from the intervals you gave in section 3.4. -- Lack of information regarding the absolute distance of interpolated points In most figures, you interpolate between two or three trained weight configuration. However, you do not say how far the interpolated points are apart. This is highly significant, because if points are close together and there is a big *hump* between them, it means that those points are more *brittle* than if they are far apart and there is a big *hump* between them. Minor points: -- LSTM is not a fixed network architecture like NiN or VGG, but a layer type. LSTM would be equivalent to CNN. Also, the VGG paper has multiple versions of VGG. You should specify which one you used. -- The font size for the legends in the upper triangle of Table 1 is too small. You can*t just write *best viewed in zoom* in the table caption and pretend that somehow fixes the problem. Personally, I prefer no legend over an unreadable legend.","['3', '4', '4', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[4, 2, 13]","[8, 7, 18]","[26, 9, 21]","[8, 2, 8]","[17, 3, 11]","[1, 4, 2]","The sentiment score is -30 because while the reviewer starts with a positive note thanking the authors and expressing interest in seeing a revised version published, the majority of the review focuses on critical points and areas for improvement. The reviewer identifies several major issues with the paper, including questionable assumptions, lack of practical proof, and unclear figures. However, the tone is not entirely negative, as the reviewer offers constructive feedback and suggestions for improvement. The politeness score is 50 because the reviewer maintains a professional and respectful tone throughout, using phrases like 'I would like to thank the authors' and 'Please be more careful.' The reviewer also acknowledges the effort put into the work. However, the criticism is direct and sometimes blunt, which prevents the score from being higher. The reviewer balances politeness with honest feedback, resulting in a moderately positive politeness score.",-30,50
An Information Retrieval Approach for Finding Dependent Subspaces of Multiple Views,Reject,2017,"['Ziyuan Lin', 'Jaakko Peltonen']","[4, 4, 4]","['Ok but not good enough - rejection', 'Ok but not good enough - rejection', 'Ok but not good enough - rejection']","This paper proposes a multiview learning approach to finding dependent subspaces optimized for maximizing cross-view similarity between neighborhoods of data samples. The motivation comes from information retrieval tasks. Authors position their work as an alternative to CCA-based multiview learning; note, however, that CCA based techniques have very different purpose and are rather broadly applicable than the setting considered here. Main points: - I am not sure what authors mean by time complexity. It would appear that they simply report the computational cost of evaluating the objective in equation (7). Is there a sense of how many iterations of the L-BFGS method? Since that is going to be difficult given the nature of the optimization problem, one would appreciate some sense of how hard or easy it is in practice to optimize the objective in (7) and how that varies with various problem dimensions. Authors argue that scalability is not their first concern, which is understandable, but if they are going to make some remarks about the computational cost, it better be clarified that the reported cost is for some small part of their overall approach rather than “time complexity”. - Since authors position their approach as an alternative to CCA, they should remark about how CCA, even though a nonconvex optimization problem, can be solved exactly with computational cost that is linear in the data size and only quadratic with dimensionality even with a naive implementation. The method proposed in the paper does not seem to be tractable, at least not immediately. - The empirical results with synthetic data are a it confusing. First of all the data generation procedure is quite convoluted, I am not sure why we need to process each coordinate separately in different groups, and then permute and combine etc. A simple benchmark where we take different linear transformations of a shared representation and add independent noise would suffice to confirm that the proposed method does something reasonable. I am also baffled why CCA does not recover the true subspace - arguably it is the level of additive noise that would impact the recoverability - however the proposed method is nearly exact so the noise level is perhaps not so severe. It is also not clear if authors are using regularization with CCA - without regularization CCA can be have in a funny manner. This needs to be clarified.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[5, 17]","[10, 23]","[11, 92]","[6, 63]","[3, 6]","[2, 23]","The sentiment score is -30 because the review expresses several concerns and criticisms about the paper, including confusion about time complexity claims, questioning the need for complex synthetic data generation, and skepticism about CCA performance comparisons. However, it's not entirely negative as it acknowledges some understandable aspects. The politeness score is 20 because the reviewer uses professional language and phrases criticisms as questions or suggestions rather than direct attacks. They use phrases like 'I am not sure' and 'one would appreciate' which maintain a respectful tone while still conveying concerns.",-30,20
Annealing Gaussian into ReLU: a New Sampling Strategy for Leaky-ReLU RBM,Reject,2017,"['Chun-Liang Li', 'Siamak Ravanbakhsh', 'Barnabas Poczos']","[5, 5, 6, 5]","['5', '5', 'Marginally above acceptance threshold', '5']","The authors proposed to use leaky rectified linear units replacing binary units in Gaussian RBM. A sampling method was presented to train the leaky-ReLU RBM. In the experimental section, AIS estimated likelihood on Cifar10 and SVHN were reported. It*s interesting for trying different nonlinear hidden units for RBM. However, there are some concerns for the current work. 1. The author did not explain why the proposed sampling method (Alg. 2) is correct. And the additional computation cost (the inner loop and the projection) should be discussed. 2. The results (both the resulting likelihood and the generative samples) of Gaussian RBM are much worse than what we have experienced. It seems that the Gaussian RBM were not trained properly. 3. The representation learned from a good generative model often helps the classification task when there are fewer label samples. Gaussian RBM works well for texture synthesis tasks in which mixing is an important issue. The authors are encouraged to do more experiments in these two direction.","['5', '4', '4', '4']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[6, 8, 16]","[12, 14, 22]","[94, 58, 239]","[40, 26, 120]","[47, 30, 105]","[7, 2, 14]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the interesting aspect of trying different nonlinear hidden units for RBM, they express several concerns about the current work. The reviewer points out issues with the sampling method explanation, poor results compared to their experience, and suggests additional experiments. This indicates a generally critical view of the paper, albeit with some positive aspects.

The politeness score is moderately positive (50) as the reviewer uses respectful language throughout. They start by acknowledging the interesting aspects of the work and use phrases like 'It's interesting' and 'The authors are encouraged to' which maintain a constructive and polite tone. Even when expressing concerns, the language remains professional and not personally critical. The suggestions for improvement are framed as encouragement rather than demands.",-20,50
Attentive Recurrent Comparators,Reject,2017,"['Pranav Shyam', 'Ambedkar Dukkipati']","[4, 5, 3]","['Ok but not good enough - rejection', '5', 'Clear rejection']","This paper introduces an attention-based recurrent network that learns to compare images by attending iteratively back and forth between a pair of images. Experiments show state-of-the-art results on Omniglot, though a large part of the performance gain comes from when extracted convolutional features are used as input. The paper is significantly improved from the original submission and reflects changes based on pre-review questions. However, while there was an attempt made to include more qualitative results e.g. Fig. 2, it is still relatively weak and could benefit from more examples and analysis. Also, why is the attention in Fig. 2 always attending over the full character? Although it is zooming in, shouldn’t it attend to relevant parts of the character? Attending to the full character on a solid background seems a trivial solution where it is then unclear where the large performance gains are coming from. While the paper is much more polished now, it is still lacking in details in some respects, e.g. details of the convolutional feature extractor used that gives large performance gain.","['5', '2', '5']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[1, 9, 16]","[6, 15, 22]","[12, 125, 121]","[4, 46, 41]","[8, 30, 65]","[0, 49, 15]","The sentiment score is slightly positive (20) because the reviewer acknowledges improvements in the paper and state-of-the-art results, but also points out several areas for improvement. The review begins with positive aspects but then transitions to critiques, indicating a balanced but slightly favorable view. The politeness score is moderately positive (50) as the reviewer uses neutral language and constructive criticism without harsh words. They acknowledge the authors' efforts to improve the paper and frame suggestions as opportunities for further enhancement rather than outright criticisms. The reviewer maintains a professional tone throughout, balancing praise with areas for improvement.",20,50
BIOACOUSTIC SEGMENTATION BY HIERARCHICAL DIRICHLET PROCESS HIDDEN MARKOV MODEL,Reject,2017,"['Vincent Roger', 'Marius Bartcus', 'Faicel Chamroukhi', 'Hervé Glotin']","[5, 4, 5]","['5', 'Ok but not good enough - rejection', '5']","This paper applies HDP-HMM to challenging bioacoustics segmentation problems including humpback whole sound and bird sound segmentation. Although the technique itself is not novel, the application of this data-driven method to bioacoustics segmentation is quite challenging, and may yield some scientific findings, and this is a valuable contribution to the bioacoustics field. My concern for this paper is that it does not have fair comparison of the other simple methods including BIC and AIC, and it is better to provide such comparisons. Especially, as the authors pointed out, the computational cost of HDP-HMM is a big issue, and the other simple methods may solve this issue.","['3', '3', '5']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']",['skipped'],['skipped'],['skipped'],['skipped'],['skipped'],['skipped'],"The sentiment score is 50 (slightly positive) because the reviewer acknowledges the paper's valuable contribution to the bioacoustics field and the challenging nature of the application. However, they express concerns about the lack of comparison with other methods and computational cost issues, which prevents a higher positive score. The politeness score is 75 (quite polite) as the reviewer uses respectful language throughout, acknowledging the paper's merits before presenting their concerns. They use phrases like 'My concern' and 'it is better to provide' instead of more direct or harsh criticism, maintaining a constructive tone.",50,75
Beyond Bilingual: Multi-sense Word Embeddings using Multilingual Context,Reject,2017,"['Shyam Upadhyay', 'Kai-Wei Chang', 'James Zou', 'Matt Taddy', 'Adam Kalai']","[4, 4, 5]","['Ok but not good enough - rejection', 'Ok but not good enough - rejection', '5']","This paper discusses multi-sense embedddings and proposes learning those by using aligned text across languages. Further, the paper suggests that adding more languages helps improve word sense disambiguation (as some ambiguities can be carried across language pairs). While this idea in itself isn*t new, the authors propose a particular setup for learning multi-sense embeddings by exploiting multilingual data. Broadly this is fine, but unfortunately the paper then falls short in a number of ways. For one, the model section is unnecessarily convoluted for what is a nice idea that could be described in a far more concise fashion. Next (and more importantly), comparison with other work is lacking to such an extent that it is impossible to evaluate the merits of the proposed model in an objective fashion. This paper could be a lot stronger if the learned embeddings were evaluated in downstream tasks and evaluated against other published methods. In the current version there is too little of this, leaving us with mostly relative results between model variants and t-SNE plots that don*t really add anything to the story.","['3', '4', '3']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[3, 10, 10, 21, 8]","[9, 16, 12, 27, 14]","[43, 317, 18, 142, 208]","[24, 146, 8, 81, 78]","[18, 157, 6, 42, 109]","[1, 14, 4, 19, 21]","The sentiment score is -50 because while the reviewer acknowledges some positive aspects ('Broadly this is fine', 'nice idea'), they express significant criticisms. The review points out several shortcomings, such as the convoluted model section, lack of comparison with other work, and insufficient evaluation. The overall tone suggests disappointment with the paper's current state. The politeness score is 20 because the reviewer uses relatively neutral language and offers constructive criticism. They avoid harsh or rude phrasing, instead using phrases like 'could be a lot stronger' and 'unfortunately'. The reviewer also acknowledges positive aspects before presenting criticisms, which is a polite approach.",-50,20
Beyond Fine Tuning: A Modular Approach to Learning on Small Data,Reject,2017,"['Aryk Anderson', 'Kyle Shaffer', 'Artem Yankov', 'Court Corley', 'Nathan Hodas']","[4, 6, 6]","['Ok but not good enough - rejection', 'Marginally above acceptance threshold', 'Marginally above acceptance threshold']","This paper proposes a method of augmenting pre-trained networks for one task with an additional inference path specific to an additional task, as a replacement for the standard “fine-tuning” approach. Pros: -The method is simple and clearly explained. -Standard fine-tuning is used widely, so improvements to and analysis of it should be of general interest. -Experiments are performed in multiple domains -- vision and NLP. Cons: -The additional modules incur a rather large cost, resulting in 2x the parameters and roughly 3x the computation of the original network (for the “stiched” network). These costs are not addressed in the paper text, and make the method significantly less practical for real-world use where performance is very often important. -Given these large additional costs, the core of the idea is not sufficiently validated, to me. In order to verify that the improved performance is actually coming from some unique aspects of the proposed technique, rather than simply the fact that a higher-capacity network is being used, some additional baselines are needed: (1) Allowing the original network weights to be learned for the target task, as well as the additional module. Outperforming this baseline on the validation set would verify that freezing the original weights provides an interesting form of regularization for the network. (2) Training the full module/stitched network from scratch on the *source* task, then fine-tuning it for the target task. Outperforming this baseline would verify that having a set of weights which never “sees” the source dataset is useful. -The method is not evaluated on ImageNet, which is far and away the most common domain in which pre-trained networks are used and fine-tuned for other tasks. I’ve never seen networks pre-trained on CIFAR deployed anywhere, and it’s hard to know whether the method will be practically useful for computer vision applications based on CIFAR results -- often improved performance on CIFAR does not translate to ImageNet. (In other contexts, such as more theoretical contributions, having results only on small datasets is acceptable to me, but network fine-tuning is far enough on the “practical” end of the spectrum that claiming an improvement to it should necessitate an ImageNet evaluation.) Overall I think the proposed idea is interesting and potentially promising, but in its current form is not sufficiently evaluated to convince me that the performance boosts don’t simply come from the use of a larger network, and the lack of ImageNet evaluation calls into question its real-world application. =============== Edit (1/23/17): I had indeed missed the fact that the Stanford Cars does do transfer learning from ImageNet -- thanks for the correction. However, the experiment in this case is only showing late fusion ensembling, which is a conventional approach compared with the *stitched network* idea which is the real novelty of the paper. Furthermore the results in this case are particularly weak, showing only that an ensemble of ResNet+VGG outperforms VGG alone, which is completely expected given that ResNet alone is a stronger base network than VGG (*ResNet+VGG > ResNet* would be a stronger result, but still not surprising). Demonstrating the stitched network idea on ImageNet, comparing with the corresponding VGG-only or ResNet-only finetuning, could be enough to push this paper over the bar for me, but the current version of the experiments here don*t sufficiently validate the stitched network idea, in my opinion.","['4', '2', '5']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[2, 3, 2, 13, 8]","[1, 9, 3, 17, 12]","[1, 9, 4, 44, 52]","[0, 6, 1, 29, 16]","[1, 3, 3, 9, 35]","[0, 0, 0, 6, 1]","The sentiment score is slightly negative (-30) because while the reviewer acknowledges some positive aspects ('Pros'), they express significant concerns ('Cons') about the method's practicality, validation, and applicability. The reviewer concludes that the idea is 'interesting and potentially promising' but 'not sufficiently evaluated'. The politeness score is moderately positive (50) as the reviewer maintains a professional tone throughout, acknowledging both strengths and weaknesses, and uses phrases like 'to me' to soften criticisms. They also thank the authors for a correction in the edit. However, the critique is direct and doesn't use overly polite language, keeping the score from being higher.",-30,50
Binary Paragraph Vectors,Reject,2017,"['Karol Grzegorczyk', 'Marcin Kurdziel']","[6, 5, 6]","['Marginally above acceptance threshold', '5', 'Marginally above acceptance threshold']","The method in this paper introduces a binary encoding level in the PV-DBOW and PV-DM document embedding methods (from Le & Mikolov*14). The binary encoding consists in a sigmoid with trained parameters that is inserted after the standard training stage of the embedding. For a document to encode, the binary vector is obtained by forcing the sigmoid to output a binary output for each of the embedding vector components. The binary vector can then be used for compact storage and fast comparison of documents. Pros: - the binary representation outperforms the Semantic hashing method from Salakhutdinov & Hinton *09 - the experimental approach sound: they compare on the same experimental setup as Salakhutdinov & Hinton *09, but since in the meantime document representations improved (Le & Mikolov*14), they also combine this new representation with an RBM to show the benefit of their binary PV-DBOW/PV-DM Cons: - the insertion of the sigmoid to produce binary codes (from Lin & al. *15) in the training process is incremental - the explanation is too abstract and difficult to follow for a non-expert (see details below) - a comparison with efficient indexing methods used in image retrieval is missing. For large-scale indexing of embedding vectors, derivations of the Inverted multi-index are probably more interesting than binary codes. See eg. Babenko & Lempitsky, Efficient Indexing of Billion-Scale Datasets of Deep Descriptors, CVPR*16 Detailed comments: Section 1: the motivation for producing binary codes is not given. Also, the experimental section could give some timings and mem usage numbers to show the benefit of binary embeddings figure 1, 2, 3: there is enough space to include more information on the representation of the model: model parameters + training objective + characteristic sizes + dropout. In particular, in fig 2, it is not clear why *embedding lookup* and *linear projection* cannot be merged in a single smaller lookup table (presumably because there is an intermediate training objective that prevents this). p2: *This way, the length of binary codes is not tied to the dimensionality of word embeddings.* -> why not? section 3: This is the experimental setup of Salakhutdinov & Hinton 2009. Specify this and whether there is any difference between the setups. *similarity of the inferred codes*: say here that codes are compared using Hamming distances. *binary codes perform very well, despite their far lower capacity* -> do you mean smaller size than real vectors? fig 5: these plots could be dropped if space is needed. section 3.1: one could argue that *transferring* from Wikipedia to anything else cannot be called transferring, since Wikipedia*s purpose is to include all topics and lexical domains section 3.2: specify how the 300D real vectors are compared. L2 distance? inner product? fig4: specify what the raw performance of the large embedding vectors is (without pre-filtering with binary codes), or equivalently, the perf of (code-size, Hamming dis) = (28, 28), (24, 24), etc.","['2', '5', '3']","['The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is fairly confident that the evaluation is correct']","[4, 15]","[9, 20]","[12, 30]","[5, 18]","[3, 2]","[4, 10]","The sentiment score is slightly positive (20) because the reviewer acknowledges some pros of the paper, such as outperforming previous methods and having a sound experimental approach. However, they also list several cons and areas for improvement, which tempers the overall positivity. The politeness score is moderately positive (50) as the reviewer maintains a professional and constructive tone throughout. They provide balanced feedback, offering both positive and negative points without using harsh language. The reviewer uses phrases like 'the experimental approach sound' and provides detailed comments and suggestions for improvement, which is considerate and helpful. The absence of overtly negative or rude language, combined with the constructive nature of the feedback, contributes to the politeness score.",20,50
Boosted Generative Models,Reject,2017,"['Aditya Grover', 'Stefano Ermon']","[6, 5, 5]","['Marginally above acceptance threshold', '5', '5']","This paper extends boosting to the task of learning generative models of data. The strong learner is obtained as a geometric average of “weak learners”, which can themselves be normalized (e.g. VAE) or un-normalized (e.g. RBMs) generative models (genBGM), or a classifier trained to discriminate between the strong learner at iteration T-1 and the true data distribution (discBGM). This latter method is closely related to Noise Contrastive Estimation, GANs, etc. The approach benefits from strong theoretical guarantees, with strict conditions under which each boosting iteration is guaranteed to improve the log-likelihood. The downside of the method appears to be the lack of normalization constant for the resulting strong learner and the use of heuristics to weight each weak learner (which seems to matter in practice, from Sec. 3.2). The discriminative approach further suffers from an expensive training procedure: each round of boosting first requires generating a “training set” worth of samples from the previous strong learner, where samples are obtained via MCMC. The experimental section is clearly the weak point of the paper. The method is evaluated on a synthetic dataset, and a single real-world dataset, MNIST: both for generation and as a feature extraction mechanism for classification. Of these, the synthetic experiments were the clearest in showcasing the method. On MNIST, the baseline models are much too weak for the results to be convincing. A modestly sized VAE can obtain 90 nats within hours on a single GPU, clearly an achievable goal. Furthermore, despite arguments to the contrary, I firmly believe that mixing base learners is an academic exercise, if only because of the burden of implementing K different models & training algorithms. This section fails to answer a more fundamental question: is it better to train a large VAE by maximizing the elbow, or e.g. train 10 iterations of boosting, using VAEs 1/10th the size of the baseline model ? Experimental details are also lacking, especially with respect to the sampling procedure used to draw samples from the BGM. The paper would also benefit from likelihood estimates obtained via AIS. With regards to novelty and prior work, there is also a missing reference to “Self Supervised Boosting” by Welling et al [R1]. After a cursory read through, there seems to be strong similarities to the GenBGM approach which ought to be discussed. Overall, I am on the fence. The idea of boosting generative models is intriguing, seems well motivated and has potential for impact. For this reason, and given the theoretical contributions, I am willing to overlook some of the issues highlighted above, and hope the authors can address some of them in time for the rebuttal. [R1] https://papers.nips.cc/paper/2275-self-supervised-boosting.pdf PROS: Novel and intriguing idea Strong theoretical guarantees CONS: Resulting boosted model is un-normalized Discriminator based boosting is expensive, due to sampling via MCMC Weak experimental section","['3', '3', '3']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[3, 9]","[9, 15]","[87, 406]","[42, 199]","[43, 200]","[2, 7]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('intriguing idea', 'strong theoretical guarantees'), they express significant concerns about the experimental section, which they call 'clearly the weak point of the paper'. They also mention several other issues like the lack of normalization and expensive training procedure. The overall tone suggests the reviewer is 'on the fence' about the paper. The politeness score is moderately positive (50) as the reviewer uses professional and respectful language throughout. They acknowledge the paper's strengths and provide constructive criticism without using harsh or dismissive language. The reviewer also expresses hope that the authors can address some issues, which is a polite way of offering feedback.",-20,50
Boosted Residual Networks,Reject,2017,"['Alan Mosca', 'George D. Magoulas']","[3, 4, 3]","['Clear rejection', 'Ok but not good enough - rejection', 'Clear rejection']","This paper proposes a boosting based ensemble procedure for residual networks by adopting the Deep Incremental Boosting method that was used for CNN*s(Mosca & Magoulas, 2016a). At each step t, a new block of layers are added to the network at a position p_t and the weights of all layers are copied to the current network to speed up training. The method is not sufficiently novel since the steps of Deep Incremental Boosting are slightly adopted. Instead of adding a layer to the end of the network, this version adds a block of layers to a position p_t (starts at a selected position p_0) and merges layer accordingly hence slightly adopts DIB. The empirical analysis does not use any data-augmentation. It is not clear whether the improvements (if there is) of the ensemble disappear after data-augmentation. Also, one of the main baselines, DIB has no-skip connections therefore this can negatively affect the fair comparison. The authors argue that they did not involve state of art Res Nets since their analysis focuses on the ensemble approach, however any potential improvement of the ensemble can be compensated with an inherent feature of Res Net variant. The boosting procedure can be computationally restrictive in case of ImageNet training and Res Net variants may perform much better in that case too. Therefore the baselines should include the state of art Res Nets and Dense Convolutional networks hence current results are preliminary. In addition, it is not clear how sensitive the boosting to the selection of injection point. This paper adopts DIB to Res Nets and provides some empirical analysis however the contribution is not sufficiently novel and the empirical results are not satisfactory for demonstrating that the method is significant. Pros -provides some preliminary results for boosting of Res Nets Cons -not sufficiently novel: an incremental approach -empirical analysis is not satisfactory","['5', '5', '5']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[3, 23]","[8, 28]","[12, 152]","[7, 85]","[3, 5]","[2, 62]","The sentiment score is -60 because the review is predominantly negative. The reviewer criticizes the paper for lack of novelty, insufficient empirical analysis, and inadequate baselines. They mention a few pros but these are outweighed by the cons. The politeness score is 0 (neutral) because the language used is professional and objective, without being particularly polite or rude. The reviewer states their criticisms directly but without using harsh or insulting language. They acknowledge some positive aspects ('provides some preliminary results') but mainly focus on the shortcomings in a matter-of-fact manner.",-60,0
Boosting Image Captioning with Attributes,Reject,2017,"['Ting Yao', 'Yingwei Pan', 'Yehao Li', 'Zhaofan Qiu', 'Tao Mei']","[4, 5, 6]","['Ok but not good enough - rejection', '5', 'Marginally above acceptance threshold']","CONTRIBUTIONS This paper extends end-to-end CNN+RNN image captioning model with auxiliary attribute predictions. It proposes five variants of network architectures, which take the attribute/image features in alternating orders or at every timestamp. The attributes (i.e., 1,000 most common words on COCO) are obtained by attribute classifiers trained by a multiple instance learning approach. The experiment results indicated that having these attributes as input improves captioning performance on standard metrics, including BLEU, METEOR, ROUGE-L, CIDEr-D. NOVELTY + SIGNIFICANCE All five variants of the network architectures, shown in Fig. 1, have followed a standard seq-to-seq LSTM model. The differences between these variants come from two aspects: 1. the order of image/attribute inputs; 2. whether to input attribute/image features at each time step. No architectural changes have been added to the proposed model over a standard seq-to-seq model. The proposed approach achieves decent performance improvement over previous work, but the technical novelty of this work is significantly limited by existing work that used similar ideas. In particular, Fang et al. 2015 and You et al. 2016 have both used attributes for image captioning. This work has used the same multiple instance learning procedure as Fang et al. 2015 to train visual detectors for common words and used detector outputs as conditional inputs to a language model. In addition, the idea of using image feature as input to every RNN timestamp has been widely explored, for instance, in Donahue et al. 2015. The authors did not offer a clear explanation about the technical contribution of this work over these existing approaches. CLARITY First, it is not clear to me which image features have been used by the baseline methods. As the baselines may rely on different image representations, the experiments would not offer a completely fair comparison. For example, the results of the attention-based models in Table 1 are directly copied from Xu et al., 2015, which were reported with Oxford VGG features, instead of GoogLeNet used by LSTM-A. Even if the baselines do not use the same types of features, it should at least be explicitly mentioned. Besides, the fact that the results of Table 1 and Table 2 are reported with different features (GoogLeNet v.s. ResNet) are not described clearly in the paper. “We select 1,000 most common words on COCO…” How would this approach guarantee that the selected attributes have clear semantic meanings, as many words among the top 1000 would be stop words, abstract nouns, non-visual verbs, etc.? It would be interesting to perform some quantitative analysis to see whether these 1000-dimensional attribute vectors actually carry semantic meanings, or merely capture biases of word distributions from the ground-truth captions. One possible way to justify the importance of semantic attributes is to experiment with ground-truth attributes or attribute classifiers trained on annotated sources such as COCO-Attribute (Patterson et al. 2016) or Visual Genome (Krishna et al. 2016). EXPERIMENTS It would be interesting to analyze how the behavior of the seq-to-seq model changes with respect to the additional attribute inputs. My hypothesis is that the model’s word choices will shift towards words with high scores. This experiments will help us understand how the model can take advantage of the auxiliary attribute inputs. Since the attributes are selected as the most frequent words from COCO, it is likely for the model to overfit the metrics, such as BLEU, that rely on word matching. On the other hand, it has been shown that these automatic caption evaluation metrics do not necessarily correlate with human judgment. Therefore, I think it is necessary to conduct a human study to convince the readers the quality improvement of the proposed model is not caused by overfitting to metrics. SUMMARY This paper demonstrates that image captioning can be improved by having attributes as auxiliary inputs. However, the model has minor novelty given existing work that has explored similar ideas. Besides, more analysis is necessary to demonstrate the semantic meanings of attributes. A human study is recommended to justify the actual performance improvement. Given these points to be improved, I would recommend rejecting this paper in its current form.","['5', '4', '5']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[9, 5, 2, 3, 13]","[15, 11, 8, 8, 19]","[246, 120, 56, 58, 559]","[113, 51, 22, 26, 261]","[90, 55, 26, 29, 153]","[43, 14, 8, 3, 145]","The sentiment score is -60 because the review is generally negative, recommending rejection of the paper in its current form. The reviewer points out several limitations, including lack of novelty, unclear explanations, and the need for more analysis and human studies. However, it's not entirely negative as the reviewer acknowledges some positive aspects like decent performance improvement.

The politeness score is 20 because the language used is professional and constructive, avoiding harsh or rude expressions. The reviewer offers specific suggestions for improvement and explains their reasoning clearly. However, it's not overly polite or effusive, maintaining a neutral, academic tone throughout.",-60,20
Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units,Reject,2017,"['Dan Hendrycks', 'Kevin Gimpel']","[4, 5, 5]","['Ok but not good enough - rejection', '5', '5']","The proposed regularizer seems to be a particular combination of existing methods. Though the implied connection between nonlinearities and stochastic regularizers is intriguing, in my opinion the empirical performance does not exceed the performance achieved by similar methods by a large enough margin to arrive at a meaningful conclusion.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[2, 10]","[8, 16]","[64, 161]","[25, 90]","[37, 66]","[2, 5]","The sentiment score is slightly negative (-30) because the reviewer expresses skepticism about the novelty and significance of the proposed method. They state that it seems to be a combination of existing methods and that the empirical performance doesn't exceed similar methods by a meaningful margin. However, the score is not deeply negative as the reviewer acknowledges the 'intriguing' connection implied by the work. The politeness score is slightly positive (20) because the reviewer uses respectful language and frames their criticism as personal opinion ('in my opinion'). They also use neutral academic language throughout, avoiding harsh or dismissive terms. The reviewer maintains a professional tone while expressing their reservations about the work.",-30,20
CAN AI GENERATE LOVE ADVICE?: TOWARD NEURAL ANSWER GENERATION FOR NON-FACTOID QUESTIONS,Reject,2017,"['Makoto Nakatsuji', 'Hisashi Ito', 'Naruhiro Ikeda', 'Shota Sagara', 'Akihisa Fujita']","[4, 4]","['Ok but not good enough - rejection', 'Ok but not good enough - rejection']","Summary: The paper presents an approach – Neural Answer Construction Model for the task of answering non-factoid questions, in particular, love-advice questions. The two main features of the proposed model are the following – 1) it incorporates the biases of semantics behind questions into word embeddings, 2) in addition to optimizing for closeness between questions and answers, it also optimizes for optimum combination of sentences in the predicted answer. The proposed model is evaluated using the dataset from a Japanese online QA service and is shown to outperform the baseline model (Tan et al. 2015) by 20% relatively (6% absolutely). The paper also experiments with few other baseline models (ablations of the proposed model). Strengths: 1. The two motivations behind the proposed approach – need to understand the ambiguous use of words depending on context, and need to generate new answers rather than just selecting from answers held by QA sites – are reasonable. 2. The novelty in the paper involves the following – 1) incorporating biases of semantics behind questions into word embeddings using paragraph2vec like model, modified to take as inputs - words from questions, question title token and question category token, 2) modelling optimum combination of sentences (conclusion and supplement sentences) in the predicted answer, 3) designing abstract scenario for answers, inspired by automated web-service composition framework (Rao & Su (2005)), and 4) extracting important topics in conclusion sentence and emphasizing them in supplemental sentence using attention mechanism (attention mechanism is similar to Tan et al. 2016). 3. The proposed method is shown to outperform the current best method (Tan et al. 2015) by 20% relatively (6% absolutely) which seems to be significant improvement. 4. The paper presents few ablations studies that provide insights on how much different components of the model (such as incorporating biases into word embeddings, incorporating attention from conclusion to supplement) are helping towards performance improvement. Weaknesses/Suggestions/Questions: 1. How are the abstract patterns determined, i,e., how did the authors determine that the answers to love-advice questions generally constitute of sympathy, conclusion, supplement for conclusion and encouragement? How much is the improvement in performance when using abstract patterns compared to the case when not using these patters, i.e. when candidate answers are picked from union of all corpuses rather than picking from respective corpuses (corpuses for sympathy, conclusion etc.). 2. It seems that the abstract patterns are specific to the type of questions. So, the abstract patterns for love-advice will be different from those for business advice. Thus, it seems like the abstract patterns need to be hand-coded for different types and hence one model cannot generalize across different types. 3. The paper should present explicit analysis of how much combinational optimization between sentences help – comparison between model performance with and without combinational optimization keeping rest of the model architecture same. The authors could also plot the accuracy of the model as a function of the combinational optimization scores. This will provide insights into how significant are the combinational optimization scores towards overall model accuracy. 4. Paper says that current systems designed for non-factoid QA cannot generalize to questions outside those stored in QA sites and claims that this is one of the contributions of this paper. In order to ground that claim, the paper should show experimentally how well the proposed method generalized to such out-of-domain questions. Although the questions asked by human experts in the human evaluation were not from the evaluation datasets, the paper should analyze how different those questions were compared to the questions present in the evaluation datasets. 5. For human evaluation, were the outputs of the proposed model and that of the QA-LSTM model judged each judged by both the human experts OR one of the human experts judged the outputs of one system and the other human expert judged the outputs of the other system? If both the sets of outputs were each judged by both the human experts, how were the ratings of the two experts combined for every questions? 6. I wonder why the authors did not do a human evaluation where they just ask human workers (not experts) to compare the output of the proposed model with that of the QA-LSTM model – which of the two outputs they would like to hear when asking for advice. Such an evaluation would not get biased by whether each sentence is good or not, whether the combination is good or not. Looking at the qualitative examples in Table 4, I personally like the output of the QA-LSTM more than that of the proposed model because they seem to provide a direct answer to the question (e.g., for the first example the output of the QA-LSTM says “You should wait until you feel excited”, whereas the output of the proposed model says “It is better to concentrate on how to confess your love to her” which seems a bit indirect to the question asked.) 7. Given a question, is the ground-truth answer different in the two tasks -- answer selection and answer construction? 8. The paper mentions that Attentive LSTM (Tan et al. 2016) is evaluated as the current best answer selection method (section 5.2). So, why is its accuracy lower than that of QA-LSTM in table 1. The authors explain this by pointing out the issue of questions being very long compared to answers and hence the attention being noisy. But, did these issues not exist in the dataset used by Tan et al. 2016? 9. The paper says the proposed method achieves 20% gain over current best (in Conclusion section) where they refer to QA-LSTM as the current best method. However, in the description of Attentive LSTM (section 5.2), the paper mentions that Attention LSTM is the current best method. So, could authors please clarify the discrepancy? 10. Minor correction: remove space between 20 and % in abstract. Review Summary: The problem of non-factoid QA being dealt with in the paper is an interesting and useful problem to solve. The motivations presented in the paper behind the proposed approach are reasonable. The experiments show that the proposed model outperforms the baseline model. However, the use of abstract patterns to determine the answer seems like hand-designing and hence it seems like these abstract patterns need to be designed for every other type of non-factoid question and hence the proposed approach is not generalizable to other types. Also, the paper needs more analysis of the results to provide insights into the contribution of different model components.","['4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']",[13],[16],[32],[19],[4],[9],"The sentiment score is slightly negative (-20) because while the reviewer acknowledges some strengths of the paper, they also point out several weaknesses and areas needing improvement. The review begins with a balanced summary and lists some strengths, but then provides a longer list of weaknesses and suggestions, indicating overall more criticism than praise. The politeness score is moderately positive (60) as the reviewer uses professional and respectful language throughout. They frame criticisms as 'weaknesses/suggestions/questions' rather than direct attacks, and use phrases like 'could authors please clarify' and 'I wonder why' to soften their critiques. The reviewer also acknowledges the interesting nature of the problem and the reasonableness of the authors' motivations, which contributes to the polite tone.",-20,60
Classify or Select: Neural Architectures for Extractive Document Summarization,Reject,2017,"['Ramesh Nallapati', 'Bowen Zhou and Mingbo Ma']","[6, 4, 4]","['Marginally above acceptance threshold', 'Ok but not good enough - rejection', 'Ok but not good enough - rejection']","This paper presents two models for extractive document summarization: the classifier architecture and the selector architecture. These two models basically use either classification or ranking in a sequential order to pick the candidate sentences for summarization. Experiments in this paper show the results are either better or close to the SOTA. Technical comments: - In equation (1), there is a position-relevant component call *positional importance*. I am wondering how important this component is? Is it possible to show the performance without this component? Especially, for the discussion on impact of document structure, when the model is trained on the shuffled order but tested on the original order. - A similar question about equation (1), is the content-richness component really necessary? Since the score function already has salience part, which could measure how important of with respect to the whole document. - For the dynamic summary representation in equation (3), why not use the same updating equation for both training and test procedures? During test time, the model actually knows the decisions that have been made so far by the decoder. In this way, the model will be more consistent during training and test. - I think section 5 is the most interesting part of this paper, and it is also convincing on the difference between the two architectures. - It is a little disappointing that the decoding algorithm used in this paper is too simple. In a minimal case, both of them could use beam search and the results could be better.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[16, 18, 8]","[21, 24, 14]","[80, 250, 57]","[49, 136, 28]","[30, 73, 29]","[1, 41, 0]","The sentiment score is 50 (slightly positive) because the reviewer acknowledges the paper's contributions and experimental results, noting they are 'better or close to the SOTA'. However, they also raise several technical questions and point out some limitations, indicating a balanced view. The politeness score is 70 (fairly polite) as the reviewer uses respectful language throughout, framing criticisms as questions or suggestions (e.g., 'I am wondering...', 'Is it possible to...') rather than direct criticisms. The reviewer also highlights positive aspects, calling section 5 'the most interesting part'. The slightly lower politeness score reflects the direct statement of disappointment about the decoding algorithm, though this is still expressed professionally.",50,70
Classless Association using Neural Networks,Reject,2017,"['Federico Raue', 'Sebastian Palacio', 'Andreas Dengel', 'Marcus Liwicki']","[5, 5, 6]","['5', '5', 'Marginally above acceptance threshold']","The paper presents an alternative way of supervising the training of neural network without explicitly using labels when only link/not-link information is available between pairs of examples. A pair of network is trained each of which is used to supervise the other one. The presentation of the paper is not very clear, the writing can be improved. Some design choice are not explained: Why is the power function used in the E-step for approximating the distribution (section 2.1)? Why do the authors only consider a uniform distribution? I understand that using a different prior breaks the assumption that nothing is known about the classes. However I do not see a practical situations where the proposed setting/work would be useful. Also, there exist a large body of work in semi-supervised learning with co-training based on a similar idea. Overall, I think this work should be clarified and improved to be a good fit for this venue.","['4', '3', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[3, 6, 31, 13]","[9, 12, 37, 19]","[44, 21, 561, 299]","[25, 12, 388, 186]","[17, 9, 105, 69]","[2, 0, 68, 44]","The sentiment score is -50 because the review expresses several criticisms and concerns about the paper, such as unclear presentation, unexplained design choices, and lack of practical usefulness. However, it's not entirely negative as it suggests improvements and acknowledges the potential of the work. The politeness score is 0 (neutral) because the reviewer maintains a professional tone without being overly polite or rude. They express their concerns directly but without harsh language, using phrases like 'can be improved' and 'should be clarified' rather than more negative expressions.",-50,0
Compact Embedding of Binary-coded Inputs and Outputs using Bloom Filters,Reject,2017,"['Joan Serrà', 'Alexandros Karatzoglou']","[3, 6, 6]","['Clear rejection', 'Marginally above acceptance threshold', 'Marginally above acceptance threshold']","Description: This paper aims at compressing binary inputs and outputs of neural network models with unsupervised *Bloom embeddings*. The embedding is based on Bloom filters: projecting an element of a set to different positions of a binary array by several independent hash functions, which allows membership checking with no missed but with possibly some false positives. Inputs and outputs are assumed to be sparse. The nonzero elements of an input are then simply encoded by Bloom filters onto the same binary array. The neural network is run with the embedded inputs. Desired outputs are assumed to be a softmax-type ranking of different alternatives. a sort of back-projection step is needed to recover a probability ranking of the desired ground truth alternatives from the lower-dimensional output. For each ground-truth class, this is simply approximated as a product of the output values at the Bloom-filtered hash positions of that class. The paper simply applies this idea, testing it on seven data sets. Scores and training times are compared to the baseline networks without embeddings. Comparison embedding methods are mostly very traditional (hashing trick, error-correcting output codes, linear projection by canonical correlation analysis) but include one recent pairwise mutual information based approach. Evaluation: It is hard to see a lot of novelty in this paper. The Bloom filters are an existing technique, which is applied very straightforwardly here. The back-projection step of equation 2 is also a straightforward continuous-valued variant of the Bloom-filter membership test. The way of recovering outputs is heuristic, since the neural network inbetween the embedded inputs and outputs is not really aware that the outputs will be run through a further back-projection step. In the comparisons of Table 3, only two embedding dimensionalities are used for each data set. This is insufficient, since it leaves open the question whether other methods could get improved performance for higher/lower embeddings, relative to the proposed method. (In appendix B, Figure 4, authors do compare their method to a variant of it for many different embedding dimensionalities; why not do this for all comparison methods too?) Overall, this seems for the most part too close to off-the-shelf existing embedding to be acceptable. Minor points: As the paper notes, dimensionality reduction of inputs by various techniques is common. The paper lists some simple embeddings such as SVD based ones, CCA etc. but a more thorough review of other approaches including the vast array of nonlinear dimensionality reduction solutions should be mentioned. The experiments in the paper seem to have an underlying assumption that inputs and outputs need to have the same type of embedding dimension. This seems unnecessary.","['3', '4', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[11, 15]","[16, 21]","[119, 90]","[60, 61]","[41, 25]","[18, 4]","The sentiment score is -50 because the reviewer expresses significant criticism of the paper's novelty and methodology. They state it's 'hard to see a lot of novelty' and that it's 'too close to off-the-shelf existing embedding to be acceptable.' However, it's not entirely negative as they acknowledge the paper's efforts and provide constructive feedback. The politeness score is 20 because while the reviewer is critical, they maintain a professional and respectful tone throughout. They use phrases like 'It is hard to see' rather than more direct or harsh language. They also provide specific suggestions for improvement, which is a polite way to offer criticism. The language is formal and objective, avoiding personal attacks or overly negative phrasing.",-50,20
Conditional Image Synthesis With Auxiliary Classifier GANs,Reject,2017,"['Augustus Odena', 'Christopher Olah', 'Jonathon Shlens']","[3, 6, 6]","['Clear rejection', 'Marginally above acceptance threshold', 'Marginally above acceptance threshold']","Apologies for the late review. This submission proposes method for class-conditional generative image modeling using auxiliary classifiers. Compared to normal GANs the generator also receives a randomly sampled class label c from the class distribution. The discriminator has two outputs and two corresponding objectives: determine whether a sample is real or generated, and independently to predict the (real or sampled) class label corresponding to the sample. Figure 2. nicely illustrates related methods - this particular method bears similarities to InfoGANs and Semi-supervised GANs. Compared to infogans, this method also encourages correspondence between the latent c and the real class labels for the real examples (whereas infogans are presented as fully unsupervised). The authors attempt at evaluating the method quantitatively by looking at the discriminability and diversity of samples. It is found - not surprisingly - that higher resolution improves discriminability (because more information is present). Discriminability: Figure 3 doesn’t have legends so it is a bit hard to understand what is going on. Furthermore, my understanding is that when evaluating discriminability the authors downsample and then bicubically upsample the image, which is much more like a blurring, very different from retraining all the models to work on low resolution in the first place. Diversity: The authors try to quantitatively evaluate diversity of samples by measuring the average MS-SSIM between randomly selected pairs of points within each class. I think this method is significantly flawed and limited, for reasons mentioned in (Theis et al, 2015, A note on the evaluation…). In its behaviour, MS-SSIM is not that dissimilar from Euclidean distance - although it is nonlinear and is bounded between -1 and 1. Evaluating diversity/entropy of samples in high dimensions is very hard, especially if the distributions involved are non-trivial for example concentrated around manifolds. Consider for example a generative model which randomly samples just two images. Assuming that the MSSSIM between these two images is -1, this generative model can easily achieve an average MSSSIM score of 0, implying a conclusion that this model has more diversity than the training data itself. Conversely, SSIM is designed not to be sensitive to contrast and average pixel intensity, so if a model is diverse in this sense, that will be ignored by this measure. Overall, the paper proposes a new way to incorporate class labels into training GAN-type models. As far as I know the particular algorithm is novel, but I consider it incremental compared to what has been done before. I think the proposed evaluation metrics are flawed, especially when evaluating the diversity of the samples for the aforementioned reasons.","['4', '5', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[2, 3]","[6, 9]","[38, 22]","[16, 4]","[22, 18]","[0, 0]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects of the paper (e.g., 'nicely illustrates related methods'), they express several criticisms and consider the work 'incremental'. They also find significant flaws in the evaluation metrics. The overall tone suggests the reviewer is not fully convinced by the paper's contributions. The politeness score is moderately positive (50) as the reviewer uses professional language throughout, begins with an apology for the late review, and frames criticisms constructively (e.g., 'I think this method is significantly flawed' rather than attacking the authors directly). However, the review doesn't go out of its way to be overly polite or encouraging, maintaining a neutral, professional tone.",-20,50
Cooperative Training of Descriptor and Generator Networks,Reject,2017,"['Jianwen Xie', 'Yang Lu', 'Ruiqi Gao', 'Song-Chun Zhu', 'Ying Nian Wu']","[4, 3, 6]","['Ok but not good enough - rejection', 'Clear rejection', 'Marginally above acceptance threshold']","The authors proposes an interesting idea of connecting the energy-based model (descriptor) and the generator network to help each other. The samples from the generator are used as the initialization of the descriptor inference. And the revised samples from the descriptor is in turn used to update the generator as the target image. The proposed idea is interesting. However, I think the main flaw is that the advantages of having that architecture are not convincingly demonstrated in the experiments. For example, readers will expect quantative analysis on how initializing with the samples from the generator helps? Also, the only quantative experiment on the reconstruction is also compared to quite old models. Considering that the model is quite close to the model of Kim & Bengio 2016, readers would also expect a comparison to that model. ** Minor - I*m wondering if the analysis on the convergence is sound when considering the fact that samples from SGLD are biased samples (with fixed step size). - Can you explain a bit more on how you get Eqn 8? when p(x|y) is also dependent on W_G?","['3', '4', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[9, 4, 24, 22]","[15, 7, 30, 28]","[77, 15, 522, 201]","[36, 7, 264, 79]","[33, 6, 167, 85]","[8, 2, 91, 37]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the idea as 'interesting', they express significant concerns about the lack of convincing demonstrations and comparisons in the experiments. The phrase 'main flaw' indicates a notable criticism. However, the tone is not entirely negative, as the reviewer sees potential in the idea. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, acknowledging the interesting aspects of the work and framing criticisms as suggestions or questions rather than harsh statements. Phrases like 'I think' and 'I'm wondering' contribute to a polite tone. The reviewer also provides specific, constructive feedback for improvement, which is a polite approach in academic reviews.",-20,50
Cortical-Inspired Open-Bigram Representation for Handwritten Word Recognition,Reject,2017,"['Théodore Bluche', 'Christopher Kermorvant', 'Claude Touzet', 'Hervé Glotin']","[5, 4, 7]","['5', 'Ok but not good enough - rejection', 'Good paper, accept']","This paper explores the use of Open Bigrams as a target representation of words, for application to handwriting image recognition. Pros: - The use of OBs is novel and interesting. - Clearly written and explained. Cons: - No comparison to previous state of the art, only with author-generated results. - More ablation studies needed -- i.e. fill in Table3 with rnn0,1 rnn0,1,2 rnn0,1* etc etc. It is not clear where the performance is coming from, as it seems that it is single character modelling (0) and word endings (*) that are actually beneficial. - While the use of Open bigrams is novel, there are works which use bag of bigrams and ngrams as models which are not really compared to or explored. E.g. https://arxiv.org/abs/1406.2227 [1] and https://arxiv.org/abs/1412.5903 [2]. Both use bag of ngrams models and achieve state of the art results, so it would be interesting to see whether open bigrams in the same experimental setup as [1] would yield better results. - Why not use a graph-based decoder like in Fig 2 b? Overall an interesting paper but the lack of comparisons and benchmarks makes it difficult to assess the reality of the contributions.","['5', '4', '5']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[5, 19, 28, 20]","[8, 25, 28, 26]","[26, 72, 17, 148]","[20, 52, 12, 115]","[6, 14, 0, 17]","[0, 6, 5, 16]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('novel and interesting', 'clearly written'), they express several significant concerns and limitations of the paper. The cons outweigh the pros, and the overall tone suggests that substantial improvements are needed. The politeness score is moderately positive (50) as the reviewer uses professional and respectful language throughout, balancing criticism with recognition of the paper's strengths. They offer constructive suggestions for improvement rather than harsh criticism, and use phrases like 'interesting paper' even while pointing out shortcomings.",-20,50
Counterpoint by Convolution,Reject,2017,"['Cheng-Zhi Anna Huang', 'Tim Cooijmans', 'Adam Roberts', 'Aaron Courville', 'Douglas Eck']","[5, 6, 6]","['5', 'Marginally above acceptance threshold', 'Marginally above acceptance threshold']","The paper tackles the task of music generation. They use an orderless NADE model for the task of *fill in the notes*. Given a roll of T timesteps of pitches, they randomly mask out some pitches, and the model is trained to predict the missing notes. This follows how the orderless NADE model can be trained. During sampling, one normally follows an ancestral sampling procedure. For this, an ordering is defined over outputs, and one runs the model on the current input, samples one of the outputs according to the order, adds this output to the next input, and continues this procedure until all outputs have been sampled. The key point of the paper is that this is a bad sampling strategy. Instead, they suggest the strategy of Yao et al. 2014, which uses a blocked Gibbs sampling approach. The blocked Gibbs strategy instead masks N inputs randomly and independently, samples them, and repeats this procedure. The point of this strategy is the make sure the sampling chain mixes well, which will happen for large N. However, since the samples are independent, having a large N gives incoherent samples. Thus, the authors follow an annealed schedule for N, making it smaller over time, which will eventually reduce to ancestral sampling (giving global structure to the sample). They conduct a variety of experiments involving both normal metrics and human evaluations, and find that this blocked Gibbs sampling outperforms other sampling procedures. This is a well written paper - great job. My main problem with the paper is that having read Uria and Yao, I don*t know how much I have learned from this work in the context of this being an ICLR submission. If this was submitted to some computational music / art conference, this paper would be a clear accept. However, for ICLR, I don*t see enough novelty compared with previous works this builds upon. Orderless NADE is an established model. The blocked Gibbs sampling and annealing scheme are basically the exact same one used in Yao. Thus, the main novelty of this paper is its application to the music domain, and finding that Yao*s method works better for sampling music. This is a good contribution, but more tailored to those working in the music domain. If the authors found that these results also hold for other domains like images (e.g. on CIFAR / tiny Imagenet) and text (e.g. document generation), then I would change my mind and accept this paper for ICLR. Even just trying musical domains other than Bach chorales would be useful. However, as it stands, the experiments are not convincing enough.","['4', '3', '5']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[8, 4, 11, 17, 19]","[13, 10, 17, 23, 24]","[24, 13, 65, 309, 89]","[14, 6, 24, 135, 59]","[9, 7, 34, 160, 22]","[1, 0, 7, 14, 8]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the paper is well-written and has some merits, they express significant concerns about its novelty and suitability for ICLR. The reviewer states, 'I don't see enough novelty compared with previous works this builds upon' and suggests the paper might be more suitable for a computational music conference. The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, offers specific praise ('This is a well written paper - great job'), and provides constructive feedback. They explain their concerns clearly and offer suggestions for improvement, maintaining a professional and courteous tone even while critiquing the work.",-20,60
DRAGNN: A Transition-Based Framework for Dynamically Connected Neural Networks,Reject,2017,"['Lingpeng Kong', 'Chris Alberti', 'Daniel Andor', 'Ivan Bogatyy', 'David Weiss']","[6, 7, 5]","['Marginally above acceptance threshold', 'Good paper, accept', '5']","The paper proposes a new neural architecture, called DRAGNN, for the transition-based framework. A DRAGNN uses TBRUs which are neural units to compute hidden activations for the current state of a transition-based system. The paper proves that DRAGNNs can cover a wide range of transition-based methods in the literature. In addition, one can easily implement multitask learning systems with DRAGNNs. The experimental results shows that using DRAGNNs the authors built (near) state-of-the-art systems for 2 tasks: parsing and summarization. The paper contains two major parts: DRAGNN and demonstrations of its usages. Regarding to the first part, the proposed DRAGNN is a neat tool for building any transition-based systems. However, it is difficult to say whether the DRAGNN is novel. Transition-based framework is already well defined and there*s a huge trend in NLP using neural networks to implement transition-based systems. In my opinion, the difference between the Stack-LSTM (Dyer et al., 2015) and DRAGNN is slight. Of course, the DRAGNN is a powerful architecture but the contribution here should be considered mainly in terms of software engineering. In the second part, the authors used DRAGNN to implement new transition-based systems for different (multi-)tasks. The implementations are neat, confirming that DRAGNN is a powerful architecture, especially for multitask learning. However, we should bear in mind that the solutions employed are already there in the literature, thus making difficult to judge the novelty of this part w.r.t. the theme of the conference.","['3', '4', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[7, 11, 2, 5, 3]","[13, 17, 8, 5, 7]","[106, 38, 22, 5, 20]","[34, 14, 8, 3, 8]","[66, 20, 13, 2, 12]","[6, 4, 1, 0, 0]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the DRAGNN as a 'neat tool' and 'powerful architecture', they express doubts about its novelty and contribution. The reviewer suggests that the main contribution is in software engineering rather than a novel approach. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, acknowledging the strengths of the work while also expressing concerns. They use phrases like 'neat tool', 'powerful architecture', and 'neat implementations' which are polite ways to give praise. Even when expressing criticism, the reviewer does so in a constructive manner, using phrases like 'it is difficult to say' and 'we should bear in mind' rather than making blunt negative statements.",-20,50
Deep Convolutional Neural Network Design Patterns,Reject,2017,"['Leslie N. Smith', 'Nicholay Topin']","[3, 3, 4]","['Clear rejection', 'Clear rejection', 'Ok but not good enough - rejection']","The paper formulates a number of rules for designing convolutional neural network architectures for image processing and computer vision problems. Essentially, it reads like a review paper about modern CNN architectures. It also proposes a few new architectural ideas inspired by these rules. These are experimentally evaluated on CIFAR-10 and CIFAR-100, but seem to achieve relatively poor performance on these datasets (Table 1), so their merit is unclear to me. I*m not sure if such a collection of rules extracted from prior work warrants publication as a research paper. It is not a bad idea to try and summarise some of these observations now that CNNs have been the model of choice for computer vision tasks for a few years, and such a summary could be useful for newcomers. However, a lot of it seems to boil down to common sense (e.g. #1, #3, #7, #11). The rest of it might be more suited for an *introduction to training CNNs* course / blog post. It also seems to be a bit skewed towards recent work that was fairly incremental (e.g. a lot of attention is given to the flurry of ResNet variants). The paper states that *it is universal in all convolutional neural networks that the activations are downsampled and the number of channels increased from the input to the final layer*, which is wrong. We already discussed this previously re: my question about design pattern 5, but I think the answer that was given (*the nature of design patterns is that they only apply some of the time*) does not excuse making such sweeping claims. This should probably be removed. *We feel that normalization puts all the layer*s input samples on more equal footing, which allows backprop to train more effectively* (section 3.2, 2nd paragraph) is very vague language that has many possible interpretations and should probably be clarified. It also seems odd to start this sentence with *we feel*, as this doesn*t seem like the kind of thing one should have an opinion about. Such claims should be corroborated by experiments and measurements. There are several other instances of this issue across the paper. The connection between Taylor series and the proposed Taylor Series Networks seems very tenuous and I don*t think the name is appropriate. The resulting function is not even a polynomial as all the terms represent different functions -- f(x) + g(x)**2 + h(x)**3 + ... is not a particularly interesting object, it is just a nonlinear function of x. Overall, the paper reads like a collection of thoughts and ideas that are not very well delineated, and the experimental results are unconvincing.","['4', '4', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[5, 4]","[11, 9]","[21, 29]","[3, 12]","[15, 17]","[3, 0]","The sentiment score is -60 because the reviewer expresses several criticisms and doubts about the paper's contribution, methodology, and conclusions. They question the merit of the proposed ideas, the novelty of the rules presented, and the appropriateness of some claims. The reviewer also finds the experimental results unconvincing. However, it's not entirely negative as they acknowledge some potential usefulness for newcomers.

The politeness score is 20 because while the reviewer is critical, they maintain a professional and relatively respectful tone throughout. They use phrases like 'I'm not sure' and 'It seems' to soften their criticisms. The reviewer also acknowledges some positive aspects, such as the potential usefulness of summarizing observations. However, there are instances of direct criticism without much softening language, which prevents a higher politeness score.",-60,20
Deep Error-Correcting Output Codes,Reject,2017,"['Guoqiang Zhong', 'Yuchen Zheng', 'Peng Zhang', 'Mengqi Li', 'Junyu Dong']","[3, 3, 3]","['Clear rejection', 'Clear rejection', 'Clear rejection']","The paper proposes a greedy supervised layer-wise initialization strategy for (deep) multi-layer perceptrons. Layer weights are initialized by training linear SVMs for binary classification where the binary targets are constructed as error correcting codes (ECOC, including one-vs-all, one-vs-one and others). The thus pertained model (together with a softmax output layer) is then globally fine-tuned by backdrop with dropout. Note that as a heuristic greedy supervised layer-wise initialization strategy this work is very similar to the author’s other ICLR submission: « Marginal Deep Architectures: Deep learning for Small and Middle Scale Applications ». The two works differ in the supervised initialization strategy employed. While layer-wise initialization strategies are worthy of further exploration, the paper doesn’t convey any insight as to what makes a better strategy. Experimental results are not sufficiently convincing by themselves alone to support a mostly incremental work; in particular I remain unconvinced that competing methods received full proper hyper-parameter tuning of their own. Results showing accuracies as bar graphs make it hard to read-off precise accuracies, and one cannot easily compare with known state-of-the-art performance references on benchmark problems (such as MNIST). CIFAR10 performance seem far from state-of-the-art. Explanations are unnecessarily detailed for standard algorithms (e.g. SVMs) and not sufficiently for aspects specific to the approach such as lesser known ECOC schemes. One important aspect remains unclear regarding the use of SVMs. Did you use linear SVMs as stated in section 3.1 (« In order to take the probabilistic outputs of the base classifiers as new representations of data, we adopt linear support vector machines (linear SVMs) as the binary classifiers ») or kernel SVMs as mentioned later « For all DeepECOC models, we used support vector machines (SVMs) with RBF kernel function». In the latter case, the paper lacks a description of how learned RBF-kernel SVMs are transferred to a deep network layer (does it yield 2 layers the firrst of which would be a large RBF neural layer?) Also in this case of kernel SVMs the computational cost is likely to skyrocket and the method will have scaling issues. Is this the reason why the method is too expensive to use on CIFAR10 from scratch, and prompts doing LBP first? If you used linear SVMs, did you use an efficient implementation specific to linear SVMS (as opposed to generic kernel SVM code with a linear kernel?). Finally for image datasets, a visual comparison of learned filters could help provide some qualitative insight.","['4', '5', '5']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[-3, 0, 7, 13, 14, 17]","[1, 6, 11, 19, 20, 23]","[1, 5, 7, 52, 138, 112]","[1, 1, 4, 37, 48, 37]","[0, 0, 0, 4, 46, 12]","[0, 4, 3, 11, 44, 63]","The sentiment score is -30 because the review is generally critical, pointing out several weaknesses in the paper such as lack of insight, unconvincing experimental results, and unclear explanations. However, it's not entirely negative as it acknowledges that the topic is worthy of exploration. The politeness score is 20 because the reviewer uses professional language and offers constructive criticism. They ask questions for clarification rather than making accusatory statements. The tone is academic and objective, avoiding personal attacks or overly harsh language. However, it's not extremely polite either, as it directly points out flaws without much softening language.",-30,20
Deep Generalized Canonical Correlation Analysis,Reject,2017,"['Adrian Benton', 'Huda Khayrallah', 'Biman Gujral', 'Drew Reisinger', 'Sheng Zhang', 'Raman Arora']","[6, 5, 7]","['Marginally above acceptance threshold', '5', 'Good paper, accept']","The authors propose a method that extends the non-linear two-view representation learning methods, and the linear multiview techniques, and combines information from multiple sources into a new non-linear representation learning techniques. In general, the method is well described and seems to lead to benefits in different experiments of phonetic transcription of hashtag recommendation. Even if the method is mostly a extension of classical tools (the scheme learns a (deep) network for each view essentially), the combination of the different sources of information seems to be effective for the studied datasets. It would be interesting to add or discuss the following issues: - what is the complexity of the proposed method, esp. the representation learning part? - would there by any alternative solution to combine the different networks/views? That could make the proposed solution more novel. - the experimental settings, especially in the synthetic experiments, should be more detailed. If possible, the datasets should be made available to encourage reproducibility. - the related work is far from complete unfortunately, especially from the perspective of the numerous multiview/multi-modal/multi-layer algorithms that have been proposed in the literature, in different applications domaines like image retrieval or classification, or bibliographic data for example (authors like A. Kumar, X. Dong, Ping-Yu Chen, M. Bronstein, and many others have proposed works in that direction in the last 5 years). No need to compare to all these works obviously, but a more complete description of the related could help appreciating better the true benefits of DGCCA.","['4', '5', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is fairly confident that the evaluation is correct']","[7, 24, 23]","[13, 30, 29]","[4, 1340, 277]","[4, 636, 194]","[0, 110, 17]","[0, 594, 66]","The sentiment score is 50 (slightly positive) because the reviewer acknowledges the benefits of the proposed method and its effectiveness, while also providing constructive criticism and suggestions for improvement. The review starts with positive remarks about the method being well-described and showing benefits in experiments. However, it also points out areas that need more detail or discussion, balancing the positive aspects with areas for improvement. The politeness score is 70 (fairly polite) because the reviewer uses respectful language throughout, offering suggestions rather than demands (e.g., 'It would be interesting to add or discuss...'), and acknowledges the potential of the work. The reviewer also provides specific, constructive feedback without using harsh or critical language, maintaining a professional and courteous tone throughout the review.",50,70
Deep Symbolic Representation Learning for Heterogeneous Time-series Classification,Reject,2017,"['Shengdong Zhang', 'Soheil Bahrampour', 'Naveen Ramakrishnan', 'Mohak Shah']","[4, 3, 5]","['Ok but not good enough - rejection', 'Clear rejection', '5']",The paper compare three representation learning algorithms over symbolized sequences. Experiments are executed on several prediction tasks. The approach is potentially very important but the proposed algorithm is rather trivial. Besides detailed analysis on hyper parameters are not described.,"['3', '4', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[9, 10, 8, 15]","[15, 10, 13, 19]","[44, 14, 14, 58]","[19, 6, 6, 28]","[5, 5, 5, 19]","[20, 3, 3, 11]","The sentiment score is slightly negative (-30) because while the reviewer acknowledges the potential importance of the approach, they criticize the algorithm as 'rather trivial' and point out the lack of detailed analysis on hyperparameters. These criticisms outweigh the positive aspects mentioned. The politeness score is neutral (0) as the language used is direct and matter-of-fact, without being particularly polite or rude. The reviewer states their observations plainly without using overly harsh language or softening their criticisms with polite phrases.",-30,0
Deep Unsupervised Clustering with Gaussian Mixture Variational Autoencoders,Reject,2017,"['Nat Dilokthanakul', 'Pedro A. M. Mediano', 'Marta Garnelo', 'Matthew C.H. Lee', 'Hugh Salimbeni', 'Kai Arulkumaran', 'Murray Shanahan']","[8, 4, 4]","['Top 50% of accepted papers, clear accept', 'Ok but not good enough - rejection', 'Ok but not good enough - rejection']","This submission proposes an approach to adapting the variational auto-encoder framework (VAE) to the clustering scenario. First the model has to be adapted (with a Gaussian mixture as a prior) and then the inference has to become consistent (by introducing a regularization term). A general positive point about this paper is that the model construction is kept simple. Indeed, the assumption about the mixture prior is simple but reasonable, and the inference follows the VAE framework where appropriate with only changing parts that do not conform with the clustering task. These changes are also motivated by some analysis. The presentation is also kept simple: the linking to VAE and related methods is made in an clear and honest way, so that it*s easy to follow the paper and understand how everything fits together. Also, the regularization term is a well motivated and reasonable addition. Given the VAE context in this paper, I*d be interested in seeing a discussion on the variance of the samples in (6). A negative issue of this paper is that all crucial regularizations rely upon ad-hoc parameters that control their strength, namely eta (eq. (3)) and alpha (eq. 7). According to the authors adjusting these parameters is crucial, and there seems to be no principled way of adjusting them. It also seems that these two parameters interact, since they both regularize z in different ways. This makes the search space over them grow multiplicatively, since the tuning problem now becomes combinatorial. The authors mention that they tune the trade-off between these two regularizers, but I*d be interested in a comment concerning how this is done (what*s the space of parameters to search on). In practical clustering applications, high sensitivity to tuned parameters is undesirable, since one also needs to cross-validate values of K at the same time. I really liked the experiments section. It is not very exhaustive in terms of comparison, but it is very exploratory in terms of demonstrating the model components, strengths and weaknesses. This is much more useful than reporting unintuitive percentage improvements relative to arbitrarily selected baselines and datasets. Overall, I am a little concerned about the practicality of this approach, given the tuning it requires. However, I am in favor of accepting this paper because it makes its strong and weak points very clear through good explanation and demonstration. Therefore, I expect further research to be built on top of this paper, so that the aforementioned issues will hopefully be alleviated in the future. Finally, the theoretical intuitions given in the paper (and author comments) improve its usefulness as a scientific manuscript.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[2, 2, 2, 3, 2, 2, 31]","[7, 8, 8, 6, 6, 7, 37]","[18, 35, 29, 25, 13, 38, 98]","[5, 5, 9, 9, 6, 11, 50]","[8, 16, 19, 12, 7, 21, 27]","[5, 14, 1, 4, 0, 6, 21]","The sentiment score is 60 (positive) because the reviewer expresses several positive points about the paper, such as praising its simplicity, clear presentation, and well-motivated additions. They also appreciate the exploratory nature of the experiments section. However, they do raise some concerns about the practicality of the approach due to parameter tuning, which prevents the score from being higher. The politeness score is 80 (very polite) because the reviewer uses respectful language throughout, acknowledging both strengths and weaknesses in a constructive manner. They use phrases like 'I really liked' and 'I'd be interested in seeing' which show engagement and courtesy. The reviewer also balances criticism with praise and expresses their overall recommendation in a considerate way.",60,80
Deep Variational Canonical Correlation Analysis,Reject,2017,"['Weiran Wang', 'Xinchen Yan', 'Honglak Lee', 'Karen Livescu']","[5, 5, 7]","['5', '5', 'Good paper, accept']","This paper considers the case where multiple views of data are learned through a probabilistic deep neural network formulation. This makes the model non-linear (unlike e.g. CCA) but makes inference difficult. Therefore, the VAE framework is invoked for inference. In [Ref 1] the authors show that maximum likelihood estimation based on their linear latent model leads to the canonical correlation directions. But in the non-linear case with DNNs it*s not clear (at least with the present analysis) what the solution is wrt to the canonical directions. There*s no such analysis in the paper, hence I find it a stretch to refer to this model as a CCA type of model. In contrast, e.g. DCCA / DCCAE are taking the canonical correlation between features into account inside the objective and provide interpretations. [Ref 1] F. R. Bach and M. I. Jordan. A probabilistic interpretation of canonical correlation analysis. Technical Report 688, 2005. There is also a significant body of very related work on non-linear multi-view models which is not discussed in this paper. For example, there*s been probabilistic non-linear multi-view models [Ref 2, 3], also extended to the Bayesian case with common/private spaces [Ref 4] and the variational / deep learning case [Ref 5]. [Ref 2] Ek et al. Gaussian process latent variable models for human pose estimation. MLMI, 2007. [Ref 3] Shon et al. Learning shared latent structure for image synthesis and robotic imitation. NIPS, 2006. [Ref 4] Damianou et al. Manifold relevance determination. ICML, 2012. [Ref 5] Damianou and Lawrence. Deep Gaussian processes. AISTATS, 2013. I can see the utility of this model as bringing together two elements: multi-view modeling and VAEs. This seems like an obvious idea but to the best of my knowledge it hasn*t been done before and is actually a potentially very useful model. However, the question is, what is the proper way of extending VAE to multiple views? The paper didn*t convince me that VAE can work well with multiple views using the shown straightforward construction. Specifically, VCCA doesn*t seem to promote the state of the art in terms of results (it actually is overall below the SOA), while the VCCA-private seems a quite ill-posed model: the dimensionalities d have to be manually tuned with exhaustive search; further, the actual model does not provide a consinstent way of encouraging the private and common variables to avoid learning redundant information. Relying only on dropout for this seems a quite ad-hoc solution (in fact, from Fig. 4 (ver2) it seems that the dropout rate is quite crucial). Perhaps good performance might be achieved with a lot of tuning (which might be why the FLICKR results got better in ver2 without changing the model), but it seems quite difficult to optimize for the above reasons. From a purely experimental point of view, VCCA-private doesn*t seem to promote the SOA either. Of course one wouldn*t expect any new published paper to beat all previous baselines, but it seems that extension of VAE to multiple views is a very interesting idea which deserves some more investigation of how to do it efficiently. Another issue is the approximate posterior being parameterized only from one of the views. This makes the model less useful as a generic multi-view model, since it will misbehave in tasks other than classification. But if classification is the main objective, then one should compare to a proper classification model, e.g. a feedforward neural network. The plots of Fig. 8 are very nice. Overall, the paper convinced me that there is merit in attaching multiple views to VAE. However, it didn*t convince me a) that the proposed way to achieve this is practical b) that there is a connection to CCA (other than being a method for multiple views). The bottom line is that, although the paper is interesting, it needs a little more work.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[8, 8, 18]","[14, 14, 23]","[37, 107, 177]","[17, 54, 103]","[16, 49, 63]","[4, 4, 11]","The sentiment score is -30 because while the reviewer acknowledges some merit in the paper's approach ('I can see the utility of this model'), they express several significant criticisms and concerns. They state that the paper 'didn't convince me' on key points, that the model seems 'ill-posed', and that it 'needs a little more work'. These criticisms outweigh the positive aspects, resulting in a moderately negative sentiment. The politeness score is 20 because the reviewer uses generally respectful language and acknowledges positive aspects of the work. They phrase criticisms as personal opinions ('I find it a stretch', 'The paper didn't convince me') rather than absolute statements. However, the politeness is not extremely high as the criticism is direct and the tone is primarily focused on the paper's shortcomings rather than on encouragement or positive reinforcement.",-30,20
Deep unsupervised learning through spatial contrasting,Reject,2017,"['Elad Hoffer', 'Itay Hubara', 'Nir Ailon']","[5, 6, 7]","['5', 'Marginally above acceptance threshold', 'Good paper, accept']","This paper proposes an unsupervised training objective based on patch contrasting for visual representation learning using deep neural networks. In particular, the feature representations of the patches from the same image are encouraged to be closer than the those from different images. The distance ratios of positive training pairs are optimized. The proposed method are empirically shown to be effective as an initialization method for supervised training. Strengths: - The training objective is reasonable. In particular, high-level features show translation invariance. - The proposed methods are effective for initializing neural networks for supervised training on several datasets. Weaknesses: - The methods are technically similar to the “exemplar network” (Dosovitskiy 2015). Cropping patches from a single image can be taken as a type of data augmentation, which is comparable to the data augmentation of positive sample (the exemplar) in (Dosovitskiy 2015). - The paper is experimentally misleading. The results reported in this paper are based on fine-tuning the whole network with supervision. However, in Table 2, the results of exemplar convnets (Dosovitskiy 2015) is from unsupervised feature learning (the network is not finetuned with labeled samples, and only a classifier is trained upon the features). Therefore, the comparison is not fair. I suspect that exemplar convnets (Dosovitskiy 2015) would achieve similar improvements from fine-tuning; so, without such comparisons (head-to-head comparison with and without fine-tuning based on the same architecture except for the loss), the experimental results are not fully convincing. Regarding the comparison to “What-where” autoencoder (Zhao et al, 2015), it will be interesting to compare against it in large-scale settings, as shown by Zhang et al, ICML 2016 (Augmenting Supervised Neural Networks with Unsupervised Objectives for Large-Scale Image Classification). Training an AlexNet is not very time-consuming with latest (e.g., TITAN-X level) GPUs. The proposed method seems useful only for natural images where different patches from the same image can be similar to each other.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[3, 4, 14]","[8, 9, 19]","[35, 26, 94]","[14, 11, 36]","[19, 14, 30]","[2, 1, 28]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some strengths of the paper, they also point out significant weaknesses and express skepticism about the experimental results. The reviewer states that the paper is 'experimentally misleading' and that the comparisons are 'not fair', which contributes to the negative sentiment. However, the presence of some positive comments prevents the score from being more negative. The politeness score is moderately positive (50) because the reviewer uses professional and respectful language throughout. They present their criticisms in a constructive manner, using phrases like 'I suspect that' and 'it will be interesting to compare', rather than making blunt or harsh statements. The reviewer also acknowledges the strengths of the paper before discussing its weaknesses, which is a polite approach to criticism.",-20,50
Demystifying ResNet,Reject,2017,"['Sihan Li', 'Jiantao Jiao', 'Yanjun Han', 'Tsachy Weissman']","[4, 5, 4]","['Ok but not good enough - rejection', '5', 'Ok but not good enough - rejection']",I think the write-up can be improved. The results of the paper also might be somewhat misleading. The behavior for when weights are 0 is not revealing of how the model works in general. I think the work also underestimates the effect of the nonlinearities on the learning dynamics of the model.,"['4', '3', '5']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[8, 8, 9, 19]","[14, 14, 15, 25]","[25, 111, 98, 341]","[15, 33, 42, 137]","[3, 60, 41, 98]","[7, 18, 15, 106]","The sentiment score is -40 because the reviewer expresses several criticisms ('can be improved', 'might be somewhat misleading', 'underestimates the effect') without any positive comments, indicating a generally negative view. However, the criticism is not extremely harsh, hence not a lower score. The politeness score is 0 because the language is neutral and professional. The reviewer states opinions directly without using particularly polite phrases, but also without any rude or confrontational language. The tone is matter-of-fact and typical for academic peer review.",-40,0
Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models,Reject,2017,"['Ashwin K Vijayakumar', 'Michael Cogswell', 'Ramprasaath R. Selvaraju', 'Qing Sun', 'Stefan Lee', 'David Crandall', 'Dhruv Batra']","[4, 6, 6]","['Ok but not good enough - rejection', 'Marginally above acceptance threshold', 'Marginally above acceptance threshold']",The paper addresses an important problem - namely on how to improve diversity in responses. It is applaudable that the authors show results on several tasks showing the applicability across different problems. In my view there are two weaknesses at this point 1) the improvements (for essentially all tasks) seem rather minor and do not really fit the overall claim of the paper 2) the approach seems quite ad hoc and it unclear to me if this is something that will and should be widely adopted. Having said this the gist of the proposed solution seems interesting but somewhat premature.,"['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[3, 24, 2, 3, 4, 19, 10]","[6, 30, 8, 4, 10, 25, 16]","[6, 27, 29, 6, 103, 199, 293]","[3, 12, 13, 4, 48, 118, 134]","[3, 14, 15, 2, 52, 56, 143]","[0, 1, 1, 0, 3, 25, 16]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the importance of the problem and the applicability of the results, they also point out two significant weaknesses. The reviewer describes the improvements as 'rather minor' and the approach as 'quite ad hoc', suggesting that the paper's claims may be overstated. The conclusion that the solution seems 'interesting but somewhat premature' further reinforces the slightly negative sentiment.

The politeness score is moderately positive (50) because the reviewer uses respectful and constructive language throughout. They begin by acknowledging the importance of the problem and praising the authors for showing results across different tasks. Even when pointing out weaknesses, the reviewer uses phrases like 'In my view' and 'it is unclear to me', which softens the criticism. The overall tone is professional and aimed at providing constructive feedback rather than harsh criticism.",-20,50
Divide and Conquer with Neural Networks,Reject,2017,"['Alex Nowak', 'Joan Bruna']","[4, 4, 3]","['Ok but not good enough - rejection', 'Ok but not good enough - rejection', 'Clear rejection']","I was holding off on this review hoping to get the missing details from the code at https://github.com/alexnowakvilla/DP, but at this time it*s still missing. After going over this paper couple of times I*m still missing the details necessary to reproduce the experiments. I think this would be a common problem for readers of this paper, so the paper needs to be improved, perhaps with a toy example going through all the stages of learning. As an example of the difficulty, take section 4.3. It talks about training *split block* which is a function that can assign each element to either partition 0 or partition 1. At this point I*m looking at it as a binary classification problem and looking for the parameters, loss, and how this loss is minimized. Instead I get a lot of unexpected information, such as *we must create artificial targets at every node of the generated tree from the available final target partition*. What are these artificial targets, and how do they relate to the problem of training the splitter? An example that explicitly goes through this construction would help with understanding.","['2', '4', '2']","['The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']","[2, 8]","[7, 14]","[6, 162]","[4, 64]","[2, 88]","[0, 10]","The sentiment score is -50 because the reviewer expresses significant concerns about the paper's clarity and reproducibility. They mention 'missing details' and state that the paper 'needs to be improved,' indicating a negative sentiment. However, it's not entirely negative as they provide constructive feedback and suggestions for improvement. The politeness score is 20 because the reviewer maintains a professional tone throughout, avoiding harsh language. They use phrases like 'I think' and 'perhaps,' which soften their criticisms. The reviewer also explains their perspective and offers specific examples to illustrate their points, which is a polite way to provide feedback. However, the score is not higher because the review is direct in its criticism without much cushioning language.",-50,20
DyVEDeep: Dynamic Variable Effort Deep Neural Networks,Reject,2017,"['Sanjay Ganapathy', 'Swagath Venkataramani', 'Balaraman Ravindran', 'Anand Raghunathan']","[6, 7, 6]","['Marginally above acceptance threshold', 'Good paper, accept', 'Marginally above acceptance threshold']","Dyvedeep presents three approximation techniques for deep vision models aimed at improving inference speed. The techniques are novel as far as I know. The paper is clear, the results are plausible. The evaluation of the proposed techniques is does not make a compelling case that someone interested in faster inference would ultimately be well-served by a solution involving the proposed methods. The authors delineate *static* acceleration techniques (e.g. reduced bit-width, weight pruning) from *dynamic* acceleration techniques which are changes to the inference algorithm itself. The delineation would be fine if the use of each family of techniques were independent of the other, but this is not the case. For example, the use of SPET would, I think, conflict with the use of factored weight matrices (I recall this from http://papers.nips.cc/paper/5025-predicting-parameters-in-deep-learning.pdf, but I suspect there may be more recent work). For this reason, a comparison between SPET and factored weight matrices would strengthen the case that SPET is a relevant innovation. In favor of the factored-matrix approach, there would I think be fewer hyperparameters and the computations would make more-efficient use of blocked linear algebra routines--the case for the superiority of SPET might be difficult to make. The authors also do not address their choice of the Xeon for benchmarking, when the use cases they identify in the introduction include *low power* and *deeply embedded* applications. In these sorts of applications, a mobile GPU would be used, not a Xeon. A GPU implementation of a convnet works differently than a CPU implementation in ways that might reduce or eliminate the advantage of the acceleration techniques put forward in this paper.","['3', '3', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[1, 6, 20, 24]","[6, 11, 26, 30]","[5, 83, 253, 356]","[1, 59, 142, 214]","[3, 11, 86, 27]","[1, 13, 25, 115]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the novelty and clarity of the paper, they express significant concerns about the evaluation and applicability of the proposed techniques. The reviewer states that the paper 'does not make a compelling case' for the methods and points out potential conflicts with existing techniques. However, it's not entirely negative as they do recognize some positive aspects.

The politeness score is moderately positive (50) because the reviewer maintains a professional and respectful tone throughout. They offer constructive criticism without using harsh language. The reviewer acknowledges the paper's strengths before delving into concerns, which is a polite approach. They also use phrases like 'I think' to soften their critiques, showing respect for the authors' work while expressing their opinions.

The language used is academic and objective, focusing on the content rather than making personal comments, which contributes to both the neutral-to-slightly-negative sentiment and the polite tone of the review.",-20,50
Dynamic Neural Turing Machine with Continuous and Discrete Addressing Schemes,Reject,2017,"['Caglar Gulcehre', 'Sarath Chandar', 'Kyunghyun Cho', 'Yoshua Bengio']","[6, 4, 7]","['Marginally above acceptance threshold', 'Ok but not good enough - rejection', 'Good paper, accept']","The authors proposed a dynamic neural Turing machine (D-NTM) model that overcomes the rigid location-based memory access used in the original NTM model. The paper has two main contributions: 1) introducing a learnable addressing to NTM. 2) curriculum learning using hybrid discrete and continuous attention. The proposed model was empirically evaluated on Facebook bAbI task and has shown improvement over the original NTM. Pros: + Comprehensive comparisons of feed-forward controllers v.s. recurrent controllers + Encouraging results on the curriculum learning on hybrid discrete and continuous attentions Cons: - Very weak NTM baseline (due to some hyper-parameter engineering?) in Table 1, 31% err. comparing to the NTM 20% err. reported in Table 1 in(Graves et al, 2016, Hybrid computing using a neural network with dynamic external memory). In fact, the NTM baseline in (Graves et al 2016) is better than the proposed D-NTM with GRU controller. Maybe it is worthwhile to reproduce their results using the hyper-parameter setting in their Table2 which could potentially lead to better D-NTM performance? - Section 3 of the paper is hard to follow. The overall clarity of the paper needs improvement.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']",['skipped'],['skipped'],['skipped'],['skipped'],['skipped'],['skipped'],"The sentiment score is slightly positive (20) because the review starts by acknowledging the authors' contributions and lists some pros of the paper. However, it also points out significant cons, which tempers the overall positive sentiment. The politeness score is moderately positive (50) as the reviewer uses neutral language and presents both pros and cons in a balanced manner without using harsh or overly critical language. The reviewer offers constructive feedback and suggestions for improvement, which is a polite approach to peer review. The use of phrases like 'Maybe it is worthwhile' when suggesting improvements also contributes to the polite tone.",20,50
Dynamic Partition Models,Reject,2017,"['Marc Goessling', 'Yali Amit']","[3, 6, 3]","['Clear rejection', 'Marginally above acceptance threshold', 'Clear rejection']","The paper addresses the problem of learning compact binary data representations. I have a hard time understanding the setting and the writing of the paper is not making it any easier. For example I can*t find a simple explanation of the problem and I am not familiar with these line of research. I read all the responses provided by authors to reviewer*s questions and re-read the paper again and I still do not fully understand the setting and thus can*t really evaluate the contributions of these work. The related work section does not exist and instead the analysis of the literature is somehow scattered across the paper. There are no derivations provided. Statements often miss references, e.g. the ones in the fourth paragraph of Section 3. This makes me conclude that the paper still requires significant work before it can be published.","['4', '3', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[12, 16, 20, 19]","[13, 21, 26, 24]","[4, 84, 82, 82]","[1, 49, 63, 63]","[1, 18, 2, 3]","[2, 17, 17, 16]","The sentiment score is -70 because the reviewer expresses significant difficulty understanding the paper and concludes that it requires 'significant work before it can be published'. This indicates a strongly negative sentiment towards the current state of the paper. The reviewer points out multiple issues such as lack of clear explanation, missing related work section, and absence of derivations. The politeness score is -20 because while the reviewer isn't overtly rude, the language is quite direct and critical without much attempt to soften the criticism or offer encouragement. Phrases like 'I have a hard time understanding' and 'I still do not fully understand' are somewhat blunt. The reviewer doesn't use any particularly polite language or offer any positive feedback, which contributes to the slightly negative politeness score.",-70,-20
ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation,Reject,2017,"['Adam Paszke', 'Abhishek Chaurasia', 'Sangpil Kim', 'Eugenio Culurciello']","[4, 4, 5, 3]","['Ok but not good enough - rejection', 'Ok but not good enough - rejection', '5', 'Clear rejection']","Paper summary: this work presents ENet, a new convnet architecture for semantic labeling which obtains comparable performance to the previously existing SegNet while being ~10x faster and using ~10x less memory. Review summary: Albeit the results seem interesting, the paper lacks detailed experimental results, and is of limited interest for the ICLR audience. Pros: * 10x faster * 10x smaller * Design rationale described in detail Cons: * The quality of the reference baseline is low. For instance, cityscapes results are 58.3 IoU while state of the art is ~80 IoU. Thus the results are of limited interest. * The results that support the design rationale are not provided. It is important to provide the experimental evidence to support each claim. Quality: the work is interesting but feels incomplete. If your model is 10x faster and smaller, why not try build a model 10x longer to obtain improved results ? The paper focuses only on nimbleness at the cost of quality (using a weak baseline). This limits the interest for the ICLR audience. Clarity: the overall text is somewhat clear, but the model description (section 3) could be more clear. Originality: the work is a compendium of “practitioners wisdom” applied to a specific task. It has thus limited originality. Significance: I find the work that establishes a new “best practices all in one” quite interesting, but however these must shine in all aspects. Being fast at the cost of quality, will limit the impact of this work. Minor comments: * Overall the text is proper english but the sentences constructions is often unsound, specific examples below. * To improve the chances of acceptance, I invite the authors to also explore bigger models and show that the same “collected wisdom” can be used both to reach high speed and high quality (with the proper trade-off curve being shown). Aiming for only one end of the quality versus speed curve limits too much the paper. * Section 1: “mobile or battery powered … require rates > 10 fps“. 10 fps with which energy budget ? Should not this be > 10 fps && < X Watt. * “Rules and ideas” -> rules seem too strong of a word, “guidelines” ? * “Is of utmost importance” -> “is of importance” (important is already important) * “Presents a trainable network … therefore we compare to … the large majority of inference the same way”; the sentence makes no sense to me, I do not see the logical link between before and after “therefore” * Scen-parsing -> scene-parsing * It is arguable if encoder and decoder can be called “separate” * “Unlike in Noh” why is that relevant ? Make explicit or remove * “Real-time” is vague, you mean X fps @ Y W ? * Other existing architectures -> Other architectures * Section 3, does not the BN layer include a bias term ? Can you get good results without any bias term ? * Table 1: why is the initial layer a downsampling one, since the results has half the size of the input ? * Section 4, non linear operations. What do you mean by “settle to recurring pattern” ? * Section 4, dimensionality changes. “Computationally expensive”, relative to what ? * Section 4, dimensionality changes. “This technique ... speeds-up ten times”, but does not provide the same results. Without an experimental validation changing an apple for an orange does not make the orange better than the apple. * Section 4, dimensionality changes. “Found one problem”, problem would imply something conceptually wrong. This is more an “issue” or an “miss-match” when using ResNet for semantic labelling. * Section 4, factorizing filters. I am unsure of why you call nx1 filter asymmetric. A filter could be 1xn yet be symmetric (e.g. -2 -1 0 1 2). Why not simply call them rectangular filters ? * Section 4, factorizing filters. Why would this change increase the variety ? I would have expected the opposite. * Section 4, regularization. Define “much better”. * Section 5.1; “640x360 is adequate for practical applications”; for _some_ applications. * Section 5.2, “very quickly” is vague and depends on the reader expectations, please be quantitative. * Section 5.2, Haver -> have * Section 5.2, in this work -> In this work * Section 5.2, unclear what you use the class weighting for. Is this for class balancing ? * Section 5.2, Cityscapes was -> Cityscapes is * Section 5.2, weighted by the average -> is each instance weighted relative the average object size. * Section 5.2, fastest model in the Cityscapes -> fastest model in the public Cityscapes","['4', '4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[2, 2, 4, 18]","[8, 7, 10, 24]","[20, 5, 24, 116]","[4, 1, 9, 80]","[13, 4, 12, 17]","[3, 0, 3, 19]","The sentiment score is -30 because the review is generally critical, pointing out several cons and limitations of the paper. The reviewer states that the work 'feels incomplete' and is of 'limited interest for the ICLR audience'. However, it's not entirely negative as the reviewer acknowledges some pros and the potential interest of the work. The politeness score is 20 because the reviewer maintains a professional tone throughout, using phrases like 'I invite the authors to' and providing constructive feedback. The reviewer also balances criticism with positive remarks. However, some direct criticisms and the overall negative sentiment prevent a higher politeness score. The reasoning is based on the balance of positive and negative comments, the specific language used, and the overall tone of the review.",-30,20
Efficient Communications in Training Large Scale Neural Networks,Reject,2017,"['Linnan Wang', 'Wei Wu', 'George Bosilca', 'Richard Vuduc', 'Zenglin Xu']","[5, 5]","['5', '5']","This paper presents a linear pipeline All-reduce approach for parallel neural networks on multiple GPU. The paper provides both theoretical analysis and experiments. Overall, the results presented in the paper are interesting, but the writing can be improved. Comments: - The authors compare their proposed approach with several alternative approaches and demonstrate strong performance of the proposed approaches. But it is unclear if the improvement is from the proposed approach or from the implementation. - The paper is not easy to follow and the writing can be improved in many place (aside from typos and missing references). Specifically, the authors should provide more intuitions of the proposed approach in the introduction and in Section 3. - The proposition and the analysis in Section 3.2 do not suggest the communication cost of linear pipeline is approximately 2x and log p faster than BE and MST, respectively, as claimed in many places in the paper. Instead, it suggests LP *cannot* be faster than these methods by 2x and log p times. More specifically, Eq (2) shows T_broadcase_BE/ T_broadcase_LP < 2. This does not provide an upper-bound of T_broadcase_LP and it can be arbitrary worse when comparing with T_broadcase_BE from this inequality. Therefore, instead of showing T_broadcase_BE/ T_broadcase_LP < 2, the authors should state T_broadcase_BE/ T_broadcase_LP > 1 when n approaches infinity. - It would be interesting to emphasize more on the differences between designing parallel algorithms on CPU v.s. on GPU to motivate the paper.","['3', '5']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[11, 3, 9, 17, 18, 2, 5, 14]","[17, 8, 14, 22, 23, 5, 11, 20]","[41, 32, 26, 165, 136, 17, 15, 267]","[19, 15, 18, 120, 95, 7, 9, 107]","[6, 15, 5, 7, 16, 8, 4, 89]","[16, 2, 3, 38, 25, 2, 2, 71]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges that the results are 'interesting', they also point out several significant issues with the paper. The reviewer mentions that the writing needs improvement, the paper is not easy to follow, and there are concerns about the accuracy of some claims and analyses. However, the score is not deeply negative as the reviewer does see value in the work.

The politeness score is moderately positive (50) because the reviewer maintains a professional and constructive tone throughout. They begin with a positive note about the 'interesting' results, and frame their criticisms as suggestions for improvement rather than harsh criticisms. The language used is respectful and focuses on the content of the paper rather than making personal comments about the authors. However, the score is not extremely high as the review is direct in its criticisms without excessive softening language.",-20,50
Efficient Summarization with Read-Again and Copy Mechanism,Reject,2017,"['Wenyuan Zeng', 'Wenjie Luo', 'Sanja Fidler', 'Raquel Urtasun']","[5, 6, 5]","['5', 'Marginally above acceptance threshold', '5']","Summary: This paper proposes a read-again attention-based representation of the document with the copy mechanism for the summarization task. The model reads each sentence in the input document twice and creates a hierarchical representation of it instead of a bidirectional RNN. During the decoding, it uses the representation of the document obtained via the read-again mechanism and points the words that are OOV in the source document. The model does abstractive summarization. The authors show improvements on DUC 2004 dataset and provide an analysis of their model with different configurations. Contributions: The main contribution of this paper is the read-again attention mechanism where the model reads the same sentence twice and obtains a better representation of the document. Writing: The text of this paper needs more work. There are several typos and the explanations of the model/architecture are not really clear, some parts of the paper feel somewhat bloated. Pros: - The proposed model is a simple extension to the model to the model proposed in [2] for summarization. - The results are better than the baselines. Cons: - The improvements are not that large. - Justifications are not strong enough. - The paper needs a better writeup. Several parts of the text are not using a clear/precise language and the paper needs a better reorganization. Some parts of the text is somewhat informal. - The paper is very application oriented. Question: - How does the training speed when compared to the regular LSTM? Some Criticisms: A similar approach to the read again mechanism which is proposed in this paper has already been explored in [1] in the context of algorithmic learning and I wouldn’t consider the application of that on the summarization task a significant contribution. The justification behind the read-again mechanism proposed in this paper is very weak. It is not really clear why additional gating alpha_i is needed for the read again stage. As authors also suggest, pointer mechanism for the unknown/rare words [2] and it is adopted for the read-again attention mechanism. However, in the paper, it is not clear where the real is the gain coming from, whether from “read-again” mechanism or the use of “pointing”. The paper is very application focused, the contributions of the paper in terms of ML point of view is very weak. It is possible to try this read-again mechanism on more tasks other than summarization, such as NMT, in order to see whether if those improvements are The writing of this paper needs more work. In general, it is not very well-written. Minor comments: Some of the corrections that I would recommend fixing, On page 4: “… better than a single value … ” —> “… scalar gating …” On page 4: “… single value lacks the ability to model the variances among these dimensions.” —> “… scalar gating couldn’t capture the ….” On page 6: “ … where h_0^2 and h_0^*2 are initial zero vectors … “ —> “… h_0^2 and h_0^*2 are initialized to a zero vector in the beginning of each sequence …* There are some inconsistencies for example parts of the paper refer to Tab. 1 and some parts of the paper refer to Table 2. Better naming of the models in Table 1 is needed. The location of Table 1 is a bit off. [1] Zaremba, Wojciech, and Ilya Sutskever. *Reinforcement learning neural Turing machines.* arXiv preprint arXiv:1505.00521 362 (2015). [2] Gulcehre, Caglar, et al. *Pointing the Unknown Words.* arXiv preprint arXiv:1603.08148 (2016).","['4', '4', '5']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[2, 13, 15, 17]","[7, 19, 21, 22]","[40, 49, 298, 375]","[18, 26, 155, 223]","[21, 14, 133, 137]","[1, 9, 10, 15]","The sentiment score is -30 because while the reviewer acknowledges some positive aspects ('The results are better than the baselines'), there are more criticisms and concerns raised. The reviewer points out several weaknesses in the paper, including the need for better writing, weak justifications, and limited improvements. The politeness score is 20 because the reviewer maintains a professional tone throughout, using phrases like 'I would recommend fixing' and 'The paper needs a better writeup' instead of harsh language. However, the criticism is direct and not overly softened, preventing a higher politeness score. The reviewer also provides constructive feedback and specific suggestions for improvement, which contributes to the slightly positive politeness score.",-30,20
Efficient iterative policy optimization,Reject,2017,['Nicolas Le Roux'],"[3, 7, 5]","['Clear rejection', 'Good paper, accept', '5']","This paper presents iterative PoWER, an off-policy variation on PoWER, a policy gradient algorithm in the reward-weighted family. I*m not familiar enough with this type lower bound scheme to comment on it. It looks like the end result is less conservative step sizes in policy parameter space. All expectation-based algorithms (and their KL-regularized cousins a-la TRPO) take smallish steps, and this might be a sensible way to accelerate them. The description of the experiments in Section VI is insufficient for reproducibility. Is *The cart moved right* supposed to be *a positive force is applied to the cart*? How is negative force applied? What is the representation of the state? What is the distribution of initial states? A linear policy is insufficient for swing up and balance of a cart-pole. Are you only doing balancing? What is the noise magnitude of the policy? How was it chosen? How long were the episodes? The footnote at the bottom of page 8 threw me off. If you*re using Newton*s method, where is the discussion of gradients and Hessians? I thought the argmax_theta operator was a stand-in for an EM-style step, which I how I read Eq (8) in the Kober paper. https://papers.nips.cc/paper/3545-policy-search-for-motor-primitives-in-robotics.pdf I might be missing something basic here. The control variates thing seems cool. I only read up on it now and I don*t think I*ve seen it before in the RL literature. Seems like a powerful tool. Section 6.2 has too much business jargon, I could barely read it.","['2', '3', '4']","['The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']",[15],[21],[70],[34],[30],[6],"The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('looks like the end result is less conservative step sizes', 'might be a sensible way to accelerate them', 'control variates thing seems cool'), they also point out several issues and insufficiencies in the paper. The reviewer expresses confusion about certain aspects and states that some parts are difficult to understand or reproduce. The politeness score is slightly positive (20) as the reviewer maintains a professional tone throughout, using phrases like 'I'm not familiar enough' and 'I might be missing something basic here', which show humility. They also offer constructive criticism and suggestions for improvement rather than harsh criticism. However, the review is not overly polite or effusive, maintaining a neutral, professional tone overall.",-20,20
Encoding and Decoding Representations with Sum- and Max-Product Networks,Reject,2017,"['Antonio Vergari', 'Robert Peharz', 'Nicola Di Mauro', 'Floriana Esposito']","[6, 3, 6]","['Marginally above acceptance threshold', 'Clear rejection', 'Marginally above acceptance threshold']","This paper tries to solve the problem of interpretable representations with focus on Sum Product Networks. The authors argue that SPNs are a powerful linear models that are able to learn parts and their combinations, however, their representations havent been fully exploited by generating embeddings. Pros: -The idea is interesting and interpretable models/representations is an important topic. -Generating embeddings to interpret SPNs is a novel idea. -The experiments are interesting but could be extended. Cons: -The author*s contribution isn*t fully clear and there are multiple claims that need support. For example, SPNs are indeed interpretable as is, since the bottom-up propagation of information from the visible inputs could be visualized at every stage, and the top-down parse could be also visualized as it has been done before (Amer & Todorovic, 2015). Another example, Proposition one claims that MPNs are perfect encoder decoders since the max nodes always have one max value, however, what if it was uniformally distributed node, or there are two equal values? Did the authors run into such cases? Did they address all edge cases? -A good comparison could have been against Generative Adversarial Networks (GANs), Generative Stochastic Networks (GSNs) and Variational Autoencoders too since they are the state-of-the-art generative models, rather than comparing with RBMs and Nade. I would suggest that the authors take sometime to evaluate their approach against the suggested methods, and make sure to clarify their contributions and eliminate over claiming statements. I agree with the other comments raised by Anon-Reviewer1.","['4', '3', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[3, 8, 16, 28]","[9, 14, 22, 30]","[58, 53, 114, 330]","[27, 27, 86, 270]","[26, 21, 10, 9]","[5, 5, 18, 51]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('The idea is interesting', 'interpretable models/representations is an important topic'), they also highlight several significant concerns and areas for improvement. The cons outweigh the pros in the review, suggesting overall dissatisfaction with the current state of the paper. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, offers constructive criticism, and provides suggestions for improvement rather than outright dismissal. The reviewer acknowledges the paper's strengths before delving into criticisms and uses phrases like 'I would suggest' to soften their recommendations, maintaining a professional and courteous tone.",-20,50
Energy-Based Spherical Sparse Coding,Reject,2017,"['Bailey Kong', 'Charless C. Fowlkes']","[5, 5, 6]","['5', '5', 'Marginally above acceptance threshold']","The paper introduces an efficient variant of sparse coding and uses it as a building block in CNNs for image classification. The coding method incorporates both the input signal reconstruction objective as well as top down information from a class label. The proposed block is evaluated against the recently proposed CReLU activation block. Positives: The proposed method seems technically sound, and it introduces a new way to efficiently train a CNN layer-wise by combining reconstruction and discriminative objectives. Negatives: The performance gain (in terms of classification accuracy) over the previous state-of-the-art is not clear. Using only one dataset (CIFAR-10), the proposed method performs slightly better than the CRelu baseline, but the improvement is quite small (0.5% in the test set). The paper can be strengthened if the authors can demonstrate that the proposed method can be generally applicable to various CNN architectures and datasets with clear and consistent performance gains over strong CNN baselines. Without such results, the practical significance of this work seems unclear.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[6, 19]","[12, 25]","[8, 139]","[3, 74]","[3, 49]","[2, 16]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positives ('technically sound', 'new way to efficiently train'), they express significant concerns about the practical significance and performance gains of the proposed method. The reviewer states that the improvement is 'quite small' and the 'practical significance of this work seems unclear'. These criticisms outweigh the positive aspects mentioned.

The politeness score is moderately positive (50) because the reviewer uses professional and respectful language throughout. They begin by summarizing the paper objectively, then clearly separate their feedback into 'Positives' and 'Negatives'. The criticism is presented constructively, suggesting ways to strengthen the paper ('can be strengthened if...'). The reviewer avoids harsh or personal language, maintaining a professional tone while providing honest feedback.",-20,50
Enforcing constraints on outputs with unconstrained inference,Reject,2017,"['Jay Yoon Lee', 'Michael L. Wick', 'Jean-Baptiste Tristan']","[4, 3, 3]","['Ok but not good enough - rejection', 'Clear rejection', 'Clear rejection']","This paper attempted to solve an interesting problem -- incorporating hard constraints in seq2seq model. The main idea is to modify the weight of the neural network in order to find a feasible solution. Overall, the idea presented in the paper is interesting, and it tries to solve an important problem. However, it seems to me the paper is not ready to publish yet. Comments: - The first section of the paper is clear and well-motivated. - The authors should report test running time. The proposed approach changes the weight matrix. As a result, it needs to reevaluate the values of hidden states and perform the greedy search for each iteration of optimizing Eq (7). This is actually pretty expensive in comparison to running the beam search or other inference methods. Therefore, I*m not convinced that the proposed approach is a right direction for solving this problem (In table, 1, the authors mention that they run 100 steps of SGD). - If I understand correctly, Eq (7) is a noncontinuous function w.r.t W_lambda and the simple SGD algorithm will not be able to find its minimum. - For dependency parsing, there are standard splits of PTB. I would suggest the authors follow the same splits of train, dev, and test in order to compare with existing results. Minor comments: several sentences are misleading and should be rewritten carefully. - Beginning of Section 3: *A major advantage of neural network is that once trained, inference is extremely efficient.* This sentence is not generally right, and I guess the authors mean if using greedy search as inference method, the inference is efficient. - The description in the end of section 2 is awkward. To me, feed-forward and RNN are general families that cover many specific types of neural networks, and the training procedures are not necessarily to aim to optimize Eq. (2). Therefore, the description here might not be true. In fact, I don*t think there is a need to bring up feed-forward networks here; instead, the authors should provide more details the connection between RNN and Eq (2) here. - The second paragraph of section 3 is related to [1], where it shows the search space of the inference can be represented as an imperative program. [1] Credit assignment compiler for joint prediction, NIPS 2016","['5', '4', '4']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[5, 12, 10, 40]","[11, 17, 15, 44]","[21, 40, 33, 364]","[15, 30, 22, 257]","[6, 9, 9, 52]","[0, 1, 2, 55]","The sentiment score is -30 because while the reviewer acknowledges the paper's interesting idea and importance of the problem, they state that 'the paper is not ready to publish yet' and raise several significant concerns. This indicates a generally negative sentiment, though not extremely so. The politeness score is 50 because the reviewer uses respectful language throughout, offering constructive criticism and suggestions rather than harsh criticism. They begin with positive points and use phrases like 'I would suggest' and 'If I understand correctly,' which maintain a polite tone. However, the review doesn't go out of its way to be overly polite or complimentary, keeping it from scoring higher.",-30,50
Exploring LOTS in Deep Neural Networks,Reject,2017,"['Andras Rozsa', 'Manuel Gunther', 'Terrance E. Boult']","[6, 6, 6]","['Marginally above acceptance threshold', 'Marginally above acceptance threshold', 'Marginally above acceptance threshold']","This paper proposes the Layerwise Origin Target Synthesis (LOTS) method, which entails computing a difference in representation at a given layer in a neural network and then projecting that difference back to input space using backprop. Two types of differences are explored: linear scalings of a single input’s representation and difference vectors between representations of two inputs, where the inputs are of different classes. In the former case, the LOTS method is used as a visualization of the representation of a specific input example, showing what it would mean, in input space, for the feature representation to be supressed or magnified. While it’s an interesting computation to perform, the value of the visualizations is not very clear. In the latter case, LOTS is used to generate adversarial examples, moving from an origin image just far enough toward a target image to cause the classification to flip. As expected, the changes required are smaller when LOTS targets a higher layer (in the limit of targetting the last layer, results similar to the original adversarial image results would be obtained). The paper is an interesting basic exploration and would probably be a great workshop paper. However, the results are probably not quite compelling enough to warrant a full ICLR paper. A few suggestions for improvement: - Several times it is claimed that LOTS can be used as a method for mining for diverse adversarial examples that could be used in training classifiers more robust to adversarial perturbation. But this simple experiment of training on LOTS generated examples isn’t tried. Showing whether the LOTS method outperforms, say, FGS would go a long way toward making a strong paper. - How many layers are in the networks used in the paper, and what is their internal structure? This isn’t stated anywhere. I was left wondering whether, say, in Fig 2 the CONV2_1 layer was immediately after the CONV1_1 layer and whether the FC8 layer was the last layer in the network. - In Fig 1, 2, 3, and 4, results of the application of LOTS are shown for many intermediate layers but miss for some reason applying it to the input (data) layer and the output/classification (softmax) layer. Showing the full range of possible results would reinforce the interpreatation (for example, in Fig 3, are even larger perturbations necessary in pixel space vs CONV1 space? And does operating directly in softmax space result in smaller perturbations than IP2?) - The PASS score is mentioned a couple times but never explained at all. E.g. Fig 1 makes use of it but does not specify such basics as whether higher or lower PASS scores are associated with more or less severe perturbations. A basic explanation would be great. - 4.2 states “In summary, the visualized internal feature representations of the origin suggest that lower convolutional layers of the VGG Face model have managed to learn and capture features that provide semantically meaningful and interpretable representations to human observers.” I don’t see that this follows from any results. If this is an important claim to the paper, it should be backed up by additional arguments or results. 1/19/17 UPDATE AFTER REBUTTAL: Given that experiments were added to the latest version of the paper, I*m increasing my review from 5 -> 6. I think the paper is now just on the accept side of the threshold.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[3, 9, 32]","[6, 15, 38]","[24, 52, 218]","[10, 28, 142]","[12, 16, 39]","[2, 8, 37]","The sentiment score is slightly negative (-20) because while the reviewer finds the paper 'interesting', they state it is not compelling enough for a full ICLR paper and suggest it would be better suited as a workshop paper. They provide several suggestions for improvement, indicating the current work is lacking. However, the tone is not harshly critical, hence only mildly negative. The politeness score is moderately positive (50) as the reviewer uses polite language throughout, acknowledging the interesting aspects of the work and framing criticisms as suggestions for improvement rather than direct attacks. They use phrases like 'interesting basic exploration' and 'a few suggestions for improvement' which maintain a respectful tone. The reviewer also updated their score positively after the authors' rebuttal, showing fairness and openness to improvement.",-20,50
Extensions and Limitations of the Neural GPU,Reject,2017,"['Eric Price', 'Wojciech Zaremba', 'Ilya Sutskever']","[5, 4, 5]","['5', 'Ok but not good enough - rejection', '5']","Overall the paper has the feel of a status update by some of the best researchers in the field. The paper is very clear, the observations are interesting, but the remarks are scattered and don*t add up to a quantum of progress in the study of what can be done with the Neural GPU model. Minor remark on the use of the term RNN in Table 1: I found Table 1 confusing because several of the columns are for models that are technically RNNs, and use of RNNs for e.g. translation and word2vec highlight that RNNs can be characterized in terms of the length of their input sequence, the length of their input, and the sizes (per step) of their input, output, and working memories. Basic model question: How are inputs presented (each character 1-hot?) and outputs retrieved when there are e.g. 512 “filters” in the model ? If inputs and outputs are 1-hot encoded, and treated with the same filters as intermediate layers, then the intermediate activation functions should be interpretable as digits, and we should be able to interpret the filters as implementing a reliable e.g. multiplication-with-carry algorithm. Looking at the intermediate values may shed some light on why the usually-working models fail on e.g. the pathological cases identified in Table 3. The preliminary experiment on input alignment is interesting in two ways: the seeds for effective use of an attentional mechanism are there, but also, it suggests that the model is not presently dealing with general expression evaluation the way a correct algorithm should. The remarks in the abstract about improving the memory efficiency of Neural GPU seem overblown -- the paragraph at the top of page 6 describes the improvements as using tf.while_loop instead of unrolling the graph, and using swap_memory to use host memory when GPU memory runs short. These both seem like good practice, but not a remarkable improvement to the efficiency of the model, in fact it would likely slow down training and inference when memory does in fact fit in the GPU. The point about trying many of random seeds to get convergence makes me wonder if the Neural GPU is worth its computational cost at all, when evaluated as means of learning algorithms that are already well understood (e.g. parsing and evaluating S-exprs). Consider spending all of the computational cycles that go into training one of these models (with the multiple seeds) on a traditional search through program space (e.g. sampling lisp programs or something). The notes on the curriculum strategies employed to get the presented results were interesting to read, as an indication of the lengths to which someone might have to go to train this sort of model, but it does leave this reviewer with the impression that despite the stated extensions of the Neural GPU model it remains unclear how useful it might be to practical problems.","['3', '4', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[11, 10, 11]","[17, 14, 17]","[120, 52, 99]","[60, 24, 49]","[57, 27, 45]","[3, 1, 5]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the paper is clear and has interesting observations, they express several criticisms and doubts about the usefulness and progress of the work. They describe the paper as feeling like a 'status update' rather than significant progress, question the efficiency claims, and express skepticism about the model's practical value. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, acknowledging positives alongside criticisms, and phrases concerns as questions or suggestions rather than harsh criticisms. They use phrases like 'interesting to read' and 'consider' which maintain a collegial tone even when expressing doubts.",-20,50
Fast Adaptation in Generative Models with Generative Matching Networks,Reject,2017,"['Sergey Bartunov', 'Dmitry P. Vetrov']","[4, 5, 7]","['Ok but not good enough - rejection', '5', 'Good paper, accept']","This paper presents a meta-learning algorithm which learns to learn generative models from a small set of examples. It’s similar in structure to the matching networks of Vinyals et al. (2016), and is trained in a meta-learning framework where the inputs correspond to datasets. Results are shown on Omniglot in terms of log-likelihoods and in terms of generated samples. The proposed idea seems reasonable, but I’m struggling to understand various aspects of the paper. The exposition is hard to follow, partly because existing methods are described using terminology fairly different from that of the original authors. Most importantly, I can’t tell which aspects are meant to be novel, since there are only a few sentences devoted to matching networks, even though this work builds closely upon them. (I brought this up in my Reviewer Question, and the paper has not been revised to make this clearer.) I’m also confused about the meta-learning setup. One natural formulation for meta-learning of generative models would be that the inputs consist of small datasets X, and the task is to predict the distribution from which X was sampled. But this would imply a uniform weighting of data points, which is different from the proposed method. Based on 3.1, it seems like one additionally has some sort of query q, but it’s not clear what this represents. In terms of experimental validation, there aren’t any comparisons against prior work. This seems necessary, since several other methods have already been proposed which are similar in spirit.","['4', '3', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[7, 15]","[12, 21]","[23, 135]","[11, 66]","[12, 65]","[0, 4]","The sentiment score is -50 because the reviewer expresses significant concerns and confusion about the paper. They mention struggling to understand various aspects, finding the exposition hard to follow, and being unclear about the novelty of the work. The reviewer also points out the lack of comparisons to prior work. However, the score is not extremely negative as the reviewer acknowledges that the proposed idea seems reasonable.

The politeness score is 20 because the reviewer maintains a professional and respectful tone throughout. They use phrases like 'I'm struggling to understand' and 'I'm confused about' rather than making accusatory statements. The reviewer also acknowledges positive aspects, such as the reasonableness of the idea. However, the score is not higher because the review is quite direct in its criticisms and doesn't use many overtly polite phrases or softening language.",-50,20
FastText.zip: Compressing text classification models,Reject,2017,"['Armand Joulin', 'Edouard Grave', 'Piotr Bojanowski', 'Matthijs Douze', 'Herve Jegou', 'Tomas Mikolov']","[5, 6, 6]","['5', 'Marginally above acceptance threshold', 'Marginally above acceptance threshold']","The paper presents a few tricks to compress a wide and shallow text classification model based on n-gram features. These tricks include (1) using (optimized) product quantization to compress embedding weights (2) pruning some of the vocabulary elements (3) hashing to reduce the storage of the vocabulary (this is a minor component of the paper). The paper focuses on models with very large vocabularies and shows a reduction in the size of the models at a relatively minor reduction of the accuracy. The problem of compressing neural models is important and interesting. The methods section of the paper is well written with good high level comments and references. However, the machine learning contributions of the paper are marginal to me. The experiments are not too convincing mainly focusing on benchmarks that are not commonly used. The implications of the paper on the state-of-the-art RNN text classification models is unclear. The use of (optimized) product quantization for approximating inner product is not particularly novel. Previous work also considered doing this. Most of the reduction in the model sizes comes from pruning vocabulary elements. The method proposed for pruning vocabulary elements is simply based on the assumption that embeddings with larger L2 norm are more important. A coverage heuristic is taken into account too. From a machine learning point of view, the proper baseline to solve this problem is to have a set of (relaxed) binary coefficients for each embedding vector and learn the coefficients jointly with the weights. An L1 regularizer on the coefficients can be used to encourage sparsity. From a practical point of view, I believe an important baseline is missing: what if one simply uses fewer vocabulary elements (e.g based on subword units - see https://arxiv.org/pdf/1508.07909.pdf) and retrain a smaller models? Given the lack of novelty and the missing baselines, I believe the paper in its current form is not ready for publication at ICLR. More comments: - The title does not make it clear that the paper focuses on wide and shallow text classification models. Please revise the title. - The paper cites an ArXiv manuscript by Carreira-Perpinan and Alizadeh (2016) several times, which has the same title as the submitted paper. Please make the paper self-contained and include any supplementary material in the appendix. - In Fig 2 does the square mark PQ or OPQ? The paper does not distinguish OPQ and PQ properly at multiple places especially in the experiments. - The paper argues the wide and shallow models are the state of the art in small datasets. Is this really correct? What about transfer learning?","['4', '4', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[8, 7, 5, 16, 15, 10]","[14, 13, 11, 22, 21, 15]","[130, 93, 75, 103, 178, 83]","[59, 36, 28, 56, 89, 46]","[67, 52, 43, 38, 59, 33]","[4, 5, 4, 9, 30, 4]","The sentiment score is -50 because the reviewer expresses several criticisms and concerns about the paper, stating that the 'machine learning contributions of the paper are marginal', the 'experiments are not too convincing', and that the paper is 'not ready for publication at ICLR'. However, they do acknowledge some positive aspects, such as the importance of the problem and the well-written methods section, which prevents the score from being more negative. The politeness score is 20 because the reviewer maintains a professional and respectful tone throughout, using phrases like 'I believe' and 'Please revise' rather than making harsh demands. They also acknowledge positive aspects of the paper before presenting criticisms. However, the overall critical nature of the review prevents the politeness score from being higher.",-50,20
Fuzzy paraphrases in learning word representations with a lexicon,Reject,2017,"['Yuanzhi Ke', 'Masafumi Hagiwara']","[5, 3, 6]","['5', 'Clear rejection', 'Marginally above acceptance threshold']","This paper proposes a method for estimating the context sensitivity of paraphrases and uses that to inform a word embedding learning model. The main idea and model are presented convincingly and seem plausible. The main weaknesses of the paper are shortcomings in the experimental evaluation and in the model exploration. The evaluation does not convincingly determine whether the model is a significant improvement over simpler methods (particularly those that do not require the paraphrase database!). Likewise, the model section did not convince me that this was the most obvious model formulation to try. The paper would be stronger if model choices were explained more convincingly or - better yet - alternatives were explored. On balance I lean towards rejecting the paper and encouraging the authors to submit a revised and improved version at a near point in the future. Detailed/minor points below: 1) While the paper is grammatically mostly correct, it would benefit from revision with the help of a native English speaker. In its current form long sections are very difficult to understand due to the unconventional sentence structure. 2) The tables need better and more descriptive labels. 3) The results are somewhat inconclusive. Particularly in the analogy task in Table 4 it is surprising that CBOW does better on the semantic aspect of the task than your embeddings which are specifically tailored to be good at this? 4) Why was *Enriched CBOW* not included in the analogy task? 5) In the related work section several papers are mentioned that learn embeddings from a combination of lexica and corpora, yet it is repeatedly said that this was the first work of such a kind / that there hasn*t been enough work on this. That feels a little misleading.","['3', '4', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']",['skipped'],['skipped'],['skipped'],['skipped'],['skipped'],['skipped'],"The sentiment score is -30 because while the reviewer acknowledges some positive aspects ('The main idea and model are presented convincingly and seem plausible'), they ultimately lean towards rejecting the paper due to significant weaknesses in experimental evaluation and model exploration. The overall tone is more critical than positive. The politeness score is 50 because the reviewer uses polite and constructive language throughout, offering specific recommendations for improvement and acknowledging both strengths and weaknesses. They avoid harsh criticism and use phrases like 'The paper would be stronger if...' which suggests a helpful rather than confrontational approach. However, the score is not higher as the review is direct in its criticism and recommendation for rejection.",-30,50
GRAM: Graph-based Attention Model for Healthcare Representation Learning,Reject,2017,"['Edward Choi', 'Mohammad Taha Bahadori', 'Le Song', 'Walter F. Stewart', 'Jimeng Sun']","[6, 6, 6]","['Marginally above acceptance threshold', 'Marginally above acceptance threshold', 'Marginally above acceptance threshold']","This paper addresses the problem of data sparsity in the healthcare domain by leveraging hierarchies of medical concepts organized in ontologies. The paper focuses on sequential prediction given a patient’s medical record (a sequence of medical codes, some of which might occur very rarely). Instead of simply assigning each medical code an independent embedding before feeding it to an RNN, the proposed approach assigns each node in the medical ontology a “basic” embedding, and composes a “final” embedding for each medical code by taking a learned weighted average (via an attention mechanism) of the medical code’s ancestors in the ontology. Notably, the paper is well written and the approach is quite intuitive. I have the following comments: - Why is the patient’s visit taken as just the sum of medical codes found in the visit, and not say the average or a learned weighted average? Wouldn’t this bias for/against the number of codes in the visit? - I don’t see why basic embeddings are not fine tuned as well. Did you find that to hurt performance? Do you have an explanation for that? - Looking at Figure 2, the results seem very close and the figures are not very clear (figure (b) top is missing). Also, I am wondering how significant the differences are so it would be nice to comment on that. Finally, I think this is an interesting application paper applying well-established deep learning techniques. The paper deals with an important issue that arises when applying deep learning models in domains with scarce data resources. However, I would like the authors to comment on what there paper offers as new insights to the ICLR community and why they think ICLR is a good avenue for their work.","['4', '4', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[12, 7, 14, 11, 16]","[18, 12, 20, 15, 22]","[102, 34, 343, 37, 355]","[41, 20, 185, 16, 180]","[56, 12, 122, 9, 109]","[5, 2, 36, 12, 66]","The sentiment score is 50 (slightly positive) because the reviewer acknowledges the paper's merits ('well written', 'intuitive approach') and its importance in addressing data sparsity issues. However, they also raise several questions and suggest improvements, indicating a balanced view. The politeness score is 80 (quite polite) due to the reviewer's constructive tone, use of phrases like 'I would like the authors to comment' and 'it would be nice to', and the absence of harsh criticism. The reviewer maintains a respectful and professional tone throughout, even when suggesting areas for improvement or clarification.",50,80
Gated-Attention Readers for Text Comprehension,Reject,2017,"['Bhuwan Dhingra', 'Hanxiao Liu', 'Zhilin Yang', 'William W. Cohen', 'Ruslan Salakhutdinov']","[6, 7]","['Marginally above acceptance threshold', 'Good paper, accept']","Summary: The authors propose a multi-hop *gated attention* model, which models the interactions between query and document representations, for answering cloze-style questions. The document representation is attended to sequentially over multiple-hops using similarity with the query representation (using a dot-product) as the scoring/attention function. The proposed method improves upon (CNN, Daily Mail, Who-Did-What datasets) or is comparable to (CBT dataset) the state-of-the-art results. Pros: 1. Nice idea on heirarchical attention for modulating the context (document) representation by the task-specific (query) representation. 2. The presentation is clear with thorough experimental comparison with the latest results. Comments: 1. The overall system presents a number of architectural elements: (1) attention at multiple layers (multi-hop), (2) query based attention for the context (or gated attention), (3) encoding the query vector at each layer independently. It is important to breakdown the gain in performance due to the above factors: the ablation study presented in section 4.4 helps establish the importance of Gated Attention (#2 above). However, it is not clear: (1) how much multiple-hops of gated-attention contribute to the performance. (2) how important is it to have a specialized query encoder for each layer. Understanding the above better, will help simplify the architecture. 2. The tokens are represented using L(w) and C(w). It is not clear if C(w) is crucial for the performance of the proposed method. There is a significant performance drop when C(w) is absent (e.g. in *GA Reader--*; although there are other changes in *GA Reader--* which could affect the performance). Hence, it is not clear how much does the main idea, i.e., gated attention contributes towards the superior performance of the proposed method.","['3', '3']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[2, 3, 15, 33, 15]","[8, 9, 21, 39, 21]","[71, 74, 82, 345, 419]","[30, 35, 34, 218, 207]","[39, 33, 35, 80, 201]","[2, 6, 13, 47, 11]","The sentiment score is 50 (slightly positive) because the reviewer acknowledges the paper's strengths ('Nice idea', 'clear presentation', 'thorough experimental comparison') and its improvement over state-of-the-art results. However, they also raise some questions and suggest areas for improvement, balancing the positive aspects. The politeness score is 80 (quite polite) because the reviewer uses respectful language throughout, acknowledging the paper's merits before offering constructive criticism. They use phrases like 'It is important to' and 'It is not clear' rather than more direct or harsh critiques, maintaining a professional and courteous tone.",50,80
Gaussian Attention Model and Its Application to Knowledge Base Embedding and Question Answering,Reject,2017,"['Liwen Zhang', 'John Winn', 'Ryota Tomioka']","[5, 4, 4]","['5', 'Ok but not good enough - rejection', 'Ok but not good enough - rejection']","The contribution of this paper can be summarized as: 1, A TransGaussian model (in a similar idea of TransE) which models the subject / object embeddings in a parameterization of Gaussian distribution. The model can be naturally adapted to path queries like the formulation of (Guu et al, 2015). 2. Along with the entity / relation representations trained by TransGaussian, an LSTM + attention model is built on natural language questions, aiming at learning a distribution (not normalized though) over relations for question answering. 3. Experiments on a generated WorldCup2014 dataset, focusing on path queries and conjunctive queries. Overall, I think the Gaussian parameterization exhibits some nice properties, and could be suitable to KB completion and question answering. However, some details and the main experimental results are not convincing enough to me. The paper writing also needs to be improved. More comments below: [Major comments] - My main concern is that that evaluation results are NOT strong. Either knowledge base completion or KB-based question answering, there are many existing and competitive benchmarks (e.g., FB15k / WebQuestions). Experimenting with such a tiny WordCup2014 dataset is not convincing. Moreover, the questions are just generated by a few templates, which is far from NL questions. I am not even not sure why we need to apply an LSTM in such scenario. The paper would be much stronger if you can demonstrate its effectiveness on the above benchmarks. - Conjunctive queries: the current model assumes that all the detected entities in the question could be aligned to one or more relations and we can take conjunctions in the end. This assumption might be not always correct, so it is more necessary to justify this on real QA datasets. - The model is named as “Gaussian attention” and I kind of think it is not very closely related to well-known attention mechanism, but more related to KB embedding literature. [Minor comments] - I find Figure 2 a bit confusing. The first row of orange blocks denote KB relations, and the second row of those denote every single word of the NL question. Maybe make it clearer? - Besides “entity recognition”, usually we still need an “entity linker” component which links the text mention to the KB entity.","['4', '4', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[11, 18, 12]","[17, 22, 18]","[93, 50, 76]","[43, 33, 38]","[12, 4, 24]","[38, 13, 14]","The sentiment score is slightly negative (-30) because while the reviewer acknowledges some positive aspects ('exhibits some nice properties', 'could be suitable'), they express significant concerns about the experimental results and dataset used. The overall tone suggests the paper needs substantial improvements. The politeness score is mildly positive (20) as the reviewer uses respectful language throughout, offers constructive feedback, and frames criticisms as personal opinions ('not convincing enough to me') rather than absolute statements. They also balance negative points with positive acknowledgments. However, the review doesn't go out of its way to be overly polite or encouraging, maintaining a professional, matter-of-fact tone.",-30,20
Generating Long and Diverse Responses with Neural Conversation Models,Reject,2017,"['Louis Shao', 'Stephan Gouws', 'Denny Britz', 'Anna Goldie', 'Brian Strope', 'Ray Kurzweil']","[5, 7, 7]","['5', 'Good paper, accept', 'Good paper, accept']","The paper proposes modification to seq2seq model to help it handle the problems when long responses are needed. Though the technical contributions may be of value, the work in my personal opinion is not in the right direction towards helping dialog systems. Essentially we try to generate long responses that sound ``nice* yet are not grounded to any reality, they just need to be related to the question and not suffers from obvious mistakes. Yet, the architectural innovations proposed may be of merit.","['3', '4', '3']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[2, 8, 1, 9, 23, 26]","[2, 11, 2, 15, 26, 31]","[2, 16, 10, 24, 36, 26]","[0, 9, 5, 10, 24, 12]","[2, 7, 5, 13, 11, 11]","[0, 0, 0, 1, 1, 3]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges that the technical contributions may be of value, they express skepticism about the overall direction of the work. The phrase 'in my personal opinion is not in the right direction' indicates a negative sentiment towards the paper's approach. However, the reviewer does mention potential merit in the architectural innovations, which prevents the score from being more negative. The politeness score is slightly positive (20) as the reviewer uses respectful language and frames their criticism as personal opinion rather than absolute fact. They also acknowledge potential value in aspects of the work, which adds a polite tone. The use of phrases like 'may be of value' and 'may be of merit' contributes to a diplomatic and considerate tone, even while expressing criticism.",-20,20
Generative Adversarial Networks as Variational Training of Energy Based Models,Reject,2017,"['Shuangfei Zhai', 'Yu Cheng', 'Rogerio Feris', 'Zhongfei Zhang']","[8, 4, 4, 4]","['Top 50% of accepted papers, clear accept', 'Ok but not good enough - rejection', 'Ok but not good enough - rejection', 'Ok but not good enough - rejection']","Our understanding of GAN to date is still vague. Although there have been some efforts relating GAN to energy models, I personally consider that the perspective of this paper, namelying understanding GAN (a variant of GAN, to be more precise) as variational training of an energy model is the most natural and elegant. The derivation up to equation (5) and the reduction to (7) are very nice. I think this is the most important contribution of the paper. The techniques introduced in sections 5 and 6 are somewhat ad hoc, and lack clarity. Referring to the version I looked at (not sure though if it is the latest), section 6 contain some errors/typos (stuff around p_z(x|	ilde{x}). The presentation of section 6 needs to improve in clarity. But I think this does not shadow the main contribution of the paper, namely, that perspectives given in sections 2-4. Overall I very much enjoy the presented insight of this paper into GAN. I do have some comment/question regarding equation (7). This equation formulates a variant of GAN, or a model resembling GAN. I am happy to see that the entropy term pops up there, which should save GAN from degenerating its generative distribution or from missing modes. The swapping of the min-max order in this formulation however makes me wonder if this variant of GAN indeed reflects the *principle* of GAN, or it is in fact a different principle, which happens to gives rise to a model that *resembles* GAN. Of course, my question may be merely philosophical rather than mathematical, and I won*t expect a precise anwer. Nonetheless, if the author can provide additional insignts on this, it would be appreciated.","['5', '3', '5', '5']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[3, 22, 18, 27]","[9, 28, 24, 33]","[55, 189, 250, 299]","[22, 80, 139, 152]","[32, 43, 90, 48]","[1, 66, 21, 99]","The sentiment score is 60 (positive) because the reviewer expresses enjoyment and appreciation for the main contributions of the paper, particularly in sections 2-4. They describe the approach as 'natural and elegant' and state that they 'very much enjoy the presented insight'. However, they also point out some weaknesses in sections 5 and 6, which prevents the score from being higher. The politeness score is 80 (quite polite) because the reviewer uses respectful language throughout, acknowledging the paper's strengths while constructively pointing out areas for improvement. They phrase criticisms gently (e.g., 'somewhat ad hoc', 'needs to improve in clarity') and offer additional insights and questions in a collegial manner. The use of phrases like 'I personally consider' and 'if the author can provide additional insights' further contributes to the polite tone.",60,80
Generative Adversarial Networks for Image Steganography,Reject,2017,"['Denis Volkhonskiy', 'Boris Borisenko', 'Evgeny Burnaev']","[5, 6, 4]","['5', 'Marginally above acceptance threshold', 'Ok but not good enough - rejection']","This paper proposes an interesting application of the GAN framework in steganography domain. In addition to the normal GAN discriminator, there is a steganalyser discriminator that receives the negative examples from the generator and positive examples from the generator images that contain a hidden payload. As a result, the generator, not only learn to generate realistic images by fooling the discriminator of the GAN, but also learn to be a secure container by fooling steganalyser discriminator. The method is tested by training an independent steganalyser S* on real images and generated images. Given that in the ICLR community, not many people are familiar with the literature of steganography, I think this paper should have provided more context about how exactly this method can be used in practice, what are the related works on setganalysis-secure message embedding and probably a more thorough sets of experiments on more than one dataset. The proposed SGAN framework (Figure 2) does make sense to me, and I think it is very general and can have more applications other than the steganography domain. But it is not clear to me why fooling the steganalyser discriminator S, necessarily mean that we can fool an independent discriminator S*? Also I find it surprising that a different seed value, can make such a huge difference in the accuracy. In short, the ideas of this paper are interesting and potentially useful, but I think the presentation of this paper should be improved so that it becomes more suitable for the ICLR and machine learning community.","['3', '4', '3']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']",['skipped'],['skipped'],['skipped'],['skipped'],['skipped'],['skipped'],"The sentiment score is slightly positive (20) because the reviewer describes the paper as 'interesting' and acknowledges its potential usefulness, but also points out several areas for improvement. The reviewer sees value in the proposed framework but feels the presentation and context could be enhanced. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, acknowledging the paper's strengths while offering constructive criticism. They use phrases like 'interesting application' and 'potentially useful' which maintain a positive tone even when suggesting improvements. The reviewer also considers the paper's audience, showing consideration for the authors by suggesting ways to make the paper more suitable for the ICLR community.",20,60
Generative Adversarial Parallelization,Reject,2017,"['Daniel Jiwoong Im', 'He Ma', 'Chris Dongjoo Kim', 'Graham Taylor']","[4, 4, 4]","['Ok but not good enough - rejection', 'Ok but not good enough - rejection', 'Ok but not good enough - rejection']","This paper proposes an extension of the GAN framework known as GAP whereby multiple generators and discriminators are trained in parallel. The generator/discriminator pairing is shuffled according to a periodic schedule. Pros: + The proposed approach is simple and easy to replicate. Cons: - The paper is confusing to read. - The results are suggestive but do not conclusively show a performance win for GAP. The main argument of the paper is that GAP leads to improved convergence and improved coverage of modes. The coverage visualizations are suggestive but there still is not enough evidence to conclude that GAP is in fact improving coverage. And for convergence it is difficult to assess the effect of GAP on the basis of learning curves. The proposed GAM-II metric is circular in that model performance depends on the collection of baselines the model is being compared with. Estimating likelihood via AIS seems to be a promising way to evaluate, as does using the Inception score. Perhaps a more systematic way to determine GAP*s effect would be to set up a grid search of hyperparameters and train an equal number of GANs and GAP-GANs for each setting. Then a histogram over final Inception scores or likelihood estimates of the trained models would help to show whether GAP tended to produce better models. Overall the approach seems promising but there are too many open questions regarding the paper in its current form. * Section 2: *Remark that when...* => seems like a to-do. * Section A.1: The proposed metric is not described in adequate detail.","['3', '4', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[18, -3, -2, 23, -3]","[23, 1, 2, 29, 1]","[24, 2, 4, 244, 1]","[17, 2, 3, 140, 1]","[0, 0, 0, 17, 0]","[7, 0, 1, 87, 0]","The sentiment score is -30 because while the reviewer acknowledges some positive aspects ('Pros'), there are more negative points ('Cons') and overall skepticism about the paper's conclusions. The reviewer states there are 'too many open questions' and that the evidence is not conclusive, indicating a generally negative sentiment. However, it's not extremely negative as the approach is called 'promising'. The politeness score is 50 because the language is professional and constructive, offering specific suggestions for improvement. The reviewer avoids harsh language, instead using phrases like 'suggestive but not conclusive' and 'difficult to assess'. The tone is critical but respectful, maintaining a polite discourse appropriate for academic peer review.",-30,50
Generative Paragraph Vector,Reject,2017,"['Ruqing Zhang', 'Jiafeng Guo', 'Yanyan Lan', 'Jun Xu', 'Xueqi Cheng']","[4, 3, 2]","['Ok but not good enough - rejection', 'Clear rejection', 'Strong rejection']","While this paper has some decent accuracy numbers, it is hard to argue for acceptance given the following: 1) motivation based on the incorrect assumption that the Paragraph Vector wouldn*t work on unseen data 2) Numerous basic formatting and Bibtex citation issues. Lack of novelty of yet another standard directed LDA-like bag of words/bigram model.","['4', '4', '5']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[1, 12, 10, 13, 17]","[7, 18, 16, 19, 23]","[63, 285, 188, 140, 625]","[30, 166, 113, 91, 397]","[28, 87, 61, 34, 149]","[5, 32, 14, 15, 79]","The sentiment score is -70 because the review is predominantly negative, citing incorrect assumptions, formatting issues, and lack of novelty as major problems. The only positive aspect mentioned is 'some decent accuracy numbers', which is outweighed by the criticisms. The politeness score is -30 because while the language isn't overtly rude, it's quite blunt and dismissive. Phrases like 'it is hard to argue for acceptance' and 'incorrect assumption' are direct criticisms without much softening. The reviewer doesn't use any polite phrases or acknowledge potential merits of the work beyond the brief mention of accuracy numbers. The tone is more matter-of-fact than deliberately impolite, hence not an extremely low score.",-70,-30
Gradients of Counterfactuals,Reject,2017,"['Mukund Sundararajan', 'Ankur Taly', 'Qiqi Yan']","[3, 3, 5]","['Clear rejection', 'Clear rejection', '5']","This paper proposes a new method, interior gradients, for analysing feature importance in deep neural networks. The interior gradient is the gradient measured on a scaled version of the input. The integrated gradient is the integral of interior gradients over all scaling factors. Visualizations comparing integrated gradients with standard gradients on real images input to the Inception CNN show that integrated gradients correspond to an intuitive notion of feature importance. While motivation and qualitative examples are appealing, the paper lacks both qualitative and quantitative comparison to prior work. Only the baseline (simply the standard gradient) is presented as reference for qualitative comparison. Yet, the paper cites numerous other works (DeepLift, layer-wise relevance propagation, guided backpropagation) that all attack the same problem of feature importance. Lack of comparison to any of these methods is a major weakness of the paper. I do not believe it is fit for publication without such comparisons. My pre-review question articulated this same concern and has not been answered.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[14, 11, 12]","[20, 16, 17]","[74, 45, 36]","[36, 26, 16]","[25, 17, 12]","[13, 2, 8]","The sentiment score is -50 because while the reviewer acknowledges some positive aspects ('motivation and qualitative examples are appealing'), they express significant concerns about the lack of comparisons to prior work, stating it's a 'major weakness' and that the paper is not fit for publication without such comparisons. This indicates a predominantly negative sentiment, though not extremely negative. The politeness score is 20 because the reviewer uses professional and respectful language throughout, avoiding harsh or rude expressions. They provide constructive criticism and clearly state their concerns without being overly critical or dismissive. However, the tone is more neutral than overtly polite, hence a slightly positive but not high score.",-50,20
Hierarchical Memory Networks,Reject,2017,"['Sarath Chandar', 'Sungjin Ahn', 'Hugo Larochelle', 'Pascal Vincent', 'Gerald Tesauro', 'Yoshua Bengio']","[5, 5, 4]","['5', '5', 'Ok but not good enough - rejection']","1. The hierarchical memory is fixed, not learned, and there is no hierarchical in the experimental section, only one layer for softmax layer. 2. It shows the 10-mips > 100-mips > 1000-mips, does it mean 1-mips is the best one we should adopt? 3. Approximated k-mips is worse than even original method. Why does it need exact k-mips? It seems the proposed method is not robust.","['3', '5', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[5, 2, 14, 3, 10, 11]","[11, 8, 20, 9, 16, 17]","[119, 19, 175, 22, 132, 107]","[54, 13, 109, 12, 46, 44]","[34, 3, 9, 7, 26, 10]","[31, 3, 57, 3, 60, 53]","The sentiment score is -50 because the review points out several significant issues with the paper, suggesting a negative sentiment. The reviewer questions the methodology, results, and robustness of the proposed method, which indicates dissatisfaction with the work. However, it's not entirely negative as the reviewer doesn't outright reject the paper or use harsh language.

The politeness score is 0 (neutral) because the reviewer's language is direct and matter-of-fact without being overtly polite or rude. The comments are presented as straightforward questions and observations without any softening language or personal attacks. The tone is professional and focused on the content of the paper rather than on the authors themselves.",-50,0
Hierarchical compositional feature learning,Reject,2017,"['Miguel Lazaro-Gredilla', 'Yi Liu', 'D. Scott Phoenix', 'Dileep George']","[5, 5, 4]","['5', '5', 'Ok but not good enough - rejection']","The paper discusses a method to learn interpretable hierarchical template representations from given data. The authors illustrate their approach on binary images. The paper presents a novel technique for extracting interpretable hierarchical template representations based on a small set of standard operations. It is then shown how a combination of those standard operations translates into a task equivalent to a boolean matrix factorization. This insight is then used to formulate a message passing technique which was shown to produce accurate results for these types of problems. Summary: ——— The paper presents an novel formulation for extracting hierarchical template representations that has not been discussed in that form. Unfortunately the experimental results are on smaller scale data and extension of the proposed algorithm to more natural images seems non-trivial to me. Quality: I think some of the techniques could be described more carefully to better convey the intuition. Clarity: Some of the derivations and intuitions could be explained in more detail. Originality: The suggested idea is reasonable but limited to binary data at this point in time. Significance: Since the experimental setup is somewhat limited according to my opinion, significance is hard to judge. Details: ——— 1. My main concern is related to the experimental evaluation. While the discussed approach is valuable, its application seems limited to binary images at this point in time. Can the authors comment? 2. There are existing techniques to extract representations of images which the authors may want to mention, e.g., work based on grammars.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[12, 2, 3, 15]","[18, 2, 4, 21]","[63, 4, 10, 43]","[25, 1, 4, 14]","[23, 3, 5, 24]","[15, 0, 1, 5]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the novelty of the approach, they express concerns about the limited experimental setup and applicability to only binary images. The reviewer also mentions that some explanations could be improved. However, it's not entirely negative as they recognize the value of the proposed method. The politeness score is moderately positive (50) because the reviewer uses respectful language throughout, offers constructive criticism, and frames their concerns as questions or suggestions rather than harsh criticisms. They also provide a balanced view, mentioning both strengths and weaknesses of the paper. The tone is professional and courteous, even when pointing out limitations.",-20,50
Higher Order Recurrent Neural Networks,Reject,2017,"['Rohollah Soltani', 'Hui Jiang']","[4, 6, 3]","['Ok but not good enough - rejection', 'Marginally above acceptance threshold', 'Clear rejection']","The authors of the paper explore the idea of incorporating skip connections *over time* for RNNs. Even though the basic idea is not particularly innovative, a few proposals on how to merge that information into the current hidden state with different pooling functions are evaluated. The different models are compared on two popular text benchmarks. Some points. 1) The experiments feature only NLP and only prediction tasks. It would have been nice to see the models in other domains, i.e. modelling a conditional distribution p(y|x), not only p(x). Further, sensory input data such as audio or video would have given further insight. 2) As pointed out by other reviewers, it does not feel as if the comparisons to other models are fair. SOTA on NLP changes quickly and it is hard to place the experiments in the complete picture. 3) It is claimed that this helps long-term prediction. I think the paper lacks a corresponding analysis, as pointed out in an earlier question of mine. 4) It is claimed that LSTM trains slow and is hard to scale. For one does this not match my personal experience. Then, the prevalence of LSTM systems in production systems (e.g. Google, Baidu, Microsoft, …) clearly speaks against this. I like the basic idea of the paper, but the points above make me think it is not ready for publication.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[1, -2, 3, 6]","[5, 2, 7, 10]","[5, 4, 7, 74]","[5, 4, 7, 49]","[0, 0, 0, 16]","[0, 0, 0, 9]","The sentiment score is -50 because while the reviewer acknowledges some positive aspects ('I like the basic idea of the paper'), the overall tone is critical and concludes that the paper is 'not ready for publication'. The reviewer points out several limitations and areas for improvement, which outweigh the positive elements. The politeness score is 20 because the language used is generally professional and constructive. The reviewer uses phrases like 'it would have been nice to see' and 'I think' which soften the criticism. However, the review is not overly polite, maintaining a neutral, objective tone throughout most of the text.",-50,20
Human perception in computer vision,Reject,2017,['Ron Dekel'],"[6, 6, 7]","['Marginally above acceptance threshold', 'Marginally above acceptance threshold', 'Good paper, accept']","The author works to compare DNNs to human visual perception, both quantitatively and qualitatively. Their first result involves performing a psychophysical experiment both on humans and on a model and then comparing the results (actually I think the psychophysical data was collected in a different work, and is just used here). The specific psychophysical experiment determined, separately for each of a set of approx. 1110 images, what the noise level of additive noise would have to be to make a just-noticeable-difference for humans in discriminating the noiseless image from the noisy one. The authors then define a metric on neural networks that allows them to measure what they posit might be a similar property for the networks. They then correlate the pattern of noise levels between neural networks that the humans. Deep neural networks end up being much better predictors of the human pattern of noise levels than simpler measure of image perturbation (e.g. RMS contrast). A second result involves comparing DNNs to humans in terms of their pattern errors in a series of highly controlled experiments using stimuli that illustrate classic properties of human visual processing -- including segmentation, crowding and shape understanding. They then used an information-theoretic single-neuron metric of discriminability to assess similar patterns of errors for the DNNs. Again, top layers of DNNs were able to reproduce the human patterns of difficulty across stimuli, at least to some extent. A third result involves comparing DNNs to humans in terms of their pattern of contrast sensitivity across a series of sine-grating images at different frequencies. (There is a classic result from vision research as to what this pattern should be, so it makes a natural target for comparison to models.) The authors define a DNN correlate for the propertie in terms of the cross-neuron average of the L1-distance between responses to a blank image and responses to a sinuisoid of each contrast and frequency. They then qualitatively compare the results of this metric for DNNs models to known results from the literature on humans, finding that, like humans, there is an apparent bandpass response for low-contrast gratings and a mostly constant response at high contrast. Pros: * The general concept of comparing deep nets to psychophysical results in a detailed, quantitative way, is really nice. * They nicely defined a set of *linking functions*, e.g. metrics that express how a specific behavioral result is to be generated from the neural network. (Ie. the L1 metrics in results 1 and 3 and the information-theoretic measure in result 2.) The framework for setting up such linking functions seems like a great direction to me. * The actual psychophysical data seems to have been handled in a very careful and thoughtful way. These folks clearly know what they*re doing on the psychophysical end. Cons: * To my mind, the biggest problem wit this paper is that that it doesn*t say something that we didn*t really know already. Existing results have shown that DNNs are pretty good models of the human visual system in a whole bunch of ways, and this paper adds some more ways. What would have been great would be: (a) showing that they metric of comparison to humans that was sufficiently sensitive that it could pull apart various DNN models, making one clearly better than the others. (b) identifying a wide gap between the DNNs and the humans that is still unfilled. They sort of do this, since while the DNNs are good at reproducing the human judgements in Result 1, they are not perfect -- gap is between 60% explained variance and 84% inter-human consistency. This 24% gap is potentially important, so I*d really like to see them have explored that gap more -- e.g. (i) widening the gap by identifying which images caused the gap most and focusing a test on those, or (ii) closing the gap by training a neural network to get the pattern 100% correct and seeing if that made better CNNs as measured on other metrics/tasks. In other words, I would definitely have traded off not having results 2 and 3 for a deeper exploration of result 1. I think their overall approach could be very fruitful, but it hasn*t really been carried far enough here. * I found a few things confusing about the layout of the paper. I especially found that the quantitative results for results 2 and 3 were not clearly displayed. Why was figure 8 relegated to the appendix? Where are the quantifications of model-human similarities for the data shown in Figure 8? Isn*t this the whole meat of their second result? This should really be presented in a more clear way. * Where is the quantification of model-human similarity for the data show in Figure 3? Isn*t there a way to get the human contrast-sensitivity curve and then compare it to that of models in a more quantitively precise way, rather than just note a qualitative agreement? It seems odd to me that this wasn*t done.","['4', '3', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']",[1],[3],[2],[0],[2],[0],"The sentiment score is slightly positive (20) because the reviewer acknowledges several pros of the paper, such as the 'nice' concept of comparing deep nets to psychophysical results and the 'great direction' of setting up linking functions. However, the reviewer also points out significant cons, including that the paper doesn't say something new and lacks deeper exploration of results. This balance of pros and cons, with a slight lean towards positive, justifies the score of 20. The politeness score is moderately high (60) because the reviewer uses respectful language throughout, acknowledging the authors' expertise ('These folks clearly know what they're doing') and framing criticisms constructively ('What would have been great would be...'). The reviewer also uses polite phrases like 'To my mind' when expressing opinions. However, the score is not higher because the criticism, while polite, is still direct and substantial.",20,60
Implicit ReasoNet: Modeling Large-Scale Structured Relationships with Shared Memory,Reject,2017,"['Yelong Shen*', 'Po-Sen Huang*', 'Ming-Wei Chang', 'Jianfeng Gao']","[6, 6, 6]","['Marginally above acceptance threshold', 'Marginally above acceptance threshold', 'Marginally above acceptance threshold']","In this paper, the authors proposed an implicit ResoNet model for knowledge base completion. The proposed model performs inference implicitly by a search controller and shared memory. The proposed approach demonstrates promising results on FB15k benchmark dataset. Pros: - The proposed approach demonstrates strong performance on FB15k dataset. - The idea of using shared memory for knowledge base completion is new and interesting. - The proposed approach is general and can be applied in various tasks. Cons: - There is no qualitative analysis on the results, and it is hard to see why the proposed approach works on the knowledge-base completion task. - The introduction section can be improved. Specifically, the authors should motivate *shared memory* more in the introduction and how it different from existing methods that using *unshared memory* for knowledge base completion. Similarly, the function of search controller is unclear in the introduction section as it is unclear what does search mean in the content of knowledge base completion. The concept of shared memory and search controller only make sense to me after reading through section 2.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[7, 7, 16, 18]","[13, 12, 22, 24]","[97, 75, 109, 545]","[44, 44, 59, 263]","[51, 26, 38, 250]","[2, 5, 12, 32]","The sentiment score is 50 (slightly positive) because the reviewer acknowledges several pros of the paper, such as strong performance, novel ideas, and general applicability. However, they also point out significant cons, including lack of qualitative analysis and need for improvement in the introduction. This balanced view suggests a moderately positive sentiment. The politeness score is 75 (quite polite) because the reviewer uses respectful language throughout, clearly separating pros and cons, and offering constructive criticism. They use phrases like 'can be improved' rather than more critical language, and their suggestions are framed as opportunities for enhancement rather than outright criticisms.",50,75
Improving Sampling from Generative Autoencoders with Markov Chains,Reject,2017,"['Antonia Creswell', 'Kai Arulkumaran', 'Anil Anthony Bharath']","[3, 3, 3]","['Clear rejection', 'Clear rejection', 'Clear rejection']","This paper attempts to learn a Markov chain to estimate a probability distribution over latent variables Z, such that P(X | Z) can be eased to generate samples from a data distribution. The paper in its current form is not acceptable due to the following reasons: 1. No quantitative evaluation. The authors do include samples from the generative model, which however are insufficient to judge performance of the model. See comment 2. 2. The description of the model is very unclear. I had to indulge in a lot of charity to interpret what the authors *must be doing*. What does Q(Z) mean? Does it mean the true posterior P(Z | X) ? What is the generative model here? Typically, it*s P(Z)P(X|Z). VAEs use a variational approximation Q(Z | X) to the true posterior P(Z | X). Are you trying to say that your model can sample from the true posterior P(Z | X)? Comments: 1. Using additive noise in the input does not seem like a reasonable idea. Any justification of why this is being done? 2. Approaches which learn transition operators are usually very amenable to data augmentation-based semi-supervised learning. I encourage the authors to improve their paper by testing their model on semi-supervised learning benchmarks.","['5', '4', '4']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[2, 2, 20]","[7, 7, 26]","[38, 31, 110]","[11, 6, 48]","[21, 21, 32]","[6, 4, 30]","The sentiment score is -70 because the review starts with a clear statement that the paper is 'not acceptable' and lists several major issues. The reviewer uses phrases like 'very unclear' and 'insufficient to judge performance', indicating strong dissatisfaction with the paper. However, it's not entirely negative as the reviewer does provide some constructive feedback and suggestions for improvement. The politeness score is -20 because while the reviewer is not overtly rude, the language is quite direct and critical without much softening. Phrases like 'I had to indulge in a lot of charity' and the blunt statement of unacceptability come across as somewhat impolite. The reviewer does not use many polite phrases or hedging language that might soften the criticism.",-70,-20
Improving Stochastic Gradient Descent with Feedback,Reject,2017,"['Jayanth Koushik', 'Hiroaki Hayashi']","[5, 6, 5]","['5', 'Marginally above acceptance threshold', '5']","The paper introduced an extension of Adam optimizer that automatically adjust learning rate by comparing the subsequent values of the cost function during training. The authors empirically demonstrated the benefit of the Eve optimizer on CIFAR convnets, logistic regression and RNN problems. I have the following concerns about the paper - The proposed method is VARIANT to arbitrary shifts and scaling to the cost function. - A more fair comparison with other baseline methods would be using additional exponential decay learning scheduling between the lower and upper threshold of d_t. I suspect 1/d_t just shrinks as an exponential decay from Figure 2. - Three additional hyper-parameters: k, K, eta_3. Overall, I think the method has its fundamental flew and the paper offers very limited novelty. There is no theoretical justification on the modification, and it would be good for the authors to discuss the potential failure mode of the proposed method. Furthermore, it is hard for me to follow Section 3.2. The writing quality and clarity of the method section can be further improved.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[2, 13]","[5, 19]","[6, 45]","[2, 23]","[4, 13]","[0, 9]","The sentiment score is -70 because the review is predominantly negative. The reviewer expresses several concerns about the paper, including a 'fundamental flaw' in the method, 'limited novelty', lack of theoretical justification, and poor writing quality. The only positive aspect mentioned is the empirical demonstration of the optimizer's benefits, but this is outweighed by the criticisms. The politeness score is 20 because while the reviewer is direct in their criticisms, they use relatively neutral language and offer constructive suggestions for improvement. Phrases like 'I have the following concerns' and 'it would be good for the authors to' indicate a polite tone, even while delivering negative feedback. The reviewer also acknowledges the work done ('The authors empirically demonstrated...') before critiquing it.",-70,20
Incremental Sequence Learning,Reject,2017,['Edwin D. de Jong'],"[3, 5, 5]","['Clear rejection', '5', '5']","First up, I want to point out that this paper is really long. Like 17 pages long -- without any supplementary material. While ICLR does not have an official page limit, it would be nice if authors put themselves in the reviewer*s shoes and did not take undue advantage of this rule. Having 1 or 2 pages in addition to the conventional 8 page limit is ok, but more than doubling the pages is quite unfair. Now for the review: The paper proposes a new artificial dataset for sequence learning. I call it artificial because it was artificially generated from the original MNIST dataset which is a smallish dataset of real images of handwritten digits. In addition to the dataset, the authors propose to train recurrent networks using a schedule over the length of the sequence, which they call *incremental learning*. The experiments show that their proposed schedule is better than not having any schedule on this data set. Furthermore, they also show that their proposed schedule is better than a few other intuitive schedules. The authors verify this by doing some ablation studies over the model on the proposed dataset. I have following issues with this paper: -- I did not find anything novel in this paper. The proposed incremental learning schedule is nothing new and is a natural thing to try when learning sequences. Similar idea have already been tried by a number of authors, including Bengio 2015, and Ranzato 2015. The only new piece of work is the ablation studies which the authors conduct to tease out and verify that indeed the improvement in performance is due to the curriculum used. -- Furthermore, the authors only test their hypothesis on a single dataset which they propose and is artificially generated. Why not use it on a real sequential dataset, such as, language modeling. Does the technique not work in that scenario? In fact I am quite positive that for language modeling where the vocabulary size is huge, the performance gains will be no where close to the 74% reported in the paper. -- I*m not convinced about the value of having this artificial dataset. Already there are so many real world sequential dataset available, including in text, speech, finance and other areas. What exactly does this dataset bring to the table is not super clear to me. While having another dataset may not be a bad thing in itself, I almost felt that this dataset was created for the sole purpose of making the proposed ideas work. It would have been so much better had the authors shown experiments on other datasets. -- As I said, the paper is way too long. A significant part of the length of the paper is due to a collection of experiments which are completely un-related to the main message of the paper. For instance, the experiment in Section 6.2 is completely unrelated to the story of the paper. Same is true with the transfer learning experiments of Section 6.4.","['4', '4', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[-3, -1, 11, 4]","[2, 4, 17, 10]","[2, 11, 92, 63]","[1, 5, 38, 27]","[1, 6, 21, 6]","[0, 0, 33, 30]","The sentiment score is -60 because the reviewer expresses several significant criticisms and issues with the paper, including lack of novelty, limited dataset usage, questionable value of the proposed dataset, and excessive length. The few positive comments (e.g., acknowledging ablation studies) are outweighed by the negative aspects. The politeness score is -20 because while the reviewer maintains a professional tone overall, there are instances of blunt criticism ('really long', 'quite unfair', 'I did not find anything novel') and phrases that could be perceived as slightly dismissive ('I'm not convinced', 'I almost felt that this dataset was created for the sole purpose of making the proposed ideas work'). The reviewer also directly criticizes the authors' judgment in the opening paragraph, which contributes to the slightly negative politeness score.",-60,-20
Information Dropout: learning optimal representations through noise,Reject,2017,"['Alessandro Achille', 'Stefano Soatto']","[6, 6, 4]","['Marginally above acceptance threshold', 'Marginally above acceptance threshold', 'Ok but not good enough - rejection']","Paper summary This paper develops a generalization of dropout using information theoretic principles. The basic idea is that when learning a representation z of input x with the aim of predicting y, we must choose a z such that it carries the least amount of information about x, as long as it can predict y. This idea can be formalized using the Information Bottleneck Lagrangian. This leads to an optimization problem which is similar to the one derived for variational dropout, the difference being that Information dropout allows for a scaling factor associated with the KL divergence term that encourages noise. The amount of noise being added is made a parameterized function of the data and this function is optimized along with the rest of the model. Experimental results on CIFAR-10 and MNIST show (small) improvements over binary dropout. Strengths - The paper highlights an important conceptual link between probabilistic variational methods and information theoretic methods, showing that dropout can be generalized using both formalisms to arrive at very similar models. - The presentation of the model is excellent. - The experimental results on cluttered MNIST are impressive. Weaknesses - The results on CIFAR-10 in Figure 3(b) seem to be on a validation set (unless the axis label is a typo). It is not clear why the test set was not used. This makes it hard to compare to results reported in Springenberg et al, as well as other results in literature. Quality The theoretical exposition is high quality. Figure 2 gives a nice qualitative assessment of what the model is doing. However, the experimental results section can be made better, for example, by matching the results on CIFAR-10 as reported in Springenberg et al. and trying to improve on those using information dropout. Clarity The paper is well written and easy to follow. Originality The derivation of the information dropout optimization problem using IB Lagrangian is novel. However, the final model is quite close to variational dropout. Significance This paper will be of general interest to researchers in representation learning because it highlights an alternative way to think about latent variables (as information bottlenecks). However, unless the model can be shown to achieve significant improvements over simple dropout, its wider impact is likely to be limited. Overall The paper presents an insightful theoretical derivation and good preliminary results. The experimental section can be improved. Minor comments and suggestions - - expecially -> especially - trough -> through - There is probably a minus sign missing in the expression for H(y|z) above Eq (2). - Figure 3(a) has error bars, but 3(b) doesn*t. It might be a good idea to have those for Figure 3(b) as well. - Please consider comparing Figure 2 with the activity map of a standard CNN trained with binary dropout, so we can see if similar filtering out is happening there already.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[2, 25]","[8, 31]","[68, 446]","[21, 241]","[43, 139]","[4, 66]","The sentiment score is 50 (slightly positive) because the reviewer acknowledges both strengths and weaknesses of the paper. They praise the theoretical exposition, presentation, and some experimental results, but also point out areas for improvement in the experimental section. The overall tone is constructive and appreciative of the work's contributions.

The politeness score is 80 (quite polite) because the reviewer uses respectful language throughout. They offer criticism in a constructive manner, using phrases like 'can be made better' and 'please consider' rather than harsh or dismissive language. The reviewer also balances critique with praise, acknowledging the paper's strengths before discussing its weaknesses. The use of 'Minor comments and suggestions' for detailed feedback further demonstrates a polite approach to reviewing.",50,80
Intelligible Language Modeling with Input Switched Affine Networks,Reject,2017,"['Jakob Foerster', 'Justin Gilmer', 'Jan Chorowski', 'Jascha Sohl-dickstein', 'David Sussillo']","[6, 7, 6]","['Marginally above acceptance threshold', 'Good paper, accept', 'Marginally above acceptance threshold']","Summary: The authors present a simple RNN with linear dynamics for language modeling. The linear dynamics greatly enhance the interpretability of the model, as well as provide the potential to improve performance by caching the dynamics for common sub-sequences. Overall, the quantitative comparison on a benchmark task is underwhelming. It’s unclear why the authors didn’t consider a more common dataset, and they only considered a single dataset. On the other hand, they present a number of well-executed techniques for analyzing the behavior of the model, many of which would be impossible to do for a non-linear RNN. Overall, I recommend that the paper is accepted, despite the results. It provides an interesting read and an important contribution to the research dialogue. Feedback The paper could be improved by shortening the number of analysis experiments and increasing the discussion of related sequence models. Some of the experiments were very compelling, whereas some of them (eg. 4.6) sort of feels like you’re just showing the reader that the model fits the data well, not that the model has any particularly important property. We trust that the model fits the data well, since you get reasonable perplexity results. LSTMS/GRUs are great for for language modeling for data with rigid combinatorial structure, such as nested parenthesis. It would have been nice if you compared your model to non-linear methods on this sort of data. Don’t be scared of negative results! It would be interesting if the non-linear methods were substantially better on these tasks. You should definitely add a discussion of Belanger and Kakade 2015 to the related work. They have different motivations (fast, scalable learning algorithms) rather than you (interpretable latent state dynamics and simple credit assignment for future predictions given past). On the other hand, they also have linear dynamics, and look at the singular vectors of the transition matrix to analyze the model. More broadly, it would be useful for readers if you discussed LDS more directly. A lot of this comparison came up in the openreview discussion, and I recommend folding this into the paper. For example, it would be useful to emphasize that the bias vectors correspond to columns of the Kalman gain matrix. One last thing regarding LDS: your model corresponds to Kalman filtering but in an LDS you can also do Kalman smoothing, where state vectors are inferred using the future in addition to the past observations. Could you do something similar in your model? What if you said that each matrix is a sparse/convex combination of a set of dictionary matrices? This parameter sharing could provide even more interpretability, since the characters are then represented by the low-dimensional weights used to combine the dictionary elements. This could also provide more scalability to word-level problems.","['4', '3', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[2, 5, 7, 9, 14]","[8, 11, 12, 15, 20]","[124, 49, 60, 130, 34]","[50, 18, 27, 52, 13]","[70, 28, 28, 74, 15]","[4, 3, 5, 4, 6]","The sentiment score is 50 (slightly positive) because while the reviewer recommends acceptance and praises aspects of the paper ('well-executed techniques', 'interesting read', 'important contribution'), they also note significant limitations ('quantitative comparison on a benchmark task is underwhelming', 'only considered a single dataset'). The overall tone is constructive but mixed. The politeness score is 75 (quite polite) due to the reviewer's respectful language, constructive feedback, and encouragement ('Don't be scared of negative results!'). They offer suggestions for improvement without being harsh or dismissive, and acknowledge the paper's strengths alongside its weaknesses.",50,75
Investigating Different Context Types and Representations for Learning Word Embeddings,Reject,2017,"['Bofang Li', 'Tao Liu', 'Zhe Zhao', 'Buzhou Tang', 'Xiaoyong Du']","[4, 6, 4]","['Ok but not good enough - rejection', 'Marginally above acceptance threshold', 'Ok but not good enough - rejection']",This paper analyzes dependency trees vs standard window contexts for word vector learning. While that*s a good goal I believe the paper falls short of a thorough analysis of the subject matter. It does not analyze Glove like objective functions which often work better than the algorithms used here. It doesn*t compare in absolute terms to other published vectors or models. It fails to gain any particularly interesting insights that will modify other people*s work. It fails to push the state of the art or make available new resources for people.,"['5', '4', '3']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[3, 9, 3, 10, 7, 3, 23]","[8, 14, 9, 16, 13, 9, 29]","[19, 40, 38, 122, 33, 46, 353]","[13, 31, 21, 58, 21, 25, 233]","[3, 4, 15, 11, 8, 19, 23]","[3, 5, 2, 53, 4, 2, 97]","The sentiment score is -60 because the review is predominantly negative. While it acknowledges a 'good goal', it then lists multiple shortcomings of the paper, using phrases like 'falls short', 'does not analyze', 'fails to gain', and 'fails to push'. These criticisms outweigh the initial positive comment, resulting in a negative overall sentiment. The politeness score is -20 because while the language isn't overtly rude, it's quite direct and critical without much attempt to soften the feedback. The repeated use of 'fails to' comes across as somewhat harsh. However, it's not extremely impolite, as it focuses on the work rather than personal attacks, which prevents it from receiving a lower score.",-60,-20
Investigating Recurrence and Eligibility Traces in Deep Q-Networks,Reject,2017,"['Jean Harb', 'Doina Precup']","[4, 4, 3]","['Ok but not good enough - rejection', 'Ok but not good enough - rejection', 'Clear rejection']","This paper combines DRQN with eligibility traces, and also experiment with the Adam optimizer for optimizing the q-network. This direction is worth exploring, and the experiments demonstrate the benefit from using eligibility traces and Adam on two Atari games. The methods themselves are not novel. Thus, the primary contributions are (1) applying eligibility traces and Adam to DRQN and (2) the experimental evaluation. The paper is well-written and easy to understand. The experiments provide quantitative results and detailed qualitative intuition for how and why the methods perform as they do. However, with only two Atari games in the results, it is difficult to tell how well it the method would perform more generally. Showing results on several more games and/or other domains would significantly improve the paper. Showing error bars from multiple random seeds would also improve the paper.","['4', '5', '5']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[2, 21]","[7, 27]","[11, 402]","[3, 215]","[8, 150]","[0, 37]","The sentiment score is 50 (slightly positive) because the reviewer acknowledges the paper's worth and contributions, praising its clarity and experimental insights. However, they also point out limitations, such as the lack of novelty in methods and limited scope of experiments. The politeness score is 75 (quite polite) as the reviewer uses respectful language throughout, offering constructive criticism and suggestions for improvement without harsh or dismissive comments. They balance positive feedback with areas for enhancement, maintaining a professional and courteous tone.",50,75
Is a picture worth a thousand words? A Deep Multi-Modal Fusion Architecture for Product Classification in e-commerce,Reject,2017,"['Tom Zahavy', 'Alessandro Magnani', 'Abhinandan Krishnan', 'Shie Mannor']","[5, 5, 4]","['5', '5', 'Ok but not good enough - rejection']","This paper introduces a large-scale multi-model product classification system. The model consists of three modules, Image CNN (VGG 16 architecture), text CNN (Kim 2014) and decision-level fusion policies. The authors have tried several fusion methods: including policies taking inputs from text and image CNN probabilities; choose either CNN; average the predictions; end-to-end training. Experimental results show that text CNN alone works better than image CNN and multi-model fusion can improve the accuracy by a small margin. It is a little bit surprising that end-to-end feature level fusion works worse than text CNN alone. The writing is clear and there are a lot of useful practical experiences of learning large-scale model. However, I lean toward rejecting the paper because the following: 1) No other dataset reported. The authors haven*t mentioned releasing the walmart dataset and it is going to be really hard to reproduce the results without the dataset. 2) Technical novelty is limited. All the decision-level fusion policies have been investigated by some previous methods before. 3) Performance gain is also limited.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[4, 13, 2, 18]","[10, 19, 4, 24]","[61, 40, 6, 503]","[23, 20, 2, 231]","[37, 6, 4, 192]","[1, 14, 0, 80]","The sentiment score is -50 because while the reviewer acknowledges some positive aspects (clear writing, useful practical experiences), they ultimately lean towards rejecting the paper due to several significant limitations. The overall tone is more negative than positive, but not entirely negative. The politeness score is 50 because the reviewer uses respectful language and acknowledges the paper's strengths before presenting criticisms. They use phrases like 'a little bit surprising' and 'lean toward rejecting' rather than harsh or dismissive language. The reviewer provides specific, constructive feedback without being overly critical or rude.",-50,50
Iterative Refinement for Machine Translation,Reject,2017,"['Roman Novak', 'Michael Auli', 'David Grangier']","[5, 5, 7, 4]","['5', '5', 'Good paper, accept', 'Ok but not good enough - rejection']","This paper proposes a method for iteratively improving the output of an existing machine translation by identifying potential mistakes and proposing a substitution, in this case using an attention-based model. It is motivated by the method in which (it is assumed) human translators operate. The paper is interesting and imaginative. However, in general terms, I am somewhat sceptical of this kind of approach -- whereby a machine learning method is used to identify and correct the predictions of another method, or itself -- because in the first case, if the new method is better, why not use it from the outset in place of the other method? And in the second case, since the method has no new information compared to previously, why is it more likely to identify more past mistakes and correct them, than identify past correct terms and turn them into new errors? That is unless there is a specific reason that an iterative approach can be shown to converge to a better solution when run over several epochs. This paper does not convince me on these points. Indeed, unsurprisingly, the authors note that *the probability of correctly labelling a word as a mistake remains low (62%)* - this admittedly beats a random-chance baseline, but is not compared to something more meaningful, such as simply contrasting the existing system with a more powerful convolutional model and labelling all discrepancies as mistakes. The oracle experiments are rather meaningless - they just serve to confirm that improving a translation is very easy when the existing mistakes have been identified, but much harder when they are not. Although I do like the paper on the whole, to really convince me that main objective -- ie. that **iterative** improvement is beneficial -- has been satifactorily demonstrated it would be necessary to include stronger baselines - and in particular, to show that an iterative refinement scheme can really improve over a system closely matched to the attention-based model, both when used in isolation and when used in system combination with a PBMT system, and to demonstrate that the PBMT system is not simply acting as a regulariser for the attention-based model. Minor comments: I find the notation excessively fiddly at times - eg F^i = (F^{i,1}, F^{i,|F^i|}) - why use |F^i| here when F is a matrix, so surely the length of the slice is not dependent on i? In the discussion in section 4 - it seems that this still creates a mismatch between the training and test conditions - could anything be done about this?","['3', '5', '3', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[2, 2, 8]","[6, 8, 14]","[15, 26, 397]","[6, 10, 154]","[8, 15, 216]","[1, 1, 27]","The sentiment score is slightly negative (-20) because while the reviewer finds the paper 'interesting and imaginative', they express significant skepticism about the approach and its effectiveness. They state that the paper 'does not convince' them on key points and that some experiments are 'rather meaningless'. However, they do mention liking the paper 'on the whole', which prevents the score from being more negative. The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, acknowledging positive aspects of the paper and framing criticisms as personal opinions ('I am somewhat sceptical', 'to really convince me'). They also offer constructive suggestions for improvement rather than outright dismissal. The tone is professional and courteous, even when expressing doubts about the methodology.",-20,60
Joint Multimodal Learning with Deep Generative Models,Reject,2017,"['Masahiro Suzuki', 'Kotaro Nakayama', 'Yutaka Matsuo']","[5, 3, 5]","['5', 'Clear rejection', '5']","The paper introduces the joint multimodal variational autoencoder, a directed graphical model for modeling multimodal data with latent variable. the model is rather straightforward extension of standard VAE where two data modalities are generated from a shared latent representation independently. In order to deal with missing input modalities or bi-directional inference between two modalities the paper introduces modality-specific encoder that is trained to minimize the KL divergence of latent variable distributions between joint and modality-specific recognition networks. The paper demonstrates its effectiveness on MNIST and CelebA datasets, both in terms of test log-likelihoods and the conditional image generation and editing. The proposed method is rather straightforward extension of VAE and therefore the model should inherent the probabilistic inference methods of VAE. For example, for missing data modalities, the model should be able to infer joint representation as well as filling in the missing modalities via iterative sampling as introduced by Rezende et al. (2014). Given marginal improvement, I am not convinced by the contribution of modality-specific encoders in Section 3.3. In addition, the inference methods introduced for generating Figure 5 looks somewhat unprincipled; I am wondering the conditional image generation results by following more principled approach (e.g., iterative sampling). Experimental results on joint image-attribute generation is also missing.","['5', '4', '3']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[16, 13, 20]","[22, 18, 26]","[53, 44, 252]","[27, 32, 156]","[12, 6, 52]","[14, 6, 44]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects of the paper, they express skepticism about the contribution and effectiveness of certain aspects of the model. The reviewer states they are 'not convinced by the contribution of modality-specific encoders' and questions some of the methods used. However, it's not entirely negative as they do recognize the paper's demonstrations of effectiveness on certain datasets. The politeness score is moderately positive (50) because the reviewer uses professional and neutral language throughout, avoiding harsh criticism or personal attacks. They present their concerns as observations and questions rather than outright dismissals. The reviewer also acknowledges the paper's strengths before delving into criticisms, which is a polite approach in academic discourse.",-20,50
Joint Training of Ratings and Reviews with Recurrent Recommender Networks,Reject,2017,"['Chao-Yuan Wu', 'Amr Ahmed', 'Alex Beutel', 'Alexander J. Smola']","[6, 6, 6]","['Marginally above acceptance threshold', 'Marginally above acceptance threshold', 'Marginally above acceptance threshold']","This paper proposes an RNN-based model for recommendation which takes into account temporal dynamics in user ratings and reviews. Interestingly, the model infers time-dependant user/item vectors by applying an RNN to previous histories. Those vectors are used to predict ratings, in a similar fashion to standard matrix factorization methods, and also bias a conditional RNN language model for reviews. The paper is well written and the architectural choices make sense. The main shortcomings of the paper are in the experiments: 1) The full model (rating+text) is only applied to one and relatively small dataset. Applying the model on multiple datasets with more data, e.g. Amazon reviews dataset (https://snap.stanford.edu/data/web-Amazon.html) would be more convincing. 2) While modelling order in review text seems like the right choice, previous papers (e.g. Almahairi et al. 2015) have shown that for rating prediction, modelling order in reviews might not be useful. A comparison with a similar model, but with bag-of-words reviews model would be nice in order to show the importance of the RNN-based review model, especially given previous literature. Finally, this paper is an application paper applying well-established deep learning techniques, and I do not feel the paper offers new insights which the ICLR community in general would benefit from. This is not to undermine the importance of this paper, but I would like the authors to comment on why they think ICLR is a good avenue for their work.","['4', '3', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[4, 17, 8, 22]","[10, 22, 14, 28]","[37, 120, 93, 341]","[20, 82, 50, 216]","[17, 21, 36, 83]","[0, 17, 7, 42]","The sentiment score is slightly positive (20) because the reviewer acknowledges the paper is well-written and the architectural choices make sense. However, they also point out significant shortcomings in the experiments and question the suitability for ICLR, which prevents a higher positive score. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, acknowledges the paper's strengths, and frames criticisms constructively as suggestions for improvement rather than harsh judgments. They also invite the authors to comment on their choice of venue, showing consideration. The reviewer maintains a professional and courteous tone while providing honest feedback.",20,60
Knowledge Adaptation: Teaching to Adapt,Reject,2017,"['Sebastian Ruder', 'Parsa Ghaffari', 'John G. Breslin']","[7, 5, 6]","['Good paper, accept', '5', 'Marginally above acceptance threshold']","This paper studies the problem of transfer learning in the context of domain adaptation. They propose to study it in the framework of knowledge distillation. Several settings are presented along with experiments on the Amazon Reviews dataset. The paper is nicely written and the problem studied is very important towards progress in AI. The results of the experiments could be improved but still justify the validity of applying distillation for transfer learning. Of course, the experimental setting is rather limited but the benchmarks are competitive enough to be meaningful. I had concerns regarding discussion of previous work but the extensive responses helped clarify this point (the authors should turn the arguments used in this thread into an appendix). I think this paper would make an interesting ICLR paper.","['3', '4', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[2, 2, 15]","[8, 3, 21]","[132, 13, 206]","[54, 5, 151]","[76, 8, 14]","[2, 0, 41]","The sentiment score is 70 (positive) because the reviewer expresses a generally favorable view of the paper. They describe it as 'nicely written' and state that the problem studied is 'very important'. While they mention that results 'could be improved', they still find them sufficient to 'justify the validity' of the approach. The reviewer concludes by stating it 'would make an interesting ICLR paper'. The politeness score is 80 (quite polite) due to the reviewer's constructive and respectful tone throughout. They acknowledge the paper's strengths, offer balanced criticism, and use phrases like 'I think' to soften their opinions. The reviewer also mentions that the authors' responses helped clarify concerns, showing a willingness to engage in productive dialogue.",70,80
LSTM-Based System-Call Language Modeling and Ensemble Method for Host-Based Intrusion Detection,Reject,2017,"['Gyuwan Kim', 'Hayoon Yi', 'Jangho Lee', 'Yunheung Paek', 'Sungroh Yoon']","[5, 5, 8]","['5', '5', 'Top 50% of accepted papers, clear accept']","This paper presents an anomaly-based host intrusion detection method. LSTM RNN is used to model the system-call sequences and the averaged sequence likelihood is then used to determine anomaly, which is the attack. This paper also compares an ensemble method with two baselines as classification model. +This is is well written and more of ideas are clearly presented. +It demonstrates an interesting application of LSTM sequential modeling to HIDS problem -The overall novelty is limited considering the major technical components like LSTM RNN and ensemble method are already established. -The contribution of the proposed ensemble method needs further evaluation because it is also possible to use ensemble ideas in kNN and kMC baselines.","['3', '4', '3']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[3, 5, 12, 22, 14]","[8, 10, 18, 28, 20]","[28, 13, 26, 136, 261]","[12, 5, 13, 79, 88]","[15, 2, 8, 3, 102]","[1, 6, 5, 54, 71]","The sentiment score is slightly positive (20) because the review begins with positive comments about the paper being well-written and presenting clear ideas. It also mentions an 'interesting application' of the method. However, the reviewer also points out limitations in novelty and the need for further evaluation, which balances out some of the positivity. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, acknowledging the paper's strengths before presenting criticisms. The criticisms are presented as constructive feedback rather than harsh judgments. The reviewer maintains a professional tone without using overly formal or informal language.",20,50
Layer Recurrent Neural Networks,Reject,2017,"['Weidi Xie', 'Alison Noble', 'Andrew Zisserman']","[7, 6, 5, 7]","['Good paper, accept', 'Marginally above acceptance threshold', '5', 'Good paper, accept']","Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons. Paper summary: this work proposes to use RNNs inside a convolutional network architecture as a complementary mechanism to propagate spatial information across the image. Promising results on classification and semantic labeling are reported. Review summary: The text is clear, the idea well describe, the experiments seem well constructed and do not overclaim. Overall it is not a earth shattering paper, but a good piece of incremental science. Pros: * Clear description * Well built experiments * Simple yet effective idea * No overclaiming * Detailed comparison with related work architectures Cons: * Idea somewhat incremental (e.g. can be seen as derivative from Bell 2016) * Results are good, but do not improve over state of the art Quality: the ideas are sound, experiments well built and analysed. Clarity: easy to read, and mostly clear (but some relevant details left out, see comments below) Originality: minor, this is a different combination of ideas well known. Significance: seems like a good step forward in our quest to learn good practices to build neural networks for task X (here semantic labelling and classification). Specific comments: * Section 2.2 “we introduction more nonlinearities (through the convolutional layers and ...”. Convolutional layers are linear operators. * Section 2.2, why exactly RNN cannot have pooling operators ? I do not see what would impede it. * Section 3 “into the computational block”, which block ? Seems like a typo, please rephrase. * Figure 2b and 2c not present ? Please fix figure or references to it. * Maybe add a short description of GRU in the appendix, for completeness ? * Section 5.1, last sentence. Not sure what is meant. The convolutions + relu and pooling in ResNet do provide non-linearities “between layers” too. Please clarify * Section 5.2.1 (and appendix A), how is the learning rate increased and decreased ? Manually ? This is an important detail that should be made explicit. Is the learning rate schedule the same in all experiments of each table ? If there is a human in the loop, what is the variance in results between “two human schedulers” ? * Section 5.2.1, last sentence; “we certainly have a strong baseline”; the Pascal VOC12 for competition 6 reports 85.4 mIoU as best known results. So no, 64.4 is not “certainly strong”. Please tune down the statement. * Section 5.2.3 Modules -> modules * The results ignore any mention of increased memory usage or computation cost. This is not a small detail. Please add a discussion on the topic. * Section 6 “adding multi-scale spatial” -> “adding spatial” (there is nothing inherently “multi” in the RNN) * Section 6 Furthermoe -> Furthermore * Appendix C, redundant with Figure 5 ?","['4', '4', '5', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[-1, 10]","[5, 16]","[7, 64]","[5, 44]","[0, 4]","[2, 16]","The sentiment score is 60 (positive) because the reviewer describes the paper as 'a good piece of incremental science' and lists several pros, including clear description, well-built experiments, and a simple yet effective idea. The reviewer also notes that it's 'not earth shattering' and lists some cons, which prevents a higher score. The politeness score is 70 (polite) because the reviewer uses respectful language throughout, acknowledges the paper's strengths, and provides constructive criticism. The specific comments are detailed and helpful, offering suggestions for improvement without being harsh. The reviewer also uses phrases like 'please clarify' and 'please fix,' which are polite ways of pointing out issues.",60,70
Learning Efficient Algorithms with Hierarchical Attentive Memory,Reject,2017,"['Marcin Andrychowicz', 'Karol Kurach']","[5, 3, 5]","['5', 'Clear rejection', '5']","This paper introduces a novel hierarchical memory architecture for neural networks, based on a binary tree with leaves corresponding to memory cells. This allows for O(log n) memory access, and experiments additionally demonstrate ability to solve more challenging tasks such as sorting from pure input-output examples and dealing with longer sequences. The idea of the paper is novel and well-presented, and the memory structure seems reasonable to have advantages in practice. However, the main weakness of the paper is the experiments. There is no experimental comparison with other external memory-based approaches (e.g. those discussed in Related Work), or experimental analysis of computational efficiency given overhead costs (beyond just computational complexity) despite that being one of the main advantages. Furthermore, the experimental setups are relatively weak, all on artificial tasks with moderate increases in sequence length. Improving on these would greatly strengthen the paper, as the core idea is interesting.","['5', '4', '4']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[6, 6]","[11, 9]","[48, 28]","[21, 12]","[25, 14]","[2, 2]","The sentiment score is slightly positive (20) because the reviewer acknowledges the novelty and potential of the paper's idea, calling it 'novel and well-presented' and 'interesting'. However, they also point out significant weaknesses in the experimental section, which tempers the positive sentiment. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, balancing praise with constructive criticism. They start with positive aspects before moving to areas of improvement, and use phrases like 'would greatly strengthen the paper' instead of more negative alternatives. The tone is professional and helpful rather than harsh or dismissive.",20,60
Learning Identity Mappings with Residual Gates,Reject,2017,"['Pedro H. P. Savarese', 'Leonardo O. Mazza', 'Daniel R. Figueiredo']","[5, 5, 6]","['5', '5', 'Marginally above acceptance threshold']","This paper proposes a network called Gated Residual Networks layer design that adds gating to shortcut connections with a scalar to regulate the gate. The authors claim that this approach will improve the training Residual Networks. It seems the authors could get competitive performance on CIFAR-10 to state of art models with only Wide Res Nets. Wide Gated ResNet requires much more parameters than DenseNet (and other Res Net variants) for obtaining a little improvement over Dense Net. More importantly, the authors state that they obtained the best results on CIFAR-10 and CIFAR-100 but the updated version of DenseNet (Huang et al. (2016b)) has new results for a version called DenseNet-BC which outperforms all of the results that authors reported (3.46 for CIFAR-10 and 17.18 for CIFAR-100 with 25.6M parameters, DenseNet-BC still outperforms with 15.3M parameters which is much less that 36.5M). The Res Net variants papers with state of art results report result for Image Net. Therefore the empirical results need also the Image Net to demonstrate that improvement claimed is achieved. The proposed trick adopts Highway Neural Networks and Residual Networks with an intuitive motivation. It is not sufficiently novel and the empirical results do not prove sufficient effectiveness of this incremental approach.","['5', '5', '4']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct']",[2],[7],[23],[12],[11],[0],"The sentiment score is -60 because the review is predominantly negative. The reviewer points out several shortcomings of the paper, such as the proposed method requiring more parameters for little improvement, being outperformed by other models, and lacking novelty. The reviewer also criticizes the absence of ImageNet results and questions the effectiveness of the approach. However, it's not entirely negative as it acknowledges some competitive performance, hence not reaching the lowest possible score.

The politeness score is 0 (neutral) because the reviewer maintains a professional and objective tone throughout. They neither use overly polite language nor resort to rudeness. The criticism is presented factually without personal attacks or overly harsh language. The reviewer simply states their observations and concerns about the paper's claims and results.",-60,0
Learning Locomotion Skills Using DeepRL: Does the Choice of Action Space Matter?,Reject,2017,"['Xue Bin Peng', 'Michiel van de Panne']","[6, 6, 6]","['Marginally above acceptance threshold', 'Marginally above acceptance threshold', 'Marginally above acceptance threshold']","The paper is straightforward, easy to read, and has clear results. Since all these parameterisations end up outputting torques, it seems like there shouldn*t be much difference between them. There is a known function that convert from one representation to another (or at least to torques). Is it not possible that the only reason proportional control is a little better is that the tracking cost is a function of positions? Would we get the same result if there was no reference-pose cost, only a locomotion cost? Would we get the same result if the task was to spin a top? My guess is no. This work is interesting, but not likely to generalise to other scenarios, and in that sense is rather limited. The video is nice.","['4', '4', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[3, 28]","[9, 33]","[51, 125]","[15, 55]","[29, 24]","[7, 46]","The sentiment score is slightly positive (20) because the reviewer acknowledges positive aspects of the paper, such as it being 'straightforward, easy to read, and has clear results.' They also mention that the video is 'nice.' However, the reviewer expresses concerns about the generalizability and limitations of the work, which prevents a higher positive score. The politeness score is moderately positive (50) as the reviewer uses neutral language and presents their criticisms as questions and personal opinions ('My guess is...') rather than harsh statements. They also start with positive comments before moving to critiques, which is a polite approach. The language is professional and respectful throughout, without any rude or overly negative phrasing.",20,50
Learning Python Code Suggestion with a Sparse Pointer Network,Reject,2017,"['Avishkar Bhoopchand', 'Tim Rocktäschel', 'Earl Barr', 'Sebastian Riedel']","[6, 6, 5]","['Marginally above acceptance threshold', 'Marginally above acceptance threshold', '5']","This paper presents an improved neural language models designed for selected long-term dependency, i.e., to predict more accurately the next identifier for the dynamic programming language such as Python. The improvements are obtained by: 1) replacing the fixed-widow attention with a pointer network, in which the memory only consists of context representation of the previous K identifies introduced for the entire history. 2) a conventional neural LSTM-based language model is combined with such a sparse pointer network with a controller, which linearly combines the prediction of both components using a dynamic weights, decided by the input, hidden state, and the context representations at the time stamp. Such a model avoids the the need of large window size of the attention to predict next identifier, which usually requires a long-term dependency in the programming language. This is partly validated by the python codebase (which is another contribution of this paper) experiments in the paper. While the paper still misses some critical information that I would like to see, including how the sparse pointer network performance chances with different size of K, and how computationally efficient it is for both training and inference time compared to LSTM w/ attention of various window size, and ablation experiments about how much (1) and (2) contribute respectively, it might be of interest to the ICLR community to see it accepted.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[2, 6, 18, 12]","[8, 12, 24, 18]","[8, 129, 88, 233]","[1, 61, 50, 119]","[6, 63, 20, 101]","[1, 5, 18, 13]","The sentiment score is 60 (positive) because the reviewer expresses interest in the paper's contributions and suggests it might be of interest to the ICLR community. They highlight the improvements and contributions made by the paper, although they also mention some missing information. The overall tone is supportive despite noting areas for improvement. The politeness score is 50 (slightly polite) as the reviewer uses neutral language throughout, avoiding harsh criticism. They acknowledge the paper's contributions and express their desire for additional information in a constructive manner, rather than being demanding or critical. The language is professional and respectful, maintaining a balanced tone between highlighting strengths and areas for improvement.",60,50
Learning Recurrent Span Representations for Extractive Question Answering,Reject,2017,"['Kenton Lee', 'Tom Kwiatkowksi', 'Ankur Parikh', 'Dipanjan Das']","[7, 6, 6]","['Good paper, accept', 'Marginally above acceptance threshold', 'Marginally above acceptance threshold']","This paper proposes RaSoR, a method to efficiently representing and scoring all possible spans in an extractive QA task. While the test set results on SQuAD have not been released, it looks likely that they are not going to be state-of-the-art; with that said, the idea of enumerating all possible spans proposed in this paper could potentially improve many architectures. The paper is very well-written and the analysis/ablations in the final sections are mostly interesting (especially Figure 2, which confirms what we would intuitively believe). Based on its potential to positively impact other researchers working on SQuAD, I recommend that the paper is accepted.","['4', '5', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is fairly confident that the evaluation is correct']","[6, 8, 8, 12]","[12, 14, 14, 18]","[67, 39, 62, 73]","[27, 17, 27, 40]","[37, 17, 31, 28]","[3, 5, 4, 5]","The sentiment score is 70 (positive) because the reviewer recommends acceptance of the paper, praises its potential impact, and compliments the writing and analysis. However, it's not 100 as there are some reservations about the test results. The politeness score is 80 (very polite) due to the constructive and respectful tone throughout. The reviewer acknowledges the paper's strengths and potential contributions while gently noting areas that might not be state-of-the-art. The language used is professional and courteous, avoiding harsh criticism and focusing on the positive aspects and potential of the work.",70,80
Learning to Protect Communications with Adversarial Neural Cryptography,Reject,2017,"['Martín Abadi', 'David G. Andersen']","[5, 6, 4]","['5', 'Marginally above acceptance threshold', 'Ok but not good enough - rejection']","The submission proposes to modify the typical GAN architecture slightly to include *encrypt* (Alice) and *decrypt* (Bob) modules as well as a module trying to decrypt the signal without a key (Eve). Through repeated transmission of signals, the adversarial game is intended to converge to a system in which Alice and Bob can communicate securely (or at least a designated part of the signal should be secure), while a sophisticated Eve cannot break their code. Examples are given on toy data: *As a proof-of-concept, we implemented Alice, Bob, and Eve networks that take N-bit random plain-text and key values, and produce N-entry floating-point ciphertexts, for N = 16, 32, and 64. Both plaintext and key values are uniformly distributed.* The idea considered here is cute. If some, but not necessarily all of the signal is meant to be secure, the modules can learn to encrypt and decrypt a signal, while an adversary is simultaneously learned that tries to break the encryption. In this way, some of the data can remain unencrypted, while the portion that is e.g. correlated with the encrypted signal will have to be encrypted in order for Eve to not be able to predict the encrypted part. While this is a nice thought experiment, there are significant barriers to this submission having a practical impact: 1) GANs, and from the convergence figures also the objective considered here, are quite unstable to optimize. The only guarantees of privacy are for an Eve that is converged to a very strong adversary (stronger than a dedicated attack over time). I do not see how one can have any sort of reliable guarantee of the safety of the data transmission from the proposed approach, at least the paper does not outline such a guarantee. 2) Public key encryption systems are readily available, computationally feasible, and successfully applied almost anywhere. The toy examples given in the paper do not at all convince me that this is solving a real-world problem at this point. Perhaps a good example will come up in the near future, and this work will be shown to be justified, but until such an example is shown, the approach is more of an interesting thought experiment.","['4', '3', '2']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']","[33, 19]","[37, 25]","[256, 135]","[176, 98]","[14, 12]","[66, 25]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the idea as 'cute' and 'a nice thought experiment', they express significant doubts about its practical impact and real-world applicability. They point out major barriers and limitations, suggesting the approach is more theoretical than practical at this stage. The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, acknowledging positive aspects ('cute idea', 'nice thought experiment') before presenting criticisms. They offer constructive feedback and leave room for potential future value of the work. The tone is professional and objective, avoiding harsh or dismissive language even when expressing skepticism.",-20,60
Linear Time Complexity Deep Fourier Scattering Network and Extension to Nonlinear Invariants,Reject,2017,"['Randall Balestriero', 'Herve Glotin']","[4, 5, 4]","['Ok but not good enough - rejection', '5', 'Ok but not good enough - rejection']","I find the general direction of the work is promising but, in my opinion, the paper has three main drawback. While the motivation and overall idea seem very reasonable, the derivation is not convincing mathematically. The experiments are limited and the presentation needs significant improvement. The writing and wording are in general poorly structured to the point that it is sometimes difficult to follow the proposed ideas. The overall organization needs improvement and the connection between sections is not properly established. The paper could be significantly improved by simply re-writing it. I*m not fully convinced by the motivation for the proposed non-linearity (|c|^2), as described on page 5. The authors argue that (Waldspurger, 2016) suggests that higher order nonlinearities might be beneficial for sparsity. But unless I*m missing something, that work seems to suggest that in the general case higher order nonlinearities can be neglected. Could you please comment on this? On the other hand, adding a second order term to the descriptor seems an interesting direction, as long as stability to small variations is preserved (which should be shown experimentally) The experimental section is rather limited. The paper would be stronger with a thorough numerical evaluation. The presented results, in my opinion, do not show convincingly a clear advantage of the proposed method over a standard implementation of the scattering transform. In order to show the merits of the proposed approach, it would be really helpful to directly compare running times and compression rates. Questions: - Can you show empirically that the proposed higher order nonlinearity produces sparser representations than the complex modulus? Other minor issues: - The proof of Section 2.1, should be preceded by a clear statement in the form of a proposition - *Hadamart* -> Hadamard - *Valid set* -> Validation set - *nonzeros coefficients* -> nonzero coefficients - Figure 3 is difficult to understand. Please provide more details. - Figure 5 is supposed to show a comparison to a standard implementation of the Scattering network, but it doesn*t seem to be such comparison in that figure. Please explain. - Please verify the references. The first reference states *MALLAT*.","['3', '3', '5']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[3, 20]","[9, 26]","[87, 148]","[23, 115]","[59, 17]","[5, 16]","The sentiment score is -40 because while the reviewer finds the general direction promising, they identify several major drawbacks including unconvincing mathematical derivation, limited experiments, and poor presentation. The overall tone is more negative than positive, but not entirely dismissive. The politeness score is 20 because the reviewer uses generally polite language ('Could you please comment on this?', 'Please explain'), and frames criticisms as their opinion ('in my opinion'). However, some direct criticisms ('poorly structured', 'needs significant improvement') prevent a higher politeness score. The reviewer also provides constructive feedback and specific suggestions for improvement, which contributes to a slightly positive politeness score.",-40,20
LipNet: End-to-End Sentence-level Lipreading,Reject,2017,"['Yannis M. Assael', 'Brendan Shillingford', 'Shimon Whiteson', 'Nando de Freitas']","[6, 4, 4]","['Marginally above acceptance threshold', 'Ok but not good enough - rejection', 'Ok but not good enough - rejection']","The authors present a well thought out and constructed system for performing lipreading. The primary novelty is the end-to-end nature of the system for lipreading, with the sentence-level prediction also differentiating this with prior work. The described neural network architecture contains convolutional and recurrent layers with a CTC sequence loss at the end, and beam search decoding with an LM is done to obtain best results. Performance is evaluated on the GRID dataset, with some saliency map and confusion matrix analysis provided as well. Overall, the work seems of high quality and clearly written with detailed explanations. The final results and analysis appear good as well. One gripe is that that the novelty lies in the choice of application domain as opposed to the methods. Lack of word-level comparisons also makes it difficult to determine the importance of using sentence-level information vs. choices in model architecture/decoding, and finally, the GRID dataset itself appears limited with the grammar and use of a n-gram dictionary. Clearly the system is well engineered and final results impress, though it*s unclear how much broader insight the results yield.","['3', '4', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[-2, -2]","[1, 1]","[1, 1]","[1, 1]","[0, 0]","[0, 0]","The sentiment score is 60 (positive) because the reviewer describes the work as 'well thought out and constructed', 'of high quality', and 'clearly written'. They mention that the 'final results and analysis appear good' and that the 'final results impress'. However, the score is not higher due to some criticisms, such as the limited novelty and dataset concerns. The politeness score is 70 (polite) because the reviewer uses respectful language throughout, acknowledging the strengths of the work before presenting criticisms. They use phrases like 'The authors present...' and 'Overall, the work seems...' which maintain a professional and courteous tone. The criticisms are presented as observations rather than harsh judgments, using phrases like 'One gripe is...' and 'it's unclear how much...', which keeps the tone polite while still providing constructive feedback.",60,70
Local minima in training of deep networks,Reject,2017,"['Grzegorz Swirszcz', 'Wojciech Marian Czarnecki', 'Razvan Pascanu']","[5, 5, 3]","['5', '5', 'Clear rejection']","The main merit of this paper is to draw again attention to how crucial initialization of deep network *can* be; and to counter the popular impression that modern architectures and improved gradient descent techniques make optimization local minima and saddle points no longer a problem. While the paper provides interesting counter-examples that showcase how bad initialization mixed with particular data can lead the optimization to get stuck at a poor solution, these feel like contrived artificial constructs. More importantly the paper does not consider popular heuristics that likely help to avoid getting stuck, such as: non-saturating activation functions (e.g. leaky RELU), batch-norm, skip connections (resnet), that can all be thought of as contributing to keep the gradients flowing. The paper puts up a big warning sign about potential initialization problems (with standard RELU nets), but without proposing new solutions or workarounds, nor carrying out a systematic analysis of how this picture is affected by most commonly used current heuristic techniques (in architecture, initialization and training). Such a broader scope analysis, especially if it did lead to insights of practical relevance, could much increase the value of the paper for the reader.","['3', '4', '5']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[14, 6, 8]","[19, 12, 14]","[34, 83, 156]","[16, 28, 63]","[9, 44, 87]","[9, 11, 6]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the paper's merit in drawing attention to an important issue, they express significant reservations about the paper's scope, methodology, and practical relevance. The reviewer points out that the examples feel 'contrived' and that the paper doesn't consider popular heuristics or propose new solutions. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, acknowledging the paper's contributions before offering constructive criticism. They use phrases like 'the main merit of this paper' and 'interesting counter-examples' before presenting their concerns, which maintains a polite tone while still conveying their critical feedback.",-20,50
Low-rank passthrough neural networks,Reject,2017,['Antonio Valerio Miceli Barone'],"[4, 6]","['Ok but not good enough - rejection', 'Marginally above acceptance threshold']","The author proposes the use of low-rank matrix in feedfoward and RNNs. In particular, they try their approach in a GRU and a feedforward highway network. Author also presents as a contribution the passthrough framework, which can describe feedforward and recurrent networks. However, this framework seems hardly novel, relatively to the formalism introduced by LSTM or highway networks. An empirical evaluation is performed on different datasets (MNIST, memory/addition tasks, sequential permuted MNIST and character level penntreebank). However, there are few problems with the evaluation: - In the highway network experiment, the author does not compare with a baseline. We can not assess what it the impact of the low-rank parameterization. Also, it would be interesting to compare the result with a highway network that have this capacity bottleneck across layer (first layer of size , second layer of size , third layer of size ) and not in the gate functions. Also, how did you select the hyperparameter values?. - It is unfortunate that the character level penntreebank does not use the same experimental setting than previous works as it prevents from direct comparison. Also the overall bpc perplexity seems relatively high for this dataset. It is therefore not clear how low-rank decomposition would perform on this task applied on a stronger baseline. -Author claims state-of-art in the memory task. However, their approach uses more parameters than the uRNN (41K against 6.5K for the memory) which makes the comparison a little bit unfair toward uRNN. It would be informative to see how low-rank RNN performs using overall 6.5K parameters. Generally, it would be good to see what is the impact of the matrix rank given a fix state size. - It would be informative as well to have the baseline and the uRNN curve in Figure 2 for the memory/addition task. - it is not clear when to use low-rank or low-rank + diagonal from the experiments. Overall, the evaluation in its current form in not really convincing, except for the sequential MNIST dataset.","['4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']",[7],[13],[36],[23],[12],[1],"The sentiment score is -50 because the review is generally critical, pointing out several issues with the evaluation and methodology. The reviewer acknowledges some positive aspects (e.g., 'Author proposes...', 'An empirical evaluation is performed...') but the majority of the review focuses on limitations and problems. The politeness score is 20 because the language is professional and constructive, avoiding harsh criticism. The reviewer uses phrases like 'it would be interesting', 'it would be informative', and 'it is unfortunate' which maintain a respectful tone while conveying concerns. The reviewer also provides specific suggestions for improvement, which is a polite way to offer criticism.",-50,20
MS MARCO: A Human-Generated MAchine Reading COmprehension Dataset,Reject,2017,"['Tri Nguyen', 'Mir Rosenberg', 'Xia Song', 'Jianfeng Gao', 'Saurabh Tiwary', 'Rangan Majumder and Li Deng']","[6, 6]","['Marginally above acceptance threshold', 'Marginally above acceptance threshold']","Paper Summary: This paper presents a new large scale machine reading comprehension dataset called MS MARCO. It is different from existing datasets in that the questions are real user queries, the context passages are real web documents, and free form answers are generated by humans instead of spans in the context. The paper also includes some analysis of the dataset and performance of QA models on the dataset. Paper Strengths: -- The questions in the dataset are real queries from users instead of humans writing questions given some context. -- Context passages are extracted from real web documents which are used by search engines to find answers to the given query. -- Answers are generated by humans instead of being spans in context. -- It is large scale dataset, with an aim of 1 million queries. Current release includes 100,000 queries. Paper Weaknesses: -- The authors say, *We have found that the distribution of actual questions users ask intelligent agents can be very different from those conceived from crowdsourcing them from the text.*, but the statement is not backed up with any study. -- The paper doesn*t clearly present what additional information can today*s QA models learn from MS MARCO which they can*t from existing datasets. -- The paper should talk about what challenges are involved in obtaining a good performance on this dataset. -- What are the human performances as compared to the models presented in the paper? -- In section 4.1, what are the train/test splits? The results are for the subset of MS MARCO where every query has multiple answers. How big is that subset? -- What is DSSM mentioned in row 2, Table 5? -- The authors should include in the paper how experiments in section 4.2 prove that MS MARCO is a better dataset. -- In Table 6, the performance of Memory Networks is already close to Best Passage. Does that mean there is not enough room for improvement there? -- The paper seems to be written in hurry, with partial analysis, evaluation and various mistakes in the text. Preliminary Evaluation: The proposed dataset MS MARCO is unique from existing datasets as it is a good representative of the QA task encountered by search engines. I think it can be a very useful dataset for the community to benefit from. Given the huge potential in the dataset, this paper lacks the analysis and evaluation needed to present the dataset*s worth. I think it can benefit a lot with a more comprehensive analysis of the dataset.","['3', '3']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[21, 2, 3, 18, 3, 2, 27]","[27, 1, 9, 24, 9, 8, 30]","[40, 2, 51, 545, 25, 13, 329]","[17, 1, 21, 263, 12, 4, 188]","[14, 1, 25, 250, 13, 9, 36]","[9, 0, 5, 32, 0, 0, 105]","The sentiment score is slightly positive (20) because the reviewer acknowledges the unique aspects and potential of the MS MARCO dataset, calling it 'very useful' for the community. However, they also point out several weaknesses and areas for improvement, which tempers the overall positivity. The politeness score is moderately high (60) as the reviewer uses professional and constructive language throughout, offering specific suggestions for improvement without using harsh or critical tones. They acknowledge both strengths and weaknesses in a balanced manner, and use phrases like 'I think it can benefit' which suggest a collaborative rather than confrontational approach.",20,60
Memory-augmented Attention Modelling for Videos,Reject,2017,"['Rasool Fakoor', 'Abdel-rahman Mohamed', 'Margaret Mitchell', 'Sing Bing Kang', 'Pushmeet Kohli']","[4, 4]","['Ok but not good enough - rejection', 'Ok but not good enough - rejection']","The authors propose a *hierarchical* attention model for video captioning. They introduce a model composed of three parts: the temporal modeler (TEM) that takes as input the video sequence and outputs a sequential representation of the video to the HAM; the hierarchical attention/memory mechanism (HAM) implements a soft-attention mechanism over the sequential video representation; and finally a decoder that generates a caption. Related to the second series of questions above, it seems as though the authors have chosen to refer to their use of an LSTM (or equivalent RNN) as the output of the Bahdanau et al (2015) attention mechanism as a hierarchical memory mechanism. I am actually sympathetic to this terminology in the sense that the recent popularity of memory-based models seems to neglect the memory implicit in the LSTM state vector, but that said, this seems to seriously misrepresent the significance fo the contribution of this paper. I appreciate the ablation study presented in Table 1. Not enough researchers bother with this kind of analysis. But it does show that the value of the contributions is not actually clear. In particular the case for the TEM is quite weak. Regarding the quantitative evaluation presented in Table 2, the authors are carving out a fairly specific set of features to describe the set of *fair* comparators from the literature. Given the variability of the models and alternate training datasets that are in use, I would find it more compelling if the authors just set about trying to achieve the best results they can, if that includes the fine-tuning of the frame model, so be it. The value of this work is as an application paper, so the discovery and incorporation of elements that can significantly improve performance would seems warranted. Overall, at this point, I do not see a sufficient contribution to warrant publication in ICLR.","['4', '5']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[6, 8, 18, 27, 15]","[11, 14, 24, 33, 21]","[26, 112, 93, 188, 322]","[11, 50, 52, 97, 180]","[15, 54, 35, 20, 102]","[0, 8, 6, 71, 40]","The sentiment score is -60 because the reviewer expresses significant doubts about the paper's contributions and concludes that it does not warrant publication. They criticize the terminology used, question the significance of the contributions, and find the evaluation methodology lacking. However, it's not entirely negative as they do appreciate the ablation study. The politeness score is 20 because while the reviewer is critical, they express their concerns in a professional and constructive manner. They use phrases like 'I appreciate' and 'I am actually sympathetic', which soften the criticism. The reviewer also provides specific suggestions for improvement, which is a polite way to offer criticism.",-60,20
Multi-modal Variational Encoder-Decoders,Reject,2017,"['Iulian V. Serban', 'Alexander G. Ororbia II', 'Joelle Pineau', 'Aaron Courville']","[4, 4, 3]","['Ok but not good enough - rejection', 'Ok but not good enough - rejection', 'Clear rejection']","The authors introduce some new prior and approximate posterior families for variational autoencoders, which are compatible with the reparameterization trick, as well as being capable of expressing multiple modes. They also introduce a gating mechanism between prior and posterior. They show improvements on bag of words document modeling, and dialogue response generation. The original abstract is overly strong in its assertion that a unimodal latent prior p(z) cannot fit a multimodal marginal int_z p(x|z)p(x)dz with a DNN response model p(x|z) (*it cannot possibly capture more complex aspects of the data distribution*, *critical restriction*, etc). While the assertion that a unimodal latent prior is necessary to model multimodal observations is false, there are sensible motivations for the piecewise constant prior and posterior. For example, if we think of a VAE as a sort of regularized autoencoder where codes are constrained to *fill up* parts of the prior latent space, then there is a sphere-packing argument to be made that filling a Gaussian prior with Gaussian posteriors is a bad use of code space. Although the authors don*t explore this much, a hypercube-based tiling of latent code space is a sensible idea. As stated, I found the message of the paper to be quite sloppy with respect to the concept of *multi-modality.* There are 3 types of multimodality at play here: multimodality in the observed marginal distribution p(x), which can be captured by any deep latent Gaussian model, multimodality in the prior p(z), which makes sense in some situations (e.g. a model of MNIST digits could have 10 prior modes corresponding to latent codes for each digit class), and multimodality in the posterior z for a given observation x_i, q(z_i|x_i). The final type of multimodality is harder to argue for, except in so far as it allows the expression of flexibly shaped distributions without highly separated modes. I believe flexible posterior approximations are important to enable fine-grained and efficient tiling of latent space, but I don*t think these need to have multiple strong modes. I would be interested to see experiments demonstrating otherwise for real world data. I think this paper should be more clear about the different types of multi-modality and which parts of their analysis demonstrate which ones. I also found it unsatisfactory that the piecewise variable analysis did not show different components of the multi-modal prior corresponding to different words, but rather just a separation between the Gaussian and the piecewise variables. As I mention in my earlier questions, I found it surprising that the learned variance and mean for the Gaussian prior helps so dramatically with G-NVDM likelihood when the powerful networks transforming to and from latent space should make it scale-invariant. Explicitly separating out the contributions of a reimplemented base model, prior-posterior interpolation and the learned prior parameters would strengthen these experiments. Overall, the very strong improvements on the text modeling task over NVDM seem hard to understand, and I would like to see an ablation analysis of all the differences between that model and the proposed one. The fact that adding more constant components helps for document modeling is interesting, and it would be nice to see more qualitative analysis of what the prior modes represent. I also would be surprised if posterior modes were highly separated, and if they were it would be interesting to explore if they corresponded to e.g. ambiguous word-senses. The experiments on dialog modeling are mostly negative results, quantitatively. The observation that the the piecewise constant variables encode time-related words and the Gaussian variables encode sentiment is interesting, especially since it occurs in both sets of experiments. This is actually quite interesting, and I would be interested in seeing analysis of why this is the case. As above, I would like to see an analysis of the sorts of words that are encoded in the different prior modes and whether they correspond to e.g. groups of similar holidays or days. In conclusion, I think the piecewise constant variational family is a good idea, although it is not well-motivated by the paper. The experimental results are very good for document modeling, but without ablation analysis against the baseline it is hard to see why they should be with such a small modification in G-NVDM. The fact that H-NVDM performs better is interesting, though. This paper should better motivate the need for different types of multi-modality, and demonstrate that those sorts of things are actually being captured by the model. As it is, the paper introduces an interesting variational family and shows that it performs better for some tasks, but the motivation and analysis is not clearly focused. To demonstrate that this is a broadly applicable family, it would also be good to do experiments on a more standard datasets like MNIST. Even without an absolute log-likelihood improvement, if the method yielded interpretable multiple modes this would be a valuable contribution.","['5', '4', '4']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[3, 4, 18, 17]","[9, 10, 24, 23]","[50, 102, 308, 309]","[19, 41, 151, 135]","[27, 53, 119, 160]","[4, 8, 38, 14]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects of the paper (e.g., 'The experimental results are very good for document modeling'), they also express several criticisms and concerns. The reviewer points out that the paper's assertions are 'overly strong' and 'sloppy' in some respects, and they request additional analyses and clarifications. The overall tone suggests that the paper has potential but needs significant improvements.

The politeness score is moderately positive (50) because the reviewer maintains a professional and constructive tone throughout. They use phrases like 'I would be interested to see' and 'It would be nice to see' when suggesting improvements, which is polite and encouraging. The reviewer also acknowledges the paper's strengths and interesting findings alongside their criticisms. However, some direct criticisms (e.g., calling the paper 'sloppy' with respect to certain concepts) prevent the score from being higher.",-20,50
Multi-task learning with deep model based reinforcement learning,Reject,2017,['Asier Mujika'],"[4, 2, 4]","['Ok but not good enough - rejection', 'Strong rejection', 'Ok but not good enough - rejection']","This paper proposes a model-based reinforcement learning approach focusing on predicting future rewards given a current state and future actions. This is achieved with a *residual recurrent neural network*, that outputs the expected reward increase at various time steps in the future. To demonstrate the usefulness of this approach, experiments are conducted on Atari games, with a simple playing strategy that consists in evaluating random sequences of moves and picking the one with highest expected reward (and low enough chance of dying). Interestingly, out of the 3 games tested, one of them exhibits better performance when the agent is trained in a multitask setting (i.e. learning all games simultaneously), hinting that transfer learning is occurring. This submission is easy enough to read, and the reward prediction architecture looks like an original and sound idea. There are however several points that I believe prevent this work from reaching the ICLR bar, as detailed below. The first issue is the discrepancy between the algorithm proposed in Section 3 vs its actual implementation in Section 4 (experiments): in Section 3 the output is supposed to be the expected accumulated reward in future time steps (as a single scalar), while in experiments it is instead two numbers, one which is the probability of dying and another one which is the probability of having a higher score without dying. This might work better, but it also means the idea as presented in the main body of the paper is not actually evaluated (and I guess it would not work well, as otherwise why implement it differently?) In addition, the experimental results are quite limited: only on 3 games that were hand-picked to be easy enough, and no comparison to other RL techniques (DQN & friends). I realize that the main focus of the paper is not about exhibiting state-of-the-art results, since the policy being used is only a simple heuristic to show that the model predictions can ne used to drive decisions. That being said, I think experiments should have tried to demonstrate how to use this model to obtain better reinforcement learning algorithms: there is actually no reinforcement learning done here, since the model is a supervised algorithm, used in a manually-defined hardcoded policy. Another question that could have been addressed (but was not) in the experiments is how good these predictions are (e.g. classification error on dying probability, MSE on future rewards, ...), compared to simpler baselines. Finally, the paper*s *previous work* section is too limited, focusing only on DQN and in particular saying very little on the topic of model-based RL. I think a paper like for instance *Action-Conditional Video Prediction using Deep Networks in Atari Games* should have been an obvious *must cite*. Minor comments: - Notations are unusual, with *a* denoting a state rather than an action, this is potentially confusing and I see no reason to stray away from standard RL notations - Using a dot for tensor concatenation is not a great choice either, since the dot usually indicates a dot product - The r_i in 3.2.2 is a residual that has nothing to do with r_i the reward - c_i is defined as *The control that was performed at time i*, but instead it seems to be the control performed at time i-1 - There is a recurrent confusion between mean and median in 3.2.2 - x should not be used in Observation 1 since the x from Fig. 3 does not go through layer normalization - The inequality in Observation 1 should be about |x_i|, not x_i - Observation 1 (with its proof) takes too much space for such a simple result - In 3.2.3 the first r_j should be r_i - The probability of dying comes out of nowhere in 3.3, since we do not know yet it will be an output of the model - *Our approach is not able to learn from good strategies* => did you mean **only* from good strategies*? - Please say that in Fig. 4 *fc* means *fully connected* - It would be nice also to say how the architecture of Fig. 4 differs from the classical DQN architecture from Mnih et al (2015) - Please clarify r_j2 as per your answer in OpenReview comments - Table 3 says *After one iteration* but has *PRL Iteration 2* in it, which is confusing - *Figure 5 shows that not only there is no degradation in Pong and Demon Attack*=> to me it seems to be a bit worse, actually - *A model that has learned only from random play is able to play at least 7 times better.* => not clear where this 7 comes from - *Demon Attack*s plot in Figure 5c shows a potential problem we mentioned earlier* => where was it mentioned?","['4', '5', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct']",[2],[8],[16],[5],[10],[1],"The sentiment score is -50 because while the reviewer acknowledges some positive aspects of the paper (e.g., 'original and sound idea', 'easy enough to read'), they express several significant concerns that 'prevent this work from reaching the ICLR bar'. These include discrepancies between the proposed algorithm and its implementation, limited experimental results, lack of comparison to other RL techniques, and insufficient coverage of previous work. The overall tone suggests the paper needs substantial improvements.

The politeness score is 50 because the reviewer maintains a professional and constructive tone throughout. They begin by highlighting positive aspects before delving into criticisms. The language used is respectful and focuses on the work rather than the authors. Phrases like 'I believe' and 'I think' are used to soften criticisms. The reviewer also provides specific suggestions for improvement, which is a polite way to offer criticism. However, the score is not higher as the review doesn't go out of its way to be exceptionally polite or encouraging.",-50,50
Multi-view Generative Adversarial Networks,Reject,2017,"['Mickaël Chen', 'Ludovic Denoyer']","[3, 5, 6]","['Clear rejection', '5', 'Marginally above acceptance threshold']","The paper proposed conditional biGAN and its extension to multi-view biGAN. The main idea of conditional biGAN is to matching the latent variable distributions of two encoders, each of which are conditioned on the observation (	ilde{x}) and the output (y), respectively, in addition to standard biGAN formulation. The description on MV-GAN require more revision. Specifically, the definition on aggregating model, Phi, mapping v and a variable s should be clarified. Looking at Equation (8), I can*t find a term that constrains the output domain of function v to be the same as a data domain. Experimental results are not convincing. In most generation results, the observation is not very well preserved. For example, in Figure 6 the second row, background changes significantly from the observation. We also observe such behavior in digit generation example. Preserving attributes like gender is interesting but doesn*t seem to be a strong indication that the model learn to correlate observation and the output through latent variable.","['3', '3', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[-1, 2, -1, 10, 12, 13]","[4, 8, 4, 16, 17, 19]","[6, 71, 12, 62, 98, 641]","[4, 14, 8, 41, 60, 248]","[2, 39, 4, 13, 24, 210]","[0, 18, 0, 8, 14, 183]","The sentiment score is -50 because the review is generally critical, pointing out several issues with the paper, such as the need for revision in the MV-GAN description, unclear definitions, and unconvincing experimental results. However, it's not entirely negative as it acknowledges some interesting aspects of the work. The politeness score is 0 (neutral) because the reviewer uses direct and factual language without being overtly polite or rude. They state their observations and criticisms plainly without using harsh language or personal attacks, but also without using particularly courteous phrases.",-50,0
NEUROGENESIS-INSPIRED DICTIONARY LEARNING: ONLINE MODEL ADAPTION IN A CHANGING WORLD,Reject,2017,"['Sahil Garg', 'Irina Rish', 'Guillermo Cecchi', 'Aurelie Lozano']","[7, 5, 5]","['Good paper, accept', '5', '5']","The authors propose a simple modification of online dictionary learning: inspired by neurogenesis, they propose to add steps of atom addition, or atom deletion, in order to extent the online dictionary learning algorithm algorithm of Mairal et al. Such extensions helps to adapt the dictionary to changing properties of the data. The online adaptation is very interesting, even if it is quite simple. The overall algorithm is quite reasonable, but not always described in sufficient details: for example, the thresholds or conditions for neuronal birth or death are not supported by a strong analysis, even if the resulting algorithm seems to perform well on quite extensive experiments. The overall idea is nevertheless interesting (even if not completely new), and the paper generally well written and pretty easy to follow. The analysis is however quite minimal: it could have been interesting to study the evolving properties of the dictionary, to analyse its accuracy for following the changes in the data, etc. Still: this is a nice work!","['4', '3', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[6, 24, 22, 16]","[12, 30, 28, 22]","[209, 130, 104, 64]","[55, 66, 67, 38]","[20, 51, 22, 17]","[134, 13, 15, 9]","The sentiment score is 60 (positive) because the reviewer describes the work as 'interesting,' 'reasonable,' and 'nice,' while also acknowledging its strengths. They mention that the idea is 'quite simple' but still valuable. The reviewer does point out some limitations, such as insufficient details and minimal analysis, which prevents a higher score. The politeness score is 80 (quite polite) because the reviewer uses respectful language throughout, balancing praise with constructive criticism. They use phrases like 'interesting,' 'well written,' and 'nice work' to soften their critiques. The reviewer also suggests improvements without being harsh or dismissive, maintaining a professional and courteous tone.",60,80
NEWSQA: A MACHINE COMPREHENSION DATASET,Reject,2017,"['Adam Trischler', 'Tong Wang', 'Xingdi Yuan', 'Justin Harris', 'Alessandro Sordoni', 'Philip Bachman', 'Kaheer Suleman']","[6, 6, 6]","['Marginally above acceptance threshold', 'Marginally above acceptance threshold', 'Marginally above acceptance threshold']","It would seem that the shelf life of a dataset has decreased rapidly in recent literature. SQuAD dataset has been heavily pursued as soon as it hit online couple months ago, the best performance on their leaderboard now reaching to 82%. This is rather surprising when taking into account the fact that the formal conference presentation of the dataset took place only a month ago at EMNLP’16, and that the reported machine performance (at the time of paper submission) was only at 51%. One reasonable speculation is that the dataset may have not been hard enough. NewsQA, the paper in submission, aims to address this concern by presenting a dataset of a comparable scale created through different QA collection strategies. Most notably, the authors solicit questions without requiring answers from the same turkers, in order to promote more diverse and hard-to-answer questions. Another notable difference is that the questions are gathered without showing the content of the news articles, and the dataset makes use of a bigger subset of CNN/Daily corpus (12K / 90K), as opposed to a much smaller subset (500 / 90K) used by SQuAD. In sum, I think NewsQA dataset presents an effort to construct a harder, large-scale reading comprehension challenge, a recently hot research topic for which we don’t yet have satisfying datasets. While not without its own weaknesses, I think this dataset presents potential values compared to what are available out there today. That said, the paper does read like it was prepared in a hurry, as there are numerous small things that the authors could have done better. As a result, I do wonder about the quality of the dataset. For one, human performance of SQuAD measured by the authors (70.5 - 82%) is lower than that reported by SQuAD (80.3 - 90.5%). I think this sort of difference can easily happen depending on the level of carefulness the annotators can maintain. After all, not all humans have the same level of carefulness or even the same level of reading comprehension. I think it’d be the best if the authors can try to explain the reason behind these differences, and if possible, perform a more careful measurement of human performance. If anything, I don’t think it looks favorable for NewsQA if the human performance is only at the level of 74.9%, as it looks as if the difficulty of the dataset comes mainly from the potential noise from the QA collection process, which implies that the low model performance could result from not necessarily because of the difficulty of the comprehension and reasoning, but because of incorrect answers given by human annotators. I’m also not sure whether the design choice of not presenting the news article when soliciting the questions was a good one. I can imagine that people might end up asking similar generic questions when not enough context has been presented. Perhaps taking a hybrid, what I would like to suggest is to present news articles where some sentences or phrases are randomly redacted, so that the question generators can have a bit more context while not having the full material in front of them. Yet another way of encouraging the turkers from asking too trivial questions is to engage an automatic QA system on the fly — turkers must construct a QA pair for which an existing state-of-the-art system cannot answer correctly.","['4', '4', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[2, 9, 7, 15, 9, 9, 4]","[8, 14, 13, 17, 15, 15, 10]","[82, 35, 59, 13, 95, 51, 37]","[39, 18, 23, 8, 52, 25, 16]","[43, 16, 35, 3, 40, 25, 19]","[0, 1, 1, 2, 3, 1, 2]","The sentiment score is 50 (slightly positive) because the reviewer acknowledges the potential value of the NewsQA dataset compared to existing options, but also expresses concerns about its quality and preparation. The reviewer sees merit in the effort to create a harder, large-scale reading comprehension challenge, while pointing out several areas for improvement. The politeness score is 75 (quite polite) because the reviewer uses respectful language throughout, offering constructive criticism and suggestions rather than harsh judgments. Phrases like 'I think,' 'I do wonder,' and 'I would like to suggest' indicate a considerate tone. The reviewer also balances positive observations with areas of concern, maintaining a professional and courteous approach to the feedback.",50,75
Near-Data Processing for Machine Learning,Reject,2017,"['Hyeokjun Choe', 'Seil Lee', 'Hyunha Nam', 'Seongsik Park', 'Seijoon Kim', 'Eui-Young Chung', 'Sungroh Yoon']","[4, 6, 5]","['Ok but not good enough - rejection', 'Marginally above acceptance threshold', '5']","For more than a decade, near data processing has been a key requirement for large scale linear learning platforms, as the time to load the data exceeds the learning time, and this has justified the introduction of approaches such as Spark Deep learning usually deals with the data that can be contained in a single machine and the bottleneck is often the CPU-GPU bus or the GPU-GPU-bus, so a method that overcomes this bottleneck could be relevant. Unfortunately, this work is still very preliminary and limited to linear training algorithms, so of little interest yet to ICLR readership. I would recommend publication to a conference where it can reach the large-scale linear ML audience first, such as ICML. This paper is clear and well written in the present form and would probably mostly need a proper benchmark on a large scale linear task. Obviously, when the authors have convincing DNN learning simulations, they are welcome to target ICLR, but can the flash memory FPGA handle it? For experiments, the choice of MNIST is somewhat bizarre: this task is small and performance is notoriously terrible when using linear approaches (the authors do not even report it)","['4', '2', '2']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', 'The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']","[-3, -3, 6, 6, 25, 9]","[2, 2, 11, 12, 31, 15]","[3, 6, 18, 31, 380, 76]","[1, 4, 13, 25, 304, 52]","[1, 1, 1, 1, 4, 1]","[1, 1, 4, 5, 72, 23]","The sentiment score is -50 because while the reviewer acknowledges some potential value in the work, they ultimately conclude it's 'very preliminary and limited' and 'of little interest yet to ICLR readership'. They recommend submitting to a different conference, indicating they don't believe it's suitable for ICLR in its current state. However, it's not entirely negative as they do suggest the paper is 'clear and well written' and offer constructive feedback for improvement. The politeness score is 50 because the reviewer uses respectful language throughout, offering constructive criticism and suggestions for improvement rather than harsh criticism. They use phrases like 'I would recommend' and 'Obviously, when the authors have convincing DNN learning simulations, they are welcome to target ICLR', which maintain a polite and encouraging tone despite the overall negative recommendation.",-50,50
Neural Code Completion,Reject,2017,"['Chang Liu', 'Xin Wang', 'Richard Shin', 'Joseph E. Gonzalez', 'Dawn Song']","[5, 5, 4, 4]","['5', '5', 'Ok but not good enough - rejection', 'Ok but not good enough - rejection']","This paper considers the code completion problem: given partially written source code produce a distribution over the next token or sequence of tokens. This is an interesting and important problem with relevance to industry and research. The authors propose an LSTM model that sequentially generates a depth-first traversal over an AST. Not surprisingly the results improve over previous approaches with more brittle conditioning mechanisms (Bielik et al. 2016). Still, simply augmenting previous work with LSTM-based conditioning is not enough of a contribution to justify an entire paper. Some directions that would greatly improve the contribution include: considering distinct traversal orders, does this change the predictive accuracy? Any other ways of dealing with UNK tokens? The ultimate goal of this paper is to improve code completion, and it would be great to go beyond simply neurifying previous methods. Comments: - Last two sentences of related work claim that other methods can only *examine a limited subset of source code*. Aside from being a vague statement, it isn*t accurate. The models described in Bielik et al. 2016 and Maddison & Tarlow 2014 can in principle condition on any part of the AST already generated. The difference in this work is that the LSTM can learn to condition in a flexible way that doesn*t increase the complexity of the computation. - In the denying prediction experiments, the most interesting number is the Prediction Accuracy, which is P(accurate | model doesn*t predict UNK). I think it would also be interesting to see P(accurate | UNK is not ground truth). Clearly the models trained to ignore UNK losses will do worse overall, but do they do worse on non-UNK tokens?","['4', '4', '4', '5']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[8, 11, -2, 14, 3, -4, 20, 10]","[14, 17, 3, 19, 8, 1, 25, 15]","[10, 20, 3, 64, 3, 2, 21, 17]","[2, 11, 2, 49, 2, 1, 17, 12]","[5, 3, 1, 6, 1, 1, 1, 2]","[3, 6, 0, 9, 0, 0, 3, 3]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the importance of the problem and some improvements over previous approaches, they express that the contribution is not sufficient to justify an entire paper. They suggest several directions for improvement, indicating that the current work is lacking. The politeness score is moderately positive (50) as the reviewer uses professional language throughout, offers constructive criticism, and provides specific suggestions for improvement. They avoid harsh language and frame their critique in a respectful manner, even when pointing out inaccuracies. The reviewer balances criticism with acknowledgment of the work's merits, maintaining a polite tone overall.",-20,50
Neural Combinatorial Optimization with Reinforcement Learning,Reject,2017,"['Irwan Bello*', 'Hieu Pham*', 'Quoc V. Le', 'Mohammad Norouzi', 'Samy Bengio']","[6, 6, 6]","['Marginally above acceptance threshold', 'Marginally above acceptance threshold', 'Marginally above acceptance threshold']","This paper applies the pointer network architecture—wherein an attention mechanism is fashioned to point to elements of an input sequence, allowing a decoder to output said elements—in order to solve simple combinatorial optimization problems such as the well-known travelling salesman problem. The network is trained by reinforcement learning using an actor-critic method, with the actor trained using the REINFORCE method, and the critic used to estimate the reward baseline within the REINFORCE objective. The paper is well written and easy to understand. Its use of a reinforcement learning and attention model framework to learn the structure of the space in which combinatorial problems of variable size can be tackled appears novel. Importantly, it provides an interesting research avenue for revisiting classical neural-based solutions to some combinatorial optimization problems, using recently-developed sequence-to-sequence approaches. As such, I think it merits consideration for the conference. I have a few comments and some important reservations with the paper: 1) I take exception to the conclusion that the pointer network approach can handle general types of combinatorial optimization problems. The crux of combinatorial problems — for practical applications — lies in the complex constraints that define feasible solutions (e.g. simple generalizations of the TSP that involve time windows, or multiple salesmen). For these problems, it is no longer so simple to exclude possible solutions from the enumeration of the solution by just « striking off » previously-visited instances; in fact, for many of these problems, finding a single feasible solution might in general be a challenge. It would be relevant to include a discussion of whether the Neural Combinatorial Optimization approach could scale to these important classes of problems, and if so, how. My understanding is that this approach, as presented, would be mostly suitable for assignment problems with a very simple constraint structure. 2) The operations research literature is replete with a large number of benchmark problems that have become standard to compare solver quality. For instance, TSPLIB contains a large number of TSP instances (http://comopt.ifi.uni-heidelberg.de/software/TSPLIB95/). Likewise, combinatorial optimization problems of various difficulties can be found here: http://www.mat.univie.ac.at/~neum/glopt/test.html. It would greatly add to the paper depth to compare the NCO solution quality on some of these problems. (The CPLEX solver famously evaluates its own progress on such a public library of problem instances.) 3) The paper should explain more clearly the structure of the critic network, and how it performs the mapping from a sequence of cities into a baseline prediction. 4) Suggestions for Algorithm 1: Line 5, 6: the notation is not clear; would be what’s intended? Line 7: I’m assuming that this assignment must be done , as in lines 5,6? Line 8: is used in two different ways, on the LHS and RHS — slight abuse of notation (but we understand the intent). 5) Suggestions for Algorithm 2: Line 13: same as for Algorithm 1 Line 15: use instead of to indicate multiplication","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[2, 6, 13, 9, 31]","[7, 12, 19, 15, 37]","[17, 58, 300, 137, 261]","[6, 23, 143, 59, 155]","[11, 27, 146, 73, 60]","[0, 8, 11, 5, 46]","The sentiment score is 50 (slightly positive) because the reviewer expresses that the paper is well-written, easy to understand, and merits consideration for the conference. They also note its novelty and potential for opening new research avenues. However, they have 'important reservations' and several critiques, which temper the overall positive sentiment. The politeness score is 80 (quite polite) because the reviewer uses respectful language throughout, acknowledging the paper's strengths before presenting criticisms. They use phrases like 'I think it merits consideration' and 'It would greatly add to the paper depth,' which are constructive and polite ways of offering suggestions. The reviewer also frames their criticisms as 'comments' and 'reservations' rather than outright flaws, maintaining a courteous tone.",50,80
Neural Graph Machines: Learning Neural Networks Using Graphs,Reject,2017,"['Thang D. Bui', 'Sujith Ravi', 'Vivek Ramavajjala']","[3, 4, 3]","['Clear rejection', 'Ok but not good enough - rejection', 'Clear rejection']","The authors introduce a semi-supervised method for neural networks, inspired from label propagation. The method appears to be exactly the same than the one proposed in (Weston et al, 2008) (the authors cite the 2012 paper). The optimized objective function in eq (4) is exactly the same than eq (9) in (Weston et al, 2008). As possible novelty, the authors propose to use the adjacency matrix as input to the neural network, when there are no other features, and show success on the BlogCatalog dataset. Experiments on text classification use neighbors according to word2vec average embedding to build the adjacency matrix. Top reported accuracies are not convincing compared to (Zhang et al, 2015) reported performance. Last experiment is on semantic intent classification, which a custom dataset; neighbors are also found according to a word2vec metric. In summary, the paper propose few applications to the original (Weston et al, 2008) paper. It rebrands the algorithm under a new name, and does not bring any scientific novelty, and the experimental section lacks existing baselines to be convincing.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[4, 11, 6]","[9, 17, 12]","[19, 80, 7]","[9, 53, 4]","[9, 23, 3]","[1, 4, 0]","The sentiment score is -80 because the review is highly critical of the paper. The reviewer states that the method is not novel, being 'exactly the same' as previous work. They also mention that the 'top reported accuracies are not convincing' and that the paper 'does not bring any scientific novelty'. The experimental section is criticized for lacking baselines. These are all strong negative points, indicating a very unfavorable view of the paper.

The politeness score is -20 because while the language is not overtly rude, it is quite blunt and dismissive. The reviewer doesn't use softening language or acknowledge any positive aspects of the work. Phrases like 'rebrands the algorithm under a new name' and 'lacks existing baselines to be convincing' are quite direct criticisms without any attempt to cushion the blow. However, the reviewer does maintain a professional tone and doesn't use personal attacks or explicitly insulting language, which is why the score is not lower.",-80,-20
Neural Machine Translation with Latent Semantic of Image and Text,Reject,2017,"['Joji Toyama', 'Masanori Misono', 'Masahiro Suzuki', 'Kotaro Nakayama', 'Yutaka Matsuo']","[3, 4, 3]","['Clear rejection', 'Ok but not good enough - rejection', 'Clear rejection']","This paper proposes a multimodal neural machine translation that is based upon previous work using variational methods but attempts to ground semantics with images. Considering way to improve translation with visual information seems like a sensible thing to do when such data is available. As pointed out by a previous reviewer, it is not actually correct to do model selection in the way it was done in the paper. This makes the gains reported by the authors very marginal. In addition, as the author*s also said in their question response, it is not clear if the model is really learning to capture useful image semantics. As such, it is unfortunately hard to conclude that this paper contributes to the direction that originally motivated it.","['4', '5', '5']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[2, 2, 16, 13, 20]","[3, 7, 22, 18, 26]","[2, 6, 53, 44, 252]","[1, 5, 27, 32, 156]","[1, 1, 12, 6, 52]","[0, 0, 14, 6, 44]","The sentiment score is -50 because the review starts with a neutral description of the paper's approach, but then points out significant flaws in the methodology and concludes that the paper's contribution is marginal. The reviewer expresses disappointment in the paper's ability to achieve its original goals. The politeness score is 20 because the reviewer uses professional and respectful language throughout, acknowledging the paper's intentions and rationale. However, the criticism is direct and doesn't employ overly polite phrasing to soften the negative feedback. The reviewer also refers to previous feedback and the authors' own responses, showing engagement with the review process.",-50,20
Non-linear Dimensionality Regularizer for Solving Inverse Problems,Reject,2017,"['Ravi Garg', 'Anders Eriksson', 'Ian Reid']","[4, 3, 4]","['Ok but not good enough - rejection', 'Clear rejection', 'Ok but not good enough - rejection']","The paper proposes a nonlinear regularizer for solving ill-posed inverse problems. The latent variables (or causal factors) corresponding to the observed data are assumed to lie near a low dimensional subspace in an RKHS induced by a predetermined kernel. The proposed regularizer can be seen as an extension of the linear low-rank assumption on the latent factors. A nuclear norm penalty on the Cholesky factor of the kernel matrix is used as a relaxation for the dimensionality of the subspace. Empirical results are reported on two tasks involving linear inverse problems -- missing feature imputation, and estimating non-rigid 3D structures from a sequence of 2D orthographic projections -- and the proposed method is shown to outperform linear low-rank regularizer. The clarity of the paper has scope for improvement (particularly, Introduction) - the back and forth b/w dimensionality reduction techniques and inverse problems is confusing at times. Clearly defining the ill-posed inverse problem first and then motivating the need for a regularizer (which brings dimensionality reduction techniques into the picture) may be a more clear flow in my opinion. The motivation behind relaxation of rank() in Eq 1 to nuclear-norm in Eq 2 is not clear to me in this setting. The relaxation does not yield a convex problem over S,C (Eq 5) and also increases the computations (Algo 2 needs to do full SVD of K(S) every time). The authors should discuss pros/cons over the alternate approach that fixes the rank of C (which can be selected using cross-validation, in the same way as is selected), leaving just the first two terms in Eq 5. For this simpler objective, an interesting question to ask would be -- are there kernel functions for which it can solved in a scalable manner? The proposed alternating optimization approach in the current form is computationally intensive and seems hard to scale to even moderate sized data -- in every iteration one needs to compute the kernel matrix over S and perform full SVD over the kernel matrix (Algo 2). Empirical evaluations are also not extensive -- (i) the dataset used for feature imputation is old and non-standard, (ii) for structure estimation from motion on CMU dataset, the paper only compares with linear low-rank regularization, (iii) there is no comment/study on the convergence of the alternating procedure (Algo 1).","['4', '5', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[8, 14, 28]","[13, 18, 34]","[53, 78, 439]","[32, 45, 229]","[13, 21, 141]","[8, 12, 69]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the paper's contributions and empirical results, they also point out several areas for improvement. The reviewer mentions that the clarity of the paper needs improvement, questions some of the methodological choices, and suggests that the empirical evaluations are not extensive enough. However, the tone is not entirely negative, as the reviewer also recognizes the paper's novel approach and its performance improvements over existing methods. The politeness score is moderately positive (50) because the reviewer maintains a professional and constructive tone throughout. They use phrases like 'in my opinion' and 'may be' when suggesting improvements, which softens the criticism. The reviewer also balances critique with recognition of the paper's strengths. However, the review doesn't go out of its way to be overly polite or complimentary, maintaining a neutral, academic tone overall.",-20,50
On orthogonality and learning recurrent networks with long term dependencies,Reject,2017,"['Eugene Vorontsov', 'Chiheb Trabelsi', 'Samuel Kadoury', 'Chris Pal']","[7, 5, 5]","['Good paper, accept', '5', '5']","This paper investigates the impact of orthogonal weight matrices on learning dynamics in RNNs. The paper proposes a variety of interesting optimization formulations that enforce orthogonality in the recurrent weight matrix to varying degrees. The experimental results demonstrate several conclusions: enforcing exact orthogonality does not help learning, while enforcing soft orthogonality or initializing to orthogonal weights can substantially improve learning. While some of the optimization methods proposed currently require matrix inversion and are therefore slow in wall clock time, orthogonal initialization and some of the soft orthogonality constraints are relatively inexpensive and may find their way into practical use. The experiments are generally done to a high standard and yield a variety of useful insights, and the writing is clear. The experimental results are based on using a fixed learning rate for the different regularization strengths. Learning speed might be highly dependent on this, and different strengths may admit different maximal stable learning rates. It would be instructive to optimize the learning rate for each margin separately (maybe on one of the shorter sequence lengths) to see how soft orthogonality impacts the stability of the learning process. Fig. 5, for instance, shows that a sigmoid improves stability—but perhaps slightly reducing the learning rate for the non-sigmoid Gaussian prior RNN would make the learning well-behaved again for weightings less than 1. Fig. 4 shows singular values converging around 1.05 rather than 1. Does initializing to orthogonal matrices multiplied by 1.05 confer any noticeable advantage over standard orthogonal matrices? Especially on the T=10K copy task? “Curiously, larger margins and even models without sigmoidal constraints on the spectrum (no margin) performed well as long as they were initialized to be orthogonal suggesting that evolution away from orthogonality is not a serious problem on this task.” This is consistent with the analysis given in Saxe et al. 2013, where for deep linear nets, if a singular value is initialized to 1 but dies away during training, this is because it must be zero to implement the desired input-output map. More broadly, an open question has been whether orthogonality is useful as an initialization, as proposed by Saxe et al., where its role is mainly as a preconditioner which makes optimization proceed quickly but doesn’t fundamentally change the optimization problem; or whether it is useful as a regularizer, as proposed by Arjovsky et al. 2015 and Henaff et al. 2015, that is, as an additional constraint in the optimization problem (minimize loss subject to weights being orthogonal). These experiments seem to show that mere initialization to orthogonal weights is enough to reap an optimization speed advantage, and that too much regularization begins to hurt performance—i.e., substantially changing the optimization problem is undesirable. This point is also apparent in Fig. 2: In terms of the training loss on MNIST (Fig. 2), no margin does almost indistinguishably from a margin of 1 or .1. However in terms of accuracy, a margin of .1 is best. This shows that large or nonexistent margins (i.e., orthogonal initializations) enable fast optimization of the training loss, but among models that attain similar training loss, the more nearly orthogonal weights perform better. This starts to separate out the optimization speed advantage conferred by orthogonality from the regularization advantage it confers. It may be useful to more explicitly discuss the initialization vs regularization dimension in the text. Overall, this paper contributes a variety of techniques and intuitions which are likely to be useful in training RNNs.","['4', '4', '5']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[4, 1, 12, 21]","[10, 6, 18, 27]","[25, 10, 118, 231]","[8, 4, 62, 98]","[13, 6, 20, 110]","[4, 0, 36, 23]","The sentiment score is 80 (positive) because the reviewer expresses a generally favorable view of the paper, praising its 'interesting optimization formulations', 'high standard' experiments, and 'clear' writing. They note that the paper contributes 'useful insights' and techniques 'likely to be useful in training RNNs'. The few criticisms are constructive and presented as suggestions for improvement rather than major flaws. The politeness score is 70 (polite) due to the reviewer's consistently respectful and professional tone. They use phrases like 'it would be instructive to' and 'it may be useful to' when suggesting improvements, avoiding harsh or demanding language. The reviewer also acknowledges the paper's strengths before offering critiques, which is a polite approach. However, the score is not 100 as the review maintains a formal, academic tone rather than being overtly friendly or complimentary.",80,70
On the Expressive Power of Deep Neural Networks,Reject,2017,"['Maithra Raghu', 'Ben Poole', 'Jon Kleinberg', 'Surya Ganguli', 'Jascha Sohl-Dickstein']","[3, 6, 5]","['Clear rejection', 'Marginally above acceptance threshold', '5']","This paper presents a theoretical and empirical approach to the problem of understanding the expressivity of deep networks. Random networks (deep networks with random Gaussian weights, hard tanh or ReLU activation) are studied according to several criterions: number of neutron transitions, activation patterns, dichotomies and trajectory length. There doesn*t seem to be a solid justification for why the newly introduced measures of expressivity really measure expressivity. For instance the trajectory length seems a very discutable measure of expressivity. The only justification given for why it should be a good measure of expressivity is proportionality with other measures of expressivity in the specific case of random networks. The paper is too obscure and too long. The work may have some interesting ideas but it does not seem to be properly replaced in context. Some findings seem trivial. detailed comments p2 *Much of the work examining achievable functions relies on unrealistic architectural assumptions such as layers being exponentially wide* I don’t think so. In *Deep Belief Networks are Compact Universal Approximators* by Leroux et al., proof is given that deep but narrow feed-forward neural networks with sigmoidal units can represent any Boolean expression i.e. A neural network with 2n?1 + 1 layers of n units (with n the number of input neutron). “Comparing architectures in such a fashion limits the generality of the conclusions” To my knowledge much of the previous work has focused on mathematical proof, and has led to very general conclusions on the representative power of deep networks (one example being Leroux et al again). It is much harder to generalise the approach you propose, based on random networks which are not used in practice. “[we study] a family of networks arising in practice: the behaviour of networks after random initialisation” These networks arise in practice as an intermediate step that is not used to perform computations; this means that the representative power of such intermediate networks is a priori irrelevant. You would need to justify why it is not. “results on random networks provide natural baselines to compare trained networks with” random networks are not “natural” for the study of expressivity of deep networks. It is not clear how the representative power of random networks (what kind of random networks seems an important question here) is linked to the representative power of (i) of the whole class of networks or (ii) the class of networks after training. Those two classes of networks are the ones we would a priori care about and you would need to justify why the study of random networks helps in understanding either (i) or (ii). p5 “As FW is a random neural network […] it would suggest that points far enough away from each other would have independent signs, i.e. a direct proportionality between the length of z(n)(t) and the number of times it crosses the decision boundary.” As you say, it seems that proportionality of the two measures depends on the network being random. This seems to invalidate generalisation to other networks, i.e. if the networks are not random, one would assume that path lengths are not proportional. p6 the expressivity w.r.t. remaining depth seems a trivial concerns, completely equivalent to the expressivity w.r.t. depth. This makes the remark in figure 5 that the number of achievable dichotomies only depends *only* on the number of layers above the layer swept seem trivial p7 in figure 6 a network width of 100 for MNIST seems much too small. Accordingly performance is very poor and it is difficult to generalise the results to relevant situations.","['3', '5', '3']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is fairly confident that the evaluation is correct']","[3, 3, 8]","[9, 9, 14]","[33, 29, 162]","[6, 10, 64]","[22, 16, 88]","[5, 3, 10]","The sentiment score is -70 because the review is predominantly negative. The reviewer criticizes the paper's justification, clarity, and relevance, using phrases like 'doesn't seem to be a solid justification', 'too obscure and too long', and 'Some findings seem trivial'. The politeness score is -20 because while the reviewer doesn't use overtly rude language, the tone is quite critical and dismissive. The reviewer challenges the authors' claims directly without much softening language, and uses phrases like 'I don't think so' and 'It is not clear how', which come across as somewhat blunt. However, the reviewer does provide detailed feedback and explanations for their criticisms, which prevents the score from being even lower.",-70,-20
Options Discovery with Budgeted Reinforcement Learning,Reject,2017,"['Aurelia Léon', 'Ludovic Denoyer']","[5, 4, 5, 4]","['5', 'Ok but not good enough - rejection', '5', 'Ok but not good enough - rejection']","In this paper, the authors study the problem of discovering options for reinforcement learning. They introduce the Bi-POMDP model, which is a POMDP where the observations are structured as a pair of elements, with the first element only available to the option-choosing component (the *option level*) and the second element only to the action-choosing component (the *action level*) and the termination component (the *acquisition model*). They also detail the BONN learning model, which consists of three artificial neural network that implement these three components. Finally, they suggest optimizing a tradeoff between the value achieved by the model and the cost of switching between options (the *cognitive effort*), and demonstrate this approach in three simple domains: Cart-Pole, Lunar Lander and two variants of a grid-world maze. The paper is interesting, and adds considerably to the increasing body of research in hierarchical reinforcement learning (HRL). I found no critical flaws in the paper, but also no high-impact insights or impressive improvements. This paper offers some good ideas that are moderately novel and may advance the field, but has some issues. The first issue is that it is unclear how much easier it is to compose Bi-POMDPs than hand-crafted options or subgoals. If Bi-POMDPs are to alleviate the design costs of using HRL with human-defined structure, one needs to show that the splitting of observations into two elements (x, y) is easier to do well enough. For example, in Section 5 the authors are correct in pointing out that sequences of actions (*macro-actions*) are open-loop, and therefore not as expressive as closed-loop options. However, by setting x empty in all but one experiment, the authors also restrict themselves to open-loop sub-policies, albeit stochastic ones. Such sub-policies may be sufficiently expressive for the simple domains in this paper, but this is unlikely so in more realistic domains. In the MAZE_2 domain, x is the agent*s position relative to the current room. Designing this domain-specific observation model requires domain knowledge, arguably no less than designing relevant subgoals. It is hard to judge the effectiveness of this approach without design principles for these domain-specific observation models, and more realistic experiments to evaluate their quality. Finally, it is revealing that the drive for hierarchy is only achieved by limiting x. In Section 3.3 the authors mention that acquisition of y is *crucial for discovering a good policy: an agent only using the observations x_t would be unable to solve the task*, which suggests that the design choice of x directly impacts one side of the trade-off between the value and the cognitive effort. Such an important factor should be addressed explicitly. The authors may be interested in the paper *Learning and Transfer of Modulated Locomotor Controllers* (Heess et al., 2016), which has a similar split observation model, and suffers from the same issue. The second issue is that the option space seems to be expressive enough to represent y with high fidelity. If this is the case, and if the learned option model indeed maintains a good image of y, then the algorithm is really solving a different problem: reinforcement learning with costly observability of y. This means that it learns how to act given a stale value of y and when to refresh it. It should then be framed accordingly and compared with the relevant literature. It may or may not be as interesting or novel. In contrast, the standard options framework calls for compression of y into the choice of option o (usually in a small finite space). The agent should learn to extract subtask-relevant information from y, in a way that generalizes to unseen states or subtasks. That said, the embedding of y in the option space can be interesting in itself, even if it is lossless (1-to-1). Unfortunately, no such analysis was offered by the authors. Minor issues: - In 3.3: error in citation, (?) appears instead. - In 4.1: it is confusing to say that *the environments are more stochastic* when epsilon is increased. It is the agent*s policy, not the environment, that becomes more stochastic, which is useful for exploration. If this hurts performance, some discussion is needed of why too much exploration is detrimental to learning. In particular, the paper does not make explicit the number of iterations in the experiments, and it is not clear whether learning with larger epsilon is worse after some fixed number of iterations or asymptotically.","['4', '5', '5', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[3, 15]","[5, 20]","[5, 147]","[2, 88]","[3, 40]","[0, 19]","The sentiment score is slightly positive (20) because the reviewer acknowledges the paper as 'interesting' and notes that it 'adds considerably to the increasing body of research'. They also state they found 'no critical flaws'. However, the score is not higher because the reviewer mentions 'no high-impact insights or impressive improvements' and points out several issues with the paper. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, offering constructive criticism without harsh words. They use phrases like 'The paper is interesting', 'This paper offers some good ideas', and suggest areas for improvement in a professional manner. The reviewer also offers helpful suggestions and references, showing a collaborative approach rather than a purely critical one.",20,60
Out-of-class novelty generation: an experimental foundation,Reject,2017,"['Mehdi Cherti', 'Balázs Kégl', 'Akın Kazakçı']","[7, 6, 4, 5]","['Good paper, accept', 'Marginally above acceptance threshold', 'Ok but not good enough - rejection', '5']","This paper examines computational creativity from a machine learning perspective. Creativity is defined as a model*s ability to generate new types of objects unseen during training. The authors argue that likelihood training and evaluation are by construction ill-suited for out-of-class generation and propose a new evaluation framework which relies on the use of held-out classes of objects to measure a model*s ability to generate new and interesting object types. I am not very familiar with the literature on computational creativity research, so I can*t judge on how well this work has been put into the context of existing work. From a machine learning perspective, I find the ideas presented in this paper new, interesting and thought-provoking. As I understand, the hypothesis is that the ability of a model to generate new and interesting types we *do not* know about correlates with its ability to generate new and interesting types we *do* know about, and the latter is a good proxy for the former. The extent to which this is true depends on the bias introduced by model selection. Just like when measuring generalization performance, one should be careful not to reuse the same held-out classes for model selection and for evaluation. Nevertheless, I appreciate the effort that has been made to formalize the notion of computational creativity within the machine learning framework. I view it as an important first step in that direction, and I think it deserves its place at ICLR, especially given that the paper is well-written and approachable for machine learning researchers.","['4', '3', '4', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[2, 20, 2]","[8, 25, 3]","[17, 68, 6]","[7, 45, 4]","[10, 9, 2]","[0, 14, 0]","The sentiment score is 80 (positive) because the reviewer expresses a favorable view of the paper, describing it as 'new, interesting and thought-provoking'. They appreciate the effort to formalize computational creativity and believe it deserves a place at ICLR. The reviewer also praises the paper as 'well-written and approachable'. While they mention some limitations, these are presented as constructive feedback rather than major criticisms. The politeness score is 70 (polite) because the reviewer uses respectful language throughout, acknowledging their own limitations ('I am not very familiar with...') and offering balanced feedback. They use phrases like 'I appreciate' and 'I view it as an important first step', which convey a courteous and constructive tone. The reviewer also frames potential issues as considerations rather than direct criticisms, maintaining a polite discourse.",80,70
"ParMAC: distributed optimisation of nested functions, with application to binary autoencoders",Reject,2017,"['Miguel A. Carreira-Perpinan', 'Mehdi Alizadeh']","[5, 4, 6, 6]","['5', 'Ok but not good enough - rejection', 'Marginally above acceptance threshold', 'Marginally above acceptance threshold']","The paper presents an architecture to parallelize the optimization of nested functions based on the method of auxiliary coordinates (MAC) (Carreira-Perpinan and Wang, 2012). This method decomposes the optimization into training individual layers and updating the auxiliary coordinates. The paper focuses on binary autoencoders and proposes to partition the data onto several machines allowing the parameters to move between machines. Relatively good speedup factors are reported especially on larger datasets and a theoretical model of performance is presented that matches with the experiments. My main concern is that even though the method is presented as a general framework for nested functions, experiments focus on a restricted family of models (i.e. binary autoencoders with linear or kernel encoders and linear decoders) with only two components. While the speedup factors are encouraging, it is hard to get a sense of their importance as the binary autoencoder model considered is not well studied by other researchers and is not widely used. I encourage the authors to apply this framework to more generic architectures and problems. Questions: 1- Does this framework apply to some form of generic multi-layer neural network? If so, some experimental results are useful. 2- What is the implication of applying this framework to more than two components (an encoder and a decoder) and non-linear components? 3- It is desired to see a plot of performance as a function of time for different setups to demonstrate the speedup after convergence. It seems the paper only focuses on the speedup factors per iteration. For example, increasing the mini-batch size may improve the speed per iteration but may hurt the convergence speed. 4- Did you consider a scenario where the dataset is too big that storing the data and auxiliary variables on multiple machines simultaneously is not possible? The paper cites an ArXiv manuscript with the same title by the authors multiple times. Please make the paper self-contained and include any supplementary material in the appendix. I believe without applying this framework to a more generic architecture beyond binary autoencoders, this paper does not appeal to a wide audience at ICLR, hence weak reject.","['4', '2', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[20, 7]","[26, 11]","[131, 6]","[96, 1]","[26, 1]","[9, 4]","The sentiment score is -50 because the reviewer expresses significant concerns about the paper's limited scope and applicability, leading to a 'weak reject' recommendation. However, it's not entirely negative as they acknowledge some positive aspects like good speedup factors. The politeness score is 50 because the reviewer uses respectful language, poses questions for improvement, and encourages the authors to expand their work, rather than being harshly critical. The reviewer maintains a professional tone throughout, balancing critique with constructive feedback.",-50,50
Parallel Stochastic Gradient Descent with Sound Combiners,Reject,2017,"['Saeed Maleki', 'Madanlal Musuvathi', 'Todd Mytkowicz', 'Yufei Ding']","[4, 4, 6]","['Ok but not good enough - rejection', 'Ok but not good enough - rejection', 'Marginally above acceptance threshold']","This paper propose a parallel mechanism for stochastic gradient descent method (SGD) in case of gradient can be computed via linear operations (including least square linear regression and polynomial regression problems). The motivation is to recover the same effect compared with sequential SGD, by using a proposed sound combiner. To make such combiner more efficient, the authors also use a randomized projection matrix to do dimension reduction. Experiments shows the proposed method has better speedup than previous methods like Hogwild! and Allreduce. I feel that there might be some fundamental misunderstanding on SGD. **The combiner matrixM generate above can be quite large and expensive to compute. The sequential SGD algorithm maintains and updates the weight vector w , and thus requires O(f) space and time, where f is the number of features. In contrast,M is a f f matrix and consequently, the space and time complexity of parallel SGD is O(f^2) . In practice, this would mean that we would need O(f) processors to see constant speedups, an infeasible proposition particularly for datasets that can have thousands if not millions of features.* I do not think one needs O(f^2) space and complexity for updating M_i * v, where v is an f-dimensional vector. Note that M_i is a low rank matrix in the form of (I - a_i a_i*). The complexity and space can be reduced to O(f) if compute it by O(v - a_i (a_i* v)) equivalently. If M_i is defined in the form of the product of n number of rank 1 matrices. The complexity and space complexity is O(fn). In the context of this paper, n should be much smaller than f. I seriously doubt that all author*s assumptions, experiments, and strategies in this paper are based on this incorrect assumption on space and complexity of SGD. Why one can have speedup is unclear for me. It is unclear what computations are in parallel and why this sequential algorithms can bring speedup if M_i*v is computed in the most efficient way. I suggest authors to make the following changes to make this paper more clear and theoretically solid - provide computational complexity per step of the proposed algorithm - convergence rate analysis (convergence analysis is not enough): we would like to see how the dimension reduction can affect the complexity.","['5', '5', '4']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[7, 16, 13]","[13, 22, 19]","[33, 111, 69]","[16, 87, 43]","[14, 15, 14]","[3, 9, 12]","The sentiment score is -50 because the reviewer expresses significant doubts about the paper's fundamental understanding of SGD and questions the validity of the authors' assumptions and strategies. The reviewer points out potential misunderstandings and suggests major changes, indicating a generally negative sentiment. However, it's not entirely negative as the reviewer acknowledges some positive aspects like better speedup in experiments. The politeness score is 20 because while the reviewer is critical, they maintain a professional tone and offer constructive suggestions. They use phrases like 'I suggest' and 'I feel that' which soften the criticism. The reviewer also provides detailed explanations for their concerns, which is a polite way to offer criticism. However, the use of phrases like 'seriously doubt' and 'incorrect assumption' slightly reduces the overall politeness, preventing a higher score.",-50,20
Parametric Exponential Linear Unit for Deep Convolutional Neural Networks,Reject,2017,"['Ludovic Trottier', 'Philippe Giguère', 'Brahim Chaib-draa']","[5, 7, 4, 6]","['5', 'Good paper, accept', 'Ok but not good enough - rejection', 'Marginally above acceptance threshold']","The paper deals with a very important issue of vanishing gradients and the quest for a perfect activation function. Proposed is an approach of learning the activation functions during the training process. I find this research very interesting, but I am concerned that the paper is a bit premature. There is a long experimental section, but I am not sure what the conclusion is. The authors appear to be somewhat confused themselves. The amount of *maybe* *could mean*, *perhaps* etc. statements in the paper is exceptionally high. For this paper to be accepted it needs a bold statement about the performance, with a solid evidence. In my opinion, that is lacking as of now. This approach is either a breakthrough or a dud, and after reading the paper I am not convinced which case it is. The theoretical section could be made a little clearer. Finally, how is the performance affected. The huge advantage if ReLU is in the fact that the formula is so simple and thus not costly to evaluate. How do PELU-s compare.","['4', '5', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[4, 13, 30]","[7, 19, 36]","[13, 99, 187]","[8, 58, 125]","[4, 28, 16]","[1, 13, 46]","The sentiment score is slightly negative (-20) because while the reviewer finds the research 'very interesting', they express significant concerns about the paper being 'premature' and lacking a 'bold statement about the performance, with solid evidence'. The reviewer is uncertain about the conclusion and effectiveness of the approach, which contributes to the negative sentiment. However, the score is not deeply negative as the reviewer acknowledges the importance of the topic and the potential for the research to be a 'breakthrough'. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, acknowledging the importance of the research and providing constructive criticism. They use phrases like 'I find this research very interesting' and 'For this paper to be accepted it needs...', which are polite ways of offering feedback. The reviewer also asks questions and suggests improvements rather than making harsh criticisms, maintaining a professional and courteous tone.",-20,50
Playing SNES in the Retro Learning Environment,Reject,2017,"['Nadav Bhonker', 'Shai Rozenberg', 'Itay Hubara']","[5, 4, 7]","['5', 'Ok but not good enough - rejection', 'Good paper, accept']","The paper presents a new environment, called Retro Learning Environment (RLE), for reinforcement learning. The authors focus on Super Nintendo but claim that the interface supports many others (including ALE). Benchmark results are given for standard algorithms in 5 new Super Nintendo games, and some results using a new *rivalry metric*. These environments (or, more generally, standardized evaluation methods like public data sets, competitions, etc.) have a long history of improving the quality of AI and machine learning research. One example in the past few years was the Atari Learning Environment (ALE) which has now turned into a standard benchmark for comparison of algorithms and results. In this sense, the RLE could be a worthy contribution to the field by encouraging new challenging domains for research. That said, the main focus of this paper is presenting this new framework and showcasing the importance of new challenging domains. The results of experiments themselves are for existing algorithms. There are some new results that show reward shaping and policy shaping (having a bias toward going right in Super Mario) help during learning. And, yes, domain knowledge helps, but this is obvious. The rivalry training is an interesting idea, when training against a different opponent, the learner overfits to that opponent and forgets to play against the in-game AI; but then oddly, it gets evaluated on how well it does against the in-game AI! Also the part of the paper that describes the scientific results (especially the rivalry training) is less polished, so this is disappointing. In the end, I*m not very excited about this paper. I was hoping for a more significant scientific contribution to accompany in this new environment. It*s not clear if this is necessary for publication, but also it*s not clear that ICLR is the right venue for this work due to the contribution being mainly about the new code (for example, mloss.org could be a better *venue*, JMLR has an associated journal track for accompanying papers: http://www.jmlr.org/mloss/) --- Post response: Thank you for the clarifications. Ultimately I have not changed my opinion on the paper. Though I do think RLE could have a nice impact long-term, there is little new science in this paper, ad it*s either too straight-forward (reward shaping, policy-shaping) or not quite developed enough (rivalry training).","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[2, 2, 4]","[8, 5, 9]","[8, 7, 26]","[3, 4, 11]","[5, 3, 14]","[0, 0, 1]","The sentiment score is -30 because the reviewer expresses a generally negative view of the paper, stating they're 'not very excited about this paper' and were 'hoping for a more significant scientific contribution'. However, they do acknowledge some potential value in the new environment (RLE) for future research, which prevents the score from being more negative. The politeness score is 20 because the reviewer maintains a professional tone throughout, acknowledging potential benefits of the work and providing constructive feedback. They avoid harsh language or direct criticism, instead using phrases like 'I'm not very excited' or 'it's not clear that'. The reviewer also thanks the authors for clarifications, which adds to the politeness. However, the overall critical nature of the review prevents a higher politeness score.",-30,20
Progressive Attention Networks for Visual Attribute Prediction,Reject,2017,"['Paul Hongsuck Seo', 'Zhe Lin', 'Scott Cohen', 'Xiaohui Shen', 'Bohyung Han']","[4, 6, 7]","['Ok but not good enough - rejection', 'Marginally above acceptance threshold', 'Good paper, accept']","This paper proposes an attention mechanism which is essentially a gating on every spatial feature. Though they claim novelty through the attention being progressive, progressive attention has been done before [Spatial Transformer Networks, Deep Networks with Internal Selective Attention through Feedback Connections], and the element-wise multiplicative gates are very similar to convolutional LSTMs and Highway Nets. There is a lack of novelty and no significant results. Pros: - The idea of progressive attention on features is good, but has been done in [Spatial Transformer Networks, Deep Networks with Internal Selective Attention through Feedback Connections] - Good visualisations. Cons: - No progressive baselines were evaluated, e.g. STN and HAN at every layer acting on featuremaps. - Not clear how the query is fed into the localisation networks of baselines. - The difference in performance between author-made synthetic data and the Visual Genome datasets between baselines and PAN is very different. Why is this? There is no significant performance gain on any standard datasets. - No real novelty.","['5', '3', '4']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[6, 13, 19, 19, 16]","[12, 19, 25, 25, 22]","[38, 255, 104, 161, 187]","[19, 141, 67, 80, 108]","[19, 86, 32, 50, 64]","[0, 28, 5, 31, 15]","The sentiment score is -70 because the review is predominantly negative, highlighting a lack of novelty and significant results. The reviewer points out that the proposed method has been done before and is similar to existing techniques. They also mention there are no significant performance gains on standard datasets. The few positive comments ('good visualisations', 'the idea... is good') are outweighed by the criticisms. The politeness score is 20 because while the language is not overtly rude, it is quite direct and critical. The reviewer uses neutral language and organizes thoughts into 'Pros' and 'Cons', which adds a degree of politeness. However, the overall tone is matter-of-fact and doesn't employ many polite phrases or soften the criticisms.",-70,20
Prototypical Networks for Few-shot Learning,Reject,2017,"['Jake Snell', 'Kevin Swersky', 'Richard Zemel']","[5, 6, 4]","['5', 'Marginally above acceptance threshold', 'Ok but not good enough - rejection']","The paper is an extension of the matching networks by Vinyals et al. in NIPS2016. Instead of using all the examples in the support set during test, the method represents each class by the mean of its learned embeddings. The training procedure and experimental setting are very similar to the original matching networks. I am not completely sure about its advantages over the original matching networks. It seems to me when dealing with 1-shot case, these two methods are identical since there is only one example seen in this class, so the mean of the embedding is the embedding itself. When dealing with 5-shot case, original matching networks compute the weighted average of all examples, but it is at most 5x cost. The experimental results reported for prototypical nets are only slightly better than matching networks. I think it is a simple, straightforward, novel extension, but I am not fully convinced its advantages.","['3', '4', '5']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[7, 3]","[13, 8]","[19, 17]","[9, 7]","[2, 1]","[8, 9]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the paper as a 'simple, straightforward, novel extension', they express doubts about its advantages over the original method. The reviewer is 'not completely sure about its advantages' and 'not fully convinced' of its benefits, especially given the similar results to the original method. The politeness score is moderately positive (50) as the reviewer uses neutral, professional language throughout. They present their concerns in a respectful manner, using phrases like 'I am not completely sure' and 'I think' rather than making harsh criticisms. The reviewer also acknowledges the positive aspects of the paper, calling it 'novel' and noting that the results are 'slightly better', which contributes to the polite tone.",-20,50
RL^2: Fast Reinforcement Learning via Slow Reinforcement Learning,Reject,2017,"['Yan Duan', 'John Schulman', 'Xi Chen', 'Peter L. Bartlett', 'Ilya Sutskever', 'Pieter Abbeel']","[4, 3, 3]","['Ok but not good enough - rejection', 'Clear rejection', 'Clear rejection']","The authors try to address the issue of data efficiency in deep reinforcement learning by meta-learning a reinforcement learning algorithm using a hand-designed reinforcement learning algorithm (TRPO in this case). The experiments suggest comparable performance to models with prior knowledge of the distribution over environments for bandit tasks, and experiments on random maze navigation from vision is shown as well, though the random maze experiments would benefit from a clearer explanation. It was not obvious from the text how their experiments supported the thesis of the paper that the learned RL algorithm was effectively performing one-shot learning. The subject of the paper is also strikingly similar to the recently-posted paper Learning to Reinforcement Learn (https://arxiv.org/pdf/1611.05763.pdf), and while this paper was posted after the ICLR deadline, the authors should probably update the text to reflect the state of this rapidly-advancing field.","['3', '4', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[10, 7, 3, 27, 11, 16]","[15, 13, 8, 33, 17, 22]","[52, 66, 47, 281, 99, 610]","[28, 27, 21, 135, 49, 291]","[19, 37, 24, 89, 45, 293]","[5, 2, 2, 57, 5, 26]","The sentiment score is slightly positive (20) because the reviewer acknowledges the authors' efforts and suggests that their experiments show comparable performance in some areas. However, they also point out areas for improvement and similarities to other work, which tempers the positivity. The politeness score is moderately positive (50) as the reviewer uses neutral language and offers constructive criticism without being harsh. They use phrases like 'would benefit from' and 'it was not obvious' rather than more critical language. The reviewer also suggests updates to reflect recent developments in the field, which is a polite way of pointing out potential oversights.",20,50
ReasoNet: Learning to Stop Reading in Machine Comprehension,Reject,2017,"['Yelong Shen', 'Po-Sen Huang', 'Jianfeng Gao', 'Weizhu Chen']","[5, 6, 5]","['5', 'Marginally above acceptance threshold', '5']",The paper proposes an architecture called ReasoNet that reason over the relation. The paper addresses important tasks but there are many other related works. The comparison to other methods are not comprehensive. The Graph Reachability dataset is not a good example to use.,"['3', '5', '3']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is fairly confident that the evaluation is correct']","[7, 7, 18, 11]","[13, 12, 24, 17]","[97, 75, 545, 168]","[44, 44, 263, 77]","[51, 26, 250, 89]","[2, 5, 32, 2]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the paper addresses important tasks, they also point out several shortcomings: lack of comprehensive comparison to other methods, and criticism of the dataset used. The tone is not overtly negative, but the criticisms outweigh the positive aspects mentioned. The politeness score is neutral (0) as the language used is neither particularly polite nor rude. The reviewer states their opinions directly without using overly harsh language or softening phrases, maintaining a professional and neutral tone throughout the brief review.",-20,0
Rectified Factor Networks for Biclustering,Reject,2017,"['Djork-Arné Clevert', 'Thomas Unterthiner', 'Sepp Hochreiter']","[4, 5, 5]","['Ok but not good enough - rejection', '5', '5']","The paper presents a repurposing of rectified factor networks proposed earlier by the same authors to biclustering. The method seems potentially quite interesting but the paper has serious problems in the presentation. Quality: The method relies mainly on techniques presented in a NIPS 2015 paper by (mostly) the same authors. The experimental procedure should be clarified further. The results (especially Table 2) seem to depend critically upon the sparsity of the reported clusters, but the authors do not explain in sufficient detail how the sparsity hyperparameter is determined. Clarity: The style of writing is terrible and completely unacceptable as a scientific publication. The text looks more like an industry white paper or advertisement, not an objective scientific paper. A complete rewrite would be needed before the paper can be considered for publication. Specifically, all references to companies using your methods must be deleted. Additionally, Table 1 is essentially unreadable. I would recommend using a figure or cleaning up the table by removing all engineering notation and reporting numbers per 1000 so that e.g. *0.475 +/- 9e-4* would become *475 +/- 0.9*. In general figures would be preferred as a primary means for presenting the results in text while tables can be included as supplementary information. Originality: The novelty of the work appears limited: the method is mostly based on a NIPS 2015 paper by the same authors. The experimental evaluation appears at least partially novel, but for example the IBD detection is very similar to Hochreiter (2013) but without any comparison. Significance: The authors* strongest claim is based on strong empirical performance in their own benchmark problems. It is however unclear how useful this would be to others as there is no code available and the details of the implementation are less than complete. Furthermore, the method depends on many specific tuning parameters whose tuning method is not fully defined, leaving it unclear how to guarantee the generalisation of the good performance.","['4', '2', '2']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', 'The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']",['skipped'],['skipped'],['skipped'],['skipped'],['skipped'],['skipped'],"The sentiment score is -70 because the review is predominantly negative. The reviewer points out 'serious problems' with the presentation, describes the writing style as 'terrible and completely unacceptable', and states that a 'complete rewrite would be needed'. They also question the novelty and significance of the work. The few positive comments (e.g., 'potentially quite interesting') are outweighed by the numerous criticisms. The politeness score is -20 because while the reviewer doesn't use explicitly rude language, the tone is quite harsh and direct. Phrases like 'terrible and completely unacceptable' and 'looks more like an industry white paper or advertisement' are particularly blunt and could be considered impolite in academic discourse. However, the reviewer does provide specific recommendations for improvement, which somewhat mitigates the negative tone.",-70,-20
Recurrent Coevolutionary Feature Embedding Processes for Recommendation,Reject,2017,"['Hanjun Dai*', 'Yichen Wang*', 'Rakshit Trivedi', 'Le Song']","[6, 6, 6]","['Marginally above acceptance threshold', 'Marginally above acceptance threshold', 'Marginally above acceptance threshold']","The paper introduces a time dependent recommender system based on point processes parametrized by time dependent user and item latent representations. The later are modeled as coupled – autoregressive processes – i.e. the representation of a user/item changes when he interacts with an item/user, and is a function of both the user and the item representations before time t. This is called coevolution here and the autoregressive process is called recurrent NN. The model may also incorporate heterogeneous inputs. Experiments are performed on several datasets, and the model is compared with different baselines. There are several contributions in the paper: 1) modeling recommendation via parametrized point processes where the parameter dynamics are modeled by latent user/item representations, 2) an optimization algorithm for maximizing the likelihood of this process, with different technical tricks that seem to break its intrinsic complexity, 3) evaluation experiments for time dependent recommendation. The paper by the same authors (NIPS 2016) describes a similar model of continuous time coevolution, and a similar evaluation. The difference lies in the details of the model: the point process model is not the same and of the latent factor dynamic model is slightly different, but the modeling approach and the arguments are exactly the same. By the end, one does not know what makes this model perform better than the one proposed in NIPS, is it the choice for the process, the new parametrization? Both are quite similar. There is no justification on the choice of the specific form of the point process in the two papers. Did the authors tried other forms as well? The same remark applies for the form of the dynamical process: the non-linearity used for the modeling of the latent user/item vectors here is limited to a sigmoid function, which probably does not change much w.r.t. a linear model, but there is no evidence of the role of this non linearity in the paper. Note that there are some inconsistencies between the results in the two papers. Concerning the evaluation, the authors introduce two criteria. I did not get exactly how they evaluate the item recommendation: it is mentioned that at each time t, the model predicts the item the user will interact with. Do you mean, the next item the user will interact with after time t? For the time prediction, why is it a relevant metric for recommendation? A comparison of the complexity, or execution time of the different methods would be helpful. The complexity of your method is apparently proportional to #items*#users, what are the complexity limits of your methods. Overall, the paper is quite nice and looks technically sound, albeit many details are missing. On the other hand, I have a mixed feeling because of the similarity with NIPS paper. The authors should have make a better work at convincing us that this is not a marginal extension of previous work by the authors. I was not convinced either by the evaluation criteria and there is no evidence that the model can be used for large datasets.","['4', '4', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[4, 3, 2, 14]","[10, 5, 7, 20]","[93, 24, 21, 343]","[47, 15, 12, 185]","[44, 8, 9, 122]","[2, 1, 0, 36]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('The paper is quite nice and looks technically sound'), they express several concerns and criticisms. These include the similarity to previous work, lack of justification for certain choices, and doubts about the evaluation criteria and scalability. The overall tone suggests more reservations than enthusiasm. The politeness score is moderately positive (30) as the reviewer uses professional language throughout and includes some positive comments. They phrase criticisms as questions or suggestions rather than harsh statements. However, the review isn't overly effusive or particularly warm in tone, keeping it from scoring higher on politeness.",-20,30
Recurrent Inference Machines for Solving Inverse Problems,Reject,2017,"['Patrick Putzky', 'Max Welling']","[5, 4, 7]","['5', 'Ok but not good enough - rejection', 'Good paper, accept']","This paper proposes the RIMs that unrolls variational inference procedure. The author claims that the novelty lies in the separation of the model and inference procedure, making the MAP inference as an end-to-end approach. The effectiveness is shown in image restoration experiments. While unrolling the inference is not new, the author does raise an interesting perspective towards the `model-free* configuration, where model and inference are not separable and can be learnt jointly. However I do not quite agree the authors* argument regarding [1] and [2]. Although both [1] and [2] have pre-defined MAP inference problem. It is not necessarily that a separate step is required. In fact, both do not have either a pre-defined prior model or an explicit prior evaluation step as shown in Fig. 1(a). I believe that the implementation of both follows the same procedure as the proposed, that could be explained through Fig. 1(c). That is to say, the whole inference procedure eventually becomes a learnable neural network and the energy is implicitly defined through learning the parameters. Moreover, the RNN block architecture (GRU) and non-linearity (tanh) restrict the flexibility and implicitly form the inherent family of variational energy and inference algorithm. This is also similar with [1] and [2]. Based on that fact, I have the similar feeling with R1 that the novelty is somewhat limited. Also some discussions should be added in terms of the architecture and nonlinearity that you have chosen.","['4', '4', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[4, 18]","[8, 24]","[7, 391]","[3, 190]","[3, 167]","[1, 34]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some interesting aspects of the paper, they express disagreement with the authors' arguments and state that the novelty is 'somewhat limited'. The overall tone suggests more criticism than praise. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, acknowledging the paper's contributions before presenting criticisms. They use phrases like 'I do not quite agree' and 'I have the similar feeling' rather than harsh or dismissive language. The reviewer also offers constructive suggestions, which contributes to the polite tone.",-20,50
Recurrent Neural Networks for Multivariate Time Series with Missing Values,Reject,2017,"['Zhengping Che', 'Sanjay Purushotham', 'Kyunghyun Cho', 'David Sontag', 'Yan Liu']","[6, 5, 6]","['Marginally above acceptance threshold', '5', 'Marginally above acceptance threshold']","This paper presents a modified gated RNN caled GRU-D that deals with time series which display a lot of missing values in their input. They work on two fronts. The first deals with the missing inputs directly by using a learned convex combination of the previous available value (forward imputation) and the mean value (mean imputation). The second includes dampening the recurrent layer not unlike a second reset gate, but parametrized according to the time elapsed since the last available value of each attributes. Positives ------------ - Clear definition of the task (handling missing values for classification of time series) - Many interesting baselines to test the new model against. - The model presented deals with the missing values in a novel, ML-type way (learn new dampening parameters). - The extensive tests done on the datasets is probably the greatest asset of this paper. Negatives ------------- - The paper could use some double checking for typos. - The Section A.2.3 really belongs in the main article as it deals with important related works. Swap it with the imprecise diagrams of the model if you need space. - No mention of any methods from the statistics litterature. Here are the two main points of this review that informs my decision: 1. The results, while promising, are below expectations. The paper hasn’t been able to convince me that GRU-simple (without intervals) isn’t just as well-suited for the task of handling missing inputs as GRU-D. In the main paper, GRU-simple is presented as the main baseline. Yet, it includes a lot of extraneous parameters (the intervals) that, according to Table 5, probably hurts the model more than it helps it. Having a third of it’s parameters being of dubious value, it brings the question of the fairness of the comparison done in the main paper, especially since in the one table where GRU-simple (without intervals) is present, GRU-D doesn’t significantly outperforms it. 2. My second concern, and biggest, is with some claims that are peppered through the paper. The first is about the relationship with the presence rate of data in the dataset and the diagnostics. I might be wrong, but that only indicates that the doctor in charge of that patient requested the relevant analyses be done according to the patient’s condition. That would mean that an expert system based on this data would always seem to be one step behind. The second claim is the last sentence of the introduction, which sets huge expectations that were not met by the paper. Another is that “simply concatenating masking and time interval vectors fails to exploit the temporal structure of missing values” is unsubstantiated and actually disproven later in the paper. Yet another is the conclusion that since GRU models displayed the best improvement between a subsample of the dataset and the whole of it means that the improvement is going to continue to grow as more data is added. This fails to consider that non-GRU models actually started with much better results than most GRU ones. Lastly is their claim to capture informative missingness by incorporating masking and time intervals directly inside the GRU architecture. While the authors did make these changes, the fact that they also concatenate the mask to the input, just like GRU-simple (without intervals), leads me to question the actual improvement made by GRU-D. Given that, while I find that the work that has been put into the paper is above average, I wouldn’t accept that paper without a reframing of the findings and a better focus on the real contribution of this paper, which I believe is the novel way to parametrize the choice of imputation method.","['4', '3', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[4, 9, 8, 13, 16]","[10, 15, 14, 19, 22]","[42, 43, 397, 171, 219]","[21, 25, 154, 88, 132]","[18, 14, 216, 75, 57]","[3, 4, 27, 8, 30]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positives ('Clear definition of the task', 'Many interesting baselines', 'novel, ML-type way'), they express significant concerns about the paper's claims and results. The reviewer states that the results are 'below expectations' and questions the fairness of comparisons and the validity of some claims. The politeness score is moderately positive (50) as the reviewer maintains a professional tone throughout, acknowledging the work put into the paper ('work that has been put into the paper is above average') and offering constructive criticism. They use phrases like 'I might be wrong' and 'My second concern' which soften the critique. The reviewer also provides specific recommendations for improvement, which is a polite way to offer criticism.",-20,50
Reference-Aware Language Models,Reject,2017,"['Zichao Yang', 'Phil Blunsom', 'Chris Dyer', 'Wang Ling']","[6, 5, 5]","['Marginally above acceptance threshold', '5', '5']","This paper presents a new type of language model that treats entity references as latent variables. The paper is structured as three specialized models for three applications: dialog generation with references to database entries, recipe generation with references to ingredients, and text generation with coreference mentions. Despite some opaqueness in details that I will discuss later, the paper does a great job making the main idea coming through, which I think is quite interesting and definitely worth pursuing further. But it seems the paper was rushed into the deadline, as there are a few major weaknesses. The first major weakness is that the claimed latent variables are hardly latent in the actual empirical evaluation. As clarified by the authors via pre-review QAs, all mentions were assumed to be given to all model variants, and so, it would seem like an over-claim to call these variables as latent when they are in fact treated as observed variables. Is it because the models with latent variables were too difficult to train right? A related problem is the use of perplexity as an evaluation measure when comparing reference-aware language models to vanilla language models. Essentially the authors are comparing two language models defined over different event space, which is not a fair comparison. Because mentions were assumed to be given for the reference-aware language models, and because of the fact that mention generators are designed similar to a pointer network, the probability scores over mentions will naturally be higher, compared to the regular language model that needs to consider a much bigger vocabulary set. The effect is analogous to comparing language models with aggressive UNK (and a small vocabulary set) to a language models with no UNK (and a much larger vocabulary set). To mitigate this problem, the authors need to perform one of the following additional evaluations: either assuming no mention boundaries and marginalizing over all possibilities (treating latent variables as truly latent), or showing other types of evaluation beyond perplexity, for example, BLEU, METEOR, human evaluation etc on the corresponding generation task. The other major weakness is writing in terms of technical accuracy and completeness. I found many details opaque and confusing even after QAs. I wonder if the main challenge that hinders the quality of writing has something to do with having three very specialized models in one paper, each having a lot of details to be worked out, which may have not been extremely important for the main story of the paper, but nonetheless not negligible in order to understand what is going on with the paper. Perhaps the authors can restructure the paper so that the most important details are clearly worked out in the main body of the paper, especially in terms of latent variable handling — how to make mention detection and conference resolution truly latent, and if and when entity update helps, which in the current version is not elaborated at all, as it is mentioned only very briefly for the third application (coreference resolution) without any empirical comparisons to motivate the update operation.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[8, 14, 19, 8]","[14, 20, 24, 13]","[60, 178, 268, 68]","[24, 91, 161, 41]","[23, 70, 90, 21]","[13, 17, 17, 6]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the paper's interesting main idea, they point out two major weaknesses: the claimed latent variables are not truly latent in the evaluation, and there are issues with writing and technical accuracy. The reviewer suggests substantial revisions and additional evaluations, indicating that the paper needs significant improvement.

The politeness score is moderately positive (60) because the reviewer uses respectful language throughout. They begin by highlighting the paper's strengths and interesting aspects before delving into criticisms. The reviewer also offers constructive suggestions for improvement rather than just pointing out flaws. Phrases like 'great job making the main idea coming through' and 'definitely worth pursuing further' contribute to the polite tone. However, the score is not higher because the criticism, while constructive, is quite extensive and direct.",-20,60
Regularizing Neural Networks by Penalizing Confident Output Distributions,Reject,2017,"['Gabriel Pereyra', 'George Tucker', 'Jan Chorowski', 'Lukasz Kaiser', 'Geoffrey Hinton']","[6, 5, 5]","['Marginally above acceptance threshold', '5', '5']","The paper experimentally investigates a slightly modified version of label smoothing technique for neural network training, and reports results on various tasks. Such smoothing idea is not new, but was not investigated previously in wide range of machine learning tasks. Comments: The paper should report the state-of-the-art results for speech recognition tasks (TIMIT, WSJ), even if models are not directly comparable. The error back-propagation of label smoothing through softmax is straightforward and efficient. Is there an efficient solution for BP of the entropy smoothing through softmax? Although the classification accuracy could remain the same, the model will not estimate the true posterior distribution with this kind of smoothing. This might be an issue in complex machine learning problems where the decision is made on higher level and based on the posterior estimations, e.g. language models in speech recognition. More motivation is necessary for the proposed smoothing.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[3, 5, 7, 13, 42]","[8, 10, 12, 18, 48]","[7, 75, 60, 84, 282]","[3, 34, 27, 44, 160]","[4, 39, 28, 32, 52]","[0, 2, 5, 8, 70]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the paper's experimental investigation, they point out several limitations and request more information. The reviewer suggests that the idea is not new and asks for more motivation for the proposed smoothing. They also raise concerns about the model's ability to estimate true posterior distribution. However, the tone is not entirely negative, as they recognize the paper's contribution to investigating the technique in a wide range of tasks. The politeness score is slightly positive (20) because the reviewer uses neutral language and phrases their criticisms as suggestions or questions rather than direct criticisms. They use phrases like 'The paper should report...' and 'More motivation is necessary...' which are constructive rather than harsh. The reviewer also acknowledges the paper's contributions, which adds to the politeness of the review.",-20,20
Representation Stability as a Regularizer for Improved Text Analytics Transfer Learning,Reject,2017,"['Matthew Riemer', 'Elham Khabiri', 'Richard Goodwin']","[5, 7, 6]","['5', 'Good paper, accept', 'Marginally above acceptance threshold']","This paper proposes a method for transfer learning, i.e. leveraging a network trained on some original task A in learning a new task B, which not only improves performance on the new task B, but also tries to avoid degradation in performance on A. The general idea is based on encouraging a model trained on A, while training on the new task B, to match fake targets produced by the model itself but when it is trained only on the original task A. Experiments show that this method can help in improving the result on task B, and is better than other baselines, including standard fine-tuning. General comments/questions: - As far as I can tell, there is no experimental result supporting the claim that your model still performs well on the original task. All experiments show that you can improve on the new task only. - The introduction makes a strong statements about the distilling logical rule engine into a neural network, which I find a bit misleading. The approach in the paper is not specific to transferring from logical rules (as stated in the Sec 2) and is simply relying on the rule engine to provide labels for unlabelled data. - One of the obvious baselines to compare with your approach is standard multi-task learning on both tasks A and B together. That is, you train the model from scratch on both tasks simultaneously (which sharing parameters). It is not clear this is the same as what is referred to in Sec. 8 as *joint training*. Can you please explain more clearly what you refer to as joint training? - Why can*t we find the same baselines in both Table 2 and Table 3? For example Table 2 is missing *joint training*, and Table 3 is missing GRU trained on the target task. - While the idea is presented as a general method for transfer learning, experiments are focused on one domain (sentiment analysis on SemEval task). I think that either experiments should include applying the idea on at least one other different domain, or the writing of the paper should be modified to make the focus more specific to this domain/task. Writing comments - The writing of the paper in general needs some improvement, but more specifically in the experiment section, where experiment setting and baselines should be explained more concisely. - Ensemble methodology paragraph does not fit the flow of the paper. I would rather explain it in the experiments section, rather than including it as part of your approach. - Table 1 seems like reporting cross-validation results, and I do not think is very informative to general reader.","['4', '4', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[3, 12, 24]","[9, 17, 26]","[46, 28, 52]","[20, 22, 39]","[25, 2, 2]","[1, 4, 11]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects of the paper (e.g., 'Experiments show that this method can help in improving the result on task B'), they also raise several critical points and concerns. These include missing experimental results, potentially misleading statements, and the need for more comprehensive experiments or a more focused approach. The overall tone suggests that significant improvements are needed.

The politeness score is moderately positive (50) because the reviewer maintains a professional and constructive tone throughout. They use phrases like 'Can you please explain' and frame their criticisms as questions or suggestions rather than harsh criticisms. The reviewer also balances negative points with positive observations. However, the score is not higher because the review is quite direct in pointing out flaws and doesn't use many explicitly polite phrases or soften criticisms extensively.",-20,50
Rethinking Numerical Representations for Deep Neural Networks,Reject,2017,"['Parker Hill', 'Babak Zamirai', 'Shengshuo Lu', 'Yu-Wei Chao', 'Michael Laurenzano', 'Mehrzad Samadi', 'Marios Papaefthymiou', 'Scott Mahlke', 'Thomas Wenisch', 'Jia Deng', 'Lingjia Tang', 'Jason Mars']","[6, 5, 5]","['Marginally above acceptance threshold', '5', '5']","The paper studies the impact of using customized number representations on accuracy, speed, and energy consumption of neural network inference. Several standard computer vision architectures including VGG and GoogleNet are considered for the experiments, and it is concluded that floating point representations are preferred over fixed point representations, and floating point numbers with about 14 bits are sufficient for the considered architectures resulting in a small loss in accuracy. The paper provides a nice overview of floating and fixed point representations and focuses on an important aspect of deep learning that is not well studied. There are several aspects of the paper that could be improved, but overall, I am leaned toward weak accept assuming that the authors address the issues below. 1- The paper is not clear that it is only focusing on neural network inference. Please include the word *inference* in the title / abstract to clarify this point and mention that the findings of the paper do not necessarily apply to neural network training as training dynamics could be different. 2- The paper does not discuss the possibility of adopting quantization tricks during training, which may result in the use of fewer bits at inference. 3- The paper is not clear whether in computing the running time and power consumption, it includes all of the modules or only multiply-accumulate units? Also, how accurate are these numbers given different possible designs and the potential difference between simulation and production? Please elaborate on the details of simulation in the paper. 4- The whole discussion about *efficient customized precision search* seem unimportant to me. When such important hardware considerations are concerned, even spending 20x simulation time is not that important. The exhaustive search process could be easily parallelized and one may rather spend more time at simulation at the cost of finding the exact best configuration rather than an approximation. That said, weak configurations could be easily filtered after evaluating just a few examples. 5- Nvidia*s Pascal GP100 GPU supports FP16. This should be discussed in the paper and relevant Nvidia papers / documents should be cited. More comments: - Parts of the paper discussing *efficient customized precision search* are not clear to me. - As future work, the impact of number representations on batch normalization and recurrent neural networks could be studied.","['3', '5', '2']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']","[2, 3, 3, 7, 13, 10, 27, 27, 15, 10, 11, 11]","[4, 6, 4, 13, 15, 15, 28, 32, 21, 16, 17, 17]","[8, 7, 6, 39, 49, 21, 94, 232, 124, 144, 62, 71]","[5, 5, 5, 19, 36, 13, 64, 186, 89, 73, 45, 52]","[3, 1, 1, 17, 3, 1, 2, 1, 7, 65, 8, 9]","[0, 1, 0, 3, 10, 7, 28, 45, 28, 6, 9, 10]","The sentiment score is 50 (slightly positive) because the reviewer states they are 'leaned toward weak accept' and acknowledges the paper provides 'a nice overview' and focuses on an important aspect. However, they also mention several aspects that could be improved, balancing the positive with constructive criticism. The politeness score is 75 (quite polite) because the reviewer uses respectful language throughout, offering suggestions for improvement rather than harsh criticisms. They use phrases like 'Please include' and 'Please elaborate' when making requests, and frame their comments as recommendations rather than demands. The reviewer also acknowledges the paper's strengths before diving into areas for improvement, which is a polite approach to feedback.",50,75
Revisiting Batch Normalization For Practical Domain Adaptation,Reject,2017,"['Yanghao Li', 'Naiyan Wang', 'Jianping Shi', 'Jiaying Liu', 'Xiaodi Hou']","[6, 4, 5]","['Marginally above acceptance threshold', 'Ok but not good enough - rejection', '5']","This paper proposes a simple domain adaptation technique in which batch normalization is performed separately in each domain. Pros: The method is very simple and easy to understand and apply. The experiments demonstrate that the method compares favorably with existing methods on standard domain adaptation tasks. The analysis in section 4.3.2 shows that a very small number of target domain samples are needed for adaptation of the network. Cons: There is little novelty -- the method is arguably too simple to be called a “method.” Rather, it’s the most straightforward/intuitive approach when using a network with batch normalization for domain adaptation. The alternative -- using the BN statistics from the source domain for target domain examples -- is less natural, to me. (I guess this alternative is what’s done in the Inception BN results in Table 1-2?) The analysis in section 4.3.1 is superfluous except as a sanity check -- KL divergence between the distributions should be 0 when each distribution is shifted/scaled to N(0,1) by BN. Section 3.3: it’s not clear to me what point is being made here. Overall, there’s not much novelty here, but it’s hard to argue that simplicity is a bad thing when the method is clearly competitive with or outperforming prior work on the standard benchmarks (in a domain adaptation tradition that started with “Frustratingly Easy Domain Adaptation”). If accepted, Sections 4.3.1 and 3.3 should be removed or rewritten for clarity for a final version.","['3', '4', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[4, 7, 19, 10, 12]","[10, 13, 25, 16, 13]","[70, 97, 177, 289, 19]","[34, 39, 77, 149, 11]","[31, 50, 70, 68, 6]","[5, 8, 30, 72, 2]","The sentiment score is slightly positive (20) because the reviewer acknowledges some pros of the paper, such as the simplicity of the method and its favorable performance compared to existing methods. However, they also point out significant cons, including lack of novelty and superfluous analysis, which tempers the positive aspects. The politeness score is moderately positive (50) as the reviewer uses neutral language and offers constructive criticism. They acknowledge the paper's strengths while also clearly stating its weaknesses, maintaining a professional tone throughout. The reviewer also provides specific suggestions for improvement, which is a polite way to offer criticism.",20,50
Revisiting Distributed Synchronous SGD,Reject,2017,"['Jianmin Chen*', 'Xinghao Pan*', 'Rajat Monga', 'Samy Bengio', 'Rafal Jozefowicz']","[6, 6, 5]","['Marginally above acceptance threshold', 'Marginally above acceptance threshold', '5']","This paper was easy to read, the main idea was presented very clearly. The main points of the paper (and my concerns are below) can be summarized as follows: 1. synchronous algoriths suffer from some struggeling nodes, for which the algorithm has to wait. From my own experience, this has never happend for me on e.g. Amazon EC2 cloud, however, it happens on our own cluster at my university, if the cluster is shared and some users make some nodes very busy. So maybe if the nodes would be dedicated to just user*s job, it wouldn*t be such a big concer (I am not sure what kind of cluster was used to produce Figure 3 and 4). Also how many experiments have you run? In my own experience, most of the time I get the gradient on time from all nodes equality fast, but maybe just in less than 0.1% of iterations I observe that it took maybe twice as long for some node. Also the increasing shape of the curve is somehow implying some weird implementation of communication. Isn*t it only because you are somehow serialize the communication? And it would be maybe much faster if a *MPI_Reduce* would be used (even if we wait for the slowest guy)? 2. asynchronous algorithms are cutting the waiting time, however, the convergence speed may be slower. Moreover, those algorithms can be divergence it special care is not given to stale gradients. Also they have a nice guarantees for convex functions, but the non-convex DNN may cause pain. 3.they propose to take gradient from the first *N* workers out of *N+b* workers available. My concern here is that they focused only on the workers, but what if the *parameter server* will became to slow? What if the parameter server would be the bottleneck? How would you address this situation? But still if the number of nodes (N) is not large, and the deep DNN is used, I can imagine that the communciation will not take more than 30% of the run-time. My largest concern is with the experiments. Different batch size implies that different learning rate should be chosen, right? How did you tune the learning rates and other parameters for e.g. Figure 5 you provide some formulas in (A2) but clearly this can bias your Figures, right? meaning, that if you tune *gamma, eta* for each N, it could be somehow more representative? also it would be nicer if you run the experiment many times and then report average, best and worst case behaviour. because now it can be just coinsidence, right?","['4', '4', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[10, 7, 31, 5]","[15, 9, 37, 8]","[26, 19, 261, 18]","[16, 10, 155, 6]","[3, 9, 60, 11]","[7, 0, 46, 1]","The sentiment score is slightly positive (20) because the reviewer starts by praising the paper as 'easy to read' and 'clearly presented'. However, they also raise several concerns and questions throughout the review, which tempers the overall positivity. The politeness score is moderately high (60) as the reviewer uses respectful language, phrases criticisms as questions or suggestions, and acknowledges their own experiences. They use phrases like 'From my own experience' and 'My concern here is' which maintain a collegial tone. The reviewer also offers constructive feedback and suggestions for improvement, rather than harsh criticism. The use of questions throughout the review softens potential criticisms and invites dialogue, contributing to the polite tone.",20,60
Riemannian Optimization for Skip-Gram Negative Sampling,Reject,2017,"['Alexander Fonarev', 'Alexey Grinchuk', 'Gleb Gusev', 'Pavel Serdyukov', 'Ivan Oseledets']","[4, 5, 6]","['Ok but not good enough - rejection', '5', 'Marginally above acceptance threshold']","The paper considers Grassmannian SGD to optimize the skip gram negative sampling (SGNS) objective for learning better word embeddings. It is not clear why the proposed optimization approach has any advantage over the existing vanilla SGD-based approach - neither approach comes with theoretical guarantees - the empirical comparisons show marginal improvements. Furthermore, the key idea here - that of projector splitting algorithm - has been applied on numerous occasions to machine learning problems - see references by Vandereycken on matrix completion and by Sepulchre on matrix factorization. The computational cost of the two approaches is not carefully discussed. For instance, how expensive is the SVD in (7)? One can always perform an efficient low-rank update to the SVD - therefore, a rank one update requires O(nd) operations. What is the computational cost of each iteration of the proposed approach?","['4', '3', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[4, 1, 6, 13, 10]","[4, 7, 12, 16, 16]","[5, 19, 68, 139, 233]","[3, 6, 46, 116, 41]","[2, 12, 19, 13, 117]","[0, 1, 3, 10, 75]","The sentiment score is -50 because the review is generally critical and skeptical of the paper's contributions. The reviewer questions the advantage of the proposed approach, points out lack of theoretical guarantees, and notes only marginal improvements in empirical comparisons. They also raise concerns about computational costs and lack of novelty. However, it's not entirely negative as they acknowledge some improvements and don't outright reject the work.

The politeness score is 0 (neutral) because the reviewer maintains a professional tone throughout. They don't use overtly polite language, but also avoid rudeness. The criticism is presented in a straightforward, matter-of-fact manner without personal attacks or overly harsh language. The reviewer asks questions and points out issues without being confrontational or dismissive.",-50,0
Sampling Generative Networks,Reject,2017,['Tom White'],"[5, 5, 6]","['5', '5', 'Marginally above acceptance threshold']","This paper proposes a variety of techniques for visualizing learned generative models, focussing specifically on VAE and GAN models. This paper is somewhat challenging to assess since it doesn*t propose a new algorithm, model, application etc. On the one hand these techniques will be highly relevant to the generative modeling community and I think this paper deserves a wide audience. The techniques proposed are simple, well explained, and of immediate use to those working on generative models. However, I*m not sure the paper is appropriate for an ICLR conference track as it doesn*t provide any greater theoretical insights into sampling generative models and there are no comparisons / quantitative evaluations of the techniques proposed. Overall, I*m very much on the fence since I think the techniques are useful and this paper should be read by those interested in generating modeling. I would be willing to increase my core if the author could present a case for why ICLR is an appropriate venue for this work.","['4', '3', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[0, 3, 29]","[6, 9, 35]","[28, 87, 495]","[11, 23, 230]","[16, 59, 149]","[1, 5, 116]","The sentiment score is slightly negative (-10) because while the reviewer acknowledges the usefulness of the techniques proposed, they express uncertainty about the paper's suitability for the ICLR conference track. The reviewer is 'on the fence' and requires more justification for the paper's inclusion. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, acknowledging the paper's potential value and expressing willingness to reconsider their score. They provide constructive feedback without harsh criticism, maintaining a professional and courteous tone.",-10,50
Semantic Noise Modeling for Better Representation Learning,Reject,2017,"['Hyo-Eun Kim', 'Sangheum Hwang', 'Kyunghyun Cho']","[4, 3, 2]","['Ok but not good enough - rejection', 'Clear rejection', 'Strong rejection']","The paper introduces supervised deep learning with layer-wise reconstruction loss (in addition to the supervised loss) and class-conditional semantic additive noise for better representation learning. Total correlation measure and additional insights from auto-encoder are used to derive layer-wise reconstruction loss and is further combined with supervised loss. When combining with supervised loss the class-conditional additive noise model is proposed, which showed consistent improvement over the baseline model. Experiments on MNIST and CIFAR-10 datasets while changing the number of training examples per class are done extensively. The derivation of Equation (3) from total correlation is hacky. Moreover, assuming graphical model between X, Y and Z, it should be more carefully derived to estimate H(X|Z) and H(Z|Y). The current proposal, encoding Z and Y from X and decoding from encoded representation is not really well justified. Is sigma in Equation 8 trainable parameter or hyperparameter? If it is trainable how it is trained? If it is not, how are they set? Does j correspond to one of the class? The proposed feature augmentation sounds like simply adding gaussian noise to the pre-softmax neurons. That being said, the proposed method is not different from gaussian dropout (Wang and Manning, ICML 2013) but applied on different layers. In addition, there is a missing reference (DisturbLabel: Regularizing CNN on the Loss Layer, CVPR 2016) that applied synthetic noise process on the loss layer. Experiments should be done for multiple times with different random subsets and authors should provide mean and standard error. Overall, I believe the proposed method is not very well justified and has limited novelty.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[8, 3, 8]","[13, 9, 14]","[26, 32, 397]","[12, 6, 154]","[7, 13, 216]","[7, 13, 27]","The sentiment score is -60 because the review is predominantly negative. While it acknowledges some positive aspects (e.g., 'consistent improvement over the baseline model'), the reviewer expresses significant concerns about the method's justification, novelty, and derivation. Phrases like 'not really well justified', 'limited novelty', and 'hacky' indicate a negative sentiment. The politeness score is 20 because the reviewer maintains a professional tone throughout, using neutral language like 'I believe' and 'should be' rather than harsh criticisms. However, the review is not overtly polite either, focusing on direct critique rather than praise or encouragement, hence the relatively low positive score.",-60,20
Semi-Supervised Detection of Extreme Weather Events in Large Climate Datasets,Reject,2017,"['Evan Racah', 'Christopher Beckham', 'Tegan Maharaj', 'Prabhat', 'Christopher Pal']","[6, 6, 4]","['Marginally above acceptance threshold', 'Marginally above acceptance threshold', 'Ok but not good enough - rejection']","This paper applies convnet-based object detection techniques to detection of weather events from 3D climate data, additionally exploring the effect of using an unsupervised autoencoder-style objective term. Pros: The application of object detection techniques to extreme weather event detection problem is unique, to my knowledge. The paper is well-written and describes the method well, including a survey of the related work. The best model makes use of 3D convolutions and unsupervised learning, both of which are relatively unexplored in the detection literature. Both of these aspects are validated and shown to produce at least small performance improvements over a 2D and/or purely supervised approach. Cons: The benefits of the 3D convolutional architecture and unsupervised learning end up being a little underwhelming, with 52.92% mAP for the 3D+semi-sup result vs. 51.42% mAP for the 2D+sup result. It’s a bit strange that 3D+sup and 2D+semi-sup are each worse than the 2D+sup base result; I’d expect each aspect to give a slight improvement over the base result, given that using both together gives the best results -- perhaps there was not a thorough enough hyperparameter search for these cases. The paper does acknowledge this and provide potential explanations in Sec. 4.3, however. As other reviewers pointed out, the use of the 0.1 IoU criterion for true positives is very loose relative to the standard 0.5 criterion. On the other hand, if the results visualized in Figure 3 are typical, a 0.1 overlap criterion could be reasonable for this domain as the detector does seem to localize events well enough that the system could be used to expedite human review of the climate images for extreme events. Still, it would be useful to also report results at higher overlap thresholds. Minor: eq 6 should (probably) be the squared L2 norm (i.e. the sum of squares) rather than the L2 norm itself. Minor: table 4 -- shouldn’t the semi-supervised models have more parameters than the corresponding supervised ones due to the decoder layers? Overall, this paper is well-written and applies some interesting underutilized techniques to a relatively unique domain. The results aren*t striking, but the model is ablated appropriately and shown to be beneficial. For a final version, it would be nice to see results at higher overlap thresholds.","['4', '3', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[2, 2, 2, 13, 21]","[7, 8, 8, 17, 27]","[21, 21, 22, 99, 231]","[9, 7, 6, 55, 98]","[12, 11, 15, 31, 110]","[0, 3, 1, 13, 23]","The sentiment score is 60 (positive) because the reviewer starts by highlighting several pros of the paper, including its uniqueness, well-written nature, and use of advanced techniques. While some cons are mentioned, they are presented as minor issues and the overall tone remains positive. The reviewer concludes that the paper is 'well-written' and applies 'interesting underutilized techniques'. The politeness score is 80 (very polite) because the reviewer uses respectful language throughout, acknowledges the paper's strengths, and frames criticisms constructively. They use phrases like 'it would be nice to see' and 'it would be useful to' when suggesting improvements, which is a polite way of offering feedback. The reviewer also balances critiques with positive comments and provides explanations for their concerns, demonstrating a considerate approach to the review process.",60,80
Semi-Supervised Learning with Context-Conditional Generative Adversarial Networks,Reject,2017,"['Emily Denton', 'Sam Gross', 'Rob Fergus']","[6, 6, 5]","['Marginally above acceptance threshold', 'Marginally above acceptance threshold', '5']","This paper presents a semi-supervised algorithm for regularizing deep convolutional neural networks. They propose an adversarial approach for image inpainting where the discriminator learns to identify whether an inpainted image comes from the data distribution or the generator, while at the same time it learns to recognize objects in an image from the data distribution. In experiments, they show the usefulness of their algorithm in which the features learned by the discriminator result in comparable or better object recognition performance to the reported state-of-the-art in two datasets. Overall, the proposed idea seems a simple yet an effective way for regularize CNNs to improve the classification performance.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[6, 2, 15]","[12, 6, 21]","[50, 16, 131]","[18, 6, 72]","[25, 9, 49]","[7, 1, 10]","The sentiment score is 80 (positive) because the reviewer describes the paper's idea as 'simple yet effective' and mentions that the proposed algorithm shows 'comparable or better' performance to state-of-the-art methods. The overall tone is appreciative of the work's contribution. The politeness score is 50 (somewhat polite) because while the language is professional and not overtly negative, it doesn't contain explicitly polite phrases. The reviewer presents their assessment in a neutral, matter-of-fact manner without using overly formal or polite language, but also without any rudeness or harsh criticism.",80,50
Sentence Ordering using Recurrent Neural Networks,Reject,2017,"['Lajanugen Logeswaran', 'Honglak Lee', 'Dragomir Radev']","[6, 6, 7]","['Marginally above acceptance threshold', 'Marginally above acceptance threshold', 'Good paper, accept']","This paper extends the *order matters* idea in (Vinyals et al., 2015) from the sentence level to an interesting application on discourse level. Experiments in this paper show the capacity of the proposed model on both order discrimination task and sentence ordering. I think the problem is interesting and the results are promising. However, there are some problems about technical details: - Why there are two components of LSTM hidden state (h_{enc}^{t-1},c_{ent}^{t-1}), what information is captured by each of these hidden states? Refer to (Vinyals et al. 2015a)? - Some notations in this paper are confusing. For example, what is the form of W in the feed-forward scoring function? Does it have the same form as the W in the bilinear score function? - What is the connection between the encoder and decoder in the proposed model? How to combine them together? I read something relevant from the caption of Figure 1, but it is still not clear to me.","['4', '4', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[4, 13, 23]","[10, 19, 29]","[22, 273, 289]","[8, 145, 151]","[14, 123, 102]","[0, 5, 36]","The sentiment score is 50 (slightly positive) because the reviewer states the paper is 'interesting' and has 'promising' results, showing a generally positive view. However, they also mention 'some problems about technical details', indicating it's not entirely positive. The politeness score is 75 (quite polite) because the reviewer uses respectful language throughout, acknowledging the paper's merits before raising concerns. They phrase their criticisms as questions rather than direct criticisms, which is a polite approach. The reviewer also uses phrases like 'I think' to soften their statements, further contributing to the polite tone.",50,75
Sequence to Sequence Transduction with Hard Monotonic Attention,Reject,2017,"['Roee Aharoni', 'Yoav Goldberg']","[5, 5, 4]","['5', '5', 'Ok but not good enough - rejection']","The paper describes a recurrent transducer that uses hard monotonic alignments: at each step a discrete decision is taken either to emit the next symbol or to consume the next input token. The model is moderately novel - similar architecture was proposed for speech recognition (https://arxiv.org/pdf/1608.01281v1.pdf). Soft monotonic alignemts are also enforced by A. Graves in https://arxiv.org/abs/1308.0850. The difficult part in training the proposed model is backpropagation through the discrete decisions. Typically, reinforcement learning techniques are used. In this contribution, the authors side-step the issue by using a problem-dependent aligner to generate optimal decisions for which they train the model. The results indicate that such specially supervised model is better than the generic soft-attention model that doesn*t require any problem-dependent external supervision. However the authors did not attempt to work on regularizing the soft-attention model, which is not fair - the extra supervision by using the ground-truth alignment is a form of regularization and it could be used as e.g. an extra signal to the soft-attention model for a better comparison. That being said the authors reash state-of-the-art results against other domain specific methods. I believe the paper would more suit a NLP venue - it sound and properly written, but its applicability is limited to the considered NLP problem.","['4', '3', '5']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[4, 12]","[10, 18]","[36, 279]","[13, 130]","[23, 125]","[0, 24]","The sentiment score is slightly positive (20) because the reviewer acknowledges the paper's novelty, state-of-the-art results, and proper writing. However, they also point out limitations and suggest improvements, indicating a mixed but generally favorable view. The politeness score is moderately positive (50) as the reviewer uses professional and respectful language throughout, offering constructive criticism without harsh or rude comments. They acknowledge the paper's strengths while politely suggesting areas for improvement and alternative approaches.",20,50
Simple Black-Box Adversarial Perturbations for Deep Networks,Reject,2017,"['Nina Narodytska', 'Shiva Kasiviswanathan']","[4, 4, 4]","['Ok but not good enough - rejection', 'Ok but not good enough - rejection', 'Ok but not good enough - rejection']","Paper summary: This work proposes a new algorithm to generate k-adversarial images by modifying a small fraction of the image pixels and without requiring access to the classification network weight. Review summary: The topic of adversarial images generation is of both practical and theoretical interest. This work proposes a new approach to the problem, however the paper suffers from multiple issues. It is too verbose (spending long time on experiments of limited interest); disorganized (detailed description of the main algorithm in sections 4 and 5, yet a key piece is added in the experimental section 6); and more importantly the resulting experiments are of limited interest to the reader, and the main conclusions are left unclear. This looks like an interesting line of work that has yet to materialize in a good document, it would need significant re-writing to be in good shape for ICLR. Pros: * Interesting topic * Black-box setup is most relevant * Multiple experiments * Shows that with flipping only 1~5% of pixels, adversarial images can be created Cons: * Too long, yet key details are not well addressed * Some of the experiments are of little interest * Main experiments lack key measures or additional baselines * Limited technical novelty Quality: the method description and experimental setup leave to be desired. Clarity: the text is verbose, somewhat formal, and mostly clear; but could be improved by being more concise. Originality: I am not aware of another work doing this exact same type of experiments. However the approach and results are not very surprising. Significance: the work is incremental, the issues in the experiments limit potential impact of this paper. Specific comments: * I would suggest to start by making the paper 30%~40% shorter. Reducing the text length, will force to make the argumentation and descriptions more direct, and select only the important experiments. * Section 4 seems flawed. If the modified single pixel can have values far outside of the [LB, UB] range; then this test sample is clearly outside of the training distribution; and thus it is not surprising that the classifier misbehaves (this would be true for most classifiers, e.g. decision forests or non-linear SVMs). These results would be interesting only if the modified pixel is clamped to the range [LB, UB]. * [LB, UB] is never specified, is it ? How does p = 100, compares to [LB, UB] ? To be of any use, p should be reported in proportion to [LB, UB] * The modification is done after normalization, is this realistic ? * Alg 2, why not clamping to [LB, UB] ? * Section 6, “implementing algorithm LocSearchAdv”, the text is unclear on how p is adjusted; new variables are added. This is confusion. * Section 6, what happens if p is _not_ adjusted ? What happens if a simple greedy random search is used (e.g. try 100 times a set of 5 random pixels with value 255) ? * Section 6, PTB is computed over all pixels ? including the ones not modified ? why is that ? Thus LocSearchAdv PTB value is not directly comparable to FGSM, since it intermingles with #PTBPixels (e.g. “in many cases far less average perturbation” claim). * Section 6, there is no discussion on the average number of model evaluations. This would be equivalent to the number of requests made to a system that one would try to fool. This number is important to claim the “effectiveness” of such black box attacks. Right now the text only mentions the upper bound of 750 network evaluations. * How does the number of network evaluations changes when adjusting or not adjusting p during the optimization ? * Top-k is claimed as a main point of the paper, yet only one experiment is provided. Please develop more, or tune-down the claims. * Why is FGSM not effective for batch normalized networks ? Has this been reported before ? Are there other already published techniques that are effective for this scenario ? Comparing to more methods would be interesting. * If there is little to note from section 4 results, what should be concluded from section 6 ? That is possible to obtain good results by modifying only few pixels ? What about selecting the “top N” largest modified pixels from FGSM ? Would these be enough ? Please develop more the baselines, and the specific conclusions of interest. Minor comments: * The is an abuse of footnotes, most of them should be inserted in the main text. * I would suggest to repeat twice or thrice the meaning of the main variables used (e.g. p, r, LB, UB) * Table 1,2,3 should be figures * Last line of first paragraph of section 6 is uninformative. * Very tiny -> small","['3', '4', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[11, 14]","[17, 20]","[135, 88]","[75, 50]","[52, 27]","[8, 11]","The sentiment score is -60 because the review is predominantly negative, highlighting multiple issues with the paper such as being too verbose, disorganized, and having experiments of limited interest. The reviewer states that the paper 'suffers from multiple issues' and 'would need significant re-writing'. However, it's not entirely negative as the reviewer acknowledges the topic is interesting and the work has potential, preventing a lower score. The politeness score is 20 because while the reviewer is critical, they maintain a professional tone throughout. They use phrases like 'I would suggest' and provide both pros and cons, showing a balanced approach. The reviewer also offers specific, constructive feedback for improvement, which is a polite way to address shortcomings. However, the directness of some criticisms prevents a higher politeness score.",-60,20
SoftTarget Regularization: An Effective Technique to Reduce Over-Fitting in Neural Networks,Reject,2017,['Armen Aghajanyan'],"[4, 3, 4]","['Ok but not good enough - rejection', 'Clear rejection', 'Ok but not good enough - rejection']","The paper introduced a regularization scheme through soft-target that are produced by mixing between the true hard label and the current model prediction. Very similar method was proposed in Section 6 from (Hinton et al. 2016, Distilling the Knowledge in a Neural Network). Pros: + Comprehensive analysis on the co-label similarity. Cons: - Weak baselines. I am not sure the authors have found the best hyper-parameters in their experiments. I just trained a 5 layer fully connected MNIST model with 512 hidden units without any regularizer and achieved 0.986 acc. using Adam and He initialization, where the paper reported 0.981 for such architecture. - The authors failed to bring the novel idea. It is very similar to (Hinton et al. 2016). This is probably not enough for ICLR.","['5', '5', '5']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']",[3],[9],[38],[15],[23],[0],"The sentiment score is -60 because the review is predominantly negative. While it acknowledges some pros ('Comprehensive analysis on the co-label similarity'), it lists significant cons, including weak baselines and lack of novelty. The reviewer also states that the paper is 'probably not enough for ICLR', indicating a recommendation against acceptance. The politeness score is 20 because while the language is not overtly rude, it's also not particularly polite. The reviewer uses neutral, professional language but doesn't soften criticisms or offer encouragement. The use of phrases like 'I am not sure' and 'The authors failed to' are direct but not impolite.",-60,20
Structured Sequence Modeling with Graph Convolutional Recurrent Networks,Reject,2017,"['Youngjoo Seo', 'Michaël Defferrard', 'Pierre Vandergheynst', 'Xavier Bresson']","[4, 4, 4]","['Ok but not good enough - rejection', 'Ok but not good enough - rejection', 'Ok but not good enough - rejection']","The paper proposes to combine graph convolution with RNNs to solve problems in which inputs are graphs. The two key ideas are: (i) a graph convolutional layer is used to extract features which are then fed in an RNN, and (ii) matrix multiplications are replaced by graph convolution operations. (i) is applied to language modelling, yielding lower perplexity on Penn Treebank (PTB) compared with LSTM. (ii) outperformed LSTM + CNN on the moving-MNIST. Both two models/ideas are actually trivial and in line with the current trend of combining different architectures. For instance, the idea of replacing matrix multiplications by graph convolution is a small extension for Shi et al. Regarding to the experiment on PTB (section 5.2), I*m skeptical about the way the experiment carried out. The reason is that, instead of using the given development set to tune the models, the authors blindly used an available configuration which is for a different model. Pros: - good experimental results Cons: - ideas are quite trivial - the experiment on PTB was carried out improperly","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[6, 2, 23, 16]","[8, 7, 29, 22]","[6, 18, 277, 97]","[4, 5, 132, 36]","[2, 10, 69, 35]","[0, 3, 76, 26]","The sentiment score is -50 because the review is generally negative, with the reviewer describing the ideas as 'trivial' and expressing skepticism about the experimental methodology. However, it's not entirely negative as the reviewer acknowledges 'good experimental results' as a pro. The politeness score is 0 (neutral) because the language used is direct and matter-of-fact without being overtly polite or rude. The reviewer states their opinions clearly but doesn't use inflammatory language or personal attacks. They provide both pros and cons, maintaining a professional tone throughout the review.",-50,0
Surprisal-Driven Feedback in Recurrent Networks,Reject,2017,['Kamil Rocki'],"[4, 3, 3]","['Ok but not good enough - rejection', 'Clear rejection', 'Clear rejection']","Summary: This paper proposes to use surprisal-driven feedback for training recurrent neural networks where they feedback the next-step prediction error of the network as an input to the network. Authors have shown a result on language modeling tasks. Contributions: The introduction of surprisal-driven feedback, which is just the feedback from the errors of the model from the previous time-steps. Questions: A point which is not fully clear from the paper is whether if you have used the ground-truth labels on the test set for the surprisal feedback part of the model? I assume that authors do that since they claim that they use the misprediction error as additional input. Criticisms: The paper is really badly written, authors should rethink the organization of the paper. Most of the equations presented in the paper, about BPTT are not necessary for the main-text and could be moved to Appendix. The justification is not convincing enough. Experimental results are lacking, only results on a single dataset are provided. Although the authors claim that they got SOTA on enwiki8, there are other papers such as the HyperNetworks that got better results (1.34) than the result they achieve. This claim is wrong. The model requires the ground-truth labels for the test-set, however, this assumption really limits the application of this technique to a very limited set of applications(more or less rules out most conditional language modeling tasks). High-level Review: Pros: - A simple modification of the model that seems to improve the results and it is an interesting modification. Cons: - The authors need to use test-set labels. - Writing of the paper is bad. - The authors assume that they have access to the ground-truth labels during the test-set. - Experimental results are lacking","['4', '5', '5']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']",[9],[13],[26],[17],[9],[0],"The sentiment score is -50 because the review contains both positive and negative aspects, but leans more towards the negative. The reviewer acknowledges some pros like 'A simple modification of the model that seems to improve the results,' but lists more cons and criticisms. The reviewer points out issues with the paper's writing, experimental results, and claims. The politeness score is 0 (neutral) because the reviewer uses direct language without being overtly polite or rude. They state criticisms plainly ('The paper is really badly written') but also acknowledge positive aspects without using particularly polite language. The review maintains a professional tone throughout, neither excessively courteous nor disrespectful.",-50,0
Tartan: Accelerating Fully-Connected and Convolutional Layers in Deep Learning Networks by Exploiting Numerical Precision Variability,Reject,2017,"['Alberto Delmás Lascorz', 'Sayeh Sharify', 'Patrick Judd', 'Andreas Moshovos']","[5, 5, 6, 4, 4]","['5', '5', 'Marginally above acceptance threshold', 'Ok but not good enough - rejection', 'Ok but not good enough - rejection']","The authors present TARTAN, a derivative of the previously published DNN accelerator architecture: “DaDianNao”. The key difference is that TARTAN’s compute units are bit-serial and unroll MAC operation over several cycles. This enables the units to better exploit any reduction in precision of the input activations for improvement in performance and energy efficiency. Comments: 1. I second the earlier review requesting the authors to be present more details on the methodology used for estimating energy numbers for TARTAN. It is claimed that TARTAN gives only a 17% improvement in energy efficiency. However, I suspect that this small improvement is clearly within the margin of error ij energy estimation. 2. TARTAN is a derivative of DaDianNao, and it heavily relies the overall architecture of DaDianNao. The only novel aspect of this contribution is the introduction of the bit-serial compute unit, which (unfortunately) turns out to incur a severe area overhead (of nearly 3x over DaDianNao*s compute units). 3. Nonetheless, the idea of bit-serial computation is certainly quite interesting. I am of the opinion that it would be better appreciated (and perhaps be even more relevant) in a circuit design / architecture focused venue.","['5', '5', '2', '1', '3']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', 'The reviewer*s evaluation is an educated guess', 'The reviewer is fairly confident that the evaluation is correct']","[4, 2, 4, 21]","[9, 6, 9, 26]","[24, 20, 25, 135]","[11, 9, 10, 96]","[10, 8, 10, 18]","[3, 3, 5, 21]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges that the idea of bit-serial computation is 'quite interesting', they express several criticisms. They question the energy efficiency improvement, point out the severe area overhead, and suggest that the work might be better suited for a different venue. These criticisms outweigh the positive aspects mentioned. The politeness score is moderately positive (50) because the reviewer uses respectful language throughout, acknowledging positive aspects ('certainly quite interesting') and framing criticisms as opinions ('I am of the opinion that...') rather than harsh statements. The reviewer also uses phrases like 'I second the earlier review' which shows collegiality. However, the review doesn't go out of its way to be overly polite or encouraging, maintaining a professional tone.",-20,50
Tensorial Mixture Models,Reject,2017,"['Or Sharir', 'Ronen Tamari', 'Nadav Cohen', 'Amnon Shashua']","[4, 7, 5]","['Ok but not good enough - rejection', 'Good paper, accept', '5']","This paper proposes a generative model for mixtures of basic local structures where the dependency between local structures is a tensor. They use tensor decomposition and the result of their earlier paper on expressive power of CNNs along with hierarchical Tucker to provide an inference mechanism. However, this is conditioned on the existence of decomposition. The authors do not discuss how applicable their method is for a general case, what is the subspace where this decomposition exists/is efficient/has low approximation error. Their answer to this question is that in deep learning era these theoretical analysis is not needed. While this claim is subjective, I need to emphasize that the paper does not clarify this claim and does not mention the restrictions. Hence, from theoretical perspective, the paper has flaws and the claims are not justified completely. Some claims cannot be justified with the current results in tensor literature as the authors also mentioned in the discussions. Therefore, they should have corrected their claims in the paper and made the clarifications that this approach is restricted to a clear subclass of tensors. If we ignore the theoretical aspect and only consider the paper from empirical perspective, the experiments the appear in the paper are not enough to accept the paper. MNIST and CIFAR-10 are very simple baselines and more extensive experiments are required. Also, the experiments for missing data are not covering real cases and are too synthetic. Also, the paper lacks the extension beyond images. Since the authors repeatedly mention that their approach goes beyond images, and since the theory part is not complete, those experiments are essential for acceptance of this paper.","['4', '3', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[3, 2, 6, 30]","[8, 7, 12, 36]","[22, 23, 55, 142]","[7, 9, 23, 83]","[15, 13, 27, 39]","[0, 1, 5, 20]","The sentiment score is -60 because the reviewer expresses several significant concerns about the paper, including theoretical flaws, unjustified claims, and insufficient experimental evidence. The reviewer states that 'from theoretical perspective, the paper has flaws' and that 'the experiments that appear in the paper are not enough to accept the paper.' However, the score is not lower because the reviewer acknowledges some positive aspects, such as the proposed model and the use of tensor decomposition. The politeness score is 20 because the reviewer maintains a professional tone throughout, using phrases like 'I need to emphasize' and 'If we ignore the theoretical aspect,' which are polite ways to express criticism. The reviewer also offers specific suggestions for improvement, which is a constructive approach. However, the score is not higher because the criticism is direct and the overall tone is more neutral than overtly polite.",-60,20
The Incredible Shrinking Neural Network: New Perspectives on Learning Representations Through The Lens of Pruning,Reject,2017,"['Nikolas Wolfe', 'Aditya Sharma', 'Lukas Drude', 'Bhiksha Raj']","[3, 3, 3]","['Clear rejection', 'Clear rejection', 'Clear rejection']","The authors have put forward a sincere effort to investigate the *fundamental nature of learning representations in neural networks*, a topic of great interest and importance to our field. They propose to do this via a few simplistic pruning algorithms, to essentially monitor performance decay as a function of unit pruning. This is an interesting idea and one that could potentially be instructive, though in total I don*t think that has been achieved here. First, I find the introduction of pruning lengthy and not particularly novel or surprising. For example, Fig 1 is not necessary, nor is most of the preamble section 3.3.0. The pruning algorithms themselves are sensible (though overly simplistic) approaches, which of course would not matter if they were effective in addressing the question. However, in looking for contributions this paper makes, an interesting, pithy, or novel take on pruning is not one of them, in my opinion. Second, and most relevant to my overall rating, Section 4 does not get deeper than scratching the surface. The figures do not offer much beyond the expected decay in performance as a percentage of neurons removed or gain value. The experiments themselves are not particularly deep, covering a toy problem and MNIST, which does not convince me that I can draw lessons to the broader story of neural networks more generally. Third, there is no essential algorithmic, architectural, or mathematical insight, which I expect out of all but the most heavily experimental papers.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[4, 14, 4, 23]","[4, 20, 9, 29]","[8, 42, 43, 359]","[5, 26, 29, 218]","[3, 11, 11, 105]","[0, 5, 3, 36]","The sentiment score is -60 because the reviewer expresses significant criticism of the paper. While they acknowledge the authors' 'sincere effort' and the topic's importance, they state that the paper doesn't achieve its goals, lacks novelty, and doesn't offer deep insights. The reviewer uses phrases like 'don't think that has been achieved here,' 'not particularly novel or surprising,' and 'does not convince me,' indicating a largely negative view of the paper's contributions. The politeness score is 20 because the reviewer maintains a professional tone and acknowledges the authors' efforts and the topic's importance. They use phrases like 'sincere effort' and 'interesting idea,' which soften the criticism. However, the overall tone is more neutral than overtly polite, hence the relatively low positive score.",-60,20
The Power of Sparsity in Convolutional Neural Networks,Reject,2017,"['Soravit Changpinyo', 'Mark Sandler', 'Andrey Zhmoginov']","[7, 5, 4]","['Good paper, accept', '5', 'Ok but not good enough - rejection']","The paper is about channel sparsity in Convolution layer. The paper is well written and it elaborately discussed and investigated different approaches for applying sparsity. The paper contains detailed literature review. In result section, it showed the approach gives good results using 60% sparsity with reducing number of parameters, which can be useful in some embedded application with limited resource i.e. mobile devices. The main point is that the paper needs more detailed investigation on different dropout schedule. As mentioned implementation details section, they deactivate the connections by applying masks to parameter tensors, which is not helpful in speeding up the training and computation in convolution layer. They can optimize implementation to reduce computation time.","['3', '4', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[5, 15, 2]","[11, 21, 8]","[42, 48, 26]","[16, 24, 8]","[23, 21, 18]","[3, 3, 0]","The sentiment score is 60 (positive) because the reviewer starts with positive comments about the paper being 'well written' and having 'detailed literature review'. They also mention that the approach gives 'good results' and can be useful for certain applications. However, it's not a perfect score as they suggest areas for improvement. The politeness score is 50 (somewhat polite) because the reviewer uses neutral language and offers constructive criticism without harsh words. They acknowledge the paper's strengths before suggesting improvements, which is a polite approach. The tone is professional and respectful throughout, but doesn't go out of its way to be overly polite or complimentary.",60,50
The Predictron: End-To-End Learning and Planning,Reject,2017,"['David Silver', 'Hado van Hasselt', 'Matteo Hessel', 'Tom Schaul', 'Arthur Guez', 'Tim Harley', 'Gabriel Dulac-Arnold', 'David Reichert', 'Neil Rabinowitz', 'Andre Barreto', 'Thomas Degris']","[6, 4, 9]","['Marginally above acceptance threshold', 'Ok but not good enough - rejection', '9']","The paper proposes an approach to learning models that are good for planning problems, using deep netowork architectures. The key idea is to ensure that models are self-consistent and accurately predict the future. The problem of learning good planning models (as opposed to simply good predictive models is really crucial and attempts so far have failed. This paper is conceptually interesting and provides a valuable perspective on how to achieve this goal. Its incorporation of key RL concepts (like discounting and eligibility traces) and the flexibility to learn these is very appealing. Hence, I think it should be accepted. This being said, I think the paper does not quite live up to its claims. Here are some aspects that need to be addressed (in order of importance): 1. Relationship to past work: the proposed representation seems essentially a non-linear implementation of the Horde architecture. It is also very similar in spirit to predictive state representations. Yet these connections are almost not discussed at all. The related work paragraph is very brief and needs expansion to situate the work in the context of other predictive modelling attempts that both were designed to be used for planning and (in the case of PSRs) were in fact successsfully used in planning tasks. Some newer work on learning action-conditional models in Atari games are also not discussed. Situating the paper better in the context of existing model learning would also help understand easier both the motivations and the novel contributions of the work (otherwise, the reader is left to try and elucidate this for themselves, and may come to the wrong conclusion). 2. The paper needs to provide some insight about the necessity of the recurrent core of the architecture. The ideas are presented nicely in general fashion, yet the proposed impolementation is quite specific and *bulky* (very high number of parameters). Is this really necessary in all tasks? Can one implement the basic ideas outside of the particular architecture proposed? Can we use feedforward approximations or is the recurrent part somehow necessary? At the very least the paper should expand the discussion on this topic, if not provide some empirical evidence. 3. The experiments are very restricted in their setup: iid data drawn from fixed distributions, correct targets. So, the proposed approach seems like an overkill for these particular tasks. There is an indirect attempt to provide evidence the learned models would be useful for planning, but no direct measurement to support this*d claim (no use of the models in planning). Compared to the original Horde paper, fewer predictions are learned, and these are more similar to each other. While I sympathize with the desire to go in steps, I think the paper stops short of where it should. At the very least, doing prediction in the context of an actual RL prediction task, with non-iid inputs, should be included in the paper. This should only require minor modifications to the experiments (same task, just different data). Ideally, in the case of the mazes, the learned models should be used in some form of simplified planning to learn paths. This would align the experiments much better with the claims in the presentation of the architecture.","['5', '4', '2']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']","[18, 10, 4, 10, 10, 2, 7, 8, 3, 18, 14]","[23, 16, 9, 16, 15, 6, 13, 12, 8, 24, 17]","[174, 91, 51, 94, 48, 18, 35, 18, 29, 85, 21]","[88, 44, 25, 51, 21, 7, 10, 9, 11, 41, 11]","[58, 45, 26, 35, 22, 10, 22, 7, 17, 32, 5]","[28, 2, 0, 8, 5, 1, 3, 2, 1, 12, 5]","The sentiment score is 50 (slightly positive) because the reviewer starts by acknowledging the paper's conceptual interest and valuable perspective, stating it 'should be accepted'. However, they also mention that the paper 'does not quite live up to its claims' and provide several critical points for improvement. This mix of positive and negative feedback results in a moderately positive sentiment. The politeness score is 70 (fairly polite) because the reviewer uses respectful language throughout, acknowledging the paper's strengths before presenting criticisms. They use phrases like 'I think' and 'Here are some aspects that need to be addressed' rather than making blunt or harsh statements. The reviewer also provides constructive feedback and suggestions for improvement, which contributes to the polite tone.",50,70
The Preimage of Rectifier Network Activities,Reject,2017,"['Stefan Carlsson', 'Hossein Azizpour', 'Ali Razavian']","[4, 4]","['Ok but not good enough - rejection', 'Ok but not good enough - rejection']","Summary: This paper looks at the structure of the preimage of a particular activity at a hidden layer of a network. It proves that any particular activity has a preimage of a piecewise linear set of subspaces. Pros: Formalizing the geometry of the preimages of a particular activity vector would increase our understanding of networks Cons: Analysis seems quite preliminary, and no novel theoretical results or clear practical conclusions. The main theoretical conclusion seems to be the preimage being this stitch of lower dimensional subspaces? Would a direct inductive approach have worked? (e.g. working backwards from the penultimate layer say?) This is definitely an interesting direction, and it would be great to see more results on it (e.g. how does the depth/width, etc affect the division of space, or what happens during training) but it doesn*t seem ready yet.","['4', '5']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[34, 6, 4, 19, 14]","[39, 12, 4, 25, 20]","[78, 51, 10, 58, 34]","[59, 19, 6, 43, 19]","[8, 27, 3, 12, 11]","[11, 5, 1, 3, 4]","The sentiment score is slightly negative (-30) because while the reviewer acknowledges some positive aspects ('interesting direction', 'would increase our understanding'), they express significant concerns about the paper being preliminary and lacking novel theoretical results or clear practical conclusions. The phrase 'doesn't seem ready yet' indicates the reviewer's overall negative sentiment towards the current state of the paper. The politeness score is slightly positive (20) as the reviewer uses respectful language throughout, acknowledging potential benefits and using phrases like 'it would be great to see more results' which show encouragement. However, the criticism is direct without excessive softening, keeping the score from being higher.",-30,20
The loss surface of residual networks: Ensembles and the role of batch normalization,Reject,2017,"['Etai Littwin', 'Lior Wolf']","[7, 3, 7]","['Good paper, accept', 'Clear rejection', 'Good paper, accept']","This paper extend the Spin Glass analysis of Choromanska et al. (2015a) to Res Nets which yield the novel dynamic ensemble results for Res Nets and the connection to Batch Normalization and the analysis of their loss surface of Res Nets. The paper is well-written with many insightful explanation of results. Although the technical contributions extend the Spin Glass model analysis of the ones by Choromanska et al. (2015a), the updated version could eliminate one of the unrealistic assumptions and the analysis further provides novel dynamic ensemble results and the connection to Batch Normalization that gives more insightful results about the structure of Res Nets. It is essential to show this dynamic behaviour in a regime without batch normalization to untangle the normalization effect on ensemble feature. Hence authors claim that steady increase in the L_2 norm of the weights will maintain the this feature but setting for Figure 1 is restrictive to empirically support the claim. At least results on CIFAR 10 without batch normalization for showing effect of L_2 norm increase and results that support claims about Theorem 4 would strengthen the paper. This work provides an initial rigorous framework to analyze better the inherent structure of the current state of art Res Net architectures and its variants which can stimulate potentially more significant results towards careful understanding of current state of art models (Rather than always to attempting to improve the performance of Res Nets by applying intuitive incremental heuristics, it is important to progress on some solid understanding too).","['3', '5', '3']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is fairly confident that the evaluation is correct']","[3, 18]","[9, 24]","[23, 416]","[9, 213]","[14, 168]","[0, 35]","The sentiment score is 70 (positive) because the reviewer expresses a generally positive view of the paper, describing it as 'well-written with many insightful explanation of results' and noting that it provides 'novel dynamic ensemble results' and 'more insightful results about the structure of Res Nets'. The reviewer also mentions that the work 'can stimulate potentially more significant results'. However, it's not a perfect score as the reviewer does suggest some improvements, such as additional empirical support for claims. The politeness score is 80 (polite) because the reviewer uses respectful and constructive language throughout. They acknowledge the paper's strengths before suggesting improvements, and frame their suggestions as ways to 'strengthen the paper' rather than criticizing. The tone is professional and supportive, encouraging further research in this direction.",70,80
Towards Information-Seeking Agents,Reject,2017,"['Philip Bachman', 'Alessandro Sordoni', 'Adam Trischler']","[6, 4, 4]","['Marginally above acceptance threshold', 'Ok but not good enough - rejection', 'Ok but not good enough - rejection']","Pros: * The general idea behind the paper seems pretty novel and potentially quite cool. * The specific technical implementation seems pretty reasonable and well-thought through. * The general types of the tasks that they try out their approach on spans a wide and interesting spectrum of cognition abilities. * The writing is pretty clear. I basically felt like I could replicate much of what they did from their paper descriptions. Cons: * The evaluation of the success of these ideas, as compared to other possible approaches, or as compared to human performance on similar tasks, is extremely cursory. * The specific tasks that they try are quite simple. I really don*t know whether their approach is better than a bunch of simpler things on these tasks. Taking these two cons together, it feels like the authors basically get the implementation done and working somewhat, and then just wrote up the paper. (I know how it feels to be under a deadline without a complete set of results.) If the authors had used their approach to solve an obviously hard problem that previously was completely unsolved, even the type of cursory evaluation level chosen here would have been fine. Or if they had done a very thorough evaluation of a bunch of standard models on each task (and humans too, ideally), and compared their model to those results, that would have been great. But given the complexity of their methods and the fact that the tasks are either not well-known benchmarks or very challenging as such, it*s really hard to tell how much of an advance is made here. But it does seem like a potentially fruitful research direction.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[9, 9, 2]","[15, 15, 8]","[51, 95, 82]","[25, 52, 39]","[25, 40, 43]","[1, 3, 0]","The sentiment score is slightly positive (20) because the reviewer starts with several pros, acknowledging the novelty, reasonable implementation, and clear writing. However, the cons are significant, particularly regarding the evaluation, which balances out much of the initial positivity. The overall tone suggests potential but with substantial room for improvement. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, acknowledging the authors' efforts and the potential of their work. They offer constructive criticism without harsh language, and even empathize with the authors about deadline pressures. The reviewer maintains a professional tone while clearly communicating areas for improvement.",20,60
Towards Understanding the Invertibility of Convolutional Neural Networks,Reject,2017,"['Anna C. Gilbert', 'Yi Zhang', 'Kibok Lee', 'Yuting Zhang', 'Honglak Lee']","[4, 7, 5]","['Ok but not good enough - rejection', 'Good paper, accept', '5']","The paper proposes to provide a theoretical explanation for why deep convolutional neural networks are invertible (at-least, when going back from certain intermediate layers to the image itself). It does so by considering the invertibility of a single layer, assuming the convolutional filters essentially correspond to incoherent measurements satisfying RIP. In my opinion, while this is an interesting direction of research, the paper is not ready for publication. I feel the treatment does not go sufficiently towards explaining the phenomenon in deep neural networks. Even after reading the response from the authors, I feel the results are only a minor variation of the standard results from compressive sensing for sparse reconstruction with incoherent measurements. A deep neural network is fundamentally different from a single layer---it is the *deep* part that makes the forward task work. As the authors note, there is significant deterioration when IHT is applied recursively----therefore, at best the theory explains the partial invertibility of a single layer. That a single layer is approximately invertible isn*t surprising, that a cascade of layers *is*. For any theoretical analysis of this phenomenon to be useful, I believe it must go beyond analyzing a single compressive measurement-type layer, and try to explain how much of the same theory holds for a cascade. I say this because it*s entirely possible that the sparse recovery theory breaks down beyond a single layer, and invertibility ends up being a property caused by correlations between the weights of different layers. In other words, there is no way to tell from the current results for individual layers whether they are in fact a step towards explaining the invertibility of whole networks.","['4', '3', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[20, 25, 3, 8, 13]","[25, 31, 8, 12, 19]","[105, 1300, 26, 21, 273]","[51, 574, 13, 10, 145]","[27, 160, 13, 8, 123]","[27, 566, 0, 3, 5]","The sentiment score is -60 because the reviewer clearly states the paper is 'not ready for publication' and expresses significant concerns about the depth and applicability of the research. They use phrases like 'does not go sufficiently towards explaining the phenomenon' and 'at best the theory explains the partial invertibility of a single layer', indicating strong reservations. However, they do acknowledge it as an 'interesting direction of research', preventing the score from being even lower. The politeness score is 20 because while the reviewer is critical, they maintain a professional tone throughout. They use phrases like 'in my opinion' and 'I feel', which soften the criticism. The reviewer also acknowledges the authors' response, showing respect for the dialogue process. However, the overall critical nature of the review prevents a higher politeness score.",-60,20
Training Group Orthogonal Neural Networks with Privileged Information,Reject,2017,"['Yunpeng Chen', 'Xiaojie Jin', 'Jiashi Feng', 'Shuicheng Yan']","[6, 5, 6]","['Marginally above acceptance threshold', '5', 'Marginally above acceptance threshold']","This paper proposes to learn groups of orthogonal features in a convnet by penalizing correlation among features in each group. The technique is applied in the setting of image classification with “privileged information” in the form of foreground segmentation masks, where the model is trained to learn orthogonal groups of foreground and background features using the correlation penalty and an additional “background suppression” term. Pros: Proposes a “group-wise model diversity” loss term which is novel, to my knowledge. The use of foreground segmentation masks to improve image classification is also novel. The method is evaluated on two standard and relatively large-scale vision datasets: ImageNet and PASCAL VOC 2012. Cons: The evaluation is lacking. There should be a baseline that leaves out the background suppression term, so readers know how much that term is contributing to the performance vs. the group orthogonal term. The use of the background suppression term is also confusing to me -- it seems redundant, as the group orthogonality term should already serve to suppress the use of background features by the foreground feature extractor. It would be nice to see the results with “Incomplete Privileged Information” on the full ImageNet dataset (rather than just 10% of it) with the privileged information included for the 10% of images where it’s available. This would verify that the method and use of segmentation masks remains useful even in the regime of more labeled classification data. The presentation overall is a bit confusing and difficult to follow, for me. For example, Section 4.2 is titled “A Unified Architecture: GoCNN”, yet it is not an overview of the method as a whole, but a list of specific implementation details (even the very first sentence). Minor: calling eq 3 a “regression loss” and writing “||0 - x||” rather than just “||x||” is not necessary and makes understanding more difficult -- I’ve never seen a norm regularization term written this way or described as a “regression to 0”. Minor: in fig. 1 I think the FG and BG suppression labels are swapped: e.g., the “suppress foreground” mask has 1s in the FG and 0s in the BG (which would suppress the BG, not the FG). An additional question: why are the results in Table 4 with 100% privileged information different from those in Table 1-2? Are these not the same setting? The ideas presented in this paper are novel and show some promise, but are currently not sufficiently ablated for readers to understand what aspects of the method are important. Besides additional experiments, the paper could also use some reorganization and revision for clarity. =============== Edit (1/29/17): after considering the latest revisions -- particularly the full ImageNet evaluation results reported in Table 5 demonstrating that the background segmentation *privileged information* is beneficial even with the full labeled ImageNet dataset -- I*ve upgraded my rating from 4 to 6. (I*ll reiterate a very minor point about Figure 1 though: I still think the *0* and *1* labels in the top part of the figures should be swapped to match the other labels. e.g., the topmost path in figure 1a, with the text *suppress foreground*, currently has 0 in the background and 1 in the foreground, when one would want the reverse of this to suppress the foreground.)","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[3, 5, 8, 16]","[9, 11, 14, 22]","[80, 65, 542, 876]","[33, 24, 214, 382]","[36, 36, 235, 200]","[11, 5, 93, 294]","The sentiment score is slightly positive (20) because the reviewer acknowledges some pros of the paper, such as its novel approach and evaluation on standard datasets. However, they also point out several cons and areas for improvement, which tempers the positivity. The overall tone suggests the paper has potential but needs significant revisions. The politeness score is moderately high (60) as the reviewer uses professional language throughout, offers constructive criticism, and even upgrades their rating after considering revisions. They phrase criticisms as suggestions for improvement rather than harsh judgments. The reviewer also takes care to separate major concerns from minor points, which is a polite way to structure feedback.",20,60
Transformation-based Models of Video Sequences,Reject,2017,"['Joost van Amersfoort', 'Anitha Kannan', ""Marc'Aurelio Ranzato"", 'Arthur Szlam', 'Du Tran', 'Soumith Chintala']","[5, 6, 3]","['5', 'Marginally above acceptance threshold', 'Clear rejection']","The paper proposes a method for future frame prediction based on transformation of previous frame rather than direct pixel prediction. Many previous works have proposed similar methods. The authors in their responses state that previous work is deterministic, yet the proposed model also does not handle multimodality. Further, i asked if they could test their method using 2 RGB frames as input and predicting the transformation as output, to be able to quantify the importance of using transformations both as input and output, since this is the first work that uses transformations as input also. The authors dismissed the suggestion by saying *if we were to use RGB frames as input and ask the model to output future frames it would produce very blurry results*, that is, misunderstanding what the suggestion was. So, currently, it does not seem to be a valid novel contribution in this work compared to previous works.","['3', '4', '3']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[3, 18, 12, 9, 10, 8]","[5, 24, 18, 15, 16, 14]","[4, 78, 106, 138, 52, 39]","[1, 48, 57, 54, 25, 13]","[3, 26, 44, 74, 25, 23]","[0, 4, 5, 10, 2, 3]","The sentiment score is -80 because the reviewer expresses strong skepticism about the paper's novelty and contribution. They state that 'it does not seem to be a valid novel contribution in this work compared to previous works,' which is a highly negative assessment. The reviewer also points out that the authors misunderstood or dismissed their suggestions, further indicating dissatisfaction. The politeness score is -20 because while the language isn't overtly rude, it's quite direct and critical. The reviewer doesn't use softening language or acknowledge any positive aspects of the work. The dismissive tone in phrases like 'the authors dismissed the suggestion' and the implication that the authors misunderstood a basic concept contributes to the slightly impolite tone.",-80,-20
Transformational Sparse Coding,Reject,2017,"['Dimitrios C. Gklezakos', 'Rajesh P. N. Rao']","[5, 4, 4]","['5', 'Ok but not good enough - rejection', 'Ok but not good enough - rejection']","This paper trains a generative model of image patches, where dictionary elements undergo gated linear transformations before being combined. The transformations are motivated in terms of Lie group operators, though in practice they are a set of fixed linear transformations. This is motivated strongly in terms of learning a hierarchy of transformations, though only one layer is used in the experiments (except for a toy case in the appendix). I like the motivation for this algorithm. The realization seems very similar to a group or block sparse coding implementation. I was disappointed by the restriction to linear transformations. The experiments were all toy cases, demonstrating that the algorithm can learn groups of Gabor- or center surround-like features. They would have been somewhat underpowered five years ago, and seemed extremely small by today*s standards. Specific comments: Based on common practices in ML literature, I have a strong bias to think of as inputs and as network weights. Latent variables are often or . Depending on your target audience, I would suggest permuting your choice of symbols so the reader can more quickly interpret your model. nit: number all equations for easier reference sec 2.2 -- It*s weird that the transformation is fixed, but is still written as a function of x. sec 2.3 -- The updated text here confuses me actually. I had thought that you were using a fixed set of linear transformations, and were motivating in terms of Lie groups, but were not actually taking matrix exponentials in your algorithm. The equations in the second half of this section suggest you are working with matrix exponentials though. I*m not sure which direction I*m confused in, but probably good to clarify the text either way. BTW -- there*s another possible solution to the local minima difficulty, which is the one used in Sohl-Dickstein, 2010. There, they introduce blurring operators matched to each transformation operator, and gradient descent can escape local minima by detouring through coarser (more blurred) scales. sec 3.2 -- I believe by degrees of freedom you mean the number of model parameters, not the number of latent coefficients that must be inferred? Should make this more clear. Is it more appropriate to compare reconstruction error while matching number of model parameters, or number of latent variables? I wonder if a convolutional version of this algorithm would be practical / would make it more suited as a generative model of whole images. ==== post rebuttal update Thank you for taking the time to write the rebuttal! I have read it, but it did not significantly effect my rating.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[5, 36]","[10, 42]","[5, 127]","[1, 65]","[4, 20]","[0, 42]","The sentiment score is -30 because while the reviewer expresses some positive aspects ('I like the motivation for this algorithm'), there are several critical points raised. The reviewer is 'disappointed' by certain aspects, describes the experiments as 'underpowered' and 'extremely small', and states that the rebuttal did not significantly affect their rating. These criticisms outweigh the positive elements, resulting in a slightly negative overall sentiment. The politeness score is 20 because the reviewer uses generally polite language ('I like', 'Thank you for taking the time'), offers constructive feedback, and frames criticisms as personal opinions ('I was disappointed', 'I wonder if'). However, some direct criticisms slightly reduce the politeness score from being highly positive.",-30,20
Tree-Structured Variational Autoencoder,Reject,2017,"['Richard Shin', 'Alexander A. Alemi', 'Geoffrey Irving', 'Oriol Vinyals']","[3, 3, 4]","['Clear rejection', 'Clear rejection', 'Ok but not good enough - rejection']","The method overall seems to be a very interesting structural approach to variational autoencoders, however it seems to lack motivation as well as the application areas sufficient to prove its effectiveness. I see the attractiveness of using structural information in this context and I find it more intuitive than using a flat sequence representation, especially when there is a clear structure in the data. However experimental results seem to fail to be convincing in that regard. One issue is the lack of a variety of applications in general, the experiments seem to be very limited in that regard, considering that the paper itself speaks about natural language applications. It would be interesting to use the latent representations learned with the model for some other end task and see how much it impacts the success of that end task compared to various baselines. In my opinion, the paper has a potentially strong idea however in needs stronger results (and possibly in a wider variety of applications) as a proof of concept.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[-1, 0, 3, 23, 12, 15]","[4, 5, 8, 28, 17, 20]","[4, 6, 14, 53, 7, 22]","[4, 5, 10, 12, 2, 9]","[0, 1, 2, 2, 0, 0]","[0, 0, 2, 39, 5, 13]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the method as 'very interesting' and sees its 'attractiveness', they express significant concerns about the lack of motivation, limited applications, and unconvincing experimental results. The overall tone suggests that the paper needs substantial improvements. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, acknowledging positive aspects and framing criticisms constructively. They use phrases like 'seems to', 'I find it', and 'It would be interesting', which soften the critique. The reviewer also offers suggestions for improvement, indicating a helpful rather than dismissive attitude.",-20,50
Understanding intermediate layers using linear classifier probes,Reject,2017,"['Guillaume Alain', 'Yoshua Bengio']","[5, 4, 4]","['5', 'Ok but not good enough - rejection', 'Ok but not good enough - rejection']","This paper proposes a method that attempts to *understand* what is happening within a neural network by using linear classifier probes which are inserted at various levels of the network. I think the idea is nice overall because it allows network designers to better understand the representational power of each layer in the network, but at the same time, this works feels a bit rushed. In particular, the fact that the authors did not provide any results in *real* networks, which are used to win competitions makes the results less strong, since researchers who want to created competitive network architectures don*t have enough evidence from this work to decides whether they should use it or not. Ideally, I would encourage the authors to consider continuing this line of research and show how to use the information given by these linear classifiers to construct better network architectures. Unfortunately, as is, I don*t think we have enough novelty to justify accepting this work in the conference.","['3', '4', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[9, 30]","[12, 36]","[17, 977]","[6, 405]","[9, 456]","[2, 116]","The sentiment score is -50 because while the reviewer acknowledges that the idea is 'nice overall', they express significant concerns about the work feeling 'rushed' and lacking results from 'real networks'. The reviewer ultimately concludes that there isn't 'enough novelty to justify accepting this work', indicating a generally negative sentiment despite some positive aspects. The politeness score is 20 because the reviewer uses polite language throughout, such as 'I think' and 'I would encourage', and frames criticisms constructively. However, the overall message is still a rejection, which prevents a higher politeness score.",-50,20
Understanding trained CNNs by indexing neuron selectivity,Reject,2017,"['Ivet Rafegas', 'Maria Vanrell', 'Luís A. Alexandre']","[7, 3, 7]","['Good paper, accept', 'Clear rejection', 'Good paper, accept']","This paper makes three main methodological contributions: - definition of Neural Feature (NF) as the pixel average of the top N images that highly activation a neuron - ranking of neurons based on color selectivity - ranking of neurons based on class selectivity The main weaknesses of the paper are that none of the methodological contributions are very significant, and no singularly significant result arises from the application of the methods. However, the main strengths of the paper are its assortment of moderately-sized interesting conclusions about the basic behavior of neural nets. For example, a few are: - “Indexing on class selectivity neurons we found highly class selective neurons like digital-clock at conv2, cardoon at conv3 and ladybug at conv5, much before the fully connected layers.” As far as I know, this had not been previously reported. - Color selective neurons are found even in higher layers. (25% color selectivity in conv5) - “our main color axis emerge (black-white, blue-yellow, orange-cyan and cyan- magenta). Curiously, these two observations correlate with evidences in the human visual system (Shapley & Hawken (2011)).” Great observation! Overall, I’d recommend the paper be accepted, because although it’s difficult to predict at this time, there’s a fair chance that one of the “smaller conclusions” would turn out to be important in hindsight a few years hence. Other small comments: - The cite for “Learning to generate chairs…” is wrong (first two authors combined resulting in a confusing cite) - What exactly is the Color Selectivity Index computing? The Opponent Color Space isn’t well defined and it wasn’t previously familiar to me. Intuitively it seems to be selecting for units that respond to a constant color, but the highest color selectivity NF in Fig 5 i for a unit with two colors, not one. Finally, the very last unit (lowest color selectivity) is almost the same edge pattern, but with white -> black instead of blue -> orange. Why are these considered to be so drastically different? This should probably be more clearly described. - For the sake of argument, imagine a mushroom sensitive neuron in conv5 that fires highly for mushrooms of *any* color but not for anything else. If the dataset contains only red-capped mushrooms, would the color selectivity index for this neuron be high or low? If it is high, it’s somewhat misleading because the unit itself actually isn’t color selective; the dataset just happens only to have red mushrooms in it. (It’s a subtle point but worth considering and probably discussing in the paper)","['4', '5', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is fairly confident that the evaluation is correct']","[3, 22, 18]","[6, 25, 24]","[4, 52, 125]","[1, 36, 70]","[2, 5, 31]","[1, 11, 24]","The sentiment score is 50 (slightly positive) because the reviewer acknowledges both strengths and weaknesses of the paper, but ultimately recommends acceptance. They mention 'main weaknesses' but also highlight 'main strengths' and 'interesting conclusions'. The overall tone is constructive and the reviewer sees potential value in the work. The politeness score is 60 (moderately polite) because the reviewer uses respectful language throughout, offers balanced feedback, and provides specific examples to support their points. They use phrases like 'Great observation!' which shows appreciation. The reviewer also offers suggestions for improvement in a constructive manner, without using harsh or dismissive language. The tone is professional and considerate, maintaining a respectful dialogue with the authors.",50,60
Universality in halting time,Reject,2017,"['Levent Sagun', 'Thomas Trogdon', 'Yann LeCun']","[5, 2, 5]","['5', 'Strong rejection', '5']","The authors explore whether the halting time distributions for various algorithms in various settings exhibit *universality*, i.e. after rescaling to zero mean and unit variance, the distribution does not depend on stopping parameter, dimensionality and ensemble. The idea of the described universality is very interesting. However I see several shortcomings in the paper: In order to be of practical relevance, the actual stopping time might be more relevant than the scaled one. The discussion of exponential tailed halting time distributions is a good start, but I am not sure how often this might be actually helpful. Still, the findings in the paper might be interesting from a theoretical point of view. Especially for ICLR, I think it would have been more interesting to look into comparisons between stochastic gradient descent, momentum, ADAM etc on different deep learning architectures. Over which of those parameters does universality hold?. How can different initializations influence the halting time distribution? I would expect a sensible initialization to cut of part of the right tail of the distribution. Additionally, I found the paper quite hard to read. Here are some clarity issues: - abstract: *even when the input is changed drastically*: From the abstract I*m not sure what *input* refers to, here - I. Introduction: *where the stopping condition is, essentially, the time to find the minimum*: this doesn*t seem to make sense, a condition is not a time. I guess the authors wanted to say that the stopping condition is that the minimum has been reached? - I.1 the notions of dimension N, epsilon and ensemble E are introduced without any clarification what they are. From the later parts of the paper I got some ideas and examples, but here it is very hard to understand what these parameters should be (just some examples would be already helpful) - I.3 *We use x^ell for ell in Z={1, dots, S} where Z is a random sample from of training samples* This formulation doesn*t make sense. Either Z is a random sample, or Z={1, ..., S}. - II.1 it took me a long time to find the meaning of M. As this parameter seems to be crucial for universality in this case, it would be very helpful to point out more explicitly what it refers to.","['3', '4', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[3, 5, 30]","[8, 11, 36]","[35, 26, 315]","[12, 1, 162]","[23, 20, 113]","[0, 5, 40]","The sentiment score is -30 because while the reviewer finds the idea interesting, they point out several shortcomings and clarity issues. The overall tone suggests more criticism than praise, but it's not entirely negative. The politeness score is 50 because the reviewer uses respectful language throughout, acknowledging positive aspects before critiquing, and phrases criticisms as suggestions or observations rather than harsh judgments. They use phrases like 'I think it would have been more interesting' and 'I found the paper quite hard to read' instead of more direct or harsh criticisms. The reviewer also provides specific examples and suggestions for improvement, which is a polite and constructive approach to peer review.",-30,50
Unsupervised Pretraining for Sequence to Sequence Learning,Reject,2017,"['Prajit Ramachandran', 'Peter J. Liu', 'Quoc V. Le']","[7, 5, 6]","['Good paper, accept', '5', 'Marginally above acceptance threshold']","In this paper, the authors propose to pretrain the encoder/decoder of seq2seq models on a large amount of unlabeled data using a LM objective. They obtain improvements using this technique on machine translation and abstractive summarization. While the effectiveness of pretraining seq2seq models has been known among researchers and explored in a few papers (e.g. Zoph et al. 2016, Dai and Le 2015), I believe this is the first paper to pretrain using a LM for both the encoder/decoder. The technique is simple, but the gains are large (e.g. +2.7 BLEU on NMT). In addition, the authors perform extensive ablation studies to analyze where the performance is coming from. Hence, I think this paper should be accepted.","['5', '5', '4']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[2, 2, 13]","[6, 8, 19]","[20, 32, 300]","[9, 9, 143]","[11, 21, 146]","[0, 2, 11]","The sentiment score is 80 (positive) because the reviewer expresses a clear positive opinion about the paper, stating that it should be accepted and highlighting its strengths. They mention the large gains achieved, the extensive ablation studies, and the novelty of the approach. The politeness score is 60 (moderately polite) because the reviewer uses respectful and professional language throughout, acknowledging the authors' work and contributions without using overly effusive praise or harsh criticism. The tone is constructive and supportive, which contributes to the politeness, but it doesn't go out of its way to be exceptionally polite either.",80,60
Vocabulary Selection Strategies for Neural Machine Translation,Reject,2017,"[""Gurvan L'Hostis"", 'David Grangier', 'Michael Auli']","[5, 4, 4, 5]","['5', 'Ok but not good enough - rejection', 'Ok but not good enough - rejection', '5']","This paper conducts a comprehensive series of experiments on vocabulary selection strategies to reduce the computational cost of neural machine translation. A range of techniques are investigated, ranging from very simple methods such as word co-occurences, to the relatively complex use of SVMs. The experiments are solid, comprehensive and very useful in practical terms. It is good to see that the best vocabulary selection method is very effective at achieving a very high proportion of the coverage of the full-vocabulary model (fig 3). However, I feel that the experiments in section 4.3 (vocabulary selection during training) was rather limited in their scope - I would have liked to see more experiments here. A major criticism I have with this paper is that there is little novelty here. The techniques are mostly standard methods and rather simple, and in particular, there it seems that there is not much additional material beyond the work of Mi et al (2016). So although the work is solid, the lack of originality lets it down. Minor comments: in 2.1, the word co-occurence measure - was any smoothing used to make this measure more robust to low counts?","['3', '5', '4', '3']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[2, 13, 9]","[1, 18, 15]","[1, 94, 137]","[0, 44, 68]","[1, 41, 67]","[0, 9, 2]","The sentiment score is slightly positive (20) because the reviewer acknowledges the comprehensive experiments and their practical usefulness. They praise the effectiveness of the best vocabulary selection method. However, the score is not higher due to criticisms about limited scope in some experiments and lack of novelty. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, balancing praise with constructive criticism. They use phrases like 'It is good to see' and frame criticisms politely, such as 'I feel that' and 'I would have liked to see'. The reviewer also provides specific feedback and asks a clarifying question at the end, showing engagement with the work.",20,60
Warped Convolutions: Efficient Invariance to Spatial Transformations,Reject,2017,"['Joao F. Henriques', 'Andrea Vedaldi']","[6, 7, 6]","['Marginally above acceptance threshold', 'Good paper, accept', 'Marginally above acceptance threshold']","The paper shows how group convolutions (for two dimensional commutative groups) can be performed by standard CNNs if the input is warped using a fixed warp. The idea is practical and seems to work well. The paper is well written. I agree with reviewer 1 that the ‘theorems’ do not deserve to be labelled as such. Theorem 1 is equivalent to the second equation from this section of the wikipedia page on convolution: https://en.wikipedia.org/wiki/Convolution#Convolutions_on_groups. The existence of Haar measure (from which commutation of Lg and convolution follows immediately) is a well known and elementary theorem in harmonic analysis, which would be treated in the first few pages of any textbook on the subject. It also immediately clear that any commutative group has an additive parameterization (theorem 2). The fact that the paper does not present new deep mathematical results is not a significant weakness in my opinion, but the derivations should not be camouflaged as such. The claim that previous methods that use group convolutions are slow and that the presented approach has better computational complexity is not supported by empirical evidence, and the theoretical analysis is still a bit misleading. For example, the authors write “Unfortunately these approaches do not possess the same memory and speed benefits that CNNs enjoy. The reason is that, ultimately, they have to enumerate all possible transformations”. The presented method also has to enumerate all transformations (in a limited range, on a discretized grid), and this is feasible only because the group is only 2 dimensional (and indeed this is also true for standard CNNs which enumerate translations). As noted in my pre-review question, I believe the computational complexity analysis is not entirely correct, and assume the authors will correct this. Equation 4 is presented as a new invention, but this has been used in previous works and is well known in mathematics, so a citation should be added. The main advantage of the presented method over several earlier methods is that it is very simple to implement, and can re-use highly optimized convolution routines. As I understand it, Dieleman et al. and Cohen & Welling also use standard convolutions (after a fixed filter / feature map warp), but these papers only consider discrete groups. So it seems like this paper occupies a unique place in the space of equivariant convolutional networks: non-commutative (-), low-dimensional (-), continuous (+) groups, simple (+) and efficient (+) algorithm. Given the proximity though, a more thorough and balanced appraisal of the merits and demerits, as well as the novelty, relative to each of the previous works would be useful. Provided that these issues are cleared up, I would recommend the paper for publication.","['5', '4', '4']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[8, 13]","[14, 19]","[83, 325]","[42, 164]","[37, 144]","[4, 17]","The sentiment score is 50 (slightly positive) because the reviewer acknowledges the paper's practical idea, good writing, and unique position in the field. However, they also point out several issues, such as mislabeling theorems and unsupported claims about computational complexity. The overall tone suggests a balanced view with both praise and constructive criticism. The politeness score is 70 (fairly polite) because the reviewer uses respectful language throughout, acknowledges the paper's strengths, and offers suggestions for improvement rather than harsh criticism. Phrases like 'I agree with reviewer 1' and 'Provided that these issues are cleared up, I would recommend the paper for publication' indicate a collegial and constructive approach to the review process.",50,70
Wav2Letter: an End-to-End ConvNet-based Speech Recognition System,Reject,2017,"['Ronan Collobert', 'Christian Puhrsch', 'Gabriel Synnaeve']","[6, 7]","['Marginally above acceptance threshold', 'Good paper, accept']","There have been numerous works on learning from raw waveforms and training letter-based CTC networks for speech recognition, however, there are very few works on combining both of them with purely ConvNet as it is done in this paper. It is interesting to see results on a large scale corpus such as Librispeech that is used in this paper, though some baseline results from hybrid NN/HMM systems should be provided. To readers, it is unclear how this system is close to state-of-the-art only from Table 2. The key contribution of this paper may be the end-to-end sequence training criterion for their CTC variant (where the blank symbol is dropped), which may be viewed as sequence training of CTC as H. Sak, et al. *Learning acoustic frame labeling for speech recognition with recurrent neural networks*, 2015. However, instead of generating the denominator lattices using a frame-level trained CTC model first, this paper directly compute the sequence-level loss by considering all the competing hypothesis in the normalizer. Therefore, the model is trained end-to-end. From this perspective, it is closely related to D. Povey*s LF-MMI for sequence-training of HMMs. As another reviewer has pointed out, references and discussions on that should be provided. This approach should be more expensive than frame-level training of CTCs, however, from Table 1, the authors* implementation is much faster. Did the systems there use the same sampling rate? You said at the end of 2.2 that the step size for your model is 20ms. Is it also the same for Baidu*s CTC system. Also, have you tried increasing the step size, e.g. to 30ms or 40ms, as people have found that it may work (equally) better, while significantly cut down the computational cost.","['4', '5']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[17, 4, 8]","[23, 9, 14]","[125, 10, 132]","[70, 4, 59]","[47, 6, 67]","[8, 0, 6]","The sentiment score is 50 (slightly positive) because the reviewer acknowledges the interesting aspects of the paper and its contributions, while also pointing out areas for improvement. The review starts with positive remarks about the novelty of combining raw waveforms and letter-based CTC networks, and the use of a large-scale corpus. However, it also suggests adding baseline results and clarifying the state-of-the-art comparison. The politeness score is 70 (fairly polite) because the reviewer uses respectful language throughout, asking questions rather than making demands, and acknowledging the paper's contributions. The reviewer offers constructive criticism and suggestions for improvement without using harsh or dismissive language. The tone is professional and academic, maintaining a balance between praise and critique.",50,70
What Is the Best Practice for CNNs Applied to Visual Instance Retrieval?,Reject,2017,"['Jiedong Hao', 'Jing Dong', 'Wei Wang', 'Tieniu Tan']","[6, 3, 3]","['Marginally above acceptance threshold', 'Clear rejection', 'Clear rejection']","The paper conducts a detailed evaluation of different CNN architectures applied to image retrieval. The authors focus on testing various architectural choices, but do not propose or compare to end-to-end learning frameworks. Technically, the contribution is clear, particularly with the promised clarifications on how multiple scales are handled in the representation. However, I am still not entirely clear whether there would be a difference in the multi-scale settting for full and cropped queries. While the paper focuses on comparing different baseline architectures for CNN-based image retrieval, several recent papers have proposed to learn end-to-end representations specific for this task, with very good result (see for instance the recent work by Gordo et al. *End-to-end Learning of Deep Visual Representations for Image Retrieval*). The authors clarify that their work is orthogonal to papers such as Gordo et al. as they assess instead the performance of networks pre-trained from image classification. In fact, they also indicate that image retrieval is more difficult than image classification -- this is because it is performed by using features originally trained for classification. I can partially accept this argument. However, given the results in recent papers, it is clear than end-to-end training is far superior in practice and it is not clear the analysis developed by the authors in this work would transfer or be useful for that case as well.","['4', '5', '5']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[2, 11, 10, 30]","[6, 17, 16, 36]","[5, 105, 75, 673]","[3, 57, 38, 437]","[2, 31, 19, 84]","[0, 17, 18, 152]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects of the paper (e.g., 'Technically, the contribution is clear'), they express several concerns and criticisms. The reviewer questions the relevance of the work compared to end-to-end learning frameworks and expresses doubt about the transferability of the analysis. However, the score is not deeply negative as the reviewer does recognize some merits of the work.

The politeness score is moderately positive (50) because the reviewer uses respectful and professional language throughout. They acknowledge the authors' clarifications and arguments, even when disagreeing (e.g., 'I can partially accept this argument'). The reviewer also provides constructive feedback and references to relevant work. While not overly effusive, the tone remains courteous and appropriate for academic discourse.",-20,50
A Simple yet Effective Method to Prune Dense Layers of Neural Networks,Reject,2017,"['Mohammad Babaeizadeh', 'Paris Smaragdis', 'Roy H. Campbell']","[5, 5, 3]","['5', '5', 'Clear rejection']","Summary: In this paper, the authors introduce NoiseOut, a way to reduce parameters by pruning neurons from a network. They do this by identifying pairs of neurons produce the most correlated outputs, and replacing the pair by one neuron, and then appropriately adjusting weights. This technique relies on neurons having high correlations however, so they introduce an additional output neuron -- a noise output, which results in the network trying to predict the mean of the noise distribution. As this is a constant, it increases correlation between neurons. Experiments test this out on MNIST and SVHN Comments: This is an interesting suggestion on how to prune neurons, but more experiments (on larger datasets) are probably need to be convincing that this is an approach that is guaranteed to work well. Equation (5) seems to be very straightforwards? It seems like that for larger datasets, more noise outputs might have to be added to ensure higher correlations? Is there a downside to this in terms of the overall accuracy? The paper is presented clearly, and was definitely interesting to read, so I encourage the authors to continue this line of work.","['4', '3', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[6, 22, 44]","[11, 28, 48]","[25, 180, 312]","[11, 105, 239]","[14, 48, 14]","[0, 27, 59]","The sentiment score is 50 (slightly positive) because the reviewer describes the paper as 'interesting' and encourages the authors to continue their work, but also suggests that more experiments are needed to be convincing. The politeness score is 80 (quite polite) because the reviewer uses respectful language throughout, acknowledges the paper's clarity and interest, and frames suggestions as encouragement rather than criticism. The phrases 'interesting suggestion', 'presented clearly', and 'definitely interesting to read' contribute to both the positive sentiment and polite tone.",50,80
A hybrid network: Scattering and Convnet,Reject,2017,['Edouard Oyallon'],"[7, 5, 7]","['Good paper, accept', '5', 'Good paper, accept']","In this paper, the authors explicitly design geometrical structure into a CNN by combining it with a Scattering network. This aids stability and limited-data performance. The paper is well written, the contribution of combining Scattering and CNNs is novel and the results seem promising. I feel that such work was a missing piece in the Scattering literature to make it useful for practical applications. I wish the authors would have investigated the effect of the stable bottom layers with respect to adversarial examples. This can be done in a relatively straightforward way with software like cleverhans [1] or deep fool [2]. It would be very interesting if the first layer*s stability in the hybrid architectures increases robustness significantly, as this would tell us that these fooling images are related to low-level geometry. Finding that this is not the case, would be very interesting as well. Further, the proposed architecture is not evaluated on real limited data problems. This would further strengthen the improved generalization claim. However, I admit that the Cifar-100 / Cifar-10 difference already seems like a promising indicator in this regard. If one of the two points above will be addressed in an additional experiment, I would be happy to raise my score from 6 to 7. Summary: + An interesting approach is presented that might be useful for real-world limited data scenarios. + Limited data results look promising. - Adversarial examples are not investigated in the experimental section. - No realistic small-data problem is addressed. Minor: - The authors should add a SOTA ResNet to Table 3, as NiN is indeed out of fashion these days. - Some typos: tacke, developping, learni. [1] https://arxiv.org/abs/1610.00768v3 [2] https://arxiv.org/abs/1511.04599","['4', '4', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']",[4],[9],[40],[15],[22],[3],"The sentiment score is 60 (positive) because the reviewer expresses that the paper is well-written, novel, and promising. They use phrases like 'well written,' 'novel,' and 'promising results.' However, they also point out some limitations and suggest improvements, which prevents the score from being higher. The politeness score is 80 (polite) because the reviewer uses respectful language throughout, offers constructive criticism, and expresses willingness to raise their score if certain points are addressed. They use phrases like 'I wish the authors would have,' and 'I would be happy to raise my score,' which indicate a polite and encouraging tone. The reviewer also provides specific suggestions for improvement and references to support their points, which is considerate and helpful.",60,80
Adjusting for Dropout Variance in Batch Normalization and Weight Initialization,Reject,2017,"['Dan Hendrycks', 'Kevin Gimpel']","[5, 7, 6]","['5', 'Good paper, accept', 'Marginally above acceptance threshold']","This paper proposes new initialization for particular architectures and a correction trick to batch normalization to correct variance introduced by dropout. While authors state interesting observations, the claims are not supported with convincing results. I guess Figure 1 is only for mnist and for only two values of p with one particular network architecture, the dataset and empirical setup is not clear. The convergence is demonstrated only for three dropout values in Figure 2 which may cause an unfair comparison. For instance how does the convergence compare for the best dropout rate after cross-validation (three figures each figure has three results for one method with different dropouts [bests cv result for each one])? Also how is the corresponding validation error and test iterations? Also only mnist does not have to generalize to other benchmarks. Figure 3 gives closer results for Adam optimizer, learning rate is not selected with random search or bayesian optimization, learning decay iterations fixed and regularization coefficient is set to a small value without tuning. A slightly better tuning of parameters may close the current gap. Also Nesterov based competitor gives unreasonably worse accuracy compared to recent results which may indicate that this experiment should not be taken into account. In Table 2, there is no significant improvement on CIFAR10. The CIFAR100 difference is not significant without including batch normalization variance re-estimation. However there is no result for *original with BN update* therefore it is not clear whether the BN update helps in general or not. SVHN also does not have result for original with BN update. There should be baselines with batch normalizations for Figure 1,2 3 to support the claims convincingly. The main criticism about batch normalization is additional computational cost by giving (Mishkin et al, 2016 ) as reference however this should not be a reason to not to compare the initialization to batch-normalization. In fact, (Mishkin et al, 2016) performs comparison to batch normalization and also with and without data augmentation with recent state of art architectures. None of the empirical results have data augmentation. It is not clear if the initialization or batch normalization update will help or make it worse for that case. Recent state of art methods methods like Res Net variant and Dense Net scale to many depths and report result for ImageNet. Although the authors claim that this can be extended to residual network variants, it is not clear if there is going to be any empirical gain for that architectures. This work requires a comprehensive and fair comparison. Otherwise the contribution is not significant.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[2, 10]","[8, 16]","[64, 161]","[25, 90]","[37, 66]","[2, 5]","The sentiment score is -70 because the review is predominantly critical. The reviewer points out numerous shortcomings in the paper's methodology, experimental setup, and results. They use phrases like 'claims are not supported with convincing results', 'unfair comparison', and 'requires a comprehensive and fair comparison'. The reviewer also states that 'the contribution is not significant' without substantial improvements. However, it's not entirely negative as they acknowledge some 'interesting observations', hence not the lowest possible score. The politeness score is 20 because while the reviewer is critical, they maintain a professional and academic tone throughout. They use phrases like 'I guess' and 'it is not clear' rather than making blunt accusations. The reviewer also offers specific suggestions for improvement, which is a polite way to provide criticism. However, the overall tone is more neutral than overtly polite, hence a slightly positive but not high score.",-70,20
An Actor-critic Algorithm for Learning Rate Learning,Reject,2017,"['Chang Xu', 'Tao Qin', 'Gang Wang', 'Tie-Yan Liu']","[5, 4, 3]","['5', 'Ok but not good enough - rejection', 'Clear rejection']","The authors present a method for adaptively setting the step size for SGD by treating the learning rate as an action in an MDP whose reward is the change in loss function. The method is presented against popular adaptive first-order methods for training deep networks (Adagrad, Adam, RMSProp, etc). The results are interesting but difficult to assess in a true apples-to-apples manner. Some specific comments: -What is the computational overhead of the actor-critic algorithm relative to other algorithms? No plots with the wall-time of optimization are presented, even though the success of methods like Adagrad was due to their wall-time performance, not the number of iterations. -Why was only a single learning rate learned? To accurately compare against other popular first order methods, why not train a separate RL model for each parameter, similar to how popular first-order methods adaptively change the learning rate for each parameter. -Since learning is a non-stationary process, while RL algorithms assume a stationary environment, why should we expect an RL algorithm to work for learning a learning rate? -In figure 6, how does the proposed method compare to something like early stopping? It may be that the actor-critic method is overfitting less simply because it is worse at optimization.","['4', '4', '5']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[15, 16, 14, 17]","[21, 22, 20, 23]","[145, 486, 167, 566]","[88, 240, 136, 280]","[9, 164, 5, 221]","[48, 82, 26, 65]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges that the results are 'interesting', they express several concerns and criticisms about the method and its evaluation. The reviewer points out that the results are 'difficult to assess in a true apples-to-apples manner' and raises multiple questions about the methodology and comparisons. However, the tone is not entirely negative, as the reviewer shows interest in the work.

The politeness score is moderately positive (50) because the reviewer maintains a professional and respectful tone throughout. They use neutral language like 'Some specific comments:' and phrase their criticisms as questions or suggestions rather than direct attacks. The reviewer also acknowledges positive aspects ('The results are interesting') before presenting their concerns. While not overly warm, the language is consistently polite and constructive.",-20,50
An Analysis of Feature Regularization for Low-shot Learning,Reject,2017,"['Zhuoyuan Chen', 'Han Zhao', 'Xiao Liu', 'Wei Xu']","[6, 6, 5]","['Marginally above acceptance threshold', 'Marginally above acceptance threshold', '5']","Summary === This paper extends and analyzes the gradient regularizer of Hariharan and Girshick 2016. In that paper a regularizer was proposed which penalizes gradient magnitudes and it was shown to aid low-shot learning performance. This work shows that the previous regularizer is equivalent to a direct penalty on the magnitude of feature values weighted differently per example. The analysis goes to to provide two examples where a feature penalty favors a better representation. The first example addresses the XOR problem, constructing a network where a feature penalty encourages a representation where XOR is linearly separable. The second example analyzes a 2 layer linear network, showing improved stability of a 2nd order optimizer when the feature penalty is added. One last bit of analysis shows how this regularizer can be interpreted as a Gaussian prior on both features and weights. Since the prior can be interpreted as having a soft whitening effect, the feature regularizer is like a soft version of Batch Normalization. Experiments show small improvements on a synthetic XOR test set. On the Omniglot dataset feature regularization is better than most baselines, but is worse than Moment Matching Networks. An experiment on ImageNet similar to Hariharan and Girshick 2016 also shows effective low-shot learning. Strengths === * The core proposal is a simple modification of Hariharan and Girshick 2016. * The idea of feature regularization is analyzed from multiple angles both theoretically and empirically. * The connection with Batch Normalization could have broader impact. Weaknesses === * In section 2 the gradient regularizer of Hariharan and Girshick is introduced. While introducing the concept, some concern is expressed about the motivation: *And it is not very clear why small gradients on every sample produces good generalization experimentally.* This seems to be the central issue to me. The paper details some related analysis, it does not offer a clear answer to this problem. * The purpose and generality of section 2.1 is not clear. The analysis provides a specific case (XOR with a non-standard architecture) where feature regularization intuitively helps learn a better representation. However, the intended take-away is not clear. The take-away may be that since a feature penalty helps in this case it should help in other cases. I am hesitant to buy that argument because of the specific architecture used in this section. The result seems to rely on the choice of an x^2 non-linearity, which is not often encountered in recent neural net literature. The point might also be to highlight the difference between a weight penalty and a feature penalty because the two seem to encourage different values of b in this case. However, there is no comparison to a weight penalty on b in section 2.1. * As far as I can tell, eq. 3 depends on either assuming an L2 or cross-entropy loss. A more general class of losses for which eq. 3 holds is not provided. This should be made clear before eq. 3 is presented. * The Omniglot and ImageNet experiments are performed with Batch Normalization, yet the paper points out that feature regularization may be similar in effect to Batch Norm. Since the ResNet CNN baseline includes Batch Norm and there are clear improvements over that baseline, the proposed regularizer has a clear additional positive effect. However, results should be provided without Batch Norm so a 1-1 comparison between the two methods can be performed. * The ImageNet experiment should be more like Hariharan and Girshick. In particular, the same split of classes should be used (provided in the appendix) and performance should be measured using n > 1 novel examples per class (using k nearest neighbors). Minor: * A brief comparison to Matching Networks is provided in section 3.2, but the performance of Matching Networks should also be reported in Table 1. * From the approach section: *Intuitively when close to convergence, about half of the data-cases recommend to update a parameter to go left, while the other half recommend to go right.* Could the intuition be clarified? There are many directions in high dimensional space and many ways to divide them into two groups. * Is the SGM penalty of Hariharan and Girshick implemented for this paper or using their code? Either is acceptable, but clarification would be appreciated. * Should the first equal sign in eq. 13 be proportional to, not equal to? * The work is dense in nature, but I think the presentation could be improved. In particular, more detailed derivations could be provided in an appendix and some details could be removed from the main version in order to increase focus on the results (e.g., the derviation in section 2.2.1). Overall Evaluation === This paper provides an interesting set of analyses, but their value is not clear. There is no clear reason why a gradient or feature regularizer should improve low-shot learning performance. Despite that, experiments support that conclusion, the analysis is interesting by itself, and the analysis may help lead to a clearer explanation. The work is a somewhat novel extension and analysis of Hariharan and Girshick 2016. Some points are not completely clear, as mentioned above.","['3', '3', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[10, 'no_match', 'no_match', 'no_match']","[15, 'no_match', 'no_match', 'no_match']","[27, 'no match', 'no match', 'no match']","[21, 'no match', 'no match', 'no match']","[6, 'no match', 'no match', 'no match']","[0, 'no match', 'no match', 'no match']","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some strengths of the paper, they also point out several significant weaknesses and areas needing improvement. The review begins with a neutral summary and lists both strengths and weaknesses, but the weaknesses section is more extensive and detailed. The reviewer expresses skepticism about some of the paper's claims and methodologies, indicating an overall slightly negative sentiment.

The politeness score is moderately positive (50) because the reviewer maintains a professional and respectful tone throughout. They use neutral language to describe the paper's content and findings, and frame criticisms as suggestions for improvement rather than harsh judgments. Phrases like 'Could the intuition be clarified?' and 'clarification would be appreciated' demonstrate a polite approach to requesting additional information. The reviewer also acknowledges the paper's strengths and interesting aspects, which contributes to the polite tone.",-20,50
CONTENT2VEC: SPECIALIZING JOINT REPRESENTATIONS OF PRODUCT IMAGES AND TEXT FOR THE TASK OF PRODUCT RECOMMENDATION,Reject,2017,"['Thomas Nedelec', 'Elena Smirnova', 'Flavian Vasile']","[3, 3, 5]","['Clear rejection', 'Clear rejection', '5']","The problem of utilizing all available information (across modalities) about a product to learn a meaningful *joint* embedding is an interesting one, and certainly seems like it a promising direction for improving recommender systems, especially in the *cold start* scenario. I*m unaware of approaches combining as many modalities as proposed in this paper, so an effective solution could indeed be significant. However, there are many aspects of the proposed architecture that seem sub-optimal to me: 1. A major benefit of neural-network based systems is that the entire system can be trained end-to-end, jointly. The proposed approach sticks together largely pre-trained modules for different modalities... this can be justifiable when there is very little training data available on which to train jointly. With 10M product pairs, however, this doesn*t seem to be the case for the Amazon dataset (although I haven*t worked with this dataset myself so perhaps I*m missing something... either way it*s not discussed at all in the paper). I consider the lack of a jointly fine-tuned model a major shortcoming of the proposed approach. 2. The discussion of *pairwise residual units* is confusing and not well-motivated. The residual formulation (if I understand it correctly) applies a ReLU layer to the concatenation of the modality specific embeddings, giving a new similarity (after dot products) that can be added to the similarity obtained from the concatenation directly. Why not just have an additional fully-connected layer that mixes the modality specific embeddings to form a final embedding (perhaps of lower dimensionality)? This should at least be presented as a baseline, if the pairwise residual unit is claimed as a contribution... I don*t find the provided explanation convincing (in what way does the residual approach reduce parameter count?). 3. More minor: The choice of TextCNN for the text embedding vectors seems fine (although I wonder how an LSTM-based approach would perform)... However the details surrounding how it is used are obscured in the paper. In response to a question, the authors mention that it runs on the concatenation of the first 10 words of the title and product description. Especially for the description, this seems insufficiently long to contain a lot of information to me. More care could be given to motivating the choices made in the paper. Finally, I*m not familiar with state of the art on this dataset... do the comparisons accurately reflect it? It seems only one competing technique is presented, with none on the more challenging cold-start scenarios. Minor detail: In the second paragraph of page 3, there is a reference that just says (cite Julian).","['3', '3', '3']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[1, 16, 12]","[6, 20, 17]","[19, 27, 50]","[7, 14, 24]","[10, 9, 26]","[2, 4, 0]","The sentiment score is -50 because while the reviewer acknowledges the interesting problem and potential significance of the approach, they express several major concerns about the proposed architecture. They use phrases like 'sub-optimal', 'major shortcoming', and 'confusing and not well-motivated' to describe aspects of the work. The overall tone suggests more criticism than praise.

The politeness score is 50 because the reviewer maintains a professional and respectful tone throughout. They use phrases like 'seems fine', 'I wonder', and 'More care could be given' instead of harsh or dismissive language. They also acknowledge potential misunderstandings on their part. However, the review doesn't go out of its way to be overly polite or complimentary, maintaining a neutral-to-slightly-positive tone in terms of politeness.",-50,50
Cat2Vec: Learning Distributed Representation of Multi-field Categorical Data,Reject,2017,"['Ying Wen', 'Jun Wang', 'Tianyao Chen', 'Weinan Zhang']","[4, 5, 4]","['Ok but not good enough - rejection', '5', 'Ok but not good enough - rejection']","A method for click prediction is presented. Inputs are a categorical variables and output is the click-through-rate. The categorical input data is embedded into a feature vector using a discriminative scheme that tries to predict whether a sample is fake or not. The embedding vector is passed through a series of SUM/MULT gates and K-most important interactions are identified (K-max pooling). This process is repeated multiple times (i.e. multiple layers) and the final feature is passed into a fully connected layer to output the click prediction rate. Authors claim: (1) Use of gates and K-max pooling allow modeling of interactions that lead to state of art results. (2) It is not straightforward to apply ideas in papers like word2vec to obtain feature embeddings and consequently they use the idea of discriminating between fake and true samples for feature learning. Theoretically convolutions can act as “sum” gates between pairs of input dimensions. Authors make these interactions explicit (i.e. imposed structure) by using gates. Now, the merit of the proposed method can be tested if a network using gates outperforms a network without gates. This baseline is critically missing – i.e. Embedding Vector followed by a series of convolution/pooling layers. Another related issue is that I am not sure if the number of parameters in the proposed model and the baseline models is similar or not. For instance – what is the total number of parameters in the CCPM model v/s the proposed model? Overall, there is no new idea in the paper. This by itself is not grounds for rejection if the paper outperforms established baselines. However, such comparison is weak and I encourage authors to perform these comparisons.","['4', '5', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[2, 15, 3, 7]","[8, 21, 7, 13]","[53, 359, 15, 375]","[21, 163, 7, 185]","[29, 155, 4, 163]","[3, 41, 4, 27]","The sentiment score is -50 because the reviewer expresses several criticisms and concerns about the paper, particularly regarding missing baselines and lack of new ideas. However, they do not completely dismiss the work, suggesting improvements and encouraging further comparisons. The politeness score is 0 (neutral) as the reviewer maintains a professional tone without being overly polite or rude. They provide constructive criticism and use phrases like 'I encourage authors to perform these comparisons' which is neither particularly polite nor impolite. The review focuses on the technical aspects of the paper without personal comments or overly harsh language.",-50,0
Character-aware Attention Residual Network for Sentence Representation,Reject,2017,"['Xin Zheng', 'Zhenzhou Wu']","[4, 4, 4]","['Ok but not good enough - rejection', 'Ok but not good enough - rejection', 'Ok but not good enough - rejection']","This paper proposes a new model for sentence classification. Pros: - Some interesting architecture choices in the network. Cons: - No evaluation of the architecture choices. An ablation study is critical here to understand what is important and what is not. - No evaluation on standard datasets. On the only pre-existing dataset evaluated on a simple TFIDF-SVM method is state-of-the-art, so results are unconvincing.","['4', '4', '5']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[17, 5]","[23, 11]","[206, 16]","[105, 5]","[20, 8]","[81, 3]","The sentiment score is -50 because while the reviewer acknowledges some 'interesting architecture choices' (a positive aspect), they express significant concerns about the lack of evaluation and unconvincing results. The cons outweigh the pros, indicating a generally negative sentiment. The politeness score is 0 (neutral) because the reviewer uses direct, matter-of-fact language without being particularly polite or rude. They state their observations and criticisms plainly without using overly harsh language or softening their critique with polite phrases.",-50,0
Chess Game Concepts Emerge under Weak Supervision: A Case Study of Tic-tac-toe,Reject,2017,"['Hao Zhao', 'Ming Lu', 'Anbang Yao', 'Yurong Chen', 'Li Zhang']","[3, 3, 3]","['Clear rejection', 'Clear rejection', 'Clear rejection']","Game of tic-tac-toe is considered. 1029 tic-tac-toe board combinations are chosen so that a single move will result into victory of either the black or the white player. There are 18 possible moves - 2 players x 9 locations. A CNN is trained from a visual rendering of the game board to these 18 possible outputs. CAM technique is used to visualize the salient regions in the inputs responsible for the prediction that CNN makes. Authors find that predictions correspond to the winning board locations. Authors claim that this: 1. is a very interesting finding. 2. CNN has figured out game rules. 3. Cross modal supervision is applicable to higher-level semantics. I don*t think (2) be can be claimed because the knowledge of game rules is not tested by any experiment. There is only *one* stage of a game - i.e. last move that is considered. Further, the results are on the training set itself - the bare minimum requirement of any implicit or explicit representation of game rules is the ability to act in previously unseen states (i.e. generalization). Even if the CNN did generalize, I would avoid making any claims about knowledge of game rules. For (3), author*s definition of cross-modal seems to be training from images to games moves. In image-classification we go from images --> labels (i.e. between two different domains). We already know CNNs can perform such mappings. CNNs have been used to map images to actions such as in DQN my Mnih et al., or DDPG by Lillicrap et al. and a lot of other classical work such as ALVIN. It*s unclear what points authors are trying to make. For (1): how interesting is an implicit attention mechanism is a subjective matter. The authors claim a difference between the concepts of *what do do* and *what will happen*. They claim by supervising for *what will happen*, the CNN can automatically learn about *what to do*. This is extensively studied in the model predictive control literature. Where model is *what will happen next*, and the model is used to infer a control law - *what to do*. However, in the experimental setup presented in the paper what will happen and what to do seem to be the exact same things. For further analysis of what the CNN has learnt I would recommend: (a) Visualizing CAM with respect to incorrect classes. For eg, visualize the CAM with respect to player would lose (instead of winning). (b) Split the data into train/val and use the predictions on the val-set for visualization. These would be much more informative about what kind of *generalizable* features the CNN pays attention to. In summary, understanding why CNN*s make what decisions they make is a very interesting area of research. While the emergence of an implicit attention mechanism may be considered to be an interesting finding by some, many claims made by the authors are not supported by experiments (see comments above).","['5', '3', '2']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']","['no_match', 26, 10, 17]","['no_match', 32, 16, 23]","['no match', 202, 65, 17]","['no match', 89, 32, 7]","['no match', 30, 25, 2]","['no match', 83, 8, 8]","The sentiment score is -50 because the reviewer expresses significant skepticism about the authors' claims and findings. They disagree with several key points and suggest that some claims are not supported by the experiments. However, it's not entirely negative as they acknowledge that the research area is interesting. The politeness score is 20 because while the reviewer is critical, they maintain a professional tone throughout. They use phrases like 'I don't think' and 'I would recommend' rather than harsh language. They also provide constructive suggestions for improvement, which is a polite approach to criticism in academic reviews.",-50,20
Coarse Pruning of Convolutional Neural Networks with Random Masks,Reject,2017,"['Sajid Anwar', 'Wonyong Sung']","[5, 4, 6]","['5', 'Ok but not good enough - rejection', 'Marginally above acceptance threshold']","This paper proposes a simple randomized algorithm for selecting which weights in a ConvNet to prune in order to reduce theoretical FLOPs when evaluating a deep neural network. The paper provides a nice taxonomy or pruning granularity from coarse (layer-wise) to fine (intra-kernel). The pruning strategy is empirically driven and uses a validation set to select the best model from N randomly pruned models. Makes claims in the intro about this being *one shot* and *near optimal* that cannot be supported: it is *N-shot* in the sense that N networks are generated and tested and there is no evidence or theory that the found solution is *near optimal.* Pros: - Nice taxonomy of pruning levels - Comparison to the recent weight-sum pruning method Cons: - Experimental evaluation does not touch upon recent models (ResNets) and large scale datasets (ImageNet) - Paper is somewhat hard to follow - Feature map pruning can obviously accelerate computation without specialized sparse implementations of convolution, but this is not the case for finer grained sparsity; since this paper considers fine-grained sparsity it should provide some evidence that introducing that sparsity can yield performance improvements Another experimental downside is that the paper does not evaluate the impact of filter pruning on transfer learning. For example, there is not much direct interest in the tasks of MNIST, CIFAR10, or even ImageNet. Instead, a main interest in both academia and industry is the value of the learned representation for transferring to other tasks. One might expect pruning to harm transfer learning. It*s possible that the while the main task has about the same performance, transfer learning is strongly hurt. This paper has missed an opportunity to explore that direction. In summary, the proposed method is simple, which is good, but the experimental evaluation is somewhat incomplete and does not cover recent models and larger scale datasets.","['4', '4', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[8, 38]","[14, 44]","[51, 191]","[15, 114]","[2, 31]","[34, 46]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('nice taxonomy', 'simple, which is good'), they also point out several significant limitations and missed opportunities. The review highlights that some claims in the introduction are not supported, the experimental evaluation is incomplete, and the paper doesn't cover recent models or larger datasets. The politeness score is moderately positive (50) as the reviewer maintains a professional tone throughout, balancing criticism with acknowledgment of positive aspects. They use phrases like 'nice taxonomy' and 'simple, which is good' to soften criticism, and present their concerns as observations rather than harsh judgments. The reviewer also provides constructive suggestions for improvement, which contributes to the polite tone.",-20,50
Collaborative Deep Embedding via Dual Networks,Reject,2017,"['Yilei Xiong', 'Dahua Lin', 'Haoying Niu', 'JIefeng Cheng', 'Zhenguo Li']","[5, 5, 4]","['5', '5', 'Ok but not good enough - rejection']","The authors proposed to learn embeddings of users and items by using deep neural network for a recommendation task. The resulting method has only minor differences from the previous CDL, in which neural networks were also used for recommendation tasks. In the experiments, since the proposed method, DualNets have use more item features than WMF and CDL, the comparisons are unfair.","['3', '4', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[0, 13, 'no_match', 15, 'no_match']","[3, 19, 'no_match', 20, 'no_match']","[4, 308, 'no match', 33, 'no match']","[2, 146, 'no match', 22, 'no match']","[2, 147, 'no match', 2, 'no match']","[0, 15, 'no match', 9, 'no match']","The sentiment score is -50 because the review is generally negative, pointing out that the proposed method has only minor differences from previous work and that the comparisons in the experiments are unfair. However, it's not extremely negative, as it doesn't completely dismiss the work. The politeness score is 0 (neutral) because the language used is direct and matter-of-fact, without being particularly polite or rude. The reviewer states their criticisms plainly without using overly harsh language or personal attacks, but also without any softening phrases or compliments that would indicate politeness.",-50,0
Communicating Hierarchical Neural Controllers for Learning Zero-shot Task Generalization,Reject,2017,"['Junhyuk Oh', 'Satinder Singh', 'Honglak Lee', 'Pushmeet Kohli']","[4, 5, 3]","['Ok but not good enough - rejection', '5', 'Clear rejection']","Description: This paper presents a reinforcement learning architecture where, based on *natural-language* input, a meta-controller chooses subtasks and communicates them to a subtask controller that choose primitive actions, based on the communicated subtask. The goal is to scale up reinforcement learning agents to large-scale tasks. The subtask controller embeds the subtask definition (arguments) into vectors by a multi-layer perceptron including an *analogy-making* regularization. The subtask vectors are combined with inputs at each layer of a CNN. CNN outputs (given the observation and the subtask) are then fed to one of two MLPs; one to compute action probabilities in the policy (exponential falloff of MLP outputs) and the other to compute termination probability (sigmoid from MLP outputs). The meta controller takes a list of sentences as instructions embeds them into a sequence of subtask arguments (not necessarily a one-to-one mapping). A context vector is computed by a CNN from the observation, the previous sentence embedding, the previous subtask and its completion state. The subtask arguments are computed from the context vector through further mechanisms involving instruction retrieval from memory pointers, and hard/soft decisions whether to update the subtask or not. Training involves policy distillation+actor-critic training for the subtask controller, and actor-critic training for the meta controller keeping the subtask controller frozen. The system is tested in a grid world where the agent moves and interacts with (picks up/transforms) various item/enemy types. It is compared to a) a flat controller not using a subtask controller, and b) subtask control by mere concatenation of the subtask embedding to the input with/without the analogy-making regularization. Evaluation: The proposed architecture seems reasonable, although it is not clear why the specific way of combining subtask embeddings in the subtask controller would be the *right* way to do it. I do not feel the grid world here really represents a *large-scale task*: in particular the 10x10 size of the grid is very small. This is disappointing since this was a main motivation of the work. Moreover, the method is not compared to any state of the art alternatives. This is especially problematic because the test is not on established benchmarks. It is not really possible, based on the shown results, to put the performance in context of other works.","['3', '5', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[3, 27, 13, 15]","[9, 33, 19, 21]","[42, 291, 273, 322]","[18, 189, 145, 180]","[23, 72, 123, 102]","[1, 30, 5, 40]","The sentiment score is -50 because the review expresses several criticisms and disappointments with the paper. The reviewer states that the grid world doesn't represent a large-scale task as claimed, which was a main motivation of the work. They also point out the lack of comparison to state-of-the-art alternatives and established benchmarks, making it difficult to contextualize the performance. However, the score isn't lower because the reviewer does acknowledge that the proposed architecture seems reasonable.

The politeness score is 20 because the reviewer maintains a professional and objective tone throughout. They use phrases like 'It is not clear why...' and 'I do not feel...' instead of more aggressive language. The criticism is presented in a constructive manner, focusing on the work rather than attacking the authors. However, the score isn't higher because the review doesn't include many explicitly polite phrases or compliments, maintaining a mostly neutral tone.",-50,20
Convolutional Neural Networks Generalization Utilizing the Data Graph Structure,Reject,2017,"['Yotam Hechtlinger', 'Purvasha Chakravarti', 'Jining Qin']","[6, 3]","['Marginally above acceptance threshold', 'Clear rejection']","Update: I thank the authors for their comments! After reading them, I decided to increase the rating. This paper proposes a variant of the convolution operation suitable for a broad class of graph structures. For each node in the graph, a set of neighbours is devised by means of random walk (the neighbours are ordered by the expected number of visits). As a result, the graph is transformed into a feature matrix resembling MATLAB’s/Caffe’s im2col output. The convolution itself becomes a matrix multiplication. Although the proposed convolution variant seems reasonable, I’m not convinced by the empirical evaluation. The MNIST experiment looks especially suspicious. I don’t think that this dataset is appropriate for the demonstration purposes in this case. In order to make their method applicable to the data, the authors remove important structural information (relative locations of pixels) thus artificially increasing the difficulty of the task. At the same time, they are comparing their approach with regular CNNs and conclude that the former performs poorly (and does not even reach an acceptable accuracy for the particular dataset). I guess, to justify the presence of MNIST (or similar datasets) in the experimental section, the authors should modify their method to incorporate additional graph structure (e.g. relative locations of nodes) in cases when the relation between nodes cannot be fully described by a similarity matrix. I believe, in its current form, the paper is not yet ready for publication but may be later resubmitted to a workshop or another conference after the concern above is addressed.","['3', '3']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[2, 4, 1]","[3, 4, 1]","[3, 2, 1]","[0, 0, 0]","[3, 1, 1]","[0, 1, 0]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects of the paper ('reasonable' approach, thanks authors for comments), they express significant concerns about the empirical evaluation and conclude that the paper is not ready for publication. The overall tone suggests the paper needs substantial improvements. The politeness score is moderately positive (60) as the reviewer uses polite language throughout, starting with a thank you, using phrases like 'I'm not convinced' instead of more harsh criticism, and offering constructive suggestions for improvement. The reviewer maintains a professional and respectful tone even while expressing concerns.",-20,60
Deep Character-Level Neural Machine Translation By Learning Morphology,Reject,2017,"['Shenjian Zhao', 'Zhihua Zhang']","[6, 5, 7]","['Marginally above acceptance threshold', '5', 'Good paper, accept']","* Summary: This paper proposes a neural machine translation model that translates the source and the target texts in an end to end manner from characters to characters. The model can learn morphology in the encoder and in the decoder the authors use a hierarchical decoder. Authors provide very compelling results on various bilingual corpora for different language pairs. The paper is well-written, the results are competitive compared to other baselines in the literature. * Review: - I think the paper is very well written, I like the analysis presented in this paper. It is clean and precise. - The idea of using hierarchical decoders have been explored before, e.g. [1]. Can you cite those papers? - This paper is mainly an application paper and it is mainly the application of several existing components on the character-level NMT tasks. In this sense, it is good that authors made their codes available online. However, the contributions from the general ML point of view is still limited. * Some Requests: -Can you add the size of the models to the Table 1? - Can you add some of the failure cases of your model, where the model failed to translate correctly? * An Overview of the Review: Pros: - The paper is well written - Extensive analysis of the model on various language pairs - Convincing experimental results. Cons: - The model is complicated. - Mainly an architecture engineering/application paper(bringing together various well-known techniques), not much novelty. - The proposed model is potentially slower than the regular models since it needs to operate over the characters instead of the words and uses several RNNs. [1] Serban IV, Sordoni A, Bengio Y, Courville A, Pineau J. Hierarchical neural network generative models for movie dialogues. arXiv preprint arXiv:1507.04808. 2015 Jul 17.","['4', '5', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[3, 19]","[5, 25]","[6, 279]","[3, 112]","[3, 68]","[0, 99]","The sentiment score is 60 (positive) because the reviewer expresses a generally positive view of the paper, praising its writing, analysis, and results. They use phrases like 'very well written,' 'compelling results,' and 'competitive compared to other baselines.' However, they also note some limitations, which prevents a higher score. The politeness score is 80 (quite polite) due to the reviewer's constructive and respectful tone throughout. They balance praise with gentle criticism and suggestions, using phrases like 'Can you...' for requests rather than demands. The reviewer also acknowledges the paper's strengths before mentioning its weaknesses, which is a polite approach to feedback.",60,80
Deep Neural Networks and the Tree of Life,Reject,2017,"['Yan Wang', 'Kun He', 'John E. Hopcroft', 'Yu Sun']","[3, 4, 4]","['Clear rejection', 'Ok but not good enough - rejection', 'Ok but not good enough - rejection']","The paper presents a simple method for constructing a visual hierarchy of ImageNet classes based on a CNN trained on discriminate between the classes. It investigates two metrics for measuring inter-class similarity: (1) softmax probability outputs, i.e., the class confusion matrix, and (2) L2 distance between fc7 features, along with three methods for constructing the hierarchy given the distance matrix: (1) approximation central point, (2) minimal spanning tree, and (3) multidimensional scaling of Borg&Groenen 2005. There are two claimed contributions: (1) Constructs a biology evolutionary tree, and (2) Gives insight into the representations produced by deep networks. Regarding (1), while the motivation of the work is grounded in biology, in practice the method is based only on visual similarity. The constructed trees thus can’t be expected to reflect the evolutionary hierarchy, and in fact there are no quantitative experiments that demonstrate that they do. Regarding (2), the technical depth of the exploration is not sufficient for ICLR. I’m not sure what we can conclude from the paper beyond the fact that CNNs are able to group categories together based on visual similarities, and deeper networks are able to do this better than more shallow networks (Fig 2). In summary, this paper is unfortunately not ready for publication at this time.","['5', '4', '4']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[12, 12, 53, 22]","[18, 18, 59, 28]","[40, 189, 184, 353]","[16, 49, 78, 156]","[0, 85, 36, 59]","[24, 55, 70, 138]","The sentiment score is -70 because the review is largely negative. The reviewer states that the paper is 'not ready for publication at this time' and questions the depth and significance of the contributions. They express doubt about the paper's ability to meet its claimed objectives. However, it's not entirely negative as they do acknowledge some aspects of the work, hence not a lower score. The politeness score is 20 because while the reviewer is critical, they maintain a professional and respectful tone throughout. They use phrases like 'unfortunately not ready' rather than harsh or dismissive language. The reviewer also provides specific reasons for their assessment, which is a polite way to give feedback. However, the overall critical nature of the review prevents a higher politeness score.",-70,20
DeepRebirth: A General Approach for Accelerating Deep Neural Network Execution on Mobile Devices,Reject,2017,"['Dawei Li', 'Xiaolong Wang', 'Deguang Kong', 'Mooi Choo Chuah']","[4, 4, 4]","['Ok but not good enough - rejection', 'Ok but not good enough - rejection', 'Ok but not good enough - rejection']","One of the main idea of this paper is to replace pooling layers with convolutions of stride 2 and retraining the model. Authors merge this into a new layer and brand it as a new type of layer. This is very misleading and adding noise to the field. And using strided convolutions rather than pooling is not actually novel (e.g. https://arxiv.org/abs/1605.02346). While the speed-up obtained are good, the lack of novelty and the rebranding attempt make this paper not a good fit for ICLR.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[7, 6, 11, 28]","[12, 10, 17, 34]","[32, 38, 51, 117]","[21, 27, 40, 86]","[8, 7, 7, 4]","[3, 4, 4, 27]","The sentiment score is -80 because the reviewer expresses strong disapproval of the paper's main idea, calling it 'very misleading' and 'adding noise to the field.' They also state that the paper lacks novelty and is not a good fit for the conference, which are highly negative assessments. The politeness score is -20 because while the language is not overtly rude, it is quite blunt and dismissive. The reviewer doesn't use any softening language or acknowledge potential merits, instead directly criticizing the authors' approach as misleading and unoriginal. The use of phrases like 'very misleading' and 'rebranding attempt' come across as somewhat accusatory, contributing to the slightly impolite tone.",-80,-20
Differentiable Canonical Correlation Analysis,Reject,2017,"['Matthias Dorfer', 'Jan Schlüter', 'Gerhard Widmer']","[3, 4, 3]","['Clear rejection', 'Ok but not good enough - rejection', 'Clear rejection']","It is not clear to me at all what this paper is contributing. Deep CCA (Andrew et al, 2013) already gives the gradient derivation of the correlation objective with respect to the network outputs which are then back-propagated to update the network weights. Again, the paper gives the gradient of the correlation (i.e. the CCA objective) w.r.t. the network outputs, so it is confusing to me when authors say that their differentiable version enables them to back-propagate directly through the computation of CCA.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[6, 10, 33]","[12, 15, 39]","[50, 44, 350]","[25, 31, 201]","[19, 9, 100]","[6, 4, 49]","The sentiment score is -70 because the reviewer expresses strong confusion and skepticism about the paper's contribution. They state 'It is not clear to me at all what this paper is contributing' and point out that the work seems redundant with existing research. This indicates a significantly negative view of the paper's value. The politeness score is -20 because while the language isn't overtly rude, it's quite direct and dismissive. The reviewer doesn't use any softening language or acknowledge potential misunderstandings on their part. Phrases like 'It is not clear to me at all' and 'it is confusing to me' are somewhat blunt in an academic context, though not extremely impolite.",-70,-20
Distributed Transfer Learning for Deep Convolutional Neural Networks by Basic Probability Assignment,Reject,2017,['Arash Shahriari'],"[3, 3, 4]","['Clear rejection', 'Clear rejection', 'Ok but not good enough - rejection']","This work proposes to use basic probability assignment to improve deep transfer learning. A particular re-weighting scheme inspired by Dempster-Shaffer and exploiting the confusion matrix of the source task is introduced. The authors also suggest learning the convolutional filters separately to break non-convexity. The main problem with this paper is the writing. There are many typos, and the presentation is not clear. For example, the way the training set for weak classifiers are constructed remains unclear to me despite the author*s previous answer. I do not buy the explanation about the use of both training and validation sets to compute BPA. Also, I am not convinced non-convexity is a problem here and the author does not provide an ablation study to validate the necessity of separately learning the filters. One last question is CIFAR has three channels and MNIST only one: How it this handled when pairing the datasets in the second set of experiments? Overall, I believe the proposed idea of reweighing is interesting, but the work can be globally improved/clarified. I suggest a reject.","['4', '3', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']",[4],[4],[8],[4],[4],[0],"The sentiment score is -50 because while the reviewer acknowledges that the proposed idea is interesting, they suggest rejecting the paper due to numerous issues with writing clarity, presentation, and unconvincing explanations. The overall tone is critical, but not entirely negative. The politeness score is 20 because the reviewer uses relatively neutral language and offers constructive criticism. They acknowledge positive aspects ('interesting' idea) and explain their concerns without using harsh or rude language. The reviewer also uses phrases like 'I suggest' and 'I believe' which soften the critique. However, the directness of the criticism and the recommendation to reject prevent a higher politeness score.",-50,20
Dynamic Steerable Frame Networks,Reject,2017,"['Jörn-Henrik Jacobsen', 'Bert De Brabandere and Arnold W.M. Smeulders']","[5, 4, 7]","['5', 'Ok but not good enough - rejection', 'Good paper, accept']","This paper presents an improved formulation of CNN, aiming to separate geometric transformation from inherent features. The network can estimate the transformation of filters given the input images. This work is based on a solid technical foundation and is motivated by a plausible rationale. Yet, the value of this work in practice is subject to questions: (1) It relies on the assumption that the input image is subject to a transformation on a certain Lie group (locally). Do such transformations constitute real challenges in practice? State-of-the-art CNNs, e.g. ResNet, are already quite resilient to such local deformations. What such components would add to the state of the art? Limited experiments on Cifar-10 does not seem to provide a very strong argument. (2) The computational cost is not discussed.","['4', '3', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[2, 'no_match']","[8, 'no_match']","[40, 'no match']","[17, 'no match']","[21, 'no match']","[2, 'no match']","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the solid technical foundation and plausible rationale of the work, they express significant doubts about its practical value and the strength of the experimental evidence. The reviewer raises questions about the real-world applicability of the approach and the lack of discussion on computational costs, which contribute to the overall negative sentiment. However, the score is not deeply negative as the reviewer does recognize some positive aspects of the work. The politeness score is moderately positive (50) because the reviewer uses respectful and professional language throughout. They present their criticisms as questions and observations rather than harsh statements, and they acknowledge the positive aspects of the work before presenting their concerns. The reviewer maintains a neutral, academic tone without using any rude or overly critical language.",-20,50
Efficient Calculation of Polynomial Features on Sparse Matrices,Reject,2017,"['Andrew Nystrom', 'John Hughes']","[3, 3, 3]","['Clear rejection', 'Clear rejection', 'Clear rejection']","The authors present here a new algorithm for the effective calculation of polynomial features on Sparse Matrices. The key idea is to use a proper mapping between matrices and their polynomial versions, in order to derive an effective CSR expansion algorithm. The authors analyse the time complexity in a convincing way with experiments. Overall, the algorithm is definitely interesting, quite simple and nice, with many possible applications. The paper is however very superficial in terms of experiments, or applications of the proposed scheme. Most importantly, the fit with the main scope of ICLR is far from obvious with this work, that should probably re-submitted to better targets.","['3', '3', '1']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer*s evaluation is an educated guess']","[0, 36]","[5, 39]","[3, 21]","[1, 14]","[2, 1]","[0, 6]","The sentiment score is slightly positive (20) because the reviewer acknowledges the algorithm as 'interesting, quite simple and nice, with many possible applications' and praises the time complexity analysis. However, the positive aspects are balanced by criticisms of the paper being 'very superficial in terms of experiments' and not fitting well with the conference scope. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, acknowledging the authors' work positively where appropriate, and expressing criticisms in a constructive manner without harsh or dismissive language. The reviewer maintains a professional tone, balancing praise with areas for improvement.",20,50
Eigenvalues of the Hessian in Deep Learning: Singularity and Beyond,Reject,2017,"['Levent Sagun', 'Leon Bottou', 'Yann LeCun']","[3, 4, 4, 4]","['Clear rejection', 'Ok but not good enough - rejection', 'Ok but not good enough - rejection', 'Ok but not good enough - rejection']","This paper investigates the hessian of small deep networks near the end of training. The main result is that many eigenvalues are approximately zero, such that the Hessian is highly singular, which means that a wide amount of theory does not apply. The overall point that deep learning algorithms are singular, and that this undercuts many theoretical results, is important but it has already been made: Watanabe. “Almost All Learning Machines are Singular”, FOCI 2007. This is one paper in a growing body of work investigating this phenomenon. In general, the references for this paper could be fleshed out much further—a variety of prior work has examined the Hessian in deep learning, e.g., Dauphin et al. “Identifying and attacking the saddle point problem in high dimensional non-convex optimization” NIPS 2014 or the work of Amari and others. Experimentally, it is hard to tell how results from the small sized networks considered here might translate to much larger networks. It seems likely that the behavior for much larger networks would be different. A reason for optimism, though, is the fact that a clear bulk/outlier behavior emerges even in these networks. Characterizing this behavior for simple systems is valuable. Overall, the results feel preliminary but likely to be of interest when further fleshed out. This paper is attacking an important problem, but should do a better job situating itself in the related literature and undertaking experiments of sufficient size to reveal large-scale behavior relevant to practice.","['4', '5', '5', '5']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[3, 29, 30]","[8, 35, 36]","[35, 126, 315]","[12, 64, 162]","[23, 37, 113]","[0, 25, 40]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the importance of the topic and some positive aspects ('clear bulk/outlier behavior emerges', 'Characterizing this behavior for simple systems is valuable'), they also point out several limitations and areas for improvement. The review suggests the results are 'preliminary' and the paper needs to better situate itself in existing literature and conduct more extensive experiments. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, acknowledging the paper's potential value while offering constructive criticism. They use phrases like 'A reason for optimism' and 'Overall, the results feel preliminary but likely to be of interest when further fleshed out', which maintain a polite and encouraging tone even while pointing out areas for improvement.",-20,50
Emergent Predication Structure in Vector Representations of Neural Readers,Reject,2017,"['Hai Wang', 'Takeshi Onishi', 'Kevin Gimpel', 'David McAllester']","[6, 6, 5]","['Marginally above acceptance threshold', 'Marginally above acceptance threshold', '5']","The paper aims to consolidate some recent literature in simple types of *reading comprehension* tasks involving matching questions to answers to be found in a passage, and then to explore the types of structure learned by these models and propose modifications. These reading comprehension datasets such as CNN/Daily Mail are on the simpler side because they do not generally involve chains of reasoning over multiple pieces of supporting evidence as can be found in datasets like MCTest. Many models have been proposed for this task, and the paper breaks down these models into *aggregation readers* and *explicit reference readers.* The authors show that the aggregation readers organize their hidden states into a predicate structure which allows them to mimic the explicit reference readers. The authors then experiment with adding linguistic features, including reference features, to the existing models to improve performance. I appreciate the re-naming and re-writing of the paper to make it more clear that the aggregation readers are specifically learning a predicate structure, as well as the inclusion of results about dimensionality of the symbol space. Further, I think the effort to organize and categorize several different reading comprehension models into broader classes is useful, as the field has been producing many such models and the landscape is unclear. The concerns with this paper are that the predicate structure demonstrated is fairly simple, and it is not clear that it provides insight towards the development of better models in the future, since the *explicit reference readers* need not learn it, and the CNN/Daily Mail dataset has very little headroom left as demonstrated by Chen et al. 2016. The desire for *dramatic improvements in performance* mentioned in the discussion section probably cannot be achieved on these datasets. More complex datasets would probably involve multi-hop inference which this paper does not discuss. Further, the message of the paper is a bit scattered and hard to parse, and could benefit from a bit more focus. I think that with the explosion of various competing neural network models for NLP tasks, contributions like this one which attempt to organize and analyze the landscape are valuable, but that this paper might be better suited for an NLP conference or journal such as TACL.","['5', '4', '3']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[3, 26, 10, 5]","[5, 29, 16, 11]","[11, 12, 161, 17]","[6, 8, 90, 7]","[5, 4, 66, 10]","[0, 0, 5, 0]","The sentiment score is 50 (slightly positive) because the reviewer appreciates certain aspects of the paper, such as the reorganization of content, inclusion of new results, and the effort to categorize reading comprehension models. However, they also express concerns about the simplicity of the demonstrated predicate structure and the paper's scattered message. The politeness score is 75 (quite polite) because the reviewer uses respectful language throughout, acknowledging the paper's contributions while offering constructive criticism. They use phrases like 'I appreciate' and 'I think' to soften their critiques, and provide specific suggestions for improvement without being harsh or dismissive.",50,75
End-to-End Answer Chunk Extraction and Ranking for Reading Comprehension,Reject,2017,"['Yang Yu', 'Wei Zhang', 'Bowen Zhou', 'Kazi Hasan', 'Mo Yu', 'Bing Xiang']","[4, 5, 6]","['Ok but not good enough - rejection', '5', 'Marginally above acceptance threshold']","SYNOPSIS: The paper proposes a new neural network-based model for reading comprehension (reading a passage of text and answering questions based on the passage). It is similar in spirit to several other recent models, with the main exception that it is able to predict answers of different lengths, as opposed to single words/tokens/entities. The authors compare their model on the Stanford Question Answering Dataset (SQuAD), and show improvements over the baselines, while apparently lagging quite far behind the current state of the art reported on the SQuAD leaderboard. THOUGHTS: The main novelty of the method is to be able to identify phrases of different lengths as possible answers to the question. However, both approaches considered -- using a POS pattern trie tree to filter out word sequences with POS tags matching those of answers in the training set, and brute-force enumeration of all phrases up to length N -- seem somewhat orthogonal to the idea of *learning end-to-end * an answer chunk extraction model. Furthermore, as other reviews have pointed out, it seems that the linguistic features actually contribute a lot to the final accuracy (Table 3). One could argue that these are easy to obtain using standard taggers, but it takes away even more from the idea of an *end-to-end trained* system. The paper is generally well written, but there are several crucial sections in parts describing the model where it was really hard for me to follow the descriptions. In particular, the attention mechanism seems fairly standard to me in a seq2seq sense (i.e. there is nothing architecturally novel about it, as is for instance the case with the Gated Attentive Reader). I may be missing something, but even after the clarification round I still don*t understand how it is novel compared to standard attention used in for instance seq2seq models. Finally, although the method is shown to outperform the baseline method reported in the original paper introducing the SQuAD dataset, it currently seems to be 12th (out of 15 systems) on the leaderboard (https://rajpurkar.github.io/SQuAD-explorer/). Of course, it may be that further training and hyperparameter optimizations may improve these results. Therefore, given the lack of model novelty (based on my understanding), and the lack of strong results (based on the leaderboard), I don*t feel the paper is ready in its current form to be accepted to the conference. Note: The GRU citation should be (Cho et al., 2014), not (Bengio et al., 2015).","['3', '3', '3']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","['no_match', 'no_match', 18, -2, 9, 19]","['no_match', 'no_match', 24, 1, 15, 25]","['no match', 'no match', 250, 1, 187, 154]","['no match', 'no match', 136, 1, 86, 84]","['no match', 'no match', 73, 0, 93, 65]","['no match', 'no match', 41, 0, 8, 5]","The sentiment score is -60 because the reviewer expresses significant concerns about the novelty and effectiveness of the proposed model, and ultimately recommends against accepting the paper. They point out several weaknesses and limitations, though they do acknowledge some positive aspects like the paper being generally well-written. The politeness score is 20 because the reviewer uses professional and respectful language throughout, offering constructive criticism and explaining their reasoning. They avoid harsh or dismissive phrasing, instead using more neutral terms like 'I don't feel the paper is ready' rather than outright rejecting it. The reviewer also acknowledges potential for improvement with further work.",-60,20
End-to-End Learnable Histogram Filters,Reject,2017,"['Rico Jonschkowski', 'Oliver Brock']","[4, 4, 3]","['Ok but not good enough - rejection', 'Ok but not good enough - rejection', 'Clear rejection']","Summary: This paper presents a differentiable histogram filter for state estimation/tracking. The proposed histogram filter is a particular Bayesian filter that represents the discretized states using beliefs. The prediction step is parameterized by a locally linear and translation-invariant motion model while the measurement model is represented by a multi-layered neural network. The whole system is learned with both supervised and unsupervised objectives and experiments are carried out on two synthetic robot localization tasks (1D and 2D). The major claim of this paper is that the problem-specific model structure (Bayesian filter for state estimation) should improve pure deep learning approach in data-efficiency and generalization ability. +This paper has nice arguments about the importance of prior knowledge to deep learning approach for specific tasks. +An end-to-end histogram filter is derived for state estimation and unsupervised learning is possible in this model. -This paper seems to have a hidden assumption that deep learning (RNN) is a natural choice for recursive state estimation and the rest of paper is built upon this assumption including LSTM baselines. However, this assumption itself may not be true, because Bayesian filter is a first-established approach for this classic problem, so it it more important to justify if deep learning is even necessary for solving the tasks presented. This requests pure Bayesian filter baselines in the experiments. -The derived histogram filter seems to be particularly designed for discretized state space. It is not clear how well it can be generalized to continuous state space using the notation *x*. More interestingly, the observation is discrete (binary) as well, which eventually makes it possible to derive a closed-form measurement update model. This setup might be too constrained. Generalizing to continuous observations is not a trivial task, not even to mention using images as observations like Haarnoja et al 2016. These design choices overall narrow down the scope of applicability.","['3', '3', '3']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[4, 20]","[9, 25]","[34, 132]","[14, 79]","[16, 17]","[4, 36]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects of the paper (e.g., 'nice arguments', 'end-to-end histogram filter'), they also raise significant concerns and limitations. The reviewer questions a key assumption of the paper and points out constraints that limit the applicability of the proposed method. The politeness score is moderately positive (50) as the reviewer uses neutral, professional language throughout. They balance positive and negative points, using phrases like 'This paper has nice arguments' alongside critiques, without using harsh or dismissive language. The review maintains a respectful tone while providing constructive feedback.",-20,50
Epitomic Variational Autoencoders,Reject,2017,"['Serena Yeung', 'Anitha Kannan', 'Yann Dauphin', 'Li Fei-Fei']","[6, 4, 5, 8]","['Marginally above acceptance threshold', 'Ok but not good enough - rejection', '5', 'Top 50% of accepted papers, clear accept']","This paper is refreshing and elegant in its handling of *over-sampling* in VAE. Problem is that good reconstruction requires more nodes in the latent layers of the VAE. Not all of them can or should be sampled from at the *creative* regime of the VAE. Which ones to choose? The paper offers and sensible solution. Problem is that real-life data-sets like CIFAR have not being tried, so the reader is hard-pressed to choose between many other, just as natural, solutions. One can e.g. run in parallel a classifier and let it choose the best epitome, in the spirit of spatial transformers, ACE, reference [1]. The list can go on. We hope that the paper finds its way to the conference because it addresses an important problem in an elegant way, and papers like this are few and far between! On a secondary note, regarding terminology: Pls avoid using *the KL term* as in section 2.1, there are so many *KL terms* related to VAE-s, it ultimately gets out of control. *Generative error* is a more descriptive term, because minimizing it is indispensable for the generative qualities of the net. The variational error for example is also a *KL term* (equation (3.4) in reference [1]), as is the upper bound commonly used in VAE-s (your formula (5) and its equivalent - the KL expression as in formula (3.8) in reference [1]). The latter expression is frequently used and is handy for, say, importance sampling, as in reference [2]. [1] https://arxiv.org/pdf/1508.06585v5.pdf [2] https://arxiv.org/pdf/1509.00519.pdf","['5', '5', '5', '5']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[7, 18, 7, 15]","[13, 24, 13, 21]","[68, 78, 67, 409]","[28, 48, 32, 226]","[34, 26, 32, 150]","[6, 4, 3, 33]","The sentiment score is 70 (positive) because the reviewer describes the paper as 'refreshing and elegant' and expresses hope that it will be accepted to the conference, calling it 'important' and noting that 'papers like this are few and far between'. However, it's not 100 because the reviewer does point out some limitations, such as the lack of testing on real-life datasets. The politeness score is 80 (quite polite) because the reviewer uses respectful language throughout, offers constructive criticism, and frames suggestions as possibilities rather than demands. The use of phrases like 'We hope' and the overall encouraging tone contribute to the politeness. The score isn't 100 because there's still some direct criticism, albeit expressed politely.",70,80
Evaluation of Defensive Methods for DNNs against Multiple Adversarial Evasion Models,Reject,2017,"['Xinyun Chen', 'Bo Li', 'Yevgeniy Vorobeychik']","[5, 5, 4]","['5', '5', 'Ok but not good enough - rejection']","This paper performs a series of experiments to systematically evaluate the robustness of several defense methods, including RAD, AEC and its improved version etc.. It provides interesting observations. Overall, RAD and distillation have the best performances, but none of the methods can really resist the *additional* attack from cg or adam. Since it is an experimental paper, my main concern is about its clarity. See the comments below for details. Pros: 1. This paper provides a good comparison of the performances for the selected methods. 2. Section 3.3 (the *additional* attack) is a interesting investigation. Although the final result about the defense methods is negative, its results are still inspiring. 3. Overall, this paper provides interesting and inspiring experimental results about the selected methods. Cons: 1. There are several other methods in the literature that are missing from the paper. For example the defense methods and the attack methods in the papers [1,2]. 2. Although a long list of experimental results are provided in the paper, many details are skipped. For example, details of the experiments that generate the results in Table 5. 3. Without further explanations and analyses about the experimental results, the contribution of the paper seems limited. 4. This paper proposed an improved version of the AEC algorithm. But its experimental results seems not promising. Minor comments: Page 3: Equation (3) is also non-convex. So the non-convexity of Equation (2) should not be the motivation of Equation (3). [1] https://arxiv.org/abs/1507.00677 [2] https://arxiv.org/abs/1511.03034","['4', '3', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[8, 14]","[14, 20]","[93, 284]","[43, 153]","[43, 94]","[7, 37]","The sentiment score is 50 (slightly positive) because the reviewer acknowledges the paper's interesting observations and inspiring experimental results, while also pointing out some cons and areas for improvement. The review begins with positive aspects ('provides interesting observations', 'good comparison', 'interesting investigation') before moving on to concerns, indicating a balanced but generally favorable view. The politeness score is 70 (fairly polite) because the reviewer uses respectful language throughout, framing criticisms as suggestions or areas for improvement rather than harsh judgments. They use phrases like 'my main concern' and 'without further explanations' instead of more direct criticisms. The reviewer also balances positive and negative feedback, which contributes to a polite tone. The use of 'pros' and 'cons' sections further demonstrates a structured and professional approach to the review.",50,70
Exploring the Application of Deep Learning for Supervised Learning Problems,Reject,2017,"['Jose Rozanec', 'Gilad Katz', 'Eui Chul Richard Shin', 'Dawn Song']","[4, 3, 5]","['Ok but not good enough - rejection', 'Clear rejection', '5']","The topic is very interesting, but the paper is not convincing. Specifically, the experiment part is weak. The study should include datasets that are familiar to the community as well as the ones *that are not often addressed by deep learning*. The comparison to other approaches is not comprehensive.","['3', '5', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","['no_match', 12, 8, 19]","['no_match', 18, 13, 25]","['no match', 55, 38, 440]","['no match', 20, 21, 254]","['no match', 17, 16, 154]","['no match', 18, 1, 32]","The sentiment score is -50 because while the reviewer acknowledges the topic as 'very interesting', they state that the paper is 'not convincing' and the experiment part is 'weak'. These criticisms outweigh the initial positive comment, resulting in a moderately negative sentiment. The politeness score is 0 (neutral) because the reviewer's language is direct and matter-of-fact without being overtly polite or rude. They state their opinions clearly without using harsh language or personal attacks, but also without employing particularly courteous phrasing.",-50,0
Filling in the details: Perceiving from low fidelity visual input,Reject,2017,"['Farahnaz A. Wick', 'Michael L. Wick', 'Marc Pomplun']","[4, 6, 5]","['Ok but not good enough - rejection', 'Marginally above acceptance threshold', '5']","This paper aims to characterize the perceptual ability of a neural network under different input conditions. This is done by manipulating the input image x in various ways (e.g. downsamplig, foveating), and training an auto-encoder to reconstruct the original full-resolution image. MSE and qualitative results are shown and compared for the different input conditions. Unfortunately, this paper seems to lack focus, presenting a set of preliminary inspections with few concrete conclusions. For example, at the end of sec 4.4, *This result is not surprising, given that FOV-R contains additional information .... These results suggests that a small number of foveations containing rich details might be all these neural networks need....*. But this hypothesis is left dangling: What detailed regions are needed, and from where? For what sort of tasks? Secondly, it isn*t clear to me what reconstruction behaviors are caused by a fundamental perception of the input, and what are artifacts of the autoencoder and pixelwise l2 loss? A prime example is texture, which the autoencoder fails to recover. But with a pixelwise loss, the network must predict high-frequency textures nearly pixel-for-pixel at training time; if this is impossible, then it will generate a pixelwise average of the training samples --- a flat region. So then the network*s inability to reconstruct textures is due to a problem generating them, specifically averaging from the training loss, not necessarily an issue in perceiving textures. A network trained a different way (perhaps an adversarial network) may infer a texture is there, even if it wouldn*t be able to generate it in a pixelwise l2 sense. Similarly, the ability to perform color reconstruction given a color glimpse I think has much to do with disambiguating the color of an object/scene: If there is an ambiguity, the network won*t know which to *choose* (white flower or yellow flower?) and output an average, which is why there are so many sepia tones. However, in its section on this, the paper only measures the reconstruction error for different amounts of color given, and does not drill very far into any hypotheses for why this behavior occurs. There are some interesting measurements here, such as the amount of color needed in the foveation to reconstruct a color image, and the discussion on global features, which may start to get at a mechanism by which glimpses may propagate to an entire reconstruction. But overall it*s hard to know what to take away from this paper. What are larger concrete conclusions that can be garnered from the details, and what mechanisms bring them about? Can these be more thoroughly explored with more focus?","['3', '4', '5']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[2, 12, 24]","[1, 17, 30]","[1, 40, 47]","[0, 30, 29]","[1, 9, 2]","[0, 1, 16]","The sentiment score is -50 because the review is generally critical of the paper, pointing out several shortcomings such as lack of focus, preliminary nature of the inspections, and lack of concrete conclusions. However, it's not entirely negative as it acknowledges some interesting measurements and discussions. The politeness score is 20 because while the reviewer is critical, they express their concerns in a professional and constructive manner, using phrases like 'Unfortunately' and 'it's hard to know what to take away from this paper' rather than harsh or rude language. The reviewer also offers suggestions for improvement, which is a polite approach to criticism.",-50,20
Finding a Jack-of-All-Trades: An Examination of Semi-supervised Learning in Reading Comprehension,Reject,2017,"['Rudolf Kadlec', 'Ondřej Bajgar', 'Peter Hrincar', 'Jan Kleindienst']","[6, 4, 3]","['Marginally above acceptance threshold', 'Ok but not good enough - rejection', 'Clear rejection']","First I would like to apologize for the delay in reviewing. summary : This work explores several experiments to transfer training a specific model of reading comprehension ( AS Reader), in an artificial and well populated dataset in order to perform in another target dataset. Here is what I understand are their several experiments to transfer learning, but I am not 100% sure. 1. The model is trained on the big artificial dataset and tested on the small target datasets (section 4.1) 2. The model is pre-trained on the big artificial dataset like before, then fine-tuned on a few examples from the target dataset and tested on the remaining target examples. Several such models are trained using different sub-sets of fine-tuning examples. The results are tested against the performance of randomly intialized then fine-tuned models (section 4.2). 3. The model is pre-trained on the big artificial dataset like before. The model is made of an embedding component and an encoder component. Alternatively, each component is reset to a random initialization, to test the importance of the pre-training in each component. Then the model is fine-tuned on a few examples from the target dataset and tested on the remaining target examples. (section 4.3) I think what makes things difficult to follow is the fact that the test set is composed by several sub tasks, and sometimes what is reported is the mean performance across the tasks, sometimes the performance on a few tasks. Sometimes what we see is the mean performance of several models? You should report standard deviations also. Could you better explain what you mean by best validation ? Interesting and unpretentious work. The clarity of the presentation could be improved maybe by simplifying the experimental setup? The interesting conclusion I think is reported at the end of the section 4.1, when the nuanced difference between the datasets are exposed. Minor: unexplained acronyms: GRU, BT, CBT. benfits p. 2 subsubset p. 6","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[11, 2, 'no_match', 22]","[15, 8, 'no_match', 23]","[29, 12, 'no match', 48]","[19, 4, 'no match', 38]","[9, 7, 'no match', 7]","[1, 1, 'no match', 3]","The sentiment score is slightly positive (20) because the reviewer describes the work as 'Interesting and unpretentious' and provides constructive feedback. However, they also point out several areas for improvement, which prevents a higher score. The politeness score is moderately high (60) due to the reviewer's apologetic opening for the delay, the use of phrases like 'I think' and 'Could you', and the overall constructive tone. The reviewer offers suggestions and asks for clarifications rather than making harsh criticisms. The language is professional and respectful throughout, though not overly formal or effusive in praise, which is why it's not scored higher.",20,60
Group Sparse CNNs for Question Sentence Classification with Answer Sets,Reject,2017,"['Mingbo Ma', 'Liang Huang', 'Bing Xiang', 'Bowen Zhou']","[4, 5, 6]","['Ok but not good enough - rejection', '5', 'Marginally above acceptance threshold']","The paper proposes the group sparse autoencoder that enforces sparsity of the hidden representation group-wise, where the group is formed based on labels (i.e., supervision). The p-th group hidden representation is used for reconstruction with group sparsity penalty, allowing learning more discriminative, class-specific patterns in the dataset. The paper also propose to combine both group-level and individual level sparsity as in Equation (9). Clarity of the paper is a bit low. - Do you use only p-th group*s activation for reconstruction? If it is true, then for Equation (9) do you use all individual hidden representation for reconstruction or still using the subset of representation corresponding to that class only? - In Equation (7), RHS misses the summation over p, and wondering it is a simple typo. - Is the algorithm end-to-end trainable? It seems to me that the group sparse CNN is no more than the GSA whose input data is the feature extracted from sequential CNNs (or any other pretrained CNNs). Other comments are as follows: - Furthermore the group sparse autoencoder is (semi-) supervised method since it uses label information to form a group, whereas the standard sparse autoencoder is fully unsupervised. That being said, it is not surprising that group sparse autoencoder learns more class-specific pattern whereas sparse autoencoder doesn*t. I think the fair comparison should be to autoencoders that combines classification for their objective function. - Although authors claim that GSA learns more group-relevant features, Figure 3 (b) is not convincing enough to support this claim. For example, the first row contains many filters that doesn*t look like 1 (e.g., very last column looks like 3). - Other than visual inspection, do you observe improvement in classification using proposed algorithm on MNIST experiments? - The comparison to the baseline model is missing. I believe the baseline model shouldn*t be the sequential CNN, but the sequential CNN + sparse autoencoder. In addition, more control experiment is required that compares between the Equation (7)-(9), with different values of alpha and eta. Missing reference: Shang et al., Discriminative Training of Structured Dictionaries via Block Orthogonal Matching Pursuit, SDM 2016 - they consider block orthgonal matching pursuit for dictionary learning whose blocks (i.e., projection matrices) are constructed based on the class labels for discirminative training.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[8, 16, 19, 18]","[14, 21, 25, 24]","[57, 134, 154, 250]","[28, 82, 84, 136]","[29, 45, 65, 73]","[0, 7, 5, 41]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the paper's contributions, they express several concerns and criticisms. The review points out issues with clarity, questions the methodology, and suggests missing comparisons and references. However, it's not entirely negative as the reviewer also recognizes the paper's novel approach.

The politeness score is moderately positive (50) because the reviewer maintains a professional and respectful tone throughout. They phrase their criticisms as questions or suggestions rather than direct attacks. For example, they use phrases like 'Do you...?' and 'I think...' which soften the critique. The reviewer also provides constructive feedback and specific recommendations for improvement, which is a polite approach to peer review.",-20,50
HFH: Homologically Functional Hashing for Compressing Deep Neural Networks,Reject,2017,"['Lei Shi', 'Shikun Feng', 'Zhifan Zhu']","[5, 6]","['5', 'Marginally above acceptance threshold']","The paper proposed a very complex compression and reconstruction method (with additional parameters) for reducing the memory footprint of deep networks. The authors show that this complex proposal is better than simple hashed net proposal. One question: Are you also counting the extra parameters for reconstruction network for the memory comparison? Otherwise, the experiments are unfair. Since hashing and reconstruction cost will dominate the feed-forward and back-propagation updates, it is imperative to compare the two methods on running time. For hashed net, this is quite simple, yet it created an additional bottleneck. Please also show the impact on running time. Small improvements for a big loss in computational cost may not be acceptable. I am not convinced that this method will be lightweight. If we are allowed complicated compression and reconstruction then we can use any off-shelf methods, but the cost will be huge","['5', '4']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[12, 10, 23]","[11, 16, 26]","[12, 42, 5]","[9, 16, 2]","[1, 24, 1]","[2, 2, 2]","The sentiment score is -50 because the reviewer expresses significant skepticism about the proposed method. They question the fairness of the experiments, the impact on running time, and whether the method is truly lightweight. The reviewer is 'not convinced' by the approach and suggests that the improvements may not be worth the computational cost. However, it's not entirely negative as they acknowledge the method shows some improvements over the baseline.

The politeness score is 0 (neutral) because the reviewer's language is neither particularly polite nor rude. They use direct, professional language to express their concerns and ask questions. There are no overtly polite phrases or rude comments; the tone is matter-of-fact and focused on the technical aspects of the paper.",-50,0
Improved Architectures for Computer Go,Reject,2017,['Tristan Cazenave'],"[3, 7, 4, 3]","['Clear rejection', 'Good paper, accept', 'Ok but not good enough - rejection', 'Clear rejection']","The paper tests various feedforward network architectures for supervised training to predict a human’s next move, given a board position. It trains on human play data taken from KGX, augmenting the data by considering all 8 rotations/reflections of each board position. The paper’s presentation is inefficient and muddled, and the results seem incremental. Presentation: The abstract and introduction point out that AlphaGo requires many RL iterations to train, and propose to improve this by swapping out the policy network with one that is more amenable to training. However, the paper only presents supervised learning results, not RL. While it’s not unreasonable to assume that a higher-capacity network that shows improvements in supervised learning will also yield dividends in RL, it’s still unsatisfying to be presented with SL improvements and be asked to assume that the RL improvements will be of a similar magnitude, whatever that may mean. It would’ve been more convincing to train both AlphaGo and this paper*s architectures on an equal number of RL self-play iterations, then have them play each other. (Both would be pre-trained using supervised training, as per the AlphaGo paper). It is not until section 3.3 that it is clearly stated this is strictly a supervised-learning paper. This should have been put front and center in the abstract and introduction. Fully 3 pages are spent on giant but simple architecture diagrams. This is both extravagant and muddles the exposition. It seems better to show just the architectures used in the experiments, and spend at most half a page doing so, so that they may be seen alongside one another. The results (Table 1, Figures 7 and 8) are hard to skim, as there is little information in the captions, and the graph axes are poorly labeled. For example, I assume “Examples” in figures 7 and 8 should be “Training examples”, and the number of training examples isn’t 0-50, but some large multiple thereof. Results: The take-home seems to be that that deeper networks do better, and residual architectures and spatial batch normalization each improve the results in this domain, as they are known to do in others. Furthermore, we are asked to assume that the improvements in an RL setting will be similar to the improvements in SL shown here. These results seem too incremental to justify an ICLR publication.","['4', '5', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']",[20],[26],[148],[79],[27],[42],"The sentiment score is -70 because the review is predominantly negative. The reviewer criticizes the paper's presentation as 'inefficient and muddled' and the results as 'incremental'. They express dissatisfaction with the paper's focus on supervised learning instead of reinforcement learning, and state that the results 'seem too incremental to justify an ICLR publication'. The politeness score is -20 because while the reviewer doesn't use explicitly rude language, their tone is quite critical and dismissive. They use phrases like 'unsatisfying', 'extravagant', and 'muddles the exposition', which come across as somewhat harsh. The reviewer does not offer much in the way of positive feedback or constructive suggestions, which contributes to the overall negative and impolite tone.",-70,-20
Improving Invariance and Equivariance Properties of Convolutional Neural Networks,Reject,2017,"['Christopher Tensmeyer', 'Tony Martinez']","[4, 5, 4]","['Ok but not good enough - rejection', '5', 'Ok but not good enough - rejection']","This paper empirically studies the invariance, equivariance and equivalence properties of representations learned by convolutional networks under various kinds of data augmentation. Additional loss terms are presented which can make a representation more invariant or equivariant. The idea of measuring invariance, equivariance and equivalence of representations is not new (Lenc & Vedaldi). The authors are the first to systematically study the effect of data augmentation on these properties, but it is unclear in what way the results are surprising, interesting, or useful. It is not really surprising that data augmentation increases invariance, or that training with the same augmentation leads to more similar representations than training with different augmentations. Regarding the presented method to increase invariance and equivariance: while it could be that a representation will generalize better if it is invariant or equivariant, it is not clear why one would want to increase in/equivariance if it does not indeed lead to improvements in performance. The paper presents no evidence that training for increased invariance / equivariance leads to substantial improvements in performance. Combined with the fact that the loss (eq. 6) would substantially increase the computational burden, I don’t think this technique will be very useful. Minor comments: -R^{nxn} should be R^{n 	imes n} -In eq. 2: ‘equivaraince’ -In 3.3, argmax is not properly formatted -I think data augmentation was already considered essential before Krizhevsky et al. Not really correct to attribute this to them. - About the claim “This is related to the idea of whether CNNs collapse (invariance) or linearize (equivariance) view manifolds of 3D objects”. The idea that equivariance means that the manifold (orbit) is linearized, is incorrect. A linear representation M_g can create nonlinear manifolds. A simple example is given by a rotation matrix in 2D (clearly linear), generating a nonlinear manifold (the circle). - Equivariance in eq. 2 should be called “non-equivariance”. If the value is low, the representation is equivariant, while if it is high it is non-equivariant. - “Eq. 2 also uses the paradigm that”, uses the word paradigm in a strange manner - In the definition of x’_ij, should one of the g_j be inverted? Otherwise it seems like the transformation is applied twice, instead of being undone.","['4', '3', '5']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[1, -3]","[7, 1]","[40, 2]","[23, 1]","[15, 1]","[2, 0]","The sentiment score is -50 because the reviewer expresses several criticisms and doubts about the paper's novelty and usefulness. They state that the results are not surprising or particularly interesting, and question the practical value of the presented method. However, it's not entirely negative as they acknowledge some positive aspects, such as the systematic study of data augmentation effects. The politeness score is 20 because while the reviewer is critical, they maintain a professional and respectful tone throughout. They use phrases like 'it could be that' and 'I don't think' rather than making harsh declarative statements. They also provide specific, constructive feedback and minor corrections, which is helpful to the authors. The language is not overtly polite, but it avoids rudeness and maintains a neutral, academic tone.",-50,20
Inefficiency of stochastic gradient descent with larger mini-batches (and more learners),Reject,2017,"['Onkar Bhardwaj', 'Guojing Cong']","[6, 4, 5]","['Marginally above acceptance threshold', 'Ok but not good enough - rejection', '5']",This paper theoretically justified a faster convergence (in terms of average gradient norm attained after processing a fixed number of samples) of using small mini-batches for SGD or ASGD with smaller number of learners. This indicates that there is an inherent inefficiency in the speed-up obtained with parallelizing gradient descent methods by taking advantage of hardware. This paper looks good overall and makes some connection between algorithm design and hardware properties. My main concern is that Lemma 1 looks incorrect to me. The factor D_f / S should be D_f/ (S*M) for me. Please clarify this and check the subsequent theorem.,"['4', '4', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[9, 14]","[15, 20]","[21, 78]","[12, 60]","[2, 7]","[7, 11]","The sentiment score is 50 (slightly positive) because the reviewer states that the paper 'looks good overall' and appreciates the connection made between algorithm design and hardware properties. However, they express a 'main concern' about a potential error in Lemma 1, which prevents the sentiment from being more positive. The politeness score is 75 (quite polite) because the reviewer uses respectful language throughout, acknowledging the paper's strengths before raising concerns. They phrase their criticism as a request for clarification rather than an outright accusation of error, using phrases like 'Please clarify this' which maintains a courteous tone. The review is constructive and professional, avoiding any harsh or rude language.",50,75
Inference and Introspection in Deep Generative Models of Sparse Data,Reject,2017,"['Rahul G. Krishnan', 'Matthew Hoffman']","[6, 7, 5, 5]","['Marginally above acceptance threshold', 'Good paper, accept', '5', '5']","The paper claims improved inference for density estimation of sparse data (here text documents) using deep generative Gaussian models (variational auto-encoders), and a method for deriving word embeddings from the model*s generative parameters that allows for a degree of interpretability similar to that of Bayesian generative topic models. To discuss the contributions I will quickly review the generative story in the paper: first a K-dimensional latent representation is sampled from a multivariate Gaussian, then an MLP (with parameters 	heta) predicts unnormalised potentials over a vocabulary of V words, the potentials are exponentiated and normalised to make the parameters of a multinomial from where word observations are repeatedly sampled to make a document. Here intractable inference is replaced by the VAE formulation where an inference network (with parameters phi) independently predicts for each document the mean and variance of a normal distribution (amenable to reparameterised gradient computation). The first, and rather trivial, contribution is to use tf-idf features to inject first order statistics (a global information) into local observations. The authors claim that this is particularly helpful in the case of sparse data such as text. The second contribution is more interesting. In optimising generative parameters (	heta) and variational parameters (phi), the authors turn to a treatment which is reminiscent of the original SVI procedure. That is, they see the variational parameters phi as *global* variational parameters, and the predicted mean mu(x) and covariance Sigma(x) of each observation x are treated as *local* variational parameters. In the original VAE, local parameters are not directly optimised, instead they are indirectly optimised via optimisation of the global parameters utilised in their prediction (shared MLP parameters). Here, local parameters are optimised holding generative parameters fixed (line 3 of Algorithm 1). The optimised local parameters are then used in the gradient step of the generative parameters (line 4 of Algorithm 1). Finally, global variational parameters are also updated (line 5). Whereas indeed other authors have proposed to optimise local parameters, I think that deriving this procedure from the more familiar SVI makes the contribution less of a trick and easier to relate to. Some things aren*t entirely clear to me. I think it would have been nice if the authors had shown the functional form of the gradient used in step 3 of Algorithm 1. The gradient step for global variational parameters (line 5 of Algorithm 1) uses the very first prediction of local parameters (thus ignoring the optimisation in step 3), this is unclear to me. Perhaps I am missing a fundamental reason why that has to be the case (either way, please clarify). The authors argue that this optimisation turns out helpful to modelling sparse data because there is evidence that the generative model p_	heta(x|z) suffers from poor initialisation. Please, discuss why you expect the initialisation problem to be worse in the case of sparse data. The final contribution is a neat procedure to derive word embeddings from the generative model parameters. These embeddings are then used to interpret what the model has learnt. Interestingly, these word embeddings are context-sensitive once that the latent variable models an entire document. About Figures 2a and 2b: the caption says that solid lines indicate validation perplexity for M=1 (no optimisation of local parameters) and dashed lines indicate M=100 (100 iterations of optimisation of local parameters), but the legends of the figures suggest a different reading. If I interpret the figures based on the caption, then it seems that indeed deeper networks exposed to more data benefit from optimisation of local parameters. Are the authors pretty sure that in Figure 2b models with M=1 have reached a plateau (so that longer training would not allow them to catch up with M=100 curves)? As the authors explain in the caption, x-axis is not comparable on running time, thus the question. The analysis of singular values seems like an interesting way to investigate how the model is using its capacity. However, I can barely interpret Figures 2c and 2d, I think the authors could have walked readers through them. As for the word embedding I am missing an evaluation on a predictive task. Also, while illustrative, Table 2b is barely reproducible. The text reads *we create a document comprising a subset of words in the the context’s Wikipedia page.* which is rather vague. I wonder whether this construct needs to be carefully designed in order to get Table 2b. In sum, I have a feeling that the inference technique and the embedding technique are both useful, but perhaps they should have been presented separately so that each could have been explored in greater depth.","['4', '3', '3', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[3, 'no_match']","[9, 'no_match']","[32, 'no match']","[13, 'no match']","[19, 'no match']","[0, 'no match']","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some contributions as interesting and potentially useful, they also express several concerns and criticisms. The reviewer points out areas that are unclear, questions some methodological choices, and suggests that the paper's two main contributions might have been better presented separately. However, the tone isn't entirely negative, as the reviewer does recognize some value in the work. The politeness score is moderately positive (60) because the reviewer maintains a professional and respectful tone throughout. They use phrases like 'please clarify' and 'I think it would have been nice if', which are polite ways of requesting more information or suggesting improvements. The reviewer also acknowledges the potential value of the work, even while critiquing it. The language is consistently academic and constructive, avoiding any harsh or rude phrasing.",-20,60
Introducing Active Learning for CNN under the light of Variational Inference,Reject,2017,"['Melanie Ducoffe', 'Frederic Precioso']","[6, 6, 6]","['Marginally above acceptance threshold', 'Marginally above acceptance threshold', 'Marginally above acceptance threshold']","The paper proposes to perform active learning using pool selection of deep learning mini-batches using an approximation of the bayesian posterior. Several terms are in turn approximated. The Maximum Likelihood Estimation (MLE) bayesian inference approach to active learning, the various approximations, and more generally the theoretical framework is very interesting but difficult to follow. The paper is written in poor English and is sometimes a bit painful to read. Alternative Active learning strategies and techniques do not need to be described with such detail. On the other hand, the proposed approach has a lot of complex approximations which would benefit from a more detailed/structured presentation. Another dataset would be a big plus (both datasets concern gray digits and USPS and are arguably somewhat similar).","['1', '2', '2']","['The reviewer*s evaluation is an educated guess', 'The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', 'The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']","[3, 17]","[9, 23]","[16, 116]","[7, 84]","[8, 16]","[1, 16]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the interesting theoretical framework, they also point out several issues with the paper. The reviewer mentions that the paper is difficult to follow, poorly written, and lacks diversity in datasets. However, they do recognize some positive aspects, which prevents the score from being more negative. The politeness score is neutral (0) as the reviewer maintains a professional tone without being overtly polite or rude. They provide direct criticism but do so in a matter-of-fact manner without using harsh language or personal attacks. The reviewer balances negative comments with positive observations, which contributes to the neutral politeness score.",-20,0
L-SR1: A Second Order Optimization Method for Deep Learning,Reject,2017,"['Vivek Ramamurthy', 'Nigel Duffy']","[4, 4, 5]","['Ok but not good enough - rejection', 'Ok but not good enough - rejection', '5']","The paper proposes a new second-order method L-SR1 to train deep neural networks. It is claimed that the method addresses two important optimization problems in this setting: poor conditioning of the Hessian and proliferation of saddle points. The method can be viewed as a concatenation of SR1 algorithm of Nocedal & Wright (2006) and limited-memory representations Byrd et al. (1994). First of all, I am missing a more formal, theoretical argument in this work (in general providing more intuition would be helpful too), which instead is provided in the works of Dauphin (2014) or Martens. The experimental section in not very convincing considering that the performance in terms of the wall-clock time is not reported and the advantage over some competitor methods is not very strong even in terms of epochs. I understand that the authors are optimizing their implementation still, but the question is: considering the experiments are not convincing, why would anybody bother to implement L-SR1 to train their deep models? The work is not ready to be published.","['4', '3', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[6, 19]","[10, 23]","[2, 12]","[2, 7]","[0, 2]","[0, 3]","The sentiment score is -80 because the review is predominantly negative. The reviewer states that the paper lacks formal theoretical arguments, the experimental section is not convincing, and concludes that 'The work is not ready to be published.' These are strong negative statements indicating significant dissatisfaction with the paper. The politeness score is -20 because while the language is not overtly rude, it is quite direct and critical without much attempt to soften the criticism. Phrases like 'I am missing,' 'not very convincing,' and 'why would anybody bother' come across as somewhat dismissive and impolite. However, the reviewer does use some neutral language and acknowledges the authors' ongoing work on implementation, which prevents the score from being even lower.",-80,-20
Learning Approximate Distribution-Sensitive Data Structures,Reject,2017,"['Zenna Tavares', 'Armando Solar-Lezama']","[4, 4, 3]","['Ok but not good enough - rejection', 'Ok but not good enough - rejection', 'Clear rejection']","A method for training neural networks to mimic abstract data structures is presented. The idea of training a network to satisfy an abstract interface is very interesting and promising, but empirical support is currently too weak. The paper would be significantly strengthened if the method could be shown to be useful in a realistic application, or be shown to work better than standard RNN approaches on algorithmic learning tasks. The claims about mental representations are not well supported. I would remove the references to mind and brain, as well as the more philosophical points, or write a paper that really emphasizes one of these aspects and supports the claims.","['3', '3', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[1, 13]","[7, 19]","[7, 157]","[4, 81]","[2, 49]","[1, 27]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the idea as 'interesting and promising', they express significant concerns about the weak empirical support and suggest major changes to strengthen the paper. The overall tone indicates that substantial improvements are needed. The politeness score is moderately positive (50) as the reviewer uses respectful language, acknowledges positive aspects, and frames criticisms as suggestions for improvement rather than harsh judgments. They use phrases like 'would be significantly strengthened if' and 'I would remove' which are polite ways of offering critique.",-20,50
Learning Disentangled Representations in Deep Generative Models,Reject,2017,"['N. Siddharth', 'Brooks Paige', 'Alban Desmaison', 'Jan-Willem van de Meent', 'Frank Wood', 'Noah D. Goodman', 'Pushmeet Kohli', 'Philip H.S. Torr']","[6, 6, 5]","['Marginally above acceptance threshold', 'Marginally above acceptance threshold', '5']","This paper investigates deep generative models with multiple stochastic nodes and gives them meaning by semi-supervision. From a methodological point of view, there is nothing fundamentally novel (it is very similar to the semi-supervised work of Kingma et al; although this work has sometimes more than two latent nodes, it is not a complex extension). There is a fairly classical auxiliary variable trick used to make sure the inference network for y is trained over all data points (by supposing y is in fact is a latent variable with an observation 	ilde y; the observation is y if y is observed, or uninformative for unobserved y). Alternatively, one can separate the inference used to learn the generative model (which throws out inference over y if it is observed), from an inference used to *exercise* the model (approximate the complex p(y|x) in the model by a simpler q(y|x) - effectively inferring the target p(y|x) for the data where only x is collected). Results are strong, although on simple datasets. Overall this is a well written, interesting paper, but lacking in terms of methodological advances. Minor: - I feel the title is a bit too general for the content of the paper. I personally don*t agree with the strong contrast made between deep generative models and graphical models (deep generative models are graphical models, but they are more typically learned and un-interpretable than classical graphical models; and having multiple stochastic variables is not exclusive to graphical models, see DRAW, Deep Kalman Filter, Recurrent VAE, etc.). The word *structure* is a bit problematic; here, the paper seems more concerned with disentangling and semanticizing the latent representation of a generative model by supervision. It is debatable whether the models themselves have structure.","['5', '4', '3']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[8, 5, 4, 5, 22, 11, 15, 27]","[14, 11, 10, 11, 28, 17, 21, 33]","[59, 53, 25, 73, 153, 246, 322, 624]","[23, 25, 9, 28, 71, 125, 180, 299]","[33, 27, 15, 41, 71, 97, 102, 242]","[3, 1, 1, 4, 11, 24, 40, 83]","The sentiment score is slightly positive (20) because while the reviewer acknowledges the paper as 'well written' and 'interesting' with 'strong' results, they also note that it's 'lacking in terms of methodological advances' and not 'fundamentally novel'. The overall tone is more positive than negative, but with significant reservations. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, acknowledging the paper's strengths before offering criticisms. They use phrases like 'I feel' and 'I personally don't agree' to soften disagreements, and provide constructive feedback rather than harsh criticism. The review maintains a professional and courteous tone, even when pointing out limitations.",20,60
Learning Word-Like Units from Joint Audio-Visual Analylsis,Reject,2017,"['David Harwath', 'James R. Glass']","[6, 5, 5]","['Marginally above acceptance threshold', '5', '5']","This paper is a follow-up on the NIPS 2016 paper *Unsupervised learning of spoken language with visual context*, and does exactly what that paper proposes in its future work section: *to perform acoustic segmentation and clustering, effectively learning a lexicon of word-like units* using the embeddings that their system learns. The analysis is very interesting and I really like where the authors are going with this. My main concern is novelty. It feels like this work is a rather trivial follow-up on an existing model, which is fine, but then the analysis should be more satisfying: currently, it feels like the authors are just illustrating some of the things that the NIPS model (with some minor improvements) learns. For a more interesting analysis, I would have liked things like a comparison of different segmentation approaches (both in audio and in images), i.e., suppose we have access to the perfect segmentation in both modalities, what happens? It would also be interesting to look at what is learned with the grounded representation, and evaluate e.g. on multi-modal semantics tasks. Apart from that, the paper is well written and I really like this research direction. It is very important to analyze what models learn, and this is a good example of the types of questions one should ask. I am afraid, however, that the model is not novel enough, nor the questions deep enough, to make this paper better than borderline for ICLR.","['4', '5', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[6, 33]","[12, 39]","[71, 493]","[36, 319]","[33, 136]","[2, 38]","The sentiment score is slightly negative (-20) because while the reviewer expresses interest in the research direction and analysis, they have concerns about the novelty of the work and feel it's a 'rather trivial follow-up'. They also suggest the paper is 'borderline for ICLR', indicating it may not meet the conference standards. However, the reviewer does provide positive comments about the writing and the importance of the research direction, which prevents the score from being more negative. The politeness score is moderately positive (60) as the reviewer uses respectful language throughout, acknowledging the paper's strengths ('well written', 'very interesting', 'I really like this research direction') while offering constructive criticism. They phrase their concerns politely ('My main concern is...', 'I would have liked...') rather than using harsh or dismissive language.",-20,60
Learning a Static Analyzer: A Case Study on a Toy Language,Reject,2017,"['Manzil Zaheer', 'Jean-Baptiste Tristan', 'Michael L. Wick', 'Guy L. Steele Jr.']","[4, 3, 3]","['Ok but not good enough - rejection', 'Clear rejection', 'Clear rejection']","This paper takes a first step towards learning to statically analyze source code. It develops a simple toy programming language that includes loops and branching. The aim is to determine whether all variables in the program are defined before they are used. The paper tries a variety of off-the-shelf sequence classification models and develops a new model that makes use of a ``differentiable set** to keep track of which variables have been defined so far. Result show that an LSTM model can achieve 98% accuracy, and the differentiable set model can achieve 99.3% accuracy with sequence-level supervision and 99.7% accuracy with strong token-level supervision. An additional result is used whereby an LSTM language model is trained over correct code, and then low probability (where a threshold to determine low is tuned by hand) tokens are highlighted as sources of possible error. One further question is if the authors could clarify what reasoning patterns are needed to solve these problems. Does the model need to, e.g., statically determine whether an `if` condition can ever evaluate to true in order to solve these tasks? Or is it just as simple as checking whether a variable appears on a LHS before it appears on a RHS later in the textual representation of the program? Strengths: - Learning a static analyzer is an interesting concept, and I think there is good potential for this line of work - The ability to determine whether variables are defined before they are used is certainly a prerequisite for more complicated static analysis. - The experimental setup seems reasonable - The differentiable set seems like a useful (albeit simple) modelling tool Weaknesses: - The setup is very toy, and it*s not clear to me that this makes much progress towards the challenges that would arise if one were trying to learn a static analyzer - The models are mostly very simple. The one novelty on the modelling front (the differentiable set) provides a small win on this task, but it*s not clear if it is a useful general construct or not. Overall: I think it*s an interesting start, and I*m eager to see how this line of work progresses. In my opinion, it*s a bit too early to accept this work to ICLR, but I*d be excited about seeing what happens as the authors try to push the system to learn to analyze more properties of code, and as they push towards scenarios where the learned static analyzer would be useful, perhaps leveraging strengths of machine learning that are not available to standard programming languages analyses.","['4', '4', '5']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[7, 10, 12, 43]","[13, 15, 17, 48]","[143, 33, 40, 86]","[69, 22, 30, 60]","[73, 9, 9, 4]","[1, 2, 1, 22]","The sentiment score is slightly positive (20) because the reviewer acknowledges the paper's interesting concept and potential, while also pointing out weaknesses and suggesting it's too early for acceptance. The overall tone is encouraging for future work. The politeness score is high (80) as the reviewer uses respectful language throughout, acknowledging strengths and weaknesses in a balanced manner, and offering constructive feedback. The reviewer uses phrases like 'I think' and 'in my opinion' to soften criticisms, and expresses eagerness to see future developments in this line of research.",20,80
Learning to Understand: Incorporating Local Contexts with Global Attention for Sentiment Classification,Reject,2017,"['Zhigang Yuan', 'Yuting Hu', 'Yongfeng Huang']","[3, 3, 4]","['Clear rejection', 'Clear rejection', 'Ok but not good enough - rejection']","The authors did not bother responding or fixing any of the pre-review comments. Hence I repeat here: Please do not make incredibly unscientific statements like this one: *The working procedure of this model is just like how we human beings read a text and then answer a related question. * Really, *humans beings* have an LSTM like model to read a text? Can you cite an actual neuroscience paper for such a claim? The answer is no, so please delete such statements from future drafts. Generally, your experiments are about simple classification and the methods you*re competing against are simple models like NB-SVM. So I would change the title, abstract ad introduction accordingly and not attempt hyperbole like *Learning to Understand* in the title. Lastly, your attention level approach seems similar to dynamic memory networks by Kumar et al. they also have experiments for sentiment and it would be interesting to understand the differences to your model and compare to them. Other reviewers included further missing related work and fitting this paper into the context of current literature. Given that no efforts were made to fix the pre-review questions and feedback, I doubt this will become ready in time for publication.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","['no_match', 'no_match', 11]","['no_match', 'no_match', 17]","['no match', 'no match', 301]","['no match', 'no match', 136]","['no match', 'no match', 73]","['no match', 'no match', 92]","The sentiment score is -80 because the reviewer expresses strong dissatisfaction with the authors' lack of response to previous feedback and criticizes multiple aspects of the paper, including 'incredibly unscientific statements', 'hyperbole', and missing comparisons to related work. The reviewer also doubts the paper will be ready for publication, indicating a very negative sentiment. The politeness score is -60 because the language used is quite direct and critical, with phrases like 'The authors did not bother responding or fixing' and 'Really, *humans beings* have an LSTM like model to read a text?' The tone is confrontational and lacks the usual diplomatic language found in more polite reviews. However, it's not entirely impolite as the reviewer does provide specific feedback and suggestions for improvement, which prevents the score from being even lower.",-80,-60
Leveraging Asynchronicity in Gradient Descent for Scalable Deep Learning,Reject,2017,"['Jeff Daily', 'Abhinav Vishnu', 'Charles Siegel']","[5, 3, 3]","['5', 'Clear rejection', 'Clear rejection']","This paper describe an implementation of delayed synchronize SGD method for multi-GPU deep ne training. Comments 1) The described manual implementation of delayed synchronization and state protection is helpful. However, such dependency been implemented by a dependency scheduler, without doing threading manually. 2) The overlap of computation and communication is a known technique implemented in existing solutions such as TensorFlow(as described in Chen et.al) and MXNet. The claimed contribution of this point is somewhat limited. 3) The convergence accuracy is only reported for the beginning iterations and only on AlexNet. It would be more helpful to include convergence curve till the end for all compared networks. In summary, this is paper implements a variant of delayed SyncSGD approach. I find the novelty of the system somewhat limited (due to comment (2)). The experiments should have been improved to demonstrate the advantage of proposed approach.","['4', '4', '5']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[10, 14, 2]","[13, 19, 5]","[29, 104, 22]","[18, 59, 8]","[5, 20, 12]","[6, 25, 2]","The sentiment score is -50 because the reviewer expresses several criticisms and limitations of the paper, such as limited novelty and insufficient experiments. However, they do acknowledge some helpful aspects, preventing the score from being extremely negative. The politeness score is 0 (neutral) because the reviewer uses professional and straightforward language without being particularly polite or rude. They provide direct feedback and suggestions without using overly harsh or complimentary language.",-50,0
Machine Solver for Physics Word Problems,Reject,2017,"['Megan Leszczynski', 'Jose Moreira']","[4, 4, 5]","['Ok but not good enough - rejection', 'Ok but not good enough - rejection', '5']","The authors describe a system for solving physics word problems. The system consists of two neural networks: a labeler and a classifier, followed by a numerical integrator. On the dataset that the authors synthesize, the full system attains near full performance. Outside of the pipeline, the authors also provide some network activation visualizations. The paper is clear, and the data generation procedure/grammar is rich and interesting. However, overall the system is not well motivated. Why did they consider this particular problem domain, and what challenges did they specifically hope to address? Is it the ability to label sequences using LSTM networks, or the ability to classify what is being asked for in the question? This has already been illustrated, for example, by work on POS tagging and by memory networks for the babi tasks. A couple of standard architectural modifications, i.e. bi-directionality and a content-based attention mechanism, were also not considered.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[0, 11]","[6, 15]","[16, 12]","[5, 11]","[10, 1]","[1, 0]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects ('The paper is clear, and the data generation procedure/grammar is rich and interesting'), they express significant concerns about the motivation and novelty of the work ('However, overall the system is not well motivated'). The reviewer also points out missing elements that could have improved the study. The politeness score is moderately positive (50) as the reviewer uses neutral, professional language throughout and begins with positive observations before presenting criticisms. They avoid harsh or personal language, instead focusing on specific aspects of the work that could be improved.",-20,50
Making Stochastic Neural Networks from Deterministic Ones,Reject,2017,"['Kimin Lee', 'Jaehyung Kim', 'Song Chong', 'Jinwoo Shin']","[5, 6]","['5', 'Marginally above acceptance threshold']","Strengths - interesting to explore the connection between ReLU DNN and simplified SFNN - small task (MNIST) is used to demonstrate the usefulness of the proposed training methods experimentally - the proposed, multi-stage training methods are simple to implement (despite lacking theoretical rigor) Weaknesses -no results are reported on real tasks with large training set -not clear exploration on the scalability of the learning methods when training data becomes larger -when the hidden layers become stochastic, the model shares uncertainty representation with deep Bayes networks or deep generative models (Deep Discriminative and Generative Models for Pattern Recognition , book chapter in “Pattern Recognition and Computer Vision”, November 2015, Download PDF). Such connections should be discussed, especially wrt the use of uncertainty representation to benefit pattern recognition (i.e. supervised learning via Bayes rule) and to benefit the use of domain knowledge such as “explaining away”. -would like to see connections with variational autoencoder models and training, which is also stochastic with hidden layers","['5', '4']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[2, 9, 25, 9]","[8, 15, 31, 15]","[80, 30, 152, 276]","[37, 14, 78, 128]","[42, 11, 9, 122]","[1, 5, 65, 26]","The sentiment score is slightly positive (20) because the review starts by listing several strengths of the paper, indicating an overall positive impression. However, it also includes a list of weaknesses, which balances out the positivity somewhat. The politeness score is moderately positive (50) as the reviewer uses neutral language and presents both strengths and weaknesses in a professional manner without harsh criticism. The reviewer offers constructive suggestions for improvement rather than outright criticism. The use of phrases like 'would like to see' and 'should be discussed' indicates a polite way of suggesting changes without being demanding.",20,50
Marginal Deep Architectures: Deep learning for Small and Middle Scale Applications,Reject,2017,"['Yuchen Zheng', 'Guoqiang Zhong', 'Junyu Dong']","[3, 4, 4]","['Clear rejection', 'Ok but not good enough - rejection', 'Ok but not good enough - rejection']","The proposed approach consists in a greedy layer wise initialization strategy for a deep MLP model, which is followed by global gradient-descent with dropout for fine-tuning. The initialization strategy uses a first randomly initialized sigmoid layer for dimensionality expansion followed by 2 sigmoid layers whose weights are initialized by Marginal Fisher Analysis (MFA) which learns a linear dimensionality reduction based on a neighborhood graph constructed using class label information (i.e. supervised dimensionality reduction). Output layer is a standard softmax layer. The approach is thus to be added to a growing list of heuristic layer-wise initialization schemes. The particular choice of initialization strategy, while reasonable, is not sufficiently well motivated in the paper relative to alternatives, and thus feels rather arbitrary. The paper lacks clarity in the description of the approach: MFA is poorly explained with undefined notations (in Eq. 4, what is A? It has not been properly defined); the precise use of alluded denoising in the model is also unclear (is there really training of an additional denoting objective, or just input corruption?). The question of the (arguably mild) inconsistency of applying a linear dimensionality reduction algorithm, that is trained without any sigmoid, and then passing its learned representation through a sigmoid is not even raised. This, in addition to the fact that sigmoid hidden layers are no longer commonly used (why did you not also consider using RELUs?). More importantly I suspect methodological problems with the experimental comparisons: the paper mentions using *default* values for learning-rate and momentum, and having (arbitrarily?) fixed epoch to 400 (no early stopping?) and L2 regularization to 1e-4 for some models. *All* hyper parameters should always be properly hyper-optimized using a validation set (or cross-validation) including early-stopping, and this separately for each model under comparison (ideally also including layer sizes). This is all the more important since you are considering smallish datasets, so that the various initialization strategies act mainly as different indirect regularization schemes: they thus need to be carefully tuned. This casts serious doubts as to the amount of hyper-parameter tuning (close to none?) that went into training the alternative models used for comparison. The Marginal Fisher Analysis dimensionality reduction initialization strategy may well offer advantages, but as it currently stands this paper doesn’t yet make a sufficiently convincing case for it, nor provide useful insights into the nature of the expected advantages. I would also suggest, for image inputs such as CIFAR10, to use the qualitative tool of showing the filters (back projected to input space) learned by the different initialization schemes under consideration, as this could help visually gain insight as to what sets methods apart.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[8, 15]","[14, 21]","[31, 101, 387]","[15, 41, 145]","[2, 18, 57]","[14, 42, 185]","The sentiment score is -60 because the review is predominantly critical. The reviewer points out several issues with the paper, including lack of clarity, insufficient motivation for the chosen approach, methodological problems, and doubts about the experimental comparisons. While the reviewer acknowledges that the approach may have advantages, they state that the paper doesn't make a convincing case for it. The politeness score is 20 because while the reviewer is critical, they maintain a professional tone throughout. They use phrases like 'I suspect' and 'I would suggest' rather than making blunt accusations. The reviewer also offers constructive suggestions for improvement, which adds to the politeness. However, some phrases like 'casts serious doubts' and 'feels rather arbitrary' prevent the score from being higher.",-60,20
Modelling Relational Time Series using Gaussian Embeddings,Reject,2017,"['Ludovic Dos Santos', 'Ali Ziat', 'Ludovic Denoyer', 'Benjamin Piwowarski', 'Patrick Gallinari']","[4, 4, 4]","['Ok but not good enough - rejection', 'Ok but not good enough - rejection', 'Ok but not good enough - rejection']","Because the authors did not respond to reviewer feedback, I am maintaining my original review score. ----- This paper proposes to model relational (i.e., correlated) time series using a deep learning-inspired latent variable approach: they design a flexible parametric (but not generative) model with Gaussian latent factors and fit it using a rich training objective including terms for reconstruction (of observed time series) error, smoothness in the latent state space (via a KL divergence term encouraging neighbor states to be similarly distributed), and a final regularizer that encourages related time series to have similar latent state trajectories. Relations between trajectories are hard coded based on pre-existing knowledge, i.e., latent state trajectories for neighboring (wind speed) base stations should be similar. The model appears to be fit using gradient simple descent. The authors propose several elaborations, including a nonlinear transition function (based on an MLP) and a reconstruction error term that takes variance into account. However, the model is restricted to using a linear decoder. Experimental results are positive but not convincing. Strengths: - The authors target a worthwhile and challenging problem: incorporating the modeling of uncertainty over hidden states with the power of flexible neural net-like models. - The idea of representing relationships between hidden states using KL divergence between their (distributions over) corresponding hidden states is clever. Combined with the Gaussian distribution over hidden states, the resulting regularization term is simple and differentiable. - This general approach -- focusing on writing down the problem as a neural network-like loss function -- seems robust and flexible and could be combined with other approaches, including variants of variational autoencoders. Weaknesses: - The presentation is a muddled, especially the model definition in Sec. 3.3. The authors introduce four variants of their model with different combinations of decoder (with and without variance term) and linear vs. MLP transition function. It appears that the 2,2 variant is generally better but not on all metrics and often by small margins. This makes drawing a solid conclusions difficult: what each component of the loss contributes, whether and how the nonlinear transition function helps and how much, how in practice the model should be applied, etc. I would suggest two improvements to the manuscript: (1) focus on the main 2,2 variant in Sec. 3.3 (with the hypothesis that it should perform best) and make the simpler variants additional *baselines* described in a paragraph in Sec. 4.1; (2) perform more thorough experiments with larger data sets to make a stronger case for the superiority of this approach. - The authors only allude to learning (with references to gradient descent and ADAM during model description) in this framework. Inference gets its one subsection but only one sentence that ends in an ellipsis (?). - It*s unclear what is the purpose of introducing the inequality in Eq. 9. - Experimental results are not convincing: given the size of the data, the differences vs. the RNN and KF baselines is probably not significant, and these aren*t particularly strong baselines (especially if it is in fact an RNN and not an LSTM or GRU). - The position of this paper is unclear with respect to variational autoencoders and related models. Recurrent variants of VAEs (e.g., Krishnan, et al., 2015) seem to achieve most of the same goals as far as uncertainty modeling is concerned. It seems like those could easily be extended to model relationships between time series using the simple regularization strategy used here. Same goes for Johnson, et al., 2016 (mentioned in separate question). This is a valuable research direction with some intriguing ideas and interesting preliminary results. I would suggest that the authors restructure this manuscript a bit, striving for clarity of model description similar to the papers cited above and providing greater detail about learning and inference. They also need to perform more thorough experiments and present results that tell a clear story about the strengths and weaknesses of this approach.","['3', '4', '5']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[3, 3, 15, 16, 30]","[9, 5, 20, 22, 36]","[13, 6, 147, 115, 382]","[9, 4, 88, 75, 256]","[3, 1, 40, 30, 54]","[1, 1, 19, 10, 72]","The sentiment score is -30 because while the reviewer acknowledges some strengths of the paper, they express significant concerns about the presentation, experimental results, and positioning of the work. The review begins with a neutral statement about maintaining the original score, then lists more weaknesses than strengths. The tone suggests the paper needs substantial improvements. The politeness score is 50 because the reviewer uses professional language throughout, acknowledges positive aspects, and provides constructive feedback. They use phrases like 'I would suggest' and 'The authors target a worthwhile and challenging problem', which maintain a respectful tone even while critiquing. However, some direct criticisms like 'The presentation is muddled' and 'Experimental results are not convincing' prevent the score from being higher.",-30,50
Multi-label learning with semantic embeddings,Reject,2017,"['Liping Jing', 'MiaoMiao Cheng', 'Liu Yang', 'Alex Gittens', 'Michael W. Mahoney']","[4, 4, 5]","['Ok but not good enough - rejection', 'Ok but not good enough - rejection', '5']","The paper proposes a semantic embedding based approach to multilabel classification. Conversely to previous proposals, SEM considers the underlying parameters determining the observed labels are low-rank rather than that the observed label matrix is itself low-rank. However, It is not clear to what extent the difference between the two assumptions is significant SEM models the labels for an instance as draws from a multinomial distribution parametrized by nonlinear functions of the instance features. As such, it is a neural network. The proposed training algorithm is slightly more complicated than vanilla backprop. The significance of the results compared to NNML (in particular on large datasets Delicious and EUrlex) is not very clear. The paper is well written and the main idea is clearly presented. However, the experimental results are not significant enough to compensate the lack of conceptual novelty.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[14, 11, 15, 7, 15]","[20, 17, 21, 13, 21]","[134, 14, 486, 46, 364]","[67, 9, 238, 16, 128]","[19, 0, 38, 23, 178]","[48, 5, 210, 7, 58]","The sentiment score is -30 because while the reviewer acknowledges some positive aspects ('The paper is well written and the main idea is clearly presented'), they express significant concerns about the novelty and significance of the work ('It is not clear to what extent the difference between the two assumptions is significant', 'The significance of the results... is not very clear', 'the experimental results are not significant enough to compensate the lack of conceptual novelty'). These criticisms outweigh the positive comments, resulting in a slightly negative overall sentiment. The politeness score is 50 because the reviewer uses professional and respectful language throughout, avoiding harsh or personal criticisms. They present their concerns in a constructive manner, acknowledging positive aspects before presenting critiques. The tone is neutral to slightly positive, maintaining a polite and academic discourse.",-30,50
Multi-label learning with the RNNs for Fashion Search,Reject,2017,['Taewan Kim'],"[4, 3, 3]","['Ok but not good enough - rejection', 'Clear rejection', 'Clear rejection']","The paper presents a large-scale visual search system for finding product images given a fashion item. The exploration is interesting and the paper does a nice job of discussing the challenges of operating in this domain. The proposed approach addresses several of the challenges. However, there are several concerns. 1) The main concern is that there are no comparisons or even mentions of the work done by Tamara Berg’s group on fashion recognition and fashion attributes, e.g., - “Automatic Attribute Discovery and Characterization from Noisy Web Data” ECCV 2010 - “Where to Buy It: Matching Street Clothing Photos in Online Shops” ICCV 2015, - “Retrieving Similar Styles to Parse Clothing, TPAMI 2014, etc It is difficult to show the contribution and novelty of this work without discussing and comparing with this extensive prior art. 2) There are not enough details about the attribute dataset and the collection process. What is the source of the images? Are these clean product images or real-world images? How is the annotation done? What instructions are the annotators given? What annotations are being collected? I understand data statistics for example may be proprietary, but these kinds of qualitative details are important to understand the contributions of the paper. How can others compare to this work? 3) There are some missing baselines. How do the results in Table 2 compare to simpler methods, e.g., the BM or CM methods described in the text? While the paper presents an interesting exploration, all these concerns would need to be addressed before the paper can be ready for publication.","['4', '4', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']",['skipped'],['skipped'],['skipped'],['skipped'],['skipped'],['skipped'],"The sentiment score is -50 because while the reviewer acknowledges some positive aspects ('interesting exploration', 'nice job discussing challenges'), they express several significant concerns that need to be addressed before publication. The overall tone suggests the paper needs substantial revisions. The politeness score is 50 because the reviewer uses respectful language throughout, acknowledging positives before presenting criticisms, and frames concerns as areas for improvement rather than outright flaws. They use phrases like 'it is difficult to show' and 'there are some missing baselines' rather than more accusatory language. The reviewer also offers specific suggestions for improvement, which is constructive and polite.",-50,50
Multiagent System for Layer Free Network,Reject,2017,"['Hiroki Kurotaki', 'Kotaro Nakayama', 'Yutaka Matsuo']","[3, 1, 2]","['Clear rejection', 'Trivial or wrong', 'Strong rejection']","The multiagent system is proposed as a generalization of neural network. The proposed system can be used with less restrictive network structures more efficiently by computing only those necessary computations in the graph. Unfortunately, I don*t find the proposed system different from the framework of artificial neural network. Although for today*s neural network structures are designed to have a lot of matrix-matrix multiplications, but it is not limited to have such architecture. In other words, the proposed multiagent system can be framed in the artificial neural network with more complicated layer/connectivity structures while considering each neuron as layer. The computation efficiency is argued among different sparsely connected denoising autoencoder in multiagent system framework only but the baseline comparison should be against the fully-connected neural network that employs matrix-matrix multiplication.","['3', '5', '5']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","['no_match', 13, 20]","['no_match', 18, 26]","['no match', 44, 252]","['no match', 32, 156]","['no match', 6, 52]","['no match', 6, 44]","The sentiment score is -50 because the reviewer expresses skepticism about the novelty and effectiveness of the proposed system. They state that they 'don't find the proposed system different from the framework of artificial neural network' and suggest that the comparison made is not adequate. This indicates a negative sentiment, though not extremely harsh. The politeness score is 20 because while the reviewer is critical, they use relatively polite language. They start with a neutral description of the proposal and use phrases like 'Unfortunately' to soften their criticism. They also provide specific suggestions for improvement, which is constructive. The language is not overly formal or deferential, but it maintains a professional tone throughout.",-50,20
Neural Causal Regularization under the Independence of Mechanisms Assumption,Reject,2017,"['Mohammad Taha Bahadori', 'Krzysztof Chalupka', 'Edward Choi', 'Robert Chen', 'Walter F. Stewart', 'Jimeng Sun']","[5, 4, 6]","['5', 'Ok but not good enough - rejection', 'Marginally above acceptance threshold']","This paper proposes to use a causality score to weight a sparsity regularizer. In that way, selected variables trade off between being causal and discriminative. The framework is primarily evaluated on a proprietary health dataset. While the dataset does give a good motivation to the problem setting, the paper falls a bit short for ICLR due to the lack of additional controlled experiments, relatively straightforward methodology (given the approach of Chalupka et al., arXiv Preprint, 2016, which is a more interesting paper from a technical perspective), and paucity of theoretical motivation. At the core of this paper, the approach is effectively to weight a sparsity regularizer so that *causal* variables (as determined by a separate objective) are more likely to be selected. This is generally a good idea, but we do not get a proper validation of this from the experiments as ground truth is absent. A theorem on identifiability of causal+discriminative variables from a data sample combined with adequate synthetic experiments would have probably been sufficient, for example, to push the paper towards accept from a technical perspective, but as it is, it is lacking in insight and reproducibility.","['4', '4', '5']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[7, 6, 12, 46, 11, 16]","[12, 9, 18, 50, 15, 22]","[34, 14, 102, 28, 37, 355]","[20, 4, 41, 16, 16, 180]","[12, 9, 56, 3, 9, 109]","[2, 1, 5, 9, 12, 66]","The sentiment score is -50 because the review is generally critical, pointing out several shortcomings of the paper. The reviewer states that the paper 'falls a bit short for ICLR' due to lack of experiments, straightforward methodology, and lack of theoretical motivation. They also mention that the paper is 'lacking in insight and reproducibility'. However, it's not entirely negative as the reviewer acknowledges some positive aspects, such as the good motivation provided by the dataset and the general idea being 'good'. The politeness score is 20 because while the reviewer is critical, they express their concerns in a professional and constructive manner. They use phrases like 'falls a bit short' instead of more harsh language, and provide specific suggestions for improvement. The tone is direct but not rude, maintaining a level of academic courtesy.",-50,20
New Learning Approach By Genetic Algorithm In A Convolutional Neural Network For Pattern Recognition,Reject,2017,"['Mohammad Ali Mehrolhassani', 'Majid Mohammadi']","[3, 3, 2]","['Clear rejection', 'Clear rejection', 'Strong rejection']","The authors seems to have proposed a genetic algorithm for learning the features of a convolutional network (LeNet-5 to be precise). The algorithm is validated on some version of the MNIST dataset. Unfortunately the paper is extremely hard to understand and it is not at all clear what the exact training algorithm is. Neither do the authors ever motivate why do such a training as opposed to the standard back-prop. What are its advantages/dis-advantages? Furthermore the experimental section is equally unclear. The authors seem to have merged the training and validation set of the MNIST dataset and use only a subset of it. It is not clear why is that the case and what subset they use. In addition, to the best of my understanding, the results reported are RMSE as opposed to classification error. Why is that the case? In short, the paper is extremely hard to follow and it is not at all clear what the training algorithm is and how is it better than standard way of training. The experimental section is equally confusing and unconvincing. Other comments: -- The figures still say LeCun-5 -- The legends of the plots are not in english. Hence I*m not sure what is going on there. -- The paper is riddled with typos and hard to understand phrasing.","['5', '5', '5']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","['no_match', 'skipped']","['no_match', 'skipped']","['no match', 'skipped']","['no match', 'skipped']","['no match', 'skipped']","['no match', 'skipped']","The sentiment score is -80 because the review is overwhelmingly negative. The reviewer states that the paper is 'extremely hard to understand', 'unclear', and 'unconvincing'. They criticize multiple aspects of the paper, including the training algorithm, motivation, experimental section, and overall clarity. There are no positive comments. The politeness score is -20 because while the reviewer doesn't use explicitly rude language, the tone is quite harsh and critical. They use phrases like 'extremely hard to follow' and 'riddled with typos' which, while not impolite, are quite blunt. The reviewer also doesn't offer any constructive feedback or suggestions for improvement, which contributes to the slightly negative politeness score.",-80,-20
OMG: Orthogonal Method of Grouping With Application of K-Shot Learning,Reject,2017,"['Haoqi Fan', 'Yu Zhang', 'Kris M. Kitani']","[4, 4, 4]","['Ok but not good enough - rejection', 'Ok but not good enough - rejection', 'Ok but not good enough - rejection']","This paper proposes a k-shot learning framework that can be used on existing pre-trained networks by grouping filters that produce similar activations. The grouped filters are learned together to address overfitting when only few training samples are available. The idea of the paper is interesting there are some encouraging results, but the current version doesn*t seem ready for publication: Performance: The method should be compared with other state-of-the-art k-shot learning methods (e.g., Matching Networks by Vinyals et al., 2016). It*s not clear how this method compares against them. Missing explanation: Experimental setting for k-shot learning should be more detailed. Measure: Accuracy difference does not look like a good idea for comparing the baseline method and the proposed one. Just raw accuracies would be fine. Many grammatical errors and inappropriate formatting of citations, such as: M. et al. (2011) ImageNet (Alex et al. (2012)) Judy et al. (2013): this reference appears three times in the reference section.","['4', '4', '5']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[-4, 10]","[1, 16]","[1, 287]","[0, 155]","[1, 109]","[0, 23]","The sentiment score is -30 because while the reviewer acknowledges the paper's interesting idea and encouraging results, they state that the paper 'doesn't seem ready for publication' and list several significant issues. This indicates a generally negative sentiment, though not extremely so. The politeness score is 20 because the reviewer uses relatively neutral language and offers constructive criticism. They begin with positive aspects before moving to critiques, which is a polite approach. However, the directness of some statements (e.g., 'Accuracy difference does not look like a good idea') prevents a higher politeness score. The reviewer also points out errors without using harsh language, maintaining a professional tone throughout.",-30,20
PREDICTION OF POTENTIAL HUMAN INTENTION USING SUPERVISED COMPETITIVE LEARNING,Reject,2017,"['Masayoshi Ishikawa', 'Mariko Okude', 'Takehisa Nishida & Kazuo Muto']","[2, 2, 4]","['Strong rejection', 'Strong rejection', 'Ok but not good enough - rejection']","This paper introduces a neural network architecture and training procedure for predicting the speed of a vehicle several seconds into the future based on video and vehicle state input. The architecture allows several RNNs to compete to make the best predictions, with only the best prediction receiving back propagation training at each time step. Preliminary experimental results show that this scheme can yield reduced prediction error. It is not clear how the best-performing RNN is chosen for each time point at test time. That is, how is the “integrated prediction” obtained in Fig. 7? Is the prediction the one with minimum error over all of the output layers? If so, this means the prediction cannot be made until you already know the value to be predicted. It seems possible that a larger generic RNN might be able to generate accurate predictions. If I understand correctly, the competitive architectures have many more parameters than the baseline. Is the improved performance here due to the competitive scheme, or just a larger model? A large amount of additional work is required to sustain the claim that this scheme is successfully extracting driver ‘intentions’. It would be interesting to see if the scheme, suitably extended, can automatically infer the intention to stop at a stop sign vs slowing but not stopping due to a car in front, say, or to pass a car vs simply changing lanes. Adding labels to the dataset may enable this comparison more clearly. More generally, the intention of the driver seems more related to the goals they are pursuing at the moment; there is a fair amount of work in inverse reinforcement learning that examines this problem (some of it in the context of driving style as well).","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[-4, 'no_match', 'no_match']","[1, 'no_match', 'no_match']","[1, 'no match', 'no match']","[1, 'no match', 'no match']","[0, 'no match', 'no match']","[0, 'no match', 'no match']","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some positive aspects of the paper (e.g., 'Preliminary experimental results show that this scheme can yield reduced prediction error'), they raise several critical questions and point out areas that need significant improvement. The reviewer expresses skepticism about the effectiveness of the proposed method compared to simpler alternatives and suggests that 'A large amount of additional work is required' to support the paper's claims.

The politeness score is moderately positive (50) because the reviewer maintains a professional and constructive tone throughout. They use neutral language to express their concerns (e.g., 'It is not clear how...', 'It seems possible that...') rather than harsh criticism. The reviewer also offers suggestions for improvement and points out potentially interesting extensions of the work, which demonstrates a helpful attitude.",-20,50
Representing inferential uncertainty in deep neural networks through sampling,Reject,2017,"['Patrick McClure', 'Nikolaus Kriegeskorte']","[4, 4, 5]","['Ok but not good enough - rejection', 'Ok but not good enough - rejection', '5']","This paper investigates whether the variational inference interpretation of dropout, as introduced in [Gal & Ghahramani (2016), and Kingma et al (2015)], can lead to good estimates of mode uncertainty outside of the training distribution. This is an area of research that indeed warrants more experimental investigation. One very interesting finding is that MC integration leads to much calibration, thus probably much better out-of-sample prediction, than the more usual. Critique: - As explained in Kingma et al (2015), when using continuous posterior distributions over the weights, the dropout rate can be optimized, leading to better regularization. While the paper is cited in the introduction, this adaptive form of dropout is missing from experiments, without clarification. - Only the dropout rate p=0.5 was used across experiments, while the optimal rate is problem dependent, as found by earlier published work. - No new ideas are presented, and the analysis in the paper is quite limited. As it stands, this would be more appropriate for a workshop.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[7, 17]","[13, 23]","[18, 48]","[7, 3]","[8, 14]","[3, 31]","The sentiment score is -50 because while the reviewer acknowledges the paper's interesting findings and the importance of the research area, they also provide significant critiques. The reviewer points out missing experiments, limited analysis, and suggests the paper might be more suitable for a workshop rather than a full publication. This mix of positive and negative comments, with a lean towards criticism, justifies a moderately negative score. The politeness score is 0 (neutral) because the reviewer's language is neither particularly polite nor rude. They present their critiques in a straightforward, matter-of-fact manner without using overly harsh language or excessive praise. The review maintains a professional tone throughout, focusing on the content rather than using emotionally charged language.",-50,0
Revisiting Denoising Auto-Encoders,Reject,2017,['Luis Gonzalo Sanchez Giraldo'],"[4, 4, 5]","['Ok but not good enough - rejection', 'Ok but not good enough - rejection', '5']","The paper proposes a modified DAE objective where it is the mapped representation of the corrupted input that is pushed closer to the representation of the uncorrupted input. This thus borrows from both denoising (DAE) for the stochasticity and from the contractive (CAE) auto-encoders objectives (which the paper doesn’t compare to) for the representational closeness, and as such appears rather incremental. In common with the CAE, a collapse of the representation can only be avoided by additional external constraints, such as tied weights, batch normalization or other normalization heuristics. While I appreciates that the authors added a paragraph discussing this point and the usual remediations after I had raised it in an earlier question, I think it would deserve a proper formal treatment. Note that such external constraints do not seem to arise from the information-theoretic formalism as articulated by the authors. This casts doubt regarding the validity or completeness of the proposed formal motivation as currently exposed. What the extra regularization does from an information-theoretic perspective remains unclearly articulated (e.g. interpretation of lambda strength?). On the experimental front, empirical support for the approach is very weak: few experiments on synthetic and small scale data. The modified DAE*s test errors on MNIST are larger than those of Original DAE all the time expect for one precise setting of lambda, and then the original DAE performance is still within the displayed error-bar of the modified DAE. So, it is unclear whether the improvement is actually statistically significant.","['5', '4', '4']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']",[9],[15],[32],[16],[9],[7],"The sentiment score is -60 because the review is predominantly negative. The reviewer describes the paper as 'rather incremental', expresses doubt about the validity of the formal motivation, and states that empirical support is 'very weak'. The only positive aspect is the acknowledgment that the authors added a paragraph addressing a previous concern. The politeness score is 20 because while the reviewer is critical, they use professional language and avoid personal attacks. They use phrases like 'I appreciates that the authors added...' which shows some politeness. However, the overall tone is more neutral than overtly polite, hence the relatively low positive score.",-60,20
Rotation Plane Doubly Orthogonal Recurrent Neural Networks,Reject,2017,"['Zoe McCarthy', 'Andrew Bai', 'Xi Chen', 'Pieter Abbeel']","[4, 4, 5]","['Ok but not good enough - rejection', 'Ok but not good enough - rejection', '5']","My main objection with this work is that it operates under a hypothesis (that is becoming more and more popular in the literature) that all we need is to have gradients flow in order to solve long term dependency problems. The usual approach is then to enforce orthogonal matrices which (in absence of the nonlinearity) results in unitary jacobians, hence the gradients do not vanish and do not explode. However this hypothesis is taken for granted (and we don*t know it is true yet) and instead of synthetic data, we do not have any empirical evidence that is strong enough to convince us the hypothesis is true. My own issues with this way of thinking is: a) what about representational power; restricting to orthogonal matrices it means we can not represent the same family of functions as before (e.g. we can*t have complex attractors and so forth if we run the model forward without any inputs). You can only get those if you have eigenvalues larger than 1. It also becomes really hard to deal with noise (since you attempt to preserve every detail of the input, or rather every part of the input affects the output). Ideally you would want to preserve only what you need for the task given limited capacity. But you can*t learn to do that. My issue is that everyone is focused on solving this preserved issue without worrying of the side-effects. I would like one of these papers going for jacobians having eigenvalues of 1 show this helps in realistic scenarios, on complex datasets.","['4', '4', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[6, -3, 16]","[10, 2, 22]","[17, 3, 610]","[9, 0, 291]","[5, 2, 293]","[3, 1, 26]","The sentiment score is -60 because the reviewer expresses significant objections to the work, questioning its fundamental hypothesis and pointing out several issues. The reviewer uses phrases like 'main objection,' 'taken for granted,' and 'My own issues,' indicating a largely negative view of the paper. However, it's not entirely negative as the reviewer acknowledges the popularity of the approach in literature. The politeness score is 20 because while the reviewer is critical, they express their concerns in a professional and academic manner. They use phrases like 'My main objection' and 'My own issues' to frame their criticism personally rather than as absolute statements. The reviewer also provides detailed explanations for their concerns, which is a respectful way to give feedback. However, the tone is not overtly polite, maintaining a neutral academic style, hence the slightly positive but not high score.",-60,20
Rule Mining in Feature Space,Reject,2017,"['Stefano Teso', 'Andrea Passerini']","[4, 3, 4]","['Ok but not good enough - rejection', 'Clear rejection', 'Ok but not good enough - rejection']","This paper aims to mine explicit rules from KB embedding space, and casts it into a sparse reconstruction problem. Experiments demonstrate its ability of extracting reasonable rules on a few link prediction datasets. The solution part sounds plausible. However, it confuses me that why we need to mine rules from learned KB embeddings. - It is still unclear what information these KB embeddings encode and it looks strange that we aim to learn rules including negation / disjunction from them. - If the goal is to extract useful rules (for other applications), it is necessary to compare it to “graph random walk” (http://rtw.ml.cmu.edu/papers/lao-emnlp11.pdf) which could learn rules from KB graph directly. - As there is only one KNN baseline, the experimental results seem pretty weak. At the least, it is necessary to show the original precision / recall of RESCAL, together with the proposed rule mining approach (with different max length), so we know how much the current information the current rule miner could recover. In addition, the four datasets are all very small. Would it be able to scale it to WordNet or Freebase? [Minor comments] “Relational embedding” and “relation embedding” are used mixedly throughout the paper. I am not sure if they are well-defined terms (it is better to cite relevant paper).","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[8, 17]","[14, 23]","[84, 146]","[35, 59]","[35, 41]","[14, 46]","The sentiment score is -50 because while the reviewer acknowledges some positive aspects ('sounds plausible', 'demonstrate its ability'), they express significant concerns and confusion about the paper's approach and experimental results. The reviewer questions the fundamental premise of mining rules from KB embeddings and points out several weaknesses in the methodology and comparisons. The politeness score is 20 because the reviewer uses relatively neutral language and phrases criticisms as questions or suggestions ('it confuses me', 'it is necessary to', 'Would it be able to') rather than direct attacks. However, the overall tone is still critical and not overly polite.",-50,20
Sample Importance in Training Deep Neural Networks,Reject,2017,"['Tianxiang Gao', 'Vladimir Jojic']","[2, 7, 3]","['Strong rejection', 'Good paper, accept', 'Clear rejection']","The paper proposes a new criterion (sample importance) to study the impact of samples during the training of deep neural networks. This criterion is not clearly defined (the term phi^t_{i,j} is never defined, only phi^t_i is defined; Despite the unclear definition, it is understood that sample importance is the squared l2 norm of the gradient for a sample i and at time t strangely scaled by the squared learning rate (the learning rate should have nothing to do with the importance of a sample in this context). The paper presents experiments on the well known MNIST and CIFAR datasets with correspondingly appropriate network architectures and choice of hyper-parameters and initialisations. The size of the hidden layers is a bit small for Mnist and very small for CIFAR (this could explain the very poor performance in figure 6: 50% error on CIFAR) The study of the evolution of sample importance during training depending on layers seems to lead to trivial conclusions - “the overall sample importance is different under different epochs” => yes the norm of the gradient is expected to vary - “Output layer always has the largest average sample importance per parameter, and its contribution reaches the maximum in the early training stage and then drops” => 1. yes since the gradient flows backwards, the gradient is expected to be stronger for the output layer and it is expected to become more diffuse as it propagates to lower layers which are not stable. As learning progresses one would expect the output layer to have progressively smaller gradients. 2. the norm of the gradient depends on the scaling of the variables The question of Figure 4 is absurd “Is Sample Importance the same as Negative log-likelihood of a sample?”. Of course not. The results are very bad on CIFAR which discredits the applicability of those results. On Mnist performance is not readable (figure 7): Error rate should only be presented between 0 and 10 or 20% Despite these important issues (there are others), the paper manages to raises some interesting things: the so-called easy samples and hard samples do seem to correspond (although the study is very preliminary in this regard) to what would intuitively be considered easy (the most representative/canonical samples) and hard (edge cases) samples. Also the experiments are very well presented.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[7, 14]","[12, 17]","[24, 27]","[11, 19]","[10, 3]","[3, 5]","The sentiment score is -50 because the review is predominantly critical, pointing out several issues with the paper such as unclear definitions, trivial conclusions, and poor performance on CIFAR dataset. However, it's not entirely negative as it acknowledges some interesting aspects and well-presented experiments, preventing it from being at the extreme negative end. The politeness score is 0 (neutral) because the reviewer maintains a professional tone without being overtly polite or rude. The criticism is direct but not personal, using phrases like 'The question... is absurd' or 'The results are very bad' which are blunt but not impolite in a scientific context. The reviewer also balances criticism with some positive remarks at the end.",-50,0
Sequence generation with a physiologically plausible model of handwriting and Recurrent Mixture Density Networks,Reject,2017,"['Daniel Berio', 'Memo Akten', 'Frederic Fol Leymarie', 'Mick Grierson', 'Réjean Plamondon']","[3, 3, 3]","['Clear rejection', 'Clear rejection', 'Clear rejection']","This paper has no machine learning algorithmic contribution: it just uses the the same combination of LSTM and bivariate mixture density network as Graves, and the detailed explanation in the appendix even misses one key essential point: how are the Gaussian parameters obtained as a transformation of the output of the LSTM. There are also no numerical evaluation suggesting that the algorithm is some form of improvement over the state-of-the-art. So I do not think such a paper is appropriate for a conference like ICLR. The part describing the handwriting tasks and the data transformation is well written and interesting to read, it could be valuable work for a conference more focused on handwriting recognition, but I am no expert in the field.","['3', '5', '3']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is fairly confident that the evaluation is correct']","[3, 2, 26, 11, 36]","[9, 5, 31, 16, 42]","[10, 7, 67, 47, 144]","[7, 2, 45, 32, 60]","[2, 5, 6, 11, 6]","[1, 0, 16, 4, 78]","The sentiment score is -60 because the reviewer expresses significant criticism of the paper's lack of novel algorithmic contribution and its unsuitability for the ICLR conference. However, they do acknowledge some positive aspects, such as the well-written description of handwriting tasks. The politeness score is 20 because while the reviewer is direct in their criticism, they maintain a professional tone and offer some constructive feedback. They also acknowledge their lack of expertise in a specific area, which shows humility. The language used is not overtly rude, but rather matter-of-fact in presenting the paper's shortcomings.",-60,20
Significance of Softmax-Based Features over Metric Learning-Based Features,Reject,2017,"['Shota Horiguchi', 'Daiki Ikami', 'Kiyoharu Aizawa']","[5, 7, 4]","['5', 'Good paper, accept', 'Ok but not good enough - rejection']","I agree with the other two reviewers that it is an interesting topic to investigate the feature learned by DML. For classification task though, I feel intuitively softmax should have advantages over distance metric learning method because the loss function is designed to assign the correct class for the given image. All the experimental results show that the softmax features work better than Rippel et al DML method. However, does it support the claim that softmax-based features work much better than DML learned features? I have doubts on this claim. Also the experiments are a little bit misleading. What is vanilla googleNet softmax finetuned results? It seems it is not Rippel et al. (softmax prob) result. I am wondering whether the improvement comes from a) using retrieval (nearest neighbor) for classification or b) adding a new layer on top of pool5 or c) L2 normalization of the features. It is not clear to me at all. It appears to me the comparison is not apple vs apple between the proposed method and Rippel et al. It would be great if we know adding feature reduction or adding another layer on top of pool5 can improve finetued softmax result. However, I am not sure what is the biggest contributing factor to the superior results. Before getting more clarifications from the authors, I lean toward rejection.","['4', '4', '5']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[2, 1, 32]","[8, 7, 38]","[60, 24, 457]","[28, 14, 339]","[28, 9, 45]","[4, 1, 73]","The sentiment score is -50 because the reviewer expresses significant doubts about the paper's claims and methodology, leaning towards rejection. While they acknowledge the topic is interesting, they question the main conclusions and experimental setup. The politeness score is 20 because the reviewer uses generally polite language, such as 'I agree,' 'It would be great if,' and 'I am wondering,' while still clearly expressing their concerns. They avoid harsh or rude language, instead framing their criticisms as questions or areas needing clarification. However, the overall tone is more critical than overtly polite, hence the moderate positive score.",-50,20
Skip-graph: Learning graph embeddings with an encoder-decoder model,Reject,2017,"['John Boaz Lee', 'Xiangnan Kong']","[5, 5, 6, 7]","['5', '5', 'Marginally above acceptance threshold', 'Good paper, accept']","This paper proposes an unsupervised graph embedding learning method based on random walk and skip-thought model. They show promising results compared to several competitors on four chemical compound datasets. Strength: 1, The idea of learning the graph embedding by applying skip-thought model to random walk sequences is interesting. 2, The paper is well organized. Weakness: 1, As the current datasets are small (e.g., the average number of nodes per graph is around 30), it would be great to explore larger graph datasets to further investigate the method. 2, Comparisons with recent work like LINE and node2vec are missing. You can compare them easily by applying the same aggregation strategy to their node embeddings. Detailed Questions: 1, The description about how to split the random walk sequence into 3 sub-sequences is missing. Also, the line “l_min >= (n_k - 1), … >= l_max” in section 2.2.2 is a mistake. 2, Can you provide the standard deviations of the 5-fold cross validation in Table 2? I’m curious about how stable the algorithm is.","['4', '4', '1', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer*s evaluation is an educated guess', 'The reviewer is fairly confident that the evaluation is correct']","[7, 8]","[12, 14]","[28, 154]","[12, 104]","[9, 31]","[7, 19]","The sentiment score is 50 (slightly positive) because the reviewer begins by highlighting the paper's strengths, such as the interesting idea and good organization. However, they also point out weaknesses and areas for improvement, balancing the positive aspects. The politeness score is 75 (quite polite) because the reviewer uses respectful language throughout, framing criticisms as suggestions (e.g., 'it would be great to explore') and asking questions rather than making demands. They also acknowledge the paper's strengths before discussing weaknesses, which is a polite approach. The reviewer's tone is constructive and professional, without any harsh or rude language.",50,75
Spatio-Temporal Abstractions in Reinforcement Learning Through Neural Encoding,Reject,2017,"['Nir Baram', 'Tom Zahavy', 'Shie Mannor']","[4, 4, 4]","['Ok but not good enough - rejection', 'Ok but not good enough - rejection', 'Ok but not good enough - rejection']","The paper starts by pointing out the need for methods that perform both state and temporal representation learning for RL and which allow gaining insight into what is being learned (perhaps in order to allow a human operator to intervene if necessary). This is a very important goal from a practical point of view, and it is great to see research in this direction. For this reason, I would like to encourage the authors to pursue this further. However, I am not at all convinced that the current incarnation of this work is the right answer. Part of the issues are more related to presentation, part may require rethinking. In order to get the *interpretability*, the authors opt for some fairly specific ways of performing abstraction. For example, their skills always start In a single skill initiation state, and likewise end in one state. This seems unnecessarily restrictive, and it is not clear why this restriction is needed (other than convenience). Similarly, clustering is the basis for forming the higher level states, and there is a specific kind of clustering used here. Again, it is not clear why this has to be done via clustering as opposed to other methods. Ensuring temporal coherence in the particular form employed also seems restrictive. There is a reference to supplementary material where some of these choices are explained, but I could not find this in the posted version of the paper. The authors should either explain clearly why these specific choices are necessary, or (even better) try to think if they can be relaxed while still keeping interpretability. From a presentation point of view, the paper would benefit from formal definitions of AMDP and SAMDP, as well as from formal descriptions of the algorithms employed in constructing these representations (eg Bellman equations for the models, and update rules for the algorithms learning them). While intuitions are given, the math is not precisely stated. The overhead of constructing an SAMDP (computation time and space) should be clarified as well. The experiments are well carried out and it is nice to have both gridworld experiments, where visualization are easy to perform and understand, as well as Atari games (gridworld still have their place despite what other reviewers might say). The results are positive, but because the proposed approach has many moving parts which rely on specific choices, significance and general ease of use are unclear at this point. Perhaps having the complete supplementary would have helped in this respect. Small comment: The two lines after Eq 2 contain typos in the notation and a wrong sign in the equation.","['5', '5', '4']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[2, 4, 18]","[6, 10, 24]","[10, 61, 503]","[3, 23, 231]","[7, 37, 192]","[0, 1, 80]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the importance of the research direction and encourages further pursuit, they express significant doubts about the current approach. Phrases like 'I am not at all convinced' and 'Part of the issues' indicate a critical stance. However, the reviewer also offers constructive feedback and recognizes positive aspects, which prevents the score from being more negative. The politeness score is moderately positive (60) due to the reviewer's use of respectful language throughout. They start by acknowledging the importance of the work, use phrases like 'I would like to encourage the authors,' and offer constructive criticism. The tone remains professional and courteous, even when pointing out issues. The reviewer also compliments aspects of the work, such as the well-carried out experiments. The polite phrasing of criticisms and the balance of positive and negative comments contribute to the politeness score.",-20,60
SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size,Reject,2017,"['Forrest N. Iandola', 'Song Han', 'Matthew W. Moskewicz', 'Khalid Ashraf', 'William J. Dally', 'Kurt Keutzer']","[5, 7, 7]","['5', 'Good paper, accept', 'Good paper, accept']","Strengths ?-- An interesting proposal for a smaller CNN architecture designed for embedded CNN applications. ?-- Balanced exploration of CNN macroarchitecture and microarchitecture with fire modules. ?-- x50 less memory usage than AlexNet, keeping similar accuracy ?-- strong experimental results Weaknesses ?--Would be nice to test Sqeezenet on multiple tasks ?--lack of insights and rigorous analysis into what factors are responsible for the success of SqueezeNet. For example, how are ResNet and GoogleNet connected to the current architecture? Another old paper (Analysis of correlation structure for a neural predictive model with application to speech recognition, Neural Networks, 1994) also showed that the “by-pass” architecture by mixing linear and nonlinear prediction terms improves long term dependency in NN based on rigorous perturbation analysis. Can the current work be placed more rigorously on theoretical analysis?","['4', '3', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[7, 3, 17, 7, 33, 31]","[10, 9, 20, 8, 39, 37]","[35, 150, 24, 16, 274, 389]","[20, 66, 16, 6, 174, 204]","[15, 67, 7, 6, 23, 120]","[0, 17, 1, 4, 77, 65]","The sentiment score is 50 (moderately positive) because the review starts by listing several strengths of the proposal, including its interesting approach, balanced exploration, and strong experimental results. However, it also mentions some weaknesses, which prevents it from being highly positive. The politeness score is 20 (slightly polite) because the reviewer uses neutral language and frames the weaknesses as suggestions for improvement ('Would be nice to...', 'Can the current work be...') rather than direct criticisms. The reviewer also acknowledges the positive aspects before mentioning areas for improvement, which is a polite approach. However, the language isn't overtly polite or formal, hence the moderate score.",50,20
Submodular Sum-product Networks for Scene Understanding,Reject,2017,"['Abram L. Friesen', 'Pedro Domingos']","[5, 4, 4]","['5', 'Ok but not good enough - rejection', 'Ok but not good enough - rejection']","The paper discusses sub modular sum-product networks as a tractable extension for classical sum-product networks. The proposed approach is evaluated on semantic segmentation tasks and some early promising results are provided. Summary: ——— I think the paper presents a compelling technique for hierarchical reasoning in MRFs but the experimental results are not yet convincing. Moreover the writing is confusing at times. See below for details. Quality: I think some of the techniques could be described more carefully to better convey the intuition. Clarity: Some of the derivations and intuitions could be explained in more detail. Originality: The suggested idea is great. Significance: Since the experimental setup is somewhat limited according to my opinion, significance is hard to judge at this point in time. Detailed comments: ——— 1. I think the clarity of the paper would benefit significantly from fixes to inaccuracies. E.g., alpha-expansion and belief propagation are not `scene-understanding algorithms’ but rather approaches for optimizing energy functions. Computing the MAP state of an SSPN in time sub-linear in the network size seems counterintuitive because it means we are not allowed to visit all the nodes in the network. The term `deep probabilistic model’ should probably be defined. The paper states that InferSSPN computes `the approximate MAP state of the SSPN (equivalently, the optimal parse of the image)’ and I’m wondering how the `approximate MAP state* can be optimal. Etc. 2. Albeit being formulated for scene understanding tasks, no experiments demonstrate the obtained results of the proposed technique. To assess the applicability of the proposed approach a more detailed analysis is required. More specifically, the technique is evaluated on a subset of images which makes comparison to any other approach impossible. According to my opinion, either a conclusive experimental evaluation using, e.g., IoU metric should be given in the paper, or a comparison to publicly available results is possible. 3. To simplify the understanding of the paper a more intuitive high-level description is desirable. Maybe the authors can even provide an intuitive visualization of their approach.","['4', '3', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[8, -2]","[14, 1]","[24, 4]","[14, 1]","[10, 3]","[0, 0]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the paper presents a 'compelling technique' and has a 'great' suggested idea, they also express concerns about the experimental results not being convincing, the writing being confusing, and the need for more detailed analysis. The overall tone suggests more improvements are needed than positive aspects. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, offering constructive criticism and suggestions for improvement rather than harsh criticism. They use phrases like 'I think' and 'according to my opinion' which soften their critiques. The reviewer also balances negative feedback with positive comments, acknowledging the potential of the work while pointing out areas for improvement.",-20,50
Taming the waves: sine as activation function in deep neural networks,Reject,2017,"['Giambattista Parascandolo', 'Heikki Huttunen', 'Tuomas Virtanen']","[4, 4, 4]","['Ok but not good enough - rejection', 'Ok but not good enough - rejection', 'Ok but not good enough - rejection']","Summary: In this paper, the authors explore the advantages/disadvantages of using a sin activation function. They first demonstrate that even with simple tasks, using sin activations can result in complex to optimize loss functions. They then compare networks trained with different activations on the MNIST dataset, and discover that the periodicity of the sin activation is not necessary for learning the task well. They then try different algorithmic tasks, where the periodicity of the functions is helpful. Pros: The closed form derivations of the loss surface were interesting to see, and the clarity of tone on the advantages *and* disadvantages was educational. Cons: Seems like more of a preliminary investigation of the potential benefits of sin, and more evidence (to support or in contrary) is needed to conclude anything significant -- the results on MNIST seem to indicate truncated sin is just as good, and while it is interesting that tanh maybe uses more of the saturated part, the two seem relatively interchangeable. The toy algorithmic tasks are hard to conclude something concrete from.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[2, 24, 18]","[7, 30, 24]","[29, 277]","[15, 166]","[13, 68]","[1, 43]","The sentiment score is 50 (slightly positive) because the reviewer acknowledges both pros and cons of the paper. They mention 'interesting' and 'educational' aspects, which are positive, but also point out limitations and the need for more evidence. The tone is balanced, leaning towards constructive criticism rather than outright praise or criticism. The politeness score is 75 (quite polite) because the reviewer uses respectful language throughout, acknowledging the authors' work positively ('interesting to see', 'clarity of tone... was educational') while offering constructive feedback. The critique is presented in a professional manner without harsh or dismissive language. The reviewer also uses phrases like 'seems like' and 'hard to conclude' which softens the criticism, maintaining a polite tone.",50,75
Ternary Weight Decomposition and Binary Activation Encoding for Fast and Compact Neural Network,Reject,2017,"['Mitsuru Ambai', 'Takuya Matsumoto', 'Takayoshi Yamashita', 'Hironobu Fujiyoshi']","[6, 5, 4]","['Marginally above acceptance threshold', '5', 'Ok but not good enough - rejection']","This paper addresses to reduce test-time computational load of DNNs. Another factorization approach is proposed and shows good results. The comparison to the other methods is not comprehensive, the paper provides good insights.","['3', '3', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[9, 15, 11, 20]","[14, 19, 17, 26]","[20, 45, 141, 174]","[9, 33, 104, 137]","[5, 3, 22, 16]","[6, 9, 15, 21]","The sentiment score is 50 (slightly positive) because the review acknowledges the paper's contribution to reducing computational load of DNNs and mentions 'good results' and 'good insights'. However, it's not overwhelmingly positive as it notes the comparison to other methods is not comprehensive. The politeness score is 0 (neutral) because the language is direct and factual without being particularly polite or rude. The reviewer states observations without using overtly positive or negative language, maintaining a professional, neutral tone throughout the brief review.",50,0
The Variational Walkback Algorithm,Reject,2017,"['Anirudh Goyal', 'Nan Rosemary Ke', 'Alex Lamb', 'Yoshua Bengio']","[4, 5, 4]","['Ok but not good enough - rejection', '5', 'Ok but not good enough - rejection']","The authors present a method for training probabilistic models by maximizing a stochastic variational-lower-bound-type objective. Training involves sampling and then learning a transition-based inference to *walk back* samples to the data. Because of its focus on transitions, it can be used to learn a raw transition operator rather than purely learning an energy-based model. The objective is intuitively appealing because of its similarity to previous successful but less principled training methods for MRFs like Contrastive Divergence. The idea for the algorithm is appealing, and it looks like it could find a nice place in the literature. However, the submission in its current form is not yet ready for publication. Experiments are qualitative and the generated samples are not obviously indicative of a high model quality. As pointed out elsewhere, the mathematical analysis does not currently demonstrate tightness of the variational bound in the case of a learned transition operator. More evaluation using e.g. annealed importance sampling to estimate held-out likelihoods is necessary. Assuming that the analysis can be repaired, the ability to directly parametrize a transition operator, an interesting strength of this method, should be explored in further experiments and contrasted with the more standard energy-based modeling. This looks like a promising idea, and other reviews and questions have already raised some important technical points which should help strengthen this paper for future submission.","['4', '5', '5']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[4, 7, 6, 30]","[10, 13, 12, 36]","[102, 61, 64, 977]","[40, 22, 22, 405]","[61, 38, 38, 456]","[1, 1, 4, 116]","The sentiment score is slightly positive (20) because the reviewer acknowledges the potential of the method, calling it 'appealing' and 'promising'. However, they also point out significant shortcomings, stating it's 'not yet ready for publication'. This mix of positive potential and current limitations results in a mildly positive sentiment. The politeness score is moderately high (60) as the reviewer uses respectful language throughout, offering constructive criticism and suggestions for improvement without harsh or dismissive phrasing. They acknowledge the method's strengths while diplomatically pointing out areas needing work, maintaining a professional and courteous tone.",20,60
Training Long Short-Term Memory With Sparsified Stochastic Gradient Descent,Reject,2017,"['Maohua Zhu', 'Minsoo Rhu', 'Jason Clemons', 'Stephen W. Keckler', 'Yuan Xie']","[4, 5, 4]","['Ok but not good enough - rejection', '5', 'Ok but not good enough - rejection']",The findings of applying sparsity in the backward gradients for training LSTMs is interesting. But the paper seems incomplete without the proper experimental justification. Only the validation loss is reported which is definitely insufficient. Proper testing results and commonly reported evaluation criterion needs to be included to support the claim of no degradation when applying the proposed sparsity technique. Also actual justification of the gains in terms of speed and efficiency would make the paper much stronger.,"['4', '3', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[7, 9, 7, 26, 18]","[10, 15, 12, 31, 24]","[14, 60, 22, 162, 573]","[9, 33, 15, 111, 341]","[3, 20, 4, 14, 75]","[2, 7, 3, 37, 157]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges the interesting findings, they express several concerns about the paper's completeness and lack of proper experimental justification. The reviewer suggests that the paper is 'incomplete' and that more data and justification are needed, indicating a somewhat negative sentiment towards the current state of the paper. However, the criticism is not harsh, hence the score is only mildly negative. The politeness score is moderately positive (50) because the reviewer uses respectful language throughout. They begin by acknowledging the interesting aspects of the work and frame their criticisms as suggestions for improvement rather than outright criticisms. Phrases like 'would make the paper much stronger' indicate a constructive approach. The reviewer maintains a professional tone without using any rude or overly critical language.",-20,50
TreNet: Hybrid Neural Networks for Learning the Local Trend in Time Series,Reject,2017,"['Tao Lin', 'Tian Guo', 'Karl Aberer']","[6, 4, 5]","['Marginally above acceptance threshold', 'Ok but not good enough - rejection', '5']","Updated review: the authors did an admirable job of responding to and incorporating reviewer feedback. In particular, they put a lot of effort into additional experiments, even incorporating a new and much stronger baseline (the ConvNet -> LSTM baseline requested by multiple reviewers). I still have two lingering concerns previously stated -- that each model*s architecture (# hidden units, etc.) should be tuned independently and that a pure time series forecasting baselines (without the trend preprocessing) should be tried. I*m going to bump up my score from a clear rejection to a borderline. ----- This paper is concerned with time series prediction problems for which the prediction targets include the slope and duration of upcoming local trends. This setting is of great interest in several real world problem settings (e.g., financial markets) where decisions (e.g., buy or sell) are often driven by local changes and trends. The primary challenge in these problems is distinguishing true changes and trends (i.e., a downturn in share price) from noise. The authors tackle this with an interesting hybrid architecture (TreNet) with four parts: (1) preprocessing to extract trends, (2) an LSTM that accepts those trends as inputs to ostensibly capture long term dependencies, (3) a ConvNet that accepts a local window of raw data as its input at each time step, and (4) a higher *feature fusion* (i.e., dense) layer to combine the LSTM*s and ConvNet*s outputs. On three univariate time series data sets, the TreNet outperforms the competing baselines including those based on its constituent parts (LSTM + trend inputs, CNN). Strengths: - A very interesting problem setting that can plausibly be argued to differ from other sequential modeling problems in deep learning (e.g., video classification). This is a nice example of fairly thoughtful task-driven machine learning. - Accepting the author*s assumptions as true for the moment, the proposed architecture seems intuitive and well-designed. Weaknesses: - Although this is an interesting problem setting (decisions driven by trends and changes), the authors did not make a strong argument for why they formulated the machine learning task as they did. Trend targets are not provided from *on high* (by data oracle) but extracted from raw data using a deterministic algorithm. Thus, one could just easily formulate this as plain time series forecasting problem in which we forecast the next 100 steps and then apply the trend extractor to convert those predictions into a trend. If the forecasts are accurate, so will be the extracted trends. - The proposed architecture, while interesting, is not justified, in particular the choice to feed the extracted trends and raw data into separate LSTM and ConvNet layers that are only combined at the end by a shallow MLP. An equally straightforward but more intuitive choice would have been to feed the output of the ConvNet into the LSTM, perhaps augmented by the trend input. Without a solid rationale, this unconventional choice comes across as arbitrary. - Following up on that point, the raw->ConvNet->LSTM and {raw->ConvNet,trends}->LSTM architectures are natural baselines for experiments. - The paper presupposes, rather than argues, the value of the extracted trends and durations as inputs. It is not unreasonable to think that, with enough training data, a sufficiently powerful ConvNet->LSTM architecture should be able to learn to detect these trends in raw data, if they are predictive. - Following up on that point, two other obvious baselines that were omitted: raw->LSTM and {raw->ConvNet,trends}->MLP. Basically, the authors propose a complex architecture without demonstrating the value of each part (trend extraction, LSTM, ConvNet, MLP). The baselines are unnecessarily weak. One thing I am uncertain about in general: the validity of the practice of using the same LSTM and ConvNet architectures in both the baselines and the TreNet. This *sounds* like an apples-to-apples comparison, but in the world of hyperparameter tuning, it could in fact disadvantage either. It seems like a more thorough approach would be to optimize each architecture independently. Regarding related work and baselines: I think it is fair to limit the scope of in-depth analysis and experiments to a set of reasonable, representative baselines, at least in a conference paper submitted to a deep learning conference. That said, the authors ignored a large body of work on financial time series modeling using probabilistic models and related techniques. This is another way to frame the above *separate trends from noise* problem: treat the observations as noisy. One semi-recent example: J. Hernandez-Lobato, J. Lloyds, and D. Hernandez-Lobato. Gaussian process conditional copulas with applications to financial time series. NIPS 2013. I appreciate this research direction in general, but at the moment, I believe that the work described in this manuscript is not suitable for inclusion at ICLR. My policy for interactive review is to keep an open mind and willingness to change my score, but a large revision is unlikely. I would encourage the authors to instead use their time and energy -- and reviewer feedback -- in order to prepare for a future conference deadline (e.g., ICML).","['4', '4', '5']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[32, 6, 28]","[38, 9, 34]","[215, 24, 470]","[95, 17, 335]","[27, 6, 42]","[93, 1, 93]","The sentiment score is slightly negative (-20) because while the reviewer acknowledges some strengths and improvements, they still express significant concerns and ultimately do not recommend acceptance. The review starts positively but then lists several weaknesses and lingering concerns. The politeness score is moderately positive (50) as the reviewer uses respectful language throughout, acknowledges the authors' efforts, and provides constructive feedback. They use phrases like 'admirable job', 'interesting problem setting', and 'I appreciate this research direction', which contribute to a polite tone. However, the critique is direct and doesn't use overly deferential language, keeping it from scoring higher on politeness.",-20,50
Two Methods for Wild Variational Inference,Reject,2017,"['Qiang Liu', 'Yihao Feng']","[3, 3, 3]","['Clear rejection', 'Clear rejection', 'Clear rejection']","The paper proposes two methods for what is called wild variational inference. The goal is to obtain samples from the variational approximate distribution q without requiring to evaluate the density q(z) by which it becomes possible to consider more flexible family of distributions. The authors apply the proposed method to the problem of optimizing the hyperparamter of the SGLD sampler. The experiments are performed on a 1-d mixture of gaussian distribution and Bayesian logistic regression tasks. The key contribution seems to connect the previous findings in SVGD and KSD to the concept of inference networks, and to use them for hyperparameter optimization of SGLD. This can not only be considered as a rather simple connection/extension, but also the toyish experiments are not enough to convince readers on the significance of the proposed model. Particularly, I*m wondering how the particle based methods can deal with the multimodality (not the simple 1d gaussian mixture case) in general. Also, the method seems still to require to evaluate the true gradient of the target distribution (e.g., the posterior distribution) for each z ~ q. This seems to be a computational problem for large dataset settings. In the experiments, the authors compare the methods for the same number of update steps. But, considering the light computation of SGLD per update, I think SGLD can make much more updates per unit time than the proposed methods, particularly for large datasets. The Bayesian logistic regression on 54 dimensions seems also a quite simple experiment, considering that its posterior is close to a Gaussian distribution. Also, including Hamiltonian Monte Carlo (HMC) with automatic hyperparameter tuning mechanism (like, no u-turn sampler) would be interesting. The paper is written very unclearly. Especially, it is not clear what is the exact contributions of the paper compared to the other previous works including the authors* works. The main message is quite simple but most of the pages are spent to explain previous works. Overall, I*d like to suggest to have more significant high-dimension, large scale experiments, and to improve the writing.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[8, 1]","[14, 7]","[183, 30]","[88, 11]","[86, 18]","[9, 1]","The sentiment score is -60 because the reviewer expresses several criticisms and doubts about the paper's contributions, methodology, and experiments. They describe the work as a 'simple connection/extension' with 'toyish experiments' that are not convincing. The reviewer also points out unclear writing and suggests significant improvements. However, it's not entirely negative as they acknowledge some positive aspects, like connecting previous findings. The politeness score is 20 because while the reviewer is critical, they maintain a professional tone and use phrases like 'I'd like to suggest' rather than making demands. They also acknowledge some positive aspects of the work. The language is not overtly rude, but it's also not particularly warm or encouraging, hence a slightly positive score.",-60,20
Unsupervised Learning Using Generative Adversarial Training And Clustering,Reject,2017,"['Vittal Premachandran', 'Alan L. Yuille']","[3, 3, 3]","['Clear rejection', 'Clear rejection', 'Clear rejection']","The papers investigates the task of unsupervised learning with deep features via k-means clustering. The entire pipeline can be decomposed into two steps: (1) unsupervised feature learning based on GAN framework and (2) k-means clustering using learned deep network features. Following the GAN framework and its extension InfoGAN, the first step is to train a pair of discriminator network and generator network from scratch using min-max objective. Then, it applies k-means clustering on the top layer features from discriminator network. For evaluation, the proposed unsupervised feature learning approach is compared against traditional hand-crafted features such as HOG and supervised method on three benchmark datasets. Normalized Mutual Information (NMI) and Adjusted RAND Index (ARI) have been used as the evaluation metrics for experimental comparison. Although the proposed method may be potentially useful in practice (if refined further), I find the method lacks novelty, and the experimental results are not significant enough.","['4', '5', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[8, 35]","[10, 41]","[17, 704]","[9, 337]","[5, 265]","[3, 102]","The sentiment score is -50 because the reviewer expresses a generally negative view of the paper, stating that it 'lacks novelty' and that the 'experimental results are not significant enough.' However, they do acknowledge that the method 'may be potentially useful in practice,' which prevents the score from being more negative. The politeness score is 0 (neutral) because the reviewer uses professional and objective language throughout, neither being particularly polite nor rude. They present their criticisms directly but without harsh or insulting language, maintaining a neutral tone typical of academic peer reviews.",-50,0
Unsupervised Learning of State Representations for Multiple Tasks,Reject,2017,"['Antonin Raffin', 'Sebastian Höfer', 'Rico Jonschkowski', 'Oliver Brock', 'Freek Stulp']","[6, 5, 6]","['Marginally above acceptance threshold', '5', 'Marginally above acceptance threshold']","This paper is about learning unsupervised state representations using multi-task reinforcement learning. The authors propose a novel approach combining gated neural networks with multitask learning with robotics priors. They evaluated their approach on two simulated datasets and showed promising results. The paper is clearly written and is theoretically sound. Positives: + Gating to enable learning a joint representation + Multi-task learning extended from a single task in prior work + Combining multiple types of losses to learn a strong representation (Coherence, Proportionality, Causality, Repeatability, Consistency and Separation) Negatives: - Parameters choice is arbitrary (w parameters) - Limiting the multi-task learning to be different to individual tasks rather than sharing and transferring knowledge between tasks - The experiments could have been conducted using a standardized simulation tool such as OpenAI Gym to make it easy to compare. I would recommend that the authors consider a more standardized way of picking the model parameters and evaluate on a more standard and high-dimensional datasets.","['4', '4', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[0, 9, 4, 20, 17]","[5, 14, 9, 25, 22]","[11, 24, 34, 132, 98]","[2, 17, 14, 79, 51]","[7, 4, 16, 17, 11]","[2, 3, 4, 36, 36]","The sentiment score is 60 (positive) because the reviewer starts with a neutral summary and then lists several positives, including that the paper is 'clearly written and theoretically sound'. They do mention some negatives, but these are presented as constructive criticism rather than major flaws. The politeness score is 70 (polite) because the reviewer uses respectful language throughout, acknowledges the paper's strengths, and frames criticisms as recommendations for improvement rather than harsh judgments. The reviewer also uses phrases like 'I would recommend' which maintains a collegial tone.",60,70
b-GAN: Unified Framework of Generative Adversarial Networks,Reject,2017,"['Masatosi Uehara', 'Issei Sato', 'Masahiro Suzuki', 'Kotaro Nakayama', 'Yutaka Matsuo']","[5, 6, 4]","['5', 'Marginally above acceptance threshold', 'Ok but not good enough - rejection']","In this paper, the authors extend the f-GAN by using Bregman divergences for density ratio matching. The argument against f-GAN (which is a generalization of the regular GAN) is that the actual objective optimized by the generator during training is different from the theoretically motivated objective due to gradient issues with the theoretically motivated objective. In b-GANS, the discriminator is a density ratio estimator (r(x) = p(x) / q(x)), and the generator tries to minimize the f-divergence between p and q by writing p(x) = r(x)q(x). My main problem with this paper is that it is unclear why any of this is useful. The connection to density estimation is interesting, but any derived conclusions between the two seem questionable. For example, in previous density estimation literature, the Pearson divergence is more stable. The authors claim that the same holds for GANS and try to show this in their experiments. Unfortunately, the experiments section is very confusing with unilluminating figures. Looking at the graph of density ratios is not particularly illuminating. They claim that for the Pearson divergence and modified KL-divergence, *the learning did not stop* by looking at the graph of density ratios. This is completely hand-wavey and no further evidence is given to back this claim. Also, why was the normal GAN objective not tried in light of this analysis? Furthermore, it seems that despite criticizing normal GANs for using a heuristic objective for the generator, multiple heuristics objectives and tricks are used to make b-GAN work. I think this paper would be much improved if it was rewritten in a clear fashion. As it stands, it is difficult to understand the motivation or intuition behind this work.","['3', '4', '3']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","['no_match', 12, 16, 13, 20]","['no_match', 18, 22, 18, 26]","['no match', 144, 53, 44, 252]","['no match', 72, 27, 32, 156]","['no match', 53, 12, 6, 52]","['no match', 19, 14, 6, 44]","The sentiment score is -70 because the reviewer expresses significant criticism and skepticism about the paper's usefulness and clarity. They state their 'main problem' with the paper, describe the experiments as 'very confusing with unilluminating figures', and use phrases like 'completely hand-wavey' and 'difficult to understand'. The politeness score is -20 because while the reviewer doesn't use overtly rude language, their tone is quite critical and dismissive. They use phrases like 'My main problem with this paper is...' and 'Unfortunately, the experiments section is very confusing', which come across as somewhat blunt and impolite. However, they do offer some constructive feedback at the end, suggesting the paper could be improved if rewritten, which prevents the score from being even lower.",-70,-20
Making Neural Programming Architectures Generalize via Recursion,Accept,2017,"['Jonathon Cai', 'Richard Shin', 'Dawn Song']","[8, 8, 9]","['Top 50% of accepted papers, clear accept', 'Top 50% of accepted papers, clear accept', '9']","This paper argues that being able to handle recursion is very important for neural programming architectures — that handling recursion allows for strong generalization to out of domain test cases and learning from smaller amounts of training data. Most of the paper is a riff on the Reed & de Freitas paper on Neural Programmer Interpreters from ICLR 2016 which learns from program traces — this paper trains NPI models on traces that have recursive calls. The authors show how to verify correctness by evaluating the learned program on only a small set of base cases and reduction rules and impressively, show that the NPI architecture is able to perfectly infer Bubblesort and the Tower of Hanoi problems. What I like is that the idea is super simple and as the authors even mention, the only change is to the execution traces that the training pipeline gets to see. I’m actually not sure what the right take-away is — does this mean that we have effectively solved the neural programming problem when the execution traces are available? (and was the problem too easy to begin with?). For example, a larger input domain (as one of the reviewers also mentions) is MNIST digits and we can imagine a problem where the NPI must infer how to sort MNIST digits from highest to lowest. In this setting, having execution traces would effectively decouple the problem of recognizing the digits from that of inferring the program logic — and so the problem would be no harder than learning to recognize MNIST digits and learning to bubble sort from symbols. What is a problem where we have access to execution traces but cannot infer it using the proposed method?","['4', '3', '5']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[3, 8, 19]","[6, 13, 25]","[5, 38, 451]","[3, 21, 256]","[2, 16, 163]","[0, 1, 32]","The sentiment score is 70 (positive) because the reviewer expresses appreciation for the paper's ideas, calling them 'super simple' and 'impressive'. They also highlight the paper's contributions and strengths. However, it's not 100 as the reviewer raises some questions and potential limitations. The politeness score is 50 (slightly positive) because the language is professional and respectful, with no harsh criticisms. The reviewer uses phrases like 'What I like is...' and asks thoughtful questions rather than making blunt criticisms. However, it doesn't go out of its way to be overly polite or complimentary, maintaining a neutral, academic tone overall.",70,50
End-to-end Optimized Image Compression,Accept,2017,"['Johannes Ballé', 'Valero Laparra', 'Eero P. Simoncelli']","[7, 9, 8, 8, 8]","['Good paper, accept', '9', 'Top 50% of accepted papers, clear accept', 'Top 50% of accepted papers, clear accept', 'Top 50% of accepted papers, clear accept']","Two things I*d like to see. 1) Specifics about the JPEG and JPEG2000 implementations used, and how they were configured. One major weakness I see in many papers is they do not include specific encoders and configuration used in comparisons. Without knowing this, it*s hard to know if the comparison was done with a suitably strong JPEG implementation that was properly configured, for example. 2) The comparison to JPEG2000 is unfortunately not that interesting, since that codec does not have widespread usage and likely never will. A better comparison would be with WebP performance. Or, even better, both. Very nice results. Is a software implementation of this available to play with?","['4', '4', '4', '4', '3']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[-1, 22]","[5, 28]","[7, 164]","[4, 88]","[3, 33]","[0, 43]","The sentiment score is 50 (slightly positive) because the reviewer expresses interest in the work ('Very nice results') and offers constructive suggestions for improvement, indicating a generally positive view. However, they also point out some weaknesses, which prevents a higher score. The politeness score is 70 (quite polite) due to the reviewer's respectful tone throughout. They frame their suggestions as things they'd 'like to see' rather than demands, and they conclude with a compliment and an interested question about software availability. The language is professional and courteous, without any harsh criticism or rude remarks.",50,70
Optimization as a Model for Few-Shot Learning,Accept,2017,"['Sachin Ravi', 'Hugo Larochelle']","[9, 6, 8]","['9', 'Marginally above acceptance threshold', 'Top 50% of accepted papers, clear accept']","This work presents an LSTM based meta-learning framework to learn the optimization algorithm of a another learning algorithm (here a NN). The paper is globally well written and the presentation of the main material is clear. The crux of the paper: drawing the parallel between Robbins Monroe update rule and the LSTM update rule and exploit it to satisfy the two main desiderata of few shot learning (1- quick acquisition of new knowledge, 2- slower extraction of general transferable knowledge) is intriguing. Several tricks re-used from (Andrychowicz et al. 2016) such as parameter sharing and normalization, and novel design choices (specific implementation of batch normalization) are well motivated. The experiments are convincing. This is a strong paper. My only concerns/questions are the following: 1. Can it be redundant to use the loss, gradient and parameters as input to the meta-learner? Did you do ablative studies to make sure simpler combinations are not enough. 2. It would be great if other architectural components of the network can be learned in a similar fashion (number of neurons, type of units, etc.). Do you have an opinion about this? 3. The related work section (mainly focused on meta learning) is a bit shallow. Meta-learning is a rather old topic and similar approaches have been tried to solve the same problem even if they were not using LSTMs: - Samy Bengio PhD thesis (1989) is all about this ;-) - Use of genetic programming for the search of a new learning rule for neural networks (S. Bengio, Y. Bengio, and J. Cloutier. 1994) - I am convince Schmidhuber has done something, make sure you find it and update related work section. Overall, I like the paper. I believe the discussed material is relevant to a wide audience at ICLR.","['5', '4', '4']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[3, 13]","[8, 19]","[9, 163]","[4, 65]","[5, 74]","[0, 24]","The sentiment score is 80 (positive) because the reviewer describes the paper as 'strong' and 'intriguing', with 'convincing' experiments. They state that the paper is 'globally well written' and 'clear'. The reviewer expresses that they 'like the paper' and believe it's 'relevant to a wide audience'. The only slight negatives are a few questions and concerns, but these are presented as minor points for improvement rather than major criticisms. The politeness score is 70 (polite) because the reviewer uses respectful language throughout, offering praise and constructive feedback. They phrase their concerns as questions rather than criticisms, which is a polite approach. The reviewer also offers helpful suggestions for improvement, such as updating the related work section, which shows consideration for the authors' work. The language is professional and courteous throughout, without any rudeness or harsh criticism.",80,70
Learning End-to-End Goal-Oriented Dialog,Accept,2017,"['Antoine Bordes', 'Y-Lan Boureau', 'Jason Weston']","[8, 7, 8]","['Top 50% of accepted papers, clear accept', 'Good paper, accept', 'Top 50% of accepted papers, clear accept']","Attempts to use chatbots for every form of human-computer interaction has been a major trend in 2016, with claims that they could solve many forms of dialogs beyond simple chit-chat. This paper represents a serious reality check. While it is mostly relevant for Dialog/Natural Language venues (to educate software engineer about the limitations of current chatbots), it can also be published at Machine Learning venues (to educate researchers about the need for more realistic validation of ML applied to dialogs), so I would consider this work of high significance. Two important conjectures are underlying this paper and likely to open to more research. While they are not in writing, Antoine Bordes clearly stated them during a NIPS workshop presentation that covered this work. Considering the metrics chosen in this paper: 1) The performance of end2end ML approaches is still insufficient for goal oriented dialogs. 2) When comparing algorithms, relative performance on synthetic data is a good predictor of performance on natural data. This would be quite a departure from previous observations, but the authors made a strong effort to match the synthetic and natural conditions. While its original algorithmic contribution consists in one rather simple addition to memory networks (match type), it is the first time these are deployed and tested on a goal-oriented dialog, and the experimental protocol is excellent. The overall paper clarity is excellent and accessible to a readership beyond ML and dialog researchers. I was in particular impressed by how the short appendix on memory networks summarized them so well, followed by the tables that explained the influence of the number of hops. While this paper represents the state-of-the-art in the exploration of more rigorous metrics for dialog modeling, it also reminds us how brittle and somewhat arbitrary these remain. Note this is more a recommendation for future research than for revision. First they use the per-response accuracy (basically the next utterance classification among a fixed list of responses). Looking at table 3 clearly shows how absurd this can be in practice: all that matters is a correct API call and a reasonably short dialog, though this would only give us a 1/7 accuracy, as the 6 bot responses needed to reach the API call also have to be exact. Would the per-dialog accuracy, where all responses must be correct, be better? Table 2 shows how sensitive it is to the experimental protocol. I was initially puzzled that the accuracy for subtask T3 (0.0) was much lower that the accuracy for the full dialog T5 (19.7), until the authors pointed me to the tasks definitions (3.1.1) where T3 requires displaying 3 options while T5 only requires displaying one. For the concierge data, what would happen if ‘correct’ meant being the best, not among the 5-best? While I cannot fault the authors for using standard dialog metrics, and coming up with new ones that are actually too pessimistic, I can think of one way to represent dialogs that could result in more meaningful metrics in goal oriented dialogs. Suppose I sell Virtual Assistants as a service, being paid upon successful completion of a dialog. What is the metric that would maximize my revenue? In this restaurant problem, the loss would probably be some weighted sum of the number of errors in the API call, the number of turns to reach that API call and the number of rejected options by the user. However, such as loss cannot be measured on canned dialogs and would either require a real human user or an realistic simulator Another issue closely related to representation learning that this paper fails to address or explain properly is what happens if the vocabulary used by the user does not match exactly the vocabulary in the knowledge base. In particular, for the match type algorithm to code ‘Indian’ as ‘type of cuisine’, this word would have to occur exactly in the KB. I can imagine situations where the KB uses some obfuscated terminology, and we would like ML to learn the associations rather than humans to hand-describe them.","['4', '4', '5']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[1, -2, -2, 12, -1, 16]","[7, 4, 4, 18, 4, 22]","[24, 4, 14, 112, 52, 178]","[12, 2, 7, 36, 25, 96]","[12, 2, 7, 60, 27, 58]","[0, 0, 0, 16, 0, 24]","The sentiment score is 70 (positive) because the reviewer expresses high regard for the paper's significance, clarity, and experimental protocol. They describe it as a 'serious reality check' and 'state-of-the-art', with 'excellent' clarity. However, it's not 100 as they also point out some limitations and areas for improvement. The politeness score is 80 (very polite) due to the consistently respectful and constructive tone. The reviewer uses phrases like 'I was impressed' and frames criticisms as suggestions for future research rather than flaws. They acknowledge the paper's strengths before offering any critiques, maintaining a balanced and courteous approach throughout.",70,80
Towards Principled Methods for Training Generative Adversarial Networks,Accept,2017,"['Martin Arjovsky', 'Leon Bottou']","[8, 10, 7, 10]","['Top 50% of accepted papers, clear accept', 'Top 5% of accepted papers, seminal paper', 'Good paper, accept', 'Top 5% of accepted papers, seminal paper']","This paper makes a valuable contribution to provide a more clear understanding of generative adversarial network (GAN) training procedure. With the new insight of the training dynamics of GAN, as well as its variant, the authors reveal the reason that why the gradient is either vanishing in original GAN or unstable in its variant. More importantly, they also provide a way to avoid such difficulties by introducing perturbation. I believe this paper will inspire more principled research in this direction. I am very interested in the perturbation trick to avoid the gradient instability and vanishment. In fact, this is quite related to dropout trick in where the perturbation can be viewed as Bernoulli distribution. It will be great if the connection can be discussed. Besides the theoretical analysis, is there any empirical study to justify this trick? Could you please add some experiments like Fig 2 and 3 for the perturbated GAN for comparison?","['3', '5', '4', '5']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[3, 29]","[8, 35]","[26, 127]","[11, 64]","[15, 38]","[0, 25]","The sentiment score is 80 (positive) because the reviewer expresses strong approval of the paper, calling it a 'valuable contribution' and stating that it will 'inspire more principled research'. The reviewer also shows interest in the paper's findings and methodology. The politeness score is 90 (very polite) due to the respectful and constructive tone throughout. The reviewer uses phrases like 'I believe' and 'It will be great if', and frames suggestions as polite requests ('Could you please...'). The reviewer also acknowledges the paper's strengths before offering suggestions for improvement, which is a hallmark of polite academic discourse.",80,90
Reinforcement Learning with Unsupervised Auxiliary Tasks,Accept,2017,"['Max Jaderberg', 'Volodymyr Mnih', 'Wojciech Marian Czarnecki', 'Tom Schaul', 'Joel Z Leibo', 'David Silver', 'Koray Kavukcuoglu']","[7, 8, 8]","['Good paper, accept', 'Top 50% of accepted papers, clear accept', 'Top 50% of accepted papers, clear accept']","This paper proposes a way of adding unsupervised auxiliary tasks to a deep RL agent like A3C. Authors propose a bunch of auxiliary control tasks and auxiliary reward tasks and evaluate the agent in Labyrinth and Atari. Proposed UNREAL agent performs significantly better than A3C and also learns faster. This is definitely a good contribution to the conference. However, this is not a surprising result since adding additional auxiliary tasks that are relevant to the goal should always help in better and faster feature shaping. This paper is a proof of concept for this idea. The paper is well written and easy to follow by any reader with deep RL expertise. Can authors comment about the computational resources needed to train the UNREAL agent? The overall architecture is quite complicated. Are the authors willing to release the source code for their model? -------------------------------------------------------- After rebuttal: No change in the review.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[4, 12, 6, 10, 7, 18, 9]","[8, 18, 12, 16, 13, 23, 14]","[45, 55, 84, 95, 92, 174, 101]","[18, 28, 29, 52, 29, 88, 46]","[25, 24, 44, 35, 54, 58, 46]","[2, 3, 11, 8, 9, 28, 9]","The sentiment score is 60 (positive) because the reviewer states that the paper is 'definitely a good contribution to the conference' and praises the writing as 'well written and easy to follow'. However, it's not extremely positive as the reviewer notes that the result is 'not surprising'. The politeness score is 70 (polite) due to the constructive tone, the use of phrases like 'Can authors comment' instead of demands, and the absence of harsh criticism. The reviewer offers balanced feedback, acknowledging strengths while also raising questions, which contributes to the polite tone.",60,70
Multi-Agent Cooperation and the Emergence of (Natural) Language,Accept,2017,"['Angeliki Lazaridou', 'Alexander Peysakhovich', 'Marco Baroni']","[7, 7, 7]","['Good paper, accept', 'Good paper, accept', 'Good paper, accept']","Thank you for an interesting read. Pros - This paper tackles a very crucial problem of understanding communications between 2 agents. As more and more applications of reinforcement learning are being explored, this approach brings us back to a basic question. Is the problem solving approach of machines similar to that of humans. - The task is simple enough to make the post learning analysis intuitive. - It was interesting to see how informed agents made use of multiple symbols to transmit the message, where as agnostic agents relied only on 2 symbols. Cons - The task effectively boils down to image classification, if the 2 images sent are from different categories. The symbols used are effectively the image class which the second agent learns to assign to either of the images. By all means, this approach boils down to a transfer learning problem which could probably be trained much faster than a reinforcement learning algorithm.","['3', '3', '3']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']","[7, 4, 16]","[13, 10, 22]","[65, 51, 162]","[33, 22, 90]","[30, 24, 48]","[2, 5, 24]","The sentiment score is 50 (slightly positive) because the review begins with 'Thank you for an interesting read' and lists several pros, indicating a generally positive view. However, it also includes cons, which balances out the sentiment. The politeness score is 80 (quite polite) due to the courteous opening, the use of neutral language throughout, and the balanced presentation of both pros and cons without harsh criticism. The reviewer maintains a professional and respectful tone throughout the review.",50,80
Understanding deep learning requires rethinking generalization,Accept,2017,"['Chiyuan Zhang', 'Samy Bengio', 'Moritz Hardt', 'Benjamin Recht', 'Oriol Vinyals']","[10, 9, 10]","['Top 5% of accepted papers, seminal paper', '9', 'Top 5% of accepted papers, seminal paper']","This paper offers a very interesting empirical observation regarding the memorization capacity of current large deep convolutional networks. It shows they are able to perfectly memorize full training-set input-to-label mapping, even with random labels (i.e. when label has been rendered independent of input), using the same architecture and hyper-parameters as used for training with correct labels, except for a longer time to convergence. Extensive experiments support the main argument of the paper. Reflexions and observations about finite-sample expressivity and implicit regularization with linear models fit logically within the main theme and are equally thought-provoking. While this work doesn’t propose much explanations for the good generalization abilities of what it clearly established as overparameterized models, it does compel the reader to think about the generalization problem from a different angle than how it is traditionally understood. In my view, raising good questions and pointing to apparent paradox is the initial spark that can lead to fundamental progress in understanding. So even without providing any clear answers, I think this work is a very valuable contribution to research in the field. Detailed question: in your solving of Eq. 3 for MNIST and CIFAR10, did you use integer y class targets, or a binary one-versus all approach yielding 10 discriminant functions (hence a different alpha vector for each class)?","['4', '3', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[8, 31, 12, 16, 11]","[14, 37, 18, 22, 17]","[70, 263, 134, 213, 210]","[29, 156, 62, 90, 101]","[33, 61, 65, 91, 99]","[8, 46, 7, 32, 10]","The sentiment score is 90 because the reviewer expresses a very positive view of the paper, describing it as 'very interesting', 'thought-provoking', and 'a very valuable contribution to research in the field'. They appreciate the paper's empirical observations, extensive experiments, and its ability to raise important questions. The politeness score is 80 because the reviewer uses respectful and professional language throughout, acknowledging the paper's strengths and contributions without any harsh criticism. They offer a detailed question at the end, which is framed politely as a request for clarification rather than a criticism. The overall tone is constructive and appreciative, indicating a high level of politeness in academic discourse.",90,80
Neural Architecture Search with Reinforcement Learning,Accept,2017,"['Barret Zoph', 'Quoc Le']","[9, 9, 9]","['9', '9', '9']","This paper explores an important part of our field, that of automating architecture search. While the technique is currently computationally intensive, this trade-off will likely become better in the near future as technology continues to improve. The paper covers both standard vision and text tasks and tackle many benchmark datasets, showing there are gains to be made by exploring beyond the standard RNN and CNN search space. While one would always want to see the technique applied to more datasets, this is already far more sufficient to show the technique is not only competitive with human architectural intuition but may even surpass it. This also suggests an approach to tailor the architecture to specific datasets without resulting in hand engineering at each stage. This is a well written paper on an interesting topic with strong results. I recommend it be accepted.","['4', '4', '5']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","[6, -1, 4, 6, 22]","[10, 5, 10, 12, 28]","[15, 13, 16, 22, 202]","[4, 7, 9, 10, 128]","[2, 3, 1, 3, 8]","[9, 3, 6, 9, 66]","The sentiment score is 90 because the review is overwhelmingly positive. The reviewer praises the paper's importance, its comprehensive coverage of tasks and datasets, and its strong results. They explicitly recommend acceptance, stating it's 'a well written paper on an interesting topic with strong results.' The only slight reservation is the mention of computational intensity, but this is framed as a temporary issue. The politeness score is 80 because the language used is consistently respectful and constructive. The reviewer acknowledges the paper's strengths and potential impact without using overly effusive language. They offer a balanced view, mentioning a potential limitation (computational intensity) but framing it positively. The tone is professional and encouraging throughout, without being excessively formal or distant.",90,80
Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic,Accept,2017,"['Shixiang Gu', 'Timothy Lillicrap', 'Zoubin Ghahramani', 'Richard E. Turner', 'Sergey Levine']","[7, 7, 8, 7]","['Good paper, accept', 'Good paper, accept', 'Top 50% of accepted papers, clear accept', 'Good paper, accept']","**Edit: Based on the discussion below, my main problem (#2) was not correct. I have changed my overall rating from a 3 to a 7** This paper makes a fascinating observation: one can introduce an action-dependent baseline (control variate) into REINFORCE, which introduces bias, and then include a correction term to remove the bias. The variance of the correction term is low relative to the REINFORCE update and the action-dependent baseline, and so this results in benefits. However, the paper is poorly executed. Below I list my concerns. 1. The paper tries to distinguish between *policy gradient* methods and *actor critic* methods by defining them in a non-standard way. Specifically, when this paper says *policy gradient* it means REINFORCE. Historically, the two have meant different things: some policy gradient algorithms are actor-critics (e.g., Degris et al*s INAC algorithm) while others are not (e.g. REINFORCE). 2. The proposed Q-Prop algorithm includes many interesting design choices that make in unclear what the real source of improved performance is. Is the improved performance due to the use of the action-dependent control variate? Would the same setup but using a state-value baseline still perform just as well? Are the performance benefits due to the use of an off-policy advantage estimation algorithm, GAE(lambda)? Or, would performance have been similar with an on-policy advantage estimation algorithm? What about if a different off-policy advantage estimation algorithm was used, like Retrace(lambda), GTD2, ETD, or WIS-LSTD? Or, is the improved performance due to the use of a replay buffer? Comparisons are not performed between variants of Q-Prop that show the importances of these different components. Rather the authors opt to show better performance on a benchmark task. I find this to be non-scientific, and more of a paper showing a feat of engineering (by combining many different ideas) than it is a research paper that studies the details of which parts of Q-Prop make it work well. For example, after reading this paper, it is not clear whether having the action-dependent baseline (or using the first order Taylor approximation for the baseline) is beneficial or not - it could be that the strong performance comes from GAE(lambda) or the use of a replay buffer. At the very least I would have expected comparisons to Q-Prop using a state-value baseline (which would then be a variant of REINFORCE using off-policy data and a replay buffer, and which would show whether the action-dependent baseline is important). 3. There is a fair amount of discussion about unbiased policy gradient algorithms, which is not accurate. Most policy gradient algorithms are biased, and making them unbiased tends to hurt performance. This is discussed in the paper *Bias in Natural Actor-Critic Algorithms*, which applies to non-natural algorithms as well. Also, I suspect that the use of GAE(lambda) results in the exact sort of bias discussed in that paper, even when lambda=1. As a result, Q-Prop may act more like an average reward method than expected. This should be discussed. 4. The proposed algorithm can be applied to deep architectures, just as most linear-time policy gradient algorithms can. However, it does not have to be applied to deep architectures. The emphasis on *deep* therefore seems to detract from the core ideas of the paper. 5. The paper repeatedly says that importance sampling based methods result in high variance. This ignores weighted importance sampling methods that have very low variance. A good example of this is Mahmood et al*s WIS-LSTD algorithm. WIS-LSTD has high computational complexity, so it would only be compared to on non-deep RL problems, of which there are plenty. Alternatively, algorithms like Retrace(lambda) have quite low variance since the likelihood ratios are never bigger than one. Others might argue that ETD algorithms are currently the most effective. The simple dismissal of these algorithms because the original importance sampling estimator proposed in 2000 has high variance is not sufficient. 6. The paper does not compare to natural actor-critic algorithms. Once the weights, w, have been computed, REINFORCE uses samples of states from the normalized discounted state distribution and samples of the corresponding returns to estimate the policy gradient. One of the main reasons Q-Prop should work better than REINFORCE is that it includes a control variate that reduces the variance of the policy gradient update after w has been computed. Now, compare this to natural policy gradient algorithms. Once the weights, w, have been computed (admittedly, using compatible features for the advantage estimation but any features for the state-value estimation) the resulting update is = w. That is, is has zero variance and does not require additional sampling. It is as though a perfect control variate was used. Furthermore, natural gradient algorithms can be applied to deep architectures. Degris et al*s INAC algorithm is linear time. Desjardin et al*s *natural neural networks* paper also discusses efficient implementations of natural gradients for neural networks. Dabney*s Natural Temporal Difference algorithms have linear time variants that fit this paper*s description of actor-critic algorithms. To summarize, given the weights w, REINFORCE has high variance, and Q-Prop claims to reduce the variance of REINFORCE. However, natural policy gradient methods have zero variance given the weights w. So, what is the benefit of Q-Prop over natural gradient algorithms using off-policy value function estimation methods to estimate Q (or A)? That is, why should we expect Q-Prop to perform better than NAC-LSTD using GAE(lambda) with experience replay in place of LSTD? 7. Equation (2) is false. The right side is proportional to the left side, not equal to it. There is a (1-gamma) term missing. There are also other typos throughout (e.g., Q and A sometimes are missing their action arguments). Although I have listed my concerns, I would like to re-iterate that I do find the idea of an action-dependent baseline fascinating. My problem with this paper is with its execution, not with the novelty, impact, or quality of the core idea.","['5', '4', '3', '4']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[6, 10, 25, 11, 9]","[12, 16, 30, 17, 15]","[93, 128, 310, 151, 757]","[40, 48, 184, 69, 335]","[51, 70, 66, 72, 401]","[2, 10, 60, 10, 21]","The sentiment score is -50 because while the reviewer finds the core idea 'fascinating', they express numerous concerns about the paper's execution, methodology, and comparisons. The review starts positively but quickly shifts to a critical tone, listing 7 major issues. The politeness score is 20 because the reviewer uses respectful language throughout, acknowledging the paper's interesting aspects, and frames criticisms as 'concerns' rather than outright dismissals. They also reiterate their positive view of the core idea at the end. However, the extensive critique and phrases like 'poorly executed' and 'non-scientific' prevent a higher politeness score.",-50,20
Learning to Act by Predicting the Future,Accept,2017,"['Alexey Dosovitskiy', 'Vladlen Koltun']","[7, 8, 8]","['Good paper, accept', 'Top 50% of accepted papers, clear accept', 'Top 50% of accepted papers, clear accept']","Deep RL (using deep neural networks for function approximators in RL algorithms) have had a number of successes solving RL in large state spaces. This empirically driven work builds on these approaches. It introduces a new algorithm which performs better in novel 3D environments from raw sensory data and allows better generalization across goals and environments. Notably, this algorithm was the winner of the Visual Doom AI competition. The key idea of their algorithm is to use additional low-dimensional observations (such as ammo or health which is provided by the game engine) as a supervised target for prediction. Importantly, this prediction is conditioned on a goal vector (which is given, not learned) and the current action. Once trained the optimal action for the current state can be chosen as the action that maximises the predicted outcome according the goal. Unlike in successor feature representations, learning is supervised and there is no TD relationship between the predictions of the current state and the next state. There have been a number of prior works both in predicting future states as part of RL and goal driven function approximators which the authors review in section 2. The key contributions of this work are the focus on Monte Carlo estimation (rather than TD), the use of low-dimensional ‘measurements’ for prediction, the parametrized goals and, perhaps most importantly, the empirical comparison to relevant prior work. In addition to the comparison with Visual Doom AI, the authors show that their algorithm is able to learn generalizable policies which can respond, without further training, to limited changes in the goal. The paper is well-communicated and the empirical results compelling and will be of significant interest. Some minor potential improvements: There is an approximation in the supervised training as it is making an on-policy assumption but it learns from a replay buffer (with the Monte Carlo regression the expectation of the remainder of the trajectory is assumed to follow the current policy, but is being sampled from episodes generated by prior versions of the policy). This should be discussed. The algorithm uses additional metadata (the information about which parts of the sensory input are worth predicting) that the compared algorithms do not. I think this, and the limitations of this approach (e.g. it may not work well in a sensory environment if such measurements are not provided) should be mentioned more clearly.","['4', '4', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[5, 18]","[10, 24]","[96, 295]","[41, 135]","[44, 103]","[11, 57]","The sentiment score is 80 (positive) because the reviewer expresses a generally positive view of the paper, describing it as 'well-communicated' with 'compelling' empirical results that 'will be of significant interest'. The reviewer acknowledges the paper's contributions and its success in competitions. The politeness score is 70 (polite) as the reviewer uses respectful language throughout, acknowledging the authors' work positively and framing suggestions as 'minor potential improvements' rather than criticisms. The reviewer maintains a professional tone, offering constructive feedback without harsh language. The slightly lower politeness score compared to sentiment reflects the presence of some critical comments, though these are presented diplomatically.",80,70
On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima,Accept,2017,"['Nitish Shirish Keskar', 'Dheevatsa Mudigere', 'Jorge Nocedal', 'Mikhail Smelyanskiy', 'Ping Tak Peter Tang']","[8, 10, 6]","['Top 50% of accepted papers, clear accept', 'Top 5% of accepted papers, seminal paper', 'Marginally above acceptance threshold']","Interesting paper, definitely provides value to the community by discussing why large batch gradient descent does not work too well","['3', '3', '4']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[3, 8, 39, 18, 29]","[8, 14, 45, 23, 34]","[46, 55, 76, 69, 62]","[16, 22, 8, 41, 27]","[27, 31, 6, 17, 15]","[3, 2, 62, 11, 20]","The sentiment score is 60 (positive) because the reviewer describes the paper as 'interesting' and states that it 'definitely provides value to the community'. These are clear positive indicators. However, it's not extremely high as the praise is somewhat restrained. The politeness score is 20 (slightly polite) because while the language is not overtly polite, it is professional and respectful. The use of 'interesting' and acknowledging the paper's value shows a degree of courtesy. The reasoning is brief and straightforward, without excessive formality or politeness markers, hence the relatively low positive score.",60,20
Semi-supervised Knowledge Transfer for Deep Learning from Private Training Data,Accept,2017,"['Nicolas Papernot', 'Martín Abadi', 'Úlfar Erlingsson', 'Ian Goodfellow', 'Kunal Talwar']","[9, 7, 9]","['9', 'Good paper, accept', '9']","This paper addresses the problem of achieving differential privacy in a very general scenario where a set of teachers is trained on disjoint subsets of sensitive data and the student performs prediction based on public data labeled by teachers through noisy voting. I found the approach altogether plausible and very clearly explained by the authors. Adding more discussion of the bound (and its tightness) from Theorem 1 itself would be appreciated. A simple idea of adding perturbation error to the counts, known from differentially-private literature, is nicely re-used by the authors and elegantly applied in a much broader (non-convex setting) and practical context than in a number of differentially-private and other related papers. The generality of the approach, clear improvement over predecessors, and clarity of the writing makes the method worth publishing.","['4', '3', '4']","['The reviewer is confident but not absolutely certain that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct', 'The reviewer is confident but not absolutely certain that the evaluation is correct']","[4, 33, 22, 9, 17]","[10, 39, 26, 12, 23]","[156, 257, 58, 107, 183]","[58, 176, 33, 48, 98]","[91, 14, 17, 55, 68]","[7, 67, 8, 4, 17]","The sentiment score is 90 (highly positive) because the reviewer uses phrases like 'very clearly explained', 'nicely re-used', 'elegantly applied', and 'worth publishing'. They also mention 'clear improvement over predecessors' and praise the generality and clarity of the approach. The politeness score is 80 (quite polite) as the reviewer uses respectful language throughout, acknowledging the authors' work positively without being overly effusive. They offer a constructive suggestion ('Adding more discussion...would be appreciated') rather than demanding changes. The tone is professional and appreciative, maintaining a courteous academic discourse.",90,80
Amortised MAP Inference for Image Super-resolution,Accept,2017,"['Casper Kaae Sønderby', 'Jose Caballero', 'Lucas Theis', 'Wenzhe Shi', 'Ferenc Huszár']","[8, 9]","['Top 50% of accepted papers, clear accept', '9']","The paper presents a new framework to solve the SR problem - amortized MAP inference and adopts a pre-learned affine projection layer to ensure the output is consistent with LR. Also, it proposes three different methods to solve the problem of minimizing cross-entropy. Generally, it is a great paper. However, I still have several comments: 1) The proposed amortized MAP inference is novel and different from the previous SR methods. Combined with GAN, this framework can obtain plausible and good results. Compared with another GAN-based SR methods - Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network, question may arise as to what this new formulation adds to the latest state-of-the-art. 2) Using an affine projection architecture as a constraint, the model do not need any corresponding {HR, LR} image pairs for training. However, when training the affine projection layer, we still need the {HR, LR} image pairs. Does it mean that we merely transfer this training procedure to the training of affine projection? 3) The paper presents many results of the framework, including the results of natural images from ImageNet. Can the author also provide the results of Set5, Set14 or BSD100, which are conventional test dataset for SR, so that we can perform a fair comparison with previous work. 4) I see that the size of the results of nature images presented in this paper are limited to 128*128. Can this framework perform well on images with larger size? Because SR will encounter input with arbitrary size. 5) A normal GAN will have a noise term as a latent space, so that it can be better illustrated as learning a distribution. Do the author try the noise vector? Overall, this paper provides a new framework for SR with solid theoretical analysis. The idea is novel and the author explore many methods. Though there still exist questions like the necessity and more experiments are needed. I think this work will will provide good inspiration to the community.","['5', '3']","['The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'The reviewer is fairly confident that the evaluation is correct']","[3, 6, 8, 8, 7]","[7, 8, 14, 13, 13]","[20, 32, 45, 77, 36]","[6, 12, 18, 47, 13]","[9, 15, 22, 17, 20]","[5, 5, 5, 13, 3]","The sentiment score is 70 (positive) because the reviewer starts by calling it 'a great paper' and concludes that it 'will provide good inspiration to the community.' The overall tone is appreciative of the novel approach and solid theoretical analysis. However, it's not 100 as the reviewer raises several questions and suggests more experiments are needed. The politeness score is 80 (polite) because the reviewer uses respectful language throughout, framing criticisms as questions or suggestions rather than direct criticisms. Phrases like 'Can the author also provide...' and 'I think this work will...' demonstrate a courteous approach. The review maintains a professional and constructive tone without any harsh or rude language.",70,80
Learning Graphical State Transitions,Accept,2017,['Daniel D. Johnson'],"[9, 9]","['9', '9']","This paper proposes learning on the fly to represent a dialog as a graph (which acts as the memory), and is first demonstrated on the bAbI tasks. Graph learning is part of the inference process, though there is long term representation learning to learn graph transformation parameters and the encoding of sentences as input to the graph. This seems to be the first implementation of a differentiable memory as graph: it is much more complex than previous approaches like memory networks without significant gain in performance in bAbI tasks, but it is still very preliminary work, and the representation of memory as a graph seems much more powerful than a stack. Clarity is a major issue, but from an initial version that was constructive and better read by a computer than a human, the author proposed a hugely improved later version. This original, technically accurate (within what I understood) and thought provoking paper is worth publishing. The preliminary results do not tell us yet if the highly complex graph-based differentiable memory has more learning or generalization capacity than other approaches. The performance on the bAbI task is comparable to the best memory networks, but still worse than more traditional rule induction (see http://www.public.asu.edu/~cbaral/papers/aaai2016-sub.pdf). This is still clearly promising. The sequence of transformation in algorithm 1 looks sensible, though the authors do not discuss any other operation ordering. In particular, it is not clear to me that you need the node state update step T_h if you have the direct reference update step T_h,direct. It is striking that the only trick that is essential for proper performance is the ‘direct reference’ , which actually has nothing to do with the graph building process, but is rather an attention mechanism for the graph input: attention is focused on words that are relevant to the node type rather than the whole sentence. So the question “how useful are all these graph operations” remain. A much simpler version of a similar trick may have been proposed in the context of memory networks, also for ICLR*17 (see match type in *LEARNING END-TO-END GOAL-ORIENTED DIALOG* by Bordes et al) The authors also mention the time and size needed to train the model: is the issue arising for learning, inference or both? A description of the actual implementation would help (no pointer to open source code is provide). The author mentions Theano in one of my questions: how are the transformations compiled in advance as units? How is the gradient back-propagated through the graph is this one is only described at runtime? Typo: in the appendices B.2 and B.2.1, the right side of the equation that applies the update gate has h’_nu while it should be h_nu. In the references, the author could mention the pioneering work of Lee Giles on representing graphs with RNNs. Revision: I have improved my rating for the following reasons: - Pointers to an highly readable and well structured Theano source is provided. - The delta improvement of the paper has been impressive over the review process, and I am confident this will be an impactful paper. - Much simpler alternatives approaches such as Memory Networks seem to be plateauing for problems such as dialog modeling, we need alternatives. - The architecture is this work is still too complex, but this is often as we start with DNNs, and then find simplifications that actually improve performance","['3', '3']","['The reviewer is fairly confident that the evaluation is correct', 'The reviewer is fairly confident that the evaluation is correct']",[2],[8],[20],[10],[9],[1],"The sentiment score is 60 (positive) because the reviewer describes the paper as 'original, technically accurate, and thought provoking' and 'worth publishing'. They also mention that it's 'clearly promising' and that they have 'improved my rating'. However, they do point out some limitations and areas for improvement, which prevents a higher score. The politeness score is 80 (quite polite) because the reviewer uses respectful language throughout, acknowledges improvements made by the authors during the review process, and provides constructive feedback. They balance criticism with praise and use phrases like 'the authors could mention' instead of more demanding language. The high level of detail and engagement also indicates respect for the authors' work.",60,80

title,Authors,decision,year,academic_age,current_age,total_num_pub,total_num_conference,total_num_informal,total_num_journal,review,rating_score,rating_text,confidence_score,confidence_text,reasoning,sentiment_score,politeness_score
Predicting Floor-Level for 911 Calls with Neural Networks and Smartphone Sensor Data,"['William Falcon', 'Henning Schulzrinne']",Accept,2018,"[2, 33]","[6, 37]","[6, 406]","[2, 200]","[3, 116]","[1, 90]","Update: Based on the discussions and the revisions, I have improved my rating. However I still feel like the novelty is somewhat limited, hence the recommendation.

======================

The paper introduces a system to estimate a floor-level via their mobile device's sensor data using an LSTM to determine when a smartphone enters or exits a building, then using the change in barometric pressure from the entrance of the building to indoor location. Overall the methodology is a fairly simple application of existing methods to a problem, and  there remain some methodological issues (see below).

General Comments
- The claim that the bmp280 device is in most smartphones today doesn’t seem to be backed up by the “comScore” reference (a simple ranking of manufacturers).  Please provide the original source for this information.
- Almost all exciting results based on RNNs are achieved with LSTMs, so calling an RNN with LSTM hidden units a new name IOLSTM seems rather strange - this is simply an LSTM.
- There exist models for modelling multiple levels of abstraction, such as the contextual LSTM of [1]. This would be much more satisfying that the two level approach taken here, would likely perform better, would replace the need for the clustering method, and would solve issues such as the user being on the roof.  The only caveat is that it may require an encoding of the building (through a one-hot encoding) to ensure that the relationship between the floor height and barometric pressure is learnt. For unseen buildings a background class could be used, the estimators as used before, or aggregation of the other buildings by turning the whole vector on.
- It’s not clear if a bias of 1 was added to the forget gate of the LSTM or not. This has been shown to improve results [2].
- Overall the whole pipeline feels very ad-hoc, with many hand-tuned parameters.  Notwithstanding the network architecture, here I’m referring to the window for the barometric pressure, the Jaccard distance threshold, the binary mask lengths, and the time window for selecting p0.
- Are there plans to release the data and/or the code for the experiments? Currently the results would be impossible to reproduce.
- The typo of accuracy given by the authors is somewhat worrying, given that the result is repeated several times in the paper.

Typographical Issues
- Page 1:  ”floor-level accuracy” back ticks
- Page  4:   Figure  4.1→Figure  1;  Nawarathne  et  al  Nawarathne  et  al.→Nawarathne et al.
- Page 6:  ”carpet to carpet” back ticks
- Table 2:  What does -4+ mean?
- References.  The references should have capitalisation where appropriate.For example,  Iodetector→IODetector,  wi-fi→Wi-Fi,  apple→Apple, iphone→iPhone, i→I etc.

[1]  Shalini Ghosh, Oriol Vinyals, Brian Strope, Scott Roy, Tom Dean, and LarryHeck. Contextual LSTM (CLSTM) models for large scale NLP tasks. arXivpreprint arXiv:1602.06291, 2016.
[2]  Rafal Jozefowicz, Wojciech Zaremba, and Ilya Sutskever.  An empirical exploration of recurrent network architectures.  InProceedings of the 32nd International Conference on Machine Learning (ICML-15), pages 2342–2350,2015","[6, 6, 7]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Good paper, accept']","[4, 3, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review acknowledges the work done but points out limitations and suggests alternative approaches. The reviewer's tone is direct and critical, but not disrespectful. The suggestion to use a more sophisticated model and the pointing out of typos indicate a desire to see the work improved. Therefore, the sentiment is closer to neutral than overly positive, but still leans towards the positive side due to the constructive criticism offered. The language used is formal and professional, without resorting to personal attacks or overly negative phrasing.",20.0,70.0
Identifying Analogies Across Domains,"['Yedid Hoshen', 'Lior Wolf']",Accept,2018,"[6, 19]","[11, 24]","[74, 413]","[32, 213]","[41, 165]","[1, 35]","The paper presents a method for finding related images (analogies) from different domains based on matching-by-synthesis. The general idea is interesting and the results show improvements over previous approaches, such as CycleGAN (with different initializations, pre-learned or not). The algorithm is tested on three datasets.

While the approach has some strong positive points, such as good experiments and theoretical insights (the idea to match by synthesis and the proposed loss which is novel, and combines the proposed concepts), the paper lacks clarity and sufficient details.

Instead of the longer intro and related work discussion, I would prefer to see a Figure with the architecture and more illustrative examples to show that the insights are reflected in the experiments. Also, the matching part, which is discussed at the theoretical level, could be better explained and presented at a more visual level. It is hard to understand sufficiently well what the formalism means without more insight.

Also, the experiments need more details. For example, it is not clear what the numbers in Table 2 mean.



","[5, 4, 7]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Good paper, accept']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer acknowledges the merits of the paper, such as its interesting idea, improved results, good experiments, and theoretical insights. They call these aspects ""strong positive points."" However, they also point out significant areas for improvement regarding clarity and detail, particularly in the introduction, related work discussion, figure presentation, matching part explanation, and experimental details. The criticism, while direct, is presented in a constructive and helpful manner.",50.0,70.0
Bi-Directional Block Self-Attention for Fast and Memory-Efficient Sequence Modeling,"['Tao Shen', 'Tianyi Zhou', 'Guodong Long', 'Jing Jiang', 'Chengqi Zhang']",Accept,2018,"[2, 10, 8, 8, 29]","[7, 15, 13, 13, 34]","[63, 112, 200, 123, 413]","[27, 55, 89, 59, 229]","[35, 48, 81, 54, 44]","[1, 9, 30, 10, 140]","This paper introduces bi-directional block self-attention model (Bi-BioSAN) as a general-purpose encoder for sequence modeling tasks in NLP. The experiments include tasks like natural language inference, reading comprehension (SquAD), semantic relatedness and sentence classifications. The new model shows decent performance when comparing with Bi-LSTM, CNN and other baselines while running at a reasonably fast speed.

The advantage of this model is that we can use little memory (as in RNNs) and enjoy the parallelizable computation as in (SANs), and achieve similar (or better) performance.

While I do appreciate the solid experiment section, I don't think the model itself is sufficient contribution for a publication at ICLR. First, there is not much innovation in the model architecture. The idea of the Bi-BioSAN model simply to split the sentence into blocks and compute self-attention for each of them, and then using the same mechanisms as a pooling operation followed by a fusion level. I think this more counts as careful engineering of the SAN model rather than a main innovation. Second, the model introduces much more parameters. In the experiments, it can easily use 2 times parameters than the commonly used encoders. What if we use the same amount of parameters for Bi-LSTM encoders? Will the gap between the new model and the commonly used ones be smaller?

====

I appreciate the answers the authors added and I change the score to 6.","[6, 6, 9]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Top 15% of accepted papers, strong accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with a positive appreciation of the experiments and performance but then lists two major drawbacks: lack of innovation and increased parameters. The reviewer acknowledges the authors' response and improves the score, indicating a slightly positive sentiment overall. The language is formal and professional, without personal attacks.",20.0,70.0
WHAI: Weibull Hybrid Autoencoding Inference for Deep Topic Modeling,"['Hao Zhang', 'Bo Chen', 'Dandan Guo', 'Mingyuan Zhou']",Accept,2018,"[4, 13, 8, 11]","[9, 18, 12, 16]","[41, 136, 32, 213]","[15, 46, 12, 99]","[12, 24, 9, 92]","[14, 66, 11, 22]","The authors propose a hybrid Bayesian inference approach for deep topic models that integrates stochastic gradient MCMC for global parameters and Weibull-based multilayer variational autoencoders (VAEs) for local parameters. The decoding arm of the VAE consists of deep latent Dirichlet allocation, and an upward-downward structure for the encoder. Gamma distributions are approximated as Weibull distributions since the Kullback-Leibler divergence is known and samples can be efficiently drawn from a transformation of samples from a uniform distribution. 

The results in Table 1 are concerning for several reasons, i) the proposed approach underperfroms DLDA-Gibbs and DLDA-TLASGR. ii) The authors point to the scalability of the mini-batch-based algorithms, however, although more expensive, DLDA-Gibbs, is not prohibitive given results for Wikipedia are provided. iii) The proposed approach is certainly faster at test time, however, it is not clear to me in which settings such speed (compared to Gibbs) would be needed, provided the unsupervised nature of the task at hand. iv) It is not clear to me why there is no test-time difference between WAI and WHAI, considering that in the latter, global parameters are sampled via stochastic-gradient MCMC. One possible explanation being that during test time, the approach does not use samples from W but rather a summary of them, say posterior means, in which case, it defeats the purpose of sampling from global parameters, which may explain why WAI and WHAI perform about the same in the 3 datasets considered.

- \Phi is in a subset of R_+, in fact, columns of \Phi are in the P_0-dimensional simplex.
- \Phi should have K_1 columns not K.
- The first paragraph in Page 5 is very confusing because h is introduced before explicitly connecting it to k and \lambda. Also, if k = \lambda, why introduce different notations?","[5, 6, 6]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[4, 4, 2]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']","The review starts with a neutral summary of the proposed approach. However, the following paragraph raises several concerns regarding the performance and design choices of the model. The reviewer finds the results 'concerning' and questions the advantage of the proposed method over existing ones. While the language avoids harsh or accusatory tones, the overall message conveys a negative sentiment due to the numerous concerns raised. The reviewer also points out specific areas of confusion and potential errors in the paper. ",-50.0,60.0
Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models,"['Pouya Samangouei', 'Maya Kabkab', 'Rama Chellappa']",Accept,2018,"[4, 7, 38]","[6, 11, 43]","[16, 12, 910]","[9, 7, 494]","[6, 4, 140]","[1, 1, 276]","This paper presents a method to cope with adversarial examples in classification tasks, leveraging a generative model of the inputs.  Given an accurate generative model of the input, this approach first projects the input onto the manifold learned by the generative model (the idea being that inputs on this manifold reflect the non-adversarial input distribution).  This projected input is then used to produce the classification probabilities.  The authors test their method on various adversarially constructed inputs (with varying degrees of noise). 

Questions/Comments:

- I am interested in unpacking the improvement of Defense-GAN over the MagNet auto-encoder based method.  Is the MagNet auto-encoder suffering lower accuracy because the projection of an adversarial image is based on an encoding function that is learned only on true data?  If the decoder from the MagNet approach were treated purely as a generative model, and the same optimization-based projection approach (proposed in this work) was followed, would the results be comparable?  

- Is there anything special about the GAN approach, versus other generative approaches? 

- In the black-box vs. white-box scenarios, can the attacker know the GAN parameters?  Is that what is meant by the ""defense network"" (in experiments bullet 2)?

- How computationally expensive is this approach take compared to MagNet or other adversarial approaches? 

Quality: The method appears to be technically correct.

Clarity: This paper clearly written; both method and experiments are presented well. 

Originality: I am not familiar enough with adversarial learning to assess the novelty of this approach. 

Significance: I believe the main contribution of this method is the optimization-based approach to project onto a generative model's manifold.  I think this kernel has the potential to be explored further (e.g. computational speed-up, projection metrics).","[6, 6, 8]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Top 50% of accepted papers, clear accept']","[3, 3, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer provides constructive questions and feedback, aiming to improve the paper rather than reject it. They acknowledge the technical correctness, clarity, and potential significance of the work. While they have questions about novelty and comparisons, their tone is inquisitive and suggests a positive outlook overall.",60.0,80.0
Unsupervised Learning of Goal Spaces for Intrinsically Motivated Goal Exploration,"['Alexandre Péré', 'Sébastien Forestier', 'Olivier Sigaud', 'Pierre-Yves Oudeyer']",Accept,2018,"[1, 6, 19, 20]","[3, 11, 24, 25]","[6, 14, 139, 209]","[2, 6, 64, 92]","[3, 4, 44, 72]","[1, 4, 31, 45]","The paper investigates different representation learning methods to create a latent space for intrinsic goal generation in guided exploration algorithms.  The research is in principle very important and interesting.

The introduction discusses a great deal about intrinsic motivations and about goal generating algorithms. This is really great, just that the paper only focuses on a very small aspect of learning a state representation in an agent that has no intrinsic motivation other than trying to achieve random goals.
I think the paper (not only the Intro) could be a bit condensed to more concentrate on the actual contribution. 

The contribution is that the quality of the representation and the sampling of goals is important for the exploration performance and that classical methods like ISOMap are better than Autoencoder-type methods. 

Also, it is written in the Conclusions (and in other places): ""[..] we propose a new intrinsically Motivated goal exploration strategy...."". This is not really true.  There is nothing new with the intrinsically motivated selection of goals here, just that they are in another space. Also, there is no intrinsic motivation. I also think the title is misleading.

The paper is in principle interesting. However, I doubt that the experimental evaluations are substantial enough for profound conclusion. 

Several points of critic: 
- the input space was very simple in all experiments, not suitable for distinguishing between the algorithms, for instance, ISOMap typically suffers from noise and higher dimensional manifolds, etc.
- only the ball/arrow was in the input image, not the robotic arm. I understand this because in phase 1 the robot would not move, but this connects to the next point:
- The representation learning is only a preprocessing step requiring a magic first phase.
    -> Representation is not updated during exploration
- The performance of any algorithm (except FI) in the Arm-Arrow task is really bad but without comment. 
- I am skeptical about the VAE  and RFVAE results. The difference between Gaussian sampling and the KDE is a bit alarming, as the KL in the VAE training is supposed to match the p(z) with N(0,1). Given the power of the encoder/decoder it should be possible to properly represent the simple embedded 2D/3D manifold and not just a very small part of it as suggested by Fig 10. 
I have a hard time believing these results. I urge you to check for any potential errors made. If there are not mistakes then this is indeed alarming.

Questions:
- Is it true that the robot always starts from same initial condition?! Context=Emptyset. 
- For ISOMap etc, you also used a 10dim embedding?

Suggestion:
- The main problem seems to be that some algorithms are not representing the whole input space.
- an additional measure that quantifies the difference between true input distribution and reproduced input distribution could tier the algorithms apart and would measure more what seems to be relevant here.  One could for instance measure the KL-divergence between the true input and the sampled (reconstructed) input (using samples and KDE or the like). 
- This could be evaluated on many different inputs (also those with a bit more complicated structure) without actually performing the goal finding.
- BTW: I think Fig 10 is rather illustrative and should be somehow in the main part of the paper
 
On the positive side, the paper provides lots of details in the Appendix.
Also, it uses many different Representation Learning algorithms and uses measures from manifold learning to access their quality.

In the related literature, in particular concerning the intrinsic motivation, I think the following papers are relevant:
J. Schmidhuber, PowerPlay: training an increasingly general problem solver by continually searching for the simplest still unsolvable problem. Front. Psychol., 2013.

and

G. Martius, R. Der, and N. Ay. Information driven self-organization of complex robotic behaviors. PLoS ONE, 8(5):e63400, 2013.


Typos and small details:
p3 par2: for PCA you cited Bishop. Not critical, but either cite one the original papers or maybe remove the cite altogether
p4 par-2: has multiple interests...: interests -> purposes?
p4 par-1: Outcome Space to the agent is is ...
Sec 2.2 par1: are rapidly mentioned... -> briefly
Sec 2.3 ...Outcome Space O, we can rewrite the architecture as:
  and then comes the algorithm. This is a bit weird
Sec 3: par1: experimental campaign -> experiments?
p7: Context Space: the object was reset to a random position or always to the same position?
Footnote 14: superior to -> larger than
p8 par2: Exploration Ratio Ratio_expl... probably also want to add (ER) as it is later used
Sec 4: slightly underneath -> slightly below
p9 par1: unfinished sentence: It is worth noting that the....
one sentence later: RP architecture? RPE?
Fig 3: the error of the methods (except FI) are really bad. An MSE of 1 means hardly any performance!
p11 par2: for e.g. with the SAGG..... grammar?

Plots in general: use bigger font sizes.

","[7, 6, 7]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Good paper, accept']","[4, 2, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer expresses a mixed sentiment, acknowledging the paper's interesting research direction but harboring significant doubts about the methodology and results. While they find the research topic important, they criticize the paper for overstating its contributions and for weaknesses in the experimental setup. The language used is quite direct and critical, particularly when pointing out potential errors in the results, but maintains a professional tone overall.",20.0,40.0
Word translation without parallel data,"['Guillaume Lample', 'Alexis Conneau', ""Marc'Aurelio Ranzato"", 'Ludovic Denoyer', 'Hervé Jégou']",Accept,2018,"[4, 3, 13, 16, 16]","[9, 8, 18, 20, 21]","[51, 56, 106, 147, 178]","[24, 27, 57, 88, 89]","[27, 29, 44, 40, 59]","[0, 0, 5, 19, 30]","An unsupervised approach is proposed to build bilingual dictionaries without parallel corpora, by aligning the monolingual word embeddings spaces, i.a. via adversarial learning.

The paper is very well-written and makes for a rather pleasant read, save for some need for down-toning the claims to novelty as voiced in the comment re: Ravi & Knight (2011) or simply in general: it's a very nice paper, I enjoy reading it *in spite*, and not *because* of the text sales-pitching itself at times.

There are some gaps in the awareness of the related work in the sub-field of bilingual lexicon induction, e.g. the work by Vulic & Moens (2016).

The evaluation is for the most part intrinsic, and it would be nice to see the approach applied downstream beyond the simplistic task of English-Esperanto translation: plenty of outlets out there for applying multilingual word embeddings. Would be nice to see at least some instead of the plethora of intrinsic evaluations of limited general interest.

In my view, to conclude, this is still a very nice paper, so I vote clear accept, in hope to see these minor flaws filtered out in the revision.","[8, 9, 3]","[' Top 50% of accepted papers, clear accept', ' Top 15% of accepted papers, strong accept', ' Clear rejection']","[3, 4, 5]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The reviewer explicitly states ""this is still a very nice paper, so I vote clear accept"", which clearly indicates a positive sentiment. While there are points of criticism, they are presented constructively and are outweighed by the positive feedback. The language used is formal and polite, without resorting to harsh or negative vocabulary.",75.0,75.0
Critical Points of Linear Neural Networks: Analytical Forms and Landscape Properties,"['Yi Zhou', 'Yingbin Liang']",Accept,2018,"[4, 15]","[8, 20]","[81, 275]","[37, 112]","[36, 99]","[8, 64]","Authors of this paper provided full characterization of the analytical forms of the critical points for the square loss function of three types of neural networks: shallow linear networks, deep linear networks and shallow ReLU nonlinear networks. The analytical forms of the critical points have direct implications on the values of the corresponding loss functions, achievement of global minimum, and various landscape properties around these critical points.

The paper is well organized and well written. Authors exploited the analytical forms of the critical points to provide a new proof for characterizing the landscape around the critical points. This technique generalizes existing work under full relaxation of assumptions. In the linear network with one hidden layer, it generalizes the work Baldi & Hornik (1989) with arbitrary network parameter dimensions and any data matrices; In the deep linear networks, it generalizes the result in Kawaguchi (2016) under no assumptions on the network parameters and data matrices. Moreover, it also provides new characterization for shallow ReLU nonlinear networks, which is not discussed in previous work.

The results obtained from the analytical forms of the critical points are interesting, but one problem is that how to obtain the proper solution of equation (3)? In the Example 1, authors gave a concrete example to demonstrate both local minimum and local maximum do exist in the shallow ReLU nonlinear networks by properly choosing these matrices satisfying (12). It will be interesting to see how to choose these matrices for all the studied networks with some concrete examples.","[7, 7, 6]","[' Good paper, accept', ' Good paper, accept', ' Marginally above acceptance threshold']","[3, 5, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is positive about the paper, highlighting its contribution and clarity. It praises the organization, writing, and novelty of the work. While it raises a valid question about finding solutions using the presented equation, it frames it as an interesting challenge rather than a flaw. The reviewer's suggestion for additional examples further indicates a desire to see the research developed further, a positive sign.",80.0,90.0
Meta-Learning and Universality: Deep Representations and Gradient Descent can Approximate any Learning Algorithm,"['Chelsea Finn', 'Sergey Levine']",Accept,2018,"[6, 10]","[11, 15]","[306, 743]","[128, 326]","[172, 396]","[6, 21]","The paper provides proof that gradient-based meta-learners (e.g. MAML) are ""universal leaning algorithm approximators"".

Pro:
- Generally well-written with a clear (theoretical) goal
- If the K-shot proof is correct*, the paper constitutes a significant contribution to the theoretical understanding of meta-learning.
- Timely and relevant to a large portion of the ICLR community (assuming the proofs are correct)

Con:
- The theoretical and empirical parts seem quite disconnected. The theoretical results are not applied nor demonstrated in the empirical section and only functions as an underlying premise. I wonder if a purely theoretical contribution would be preferable (or with even fewer empirical results).

* It has not yet been possible for me to check all the technical details and proofs.
","[7, 6, 6]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[1, 1, 3]","["" The reviewer's evaluation is an educated guess"", "" The reviewer's evaluation is an educated guess"", ' The reviewer is fairly confident that the evaluation is correct']","The review is overall positive. It highlights the paper's significance and relevance while acknowledging its clarity. The reviewer's only major concern, the disconnect between the theoretical and empirical parts, is presented as a suggestion rather than a harsh criticism. The reviewer also admits to not fully checking the proofs, suggesting an openness to being impressed if those hold up. The language used is neutral and professional throughout.",60.0,80.0
Maximum a Posteriori Policy Optimisation,"['Abbas Abdolmaleki', 'Jost Tobias Springenberg', 'Yuval Tassa', 'Remi Munos', 'Nicolas Heess', 'Martin Riedmiller']",Accept,2018,"[8, 7, 12, 23, 10, 26]","[13, 12, 17, 28, 15, 31]","[69, 76, 51, 266, 200, 168]","[33, 37, 23, 143, 81, 100]","[30, 35, 24, 96, 111, 47]","[6, 4, 4, 27, 8, 21]","The paper presents a new algorithm for inference-based reinforcement learning for deep RL. The algorithm decomposes the policy update in two steps, an E and an M-step. In the E-step, the algorithm estimates a variational distribution q which is subsequentially used for the M-step to obtain a new policy. Two versions of the algorithm are presented, using a parametric or a non-parametric (sample-based) distribution for q. The algorithm is used in combination with the retrace algorithm to estimate the q-function, which is also needed in the policy update.

This is a well written paper presenting an interesting algorithm. The algorithm is similar to other inference-based RL algorithm, but is the first application of inference based RL to deep reinforcement learning. The results look very promising and define a new state of the art or deep reinforcement learning in continuous control, which is a very active topic right now. Hence, I think the paper should be accepted. 


I do have a few comments / corrections / questions about the paper:

- There are several approaches that already use the a combination of the KL-constraint with reverse KL on a non-parametric distribution and subsequently an M-projection to obtain again a parametric distribution, see HiREPS, non-parametric REPS [Hoof2017, JMLR] or AC-REPS [Wirth2016, AAAI]. These algorithms do not use the inference-based view but the trust region justification. As in the non-parametric case, the asymptotic performance guarantees from the EM framework are gone, why is it beneficial to formulate it with EM instead of directly with a trust region of the expected reward?

- It is not clear to me whether the algorithm really optimizes the original maximum a posteriori objective defined in Equation 1. First, alpha changes every iteration of the algorithm while the objective assumes that alpha is constant. This means that we change the objective all the time which is theoretically a bit weird. Moreover, the presented algorithm also changes the prior all the time (in order to introduce the 2nd trust region) in the M-step. Again, this changes the objective, so it is unclear to me what exactly is maximised in the end. Would it not be cleaner to start with the average reward objective (no prior or alpha) and then introduce both trust regions just out of the motivation that we need trust regions in policy search? Then the objective is clearly defined.    

- I did not get whether the additional ""one-step KL regularisation"" is obtained from the lower bound or just added as additional regularisation? Could you explain?

- The algorithm has now 2 KL constraints, for E and M step. Is the epsilon for both the same or can we achieve better performance by using different epsilons?

- I think the following experiments would be very informative:

   - MPO without trust region in M-step
   
   - MPO without retrace algorithm for getting the Q-value

   - test different epsilons for E and M step


","[7, 5, 6]","[' Good paper, accept', ' Marginally below acceptance threshold', ' Marginally above acceptance threshold']","[5, 4, 1]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', "" The reviewer's evaluation is an educated guess""]","The review starts with positive statements, highlighting the novelty, strong performance, and clear writing of the paper. The reviewer recommends acceptance, which points towards a positive sentiment. While the reviewer raises several questions and suggests additional experiments, these are constructive and aim to further improve the paper. The language used throughout is formal and polite, typical for scientific peer reviews.",75.0,80.0
The Implicit Bias of Gradient Descent on Separable Data,"['Daniel Soudry', 'Elad Hoffer', 'Mor Shpigel Nacson', 'Nathan Srebro']",Accept,2018,"[9, 4, 1, 18]","[14, 8, 6, 23]","[90, 35, 16, 269]","[35, 14, 8, 140]","[42, 19, 7, 115]","[13, 2, 1, 14]","(a) Significance
The main contribution of this paper is to characterize the implicit bias introduced by gradient descent on separable data. The authors show the exact form of this bias (L_2 maximum margin separator), which is independent of the initialization and step size. The corresponding slow convergence rate explains the phenomenon that the predictor can continue to improve even when the training loss is already small. The result of this paper can inspire the study of the implicit bias introduced by gradient descent variants or other optimization methods, such as coordinate descent. In addition, the proposed analytic framework seems promising since it may be extended to analyze other models, like neural networks.

(b) Originality
This is the first work to give the detailed characterizations of the implicit bias of gradient descent on separable data. The proposed assumptions are reasonable, but it seems to limit to the loss function with exponential tail. I’m curious whether the result in this paper can be applied to other loss functions, such as hinge loss.

(c) Clarity & Quality 
The presentation of this paper is OK. However, there are some places can be improved in this paper. For example, in Lemma 1, results (3) and (4) can be combined together. It is better for the authors to use another section to illustrate experimental settings instead of writing them in the caption of Figure 3.1. 

Minor comments: 
1. In Lemma 1 (4), w^T(t)->w(t)^T
2. In the proof of Lemma 1, it’s better to use vector 0 for the gradient L(w)
3. In Theorem 4, the authors should specify eta
4. In appendix A, page 11, beta is double used
5. In appendix D, equation (D.5) has an extra period
","[8, 5, 7]","[' Top 50% of accepted papers, clear accept', ' Marginally below acceptance threshold', ' Good paper, accept']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer highlights the significance and originality of the paper, using positive language such as ""main contribution"", ""promising"", and ""reasonable assumptions"". While they point out areas for improvement, these are presented constructively and focus on clarity and minor corrections. The overall tone suggests the reviewer sees merit in the work.",70.0,80.0
Online Learning Rate Adaptation with Hypergradient Descent,"['Atilim Gunes Baydin', 'Robert Cornish', 'David Martinez Rubio', 'Mark Schmidt', 'Frank Wood']",Accept,2018,"[11, 2, 2, 14, 23]","[15, 6, 7, 19, 28]","[67, 10, 12, 117, 153]","[16, 5, 5, 55, 71]","[48, 5, 7, 51, 71]","[3, 0, 0, 11, 11]","SUMMARY:

The authors reinvent a 20 years old technique for adapting a global or component-wise learning rate for gradient descent. The technique can be derived as a gradient step for the learning rate hyperparameter, or it can be understood as a simple and efficient adaptation technique.


GENERAL IMPRESSION:

One central problem of the paper is missing novelty. The authors are well aware of this. They still manage to provide added value.
Despite its limited novelty, this is a very interesting and potentially impactful paper. I like in particular the detailed discussion of related work, which includes some frequently overlooked precursors of modern methods.


CRITICISM:

The experimental evaluation is rather solid, but not perfect. It considers three different problems: logistic regression (a convex problem), and dense as well as convolutional networks. That's a solid spectrum. However, it is not clear why the method is tested only on a single data set: MNIST. Since it is entirely general, I would rather expect a test on a dozen different data sets. That would also tell us more about a possible sensitivity w.r.t. the hyperparameters \alpha_0 and \beta.

The extensions in section 5 don't seem to be very useful. In particular, I cannot get rid of the impression that section 5.1 exists for the sole purpose of introducing a convergence theorem. Analyzing the actual adaptive algorithm would be very interesting. In contrast, the present result is trivial and of no interest at all, since it requires knowing a good parameter setting, which defeats a large part of the value of the method.


MINOR POINTS:

page 4, bottom: use \citep for Duchi et al. (2011).

None of the figures is legible on a grayscale printout of the paper. Please do not use color as the only cue to identify a curve.

In figure 2, top row, please display the learning rate on a log scale.

page 8, line 7 in section 4.3: ""the the"" (unintended repetition)

End of section 4: an increase from 0.001 to 0.001002 is hardly worth reporting - or am I missing something?
","[6, 7, 7]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Good paper, accept']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer acknowledges the limited novelty but finds the paper interesting and potentially impactful. They praise the detailed discussion of related work. However, they criticize the experimental evaluation for not being extensive enough and find the extensions in section 5 not very useful. The language used is polite and professional, providing constructive criticism with specific examples and suggestions for improvement.",60.0,80.0
Syntax-Directed Variational Autoencoder for Structured Data,"['Hanjun Dai', 'Yingtao Tian', 'Bo Dai', 'Steven Skiena', 'Le Song']",Accept,2018,"[5, 3, 16, 34, 15]","[10, 8, 21, 38, 20]","[91, 41, 315, 233, 343]","[47, 19, 147, 122, 185]","[42, 22, 135, 44, 122]","[2, 0, 33, 67, 36]","NOTE: 

Would the authors kindly respond to the comment below regarding Kekulisation of the Zinc dataset? Fair comparison of the data is a serious concern. I have listed this review as a good for publication due to the novelty of ideas presented, but the accusation of misrepresentation below is a serious one and I would like to know the author's response.

*Overview*

This paper presents a method of generating both syntactically and semantically valid data from a variational autoencoder model using ideas inspired by compiler semantic checking. Instead of verifying the semantic correctness offline of a particular discrete structure, the authors propose “stochastic lazy attributes”, which amounts to loading semantic constraints into a CFG and using a tailored latent-space decoder algorithm that guarantees both syntactic semantic valid. Using Bayesian Optimization, search over this space can yield decodings with targeted properties.

Many of the ideas presented are novel. The results presented are state-of-the art. As noted in the paper, the generation of syntactically and semantically valid data is still an open problem. This paper presents an interesting and valuable solution, and as such constitutes a large advance in this nascent area of machine learning.

*Remarks on methodology*

By initializing a decoding by “guessing” a value, the decoder will focus on high-probability starting regions of the space of possible structures. It is not clear to me immediately how this will affect the output distribution. Since this process on average begins at high-probability region and makes further decoding decisions from that starting point, the output distribution may be biased since it is the output of cuts through high-probability regions of the possible outputs space. Does this sacrifice exploration for exploitation in some quantifiable way? Some exploration of this issue or commentary would be valuable. 

*Nitpicks*

I found the notion of stochastic predetermination somewhat opaque, and section 3 in general introduces much terminology, like lazy linking, that was new to me coming from a machine learning background. In my opinion, this section could benefit from a little more expansion and conceptual definition.

The first 3 sections of the paper are very clearly written, but the remainder has many typos and grammatical errors (often word omission). The draft could use a few more passes before publication.
","[7, 5, 3]","[' Good paper, accept', ' Marginally below acceptance threshold', ' Clear rejection']","[3, 1, 2]","[' The reviewer is fairly confident that the evaluation is correct', "" The reviewer's evaluation is an educated guess"", ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']","The reviewer explicitly states ""This paper presents an interesting and valuable solution, and as such constitutes a large advance in this nascent area of machine learning."" This indicates a positive sentiment. They do have a ""serious concern"", but acknowledge the novelty and list it as ""good for publication."" The language used is formal and polite, but direct with phrases like ""It is not clear to me"" and ""The draft could use a few more passes."" However, it avoids harsh or accusatory language.",75.0,75.0
Learning to Count Objects in Natural Images for Visual Question Answering,"['Yan Zhang', 'Jonathon Hare', 'Adam Prügel-Bennett']",Accept,2018,"[1, 15, 26]","[3, 20, 31]","[8, 101, 107]","[4, 57, 48]","[4, 27, 21]","[0, 17, 38]","Summary
 - This paper mainly focuses on a counting problem in visual question answering (VQA) using attention mechanism. The authors propose a differentiable counting component, which explicitly counts the number of objects. Given attention weights and corresponding proposals, the model deduplicates overlapping proposals by eliminating intra-object edges and inter-object edges using graph representation for proposals. In experiments, the effectiveness of proposed model is clearly shown in counting questions on both a synthetic toy dataset and the widely used VQA v2 dataset.

Strengths
 - The proposed model begins with reasonable motivation and shows its effectiveness in experiments clearly. 
 - The architecture of the proposed model looks natural and all components seem to have clear contribution to the model.
 - The proposed model can be easily applied to any VQA model using soft attention. 
 - The paper is well written and the contribution is clear.

Weaknesses
 - Although the proposed model is helpful to model counting information in VQA, it fails to show improvement with respect to a couple of important baselines: prediction from image representation only and from the combination of image representation and attention weights. 
 - Qualitative examples of intermediate values in counting component--adjacency matrix (A), distance matrix (D) and count matrix (C)--need to be presented to show the contribution of each part, especially in the real examples that are not compatible with the strong assumptions in modeling counting component.

Comments
 - It is not clear if the value of count ""c"" is same with the final answer in counting questions. 

","[6, 6, 4]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Ok but not good enough - rejection']","[3, 3, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review acknowledges the strengths of the paper, such as clear motivation, effective architecture, and good writing. However, it also points out a significant weakness: the model doesn't show improvement over some important baselines. The reviewer provides constructive criticism and suggestions for improvement. The language used is objective and professional.",50.0,80.0
Smooth Loss Functions for Deep Top-k Classification,"['Leonard Berrada', 'Andrew Zisserman', 'M. Pawan Kumar']",Accept,2018,"[3, 34, 17]","[8, 39, 22]","[15, 745, 118]","[5, 455, 57]","[9, 187, 43]","[1, 103, 18]","This paper introduces a smooth surrogate loss function for the top-k SVM, for the purpose of plugging the SVM to the deep neural networks. The idea is to replace the order statistics, which is not smooth and has a lot of zero partial derivatives, to the exponential of averages, which is smooth and is a good approximation of the order statistics by a good selection of the ""temperature parameter"". The paper is well organized and clearly written. The idea deserves a publication.

On the other hand, there might be better and more direct solutions to reduce the combinatorial complexity. When the temperature parameter is small enough, both of the original top-k SVM surrogate loss (6) and the smooth loss (9) can be computed precisely by sorting the vector s first, and take a good care of the boundary around s_{[k]}.","[8, 7, 6]","[' Top 50% of accepted papers, clear accept', ' Good paper, accept', ' Marginally above acceptance threshold']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review starts with positive statements, praising the paper's novelty, organization, and clarity. The reviewer believes the paper deserves publication. However, they also provide constructive criticism, suggesting potential alternative approaches. The language used is objective and professional throughout. Therefore, the sentiment is positive, but not overly enthusiastic, and the politeness is high.",60.0,80.0
Minimal-Entropy Correlation Alignment for Unsupervised Deep Domain Adaptation,"['Pietro Morerio', 'Jacopo Cavazza', 'Vittorio Murino']",Accept,2018,"[7, 3, 29]","[12, 7, 34]","[85, 41, 443]","[41, 18, 286]","[33, 20, 59]","[11, 3, 98]","Summary:
This paper proposes minimal-entropy correlation alignment, an unsupervised domain adaptation algorithm which links together two prior class of methods: entropy minimization and correlation alignment. Interesting new idea. Make a simple change in the distance function and now can perform adaptation which aligns with minimal entropy on target domain and thus can allow for removal of hyperparameter (or automatic validation of correct one).

Strengths
-  The paper is clearly written and effectively makes a simple claim that geodesic distance minimization is better aligned to final performance than euclidean distance minimization between source and target. 
- Figures 1 and 2 (right side) are particularly useful for fast understanding of the concept and main result.


Questions/Concerns:
- Can entropy minimization on target be used with other methods for DA param tuning? Does it require that the model was trained to minimize the geodesic correlation distance between source and target?
- It would be helpful to have a longer discussion on the connection with Geodesic flow kernel [1] and other unsupervised manifold based alignment methods [2]. Is this proposed approach an extension of this prior work to the case of non-fixed representations in the same way that Deep CORAL generalized CORAL?
- Why does performance suffer compared to TRIPLE on the SYN->SVHN task? Is there some benefit to the TRIPLE method which may be combined with the MECA approach?

					
[1] Boqing Gong, Yuan Shi, Fei Sha, and Kristen Grauman. Geodesic flow kernel for unsupervised domain adaptation. In CVPR, 2012.
					
[2] Raghuraman Gopalan and Ruonan Li. Domain adaptation for object recognition: An unsupervised approach. In ICCV, 2011. 
","[7, 6, 8]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Top 50% of accepted papers, clear accept']","[5, 5, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with positive remarks, highlighting the novelty and clarity of the paper. The reviewer finds the core idea interesting and praises the illustrative figures. While the reviewer poses several 'Questions/Concerns,' these are constructive and aim to encourage further exploration and connections with existing literature. The tone throughout is inquisitive rather than critical, suggesting a positive disposition towards the paper.",60.0,80.0
PixelNN: Example-based Image Synthesis,"['Aayush Bansal', 'Yaser Sheikh', 'Deva Ramanan']",Accept,2018,"[6, 15, 19]","[11, 20, 24]","[31, 148, 255]","[12, 76, 134]","[18, 37, 102]","[1, 35, 19]","This paper proposes a compositional nearest-neighbors approach to image synthesis, including results on several conditional image generation datasets. 

Pros:
- Simple approach based on nearest-neighbors, likely easier to train compared to GANs.
- Scales to high-resolution images.

Cons:
- Requires a potentially costly search procedure to generate images.
- Seems to require relevant objects and textures to be present in the training set in order to succeed at any given conditional image generation task.","[7, 6, 8]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Top 50% of accepted papers, clear accept']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is written in a neutral, scientific tone. It lists both positive and negative aspects of the paper without using any explicitly positive or negative adjectives. The cons are presented as objective observations rather than harsh criticisms.",0.0,50.0
Trust-PCL: An Off-Policy Trust Region Method for Continuous Control,"['Ofir Nachum', 'Mohammad Norouzi', 'Kelvin Xu', 'Dale Schuurmans']",Accept,2018,"[3, 10, 4, 30]","[8, 15, 8, 35]","[104, 136, 29, 286]","[42, 59, 13, 182]","[61, 72, 15, 82]","[1, 5, 1, 22]","This paper presents a policy gradient method that employs entropy regularization and entropy constraint at the same time. The entropy regularization on action probability is to encourage the exploration of the policy, while the entropy constraint is to stabilize the gradient.

The major weakness of this paper is the unclear presentation. For example, the algorithm is never fully described, though a handful variants are discussed. How the off-policy version is implemented is missing.

In experiments, why the off-policy version of TRPO is not compared. Comparing the on-policy results, PCL does not show a significant advantage over TRPO. Moreover, the curves of TRPO is so unstable, which is a bit uncommon. 

What is the exploration strategy in the experiments? I guess it was softmax probability. However, in many cases, softmax does not perform a good exploration, even if the entropy regularization is added.

Another issue is the discussion of the entropy regularization in the objective function. This regularization, while helping exploration, do changes the original objective. When a policy is required to pass through a very narrow tunnel of states, the regularization that forces a wide action distribution could not have a good performance. Thus it would be more interesting to see experiments on more complex benchmark problems like humanoids.","[5, 6, 5]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Marginally below acceptance threshold']","[4, 4, 1]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', "" The reviewer's evaluation is an educated guess""]","The review starts with a neutral summary of the paper's topic. However, it quickly points out a ""major weakness"" and lists several concerns and questions about the methodology, clarity, and results. The reviewer also expresses skepticism towards the claimed advantages of the proposed method. The language, while direct and critical, maintains a professional and academic tone. There are no personal attacks or disrespectful remarks.",-50.0,50.0
Stochastic Activation Pruning for Robust Adversarial Defense,"['Guneet S. Dhillon', 'Kamyar Azizzadenesheli', 'Zachary C. Lipton', 'Jeremy D. Bernstein', 'Jean Kossaifi', 'Aran Khanna', 'Animashree Anandkumar']",Accept,2018,"[1, 3, 5, 2, 5, 2, 13]","[4, 8, 10, 7, 10, 4, 18]","[7, 90, 203, 23, 56, 8, 419]","[3, 26, 77, 10, 19, 3, 154]","[4, 56, 117, 13, 25, 4, 223]","[0, 8, 9, 0, 12, 1, 42]","The authors propose to improve the robustness of trained neural networks against adversarial examples by randomly zeroing out weights/activations. Empirically the authors demonstrate, on two different task domains, that one can trade off some accuracy for a little robustness -- qualitatively speaking.

On one hand, the approach is simple to implement and has minimal impact computationally on pre-trained networks. On the other hand, I find it lacking in terms of theoretical support, other than the fact that the added stochasticity induces a certain amount of robustness. For example, how does this compare to random perturbation (say, zero-mean) of the weights? This adds stochasticity as well so why and why not this work? The authors do not give any insight in this regard.

Overall, I still recommend acceptance (weakly) since the empirical results may be valuable to a general practitioner. The paper could be strengthened by addressing the issues above as well as including more empirical results (if nothing else).

","[6, 6, 7]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Good paper, accept']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer acknowledges both the strengths and weaknesses of the paper. While they find the theoretical contribution lacking, they acknowledge the potential practical value of the empirical results. The use of ""weakly"" when recommending acceptance and the suggestion for more empirical results indicate a slightly hesitant but overall positive sentiment. The language is formal and professional throughout, without resorting to harsh or critical language.",30.0,70.0
Learning Sparse Latent Representations with the Deep Copula Information Bottleneck,"['Aleksander Wieczorek*', 'Mario Wieser*', 'Damian Murezzan', 'Volker Roth']",Accept,2018,"[8, 1, 4, 20]","[11, 5, 4, 25]","[17, 19, 4, 97]","[7, 8, 3, 52]","[6, 9, 1, 23]","[4, 2, 0, 22]","The paper proposed a copula-based modification to an existing deep variational information bottleneck model, such that the marginals of the variables of interest (x, y) are decoupled from the DVIB latent variable model, allowing the latent space to be more compact when compared to the non-modified version. The experiments verified the relative compactness of the latent space, and also qualitatively shows that the learned latent features are more 'disentangled'. However, I wonder how sensitive are the learned latent features to the hyper-parameters and optimizations?

Quality: Ok. The claims appear to be sufficiently verified in the experiments. However, it would have been great to have an experiment that actually makes use of the learned features to make predictions. I struggle a little to see the relevance of the proposed method without a good motivating example.

Clarity: Below average. Section 3 is a little hard to understand. Is q(t|x) in Fig 1 a typo? How about t_j in equation (5)? There is a reference that appeared twice in the bibliography (1st and 2nd).

Originality and Significance: Average. The paper (if I understood it correctly) appears to be mainly about borrowing the key ideas from Rey et. al. 2014 and applying it to the existing DVIB model.","[6, 6, 5, 6]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally below acceptance threshold', ' Marginally above acceptance threshold']","[1, 3, 4, 3]","["" The reviewer's evaluation is an educated guess"", ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer acknowledges the paper's contributions (""The claims appear to be sufficiently verified..."") but raises concerns and suggests improvements. The tone is critical but professional, suggesting room for improvement rather than outright rejection. The score considers both the positive and less positive aspects of the review.",20.0,50.0
Meta-Learning for Semi-Supervised Few-Shot Classification,"['Mengye Ren', 'Eleni Triantafillou', 'Sachin Ravi', 'Jake Snell', 'Kevin Swersky', 'Joshua B. Tenenbaum', 'Hugo Larochelle', 'Richard S. Zemel']",Accept,2018,"[4, 3, 4, 4, 9, 25, 14, 30]","[8, 8, 8, 9, 14, 30, 19, 35]","[65, 14, 9, 16, 75, 610, 162, 243]","[29, 7, 4, 8, 35, 353, 64, 136]","[36, 7, 5, 8, 38, 226, 74, 83]","[0, 0, 0, 0, 2, 31, 24, 24]","In this paper, the authors studied the problem of semi-supervised few-shot classification, by extending the prototypical networks into the setting of semi-supervised learning with examples from distractor classes.  The studied problem is interesting, and the paper is well-written. Extensive experiments are performed to demonstrate the effectiveness of the proposed methods.  While the proposed method is a natural extension of the existing works (i.e., soft k-means and meta-learning).On top of that, It seems the authors have over-claimed their model capability at the first place as the proposed model cannot properly classify the distractor examples but just only consider them as a single class of outliers. Overall, I would like to vote for a weakly acceptance regarding this paper.","[6, 6, 6]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review starts with positive statements, highlighting the interesting problem and well-written nature of the paper. It acknowledges the extensive experiments and effectiveness of the method. However, it then introduces a significant concern about overclaiming and limitations in the model's capabilities. This concern is serious enough to dampen the initial positivity, leading to a weak acceptance rather than a strong one. Therefore, the sentiment is slightly above neutral but not overly positive. The language used throughout is professional and polite, without resorting to harsh or critical tones.",30.0,70.0
Deep contextualized word representations,"['Matthew E Peters', 'Mark Neumann', 'Mohit Iyyer', 'Matt Gardner', 'Christopher Clark', 'Kenton Lee', 'Luke Zettlemoyer']",Accept,2018,"[-2, 7]","[1, 12]","[1, 23]","[1, 9]","[0, 2]","[0, 12]","This paper proposes a method to learn contextualized word representations (ELMO) by pretraining a multilayer bidirectional LSTM language model and using representations from all levels of the LSTM in the input or output layer of a supervised task of interest. 
Experiments on various datasets (SNLI, SQuAD, SRL, Coref, NER, SST) show that the proposed method improve over baseline models.
Ablation analysis demonstrate that using all layers of ELMO is always better than just using only the final layer, and that representations learned by ELMO capture basic notions of word senses and part of speeches.

The paper is well written and I think learning contextualized word representations is an important topic.
However, one thing that I am not sure about from experiments in the paper is whether the improvements come from an increase in model capacity and (unlabeled) data used to train the model, or whether there are more interesting things going on.
- What makes the proposed approach different than just a deeper architecture for each of the considered tasks, where some parts of the network are trained using unlabeled data?
- Is the pretraining with unlabeled data necessary, or can we just have this deep architecture and train everything with the available supervised data?
- An ELMO enhanced model has more parameters than the baseline model for each task. What is the performance of the (non deep) baseline method with comparable number of parameters (bigger hidden size)?

More generally, it is not surprising that given sufficient training data, a deeper model (e.g., ELMO enhanced models) with multiple connections across layers will perform better than shallower models with fewer parameters.
I would like to see more analysis and/or explanations on why the proposed method contributes more beyond this.","[5, 6, 6]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[5, 4, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer starts with positive remarks, praising the paper's clarity and topic relevance. However, they express uncertainty about the novelty of the approach, questioning if the improvements stem simply from increased model capacity or if there's a more fundamental advantage to ELMO. The reviewer raises specific, insightful questions to address this uncertainty. While they acknowledge the expected performance boost from deeper models, they push for a deeper analysis of ELMO's unique contribution beyond this general trend. The tone remains professional and focused on scientific inquiry, suggesting areas for improvement rather than outright criticism.  Therefore, the sentiment leans slightly positive due to the initial praise, but the numerous questions and desire for further analysis indicate it's not overwhelmingly positive. The language used is polite and constructive throughout.",40.0,80.0
Stochastic Variational Video Prediction,"['Mohammad Babaeizadeh', 'Chelsea Finn', 'Dumitru Erhan', 'Roy H. Campbell', 'Sergey Levine']",Accept,2018,"[7, 41, 10]","[11, 45, 15]","[27, 376, 123]","[19, 247, 57]","[7, 16, 51]","[1, 113, 15]","The submission presents a method or video prediction from single (or multiple) frames, which is capable of producing stochastic predictions by means of training a variational encoder-decoder model. Stochastic video prediction is a (still) somewhat under-researched direction, due to its inherent difficulty.

The method can take on several variants: time-invariant [latent variable] vs. time-variant, or action-conditioned vs unconditioned. The generative part of the method is mostly borrowed from Finn et al. (2016). Figure 1 clearly motivates the problem. The method itself is fairly clearly described in Section 3; in particular, it is clear why conditioning on all frames during training is helpful. As a small remark, however, it remains unclear what the action vector a_t is comprised of, also in the experiments.

The experimental results are good-looking, especially when looking at the provided web site images. 
The main goal of the quantitative comparison results (Section 5.2) is to determine whether the true future is among the generated futures. While this is important, a question that remains un-discussed is whether all generated stochastic samples are from realistic futures. The employed metrics (best PSNR/SSIM among multiple samples) can only capture the former, and are also pixel-based, not perceptual.

The quantitative comparisons are mostly convincing, but Figure 6 needs some further clarification. It is mentioned in the text that ""time-varying latent sampling is more stable beyond the time horizon used during training"". While true for Figure 6b), this statement is contradicted by both Figure 6a) and 6c), and Figure 6d) seems to be missing the time-invariant version completely (or it overlaps exactly, which would also need explanation). As such, I'm not completely clear on whether the time variant or invariant version is the stronger performer.

The qualitative comparisons (Section 5.3) are difficult to assess in the printed material, or even on-screen. The animated images on the web site provide a much better impression of the true capabilities, and I find them convincing.

The experiments only compare to Reed et al. (2017)/Kalchbrenner et al. (2017), with Finn at al. (2016) as a non-stochastic baseline, but no comparisons to, e.g., Vondrick et al. (2016) are given. Stochastic prediction with generative adversarial networks is a bit dismissed in Section 2 with a mention of the mode-collapse problem.

Overall the submission makes a significant enough contribution by demonstrating a (mostly) working stochastic prediction method on real data.","[7, 7, 7]","[' Good paper, accept', ' Good paper, accept', ' Good paper, accept']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review is mostly positive. The reviewer acknowledges the significance of the work and finds the results promising, especially the qualitative ones. However, they also point out some areas that need clarification and further investigation, such as the quantitative metrics used and the comparison with other methods. The language is polite and professional throughout.",60.0,80.0
An efficient framework for learning sentence representations,"['Lajanugen Logeswaran', 'Honglak Lee']",Accept,2018,"[5, 14]","[10, 19]","[22, 273]","[8, 145]","[14, 123]","[0, 5]","[REVISION]

Thank you for your clarification. I appreciate the effort and think it has improved the paper. I have updated my score accordingly

====== 

This paper proposes a new objective for learning SkipThought-style sentence representations from corpora of ordered sentences. The algorithm is much faster than SkipThoughts as it swaps the word-level decoder for a contrastive classification loss. 

Comments:

Since one of the key advantages of this method is the speed, I was surprised there was not a more formal comparison of the speed of training different models. For instance, it would be more convincing if two otherwise identical encoders were trained on the same machine on the books corpus with the proposed objective and the skipthoughts decoding objective, and the representations compared after X hours of training. The reported 2 weeks required to train Skipthoughts comes from the paper, but things might be faster now with more up-to-date deep learning libraries etc. If this was what was in fact done, then it's probably just a case of presenting the comparison in a more formal way. I would also lose the sentence ""we are able to train many models in the time it takes to train most unsupervised"" (see next point for reasons why this is questionable).

It would have been interesting to apply this method with BOW encoders, which should be even faster than RNN-based encoders reported in this paper. The faster BOW models tend to give better performance on cosine-similarity evaluations ( quantifying the nearest-neighbour analysis that the authors use in this paper). Indeed, it would be interesting (although of course not definitive) to see comparison of the proposed algorithm (with BOW and RNN encoders) on cosine sentence similarity evaluations. 

The proposed novelty is simple and intuitive, which I think is a strength of the method. However, a simple idea makes overlap with other proposed approaches more likely, and I'd like the author to check through the public comments to ensure that all previous related ideas are noted in this paper. 

I think the authors could do more to emphasise what the point is of trying to learn sentence embeddings. An idea of the eventual applications of these embeddings would make it easier to determine, for instance, whether the supervised ensembling method applied here would be applicable in practice. Moreover, many papers have emphasised the limitations of the evaluations used in this paper (although they are still commonly used) so it would be good to acknowledge that it's hard to draw too many conclusions from such numbers. That said, the numbers are comparable Skipthoughts, so it's clear that this method learns representations of comparable quality. 

The justification for the proposed algorithm is clear in terms of efficiency, but I don't think it's immediately clear from a semantic / linguistic point of view. The statement ""The meaning of a sentence is the property that creates bonds...."" seems to have been cooked up to justify the algorithm, not vice versa. I would cut all of that speculation out and focus on empirically verifiable advantages. 

The section of image embeddings comes completely out of the blue and is very hard to interpret. I'm still not sure I understand this evaluation (short of looking up the Kiros et al. paper), or how the proposed model is applied to a multi-modal task.

There is much scope to add more structured analysis of the type hinted by the nearest neighbours section. Cherry picked lists don't tell the reader much, but statistics or more general linguistic trends can be found in these neighbours and aggregated, that could be very interesting. 

","[6, 8, 6]","[' Marginally above acceptance threshold', ' Top 50% of accepted papers, clear accept', ' Marginally above acceptance threshold']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is generally positive, acknowledging the paper's improvements and the strength of its simple, intuitive approach. The reviewer finds the proposed method promising, as evidenced by phrases like ""this is a strength of the method"" and ""it's clear that this method learns representations of comparable quality."" However, the review also includes constructive criticism and suggestions for improvement, indicating that the paper isn't perfect. The reviewer maintains a professional and respectful tone throughout, focusing on the paper's content and offering concrete advice.",60.0,80.0
On the importance of single directions for generalization,"['Ari S. Morcos', 'David G.T. Barrett', 'Neil C. Rabinowitz', 'Matthew Botvinick']",Accept,2018,"[1, 7, 4, 18]","[6, 12, 8, 23]","[66, 32, 29, 110]","[22, 15, 11, 40]","[43, 17, 17, 54]","[1, 0, 1, 16]","This is an ""analyze why"" style of paper:  the authors attempt to explain the relationship between some network property (in this case, ""reliance on single directions""), and a desired performance metric (in this case, generalization ability).   The authors quantify a variety of related ways to measure ""reliance on single directions"" and show that the more reliant on a single directions a given network is, the less well it generalizes.   

Clarity:  The paper is fairly clearly written.  Sometimes key details are in the footnotes (e.g. see footnote 3) -- not sure why -- but on the  whole, I think the followed the paper reasonably well.  

Quality: The work makes a good-faith attempt to be fairly systematic -- e.g evaluating several different types of network structures, with reasonable numbers of random initializations, and also illustrates the main point in several different comparatively independent-seeming ways.  I feel fairly confident that the results are basically right within the somewhat limited domain that the authors explore. 

Originality: This work is one in a series of papers about the topic of trying to understand what leads to good generalization in deep neural networks. I don't know that the concept of ""reliance on a single direction"" seems especially novel to me, but on the other hand, I can't think of another paper that precisely investigates this notion the way it is done here.   

Significance: The work touches on some important issues.  I think the demonstration that the existence of strongly class-selective neurons is not a good correlate for generalization is interesting.   This point illustrates something that has made me a bit uncomfortable with the trend toward ""interpretable machine learning"" that has been arising recently:  in many of those results, it is shown that some fraction of the units at various levels of a trained deepnet have optimal driving stimuli that seem somewhat interpretable, with the implication that the existence of such units is an important correlate of network performance.  There has even been some claims that better-performing networks have more ""single-direction"" interpretable units [1].  The fact that the current results seem directly in contradiction to that line of work is interesting, and the connections to batch normalization and dropout are for the same reason interesting.  However, I wish the authors had grappled more directly with the apparent contradiction with (e.g.) [1].   There is probably a kind of tradeoff here.   The closer the training dataset is to what is being tested for ""generalization"", the more likely that having single-direction units is useful; and vice-versa.   I guess the big question is: what types of generalization are actually demanded / desired in real deployed machine learning systems (or in the brain)?  How does those cases compare with the toy examples analyzed here?   The paper doesn't go far enough in really addressing these questions, but it is sort of beginning to make an effort. 

However, for me the main failing of the paper is that it's fairly descriptive without being that prescriptive. Does using their metric of reliance on a single direction, as a regularizer in and of itself, add anything above any beyond existing regularizers (e.g. batch normalization or dropout)?  It doesn't seem like they tried. This seems to me the key question to understanding the significance of their results.   Is ""reliance on single direction"" actually a good regularizer as such, especially for ""real"" problems like (e.g.) training a deep Convnet on (e.g.) ImageNet or some other challenging dataset?  Would penalizing for this quantity improve the generalization of a network trained on ImageNet to other visual datasets (e.g. MS-COCO)?  If so, this would be a very significant result and would make me really care about their idea of ""reliance on a singe direction"".  If such results do not hold, it seems to me like one more theoretical possibility that would bite the dust when tested at scale.  

[1] http://netdissect.csail.mit.edu/final-network-dissection.pdf","[5, 7, 9]","[' Marginally below acceptance threshold', ' Good paper, accept', ' Top 15% of accepted papers, strong accept']","[4, 3, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer provides a balanced perspective, highlighting both the strengths and weaknesses of the paper. They acknowledge the clarity, quality, and originality of the work, indicating a positive sentiment. However, they also express concerns about the paper's lack of prescriptive insights and its limited scope, which brings the overall sentiment towards neutral. The language used is professional and respectful throughout the review.",50.0,100.0
Combining Symbolic Expressions and Black-box Function Evaluations in Neural Programs,"['Forough Arabshahi', 'Sameer Singh', 'Animashree Anandkumar']",Accept,2018,"[6, 13, 13]","[11, 18, 18]","[18, 198, 419]","[8, 110, 154]","[10, 81, 223]","[0, 7, 42]","SUMMARY 

The model evaluates symbolic algebraic/trigonometric equalities for validity, with an output unit for validity level at the root of a tree of LSTM nodes feeding up to the root; the structure of the tree matches the parse tree of the input equation and the type of LSTM cell at each node matches the symbol at that node in the equation: there is a different cell type for each symbol. It is these cell types that are learned. The training data includes labeled true and false algebraic/trigonometric identities (stated over symbols for variables) as well as function-evaluation equalities such as ""tan(0.28) = 0.29"" and decimal-expansion equations like ""0.29 = 2*10^(-1) + 9*10^(-2)"".  I believe continuous values like ""0.29"" in the preceding expressions are encoded as the literal value of a single unit (feeding into an embedding unit of type W_{num}), whereas the symbols proper (including digit numerals) are encoded as 1-hot vectors (feeding into an embedding unit of type W_{symb}).
Performance is at least 97% when testing on unseen expressions of the same depth (up to 4) as the training data. Performance when trained on 3 levels (among 1 - 4) and testing on generalization to the held-out level is at least 96% when level 2 is held out, at least 92% when level 4 is withheld. Performance degrades (even on symbolic identities) when the function-evaluation equalities are omitted, and degrades when LSTM cells are replaced by plain RNN cells. The largest degradation is when the tree structure is replaced (presumably) by a sequence structure.
Performance was also tested on a fill-in-the-blank test, where a symbol from a correct equation was removed and all possible replacements for that symbol with expressions of depth up to 2 were tested, then ranked by the resulting validity score from the model. From the graph it looks like an accuracy of about 95% was achieved for the 1-best substituted expression (accuracy was about 32% for a sequential LSTM).

WEAKNESSES

* The title is misleading; ""blackbox function evaluation"" does not suggest what is intended, which is training on function-evaluation equations. The actual work is more interesting than what the title suggests.
* The biggest performance boost (roughly 15%) arises from use of the tree structure, which is given by an oracle (implemented in a symbolic expression parser, presumably): the network does not design its own example-dependent structure.
* What does the sympy baseline mean in Table 2? We are only told that sympy is a ""symbolic solver"". Yet the sympy performance scores are in the 70-80% range. If the solver’s performance is that weak, why is it used during generation of training data to determine the validity of possible equations?
* Given that this is a conference on ""learning representations"" it would have been nice to see at least a *little* examination of the learned representations. It would be easy to do some interesting tests. How well does the vector embedding for ""2*10^(-1) + 9*10^(-2)"" match the vector for the real value 0.29? W_{num} embeds a continuum of real values in R^d: what is this 1-dimensional embedding manifold like? How do the embeddings of different integers provided by W_{sym} relate to one another? My rating would have been higher had there been some analysis of the learned representations.
* We are told only that the ""hidden dimension … varies""; it would be nice if the text or results tables gave at least some idea of what magnitude of embedding dimension we’re talking about.

STRENGTHS

The weaknesses above notwithstanding, this is a very interesting piece of work with impressive results. 
* The number of functions learned, 28, is a quantum jump from previous studies using 8 or fewer functions.
* It is good to see the power of training the same system to learn the semantics of functions from the equations they satisfy AND from the values they produce. 
* The inclusion of decimal-expansion equations for relating numeral embeddings to number embeddings is clever. 
* The general method used for randomly generating a non-negligible proportion of true equations is useful.
* The evaluation of the model is thorough and clear.
* In fact the exposition in the paper as a whole is very clear.","[8, 5, 6]","[' Top 50% of accepted papers, clear accept', ' Marginally below acceptance threshold', ' Marginally above acceptance threshold']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer provides both positive and negative points, but the overall tone suggests they find the work interesting and promising. The presence of constructive criticism and suggestions for improvement indicate a positive attitude towards the paper's potential. The reviewer explicitly mentions the work as ""very interesting"" and praises the clarity and thoroughness of the evaluation. Therefore, the sentiment leans towards the positive side. The language used is formal, respectful, and constructive, without resorting to harsh or derogatory terms. The reviewer maintains a professional and polite tone throughout the review.",60.0,80.0
Deep Complex Networks,"['Chiheb Trabelsi', 'Olexa Bilaniuk', 'Ying Zhang', 'Dmitriy Serdyuk', 'Sandeep Subramanian', 'Joao Felipe Santos', 'Soroush Mehri', 'Negar Rostamzadeh', 'Yoshua Bengio', 'Christopher J Pal']",Accept,2018,"[-2, 4, 6, 20]","[3, 9, 11, 25]","[4, 40, 16, 232]","[1, 17, 6, 130]","[3, 21, 8, 80]","[0, 2, 2, 22]","The paper presents an extensive framework for complex-valued neural networks. Related literature suggests a variety of motivations for complex valued neural networks: biological evidence, richer representation capacity, easier optimization, faster learning, noise-robust memory retrieval mechanisms and more. 

The contribution of the current work does not lie in presenting significantly superior results, compared to the traditional real-valued neural networks, but rather in developing an extensive framework for applying and conducting research with complex-valued neural networks. Indeed, the most standard work nowadays with real-valued neural networks depends on a variety of already well-established techniques for weight initialization, regularization, activation function, convolutions, etc. In this work, the complex equivalent of many of these basics tools are developed, such as a number of complex activation functions, complex batch normalization, complex convolution, discussion of complex differentiability, strategies for complex weight initialization, complex equivalent of a residual neural network. 

Empirical results show that the new complex-flavored neural networks achieve generally comparable performance to their real-valued counterparts, on a variety of different tasks. Then again, the major contribution of this work is not advancing the state-of-the-art on many benchmark tasks, but constructing a solid framework that will enable stable and solid application and research of these well-motivated models. 
","[8, 4, 7]","[' Top 50% of accepted papers, clear accept', ' Ok but not good enough - rejection', ' Good paper, accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review acknowledges the paper's contribution as providing a framework rather than groundbreaking results. It highlights the importance of this framework for future research. The language is positive, focusing on the paper's strengths and potential impact. There's no negative phrasing or condescending tone.",75.0,85.0
Compressing Word Embeddings via Deep Compositional Code Learning,"['Raphael Shu', 'Hideki Nakayama']",Accept,2018,"[3, 16]","[8, 21]","[26, 133]","[10, 82]","[15, 40]","[1, 11]","The authors proposed to compress word embeddings by approximate matrix factorization, and to solve the problem with the Gumbel-soft trick. The proposed method achieved compression rate 98% in a sentiment analysis task, and compression rate over 94% in machine translation tasks, without a performance loss. 

This paper is well-written and easy to follow.  The motivation is clear and the idea is simple and effective.

It would be better to provide deeper analysis in Subsection 6.1. The current analysis is too simple. It may be interesting to explain the meanings of individual components. Does each component is related to a certain topic? Is it meaningful to perform ADD or SUBSTRACT on the leaned code? 

It may also be interesting to provide suitable theoretical analysis, e.g., relationships with the SVD of the embedding matrix.
","[7, 8, 6]","[' Good paper, accept', ' Top 50% of accepted papers, clear accept', ' Marginally above acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with positive statements, highlighting the paper's clarity, motivation, simplicity, and effectiveness. While it suggests improvements and further analysis, these are presented as constructive suggestions rather than harsh criticisms. The language used is formal and respectful throughout.",75.0,80.0
Viterbi-based Pruning for Sparse Matrix with Fixed and High Index Compression Ratio,"['Dongsoo Lee', 'Daehyun Ahn', 'Taesu Kim', 'Pierce I. Chuang', 'Jae-Joon Kim']",Accept,2018,"[8, 1, 16, 10, 16]","[13, 6, 21, 15, 21]","[47, 12, 51, 39, 82]","[25, 10, 27, 23, 49]","[17, 0, 12, 10, 8]","[5, 2, 12, 6, 25]","It seems like the authors have carefully thought about this problem, and have come up with some elegant solutions, but I am not sold on whether it's an appropriate match for this conference, mainly because it's not clear how many machine learning people will be interested in this approach.

There was a time about 2 or 3 years ago when sparse-matrix approaches seemed to have a lot of promise, but I get the impression that a lot of people have moved on.  The issue is that it's hard to construct a scenario where it makes sense from a speed or memory standpoint to do this.  The authors seem to have found a way to substantially compress the indexes, but it's not clear to me that this really ends up solving any practical problem.  Towards the end of the paper I see mention of a 38.1% reduction in matrix size.  That is way too little to make sense in any practical application, especially when you consider the overhead of decompression.   It seems to me that you could easily get a factor of 4 to 8 of compression just by finding a suitable way to encode the floating-point numbers in many fewer bits (since the weight parameters are quite Gaussian-distributed and don't need to be that accurate).
","[6, 6, 7]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Good paper, accept']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer acknowledges the authors' effort and finds the solutions elegant, indicating a somewhat positive sentiment. However, they express significant doubts about the work's relevance and practicality, pulling the sentiment towards the negative side. The language used is critical but professional, avoiding personal attacks or disrespectful tone.",-20.0,60.0
Interpretable Counting for Visual Question Answering,"['Alexander Trott', 'Caiming Xiong', 'Richard Socher']",Accept,2018,"[2, 10, 12]","[7, 15, 17]","[19, 383, 229]","[6, 165, 111]","[12, 208, 111]","[1, 10, 7]","This paper proposed a new approach for counting in VQA called Interpretable Counting in Visual Question Answering.  The authors create a new dataset (HowMany-QA) by processing the VQA 2.0 and Visual Genome dataset. In the paper, the authors use object detection framework (R-FCN) to extract bounding boxes information as well as visual features and propose three different strategies for counting. 1: SoftCount; 2: UpDown; 3: IRLC.  The authors show results on HowMany-QA dataset for the proposed methods, and the proposed IRLC method achieves the best performance among all the baselines.   

[Strenghts]

This paper first introduced a cleaned visual counting dataset by processing existing VQA 2.0 and Visual Genome dataset, which can filter out partial non-counting questions. The proposed split is a good testbed for counting in VQA. 

The authors proposed 3 different methods for counting, which both use object detection feature trained on visual genome dataset.  The object detector is trained with multiple objectives including object detection, relation detection, attribute classification and caption grounding to produce rich object representation. The author first proposed 2 baselines: SoftCount uses a Huber loss, UpDown uses a cross entropy loss. And further proposed interpretable RL counter which enumerates the object as a sequential decision process. The proposed IRLC more intuitive and outperform the previous VQA method  (UpDown) on both accuracy and RMSE. 

[Weaknesses]

This paper proposed an interesting and intuitive counting model for VQA. However, there are several weaknesses existed:

1: The object detector is pre-trained with multiple objectives. However, there is no ablation study to show the differences. Since the model only uses the object and relationship feature as input, the authors could show results on counting with different  objects detector. For example, object detector trained using object + relation v.s. object + relation + attribute v.s. object + relation + attribute + caption. 

2: Figure 9 shows an impressive result of the proposed method. Given the detection result, there are a lot of repetitive candidates detection bounding boxes. Without any strong supervision, IRLC could select the correct bounding boxes associated with the different objects. This is interesting, however, the authors didn't show any quantitative results on this. One experiment could verify the performance on IRLC is to compute the IOU between the GT COCO bounding box annotation on a small validation set. The validation set could be obtained by comparing the number of the bounding box and VQA answer with respect to similar COCO categories and VQA entities. 

3: The proposed IRLC is not significantly outperform baseline method (SoftCount) with respect to RMSE (0.1). However, it would be interesting to see how the counting performance can change the result of object detection. As Chattopadhyay's CVPR2017 paper Sec 5.3 on the same subset as in point 2. 

[Summary]

This paper proposed an interesting and interpretable model for counting in VQA. It formulated the counting as a sequential decision process that enumerated the subset of target objects. The authors introduce several new techniques in the IRLC counter. However, there is a lack of ablation study on the proposed model. Taking all these into account, I suggest accepting this paper if the authors could provide more ablation study on the proposed methods. 
","[7, 6, 7]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Good paper, accept']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer acknowledges the strengths of the paper, such as the introduction of a new dataset and an intuitive counting model. They find the results interesting, especially the performance of IRLC. However, they also point out weaknesses like the lack of ablation studies and quantitative results for certain aspects. Despite the weaknesses, the reviewer recommends accepting the paper if the authors address the lack of ablation study, indicating a generally positive sentiment. The language used is constructive, professional, and polite throughout the review.",60.0,80.0
Semantically Decomposing the Latent Spaces of Generative Adversarial Networks,"['Chris Donahue', 'Zachary C. Lipton', 'Akshay Balsubramani', 'Julian McAuley']",Accept,2018,"[22, 5, 6, 13]","[27, 10, 10, 18]","[38, 203, 30, 294]","[19, 77, 11, 150]","[19, 117, 18, 133]","[0, 9, 1, 11]","Summary:

This paper investigated the problem of controlled image generation. Assuming images can be disentangled by identity-related factors and style factors, this paper proposed an algorithm that produces a pair of images with the same identity. Compared to standard GAN framework, this algorithm first generated two latent variables for the pair images. The two latent variables are partially shared reflecting the shared identity information. The generator then transformed the latent variables into high-resolution images with a deconvolution decoder networks. The discriminator was used to distinguish paired images from database or paired images sampled by the algorithm. Experiments were conducted using DCGAN and BEGAN on portrait images and shoe product images. Qualitative results demonstrated that the learned style representations capture viewpoint, illumination and background color while the identity was well preserved by the identity-related representations.


== Novelty & Significance ==
Paired image generation is an interesting topic but this has been explored to some extent. Compared to existing coupled generation pipeline such as CoGAN, I can see the proposed formulation is more application-driven.

== Technical Quality ==
In Figure 3, the portrait images in the second row and fourth row look quite similar. I wonder if the trained model works with only limited variability (in terms of identity).
In Figure 4, the viewpoint is quite limited (only 4 viewpoints are provided).

I am not very convinced whether SD-GAN is a generic algorithm for controlled image generation. Based on the current results, I suspect it only works in fairly constrained settings. 
It would be good to know if it actually works in more challenging datasets such as SUN bedroom, CUB and Oxford Flowers. 

“the AC-DCGAN model cannot imagine new identities”
I feel the author of this paper made an unfair argument when comparing AC-DCGAN with the proposed method. First, during training, the proposed SD-GAN needs to access the identity information and there is only limited identity in the dataset. Based on the presentation, it is not very clear how does the model generate novel identities (in contrast to simply interpolating existing identities). For example, is it possible to generate novel viewpoints in Figure 4?

Missing references on conditional image generation and coupled image generation:
-- Generative Adversarial Text-to-Image Synthesis. Reed et al., In ICML 2016.
-- Attribute2Image: Conditional Image Generation from Visual Attributes. Yan et al., In ECCV 2016.
-- Domain Separation Networks. Bousmalis et al., In NIPS 2016.
-- Unsupervised Image-to-Image Translation Networks. Liu et al., In NIPS 2017.

Overall, I rate this paper slightly above borderline. It showed some good visualization results on controlled image generation. But the comparison to AC-GAN is not very fair, since the identity pairs are fully supervised for the proposed method. As far as I can see, there are no clear-cut improvements quantitatively. Also, there is no comparison with CoGAN, which I believe is the most relevant work for coupled image generation. 
","[6, 7, 6]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Marginally above acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer acknowledges some positive aspects of the paper, such as the interesting topic and good visualization results. However, they also express several concerns, including limited variability in the results, lack of clarity on generating novel identities, unfair comparison with AC-GAN, and the absence of quantitative improvements and comparison with CoGAN. The overall tone suggests the reviewer finds the paper slightly above borderline, indicating a somewhat negative sentiment.",-10.0,60.0
Adaptive Quantization of Neural Networks,"['Soroosh Khoram', 'Jing Li']",Accept,2018,"[5, 1, 1, 4, 1, 33, 17]","[9, 6, 6, 7, 5, 38, 22]","[17, 8, 17, 20, 8, 54, 105]","[9, 6, 5, 11, 5, 33, 60]","[7, 2, 12, 5, 2, 5, 20]","[1, 0, 0, 4, 1, 16, 25]","The authors present an interesting idea to reduce the size of neural networks via adaptive compression, allowing the network to use high precision where it is crucial and low precision in other parts. The problem and the proposed solution is well motivated. However, there are some elements of the manuscript that are hard to follow and need further clarification/information. These need to definitely be addressed before this paper can be accepted.

Specific comments/questions:
- Page 1: Towards the bottom, in the 3rd to last line, reference is missing.
- Page 1: It is a little hard to follow the motivation against existing methods.
- Page 2: DenseNets and DeepCompression need citations
- Lemma 2.1 seems interesting - is this original work? This needs to be clarified.
- Lemma 2.2: Reference to Equation 17 (which has not been presented in the manuscript at this point) seems a little confusing and I am unable to following the reasoning and the subsequent proof which again refers to Equation 17.
- Alg 2: Should it be $\Delta$ or $\Delta_{k+1}$? Because in one if branch, we use $\Delta$, in the other, we use the subscripted one.
- Derivation in section 2.3 has some typographical errors.
- What is $d$ in Equation 20 (with cases)? Without this information, it is unclear how the singular points are handled.
- Page 6, first paragraph of Section 3: The evaluation is a little confusing - when is the compression being applied during the training process, and how is the training continued post-compression? What does each compression 'pass' constitute of?
- Figure 1b: what is the 'iteration' on the horizontal axis, is it the number of iterations of Alg3 or Alg2? Hoping it is Alg3 but needs to be clarified in the text.
- Section 3: What about compression results for CIFAR and SVNH? ","[6, 6, 6]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with positive remarks, highlighting the interesting idea and well-motivated problem. However, it then lists several points requiring clarification, indicating a lack of clarity. While the reviewer uses phrases like ""it is a little hard to follow,"" suggesting a collaborative approach, the number of issues raised points towards a more critical stance. Overall, the sentiment leans slightly towards the negative due to the need for significant revisions. The language used is polite and professional throughout, employing constructive criticism and avoiding harsh language.",-20.0,70.0
Rethinking the Smaller-Norm-Less-Informative Assumption in Channel Pruning of Convolution Layers,"['Jianbo Ye', 'Xin Lu', 'Zhe Lin', 'James Z. Wang']",Accept,2018,"[10, 7, 14, 22]","[13, 9, 19, 27]","[40, 38, 255, 169]","[17, 22, 141, 79]","[14, 13, 86, 26]","[9, 3, 28, 64]","This paper is well written and it was easy to follow. The authors propose prunning model technique by enforcing sparsity on the scaling parameter of batch normalization layers. This is achieved by forcing the output of some channels being constant during training. This is achieved an adaptation of ISTA algorithm to update the batch-norm parameter. 

The authors evaluate the performance of the proposed approach on different classification and segmentation tasks. The method seems to be relatively straightforward to train and achieve good performance (in terms of performance/parameter reduction) compared to other methods on Imagenet.

Some of the hyperparameters used (alpha and specially rho) seem to be used very ad-hoc. Could the authors explain their choices? How sensible is the algorithm to these hyperparameters?
It would be nice to see empirically how much of computation the proposed approach takes during training. How much longer does it takes to train the model with the ISTA based constraint?

Overall this is a good paper and I believe it should be accepted, given the authors are more clear on the details pointed above.
","[7, 5, 6]","[' Good paper, accept', ' Marginally below acceptance threshold', ' Marginally above acceptance threshold']","[3, 5, 5]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review starts with positive statements about the paper's clarity and the proposed method's novelty and effectiveness. The reviewer finds merit in the research and acknowledges its contribution. While the reviewer poses some questions and requests clarifications, these are presented constructively and aim to improve the paper. The concluding sentence suggests acceptance, contingent on addressing the raised points, indicating an overall positive disposition towards the paper.",75.0,80.0
Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play,"['Sainbayar Sukhbaatar', 'Zeming Lin', 'Ilya Kostrikov', 'Gabriel Synnaeve', 'Arthur Szlam', 'Rob Fergus']",Accept,2018,"[8, 3, 5, 9, 10, 16]","[13, 7, 10, 14, 15, 21]","[43, 22, 40, 132, 138, 131]","[17, 11, 17, 59, 54, 72]","[26, 9, 23, 67, 74, 49]","[0, 2, 0, 6, 10, 10]","This paper proposes an interesting model of self-play where one agent learns to propose tasks that are easy for her but difficult for an opponent. This creates a moving target of self-play objectives and learning curriculum.

The idea is certainly elegant and clearly described. 
I don't really feel qualified to comment on the novelty, since this paper is somewhat out of my area of expertise, but I did notice that the authors' own description of Baranes and Oudeyer (2013) seems very close to the proposal in this paper. Given the existence of similar forms of self-play the key issue with paper I see is that there is no strong self-play baseline in the experimental evaluation. It is hard to tell whether this neat idea is really an improvement.

Is progress guaranteed? Is it not possible for Alice to imemdiately find an easy task for her where Bob times out, gets no reward signal, and therefore is unable to learn anything? Then repeating that task will loop forever without progress. This suggests that the adversarial setting is quite brittle.

I also find that the paper is a little light on the technical side.","[5, 8, 8]","[' Marginally below acceptance threshold', ' Top 50% of accepted papers, clear accept', ' Top 50% of accepted papers, clear accept']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer finds the idea interesting and elegant, which points towards a positive sentiment. However, they also express concerns about the novelty and lack of strong baseline comparisons, indicating some reservations. Overall, the tone is polite and professional, but the criticism, while constructive, does bring the politeness down a bit from extremely polite. Thus, the sentiment is somewhat positive but with reservations, and the politeness is professionally polite but not overly so.",50.0,50.0
Large scale distributed neural network training through online distillation,"['Rohan Anil', 'Gabriel Pereyra', 'Alexandre Passos', 'Robert Ormandi', 'George E. Dahl', 'Geoffrey E. Hinton']",Accept,2018,"[3, 4, 11, 12, 9, 43]","[8, 8, 16, 15, 13, 48]","[34, 7, 23, 23, 47, 282]","[11, 3, 10, 17, 17, 160]","[22, 4, 12, 3, 25, 52]","[1, 0, 1, 3, 5, 70]","This paper provides a very original & promising method to scale distributed training beyond the current limits of mini-batch stochastic gradient descent. As authors point out, scaling distributed stochastic gradient descent to more workers typically requires larger batch sizes in order to fully utilize computational resource, and increasing the batch size has a diminishing return. This is clearly a very important problem, as it is a major blocker for current machine learning models to scale beyond the size of models and datasets we currently use. Authors propose to use distillation as a mechanism of communication between workers, which is attractive because prediction scores are more compact than model parameters, model-agnostic, and can be considered to be more robust to out-of-sync differences. This is a simple and sensible idea, and empirical experiments convincingly demonstrate the advantage of the method in large scale distributed training.

I would encourage authors to experiment in broader settings, in order to demonstrate that the general applicability of the proposed method, and also to help readers better understand its limitations. Authors only provide a single positive data point; that co-distillation was useful in scaling up from 128 GPUs to 258 GPUs, for the particular language modeling problem (commoncrawl) which others have not previously studied. In order for other researchers who work on different problems and different system infrastructure to judge whether this method will be useful for them, however, they need to understand better when codistillation succeeds and when it fails. It will be more useful to provide experiments with smaller and (if possible) larger number of GPUs (16, 32, 64, and 512?, 1024?), so that we can more clearly understand how useful this method is under the regime mini-batch stochastic gradient continues to scale. Also, more diversity of models would also help understanding robustness of this method to the model. Why not consider ImageNet? Goyal et al reports that it took an hour for them to train ResNet on ImageNet with 256 GPUs, and authors may demonstrate it can be trained faster.

Furthermore, authors briefly mention that staleness of parameters up to tens of thousands of updates did not have any adverse effect, but it would good to know how the learning curve behaves as a function of this delay. Knowing how much delay we can tolerate will motivate us to design different methods of communication between teacher and student models.","[8, 4, 6]","[' Top 50% of accepted papers, clear accept', ' Ok but not good enough - rejection', ' Marginally above acceptance threshold']","[4, 3, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer clearly sees the value in the paper, using language like ""very original & promising"", ""clearly a very important problem"", ""simple and sensible idea"", and ""convincingly demonstrate"". This suggests a positive sentiment. However, the reviewer also provides constructive criticism and suggestions for improvement, indicating that the paper is not perfect. Therefore, the sentiment is likely to be positive but not extremely high. The language used is polite and professional throughout, with constructive suggestions and no harsh criticism.",75.0,100.0
Universal Agent for Disentangling Environments and Tasks,"['Jiayuan Mao', 'Honghua Dong', 'Joseph J. Lim']",Accept,2018,"[2, 1, 12]","[7, 3, 17]","[58, 5, 77]","[27, 3, 45]","[29, 2, 29]","[2, 0, 3]","In this paper a modular architecture is proposed with the aim of separating environment specific (dynamics) knowledge and task-specific knowledge into different modules. Several complex but discrete control tasks, with relatively small action spaces, are cast as continuous control problems, and the task specific module is trained to produce non-linear representations of goals in the domain of transformed high-dimensional inputs.

Pros
- “Monolithic” policy representations can make it difficult to reuse or jointly represent policies for related tasks in the same environment; a modular architecture is hence desirable.
- An extensive study of methods for dimensionality reduction is performed for a task with sparse rewards.
- Despite all the suggestions and questions below, the method is clearly on par with standard A3C across a wide range of tasks, which makes it an attractive architecture to explore further.

Cons
- In general, learning a Path function could very well turn out to be no simpler than learning a good policy for the task at hand. I have 2 main concerns:
The data required for learning a good Path function may include similar states to those visited by some optimal policy. However, there is no such guarantee for random walks; indeed, for most Atari games which have several levels, random policies don’t reach beyond the first level, so I don’t see how a Path function would be informative beyond the ‘portions’ of the state space which were visited by policies used to collect data.
Hence, several policies which are better than random are likely to be required for sampling this data, in general. In my mind this creates a chicken-and-egg issue: how to get the data, to learn the right Path function which does not make it impossible to still reach optimal performance on the task at hand? How can we ensure that some optimal policy can still be represented using appropriate Goal function outputs? I don’t see this as a given in the current formulation.
- Although the method is atypical compared to standard HRL approaches, the same pitfalls may apply, especially that of ‘option collapse’: given a fixed Path function, the Goal function need only figure out which goal state outputs almost always lead to the same output action in the original action space, irrespective of the current state input phi(s), and hence bypass the Path function altogether; then, the role of phi(s) could be taken by tau(s), and we would end up with the original RL problem but in an arguably noisier (and continuous) action space. I recommend comparing the Jacobian w.r.t the phi(s) and tau(s) inputs to the Path function using saliency maps [1, 2]; alternatively, evaluating final policies with out of date input states s to phi, and the correct tau(s) inputs to Path function should degrade performance severely if it playing the role assumed. Same goes for using a running average of phi(s) and the correct tau(s) in final policies.
- The ability to use state restoration for Path function learning is actually introducing a strong extra assumption compared to standard A3C, which does not technically require it. For cheap emulators and fully deterministic games (Atari) this assumption holds, but in general restoring expensive, stochastic environments to some state is hard (e.g. robot arms playing ping-pong, ball at given x, y, z above the table, with given velocity vector).
- If reported results are single runs, please replace with averages over several runs, e.g. a few random seeds. Given the variance in deep RL training curves, it is hard to make definitive claims from single runs. If curves are already averages over several experiment repeats, some form of error bars or variance plot would also be informative.
- How much data was actually used to learn the Path function in each case? If the amount is significant compared to task-specific training, then UA/A3C-L curves should start later than standard A3C curves, by that amount of data.


References
[1] Simonyan, K., Vedaldi, A., and Zisserman, A. Deep inside
convolutional networks: Visualising image classification
models and saliency maps. arXiv preprint arXiv:1312.6034, 2013.
[2] Z Wang, T Schaul, M Hessel, H Van Hasselt, M Lanctot, N De Freitas, Dueling network architectures for deep reinforcement learning arXiv preprint arXiv:1511.06581
","[7, 6, 6]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[4, 3, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review presents a generally positive sentiment by highlighting the merits of the proposed modular architecture and acknowledging its competitive performance against a standard method (A3C). The reviewer sees the value in the approach ('attractive architecture to explore further'). However, they also raise several significant concerns and questions, indicating potential flaws and areas needing clarification. The language used is polite and professional, employing a constructive tone typical of academic discourse. While the reviewer points out potential issues, they do so in a manner that aims to guide the authors towards improving their work.",50.0,80.0
FusionNet: Fusing via Fully-aware Attention with Application to Machine Comprehension,"['Hsin-Yuan Huang', 'Chenguang Zhu', 'Yelong Shen', 'Weizhu Chen']",Accept,2018,"[24, 11, 8, 12]","[29, 16, 13, 17]","[37, 47, 97, 168]","[8, 24, 44, 77]","[27, 23, 51, 89]","[2, 0, 2, 2]","The primary intellectual point the authors make is that previous networks for machine comprehension are not fully attentive. That is, they do not provide attention on all possible layers on abstraction such as the word-level and the phrase-level. The network proposed here, FusionHet, fixes problem. Importantly, the model achieves state-of-the-art performance of the SQuAD dataset.

The paper is very well-written and easy to follow. I found the architecture very intuitively laid out, even though this is not my area of expertise. Moreover, I found the figures very helpful -- the authors clearly took a lot of time into clearly depicting their work! What most impressed me, however, was the literature review. Perhaps this is facilitated by the SQuAD leaderboard, which makes it simple to list related work. Nevertheless, I am not used to seeing comparison to as many recent systems as are presented in Table 2. 

All in all, it is difficult not to highly recommend an architecture that achieves state-of-the-art results on such a popular dataset.","[8, 7, 7]","[' Top 50% of accepted papers, clear accept', ' Good paper, accept', ' Good paper, accept']","[3, 5, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer explicitly states the paper is ""very well-written"", the architecture ""intuitively laid out"", the figures ""very helpful"", the literature review ""impressive"", and that they ""highly recommend"" the paper. All of these are strong indicators of a positive sentiment. The reviewer does not use any harsh or negative language, indicating a high level of politeness.",90.0,90.0
Global Optimality Conditions for Deep Neural Networks,"['Chulhee Yun', 'Suvrit Sra', 'Ali Jadbabaie']",Accept,2018,"[6, 16, 19]","[11, 21, 24]","[33, 191, 366]","[18, 98, 187]","[15, 74, 114]","[0, 19, 65]","Summary:
The paper gives theoretical results regarding the existence of local minima in the objective function of deep neural networks. In particular:
- in the case of deep linear networks, they characterize whether a critical point is a global optimum or a saddle point by a simple criterion. This improves over recent work by Kawaguchi who showed that each critical point is either a global minimum or a saddle point (i.e., none is a local minimum), by relaxing some hypotheses and adding a simple criterion to know in which case we are.
- in the case of nonlinear network, they provide a sufficient condition for a solution to be a global optimum, using a function space approach.

Quality:
The quality is very good. The paper is technically correct and nontrivial. All proofs are provided and easy to follow.

Clarity:
The paper is very clear. Related work is clearly cited, and the novelty of the paper well explained. The technical proofs of the paper are in appendices, making the main text very smooth.

Originality:
The originality is weak. It extends a series of recent papers correctly cited. There is some originality in the proof which differs from recent related papers.

Significance:
The result is not completely surprising, but it is significant given the lack of theory and understanding of deep learning. Although the model is not really relevant for deep networks used in practice, the main result closes a question about characterization of critical points in simplified models if neural network, which is certainly interesting for many people.","[7, 5, 8]","[' Good paper, accept', ' Marginally below acceptance threshold', ' Top 50% of accepted papers, clear accept']","[4, 5, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review is largely positive. It highlights the paper's quality, clarity, and significance. While it acknowledges the originality is somewhat weak, it doesn't detract from the overall positive assessment. The language used is neutral, lacking strong positive or negative connotations.",75.0,0.0
Guide Actor-Critic for Continuous Control,"['Voot Tangkaratt', 'Abbas Abdolmaleki', 'Masashi Sugiyama']",Accept,2018,"[6, 8, 20]","[10, 13, 25]","[38, 69, 631]","[11, 33, 253]","[16, 30, 202]","[11, 6, 176]","The paper presents a clever trick for updating the actor in an actor-critic setting: computing a guide actor that diverges from the actor to improve critic value, then updating the actor parameters towards the guide actor. This can be done since, when the parametrized actor is Gaussian and the critic value can be well-approximated as quadratic in the action, the guide actor can be optimized in closed form.

The paper is mostly clear and well-presented, except for two issues: 1) there is virtually nothing novel presented in the first half of the paper (before Section 3.3); and 2) the actual learning step is only presented on page 6, making it hard to understand the motivation behind the guide actor until very late through the paper.

The presented method itself seems to be an important contribution, even if the results are not overwhelmingly positive. It'd be interesting to see a more elaborate analysis of why it works well in some domains but not in others. More trials are also needed to alleviate any suspicion of lucky trials.

There are some other issues with the presentation of the method, but these don't affect the merit of the method:

1. Returns are defined from an initial distribution that is stationary for the policy. While this makes sense in well-mixing domains, the experiment domains are not well-mixing for most policies during training, for example a fallen humanoid will not get up on its own, and must be reset.

2. The definition of beta(a|s) as a mixture of past actors is inconsistent with the sampling method, which seems to be a mixture of past trajectories.

3. In the first paragraph of Section 3.3: ""[...] the quality of a guide actor mostly depends on the accuracy of Taylor's approximation."" What else does it depend on? Then: ""[...] the action a_0 should be in a local vicinity of a.""; and ""[...] the action a_0 should be similar to actions sampled from pi_theta(a|s)."" What do you mean ""should""? In order for the Taylor approximation to be good?

4. The line before (19) is confusing, since (19) is exact and not an approximation. For the approximation (20), it isn't clear if this is a good approximation. Why/when is the 2nd term in (19) small?

5. The parametrization nu of \hat{Q} is never specified in Section 3.6. This is important in order to evaluate the complexities involved in computing its Hessian.
","[7, 6, 4]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Ok but not good enough - rejection']","[4, 2, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is mostly positive. The reviewer finds the core idea clever and the method an important contribution. While they point out limitations and areas for improvement, these are presented constructively and with specific suggestions. The language is formal and objective, typical of academic peer reviews. 

Here's a breakdown:
* Sentiment: The reviewer sees value in the work ('clever trick', 'important contribution') but acknowledges limitations ('results are not overwhelmingly positive'). Overall, it leans positive. 
* Politeness: The reviewer uses direct language but not in a disrespectful way. They point out flaws ('virtually nothing novel', 'confusing') but offer solutions or ask for clarification. This is acceptable in a peer review setting.",60.0,70.0
Kronecker-factored Curvature Approximations for Recurrent Neural Networks,"['James Martens', 'Jimmy Ba', 'Matt Johnson']",Accept,2018,"[9, 8, 5]","[14, 13, 8]","[38, 96, 5]","[19, 46, 4]","[16, 49, 0]","[3, 1, 1]","This paper extends the Kronecker-factor Approximate Curvature (K-FAC) optimization method to the setting of recurrent neural networks. The K-FAC method is an approximate 2nd-order optimization method that builds a block diagonal approximation of the Fisher information matrix, where the block diagonal elements are Kronecker products of smaller matrices. 

In order to approximate the Fisher information matrix for RNNs, the authors assume that the derivative of the loss function with respect to each weight matrix at each time step is independent of the length of the sequence, that these derivatives are temporally homogeneous, that the input and derivatives of the output are independent across every point in time, and that either the one-step cross-covariance of these derivatives is symmetric or that the training sequences are effectively infinite in length. Based on these assumptions, the authors show that the Fisher information can be reduced into a form in which the derivatives of the weight matrices can be approximated by a linear Gaussian graphical model and in which the approximate 2nd order method can be efficiently carried out. The authors compare their method to SGD on two language modeling tasks and against Adam for learning differentiable neural computers.

The paper is relatively clear, and the authors do a reasonable job of introducing related work of the original K-FAC algorithm as well as its extension to CNNs before systematically deriving their method for RNNs. The problem of extending the K-FAC algorithm is natural, and the steps taken in this paper seem natural yet also original and non-trivial.  

The main issue that I have with this paper is the lack of theoretical justification or even intuition for the many approximations carried out in the course of approximating the Fisher information matrix. In many instances, it seemed like these approximations were made purely for convenience and tractability without much regard for (even approximate) correctness. This quality of this paper would be greatly  strengthened if it had some bounds on approximation error or even empirical results testing the validity of the assumptions in the paper. Moreover, the experiments do not demonstrate levels of statistical significance in the results, so it is difficult to assert the practical significance of this work.  

Specific comments and questions
Page 2, ""r is is"". Typo.
Page 2, ""DV"". I found the introduction of V without any explanation to be confusing.
Page 2, ""P_{y|x}(\theta)"". The relation between P_{y|x}(\theta) and f(x,\theta) is never explained.
Page 3, ""common practice of computing the natural gradient as (F + \lambda I) \nabla h instead of F^{-1} \nabla h"". I don't see how the former can serve as a replacement for the latter.
Page 3, ""approximate g and a as statistically independent"". Even though K-FAC already exists, it would be good to explain why this assumption is reasonable, since similar assumptions are made for the work presented in this paper.
Page 4, ""This new approximation, called ""KFC"", is derived by assuming...."". Same as previous comment. It would be good to briefly discuss why these assumptions are reasonable.
Page 5, Independence of T and w_t's, temporal homogeneity of w_t's,, and independence between a_t's and g_t's. I can see why these are convenient assumptions, but why are they reasonable? Moreover, why is it further natural to assume that A and G are temporally homogeneous as well?
Page 7, ""But insofar as the w_t's ... encode the relevant information contained in these external variables, they should be approximately Markovian"". I am not sure what this means.
Page 7, ""The linear-Gaussian assumption meanwhile is a more severe one to make, but it seems necessary for there to be any hope that the required expectations remain tractable"". I am not sure that this is a good enough justification for such an idea, unless there are compelling approximation error bounds. 
Page 8, Option 1. In what situations is it reasonable to assume that V_1 is symmetric? 
Pages 8-9, Option 2. What is a good finite sample size in which the assumption that the training sequences are infinitely long is reasonable in practice? Can the error |\kappa(x) - \zeta_T(x)| be translated into a statement on the approximation error?
Page 9, ""V_1 = V_{1,0} = ..."". Typos (that appear to have been caught by the authors already).
Page 9, ""The 2nd-order statistics ... are accumulated through an exponential moving average during training"". How sensitive is the performance of this method to the decay rate of the exponential moving average? 
Page 10, ""The additional computations required to get the approximate Fisher inverse from these statistics ... are performed asynchronously on the CPU's"". I find it a bit unfair to compare SGD to K-FAC in terms of wall clock time without also using the extra CPU's for SGD as well (e.g. via Hogwild or synchronous parallel SGD).
Page 10, ""The hyperparameters of our approach..."". What is the sensitivity of the experimental results to these hyperparameters? Moreover, how sensitive are the results to initialization?
Page 10, ""we found that each parameter update of our method required about 80% more wall-clock time than an SGD update"". How much of this is attributed to the fact that the statistics are computed asynchronously?
Pages 10-12, Experiments. There are no error bars in any of the plots, so it is impossible to ascertain the statistical significance of any of these results. 
Page 11: Figure 2. Where is the Adam batchsize 50 line in the left plot? Why did the Adam batchsize 200 line disappear halfway through the right plot?
  


  ","[5, 7, 7]","[' Marginally below acceptance threshold', ' Good paper, accept', ' Good paper, accept']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is mostly positive in its assessment of the paper's contributions, noting the originality and natural extension of the K-FAC algorithm to RNNs. The reviewer finds the paper relatively clear and well-written. However, the review raises significant concerns about the lack of theoretical justification for the approximations made, which weakens the paper's impact. The numerous specific comments and questions further highlight areas where the paper could improve in terms of clarity, rigor, and experimental validation. Therefore, the sentiment is closer to neutral due to the significant concerns raised. The language used is polite and professional, focusing on constructive criticism and suggestions for improvement.",40.0,80.0
Learning a neural response metric for retinal prosthesis,"['Nishal P Shah', 'Sasidhar Madugula', 'EJ Chichilnisky', 'Yoram Singer', 'Jonathon Shlens']",Accept,2018,"[1, 6, 25, 27]","[6, 11, 30, 31]","[7, 7, 23, 160]","[7, 6, 16, 99]","[0, 0, 0, 21]","[0, 1, 7, 40]","In their paper, the authors propose to learn a metric between neural responses by either optimizing a quadratic form or a deep neural network. The pseudometric is optimized by positing that the distance between two neural responses to two repeats of the same stimulus should be smaller than the distance between responses to different stimuli. They do so with the application of improving neural prosthesis in mind. 

First of all, I am doubtful about this application: I don't think the task of neural prosthesis can ever be to produce idential output pattern to the same stimuli. Nevertheless, a good metric for neural responses that goes beyond e.g. hamming distance or squared error between spike density function would be clearly useful for understanding neural representations.

Second, I find the framework proposed by the authors interesting, but not clearly motivated from a neurobiological perspective, as the similarity between stimuli does not appear to play a role in the optimized loss function. For two similar stimuli, natural responses of neural population can be more similar than the responses to two repetitions of the same stimulus.

Third, the results presented by the authors are not convincing throughout. For example, 4B suggests that indeed the Hamming distance achieves lower error than the learned representation.

Nevertheless, it is an interesting approach that is worthwhile pursuing further. ","[6, 5, 7]","[' Marginally above acceptance threshold', ' Marginally below acceptance threshold', ' Good paper, accept']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer expresses doubt and finds the results ""not convincing"", which points towards a negative sentiment. However, they acknowledge the approach as ""interesting"" and ""worthwhile pursuing further"", suggesting it's not entirely negative. Therefore, the sentiment is somewhat mixed but leans negative. The language used is professional and avoids harsh criticism, indicating politeness. ",-25.0,50.0
Certified Defenses against Adversarial Examples ,"['Aditi Raghunathan', 'Jacob Steinhardt', 'Percy Liang']",Accept,2018,"[4, 10, 14]","[9, 15, 19]","[52, 115, 346]","[22, 45, 166]","[30, 62, 170]","[0, 8, 10]","The authors propose a new defense against security attacks on neural networks. The attack model involves a standard l_inf norm constraint. Remarkably, the approach outputs a security certificate (security guarantee) on the algorithm, which makes it appealing for security use in practice. Furthermore, the authors include an approximation of the certificate into their objective function, thus training networks that are more robust against attacks. The approach is evaluated for several attacks on MNIST data.

First of all, the paper is very well written and structured. As standard in the security community, the attack model is precisely formalized (I find this missing in several other ML papers on the topic). The certificate is derived with rigorous and sound math. An innovative approximation based on insight into a relation to the MAXCUT algorithm is shown. An innovative training criterion based on that certificate is proposed. Both the performance of the new training objective and the tightness of the cerificate are analyzed empirically showing that good agreement with the theory and good results in terms of robustness against several attacks.

In summary, this is an innovative paper that treats the subject with rigorous mathematical formalism and is successful in the empirical evaluation. For me, it is a clear accept. The only drawback I see is the missing theoretical and empirical comparison to the recent NIPS 2017 paper by Hein et al.
","[8, 5, 8]","[' Top 50% of accepted papers, clear accept', ' Marginally below acceptance threshold', ' Top 50% of accepted papers, clear accept']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer explicitly states ""this is an innovative paper"", ""rigorous mathematical formalism"", ""successful in the empirical evaluation"", and ""a clear accept."" This points to a highly positive sentiment. The mention of a ""drawback"" is minor and doesn't significantly detract from the positive assessment. The language used is professional and courteous throughout, indicating a high level of politeness.",90.0,90.0
Activation Maximization Generative Adversarial Nets,"['Zhiming Zhou', 'Han Cai', 'Shu Rong', 'Yuxuan Song', 'Kan Ren', 'Weinan Zhang', 'Jun Wang', 'Yong Yu']",Accept,2018,"[12, 13, 8, 2, 10, 8, 16, 18]","[17, 18, 8, 7, 15, 13, 21, 23]","[34, 92, 10, 25, 68, 375, 359, 503]","[16, 35, 7, 11, 27, 185, 163, 340]","[15, 29, 3, 12, 25, 163, 155, 106]","[3, 28, 0, 2, 16, 27, 41, 57]","+ Pros:
- The paper properly compares and discusses the connection between AM-GAN and class conditional GANs in the literature (AC-GAN, LabelGAN)
- The experiments are thorough
- Relation to activation maximization in neural visualization is also properly mentioned
- The authors publish code and honestly share that they could not reproduce AC-GAN's results and thus using to its best variant AC-GAN* that they come up with. I find this an important practice worth encouraging!
- The analysis of Inception score is sound.
+ Cons:
- A few presentation/clarity issues as below
- This paper leaves me wonder why AM-GAN rather than simply characterizing D as a 2K-way classifier (1K real vs 1K fake).

+ Clarity: 
The paper is generally well-written. However, there are a few places that can be improved:
- In 2.2, the authors mentioned ""In fact, the above formulation is a modified version of the original AC-GAN.."", which puts readers confusion whether they were previously just discussed AC-GAN or AC-GAN* (because the previous paragraph says ""AC-GAN are defined as.."".
- Fig. 2: it's not clear what the authors trying to say if looking at only figures and caption. I'd suggest describe more in the caption and follow the concept figure in Odena et al. 2016.
- A few typos here and there e.g. ""[a]n diversity measurement""

+ Originality: AM-GAN is an incremental work by applying AM to GAN. However, I have no problems with this.
+ Significance: 
- Authors show that in quantitative measures, AM-GAN is better than existing GANs on CIFAR-10 / TinyImageNet. Although I don't find much a real difference by visually comparing of samples of AM-GAN to AC-GAN*.

Overall, this is a good paper with thorough experiments supporting their findings regarding AM-GAN and Inception score!","[7, 8, 5]","[' Good paper, accept', ' Top 50% of accepted papers, clear accept', ' Marginally below acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is mostly positive. The reviewer lists several positive aspects of the paper, such as thorough experiments, honest reporting of results, and sound analysis. While there are some criticisms regarding clarity and originality, they are presented constructively and do not overshadow the positive aspects. The reviewer explicitly states that this is a 'good paper' with thorough experiments.",60.0,80.0
Evidence Aggregation for Answer Re-Ranking in Open-Domain Question Answering,"['Shuohang Wang', 'Mo Yu', 'Jing Jiang', 'Wei Zhang', 'Xiaoxiao Guo', 'Shiyu Chang', 'Zhiguo Wang', 'Tim Klinger', 'Gerald Tesauro', 'Murray Campbell']",Accept,2018,"[4, 10, 14, 12, 6, 8, 12, 15, 32, 38]","[9, 15, 19, 15, 11, 13, 17, 20, 37, 42]","[94, 187, 145, 33, 86, 212, 202, 38, 118, 78]","[43, 86, 105, 15, 39, 106, 73, 22, 67, 36]","[50, 93, 29, 14, 38, 95, 57, 15, 28, 23]","[1, 8, 11, 4, 9, 11, 72, 1, 23, 19]","Traditional open-domain QA systems typically have two steps: passage retrieval and aggregating answers extracted from the retrieved passages.  This paper essentially follows the same paradigm, but leverages the state-of-the-art reading comprehension models for answer extraction, and develops the neural network models for the aggregating component.  Although the idea seems incremental, the experimental results do seem solid.  The paper is generally easy to follow, but in several places the presentation can be further improved.

Detailed comments/questions:
  1. In Sec. 2.2, the justification for adding H^{aq} and \bar{H}^{aq} is to downweigh the impact of stop word matching.  I feel this is a somewhat indirect and less effective design, if avoiding stop words is really the reason.  A standard preprocessing step may be better.
  2. In Sec. 2.3, it seems that the final score is just the sum of three individual normalized scores. It's not truly a ""weighted"" combination, where the weights are typically assumed to be tuned.
  3. Figure 3: Connecting the dots in the two subfigures on the right does not make sense.  Bar charts should be used instead.
  4. The end of Sec. 4.2: I feel it's a bad example, as the passage does not really support the answer. The fact that ""Sesame Street"" got picked is probably just because it's more famous.
  5. It'd be interesting to see how traditional IR answer aggregation methods perform, such as simple classifiers or heuristics by word matching (or weighted by TFIDF) and counting.  This will demonstrates the true advantages of leveraging modern NN models.

Pros:
  1. Updating a traditional open-domain QA approach with neural models
  2. Experiments demonstrate solid positive results

Cons:
  1. The idea seems incremental
  2. Presentation could be improved
","[6, 6, 8]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Top 50% of accepted papers, clear accept']","[4, 2, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is fairly confident that the evaluation is correct']","The review acknowledges the paper's merits (solid results, easy to follow) but points out it's incremental. The detailed comments are constructive and suggest improvements. Overall, it leans positive but with room for improvement.",40.0,80.0
Few-Shot Learning with Graph Neural Networks,"['Victor Garcia Satorras', 'Joan Bruna Estrach']",Accept,2018,"[2, 9]","[7, 14]","[18, 162]","[7, 64]","[11, 88]","[0, 10]","This paper studies the problem of one-shot and few-shot learning using the Graph Neural Network (GNN) architecture that has been proposed and simplified by several authors. The data points form the nodes of the graph with the edge weights being learned, using ideas similar to message passing algorithms similar to Kearnes et al and Gilmer et al. This method generalizes several existing approaches for few-shot learning including Siamese networks, Prototypical networks and Matching networks. The authors also conduct experiments on the Omniglot and mini-Imagenet data sets, improving on the state of the art.

There are a few typos and the presentation of the paper could be improved and polished more. I would also encourage the authors to compare their work to other unrelated approaches such as Attentive Recurrent Comparators of Shyam et al, and the Learning to Remember Rare Events approach of Kaiser et al, both of which achieve comparable performance on Omniglot. I would also be interested in seeing whether the approach of the authors can be used to improve real world translation tasks such as GNMT. ","[7, 7, 7]","[' Good paper, accept', ' Good paper, accept', ' Good paper, accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with positive statements, highlighting the relevance and novelty of the paper's contributions. It acknowledges the paper's achievements in improving state-of-the-art results. While it points out areas for improvement like typos and presentation, these are presented as constructive suggestions rather than harsh criticisms. The reviewer also provides specific recommendations for further comparisons and potential applications, indicating a genuine interest in the paper's development. Overall, the tone is encouraging and suggestive of further work, making it a positive review.",60.0,80.0
Action-dependent Control Variates for Policy Optimization via Stein Identity,"['Hao Liu*', 'Yihao Feng*', 'Yi Mao', 'Dengyong Zhou', 'Jian Peng', 'Qiang Liu']",Accept,2018,"[16, 2, 17, 18, 10, 9]","[21, 7, 22, 21, 15, 14]","[564, 29, 76, 62, 166, 183]","[260, 11, 34, 37, 84, 88]","[66, 17, 21, 17, 62, 86]","[238, 1, 21, 8, 20, 9]","The paper proposes action-dependent baselines for reducing variance in policy gradient, through the derivation based on Stein’s identity and control functionals. The method relates closely to prior work on action-dependent baselines, but explores in particular on-policy fitting and a few other design choices that empirically improve the performance. 

A criticism of the paper is that it does not require Stein’s identity/control functionals literature to derive Eq. 8, since it can be derived similarly to linear control variate and it has also previously been discussed in IPG [Gu et. al., 2017] as reparameterizable control variate. The derivation through Stein’s identity does not seem to provide additional insights/algorithm designs beyond direct derivation through reparameterization trick.

The empirical results appear promising, and in particular in comparison with Q-Prop, which fits Q-function using off-policy TD learning. However, the discussion on the causes of the difference should be elaborated much more, as it appears there are substantial differences besides on-policy/off-policy fitting of the Q, such as:

-FitLinear fits linear Q (through parameterization based on linearization of Q) using on-policy learning, rather than fitting nonlinear Q and then at application time linearize around the mean action. A closer comparison would be to use same locally linear Q function for off-policy learning in Q-Prop.

-The use of on-policy fitted value baseline within Q-function parameterization during on-policy fitting is nice. Similar comparison should be done with off-policy fitting in Q-Prop.

I wonder if on-policy fitting of Q can be elaborated more. Specifically, on-policy fitting of V seems to require a few design details to have best performance [GAE, Schulman et. al., 2016]: fitting on previous batch instead of current batch to avoid overfitting  (this is expected for your method as well, since by fitting to current batch the control variate then depends nontrivially on samples that are being applied), and possible use of trust-region regularization to prevent V from changing too much across iterations. 

The paper presents promising results with direct on-policy fitting of action-dependent baseline, which is promising since it does not require long training iterations as in off-policy fitting in Q-Prop. As discussed above, it is encouraged to elaborate other potential causes that led to performance differences. The experimental results are presented well for a range of Mujoco tasks. 

Pros:

-Simple, effective method that appears readily available to be incorporated to any on-policy PG methods without significantly increase in computational time

-Good empirical evaluation

Cons:

-The name Stein control variate seems misleading since the algorithm/method does not rely on derivation through Stein’s identity etc. and does not inherit novel insights due to this derivation.
","[7, 7, 7]","[' Good paper, accept', ' Good paper, accept', ' Good paper, accept']","[4, 3, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review is mostly positive. The reviewer acknowledges the paper's contribution (""The paper presents promising results..."") and finds the empirical results to be good. However, they also raise some criticisms and suggest several improvements. The language used is formal and professional, without any signs of rudeness.",60.0,80.0
Decision Boundary Analysis of Adversarial Examples,"['Warren He', 'Bo Li', 'Dawn Song']",Accept,2018,"[7, 10, 20]","[11, 15, 25]","[27, 300, 440]","[16, 124, 254]","[9, 148, 154]","[2, 28, 32]","The paper presents a new approach to generate adversarial attacks to a neural network, and subsequently present a method to defend a neural network from those attacks. I am not familiar with other adversarial attack strategies aside from the ones mentioned in this paper, and therefore I cannot properly assess how innovative the method is.

My comments are the following:

1- I would like to know if benign examples are just regular examples or some short of simple way of computing adversarial attacks.

2- I think the authors should provide a more detailed and formal description of the OPTMARGIN method. In section 3.2 they explain that ""Our attack uses existing optimization attack techniques to..."", but one should be able to understand the method without reading further references. Specially a formal representation of the method should be included.

3- Authors mention that OPTSTRONG attack does not succeed in finding adversarial examples (""it succeeds on 28% of the samples on MNIST;73% on CIFAR-10""). What is the meaning of success rate in here? Is it the % of times that the classifier is confused?

4- OPTSTRONG produces images that are notably more distorted than OPTBRITTLE (by RMS and also visually in the case of MNIST). So I actually cannot tell which method is better, at least in the MNIST experiment. One could do a method that completely distort the image and therefore will be classified with as a class. But adversarial images should be visually undistinguishable from original images. Generated CIFAR images seem similar than the originals, although CIFAR images are very low resolution, so judging this is hard.

4- As a side note, it would be interesting to have an explanation about why region classification is providing a worse accuracy than point classification for CIFAR-10 benign samples.

As a summary, the authors presented a method that successfully attacks other existing defense methods, and present a method that can successfully defend this attack. I would like to see more formal definitions of the methods presented. Also, just by looking at RMS it is expected that this method works better than OPTBRITTLE, since the images are more distorted. It would be needed to have a way of visually evaluate the similarity between original images and generated images.","[6, 6, 6]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[3, 3, 2]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']","The reviewer acknowledges the paper's contribution (""successfully attacks other existing defense methods"") but also expresses uncertainty about its novelty and requires further clarification. The reviewer maintains a neutral tone throughout, posing questions and suggestions rather than outright criticisms.",10.0,80.0
Memory-based Parameter Adaptation,"['Pablo Sprechmann', 'Siddhant M. Jayakumar', 'Jack W. Rae', 'Alexander Pritzel', 'Adria Puigdomenech Badia', 'Benigno Uria', 'Oriol Vinyals', 'Demis Hassabis', 'Razvan Pascanu', 'Charles Blundell']",Accept,2018,"[12, 1, 3, 3, 3, 7, 12, 10, 9, 9]","[17, 4, 7, 7, 7, 7, 17, 15, 14, 14]","[56, 24, 36, 27, 25, 19, 209, 54, 156, 88]","[31, 10, 17, 11, 11, 8, 101, 10, 63, 35]","[20, 14, 19, 14, 13, 9, 98, 26, 87, 48]","[5, 0, 0, 2, 1, 2, 10, 18, 6, 5]","Overall, the idea of this paper is simple but interesting. Via weighted mean NLL over retrieved neighbors, one can update parameters of output network for a given query input. The MAP interpretation provides a flexible Bayesian explanation about this MbPA.

The paper is written well, and the proposed method is evaluated on a number of relevant applications (e.g., continuing learning, incremental learning, unbalanced data, and domain shifts.)

Here are some comments:
1 MbPA is built upon memory. How large should it be? Is it efficient to retrieve neighbors for a given query?
2 For each test, how many steps of MbPA do we need in general? Furthermore, it is a bit unfair for me to retrain deep model, based on test inputs. It seems that, you are implicitly using test data to fit model.
","[6, 6, 8]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Top 50% of accepted papers, clear accept']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer finds the paper's idea interesting and well-written, indicating a positive sentiment. They also validate the thoroughness of the paper with positive remarks on the experiments. However, the reviewer also raises valid questions and concerns, suggesting potential areas of improvement. Therefore, the sentiment is positive but not overly enthusiastic. The language used is constructive and professional, employing a polite and respectful tone throughout.",60.0,80.0
FearNet: Brain-Inspired Model for Incremental Learning,"['Ronald Kemker', 'Christopher Kanan']",Accept,2018,"[2, 9]","[7, 14]","[15, 109]","[3, 46]","[9, 52]","[3, 11]","Quality: The paper presents a novel solution to an incremental classification problem based on a dual memory system. The proposed solution is inspired by the memory storage mechanism in brain.

Clarity: The problem has been clearly described and the proposed solution is described in detail. The results of numerical experiments and the real data analysis are satisfactory and clearly shows the superior performance of the method compared to the existing ones.

Originality: The solution proposed is a novel one based on a dual memory system inspired by the memory storage mechanism in brain. The memory consolidation is inspired by the mechanisms that occur during sleep. The numerical experiments showing the FearNet performance with sleep frequency also validate the comparison with the brain memory system.

Significance: The work discusses a significant problem of incremental classification. Many of the shelf deep neural net methods require storage of previous training samples too and that slows up the application to larger dataset. Further the traditional deep neural net also suffers from the catastrophic forgetting. Hence, the proposed work provides a novel and scalable solution to the existing problem.

pros: (a) a scalable solution to the incremental classification problem using a brain inspired dual memory system
          (b) mitigates the catastrophic forgetting problem using a memory consolidation by pseudorehearsal.
          (c) introduction of a subsystem that allows which memory system to use for the classification

cons: (a)  How FearNet would perform if imbalanced classes are seen in more than one study sessions?
          (b) Storage of class statistics during pseudo rehearsal could be computationally expensive. How to cope with that?
          (c) How FearNet would handle if there are multiple data sources?","[7, 6, 7]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Good paper, accept']","[4, 2, 2]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']","The review uses positive language like ""novel"", ""clearly described"", ""satisfactory"", ""superior performance"", ""significant problem"", ""scalable solution"". It acknowledges the strengths of the paper and raises relevant questions for improvement, indicating a positive but discerning stance. ",75.0,80.0
 Neural Map: Structured Memory for Deep Reinforcement Learning,"['Emilio Parisotto', 'Ruslan Salakhutdinov']",Accept,2018,"[4, 16]","[9, 21]","[32, 419]","[13, 207]","[17, 201]","[2, 11]","This paper presents a fully differentiable neural architecture for mapping and path planning for navigation in previously unseen environments, assuming near perfect* relative localization provided by velocity. The model is more general than the cognitive maps (Gupta et al, 2017) and builds on the NTM/DNC or related architectures (Graves et al, 2014, 2016, Rae et al, 2017) thanks to the 2D spatial structure of the associative memory. Basically, it consists of a 2D-indexed grid of features (the map) M_t that can be summarized at each time point into read vector r_t, and used for extracting a context c_t for the current agent state s_t, compute (thanks to an LSTM/GRU) an updated write vector w_{t+1}^{x,y} at the current position and update the map using that write vector. The position {x,y} is a binned representation of discrete or continuous coordinates. The absolute coordinate map can be replaced by a relative ego-centric map that is shifted (just like in Gupta et al, 2017) as the agent moves.

The experiments are exhaustive and include remembering the goal location with or without cues (similarly to Mirowski et al, 2017, not cited) in simple mazes of size 4x4 up to 8x8 in the 3D Doom environment. The most important aspect is the capability to build a feature map of previously unseen environments.

This paper, showing excellent and important work, has already been published on arXiv 9 months ago and widely cited. It has been improved since, through different sets of experiments and apparently a clearer presentation, but the ideas are the same. I wonder how it is possible that the paper has not been accepted at ICML or NIPS (assuming that it was actually submitted there). What are the motivations of the reviewers who rejected the paper - are they trying to slow down competing research, or are they ignorant, and is the peer review system broken? I quite like the formulation of the NIPS ratings: ""if this paper does not get accepted, I am considering boycotting the conference"".

* The noise model experiment in Appendix D is commendable, but the noise model is somewhat unrealistic (very small variance, zero mean Gaussian) and assumes only drift in x and y, not along the orientation. While this makes sense in grid world environments or rectilinear mazes, it does not correspond to realistic robotic navigation scenarios with wheel skid, missing measurements, etc... Perhaps showing examples of trajectories with drift added would help convince the reader (there is no space restriction in the appendix).","[9, 7, 6]","[' Top 15% of accepted papers, strong accept', ' Good paper, accept', ' Marginally above acceptance threshold']","[5, 4, 5]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review starts with a detailed and appreciative summary of the paper's contributions, highlighting its strengths and novelty. The reviewer clearly understands the work and finds it technically sound and significant. Phrases like ""excellent and important work,"" ""exhaustive experiments,"" and ""most important aspect"" all point towards a positive sentiment. However, the reviewer then questions why the paper wasn't accepted at other venues, suggesting potential bias or flaws in the peer review system. This questioning, while reflecting a degree of frustration on behalf of the authors, doesn't detract from the positive view of the paper itself. The critique regarding the noise model, while valid, is presented constructively with suggestions for improvement. Overall, the tone is professional and respectful, even when expressing concerns.",80.0,70.0
Latent Constraints: Learning to Generate Conditionally from Unconditional Generative Models,"['Jesse Engel', 'Matthew Hoffman', 'Adam Roberts']",Accept,2018,"[4, 13, 12]","[9, 17, 17]","[61, 85, 65]","[30, 49, 24]","[31, 28, 34]","[0, 8, 7]","# Paper overview:
This paper presents an analysis of a basket of approaches which together enable one to sample conditionally from a class of 
generative models which have been trained to match a joint distribution. Latent space constraints (framed as critics) are learned which confine the generating distribution to lie in a conditional subspace, which when combined with what is termed a 'realism' constraint enables the generation of realistic conditional images from a more-or-less standard VAE trained to match the joint data-distribution.

'Identity preserving' transformations are then introduced within the latent space, which allow the retrospective minimal modification of sample points such that they lie in the conditional set of interest (or not).  Finally, a brief foray into unsupervised techniques for learning these conditional constraints is made, a straightforward extension which I think clouds rather than enlightens the overall exposition.

# Paper discussion:
I think this is a nicely written paper, which gives a good explanation of the problem and their proposed innovations, however I am curious to see that the more recent ""Plug & Play Generative Networks: Conditional Iterative Generation of Images in Latent Space"" by Nguyen et al. was not cited.  This is an empirically very successful approach for conditional generation at 'test-time'. 

Other minor criticisms include:
* I find the 'realism' constraint a bit weak, but perhaps it is simply a naming issue.  Did you experiment with alternative approaches for encouraging marginal probability mass?

* The regularisation term L_dist, why this and not log(1 + exp(z' - z)) (or many arbitrary others)? 

* The claim of identity preservation is (to me) a strong one: it would truly be hard to minimise the trajectory distance wrt. the actual 'identity' of the subject.

* For Figure 6 I would prefer a different colourscheme: the red does not show up well on screen.

* ""Furthermore, CGANs and CVAEs suffer from the same problems of mode-collapse and blurriness as their unconditional cousins"" -> this is debateable, there are many papers which employ various methods to (attempt to) alleviate this issue.


# Conclusion:
I think this is a nice piece of work, if the authors can confirm why ""Plug & Play Generative Networks: Conditional Iterative Generation of Images in Latent Space"" is not placed relative to this work in the paper, I would be happy to see it published.  If stuck for space, I would personally recommend moving the one-shot generation section to the appendix as I do not think it adds a huge amount to the overall exposition.","[7, 7, 7]","[' Good paper, accept', ' Good paper, accept', ' Good paper, accept']","[3, 4, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer provides generally positive feedback, stating that the paper is ""nicely written"" and offers a ""good explanation."" While they raise several questions and suggestions for improvement, these are presented constructively and aim to enhance the paper's clarity and impact. The reviewer's willingness to recommend publication contingent on addressing their concerns further indicates a positive overall sentiment.",65.0,80.0
Matrix capsules with EM routing,"['Geoffrey E Hinton', 'Sara Sabour', 'Nicholas Frosst']",Accept,2018,[6],[9],[9],[5],[1],[3],"The paper describes another instantiation of ""capsules"" which attempt to learn part-whole relationships and the geometric pose transformations between them.  Results are presented on the smallNORB test set obtaining impressive performance.

Although I like very much this overall approach, this particular paper is so opaquely written that it is difficult to understand exactly what was done and how the network works.  It sounds like the main innovation here is using a 4x4 matrix for the pose parameters, and an iterative EM algorithm to find the correspondence between capsules (routing by agreement).  But what exactly the pose matrix represents, and how they get transformed from one layer to the next, is left almost entirely to the reader's imagination.  In addition, how EM factors in, what the probabilities P_ih represent, etc. is not clear.  I think the authors could do a much better job explaining this model, the rationale behind it, and how it works.

Perhaps the most interesting and compelling result is Figure 2, which shows how ambiguity in object class assignment is resolved with each iteration.  This is very intriguing, but it would be great to understand what is going on and how this is happening.

Although the results are impressive, if one can't understand how this was achieved it is hard to know what to make of it.

","[4, 7, 6]","[' Ok but not good enough - rejection', ' Good paper, accept', ' Marginally above acceptance threshold']","[2, 3, 3]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer starts with a positive note, showing appreciation for the overall approach. However, the review quickly becomes critical of the paper's clarity, using terms like ""opaquely written"" and stating it's ""difficult to understand."" The reviewer acknowledges the impressive results but expresses concern about the lack of clarity in achieving them. The language, while critical, maintains a professional and constructive tone. ",-20.0,60.0
A Hierarchical Model for Device Placement,"['Azalia Mirhoseini', 'Anna Goldie', 'Hieu Pham', 'Benoit Steiner', 'Quoc V. Le', 'Jeff Dean']",Accept,2018,"[9, 10, 7, 3, 14, 27]","[14, 15, 12, 8, 19, 31]","[68, 24, 58, 26, 299, 105]","[38, 10, 23, 10, 143, 44]","[22, 13, 27, 15, 145, 32]","[8, 1, 8, 1, 11, 29]","This paper proposes a device placement algorithm to place operations of tensorflow on devices. 

Pros:

1. It is a novel approach which trains the placement end to end.
2. The experiments are solid to demonstrate this method works very well.
3. The writing is easy to follow.
4. This would be a very useful tool for the community if open sourced.

Cons:

1. It is not very clear in the paper whether the training happens for each model yielding separate agents, or a shared agent is trained and used for all kinds of models. The latter would be more exciting. The adjacency matrix varies size for different graphs, so I guess a separate agent is trained for each graph? However, if the agent is not shared, why not just use integer to represent each operation in the graph, since overfitting would be more desirable in this case.
2. Averaging the embedding is hard to understand especially for the output sizes and number of outputs.
3. It is not clear how the adjacency information is used.
","[8, 5, 5]","[' Top 50% of accepted papers, clear accept', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is positive overall. It highlights the novelty and usefulness of the work, as well as the clarity of the writing. While it raises valid concerns and questions, it does so in a neutral and constructive manner, suggesting potential improvements and seeking clarification rather than criticizing. ",60.0,70.0
Apprentice: Using Knowledge Distillation Techniques To Improve Low-Precision Network Accuracy,"['Asit Mishra', 'Debbie Marr']",Accept,2018,"[11, 4]","[14, 7]","[48, 25]","[37, 17]","[8, 6]","[3, 2]","The paper aims at improving the accuracy of a low precision network based on knowledge distillation from a full-precision network. Instead of distillation from a pre-trained network, the paper proposes to train both teacher and student network jointly. The paper shows an interesting result that the distilled low precision network actually performs better than high precision network.

I found the paper interesting but the contribution seems quite limited.

Pros:
1. The paper is well written and easy to read.
2. The paper reported some interesting result such as that the distilled low precision network actually performs better than high precision network, and that training jointly outperforms the traditional distillation method (fixing the teacher network) marginally.

Cons:
1. The name Apprentice seems a bit confusing with apprenticeship learning.
2. The experiments might be further improved by providing a systematic study about the effect of precisions in this work (e.g., producing more samples of precisions on activations and weights).
3. It is unclear how the proposed method outperforms other methods based on fine-tuning. It is also quite possible that after fine-tuning the compressed model usually performs quite similarly to the original model.","[7, 7, 8]","[' Good paper, accept', ' Good paper, accept', ' Top 50% of accepted papers, clear accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer found the paper interesting and well-written, highlighting the positive results. However, they also pointed out limitations regarding the contribution and suggested further experiments. The language used is constructive and professional, without resorting to harsh or negative phrasing. Therefore, the sentiment leans slightly towards the positive side due to the interest shown and the positive aspects mentioned, while the politeness remains highly positive.",60.0,80.0
Residual Connections Encourage Iterative Inference,"['Stanisław Jastrzebski', 'Devansh Arpit', 'Nicolas Ballas', 'Vikas Verma', 'Tong Che', 'Yoshua Bengio']",Accept,2018,"[6, 8, 7, 27, 7, 31]","[10, 13, 12, 31, 12, 36]","[53, 45, 75, 34, 38, 975]","[16, 20, 39, 16, 14, 405]","[29, 25, 35, 15, 18, 454]","[8, 0, 1, 3, 6, 116]","
This paper shows that residual networks can be viewed as doing a sort of iterative inference, where each layer is trained to use its “nonlinear part” to push its values in the negative direction of the loss gradient.  The authors demonstrate this using a Taylor expansion of a standard residual block first, then follow up with several experiments that corroborate this interpretation of iterative inference.  Overall the strength of this paper is that the main insight is quite interesting — though many people have informally thought of residual networks as having this interpretation — this paper is the first one to my knowledge to explain the intuition in a more precise way.  

Some weaknesses of the paper on the other hand — some of the parts of the paper (e.g. on weight sharing) are only somewhat related to the main topic of the paper. In fact, the authors moved the connection to SGD to the appendix, which I thought would be *more* related.   Additionally, parts of the paper are not as clearly written as they could be and lack rigor.  This includes the mathematical derivation of the main insight — some of the steps should be spelled out more explicitly.  The explanation following is also handwavey despite claims to being formal.   

Some other lower level thoughts:
* Regarding weight sharing for residual layers, I don’t understand why we can draw the conclusion that the initial gradient explosion is responsible for the lower generalization capability of the model with shared weights.  Are there other papers in literature that have shown this connection?
* The name “cosine loss” suggests that this function is actually being minimized by a training procedure, but it is just a value that is being plotted… perhaps just call it the cosine?
* I recommend that the authors also check out Figurnov et al CVPR 2017 (""Spatially Adaptive Computation Time for Residual Networks"") which proposes an “adaptive” version of ResNet based on the intuition of adaptive inference.
* The plots in the later parts of the paper are quite small and hard to read.  They are also spaced together too tightly (horizontally), making it difficult to immediately see what each plot is supposed to represent via the y-axis label.
* Finally, the citations need to be fixed (use \citep{} instead of \cite{})

","[7, 6, 5]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Marginally below acceptance threshold']","[4, 3, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review starts with positive remarks, highlighting the interesting main insight and its novelty. However, it then lists several weaknesses, including lack of clarity, rigor, and relevance of some parts. The reviewer also provides constructive criticism and suggestions for improvement. Overall, the feedback is balanced but leans slightly more towards the negative due to the mentioned weaknesses.",20.0,70.0
Twin Networks: Matching the Future for Sequence Generation,"['Dmitriy Serdyuk', 'Nan Rosemary Ke', 'Alessandro Sordoni', 'Adam Trischler', 'Chris Pal', 'Yoshua Bengio']",Accept,2018,"[4, 8, 10, 3, 22, 31]","[9, 13, 15, 8, 27, 36]","[28, 61, 95, 82, 230, 975]","[10, 22, 52, 39, 98, 405]","[17, 38, 40, 43, 109, 454]","[1, 1, 3, 0, 23, 116]","
1) Summary
This paper proposes a recurrent neural network (RNN) training formulation for encouraging RNN the hidden representations to contain information useful for predicting future timesteps reliably. The authors propose to train a forward and backward RNN in parallel. The forward RNN predicts forward in time and the backward RNN predicts backwards in time. While the forward RNN is trained to predict the next timestep, its hidden representation is forced to be similar to the representation of the backward RNN in the same optimization step. In experiments, it is shown that the proposed method improves training speed in terms of number of training iterations, achieves 0.8 CIDEr points improvement over baselines using the proposed training, and also achieves improved performance for the task of speech recognition.


2) Pros:
+ Novel idea that makes sense for learning a more robust representation for predicting the future and prevent only local temporal correlations learned.
+ Informative analysis for clearly identifying the strengths of the proposed method and where it is failing to perform as expected.
+ Improved performance in speech recognition task.
+ The idea is clearly explained and well motivated.


3) Cons:
Image captioning experiment:
In the experimental section, there is an image captioning result in which the proposed method is used on top of two baselines. This experiment shows improvement over such baselines, however, the performance is still worse compared against baselines such as Lu et al, 2017 and Yao et al, 2016. It would be optimal if the authors can use their training method on such baselines and show improved performance, or explain why this cannot be done.


Unconditioned generation experiments:
In these experiments, sequential pixel-by-pixel MNIST generation is performed in which the proposed method did not help. Because of this, two conditioned set ups are performed: 1) 25% of pixels are given before generation, and 2) 75% of pixels are given before generation. The proposed method performs similar to the baseline in the 25% case, and better than the baseline in the 75% case. For completeness, and to come to a stronger conclusion on how much uncertainty really affects the proposed method, this experiment needs a case in which 50% of the pixels are given. Observing 25% of the pixels gives almost no information about the identity of the digit and it makes sense that it’s hard to encode the future, however, 50% of the pixels give a good idea of what the digit identity is. If the authors believe that the 50% case is not necessary, please feel free to explain why.


Additional comments:
The method is shown to converge faster compared to the baselines, however, it is possible that the baseline may finish training faster (the authors do acknowledge the additional computation needed in the backward RNN).
It would be informative for the research community to see the relationship of training time (how long it takes in hours) versus how fast it learns (iterations taken to learn).

Experiments on RL planning tasks would be interesting to see (Maybe on a simple/predictable environment).


4) Conclusion
The paper proposes a method for training RNN architectures to better model the future in its internal state supervised by another RNN modeling the future in reverse. Correctly modeling the future is very important for tasks that require making decisions of what to do in the future based on what we predict from the past. The proposed method presents a possible way of better modeling the future, however, some the results do not clearly back up the claim yet. The given score will improve if the authors are able to address the stated issues.


POST REBUTTAL RESPONSE:
The authors have addressed the comments on the MNIST experiments and show better results, however, as far as I can see, they did not address my concern about the comparisons on the image captioning experiment. In the image captioning experiment the authors choose two networks (Show & Tell and Soft attention) that they improve using the proposed method that end up performing similar to the second best baseline (Yao et al. 2016) based on Table 3 and their response. I requested for the authors to use their method on the best performing baselines (i.e. Yao et al. 2016 or Liu et al. 2017) or explain why this cannot be done (maybe my request was not clearly stated). Applying the proposed method on the strong baselines would highlight the author's claims more strongly than just applying on the average performing chosen baselines. This request was not addressed and instead the authors just improved the average performing baselines in Table 3 to meet the best baselines. Given, that the authors were able to improve the results in the sequential MNIST and improve the average baselines, my rating improves one point. However, I still have concerns about this method not being shown to improve the best methods presented in Table 3 which would give a more solid result. My rating changes to marginally above threshold for acceptance.","[6, 8, 7]","[' Marginally above acceptance threshold', ' Top 50% of accepted papers, clear accept', ' Good paper, accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is mostly positive, highlighting the novelty and clarity of the paper. The reviewer acknowledges the strengths like improved performance in speech recognition and informative analysis. However, they also point out specific concerns and areas for improvement, particularly regarding the image captioning and MNIST generation experiments. The reviewer's request for applying the method to stronger baselines in image captioning wasn't fully addressed, which slightly holds back the enthusiasm. The language used is polite and professional, focusing on constructive criticism and suggestions.",60.0,80.0
Towards Image Understanding from Deep Compression Without Decoding,"['Robert Torfason', 'Fabian Mentzer', 'Eirikur Agustsson', 'Michael Tschannen', 'Radu Timofte', 'Luc Van Gool']",Accept,2018,"[3, 3, 5, 7, 10, 35]","[3, 8, 10, 12, 15, 40]","[4, 29, 56, 71, 468, 1396]","[3, 12, 29, 31, 250, 801]","[1, 16, 25, 38, 194, 420]","[0, 1, 2, 2, 24, 175]","Neural-net based image compression is a field which is about to get hot, and this paper asks the obvious question: can we design a neural-net based image compression algorithm such that the features it produces are useful for classification & segmentation?

The fact that it's an obvious question does not mean that it's a question that's worthless. In fact, I am glad someone asked this question and tried to answer it. 

Pros:
- Clear presentation, easy to follow.
- Very interesting, but obvious, question is explored. 
- The paper is very clear, and uses building blocks which have been analyzed before, which leaves the authors free to explore their interactions rather than each individual building block's property.
- Results are shown on two tasks (classification / segmentation) rather than just one (the obvious one would have been to only discuss results on classification), and relatively intuitive results are shown (i.e., more bits = better performance). What is perhaps not obvious is how much impact does doubling the bandwidth have (i.e., initially it means more, then later on it plateaus, but much earlier than expected).
- Joint training of compression + other tasks. As far as I know this is the first paper to talk about this particular scenario.
- I like the fact that classical codecs were not completely discarded (there's a comparison with JPEG 2K).
- The discussion section is of particular interest, discussing openly the pros/cons of the method (I wish more papers would be as straightforward as this one).

Cons:
- I would have liked to have a discussion on the effect of the encoder network. Only one architecture/variant was used.
- For PSNR, SSIM and MS-SSIM I would like a bit more clarity whether these were done channel-wise, or on the grayscale channel.
- While runtime is given as pro, it would be nice for those not familiar with the methods to provide some runtime numbers (i.e., breakdown how much time does it take to encode and how much time does it take to classify or segment, but in seconds, not flops). For example, Figure 6 could be augmented with actual runtime in seconds.
- I wish the authors did a ctrl+F for ""??"" and fixed all the occurrences.
- One of the things that would be cool to add later on but I wished to have beeyn covered is whether it's possible to learn not only to compress, but also downscale. In particular, the input to ResNet et al for classification is fixed sized, so the question is -- would it be possible to produced a compact representation to be used for classification given arbitrary image resolutions, and if yes, would it have any benefit?

General comments:
- The classification bits are all open source, which is very good. However, there are very few neural net compression methods which are open sourced. Would you be inclined to open source the code for your implementation? It would be a great service to the community if yes (and I realize that it could already be open sourced -- feel free to not answer if it may lead to break anonymity, but please take this into consideration).
","[9, 6, 6]","[' Top 15% of accepted papers, strong accept', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[5, 3, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is overwhelmingly positive. The reviewer explicitly states their gladness that this research question is being explored and lists numerous pros, including clarity, interesting results, and valuable discussion. While the reviewer provides constructive criticism and suggestions for improvement, these are presented as opportunities to enhance the already strong work. The language is polite and encouraging throughout, using phrases like ""I would have liked"" and ""it would be nice"" to frame suggestions.",80.0,90.0
Mixed Precision Training,"['Paulius Micikevicius', 'Sharan Narang', 'Jonah Alben', 'Gregory Diamos', 'Erich Elsen', 'David Garcia', 'Boris Ginsburg', 'Michael Houston', 'Oleksii Kuchaiev', 'Ganesh Venkatesh', 'Hao Wu']",Accept,2018,"[-1, 19]","[3, 24]","[4, 145]","[4, 72]","[0, 3]","[0, 70]","The paper provides methods for training deep networks using half-precision floating point numbers without losing model accuracy or changing the model hyper-parameters. The main ideas are to use a master copy of weights when updating the weights, scaling the loss before back-prop and using full precision variables to store products. Experiments are performed on a large number of state-of-art deep networks, tasks and datasets which show that the proposed mixed precision training does provide the same accuracy at half the memory.

Positives
- The experimental evaluation is fairly exhaustive on a large number of deep networks, tasks and datasets and the proposed training preserves the accuracy of all the tested networks at half the memory cost.

Negatives
- The overall technical contribution is fairly small and are ideas that are regularly implemented when optimizing systems.
- The overall advantage is only a 2x reduction in memory which can be gained by using smaller batches at the cost of extra compute. ","[5, 7, 8]","[' Marginally below acceptance threshold', ' Good paper, accept', ' Top 50% of accepted papers, clear accept']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review acknowledges the positive aspects of the paper, such as its exhaustive experimental evaluation and the achievement of maintaining accuracy with reduced memory usage. However, it also points out limitations, stating that the technical contribution is relatively minor and the benefits could be achieved through alternative methods. The language used is professional and critical but not disrespectful, indicating a neutral to slightly negative sentiment overall.",-10.0,50.0
Relational Neural Expectation Maximization: Unsupervised Discovery of Objects and their Interactions,"['Sjoerd van Steenkiste', 'Michael Chang', 'Klaus Greff', 'Jürgen Schmidhuber']",Accept,2018,"[3, 15, 7, 29]","[8, 20, 12, 34]","[34, 18, 45, 440]","[13, 10, 16, 232]","[18, 4, 26, 132]","[3, 4, 3, 76]","Summary
---
This work applies a representaion learning technique that segments entities to learn simple 2d intuitive physics without per-entity supervision. It adds a relational mechanism to Neural Expectation Maximization and shows that this mechanism provides a better simulation of bouncing balls in a synthetic environment.

Neural Expectation Maximization (NEM) decomposes an image into K latent variables (vectors of reals) theta_k. A decoder network reconstructs K images from each of these latent variables and these K images are combined into a single reconstruction using pixel-wise mixture components that place more weight on pixels that match the ground truth. An encoder network f_enc() then updates the latent variables to better explain the reconstructions they produced.
The neural nets are learned so that the latent variables reconstruct the image well when used by the mixture model and match a prior otherwise. Previously NEM has been shown to learn variables which represent individual objects (simple shapes) in a compositional manner, using one variable per object.

Other recent neural models can learn to simulate simple 2d physics environments (balls bouncing around in a 2d plane). That work supervises the representation for each entity (ball) explicitly using states (e.g. position and velocity of balls) which are known from the physics simulator used to generate the training data. The key feature of these models is the use of a pairwise embedding of an object and its neighbors (message passing) to predict the object's next state in the simulation.

This paper paper combines the two methods to create Relational Neural Expectation Maximization (R-NEM), allowing direct interaction at inference time between the latent variables that encode a scene. The encoder network from NEM can be seen as a recurrent network which takes one latent variable theta_k at time t and some input x to produce the next latent variable theta_k at time t+1. R-NEM adds a relational module which computes an embedding used as a third input to the recurrent encoder. Like previous relational models, this one uses a pairwise embedding of the object being updated (object k) and its neighbors. Unlike previous neural physics models, R-NEM uses a soft attention mechanism to determine which objects are neighbors and which are not. Also unlike previous neural models, this method does not require per-object supervision.

Experiments show that R-NEM learns compositional representations that support intuitive physics more effectively than ablative baselines. These experiments
show:
1) R-NEM reconstructs images more accurately than baselines (RNN/LSTM) and NEM (without object interaction).
2) R-NEM is trained with 4 objects per image. It does a bit worse at reconstructing images with 6-8 objects per image, but still performs better than baselines.
3) A version of R-NEM without neighborhood attention in the relation module matches the performance of R-NEM using 4 objects and performs worse than R-NEM at 6-8 objects.
4) R-NEM learns representations which factorize into one latent variable per object as measured by the Adjusted Rand Index, which compares NEM's pixel clustering to a ground truth clustering with one cluster per object.
5) Qualitative and quantitative results show that R-NEM can simulate 2d ball physics for many time steps more effectively than an RNN and while only suffering gradual divergence from the ground truth simulation.

Qualitative results show that the attentional mechanism attends to objects which are close to the context object together, acting like the heuristic neighborhood mechanism from previous work.

Follow up experiments extend the basic setup significantly. One experiment shows that R-NEM demonstrates object permanence by correctly tracking a collision when one of the objects is completely occluded. Another experiment applies the method to the Space Invaders Atari game, showing that it treats columns of aliens as entities. This representation aligns with the game's goal.


Strengths
---

The paper presents a clear, convincing, and well illustrated story.

Weaknesses
---

* RNN-EM BCE results are missing from the simulation plot (right of figure 4).

Minor comments/concerns:

* 2nd paragraph in section 4: Are parameters shared between these 3 MLPs (enc,emb,eff)? I guess not, but this is ambiguous.

* When R-NEM is tested against 6-8 balls is K set to the number of balls plus 1? How does performance vary with the number of objects?

* Previous methods report performance across simulations of a variety of physical phenomena (e.g., see ""Visual Interaction Networks""). It seems that supervision isn't needed for bouncing ball physics, but I wonder if this is the case for other kinds of phenomena (e.g., springs in the VIN paper). Can this method eliminate the need for per-entity supervision in this domain?

* A follow up to the previous comment: Could a supervised baseline that uses per-entity state supervision and neural message passsing (like the NPE from Chang et. al.) be included?

* It's a bit hard to qualitatively judge the quality of the simulations without videos to look at. Could videos of simulations be uploaded (e.g., via anonymous google drive folder as in ""Visual Interaction Networks"")?

* This uses a neural message passing mechanism like those of Chang et. al. and Battaglia et. al. It would be nice to see a citation to neural message passing outside of the physics simulation domain (e.g. to ""Neural Message Passing for Quantum Chemistry"" by Gilmer et. al. in ICML17).

* Some work uses neighborhood attention coefficients for neural message passing. It would be nice to see a citation included.
    * See ""Neighborhood Attention"" in ""One-Shot Imitation Learning"" by Duan et. al. in NIPS17
    * Also see ""Programmable Agents"" by Denil et. al.


Final Evaluation
---

This paper clearly advances the body of work on neural intuitive physics by incorporating NEM entity representation to allow for less supervision. Alternatively, it adds a message passing mechanism to the NEM entity representation technique. These are moderately novel contributions and there are only minor weaknesses, so this is a clear accept.","[7, 8, 7]","[' Good paper, accept', ' Top 50% of accepted papers, clear accept', ' Good paper, accept']","[4, 5, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']","The review is clearly positive. The reviewer states ""This is a clear accept"" and  ""This paper clearly advances the body of work..."", which indicates a positive sentiment. The weaknesses listed are relatively minor, further supporting this assessment. The language used is polite and professional throughout, suggesting a neutral to positive politeness score.",75.0,80.0
Learning Intrinsic Sparse Structures within Long Short-Term Memory,"['Wei Wen', 'Yuxiong He', 'Samyam Rajbhandari', 'Minjia Zhang', 'Wenhan Wang', 'Fang Liu', 'Bin Hu', 'Yiran Chen', 'Hai Li']",Accept,2018,"[27, 15, 7, 9, 7, 29, 25, 17, 16]","[32, 20, 12, 14, 12, 34, 30, 22, 21]","[141, 128, 35, 68, 32, 486, 300, 540, 414]","[63, 76, 21, 42, 17, 262, 106, 311, 261]","[28, 35, 13, 24, 11, 26, 29, 94, 76]","[50, 17, 1, 2, 4, 198, 165, 135, 77]","The paper spends lots of (repeated)  texts on motivating and explaining ISS. But the algorithm is simple, using group lasso to find components that are can retained to preserve the performance.  Thus the novelty is limited.

The experiments results are good.

Sec 3.1 should be made more concise. ","[6, 7, 7]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Good paper, accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is somewhat negative in the beginning as it criticizes the paper for being too verbose and lacking novelty. However, it calls the experimental results ""good,"" which adds a slightly more positive element. The language used is quite direct and critical, but not necessarily rude. ",-20.0,40.0
Empirical Risk Landscape Analysis for Understanding Deep Neural Networks,"['Pan Zhou', 'Jiashi Feng']",Accept,2018,"[12, 9]","[17, 14]","[406, 542]","[134, 214]","[119, 235]","[153, 93]","Overall, this work seems like a reasonable attempt to answer the question of how the empirical loss landscape relates to the true population loss landscape.  The analysis answers:

1) When empirical gradients are close to true gradients
2) When empirical isolated saddle points are close to true isolated saddle points
3) When the empirical risk is close to the true risk.

The answers are all of the form that if the number of training examples exceeds a quantity that grows with the number of layers, width and the exponential of the norm of the weights with respect to depth, then empirical quantities will be close to true quantities.  I have not verified the proofs in this paper (given short notice to review) but the scaling laws in the upper bounds found seem reasonably correct. 

Another reviewer's worry about why depth plays a role in the convergence of empirical to true values in deep linear networks is a reasonable worry, but I suspect that depth will necessarily play a role even in deep linear nets because the backpropagation of gradients in linear nets can still lead to exponential propagation of errors between empirical and true quantities due to finite training data.  Moreover the loss surface of deep linear networks depends on depth even though the expressive capacity does not.   An analysis of dynamics on this loss surface was presented in Saxe et. al. ICLR 2014 which could be cited to address that reviewer's concern.  However, the reviewer's suggestion that the results be compared to what is known more exactly for simple linear regression is a nice one. 

Overall, I believe this paper is a nice contribution to the deep learning theory literature. However,  it would even better to help the reader with more intuitive statements about the implications of their results for practice, and the gap between their upper bounds and practice, especially given the intense interest in the generalization error problem.   Because their upper bounds look similar to those based on Rademacher complexity or VC dimension (although they claim theirs are a little tighter) - they should put numbers in to their upper bounds taken from trained neural networks, and see what the numerical evaluation of their upper bounds turn out to be in situations of practical interest where deep networks show good generalization performance despite having significantly less training data than number of parameters.  I suspect their upper bounds will be loose, but still  - it would be an excellent contribution to the literature to quantitatively compare theory and practice with bounds that are claimed to be slightly tigher than previous bounds.  Even if they are loose - identifying the degree of looseness could inspire interesting future work. 
","[7, 7, 3]","[' Good paper, accept', ' Good paper, accept', ' Clear rejection']","[3, 3, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer explicitly states ""Overall, I believe this paper is a nice contribution to the deep learning theory literature."" which points towards a positive sentiment. They do have some suggestions for improvement, but those are constructive and not delivered with a negative tone, hence the politeness score is also on the positive side.",60.0,70.0
A DIRT-T Approach to Unsupervised Domain Adaptation,"['Rui Shu', 'Hung Bui', 'Hirokazu Narui', 'Stefano Ermon']",Accept,2018,"[5, 3, 2, 10]","[9, 5, 4, 15]","[53, 13, 4, 406]","[22, 6, 1, 199]","[23, 7, 2, 200]","[8, 0, 1, 7]","The paper was a good contribution to domain adaptation. It provided a new way of looking at the problem by using the cluster assumption. The experimental evaluation was very thorough and shows that VADA and DIRT-T performs really well. 

I found the math to be a bit problematic. For example, L_d in (4) involves a max operator. Although I understand what the authors mean, I don't think this is the correct way to write this. (5) should discuss the min-max objective. This will probably involve an explanation of the gradient reversal etc. Speaking of GRL, it's mentioned on p.6 that they replaced GRL with the traditional GAN objective. This is actually pretty important to discuss in detail: did that change the symmetric nature of domain-adversarial training to the asymmetric nature of traditional GAN training? Why was that important to the authors?

The literature review could also include Shrivastava et al. and Bousmalis et al. from CVPR 2017. The latter also had MNIST/MNIST-M experiments.","[7, 8, 7]","[' Good paper, accept', ' Top 50% of accepted papers, clear accept', ' Good paper, accept']","[2, 4, 4]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer explicitly states the paper is a ""good contribution"" and praises the experimental evaluation. This points to a positive sentiment. The reviewer also provides constructive criticism and suggestions for improvement, but these are framed politely with phrases like ""I found the math to be a bit problematic"" and ""The literature review could also include..."". This suggests a neutral to positive politeness level.",60.0,70.0
Parameter Space Noise for Exploration,"['Matthias Plappert', 'Rein Houthooft', 'Prafulla Dhariwal', 'Szymon Sidor', 'Richard Y. Chen', 'Xi Chen', 'Tamim Asfour', 'Pieter Abbeel', 'Marcin Andrychowicz']",Accept,2018,"[3, 5, 3, 4, 13, 4, 20, 17, 7]","[6, 5, 8, 8, 13, 8, 25, 22, 11]","[17, 22, 22, 14, 9, 47, 328, 608, 48]","[2, 10, 8, 4, 3, 21, 219, 291, 21]","[11, 8, 14, 9, 6, 24, 39, 291, 25]","[4, 4, 0, 1, 0, 2, 70, 26, 2]","This paper explores the idea of adding parameter space noise in service of exploration. The paper is very well written and quite clear. It does a good job of contrasting parameter space noise to action space noise and evolutionary strategies.

However, the results are weak. Parameter noise does better in some Atari + Mujoco domains, but shows little difference in most domains. The domains where parameter noise (as well as evolutionary strategies) does really well are Enduro and the Chain environment, in which a policy that repeatedly chooses a particular action will do very well. E-greedy approaches will always struggle to choose the same random action repeatedly. Chain is great as a pathological example to show the shortcomings of e-greedy, but few interesting domains exhibit such patterns. Similarly for the continuous control with sparse rewards environments – if you can construct an environment with sparse enough reward that action-space noise results in zero rewards, then clearly parameter space noise will have a better shot at learning. However, for complex domains with sparse reward (e.g. Montezuma’s Revenge) parameter space noise is just not going to get you very far.

Overall, I think parameter space noise is a worthy technique to have analyzed and this paper does a good job doing just that. However, I don’t expect this technique to make a large splash in the Deep RL community, mainly because simply adding noise to the parameter space doesn’t really gain you much more than policies that are biased towards particular actions. Parameter noise is not a very smart form of exploration, but it should be acknowledged as a valid alternative to action-space noise.

A non-trivial amount of work has been done to find a sensible way of adding noise to parameter space of a deep network and defining the specific distance metrics and thresholds for (dual-headed) DQN, DDPG, and TRPO.
","[6, 7, 7]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Good paper, accept']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with positive remarks, highlighting the paper's clarity and contribution to understanding parameter space noise. However, it then expresses significant concerns about the weakness of the results, arguing that the method's benefits are limited to specific scenarios. While acknowledging the value of the analysis, the reviewer doubts its practical impact on the Deep RL field. The tone remains professional and avoids harsh language, focusing on constructive criticism. Therefore, the sentiment leans towards the negative side due to the reservations about the method's effectiveness, but not overly so due to the balanced perspective presented.",-20.0,70.0
Compositional Attention Networks for Machine Reasoning,"['Drew A. Hudson', 'Christopher D. Manning']",Accept,2018,"[1, 26]","[5, 31]","[13, 408]","[5, 284]","[8, 104]","[0, 20]","Summary: 
The paper presents a new model called Compositional Attention Networks (CAN) for visual reasoning. The complete model consists of an input unit, a sequence of the proposed Memory, Attention and Composition (MAC) cell, and an output unit. Experiments on CLEVR dataset shows that the proposed model outperforms previous models.

Strengths: 
— The idea of building a compositional model for visual reasoning and visual question answering makes a lot of sense, and, I think, is the correct direction to go forward in these fields.
— The proposed model outperforms existing models pushing the state-of-the-art.
— The proposed model is computationally cheaper and generalizes well with less training data as compared to existing models.
— The proposed model has been described in detail in the paper.

Weaknesses: 
— Given that the performance of state-on-art on CLEVR dataset is already very high ( <5% error) and the performance numbers of the proposed model are not very far from the previous models, it is very important to report the variance in accuracies along with the mean accuracies to determine if the performance of the proposed model is statistically significantly better than the previous models.
— It is not clear which part of the proposed model leads to how much improvement in performance. Ablations studies are needed to justify the motivations for each of the components of the proposed model.
— Analysis of qualitative results (including attention maps, gate values, etc.) is needed to justify if the model is actually doing what the authors think it should do. For example, the authors mention an example on page 6 at the end of Section 3.2.2, but do not justify if this is actually what the model is doing.
— Why is it necessary to use both question and memory information to answer the question even when the question was already used to compute the memory information? I would think that including the question information helps in learning the language priors in the dataset. Have the authors looked at some qualitative examples where the model which only uses memory information gives an incorrect answer but adding the question information results in a correct answer?
— Details such as using Glove word embeddings are important and can affect the performance of models significantly. Therefore, they should be clearly mentioned in the main paper while comparing with other models which do not use them.
— The comparisons of number of epochs required for training and the training time need fixed batch sizes and CPU/GPU configurations. Is that true? These should be reported in this section.
— The authors claim that their model is robust to linguistic variations and diverse vocabulary, by which I am guessing they are referring to experiments on CLEVR-Humans dataset. What is there in the architecture of the proposed model which provides this ability? If it is the Glove vectors, it should be clearly mentioned since any other model using Glove vectors should have this ability.
— On page 6, second paragraph, the authors mention that there are cases which necessitate the model to ignore current memories. Can the authors show some qualitative examples for such cases?
— In the intro, the authors claim that their proposed cell encourages transparency. But, the design of their cell doesn’t seem to do so, nor it is justified in the paper.

Overall: The performance reported in the paper is impressive and outperforms previous state-of-the-art, but without proper statistical significance analysis of performance, ablation studies, analysis of various attention maps, memory gates, etc. and qualitative results, I am not sure if this work would be directly useful for the research community.","[7, 6, 7]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Good paper, accept']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer acknowledges the strengths of the paper, such as the novelty of the model and its impressive performance exceeding current state-of-the-art. However, they also raise significant concerns about the lack of certain analyses (statistical significance, ablation studies, qualitative examples) that would strengthen the paper's claims and usefulness to the research community. The overall tone is critical but professional, suggesting improvements rather than outright rejection.",40.0,70.0
Generalizing Across Domains via Cross-Gradient Training,"['Shiv Shankar*', 'Vihari Piratla*', 'Soumen Chakrabarti', 'Siddhartha Chaudhuri', 'Preethi Jyothi', 'Sunita Sarawagi']",Accept,2018,"[2, 4, 26, 14, 10, 25]","[7, 9, 31, 19, 15, 30]","[31, 23, 193, 77, 99, 154]","[14, 9, 116, 27, 62, 89]","[17, 14, 45, 33, 33, 33]","[0, 0, 32, 17, 4, 32]","The method is posed in the Bayesian setting, the main idea being to achieve the data augmentation through domain-guided perturbations of input instances. Different from traditional adaptation methods, where the adaptation step is applied explicitly, in this paper the authors exploit labeled instances from several domains to collectively train a system that can handle new domains without the adaptation step. While this is another way of looking at domain adaptation, it may be misleading to say 'without' adaptation step. By the gradient perturbations on multi-domain training data, the learning of the adaptation step is effectively done. This should be clarified in the paper. The notion of using 'scarce' training domains to cover possible choices for the target domain is interesting and novel. The experimental validation should also include a deeper analysis of this factor: how the proposed adaptation performance is affected by the scarcity of the training multi-domain data. While this is partially shown in Table 8, it seems that by adding more domains the performance is compromised (compared to the baseline) (?).  It would be useful to see how the model ranks the multiple domains in terms of their relatedness to the target domain. Figs 6-7 are unclear and difficult to read. The captions should provide more information about the main point of these figures. ","[7, 7, 8, 7]","[' Good paper, accept', ' Good paper, accept', ' Top 50% of accepted papers, clear accept', ' Good paper, accept']","[4, 5, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review acknowledges the novelty of the paper's approach to domain adaptation (""interesting and novel"") and its grounding in a Bayesian setting. While it raises constructive criticism and suggests improvements (clarifying the adaptation step, deeper analysis, improving figure clarity), the overall tone is analytical and suggestive, rather than dismissive. The reviewer uses relatively neutral language and avoids harsh criticism.",50.0,70.0
Hierarchical Subtask Discovery with Non-Negative Matrix Factorization,"['Adam C. Earle', 'Andrew M. Saxe', 'Benjamin Rosman']",Accept,2018,"[3, 13, 13]","[4, 18, 18]","[7, 43, 74]","[3, 20, 40]","[3, 19, 25]","[1, 4, 9]","This paper proposes a formulation for discovering subtasks in Linearly-solvable MDPs. The idea is to decompose the optimal value function into a fixed set of sub value functions (each corresponding to a subtask) in a way that they best approximate (e.g. in a KL-divergence sense) the original value.

Automatically discovering hierarchies in planning/RL problems is an important problem that may provide important benefits especially in multi-task environments. In that sense, this paper makes a reasonable contribution to that goal for multitask LMDPs. The simulations also show that the discovered hierarchy can be interpreted. Although the contribution is a methodological one, from an empirical standpoint, it may be interesting to provide further evidence of the benefits of the proposed approach. Overall, it would also be useful to provide a short paragraph about similarities to the literature on discovering hierarchies in MDPs.  

A few other comments and questions: 

- This may be a fairly naive question but given your text I'm under the impression that the goal in LMDPs is to find z(s) for all states (and Z in the multitask formulation). Then, your formulation for discovery subtasks seems to assume that Z is given. Does that mean that the LMDPs must first be solved and only then can subtasks be discovered? (The first sentence in the introduction seems to imply that there's hope of faster learning by doing hierarchical decomposition).

- You motivate your approach (Section 3) using a max-variance criterion (as in PCA), yet your formulation actually uses the KL-divergence. Are these equivalent objectives in this case?


Other (minor) comments: 

- In Section it would be good to define V(s) as well as 'i' in q_i (it's easy to mistake it for an index). ","[6, 7, 5]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Marginally below acceptance threshold']","[2, 3, 2]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']","The reviewer provides a generally positive overview, acknowledging the paper's contribution and reasonable approach. They also offer constructive criticism and suggestions for improvement, indicating a willingness to see the work published with revisions. The language used is formal, respectful, and focused on the research itself.",60.0,80.0
Variational image compression with a scale hyperprior,"['Johannes Ballé', 'David Minnen', 'Saurabh Singh', 'Sung Jin Hwang', 'Nick Johnston']",Accept,2018,"[12, 18, 13, 3, 4]","[17, 22, 17, 7, 8]","[53, 42, 68, 20, 33]","[28, 26, 35, 9, 14]","[21, 13, 23, 10, 16]","[4, 3, 10, 1, 3]","The paper is a step forward for image deep compression, at least when departing from the (Balle et al., 2017) scheme.
The proposed hyperpriors are especially useful for medium to high bpp and optimized for L2/ PSNR evaluation.

I find the description of the maths too laconic and hard to follow. For example, what’s the U(.|.) operator in (5)?

What’s the motivation of using GDN as non linearity instead of e.g. ReLU?

I am not getting the need of MSSSIM (dB).  How exactly was it defined/computed?

Importance of training data? The proposed models are trained on 1million images while others like (Theis et al, 2017) and [Ref1,Ref2] use smaller datasets for training.

I am missing a discussion about Runtime / complexity vs. other approaches?

Why MSSSIM is a relevant measure? The Fig. 6 seem to show better visual results for L2 loss (PSNR) than when optimized for MSSSIM, at least in my opinion.

What's the reason to use 4:4:4 for BPG and 4:2:0 for JPEG?

What is the relation between hyperprior and importance maps / content-weights [Ref1] ?

What about reproducibility of the results? Will be the codes/models made publicly available?

Relevant literature:
[Ref1] Learning Convolutional Networks for Content-weighted Image Compression (https://arxiv.org/abs/1703.10553)
[Ref2] Soft-to-Hard Vector Quantization for End-to-End Learned Compression of Images and Neural Networks  (https://arxiv.org/abs/1704.00648)
","[7, 7, 7]","[' Good paper, accept', ' Good paper, accept', ' Good paper, accept']","[5, 4, 5]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review starts with a positive note, acknowledging the paper's contribution. However, it raises many questions and requests clarifications, indicating that the reviewer finds the paper lacking in clarity and completeness. The language used is quite direct with phrases like ""I am not getting..."" or ""I am missing..."", which are not overly rude but lean towards a neutral tone rather than overly polite.",40.0,10.0
Deep Active Learning for Named Entity Recognition,"['Yanyao Shen', 'Hyokun Yun', 'Zachary C. Lipton', 'Yakov Kronrod', 'Animashree Anandkumar']",Accept,2018,"[-2, 0, 5]","[3, 5, 10]","[2, 10, 57]","[1, 4, 37]","[1, 6, 14]","[0, 0, 6]","This paper studies the application of different existing active learning strategies for the deep models for NER.

Pros:
* Active learning may be used for improving the performance of deep models for NER in practice
* All the proposed approaches are sound and the experimental results showed that active learning is beneficial for the deep models for NER

Cons:
* The novelty of this paper is marginal. The proposed approaches turn out to be a combination of existing active learning strategies for selecting data to query with the existing deep model for NER. 
* No conclusion can be drawn by comparing with the 4 different strategies.

======= After rebuttal  ================

Thank you for the clarification and revision on this paper. It looks better now.

I understand that the purpose of this paper is to give actionable insights to the practice of deep learning. However, since AL itself is a meta learning framework and neural net as the base learner has been shown to be effective for AL, the novelty and contribution of a general discussion of applying AL for deep neural nets is marginal.  What I really expected is a tightly-coupled active learning strategy that is specially designed for the particular deep neural network structure used for NER. Apparently, however, none of the strategies used in this work is designed for this purpose (e.g., the query strategy or model update strategy should at least reflex some properties of deep learning or NER). Thus, it is still below my expectation. 

Anyway, since the authors had attempted to improved this paper, and the results may provide some information to practice, I would like to slightly raise my rating to give this attempt a chance.

","[6, 7, 6]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Marginally above acceptance threshold']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer acknowledges the potential of the paper's subject and finds the approaches sound. However, they clearly state the novelty is marginal and express disappointment with the lack of a specifically designed strategy for the deep neural network structure. While the reviewer ultimately raises their rating after the rebuttal, their initial sentiment leans towards the negative due to the lack of significant contribution. The language remains professional and polite throughout.",-20.0,80.0
"A Compressed Sensing View of Unsupervised Text Embeddings, Bag-of-n-Grams, and LSTMs","['Sanjeev Arora', 'Mikhail Khodak', 'Nikunj Saunshi', 'Kiran Vodrahalli']",Accept,2018,"[29, 2, 2, 3]","[34, 7, 7, 8]","[217, 41, 25, 19]","[103, 16, 12, 8]","[76, 24, 13, 8]","[38, 1, 0, 3]","The main insight in this paper is that LSTMs can be viewed as producing a sort of sketch of tensor representations of n-grams.  This allows the authors to design a matrix that maps bag-of-n-gram embeddings into the LSTM embeddings. They then show that the result matrix satisfies a restricted isometry condition.  Combining these results allows them to argue that the classification performance based on LSTM embeddings is comparable to that based on bag-of-n-gram embeddings.

I didn't check all the proof details, but based on my knowledge of compressed sensing theory, the results seem plausible. I think the paper is a nice contribution to the theoretical analysis of LSTM word embeddings.","[7, 6, 7]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Good paper, accept']","[3, 4, 1]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', "" The reviewer's evaluation is an educated guess""]","The review is positive about the paper. It highlights the main insight as ""nice"" and the results as ""plausible."" The reviewer finds the paper to be a good contribution. There is no use of negative or overly critical language. The reviewer acknowledges not checking all the proof details, which indicates a neutral stance on that aspect.",75.0,100.0
Many Paths to Equilibrium: GANs Do Not Need to Decrease a Divergence At Every Step,"['William Fedus*', 'Mihaela Rosca*', 'Balaji Lakshminarayanan', 'Andrew M. Dai', 'Shakir Mohamed', 'Ian Goodfellow']",Accept,2018,"[2, 3, 10, 8, 13, 10]","[7, 8, 15, 13, 17, 12]","[38, 23, 90, 74, 66, 107]","[12, 8, 28, 28, 27, 48]","[24, 13, 56, 42, 33, 55]","[2, 2, 6, 4, 6, 4]","Quality: The authors study non-saturating GANs and the effect of two penalized gradient approaches. The authors consider a number of thought experiments to demonstrate their observations and validate these on real data experiments. 

Clarity: The paper is well-written and clear. The authors could be more concise when reporting results. I would suggest keeping the main results in the main body and move extended results to an appendix.

Originality: The authors demonstrate experimentally that there is a benefit of using non-saturating GANs. More specifically, the provide empirical evidence that they can fit problems where Jensen-Shannon divergence fails. They also show experimentally that penalized gradients stabilize the learning process.

Significance: The problems the authors consider is worth exploring further. The authors describe their finding in the appropriate level of details and demonstrate their findings experimentally. However, publishing this  work is in my opinion premature for the following reasons:

- The authors do not provide further evidence of why non-saturating GANs perform better or under which mathematical conditions (non-saturating) GANs will be able to handle cases where distribution manifolds do not overlap;
- The authors show empirically the positive effect of penalized gradients, but do not provide an explanation grounded in theory;
- The authors do not provide practical recommendations how to set-up GANs and not that these findings did not lead to a bullet-proof recipe to train them.

","[4, 7, 8]","[' Ok but not good enough - rejection', ' Good paper, accept', ' Top 50% of accepted papers, clear accept']","[4, 3, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The reviewer acknowledges the quality of the work and its clarity, indicating a positive sentiment. However, they also point out significant limitations and suggest the work is premature for publication. This suggests the sentiment is mixed, leaning slightly positive due to the acknowledgment of the paper's merits. The language used is constructive and professional, suggesting a neutral to polite tone.",20.0,50.0
Multi-Mention Learning for Reading Comprehension with Neural Cascades,"['Swabha Swayamdipta', 'Ankur P. Parikh', 'Tom Kwiatkowski']",Accept,2018,"[7, 9, 9]","[12, 14, 14]","[57, 62, 39]","[27, 27, 17]","[30, 31, 17]","[0, 4, 5]","This paper proposes a lightweight neural network architecture for reading comprehension, which 1) only consists of feed-forward nets; 2) aggregates information from different occurrences of candidate answers, and demonstrates good performance on TriviaQA (where documents are generally pretty long).

Overall, I think it is a nice demonstration that non-recurrent models can work so well, but I also don’t find the results strikingly surprising. It is also a bit hard to get the main takeaway messages. It seems that multi-loss is important (highlight that!), summing up multiple mentions of the same candidate answers seems to be important (This paper should be cited: Text Understanding with the Attention Sum Reader Network https://arxiv.org/abs/1603.01547). But all the other components seem to have been demonstrated previously in other papers. 

An important feature of this model is it is easier to parallelize and speed up the training/testing processes. However, I don’t see any demonstration of this in the experiments section.

Also, I am a bit disappointed by how “cascades” are actually implemented. I was expecting some sophisticated ways of combining information in a cascaded way (finding the most relevant piece of information, and then based on what it is obtained so far trying to find the next piece of relevant information and so on). The proposed model just simply sums up all the occurrences of candidate answers throughout the full document. 3-layer cascade is really just more like stacking several layers where each layer captures information of different granularity. 

I am wondering if the authors can also add results on other RC datasets (e.g., SQuAD) and see if the model can generalize or not. 
","[6, 5, 7]","[' Marginally above acceptance threshold', ' Marginally below acceptance threshold', ' Good paper, accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer finds the work to be a ""nice demonstration"" but not ""strikingly surprising."" They acknowledge the importance of some findings but point out that other components seem to be derived from previous work. The reviewer expresses disappointment with the implementation of ""cascades,"" suggesting it fell short of their expectations.  The request for additional results on other datasets further indicates a desire for stronger evidence. Overall, the feedback is mixed, leaning slightly positive due to the acknowledgement of the paper's contributions.",20.0,50.0
Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks,"['Víctor Campos', 'Brendan Jou', 'Xavier Giró-i-Nieto', 'Jordi Torres', 'Shih-Fu Chang']",Accept,2018,"[4, 9, 16, 30, 28]","[8, 13, 21, 35, 33]","[20, 35, 168, 167, 601]","[8, 20, 85, 115, 370]","[11, 12, 67, 22, 116]","[1, 3, 16, 30, 115]","UPDATE: Following the author's response I've increased my score from 5 to 6. The revised paper includes many of the additional references that I suggested, and the author response clarified my confusion over the Charades experiments; their results are indeed close to state-of-the-art on Charades activity localization (slightly outperformed by [6]), which I had mistakenly confused with activity classification (from [5]).

The paper proposes the Skip RNN model which allows a recurrent network to selectively skip updating its hidden state for some inputs, leading to reduced computation at test-time. At each timestep the model emits an update probability; if this probability is over a threshold then the next input and state update will be skipped. The use of a straight-through estimator allows the model to be trained with standard backpropagation. The number of state updates that the model learns to use can be controlled with an auxiliary loss function. Experiments are performed on a variety of tasks, demonstrating that the Skip-RNN compares as well or better than baselines even when skipping nearly half its state updates.

Pros:
- Task of reducing computation by skipping inputs is interesting
- Model is novel and interesting
- Experiments on multiple tasks and datasets confirm the efficacy of the method
- Skipping behavior can be controlled via an auxiliary loss term
- Paper is clearly written

Cons:
- Missing comparison to prior work on sequential MNIST
- Low performance on Charades dataset, no comparison to prior work
- No comparison to prior work on IMDB Sentiment Analysis or UCF-101 activity classification

The task of reducing computation by skipping RNN inputs is interesting, and the proposed method is novel, interesting, and clearly explained. Experimental results across a variety of tasks are convincing; in all tasks the Skip-RNNs achieve their goal of performing as well or better than equivalent non-skipping variants. The use of an auxiliary loss to control the number of state updates is interesting; since it sometimes improves performance it appears to have some regularizing effect on the model in addition to controlling the trade-off between speed and accuracy.

However, where possible experiments should compare directly with prior published results on these tasks; none of the experiments from the main paper or supplementary material report any numbers from any other published work.

On permuted MNIST, Table 2 could include results from [1-4]. Of particular interest is [3], which reports 98.9% accuracy with a 100-unit LSTM initialized with orthogonal and identity weight matrices; this is significantly higher than all reported results for the sequential MNIST task.

For Charades, all reported results appear significantly lower than the baseline methods reported in [5] and [6] with no explanation. All methods work on “fc7 features from the RGB stream of a two-stream CNN provided by the organizers of the [Charades] challenge”, and the best-performing method (Skip GRU) achieves 9.02 mAP. This is significantly lower than the two-stream results from [5] (11.9 mAP and 14.3 mAP) and also lower than pretrained AlexNet features averaged over 30 frames and classified with a linear SVM, which [5] reports as achieving 11.3 mAP. I don’t expect to see state-of-the-art performance on Charades; the point of the experiment is to demonstrate that Skip-RNNs perform as well or better than their non-skipping counterparts, which it does. However I am surprised at the low absolute performance of all reported results, and would appreciate if the authors could help to clarify whether this is due to differences in experimental setup or something else.

In a similar vein, from the supplementary material, sentiment analysis on IMDB and action classification on UCF-101 are well-studied problems, but the authors do not compare with any previously published results on these tasks.

Though experiments may not show show state-of-the-art performance, I think that they still serve to demonstrate the utility of the Skip-RNN architecture when compared side-by-side with a similarly tuned non-skipping baseline. However I feel that the authors should include some discussion of other published results.

On the whole I believe that the task and method are interesting, and experiments convincingly demonstrate the utility of Skip-RNNs compared to the author’s own baselines. I will happily upgrade my rating of the paper if the authors can address my concerns over prior work in the experiments.


References

[1] Le et al, “A Simple Way to Initialize Recurrent Networks of Rectified Linear Units”, arXiv 2015
[2] Arjovsky et al, “Unitary Evolution Recurrent Neural Networks”, ICML 2016
[3] Cooijmans et al, “Recurrent Batch Normalization”, ICLR 2017
[4] Zhang et al, “Architectural Complexity Measures of Recurrent Neural Networks”, NIPS 2016
[5] Sigurdsson et al, “Hollywood in homes: Crowdsourcing data collection for activity understanding”, ECCV 2016
[6] Sigurdsson et al, “Asynchronous temporal fields for action recognition”, CVPR 2017","[6, 6, 6]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is generally positive, acknowledging the novelty and efficacy of the proposed method. The reviewer finds the paper well-written and the experiments convincing. However, the lack of comparison with prior work, particularly on standard tasks like sequential MNIST and Charades, is a significant concern. The reviewer's tone is polite and constructive, offering specific suggestions for improvement rather than outright criticism. The update at the beginning shows the reviewer is willing to adjust their opinion based on the author's response, indicating a fair and open-minded approach.",60.0,80.0
Dynamic Neural Program Embeddings for Program Repair,"['Ke Wang', 'Rishabh Singh', 'Zhendong Su']",Accept,2018,"[10, 21]","[14, 26]","[133, 178]","[62, 131]","[50, 23]","[21, 24]","Summary of paper: The paper proposes an RNN-based neural network architecture for embedding programs, focusing on the semantics of the program rather than the syntax. The application is to predict errors made by students on programming tasks. This is achieved by creating training data based on program traces obtained by instrumenting the program by adding print statements. The neural network is trained using this program traces with an objective for classifying the student error pattern (e.g. list indexing, branching conditions, looping bounds).

---

Quality: The experiments compare the three proposed neural network architectures with two syntax-based architectures. It would be good to see a comparison with some techniques from Reed & De Freitas (2015) as this work also focuses on semantics-based embeddings.
Clarity: The paper is clearly written.
Originality: This work doesn't seem that original from an algorithmic point of view since Reed & De Freitas (2015) and Cai et. al (2017) among others have considered using execution traces. However the application to program repair is novel (as far as I know).
Significance: This work can be very useful for an educational platform though a limitation is the need for adding instrumentation print statements by hand.

---

Some questions/comments:
- Do we need to add the print statements for any new programs that the students submit? What if the structure of the submitted program doesn't match the structure of the intended solution and hence adding print statements cannot be automated?

---

References 

Cai, J., Shin, R., & Song, D. (2017). Making Neural Programming Architectures Generalize via Recursion. In International Conference on Learning Representations (ICLR).","[7, 6, 7]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Good paper, accept']","[3, 2, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer acknowledges the clarity and potential significance of the work, especially for educational platforms. While they point out a lack of originality in the algorithmic approach, they recognize the novelty in the application to program repair. The reviewer also raises valid concerns and questions, indicating a need for further clarification and potential improvements.",50.0,70.0
Can recurrent neural networks warp time?,"['Corentin Tallec', 'Yann Ollivier']",Accept,2018,"[2, 16]","[6, 20]","[25, 47]","[10, 15]","[15, 28]","[0, 4]","Summary:
This paper shows that incorporating invariance to time transformations in recurrent networks naturally results in a gating mechanism used by LSTMs and their variants. This is then used to develop a simple bias initialization scheme for the gates when the range of temporal dependencies relevant for a problem can be estimated or are known. Experiments demonstrate that the proposed initialization speeds up learning on synthetic tasks, although benefits for next-step prediction tasks are limited.

Quality and significance:
The core insight of the paper is the link between recurrent network design and its effect on how the network reacts to time transformations. This insight is simple, elegant and valuable in my opinion. 

It is becoming increasingly apparent recently that the benefits of the gating and cell mechanisms introduced by the LSTM, now also used in feedforward networks, go beyond avoiding vanishing gradients. The particular structural elements also induce certain inductive biases which make learning or generalization easier in many cases. Understanding the link between model architecture and behavior is very useful for the field in general, and this paper contributes to this knowledge. In light of this, I think it is reasonable to ignore the fact that the proposed initialization does not provide benefits on Penn Treebank and text8. The real value of the paper is in providing an alternative way of thinking about LSTMs that is theoretically sound and intuitive. 

Clarity:
The paper is well-written in general and easy to understand. A minor complaint is that there are an unnecessarily large number of paragraph breaks, especially on pages 3 and 4, which make reading slightly jarring.","[8, 8, 8]","[' Top 50% of accepted papers, clear accept', ' Top 50% of accepted papers, clear accept', ' Top 50% of accepted papers, clear accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer explicitly states the core insight of the paper is ""simple, elegant and valuable"" and that the work is ""theoretically sound and intuitive."" They find the paper ""well-written"" and ""easy to understand."" The reviewer goes on to state the value of the paper outweighs the limited benefits shown in the experiments. All of this points to a very positive sentiment. The reviewer does offer a minor criticism but does so respectfully.  ",90.0,90.0
Consequentialist conditional cooperation in social dilemmas with imperfect information,"['Alexander Peysakhovich', 'Adam Lerer']",Accept,2018,"[5, 10]","[9, 14]","[49, 55]","[21, 26]","[23, 27]","[5, 2]","This paper studies learning to play two-player general-sum games with state (Markov games) with imperfect information. The idea is to learn to cooperate (think prisoner's dilemma) but in more complex domains. Generally, in repeated prisoner's dilemma, one can punish one's opponent for noncooperation. In this paper, they design an apporach to learn to cooperate in a more complex game, like a hybrid pong meets prisoner's dilemma game. This is fun but I did not find it particularly surprising from a game-theoretic or from a deep learning point of view. 

From a game-theoretic point of view, the paper begins with a game-theoretic analysis of a cooperative strategy for these markov games with imperfect information. It is basically a straightforward generalization of the idea of punishing, which is common in ""folk theorems"" from game theory, to give a particular equilibrium for cooperating in Markov games. Many Markov games do not have a cooperative equilibrium, so this paper restricts attention to those that do. Even in games where there is a cooperative solution that maximizes the total welfare, it is not clear why players would choose to do so. When the game is symmetric, this might be ""the natural"" solution but in general it is far from clear why all players would want to maximize the total payoff. 

The paper follows with some fun experiments implementing these new game theory notions. Unfortunately, since the game theory was not particularly well-motivated, I did not find the overall story compelling. It is perhaps interesting that one can make deep learning learn to cooperate with imperfect information, but one could have illustrated the game theory equally well with other techniques.

In contrast, the paper ""Coco-Q: Learning in Stochastic Games with Side Payments"" by Sodomka et. al. is an example where they took a well-motivated game theoretic cooperative solution concept and explored how to implement that with reinforcement learning. I would think that generalizing such solution concepts to stochastic games and/or deep learning might be more interesting.

It should also be noted that I was asked to review another ICLR submission entitled ""MAINTAINING COOPERATION IN COMPLEX SOCIAL DILEMMAS USING DEEP REINFORCEMENT LEARNING"" which amazingly introduced the same ""Pong Player’s Dilemma"" game as in this paper. 

Notice the following suspiciously similar paragraphs from the two papers:

From ""MAINTAINING COOPERATION IN COMPLEX SOCIAL DILEMMAS USING DEEP REINFORCEMENT LEARNING"":
We also look at an environment where strategies must be learned from raw pixels. We use the method
of Tampuu et al. (2017) to alter the reward structure of Atari Pong so that whenever an agent scores a
point they receive a reward of 1 and the other player receives −2. We refer to this game as the Pong
Player’s Dilemma (PPD). In the PPD the only (jointly) winning move is not to play. However, a fully
cooperative agent can be exploited by a defector.

From ""CONSEQUENTIALIST CONDITIONAL COOPERATION IN SOCIAL DILEMMAS WITH IMPERFECT INFORMATION"":
To demonstrate this we follow the method of Tampuu et al. (2017) to construct a version of Atari Pong 
which makes the game into a social dilemma. In what we call the Pong Player’s Dilemma (PPD) when an agent 
scores they gain a reward of 1 but the partner receives a reward of −2. Thus, in the PPD the only (jointly) winning
move is not to play, but selfish agents are again tempted to defect and try to score points even though
this decreases total social reward. We see that CCC is a successful, robust, and simple strategy in this
game.","[5, 6, 7]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Good paper, accept']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer finds the paper's core idea (generalizing punishment to Markov games) not particularly novel or well-motivated. They criticize the game-theoretic foundation as not compelling and question the choice of illustrating it with deep learning. The reviewer even points out potential plagiarism, further decreasing the paper's value. The language, while critical, maintains a professional tone.",-50.0,50.0
Generating Natural Adversarial Examples,"['Zhengli Zhao', 'Dheeru Dua', 'Sameer Singh']",Accept,2018,"[-3, 7, 11]","[1, 11, 16]","[1, 38, 72]","[1, 27, 52]","[0, 1, 0]","[0, 10, 20]","
Summary:
 A method for creation of semantical adversary examples in suggested. The ‘semantic’ property is measured by building a latent space with mapping from this space to the observable (generator) and back (inverter). The generator is trained with a WGAN optimization. Semantic adversarials examples are them searched for by inverting an example to its sematic encoding and running local search around it in that space. The method is tested for generation of images on MNist and part of LSUM data and for creation of text examples which are adversarial in some sense to inference and translation sentences. It is shown that the distance between adversarial example and the original example in the latent space is proportional to the accuracy of the classifier inspected.
Page 3: It seems that the search algorithm has a additional parameter: r_0, the size of the area in which search is initiated. This should be explicitly said and the parameter value should be stated.
Page 4: 
-	the implementation details of the generator, critic and invertor networks are not given in enough details, and instead the reader is referred to other papers. This makes this paper non-clear as a stand alone document, and is a problem for a paper which is mostly based on experiments and their results: the main networks used are not described.
-	the visual examples are interesting, but it seems that they are able to find good natural adversary examples only for a weak classifier. In the MNist case, the examples for thr random forest are nautral and surprising, but those for the LE-Net are often not: they often look as if they indeed belong to the other class (the one pointed by the classifier). In the churce-vs. tower case, a  relatively weak MLP classifier was used. It would be more instructive to see the results for a better, convolutional classifier.
Page 5:
-	the description of the various networks used for text generation is insufficient for understanding:
o	The AREA is described in two sentences. It is not clear how this module is built, was loss was it used to optimize in the first place, and what elements of it are re0used for the current task
o	 ‘inverter’ here is used in a sense which is different than in previous sections of the paper: earlier it denoted the mapping from output (images) to the underlying latent space. Here it denote  a mapping between two latent spaces.
o	 It is not clear what the ‘four-layers strided CNN’ is: its structure, its role in the system. How is it optimized?
o	In general: a block diagram showing the relation between all the system’s components may be useful, plus the details about the structure and optimization of the various modules. It seems that the system here contains 5 modules instead of the three used before (critic, generator and inverter), but this is not clear enough. Also which modules are pre-trained, which are optimized together,a nd which are optimized separately is not clear.
o	SNLI data should be described: content, size, the task it is used for


Pro:
-	A novel idea of producing natural adversary examples with a GAN
-	The generated examples are in some cases useful for interpretation and network understanding 
-	The method enables creation of adversarial examples for block box classifiers
Cons
-	The idea implementation is basic. Specifically search algorithm presented is quite simplistic, and no variations other than plain local search were developed and tested
-	The generated adversarial examples created for successful complex classifiers are often not impressive and useful (they are either not semantical, or semantical but correctly classified by the classifier). Hence It is not clear if the latent space used by the method enables finding of interesting adversarial examples for accurate classifiers. 

","[7, 6, 6]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[4, 3, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review is mostly critical of the paper, pointing out significant flaws in clarity, experimental setup, and the quality of results. While it acknowledges the novelty of the idea, the cons outweigh the pros, indicating a negative sentiment. However, the language used is professional and not overtly harsh, suggesting a neutral politeness level.",-30.0,0.0
Large Scale Optimal Transport and Mapping Estimation,"['Vivien Seguy', 'Bharath Bhushan Damodaran', 'Remi Flamary', 'Nicolas Courty', 'Antoine Rolet', 'Mathieu Blondel']",Accept,2018,"[4, 5, 9, 19, 3, 9]","[8, 10, 14, 24, 7, 14]","[12, 32, 112, 129, 8, 65]","[3, 13, 42, 56, 3, 30]","[4, 13, 50, 41, 2, 28]","[5, 6, 20, 32, 3, 7]","The paper proves the weak convergence of the regularised OT problem to Kantorovich / Monge optimal transport problems.

I like the weak convergence results, but this is just weak convergence. It appears to be an overstatement to claim that the approach ""nearly-optimally"" transports one distribution to the other (Cf e.g. Conclusion). There is a penalty to pay for choosing a small epsilon -- it seems to be visible from Figure 2. Also, near-optimality would refer to some parameters being chosen in the best possible way. I do not see that from the paper. However, the weak convergence results are good.

A better result, hinting on how ""optimal"" this can be, would have been to guarantee that the solution to regularised OT is within f(epsilon) from the optimal one, or from within f(epsilon) from the one with a smaller epsilon (more possibilities exist). This is one of the things experimenters would really care about -- the price to pay for regularisation compared to the unknown unregularized optimum. 

I also like the choice of the two regularisers and wonder whether the authors have tried to make this more general, considering other regularisations ? After all, the L2 one is just an approximation of the entropic one.

Typoes:

1- Kanthorovich -> Kantorovich (Intro)
2- Cal C <-> C (eq. 4)","[6, 8, 7, 6]","[' Marginally above acceptance threshold', ' Top 50% of accepted papers, clear accept', ' Good paper, accept', ' Marginally above acceptance threshold']","[3, 3, 3, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer acknowledges the value of the paper's findings (""I like the weak convergence results..."") but expresses reservations about the strength and practical implications of those findings. They point out a discrepancy between the results and the claimed ""near-optimality,"" suggesting further investigation into the cost of regularization. The reviewer also shows interest in the choice of regularizers and suggests exploring a broader range. The overall tone is critical but constructive, offering specific suggestions for improvement. The language remains professional and respectful throughout.",40.0,70.0
Stabilizing Adversarial Nets with Prediction Methods,"['Abhay Yadav', 'Sohil Shah', 'Zheng Xu', 'David Jacobs', 'Tom Goldstein']",Accept,2018,"[9, 9, 7, 28, 15]","[12, 13, 12, 33, 20]","[13, 15, 64, 186, 296]","[8, 7, 27, 97, 113]","[5, 7, 34, 46, 166]","[0, 1, 3, 43, 17]","NOTE:
I'm very willing to change my recommendation if I turn out to be wrong 
about the issues I'm addressing and if certain parts of the experiments are fixed.

Having said that, I do (think I) have some serious issues: 
both with the experimental evaluation and with the theoretical results.
I'm pretty sure about the experimental evaluation and less sure about the theoretical results.


THEORETICAL CLAIMS:

These are the complaints I'm not as sure about:

Theorem 1 assumes that L is convex/concave.
This is not generally true for GANs.
That's fine and it doesn't necessarily make the statement useless, but:

If we are willing to assume that L is convex/concave, 
then there already exist other algorithms that will provably converge
to a saddle point (I think). [1] contains an explanation of this.
Given that there are other algorithms with the same theoretical guarantees,
and that those algorithms don't magically make GANs work better, 
I am much less convinced about the value of your theorem.

In [0] they show that GANs trained with simultaneous gradient descent are locally asymptotically stable, 
even when L is not convex/concave. 
This seems like it makes your result a lot less interesting, though perhaps I'm wrong to think this?

Finally, I'm not totally sure you can show that simultaneous gradient descent won't converge 
as well under the assumptions you made.
If you actually can't show that, then the therom *is* useless, 
but it's also the thing I've said that I'm the least sure about.


EXPERIMENTAL EVALUATION:

Regarding the claims of being able to train with a higher learning rate:
I would consider this a useful contribution if it were shown that (by some measure of GAN 'goodness')
a high goodness was achieved faster because a higher learning rate was used.
Your experiments don't support this claim presently, because you evaluate all the models at the same step.
In fact, it seems like both evaluated Stacked GAN models get worse performance with the higher learning rate.
This calls into question the usefulness of training with a higher learning rate.
The performance is not a huge amount worse though (based on my understanding of Inception Scores),
so if it turns out that you could get that performance
in 1/10th the time then that wouldn't be so bad.

Regarding the experiment with Stacked GANs, the scores you report are lower than what they report [2].
Their reported mean score for joint training is 8.59.
Are the baseline scores you report from an independent reproduction?
Also, the model they have trained uses label information. 
Does your model use label information?
Given that your reported improvements are small, it would be nice to know what the proposed mechanism is by 
which the score is improved. 
With a score of 7.9 and a standard deviation of 0.08, presumably none of the baseline model runs
had 'stability issues', so it doesn't seem like 'more stable training' can be the answer.

Finally, papers making claims about fixing GAN stability should support those claims by solving problems
with GANs that people previously had a hard time solving (due to instability).
I don't believe this is true of CIFAR10 (especially if you're using the class information).
See [3] for an example of a paper that does this by generating 128x128 Imagenet samples with a single generator.

I didn't pay as much attention to the non-GAN experiments because
a) I don't have as much context for evaluating them, because they are a bit non-standard.
b) I had a lot of issues with the GAN experiments already and I don't think the paper should be accepted unless those are addressed.


[0] https://arxiv.org/abs/1706.04156 (Gradient Descent GAN Optimization is Locally Stable)

[1] https://arxiv.org/pdf/1705.07215.pdf (On Convergence and Stability of GANs)

[2] https://arxiv.org/abs/1612.04357 (Stacked GAN)

[3] https://openreview.net/forum?id=B1QRgziT (Spectral Regularization for GANs)

EDIT: 
As discussed below, I have slightly raised my score. 
I would raise it more if more of my suggestions were implemented (although I'm aware that the authors don't have much (any?) time for this - and that I am partially to blame for that, since I didn't respond that quickly).
I have also slightly raised my confidence.
This is because now I've had more time to think about the paper, and because the authors didn't really address a lot of my criticisms (which to me seems like evidence that some of my criticisms were correct).","[4, 9, 7]","[' Ok but not good enough - rejection', ' Top 15% of accepted papers, strong accept', ' Good paper, accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer expresses serious concerns about both the experimental evaluation and theoretical results of the paper. While they are more confident about the issues in the experimental part, they find the theoretical contributions questionable as well. They provide detailed arguments for their doubts, including references to other works. The reviewer acknowledges the possibility of changing their recommendation if the issues are addressed but remains skeptical. The tone is critical but professional and polite, focusing on the scientific content rather than personal attacks.",-50.0,60.0
SEARNN: Training RNNs with global-local losses,"['Rémi Leblond', 'Jean-Baptiste Alayrac', 'Anton Osokin', 'Simon Lacoste-Julien']",Accept,2018,"[3, 4, 10, 14]","[7, 9, 15, 19]","[16, 60, 47, 140]","[5, 25, 26, 59]","[9, 33, 19, 73]","[2, 2, 2, 8]","This paper extends the concept of global rather than local optimization from the learning to search (L2S) literature to RNNs, specifically in the formation and implementation of SEARNN. Their work takes steps to consider and resolve issues that arise from restricting optimization to only local ground truth choices, which traditionally results in label / transition bias from the teacher forced model.

The underlying issue (MLE training of RNNs) is well founded and referenced, their introduction and extension to the L2S techniques that may help resolve the issue are promising, and their experiments, both small and large, show the efficacy of their technique.

I am also glad to see the exploration of scaling SEARNN to the IWSLT'14 de-en machine translation dataset. As noted by the authors, it is a dataset that has been tackled by related papers and importantly a well scaled dataset. For SEARNN and related techniques to see widespread adoption, the scaling analysis this paper provides is a fundamental component.

This reviewer, whilst not having read all of the appendix in detail, also appreciates the additional insights provided by it, such as including losses that were attempted but did not result in appreciable gains.

Overall I believe this is a paper that tackles an important topic area and provides a novel and persuasive potential solution to many of the issues it highlights.

(extremely minor typo: ""One popular possibility from L2S is go the full reduction route down to binary classification"")","[8, 5, 7]","[' Top 50% of accepted papers, clear accept', ' Marginally below acceptance threshold', ' Good paper, accept']","[4, 5, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']","The review is overwhelmingly positive. The reviewer praises the paper's novelty, the soundness of the approach, the promising results, and even the inclusion of details like scaling analysis and reporting of unsuccessful attempts. The tone is professional and appreciative throughout.",90.0,90.0
Simulated+Unsupervised Learning With Adaptive Data Generation and Bidirectional Mappings,"['Kangwook Lee', 'Hoon Kim', 'Changho Suh']",Accept,2018,"[8, 21, 17]","[13, 26, 22]","[95, 131, 140]","[45, 79, 71]","[37, 1, 41]","[13, 51, 28]","General comment:

This paper proposes a GAN-based method which learns bidirectional mappings between the real-data and the simulated data. The proposed methods builds upon the CycleGAN and the Simulated+Unsupervised (S+U) learning frameworks. The authors show that the proposed method is able to fully leverage the flexibility of simulators by presenting an improved performance on the gaze estimation task.

Detailed comments:

1. The proposed method seems to be a direct combination of the CycleGAN and the S+U learning. Firstly, the CycleGAN propose a to learn a bidirectional GAN model between for image translation. Here the author apply it by ""translating"" the the simulated data to real-data. Moreover, the mapping from simulated data to the real-data is learned, the S+U learning framework propose to train a model on the simulated data.

Hence, this paper seems to directly apply S+U learning to CycleGAN. The properties of the proposed method comes immediately from CycleGAN and S+U learning. Without deeper insights of the proposed method, the novelty of this paper is not sufficient.

2. When discussing CycleGAN, the authors claim that CycleGAN is not good at preserving the labels. However, it is not clear what the meaning of preserving labels is. It would be nice if the authors clearly define this notion and rigorously discuss why CycleGAN is insufficient to reach such a goal and why combining with S+U learning would help.

3. This work seems closely related to S+U learning. It would be nice if the authors also summarize S+U learning in Section 2, in the similar way they summarize CycleGAN in Section 2.2.

4. In Section 2.2, the authors claim that the Cycle-consistency loss in CycleGAN is not sufficient for label preservation. To improve, they propose to use the feature consistency loss. However, the final loss function also contains this cycle-consistency loss. Moreover, in the experiments, the authors indeed use the cycle-consistency loss by setting \lambda_{cyc} = 10. But the feature consistency loss may not be used by setting \lambda_{feature} = 0 or 0.5. From table Two, it appears that whether using the feature-consistency loss does not have significant effect on the performance.

It would be nice to conduct more experiments to show the effect of adding the feature-consistent loss. Say, setting \lambda_{cyc} = 0 and try different values of \lambda_{feature}. Otherwise it is unclear whether the feature-consistent loss is necessary.
","[3, 6, 6]","[' Clear rejection', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer acknowledges the paper's goal and methodology but expresses concerns about novelty and insufficient validation of claims. While the reviewer doesn't use overtly negative language, the criticism is substantial, suggesting a lukewarm reception rather than outright rejection.",-20.0,60.0
Beyond Shared Hierarchies: Deep Multitask Learning through Soft Layer Ordering,"['Elliot Meyerson', 'Risto Miikkulainen']",Accept,2018,"[5, 31]","[10, 36]","[33, 329]","[17, 204]","[15, 50]","[1, 75]","Summary: This paper proposes a different approach to deep multi-task learning using “soft ordering.”  Multi-task learning encourages the sharing of learned representations across tasks, thus using less parameters and tasks help transfer useful knowledge across. Thus enabling the reuse of universally learned representations and reuse them by assembling them in novel ways for new unseen tasks. The idea of “soft ordering” enforces the idea that there shall not be a rigid structure for all the tasks, but a soft structure would make the models more generalizable and modular. 

The methods reviewed prior work which the authors refer to as “parallel order”, which assumed that subsequences of the feature hierarchy align across tasks and sharing between tasks occurs only at aligned depths whereas in this work the authors argue that this shouldn’t be the case. They authors then extend the approach to “permuted order” and finally present their proposed “soft ordering” approach. The authors argue that their proposed soft ordering approach increase the expressivity of the model while preserving the performance. 

The “soft ordering” approach simply enable task specific selection of layers, scaled with a learned scaling factor, to be combined in which order to result for the best performance for each task. The authors evaluate their approach on MNIST, UCI, Omniglot and CelebA datasets and compare their approach to “parallel ordering” and “permuted ordering” and show the performance gain.

Positives: 
- The paper is clearly written and easy to follow
- The idea is novel and impactful if its evaluated properly and consistently 
- The authors did a great job summarizing prior work and motivating their approach

Negatives: 
- Multi-class classification problem is one incarnation of Multi-Task Learning, there are other problems where the tasks are different (classification and localization) or auxiliary (depth detection for navigation). CelebA dataset could have been a good platform for testing different tasks, attribute classification and landmark detection.  
(TODO) I would recommend that the authors test their approach on such setting.
- Figure 6 is a bit confusing, the authors do not explain why the “Permuted Order” performs worse than “Parallel Order”. Their assumptions and results as of this section should be consistent that soft order>permuted order>parallel order>single task. 
 (TODO) I would suggest that the authors follow up on this result, which would be beneficial for the reader.
- Figure 4(a) and 5(b), the results shown on validation loss, how about testing error similar to Figure 6(a)? How about results for CelebA dataset, it could be useful to visualize them as was done for MNIST, Omniglot and UCL. 
(TODO) I would suggest that the authors make the results consistent across all datasets and use the same metric such that its easy to compare.

Notation and Typos:
- Figure 2 is a bit confusing, how come the accuracy decreases with increasing number of training samples? Please clarify.
1- If I assume that the Y-Axis is incorrectly labeled and it is Training Error instead, then the permuted order is doing worse than the parallel order.
 2- If I assume that the X-Axis is incorrectly labeled and the numbering is reversed (start from max and ending at 0), then I think it would make sense.
- Figure 4 is very small and not easy to read the text. Does single task mean average performance over the tasks? 
- In eq.(3) Choosing \sigma_i for a task-specific permutation of the network is a bit confusing, since it could be thought of as a sigmoid function, I suggest using a different symbol.
 Conclusion: I would suggest that the authors address the concerns mentioned above. Their approach and idea is very interesting and relevant, and addressing these suggestions will make the paper strong for publication.","[7, 7, 6]","[' Good paper, accept', ' Good paper, accept', ' Marginally above acceptance threshold']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review is generally positive, acknowledging the novelty and clarity of the paper. The reviewer finds the idea impactful and praises the authors for summarizing prior work well. However, they also raise valid concerns and suggest specific improvements, indicating a desire to see the paper strengthened rather than rejected. The language is constructive and polite, using phrases like ""I would recommend"" and ""I would suggest"" to offer advice.",60.0,80.0
Towards better understanding of gradient-based attribution methods for Deep Neural Networks,"['Marco Ancona', 'Enea Ceolini', 'Cengiz Öztireli', 'Markus Gross']",Accept,2018,"[2, 3, 11, 30]","[5, 8, 16, 35]","[6, 25, 60, 471]","[2, 16, 20, 228]","[3, 5, 19, 25]","[1, 4, 21, 218]","This paper discusses several gradient based attribution methods, which have been popular for the fast computation of saliency maps for interpreting deep neural networks. The paper provides several advances:
- \epsilon-LRP and DeepLIFT are formulated in a way that can be calculated using the same back-propagation as training.
- This gives a more unified way of understanding, and implementing the methods.
- The paper points out situations when the methods are equivalent
- The paper analyses the methods' sensitivity to identifying single and joint regions of sensitivity
- The paper proposes a new objective function to measure joint sensitivity

Overall, I believe this paper to be a useful contribution to the literature. It both solidifies understanding of existing methods and provides new insight into quantitate ways of analysing methods. Especially the latter will be appreciated.","[7, 7, 6]","[' Good paper, accept', ' Good paper, accept', ' Marginally above acceptance threshold']","[3, 4, 5]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review starts by listing the strengths of the paper, using positive language such as ""useful contribution"" and ""new insight"". There are no negative statements or harsh criticisms. The reviewer clearly sees value in the paper's contributions.",85.0,100.0
Unsupervised Machine Translation Using Monolingual Corpora Only,"['Guillaume Lample', 'Alexis Conneau', 'Ludovic Denoyer', ""Marc'Aurelio Ranzato""]",Accept,2018,"[4, 3, 16, 13]","[9, 8, 20, 18]","[51, 56, 147, 106]","[24, 27, 88, 57]","[27, 29, 40, 44]","[0, 0, 19, 5]","This paper introduces an architecture for training a MT model without any parallel material, and tests it on benchmark datasets (WMT and captions) for two language pairs. Although the resulting performance is only about half that of a more traditional model, the fact that this is possible at all is remarkable.

The method relies on fairly standard components which will be familiar to most readers: a denoising auto-encoder and an adversarial discriminator. Not much detail is given on the actual models used, for which the authors mainly refer to prior work. This is disappointing: the article would be more self-contained by providing even a high-level description of the models, such as provided (much too late) for the discriminator architecture.

Misc comments:

""domain"" seems to be used interchangeably with ""language"". This is unfortunate as ""domain"" has another, specific meaning in NLP in general and SMT in partiular. Is this intentional (if so what is the intention?) or is this just a carry-over from other work in cross-domain learning?

Section 2.3: How do you sample permutations for the noise model, with the constraint on reordering range, in the general case of sentences of arbitrary lengths?

Section 2.5: ""the previously introduced loss [...] mitigates this concern"" -- How? Is there a reference backing this?

Figure 3: In the caption, what is meant by ""(t) = 1""? Are these epochs only for the first iteration (from M(1) to M(2))?

Section 4.1: Care is taken to avoid sampling corresponding src and tgt sentences. However, was the parallel corpus checked for duplicates or near duplicates? If not, ""aligned"" segments may still be present. (Although it is clear that this information is not used in the algorithm)

This yields a natural question: Although the two monolingual sets extracted from the parallel data are not aligned, they are still very close. It would be interesting to check how the method behaves on really comparable corpora where its advantage would be much clearer.

Section 4.2 and Table 1: Is the supervised learning approach trained on the full parallel corpus? On a parallel corpus of similar size?

Section 4.3: What are the quoted accuracies (84.48% and 77.29%) measured on?

Section 4.5: Experimental results show a regular inprovement from iteration 1 to 2, and 2 to 3. Why not keep improving performance? Is the issue training time?

References: (He, 2016a/b) are duplicates

Response read -- thanks.","[7, 7, 8]","[' Good paper, accept', ' Good paper, accept', ' Top 50% of accepted papers, clear accept']","[4, 5, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review acknowledges the paper's novelty and significance (""the fact that this is possible at all is remarkable"") which suggests a positive sentiment. However, it also points out a lack of detail and clarity in the methodology and presentation. The reviewer poses several specific questions and requests for clarification, indicating areas for improvement. Overall, the tone is constructive and professional, focusing on improving the paper rather than harshly criticizing it. Therefore, the sentiment is closer to positive than neutral. The language used is polite and professional throughout, with no instances of rudeness or disrespect.",60.0,80.0
NerveNet: Learning Structured Policy with Graph Neural Networks,"['Tingwu Wang', 'Renjie Liao', 'Jimmy Ba', 'Sanja Fidler']",Accept,2018,"[3, 9, 8, 16]","[8, 14, 13, 21]","[16, 99, 96, 298]","[7, 52, 46, 155]","[9, 43, 49, 133]","[0, 4, 1, 10]","This paper proposes NerveNet to represent and learn structured policy for continuous control tasks. Instead of using the widely adopted fully connected MLP, this paper uses Graph Neural Networks to learn a structured controller for various MuJoco environments. It shows that this structured controller can be easily transferred to different tasks or dramatically speed up the fine-tuning of transfer.

The idea to build structured policy is novel for continuous control tasks. It is an exciting direction since there are inherent structures that should be exploited in many control tasks, especially for locomotion. This paper explores this less-studied area and demonstrates promising results.

The presentation is mostly clear. Here are some questions and a list of minor suggestions:
1) In the Output Model section, I am not sure how the controller is shared. It first says that ""Nodes with the same node type should share the instance of MLP"", which means all the ""joint"" nodes should share the same controller. But later it says ""Two LeftHip should have a shared controller."" What about RightHip? or Ankle? They all belongs to the same node type ""joint"". Am I missing something here? It seems that in this paper, weights sharing is an essential part of the structured policy, it would be great if it can be described in more details.

2) In States Update of Propagation Model Section, it is not clear how the aggregated message is used in eq. (4).

3) Typo in Caption of Table 1: CentipedeFour not CentipedeSix.

4) If we just use MLP but share weights among joints (e.g. the weights from observation to action of all the LeftHips are constrained to be same), how would it compare to the method proposed in this paper?

In summary, I think that it is worthwhile to develop structured representation of policies for control tasks. It is analogue to use CNN that share weights between kernels for computer vision tasks. I believe that this paper could inspire many follow-up work. For this reason, I would recommend accepting this paper.","[7, 6, 7]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Good paper, accept']","[3, 3, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review is positive. The reviewer states ""this paper explores this less-studied area and demonstrates promising results"" and ""I believe that this paper could inspire many follow-up work. For this reason, I would recommend accepting this paper."" The reviewer provides constructive criticism and suggestions for improvement, but the overall tone is encouraging and supportive.",75.0,90.0
Leave no Trace: Learning to Reset for Safe and Autonomous Reinforcement Learning,"['Benjamin Eysenbach', 'Shixiang Gu', 'Julian Ibarz', 'Sergey Levine']",Accept,2018,"[3, 7, 8, 10]","[8, 12, 13, 15]","[64, 89, 45, 743]","[27, 38, 17, 326]","[36, 49, 24, 396]","[1, 2, 4, 21]","This paper proposes the idea of having an agent learning a policy that resets the agent's state to one of the states drawn from the distribution of starting states. The agent learns such policy while also learning how to solve the actual task. This approach generates more autonomous agents that require fewer human interventions in the learning process. This is a very elegant and general idea, where the value function learned in the reset task also encodes some measure of safety in the environment.

All that being said, I gave this paper a score of 6 because two aspects that seem fundamental to me are not clear in the paper. If clarified, I'd happily increase my score.

1) *Defining state visitation/equality in the function approximation setting:* The main idea behind the proposed algorithm is to ensure that ""when the reset policy is executed from any state, the distribution over final states matches the initial state distribution p_0"". This is formally described, for example, in line 13 of Algorithm 1.
The authors ""define a set of safe states S_{reset} \subseteq S, and say that we are in an irreversible state if the set of states visited by the reset policy over the past N episodes is disjoint from S_{reset}."" However, it is not clear to me how one can uniquely identify a state in the function approximation case. Obviously, it is straightforward to apply such definition in the tabular case, where counting state visitation is easy. However, how do we count state visitation in continuous domains? Did the authors manually define the range of each joint/torque/angle that characterizes the start state? In a control task from pixels, for example, would the exact configuration of pixels seen at the beginning be the start state? Defining state visitation in the function approximation setting is not trivial and it seems to me the authors just glossed over it, despite being essential to your work.

2) *Experimental design for Figure 5*: This setup is not clear to me at all and in fact, my first reaction is to say it is wrong. An episodic task is generally defined as: the agent starts in a state drawn from the distribution of starting states and at the moment it reaches the goal state, the task is reset and the agent starts again. It doesn't seem to be what the authors did, is that right? The sentence: ""our method learns to solve this task by automatically resetting the environment after each episode, so the forward policy can practice catching the ball when initialized below the cup"" is confusion. When is the task reset to the ""status quo"" approach? Also, let's say an agent takes 50 time steps to reach the goal and then it decides to do a soft-reset. Are the time steps it is spending on its soft-reset being taken into account when generating the reported results?


Some other minor points are:

- The authors should standardize their use of citations in the paper. Sometimes there are way too many parentheses in a reference. For example: ""manual resets are necessary when the robot or environment breaks (e.g. Gandhi et al. (2017))"", or ""Our methods can also be used directly with any other Q-learning methods ((Watkins & Dayan, 1992; Mnih et al., 2013; Gu et al., 2017; Amos et al., 2016; Metz et al., 2017))""

- There is a whole line of work in safe RL that is not acknowledged in the related work section. Representative papers are:
    [1] Philip S. Thomas, Georgios Theocharous, Mohammad Ghavamzadeh: High-Confidence Off-Policy Evaluation. AAAI 2015: 3000-3006
    [2] Philip S. Thomas, Georgios Theocharous, Mohammad Ghavamzadeh: High Confidence Policy Improvement. ICML 2015: 2380-2388

- In the Preliminaries Section the next state is said to be drawn from s_{t+1} ~ P(s'| s, a). However, this hides the fact the next state is dependent on the environment dynamics and on the policy being followed. I think it would be clearer if written: s_{t+1} ~ P(s'| s, \pi(a|s)).

- It seems to me that, in Algorithm 1, the name 'Act' is misleading. Shouldn't it be 'ChooseAction' or 'EpsilonGreedy'? If I understand correctly, the function 'Act' just returns the action to be executed, while the function 'Step' is the one that actually executes the action.

- It is absolutely essential to depict the confidence intervals in the plots in Figure 3. Ideally we should have confidence intervals in all the plots in the paper.","[7, 6, 5, 7]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Marginally below acceptance threshold', ' Good paper, accept']","[4, 5, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer provides both positive and negative feedback. They find the idea elegant and general (positive), but they also raise two fundamental concerns regarding state visitation in function approximation and the experimental design (negative). The reviewer is critical but remains professional and offers suggestions for improvement. The score of 6 suggests a leaning towards the positive side, but with significant reservations.",20.0,60.0
Neural Language Modeling by Jointly Learning Syntax and Lexicon,"['Yikang Shen', 'Zhouhan Lin', 'Chin-wei Huang', 'Aaron Courville']",Accept,2018,"[5, 7, 10, 18]","[10, 12, 15, 23]","[49, 59, 39, 309]","[23, 24, 14, 135]","[25, 31, 19, 160]","[1, 4, 6, 14]","** UPDATE ** upgraded my score to 7 based on the new version of the paper.

The main contribution of this paper is to introduce a new recurrent neural network for language modeling, which incorporates a tree structure More precisely, the model learns constituency trees (without any supervision), to capture syntactic information. This information is then used to define skip connections in the language model, to capture longer dependencies between words. The update of the hidden state does not depend only on the previous hidden state, but also on the hidden states corresponding to the following words: all the previous words belonging to the smallest subtree containing the current word, such that the current word is not the left-most one. The authors propose to parametrize trees using ""syntactic distances"" between adjacent words (a scalar value for each pair of adjacent words w_t, w_{t+1}). Given these distances, it is possible to obtain the constituents and the corresponding gating activations for the skip connections. These different operations can be relaxed to differentiable operations, so that stochastic gradient descent can be used to learn the parameters. The model is evaluated on three language modeling benchmarks: character level PTB, word level PTB and word level text8. The induced constituency trees are also evaluated, for sentence of length 10 or less (which is the standard setting for unsupervised parsing).

Overall, I really like the main idea of the paper. The use of ""syntactic distances"" to parametrize the trees is clever, as they can easily be computed using only partial information up to time t. From these distances, it is also relatively straightforward to obtain which constituents (or subtrees) a word belongs to (and thus, the corresponding gating activations). Moreover, the operations can easily be relaxed to obtain a differentiable model, which can easily be trained using stochastic gradient descent.

The results reported on the language modeling experiments are strong. One minor comment here is that it would be nice to have an ablation analysis, as it is possible to obtain similarly strong results with simpler models (such as plain LSTM).

My main concern regarding the paper is that it is a bit hard to understand. In particular in section 4, the authors alternates between discrete and relaxed values: end of section 4.1, it is implied that alpha are in [0, 1], but in equation 6, alpha are in {0, 1}, then relaxed in equation 9 to [0, 1] again. I am also wondering whether it would make more sense to start by introducing the syntactic distances, then the alphas and finally the gates? I also found the section 5 to be quite confusing. While I get the	general idea, I am not sure what is the relation between hidden states h and m (section 5.1). Is there a mixup between h defined in equation 10 and h from section 5.1? I am aware that it is not straightforward to describe the proposed method, but believe it would be a much stronger paper if written more clearly.

To conclude, I really like the method proposed in this paper, and believe that the experimental results are quite strong.
My main concern	regarding the paper is its clarity: I will gladly increase my score if the authors can improve the writing.","[7, 8, 7]","[' Good paper, accept', ' Top 50% of accepted papers, clear accept', ' Good paper, accept']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer explicitly states a positive sentiment towards the paper's core idea, finding it clever and appreciating the strong experimental results. They also offer to increase their score based on clarity improvements, indicating an overall positive disposition. However, the reviewer does point out concerns regarding the paper's clarity, particularly in sections 4 and 5. They find the presentation confusing and suggest a more logical flow of information. The language used throughout is constructive and professional, focusing on specific areas for improvement.",60.0,80.0
Adaptive Dropout with Rademacher Complexity Regularization,"['Ke Zhai', 'Huan Wang']",Accept,2018,"[8, 19]","[8, 24]","[10, 345]","[8, 147]","[1, 49]","[1, 149]","This paper studies the adjustment of dropout rates which is a useful tool to prevent the overfitting of deep neural networks. The authors derive a generalization error bound in terms of dropout rates. Based on this, the authors propose a regularization framework to adaptively select dropout rates. Experimental results are also given to verify the theory.

Major comments:
(1) The Empirical Rademacher complexity is not defined. For completeness, it would be better to define it at least in the appendix.
(2) I can not follow the inequality (5). Especially, according to the main text, f^L is a vector-valued function . Therefore, it is not clear to me the meaning of \sum\sigma_if^L(x_i,w) in (5).
(3) I can also not see clearly the third equality in (9). Note that f^l is a vector-valued function. It is not clear to me how it is related to a summation over j there.
(4) There is a linear dependency on the number of classes in Theorem 3.1. Is it possible to further improve this dependency?

Minor comments:
(1) Section 4: 1e-3,1e-4,1e-5 is not consistent with 1e^{-3}, 1e^{-4},1e^{-5}
(2) Abstract: there should be a space before ""Experiments"".
(3) It would be better to give more details (e.g., page, section) in citing a book in the proof of Theorem 3.1

Summary:
The mathematical analysis in the present version is not rigorous. The authors should improve the mathematical analysis.

----------------------------
After Rebuttal:
Thank you for revising the paper. I think there are still some possible problems. 
Let us consider eq (12) in the appendix on the contraction property of Rademacher complexity (RC).
(1) Since you consider a variant of RC with absolute value inside the supermum, to my best knowledge, the contraction property (12) should involve an additional factor of 2, see, e.g., Theorem 12 of ""Rademacher and Gaussian Complexities: Risk Bounds and Structural Results"" by Bartlett and Mendelson. Since you need to apply this contraction property L times, there should be a factor of 2^L in the error bound. This make the bound not appealing for neural networks with a moderate L.
(2) Second, the function g involves an expectation w.r.t. r before the activation function. I am not sure whether this existence of expectation w.r.t. r would make the contraction property applicable in this case.","[6, 7, 6]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Marginally above acceptance threshold']","[3, 5, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']","The review is critical of the paper's mathematical rigor, pointing out several issues with the proofs and derivations. While the reviewer acknowledges the paper's potential and the authors' efforts in addressing some concerns, they still find problems, particularly with the application of the contraction property. The language, while direct and critical, maintains a professional and objective tone. There's no personal criticism or disrespectful language used.",-20.0,60.0
Implicit Causal Models for Genome-wide Association Studies,"['Dustin Tran', 'David M. Blei']",Accept,2018,"[4, 18]","[9, 23]","[76, 259]","[31, 140]","[40, 88]","[5, 31]","This paper tackles two problems common in genome-wide association studies: confounding (i.e. structured noise) due to population structure and the potential presence of non-linear interactions between different parts of the genome. To solve the first problem this paper effectively suggests learning the latent confounders jointly with the rest of the model. For the second problem, this paper proposes “implicit causal models’, that is, models that  leverage neural architectures with an implicit density. 

The main contribution of this paper is to create a bridge between the statistical genetics community and the ML community. The method is technically sound and does indeed generalize techniques currently used in statistical genetics. The main concerns with this paper is that 1) the claim that it can detect epistatic interactions is not really supported. Yes, in principle the neural model used to model y could detect them, but no experiments are shown to really tease this case apart 2) validating GWAS results is really hard, because no causal information is usually available. The authors did a great job on the simulation framework, but table 1 falls short in terms of evaluation metric: to properly assess the performance of the method on simulated data, it would be good to have evidence that the type 1 error is calibrated (e.g. by means of qq plots vs null distribution) for all methods. At the very least, a ROC curve could be used to show the quality of the ranking of the causal SNPs for each method, irrespective of p-value cutoff.

Quality: see above. The technical parts of this paper are definitely high-quality, the experimental side could be improved.
Clarity: if the target audience of this paper is the probabilistic ML community, it’s very clear. If the statistical genetics community is expected to read this, section 3.1 could result too difficult to parse. As an aside: ICLR might be the right venue for this paper given the high ML content, but perhaps a bioinformatics journal would be a better fit, depending on intended audience.
","[6, 6, 5]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally below acceptance threshold']","[5, 5, 5]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review starts with positive remarks, highlighting the paper's contribution and technical soundness. However, it then raises significant concerns about the experimental validation and suggests improvements. The language is constructive and professional throughout. Therefore, the sentiment is positive but with reservations, reflected in a score closer to neutral than extremely positive. The politeness is consistently high.",50.0,80.0
Training GANs with Optimism,"['Constantinos Daskalakis', 'Andrew Ilyas', 'Vasilis Syrgkanis', 'Haoyang Zeng']",Accept,2018,"[15, 5, 10, 3]","[20, 10, 15, 6]","[243, 48, 158, 9]","[115, 21, 70, 2]","[103, 27, 78, 1]","[25, 0, 10, 6]","This paper proposes the use of optimistic mirror descent to train Wasserstein Generative Adversarial Networks (WGANS). The authors remark that the current training of GANs, which amounts to solving a zero-sum game between a generator and discriminator, is often unstable, and they argue that one source of instability is due to limit cycles, which can occur for FTRL-based algorithms even in convex-concave zero-sum games. Motivated by recent results that use Optimistic Mirror Descent  (OMD) to achieve faster convergence rates (than standard gradient descent) in convex-concave zero-sum games and normal form games, they suggest using these techniques for WGAN training as well. The authors prove that, using OMD, the last iterate converges to an equilibrium and use this as motivation that OMD methods should be more stable for WGAN training. They then compare OMD against GD on both toy simulations and a DNA sequence task before finally introducing an adaptive generalization of OMD, Optimistic Adam, that they test on CIFAR10. 

This paper is relatively well-written and clear, and the authors do a good job of introducing the problem of GAN training instability as well as the OMD algorithm, in particular highlighting its differences with standard gradient descent as well as discussing existing work that has applied it to zero-sum games. Given the recent work on OMD for zero-sum and normal form games, it is natural to study its effectiveness in training GANs.The issue of last iterate versus average iterate for non convex-concave problems is also presented well.  

The theoretical result on last-iterate convergence of OMD for bilinear games is interesting, but somewhat wanting as it does not provide an explicit convergence rate as in Rakhlin and Sridharan, 2013. Moreover, the result is only at best a motivation for using OMD in WGAN training since the WGAN optimization problem is not a bilinear game. 

The experimental results seem to indicate that OMD is at least roughly competitive with GD-based methods, although they seem less compelling than the prior discussion in the paper would suggest. In particular, they are matched by SGD with momentum when evaluated by last epoch performance (albeit while being less sensitive to learning rates). OMD does seem to outperform SGD-based methods when using the lowest discriminator loss, but there doesn't seem to be even an attempt at explaining this in the paper. 

I found it a bit odd that Adam was not used as a point of comparison in Section 5, that optimistic Adam was only introduced and tested for CIFAR but not for the DNA sequence problem, and that the discriminator was trained for 5 iterations in Section 5 but only once in Section 6, despite the fact that the reasoning provided in Section 6 seems like it would have also applied for Section 5. This gives the impression that the experimental results might have been at least slightly ""gamed"". 

For the reasons above, I give the paper high marks on clarity, and slightly above average marks on originality, significance, and quality.

Specific comments:
Page 1, ""no-regret dynamics in zero-sum games can very often lead to limit cycles"": I don't think limit cycles are actually ever formally defined in the entire paper.  
Page 3, ""standard results in game theory and no-regret learning"": These results should be either proven or cited.
Page 3: Don't the parameter spaces need to be bounded for these convergence results to hold? 
Page 4, ""it is well known that GD is equivalent to the Follow-the-Regularized-Leader algorithm"": For completeness, this should probably either be (quickly) proven or a reference should be provided.
Page 5, ""the unique equilibrium of the above game is...for the discriminator to choose w=0"": Why is w=0 necessary here?
Page 6, ""We remark that the set of equilibrium solutions of this minimax problem are pairs (x,y) such that x is in the null space of A^T and y is in the null space of A"": Why is this true? This should either be proven or cited.
Page 6, Initialization and Theorem 1: It would be good to discuss the necessity of this particular choice of initialization for the theoretical result. In the Initialization section, it appears simply to be out of convenience.
Page 6, Theorem 1: It should be explicitly stated that this result doesn't provide a convergence rate, in contrast to the existing OMD results cited in the paper.   
Page 7, ""we considered momentum, Nesterov momentum and AdaGrad"": Why isn't Adam used in this section if it is used in  later experiments?
Page 7-8, ""When evaluated by....the lowest discriminator loss on the validation set, WGAN trained with Stochastic OMD (SOMD) achieved significantly lower KL divergence than the competing SGD variants."": Can you explain why SOMD outperforms the other methods when using the lowest discriminator loss on the validation set? None of the theoretical arguments presented earlier in the paper seem to even hint at this. The only result that one might expect from the earlier discussion and results is that SOMD would outperform the other methods when evaluating by the last epoch. However, this doesn't even really hold, since there exist learning rates in which SGD with momentum matches the performance of SOMD.
Page 8, ""Evaluated by the last epoch, SOMD is much less sensitive to the choice of learning rate than the SGD variants"": Learning rate sensitivity doesn't seem to be touched upon in the earlier discussion. Can these results be explained by theory?
Page 8, ""we see that optimistic Adam achieves high numbers of inception scores after very few epochs of training"": These results don't mean much without error bars.
Page 8, ""we only trained the discriminator once after one iteration of generator training. The latter is inline with the intuition behind the use of optimism...."": Why didn't this logic apply to the previous section on DNA sequences, where the discriminator was trained multiple times?


After reading the response of the authors (in particular their clarification of some technical results and the extra experiments they carried out during the rebuttal period), I have decided to upgrade my rating of the paper from a 6 to a 7. Just as a note, Figure 3b is now very difficult to read. 

","[7, 6, 8]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Top 50% of accepted papers, clear accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is generally positive, highlighting the paper's clarity and originality. The reviewer finds the theoretical results interesting and the proposed method a natural extension of prior work. However, they express some concerns about the experimental validation, finding them less compelling than the theoretical motivation might suggest and pointing out potential inconsistencies in the experimental setup. The reviewer's decision to upgrade the rating after the authors' rebuttal further indicates a positive sentiment. The language used throughout the review is polite and professional, employing constructive criticism and avoiding harsh language.",60.0,80.0
Latent Space Oddity: on the Curvature of Deep Generative Models,"['Georgios Arvanitidis', 'Lars Kai Hansen', 'Søren Hauberg']",Accept,2018,"[7, 29, 11]","[11, 34, 16]","[21, 227, 100]","[11, 127, 48]","[10, 31, 40]","[0, 69, 12]","The paper makes an important observation: the generating function of a generative model (deep or not) induces a (stochastic) Riemannian metric tensor on the latent space. This metric might be the correct way to measure distances in the latent space, as opposed to the Euclidean distance.

While this seems obvious, I had actually always thought of the latent space as ""unfolding"" the data manifold as it exists in the output space. The authors propose a different view which is intriguing; however, they do not, to the best of my understand, give a definitive theoretical reason why the induced Riemannian metric is the correct choice over the Euclidean metric.

The paper correctly identifies an important problem with the way most deep generative models evaluate variance. However the solution proposed seems ad-hoc and not particularly related to the other parts of the paper. While the proposed variance estimation (using RBF networks) might work in some cases, I would love to see (perhaps in future work) a much more rigorous treatment of the subject.

Pros:
1. Interesting observation and mathematical development of a Riemannian metric on the latent space.

2. Good observation about the different roles of the mean and the variance in determining the geodesics: they tend to avoid areas of high variance.

3. Intriguing experiments and a good effort at visualizing and explaining them. I especially appreciate the interpolation and random walk experiments. These are hard to evaluate objectively, but the results to hint at the phenomena the authors describe when comparing Euclidean to Riemannian metrics in the latent space.

Cons:
1. The part of the paper proposing new variance estimators is ad-hoc and is not experimented with rigorously, comparing it to other methods in terms of calibration for example. 

Specific comments:
1. To the best of my understanding eq. (2) does not imply that the natural distance in Z is locally adaptive. I think of eq (2) as *defining* a type of distance on Z, that may or may not be natural. One could equally argue that the Euclidean distance on z is natural, and that this distance is then pushed forward by f to some induced distance over X. 

2. In the definition of paths \gamma, shouldn't they be parametrized by arc-length (also known as unit-speed)? How should we think of the curve \gamma(t^2) for example?

3. In Theorem 2, is the term ""input dimension"" appropriate? Perhaps ""data dimension"" is better?

4. I did not fully understand the role of the LAND model. Is this a model fit AFTER fitting the generative model, and is used to cluster Z like a GMM ? I would appreciate a clarification about the context of this model.","[7, 3, 7]","[' Good paper, accept', ' Clear rejection', ' Good paper, accept']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review acknowledges the importance of the paper's observation and its potential implications. While it praises the mathematical development, experimental efforts, and visualizations, it also points out a lack of theoretical justification for the core proposition and criticizes the variance estimation approach as ad-hoc. The reviewer also poses several questions and requests clarifications, indicating a need for further elaboration and refinement. Overall, the feedback is constructive and suggestive of potential improvements, leaning towards the positive side due to the acknowledged significance of the work.",60.0,80.0
Learning Awareness Models,"['Brandon Amos', 'Laurent Dinh', 'Serkan Cabi', 'Thomas Rothörl', 'Sergio Gómez Colmenarejo', 'Alistair Muldal', 'Tom Erez', 'Yuval Tassa', 'Nando de Freitas', 'Misha Denil']",Accept,2018,"[3, 3, 32, 30]","[7, 8, 37, 35]","[19, 25, 254, 243]","[9, 12, 115, 136]","[10, 13, 77, 83]","[0, 0, 62, 24]","The authors explore how sequence models that look at proprioceptive signals from a simulated or real-world robotic hand can be used to decode properties of objects (which are not directly observed), or produce entropy maximizing or minimizing motions.

The overall idea presented in the paper is quite nice: proprioception-based models that inject actions and encoder/pressure observations can be used to measure physical properties of objects that are not directly observed, and can also be used to create information gathering (or avoiding) behaviors. There is some related work that the authors do not cite that is highly relevant here. A few in particular come to mind:

Yu, Tan, Liu, Turk. Preparing for the Unknown: uses a sequence model to estimate physical properties of a robot (rather than unobserved objects)

Fu, Levine, Abbeel. One-Shot Learning of Manipulation Skills: trains a similar proprioception-only model and uses it for object manipulation, similar idea that object properties can be induced from proprioception

But in general the citations to relevant robotic manipulation work are pretty sparse.

The biggest issue with the paper though is with the results. There are no comparisons or reasonable baselines of any kind, and the reported results are a bit hard to judge. As far as I can understand, there are no quantitative results in simulation at all, and the real-world results are not good, indicating something like 15 degrees of error in predicting the pose of a single object. That doesn't seem especially good, though it's also very hard to tell without a baseline.

Overall, this seems like a good workshop paper, but probably substantial additional experimental work is needed in order to evaluate the practical usefulness of this method. I would however strongly encourage the authors to pursue this research further: it seems very promising, and I think that, with more rigorous evaluation and comparisons, it could be quite a nice paper!

One point about style: I found the somewhat lofty claims in the introduction a bit off-putting. It's great to discuss the greater ""vision"" behind the work, but this paper suffers from a bit too much high-level vision and not enough effort put into explaining what the method actually does.","[4, 7, 4]","[' Ok but not good enough - rejection', ' Good paper, accept', ' Ok but not good enough - rejection']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer finds the core idea interesting and promising (""quite nice"", ""very promising"") but has significant concerns about the lack of quantitative results and baselines. They find the real-world results unconvincing without a proper comparison. The reviewer acknowledges the work's potential but suggests it's more suitable for a workshop due to the need for further evaluation. The slightly negative sentiment stems from the lack of strong results and the over-emphasis on ""vision"" rather than concrete methodology. However, the reviewer encourages further development, indicating a belief that the work could become impactful. The language is polite, offering constructive criticism and avoiding harsh or disrespectful phrasing.",20.0,80.0
Monotonic Chunkwise Attention,"['Chung-Cheng Chiu*', 'Colin Raffel*']",Accept,2018,"[15, 9]","[20, 14]","[116, 112]","[59, 49]","[41, 58]","[16, 5]","The paper proposes an extension to a previous monotonic attention model (Raffel et al 2017) to attend to a fixed-sized window up to the alignment position. Both the soft attention approximation used for training the monotonic attention model, and the online decoding algorithm is extended to the chunkwise model. In terms of the model this is a relatively small extention of Raffel et al 2017.

Results show that for online speech recognition the model matches the performance of an offline soft attention baseline, doing significantly better than the monotonic attention model. Is the offline attention baseline unidirectional or bidirectional? In case it is unidirectional it cannot really be claimed that the proposed model's performance is competitive with an offline model.

My concern with the statement that all hyper-parameters are kept the same as the monotonic model is that the improvement might partly be due to the increase in total number of parameters in the model. Especially given that w=2 works best for speech recognition, it not clear that the model extension is actually helping. My other concern is that in speech recognition the time-scale of the encoding is somewhat arbitrary, so possibly a similar effect could be obtained by doubling the time frame through the convolutional layer. While the empirical result is strong it is not clear that the proposed model is the best way to obtain the improvement.

For document summarization the paper presents a strong result for an online model, but the fact that it is still less accurate than the soft attention baseline makes it hard to see the real significance of this. If the contribution is in terms of speed (as shown with the synthetic benchmark in appendix B) more emphesis should be placed on this in the paper. 
Sentence summarization tasks do exhibit mostly monotonic alignment, and most previous models with monotonic structure were evaluated on that, so why not test that here?

I like the fact that the model is truely online, but that contribution was made by Raffel et al 2017, and this paper at best proposes a slightly better way to train and apply that model.

---
 The additional experiments in the new version gives stronger support in favour of the proposed model architecture (vs the effect of hyperparameter choices). While I'm still on the fence on whether this paper is strong enough to be accepted for ICLR, this version is certainly improves the quality of the paper. 
","[6, 8, 7]","[' Marginally above acceptance threshold', ' Top 50% of accepted papers, clear accept', ' Good paper, accept']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review starts with a neutral statement of the paper's proposal. However, it raises several concerns about the methodology and significance of the results, suggesting the reviewer is not fully convinced. While acknowledging some strengths (strong empirical results, truly online nature), the reviewer ultimately questions the novelty and impact of the work, especially in light of the previous work by Raffel et al. 2017. The updated version seems to have addressed some concerns, but the reviewer is still not fully positive. The language used is professional and not overly critical, focusing on technical aspects and suggesting improvements.",20.0,60.0
"Emergent Communication in a Multi-Modal, Multi-Step Referential Game","['Katrina Evtimova', 'Andrew Drozdov', 'Douwe Kiela', 'Kyunghyun Cho']",Accept,2018,"[2, 2, 6, 9]","[6, 7, 11, 14]","[4, 17, 158, 396]","[1, 8, 82, 154]","[2, 8, 72, 215]","[1, 1, 4, 27]","The paper proposes a new multi-modal, multi-step reference game, where the sender has access to visual data and the receiver has access to textual messages, and also the conversation can be terminated by the receiver when proper. 

Later, the paper describes their idea and extension in details and reports comprehensive experiment results of a number of hypotheses. The research questions seems straightforward, but it is good to see those experiments review some interesting points.  One thing I am bit concerned is that the results are based on a single dataset. Do we have other datasets that can be used?

The authors also lay out further several research directions. Overall, I think this paper is easy to read and good. 

","[7, 7, 7]","[' Good paper, accept', ' Good paper, accept', ' Good paper, accept']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer provides a generally positive assessment, using phrases like ""good"", ""easy to read"", and ""interesting points"". They express minor concerns and suggestions for improvement, but the overall tone is constructive and encouraging. ",65.0,80.0
Attacking Binarized Neural Networks,"['Angus Galloway', 'Graham W. Taylor', 'Medhat Moussa']",Accept,2018,"[2, 15, 24]","[7, 20, 27]","[11, 174, 50]","[2, 78, 26]","[8, 77, 8]","[1, 19, 16]","1) Summary
This paper proposes a study on the robustness of one low-precision neural networks class - binarized neural networks (BNN) - against adversarial attacks. Specifically, the authors show that these low precision networks are not just efficient in terms of memory consumption and forward computation, but also more immune to adversarial attacks than their high-precision counterparts. In experiments, they show the advantage of BNNs by conducting experiments based on black-box and white-box adversarial attacks without the need to artificially mask gradients.


2) Pros:
+ Introduced, studied, and supported the novel idea that BNNs are robust to adversarial attacks.
+ Showed that BNNs are robust to the Fast Gradient Sign Method (FGSM) and Carlini-Wagner attacks in white-box adversarial attacks by presenting evidence that BNNs either outperform or perform similar to the high-precision baseline against the attacks.
+ Insightful analysis and discussion of the advantages of using BNNs against adversarial attacks.

3) Cons:
Missing full-precision model trained with PGD in section 3.2:
The authors mention that the full-precision model would also likely improve with PGD training, but do not have the numbers. It would be useful to have such numbers to make a better evaluation of the BNN performance in the black-box attack setting.


Additional comments:
Can the authors provide additional analysis on why BNNs perform worse than full-precision networks against black-box adversarial attacks? This could be insightful information that this paper could provide if possible.


4) Conclusion:
Overall, this paper proposes great insightful information about BNNs that shows the additional benefit of using them besides less memory consumption and efficient computation. This paper shows that the used architecture for BBNs makes them less susceptible to known white-box adversarial attack techniques.
","[7, 6, 7]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Good paper, accept']","[4, 5, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']","The review is positive about the paper's findings, highlighting the novelty and insightful analysis. While it points out a missing experiment and requests further analysis, these are framed as suggestions for improvement rather than harsh criticisms. The language is constructive and professional throughout.",75.0,80.0
The Kanerva Machine: A Generative Distributed Memory,"['Yan Wu', 'Greg Wayne', 'Alex Graves', 'Timothy Lillicrap']",Accept,2018,"[1, 6, 15, 11]","[6, 10, 18, 16]","[19, 55, 77, 127]","[7, 17, 42, 47]","[11, 33, 27, 70]","[1, 5, 8, 10]","The paper presents the Kanerva Machine, extending an interesting older conceptual memory model to modern usage. The review of Kanerva’s sparse distributed memory in the appendix was appreciated. While the analyses and bounds of the original work were only proven when restricted to uniform and binary data, the extensions proposed bring it to modern domain of non-uniform and floating point data.

The iterative reading mechanism which provides denoising and reconstruction when within tolerable error bounds, whilst no longer analytically provable, is well shown experimentally.
The experiments and results on Omniglot and CIFAR provide an interesting insight to the model's behaviour with the comparisons to VAE and DNC also seem well constructed.

The discussions regarding efficiency and potential optimizations of writing inference model were also interesting and indeed the low rank approximation of U seems an interesting future direction.

Overall I found the paper well written and reintroduced + reframed a relatively underutilized but well theoretically founded model for modern use.","[7, 7, 6]","[' Good paper, accept', ' Good paper, accept', ' Marginally above acceptance threshold']","[3, 2, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is positive about the paper, using phrases like: ""interesting"", ""well shown experimentally"", ""well constructed"", ""interesting insight"", ""well written"", ""well theoretically founded"". There are no negative remarks, only suggestions for improvement. Therefore, the sentiment is positive and the politeness is very high.",85.0,90.0
Learning how to explain neural networks: PatternNet and PatternAttribution,"['Pieter-Jan Kindermans', 'Kristof T. Schütt', 'Maximilian Alber', 'Klaus-Robert Müller', 'Dumitru Erhan', 'Been Kim', 'Sven Dähne']",Accept,2018,"[7, 7, 3, 26, 13, 9, 9]","[11, 11, 8, 31, 17, 14, 10]","[39, 22, 13, 478, 57, 74, 26]","[17, 6, 3, 182, 26, 29, 9]","[16, 14, 9, 118, 26, 41, 4]","[6, 2, 1, 178, 5, 4, 13]","I found this paper an interesting read for two reasons: First, interpretability is an increasingly important problem as machine learning models grow more and more complicated. Second, the paper aims at generalization of previous work on confounded linear model interpretation in neuroimaging (the so-called filter versus patterns problem). The problem is relevant for discriminative problems: If the objective is really to visualize the generative process,  the ""filters"" learned by the discriminative process need to be transformed to correct for spatial correlated noise. 

Given the focus on extracting visualization of the generative process, it would have been meaningful to place the discussion in a greater frame of generative model deep learning (VAEs, GANs etc etc). At present the ""state of the art"" discussion appears quite narrow, being confined to recent methods for visualization of discriminative deep models.

The authors convincingly demonstrate for the linear case, that their ""PatternNet"" mechanism can produce the generative process (i.e. discard spatially correlated ""distractors""). The PatternNet is generalized to multi-layer ReLu networks by construction of node-specific pattern vectors and back-propagating these through the network. The ""proof"" (eqs. 4-6) is sketchy and involves uncontrolled approximations. The back-propagation mechanism is very briefly introduced and depicted in figure 1.

Yet, the results are rather convincing. Both the anecdotal/qualitative examples and the more quantitative patch elimination experiment figure 4a (?number missing) 

I do not understand the remark: ""However, our method has the advantage that it is not only applicable to image models but is a generalization of the theory commonly used in neuroimaging Haufe et al. (2014).""  what ??

Overall, I appreciate the general idea. However, the contribution could have been much stronger based on a detailed derivation with testable assumptions/approximations, and if based on a clear declaration of the aim.

","[6, 8, 8]","[' Marginally above acceptance threshold', ' Top 50% of accepted papers, clear accept', ' Top 50% of accepted papers, clear accept']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer provides both positive and negative points. They find the paper interesting and the results convincing. However, they also point out areas for improvement, such as a lack of clarity in the derivation and a need for stronger theoretical grounding. The language is direct and critical in an academic sense but not rude. ",50.0,50.0
WRPN: Wide Reduced-Precision Networks,"['Asit Mishra', 'Eriko Nurvitadhi', 'Jeffrey J Cook', 'Debbie Marr']",Accept,2018,"[11, 16, 17, 4]","[14, 21, 17, 7]","[48, 78, 15, 25]","[37, 54, 12, 17]","[8, 7, 2, 6]","[3, 17, 1, 2]","This paper presents an simple and interesting idea to improve the performance for neural nets. The idea is we can reduce the precision for activations and increase the number of filters, and is able to achieve better memory usage (reduced). The paper is aiming to solve a practical problem, and has done some solid research work to validate that.  In particular, this paper has also presented a indepth study on AlexNet with very comprehensive results and has validated the usefulness of this approach.   

In addition, in their experiments, they have demonstrated pretty solid experimental results, on AlexNet and even deeper nets such as the state of the art Resnet. The results are convincing to me. 

On the other side, the idea of this paper does not seem extremely interesting to me, especially many decisions are quite natural to me, and it looks more like a very empirical practical study. So the novelty is limited.

So overall given limited novelty but the paper presents useful results, I would recommend borderline leaning towards reject.","[5, 5, 9]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Top 15% of accepted papers, strong accept']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer acknowledges the paper's practical value and solid research with comprehensive results. However, they find the idea somewhat obvious and lacking in novelty, describing it as a ""very empirical practical study."" The recommendation leans towards rejection due to limited novelty, despite the positive aspects. Therefore, the sentiment is slightly negative. The language used is polite and professional throughout.",-10.0,60.0
On the Discrimination-Generalization Tradeoff in GANs,"['Pengchuan Zhang', 'Qiang Liu', 'Dengyong Zhou', 'Tao Xu', 'Xiaodong He']",Accept,2018,"[2, 9, 18, 17, 20]","[7, 14, 21, 22, 25]","[72, 183, 62, 390, 316]","[29, 88, 37, 178, 189]","[40, 86, 17, 42, 106]","[3, 9, 8, 170, 21]","== Paper Summary ==
The paper addresses the problem of balancing capacities of generator and discriminator classes in generative adversarial nets (GANs) from purely theoretical (function analytical and statistical learning) perspective. In my point of view, the main *novel* contributions are: 
(a) Conditions on function classes guaranteeing that the induced IPMs are metrics and not pseudo-metrics (Theorem 2.2). Especially I liked an argument explaining why ReLu activations could work better in discriminator that tanh.
(b) Proving that convergence in the neural distance implies a weak convergence (Theorem 2.5)
(c) Listing particular cases when the neural distance upper bounds the so-called bounded Lipschitz distance (also know as the Fortet-Mourier distance) and the symmetrized KL-divergence (Corollary 2.8 and Proposition 2.9).

The paper is well written (although with *many* typos), the topic is clearly motivated and certainly interesting. The related literature is mainly covered well, apart from several important points listed below.

== Major comments ==
In my opinion, the authors are slightly overselling the results. Next I shortly explain why:

(1) First, point (a) above is indeed novel, but not groundbreaking. A very similar result previously appeared in [1, Theorem 5]. The authors may argue that the referenced result deals only with MMDs, that is IPMs specified to the function classes belonging to the Reproducing Kernel Hilbert Spaces. However, the technique used to prove the ""sufficient"" part of the statement is literally *identical*. 

(2) As discussed in the paragraph right after Theorem 2.5, Theorem 10 of [2] presents the same result which is on one hand stronger than Theorem 2.5 of the current paper because it allows for more general divergences than the neural distance and on the other hand weaker because in [2] the authors assumes a compact input space. Overall, Theorem 2.5 of course makes a novel contribution, because the compactness assumption is not required, however conceptually it is not that novel.

(3) In Section 3 the authors discuss the generalization properties of the neural network distance. One of the main messages (emphasized several times throughout the paper) is that surprisingly the capacity of the generator class does not enter the generalization error bound. However, this is not surprising at all as it is a consequence of the way in which the authors define the generalization. In short, the capacity of discriminators (D) naturally enters the picture, because the generalization error accounts for the mismatch between the true data distribution mu (used for testing) and its empirical version hat{mu} (used for training). However, the authors assume the model distribution (nu) is the same both during testing and training. In practice this is not true and during testing GANs use the empirical version of nu. If the authors were to account for this mismatch, capacity of G would certainly pop up as well.

(4) The error bounds of Section 3 are based on a very standard machinery (empirical processes, Rademacher complexity) and to the best of my knowledge do not lead to any new interesting conclusions in terms of GANs.

(5) Finally, I would suggest the authors to remove Section 4. I suggest this mainly because the authors admit in Remark 4.1 that the main result of this section (Theorem 4.1) is a corollary of a stronger result appearing in [2]. Also, the main part of the paper has 13 pages, while a recommended amount is 8. 

== Minor comments ==

(1) There are *MANY* typos in the paper. Only few of them are listed below.
(2) First paragraph of page 18, proof of Theorem 2.2. This part is of course well known and the authors may just cite Lemma 9.3.2. of Dudley's ""Real analysis and probability"" for instance.
(3) Theorem 2.5: ""Let ...""
(4) Page 7, ""...we may BE interested...""
(5) Corollary 3.2. I doubt that in practice anyone uses discriminator with one hidden unit. The authors may want to consider using the bound on the Rademacher complexity of DNNs recently derived in [3]. 
(6) Page 8, ""..is neural networK""
(7) Page 9: ""...interested IN evaluating...""
(8) Page 10. All most ---> almost.

[1] Gretton et al., A Kernel Two-Sample Test, JMLR 2012.
[2] Liu et al, Approximation and Convergence Properties of Generative Adversarial Learning, 2017
[3] Bartlett et al, Spectrally-normalized margin bounds for neural networks, 2017","[6, 3, 7]","[' Marginally above acceptance threshold', ' Clear rejection', ' Good paper, accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review acknowledges the paper's contributions as novel but not groundbreaking. The reviewer raises several major concerns, suggesting the authors oversell their results and pointing out limitations and prior work that diminish the perceived impact. The tone, while critical, remains professional and provides constructive feedback with specific examples and references. Therefore, the sentiment leans slightly negative due to the major concerns raised, but the politeness remains positive due to the constructive and professional tone.",-20.0,70.0
The Reactor: A fast and sample-efficient Actor-Critic agent for  Reinforcement Learning,"['Audrunas Gruslys', 'Will Dabney', 'Mohammad Gheshlaghi Azar', 'Bilal Piot', 'Marc Bellemare', 'Remi Munos']",Accept,2018,"[5, 2, 9, 7, 12, 23]","[9, 7, 14, 12, 17, 28]","[18, 67, 47, 60, 102, 266]","[7, 30, 19, 31, 45, 143]","[10, 36, 26, 27, 51, 96]","[1, 1, 2, 2, 6, 27]","This paper proposes a novel reinforcement learning algorithm (« The Reactor ») based on the combination of several improvements to DQN: a distributional version of Retrace, a policy gradient update rule called beta-LOO aiming at variance reduction, a variant of prioritized experience replay for sequences, and a parallel training architecture. Experiments on Atari games show a significant improvement over prioritized dueling networks in particular, and competitive performance compared to Rainbow, at a fraction of the training time.

There are definitely several interesting and meaningful contributions in this submission, and I like the motivations behind them. They are not groundbreaking (essentially extending existing techniques) but are still very relevant to current RL research.

Unfortunately I also see it as a step back in terms of comparison to other algorithms. The recent Rainbow paper finally established a long overdue clear benchmark on Atari. We have seen with the « Deep Reinforcement Learning that Matters » paper how important (and difficult) it is to properly compare algorithms on deep RL problems. I assume that this submission was mostly written before Rainbow came out, and that comparisons to Rainbow were hastily added just before the ICLR deadline: this would explain why they are quite limited, but in my opinion it remains a major issue, which is the main reason why I am advocating for rejection.

More precisely, focusing on the comparison to Rainbow which is the main competitor here, my concerns are the following:
- There is almost no discussion on the differences between Reactor and Rainbow (actually the paper lacks a « related work » section). In particular Rainbow also uses a version of distributional multi-step, which as far as I can tell may not be as well motivated (from a mathematical point of view) as the one in this submission (since it does not correct for the « off-policyness » of the replay data), but still seems to work well on Atari.
- Rainbow is not distributed. This was a deliberate choice by its authors to focus on algorithmic comparisons. However, it seems to me that it could benefit from a parallel training scheme like Reactor’s. I believe a comparison between Reactor and Rainbow needs to either have them both parallelized or none of them (especially for a comparison on time efficiency like in Fig. 2)
- Rainbow uses the traditional feedforward DQN architecture while Reactor uses a recurrent network. It is not clear to which extent this has an impact on the results.
- Rainbow was stopped at 200M steps, at which point it seems to be overall superior to Reactor at 200M steps. The results as presented here emphasize the superiority of Reactor at 500M steps, but a proper comparison would require Rainbow results at 500M steps as well.

In addition, although I found most of the paper to be clear enough, some parts were confusing to me, in particular:
- « multi-step distributional Bellman operator » in 3.2: not clear exactly what the target distribution is. If I understand correctly this is the same as the Rainbow extension, but this link is not mentioned.
- 3.4.1 (network architecture): a simple diagram in the appendix would make it much easier to understand (Table 3 is still hard to read because it is not clear which layers are connected together)
- 3.3 (prioritized sequence replay): again a visual illustration of the partitioning scheme would in my opinion help clarify the approach

A few minor points to conclude:
- In eq. 6, 7 and the rest of this section, A does not depend (directly) on theta so it should probably be removed to avoid confusion. Note also that using the letter A may not be best since A is used to denote an action in 3.1.
- In 3.1: « Let us assume that for the chosen action A we have access to an estimate R(A) of Qπ(A) » => « unbiased estimate »
- In last equation of p.5 it is not clear what q_i^n is
- There is a lambda missing on p.6 in the equation showing that alphas are non-negative on average, just before the min
- In the equation above eq. 12 there is a sum over « i=1 »
- That same equation ends with some h_z_i that are not defined
- In Fig. 2 (left) for Reactor we see one worker using large batches and another one using many threads. This is confusing.
- 3.3 mentions sequences of length 32 but 3.4 says length 33.
- 3.3 says tree operations are in O(n ln(n)) but it should be O(ln(n))
- At very end of 3.3 it is not clear what « total variation » is.
- In 3.4 please specify the frequency at which the learner thread downloads shared parameters and uploads updates
- Caption of Fig. 3 talks about « changing the number of workers » for the left plot while it is in the right plot
- The explanation on what the variants of Reactor (ND and 500M) mean comes after results are shown in Fig. 2.
- Section 4 starts with Fig. 3 without explaining what the task is, how performance is measured, etc. It also claims that Distributional Retrace helps while this is not the case in Fig. 3 (I realize it is explained afterwards, but it is confusing when reading the sentence « We can also see... »). Finally it says priorization is the most important component while the beta-LOO ablation seems to perform just the same.
- Footnote 3 should say it is 200M observations except for Reactor 500M
- End of 4.1: « The algorithms that we compare Reactor against are » => missing ACER, A3C and Rainbow
- There are two references for « Sample efficient actor-critic with experience replay »
- I do not see the added benefit of the Elo computation. It seems to convey essentially the same information as average rank.

And a few typos:
- Just above 2.1.3: « increasing » => increasingly
- In 3.1: « where V is a baseline that depend » => depends
- p.7: « hight » => high, and « to all other sequences » => of all other sequences
- Double parentheses in Bellemare citation at beginning of section 4
- Several typos in appendix (too many to list)

Note: I did not have time to carefully read Appendix 6.3 (contextual priority tree)

Edit after revision: bumped score from 5 to 7 because (1) authors did many improvements to the paper, and (2) their explanations shed light on some of my concerns","[7, 7, 7]","[' Good paper, accept', ' Good paper, accept', ' Good paper, accept']","[4, 2, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review acknowledges the paper's contributions as ""interesting and meaningful"" but expresses concerns about the comparison to other algorithms, particularly Rainbow. The reviewer finds the comparison lacking and suggests that it needs to be more robust for the paper to be considered for publication. The reviewer also lists several points that they found confusing and provides suggestions for improvement. Despite the criticism, the language used is professional and polite, suggesting a desire to help the authors improve their work. The reviewer's decision to increase the score after revision further indicates a positive attitude towards the paper's potential.",40.0,70.0
Towards Synthesizing Complex Programs From Input-Output Examples,"['Xinyun Chen', 'Chang Liu', 'Dawn Song']",Accept,2018,"[9, 11, 20]","[14, 14, 25]","[93, 66, 440]","[43, 40, 254]","[43, 22, 154]","[7, 4, 32]","This paper proposes a method for learning parsers for context-free languages. They demonstrate that this achieves perfect accuracy on training and held-out examples of input/output pairs for two synthetic grammars. In comparison, existing approaches appear to achieve little to no generalization, especially when tested on longer examples than seen during training.

The approach is presented very thoroughly. Details about the grammars, the architecture, the learning algorithm, and the hyperparameters are clearly discussed, which is much appreciated. Despite the thoroughness of the task and model descriptions, the proposed method is not well motivated. The description of the relatively complex two-phase reinforcement learning algorithm is largely procedural, and it is not obvious how necessary the individual pieces of the algorithm are. This is particularly problematic because the only empirical result reported is that it achieves 100% accuracy. Quite a few natural questions left unanswered, limiting what readers can learn from this paper, e.g.
- How quickly does the model learn? Is there a smooth progression that leads to perfect generalization?
- Presumably the policy learned in Phase 1 is a decent model by itself, since it can reliably find candidate traces. How accurate is it? What are the drawbacks of using that instead of the model from the second phase? Are there systematic problems, such as overfitting, that necessitate a second phase?
- How robust is the method to hyperparameters and multiple initializations? Why choose F = 10 and K = 3? Presumably, there exists some hyperparameters where the model does not achieve 100% test accuracy, in which case, what are the failure modes?

Other misc. points:
- The paper mentions that ""the training curriculum is very important to regularize the reinforcement learning process."" Unless I am misunderstanding the experimental setup, this is not supported by the result, correct? The proposed method achieves perfect accuracy in every condition.
- The reimplementations of the methods from Grefenstette et al. 2015 have surprisingly low training accuracy (in some cases 0% for Stack LSTM and 2.23% for DeQueue LSTM). Have you evaluated these reimplementations on their reported tasks to tease apart differences due to varying tasks and differences due to varying implementations?","[5, 7, 8]","[' Marginally below acceptance threshold', ' Good paper, accept', ' Top 50% of accepted papers, clear accept']","[2, 4, 3]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer acknowledges the thoroughness of the paper and its clear presentation as positives. However, they express concerns about the lack of motivation for the proposed method and the absence of analysis beyond achieving 100% accuracy. The reviewer raises several unanswered questions, indicating a lack of crucial insights into the method's workings and limitations. The tone is critical but professional, focusing on the research's shortcomings rather than directly attacking the authors.",20.0,60.0
Do GANs learn the distribution? Some Theory and Empirics,"['Sanjeev Arora', 'Andrej Risteski', 'Yi Zhang']",Accept,2018,"[29, 7, 2]","[34, 12, 6]","[217, 83, 19]","[103, 35, 9]","[76, 45, 10]","[38, 3, 0]","This paper proposes a clever new test based on the birthday paradox for measuring diversity in generated samples. The main goal is to quantify mode collapse in state-of-the-art generative models. The authors also provide a specific theoretical construction that shows bidirectional GANs cannot escape specific cases of mode collapse.
Using the birthday paradox test, the experiments show that GANs can learn and consistently reproduce the same examples, which are not necessarily exactly the same as training data (eg. the triplets in Figure 1).
The results are interpreted to mean that mode collapse is strong in a number of state-of-the-art generative models.
Bidirectional models (ALI, BiGANs) however demonstrate significantly higher diversity that DCGANs and MIX+DCGANs.
Finally, the authors verify empirically the hypothesis that diversity grows linearly with the size of the discriminator.

This is a very interesting area and exciting work. The main idea behind the proposed test is very insightful. The main theoretical contribution stimulates and motivates much needed further research in the area. In my opinion both contributions suffer from some significant limitations. However, given how little we know about the behavior of modern generative models, it is a good step in the right direction.


1. The biggest issue with the proposed test is that it conflates mode collapse with non-uniformity. The authors do mention this issue, but do not put much effort into evaluating its implications in practice, or parsing Theorems 1 and 2. My current understanding is that, in practice, when the birthday paradox test gives a collision I have no way of knowing whether it happened because my data distribution is modal, or because my generative model has bad diversity. Anecdotally, real-life distributions are far from uniform, so this should be a common issue. I would still use the test as a part of a suite of measurements, but I would not solely rely on it. I feel that the authors should give a more prominent disclaimer to potential users of the test.

2. Also, given how mode collapse is the main concern, it seems to me that a discussion on coverage is missing. The proposed test is a measure of diversity, not coverage, so it does not discriminate between a generator that produces all of its samples near some mode and another that draws samples from all modes of the true data distribution. As long as they yield collisions at the same rate, these two generative models are ‘equally diverse’. Isn’t coverage of equal importance?

3. The other main contribution of the paper is Theorem 3, which shows—via a very particular construction on the generator and encoder—that bidirectional GANs can also suffer from serious mode collapse. I welcome and are grateful for any theory in the area. This theorem might very well capture the underlying behavior of bidirectional GANs, however, being constructive, it guarantees nothing in practice. In light of this, the statement in the introduction that “encoder-decoder training objectives cannot avoid mode collapse” might need to be qualified. In particular, the current statement seems to obfuscate the understanding that training such an objective would typically not result into the construction of Theorem 3.","[6, 7, 7]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Good paper, accept']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with positive remarks, highlighting the cleverness and insightfulness of the paper's contributions. The reviewer finds the topic interesting and considers the work a step in the right direction. However, the review also raises significant limitations, questioning the practical implications of the proposed test and the generalizability of the theoretical results. While acknowledging the contributions, the reviewer expresses reservations, suggesting that the paper's claims might be too strong. Overall, the sentiment leans towards the positive side due to the initial praise and encouragement, but the presence of considerable concerns prevents it from being overwhelmingly positive.",60.0,80.0
Learning One-hidden-layer Neural Networks with Landscape Design,"['Rong Ge', 'Jason D. Lee', 'Tengyu Ma']",Accept,2018,"[11, 12, 8]","[16, 17, 13]","[155, 192, 181]","[63, 77, 76]","[78, 106, 97]","[14, 9, 8]","This paper studies the problem of learning one-hidden layer neural networks and is a theory paper. A well-known problem is that without good initialization, it is not easy to learn the hidden parameters via gradient descent. This paper establishes an interesting connection between least squares population loss and Hermite polynomials. Following from this connection authors propose a new loss function. Interestingly, they are able to show that the loss function globally converges to the hidden weight matrix. Simulations confirm the findings.

Overall, pretty interesting result and solid contribution. The paper also raises good questions for future works. For instance, is designing alternative loss function useful in practice? In summary, I recommend acceptance. The paper seems rushed to me so authors should polish up the paper and fix typos.

Two questions:
1) Authors do not require a^* to recover B^*. Is that because B^* is assumed to have unit length rows? If so they should clarify this otherwise it confuses the reader a bit.
2) What can be said about rate of convergence in terms of network parameters? Currently a generic bound is employed which is not very insightful in my opinion.

","[9, 7, 6]","[' Top 15% of accepted papers, strong accept', ' Good paper, accept', ' Marginally above acceptance threshold']","[3, 3, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer explicitly recommends acceptance and calls the results ""pretty interesting"" and a ""solid contribution."" They also highlight positive aspects like the ""interesting connection"" the paper establishes and the confirmation of findings through simulations. This points to a positive sentiment. While the reviewer mentions the paper seems rushed and needs polishing, this is presented as an area for improvement rather than a severe flaw, hence not drastically impacting the sentiment score. The language used is constructive and professional throughout, suggesting politeness. The reviewer asks specific questions and provides suggestions, but does so respectfully.",75.0,80.0
Learning From Noisy Singly-labeled Data,"['Ashish Khetan', 'Zachary C. Lipton', 'Animashree Anandkumar']",Accept,2018,"[3, 5, 13]","[7, 10, 18]","[29, 203, 419]","[10, 77, 154]","[15, 117, 223]","[4, 9, 42]","This paper proposes a method for learning from noisy labels, particularly focusing on the case when data isn't redundantly labeled (i.e. the same sample isn't labeled by multiple non-expert annotators). The authors provide both theoretical and experimental validation of their idea. 

Pros:
+ The paper is generally very clearly written. The motivation, notation, and method are clear.
+ Plentiful experiments against relevant baselines are included, validating both the no-redundancy and plentiful redundancy cases. 
+ The approach is a novel twist on an existing method for learning from noisy data. 

Cons: 
- All experiments use simulated workers; this is probably common but still not very convincing.
- The authors missed an important related work which studies the same problem and comes up with a similar conclusion: Lin, Mausam, and Weld. ""To re (label), or not to re (label)."" HCOMP 2014.
- The authors should have compared their approach to the ""base"" approach of Natarajan et al. 
- It seems too simplistic too assume all workers are either hammers or spammers; the interesting cases are when annotators are neither of these.
- The ResNet used for each experiment is different, and there is no explanation of the choice of architecture.

Questions: 
- How would the model need to change to account for example difficulty? 
- Why are Joulin 2016, Krause 2016 not relevant?
- Best to clarify what the weights in the weighted sum of Natarajan are. 
- ""large training error on wrongly labeled examples"" -- how do we know they are wrongly labeled, i.e. do we have a ground truth available apart from the crowdsourced labels? Where does this ground truth come from?
- Not clear what ""Ensure"" means in the algorithm description.
- In Sec. 4.4, why is it important that the samples are fresh?
","[7, 7, 6]","[' Good paper, accept', ' Good paper, accept', ' Marginally above acceptance threshold']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review acknowledges the paper's strengths, such as clarity, experimental validation, and novelty. However, it also raises several valid concerns, including the lack of real-world data, missing related work, and potentially oversimplified assumptions. The reviewer asks pointed questions that suggest potential weaknesses in the methodology and analysis. Overall, the feedback is balanced but leans towards requesting significant improvements. Therefore, the sentiment is slightly positive, and the language used is polite and constructive.",60.0,80.0
Understanding Short-Horizon Bias in Stochastic Meta-Optimization,"['Yuhuai Wu', 'Mengye Ren', 'Renjie Liao', 'Roger Grosse.']",Accept,2018,"[3, 4, 9, 12]","[8, 8, 14, 17]","[69, 65, 99, 123]","[28, 29, 52, 60]","[38, 36, 43, 61]","[3, 0, 4, 2]","This paper studies the issue of truncated backpropagation for meta-optimization. Backpropagation through an optimization process requires unrolling the optimization, which due to computational and memory constraints, is typically restricted or truncated to a smaller number of unrolled steps than we would like.

This paper highlights this problem as a fundamental issue limiting meta-optimization approaches. The authors perform a number of experiments on a toy problem (stochastic quadratics) which is amenable to some theoretical analysis as well as a small fully connected network trained on MNIST.  

(side note: I was assigned this paper quite late in the review process, and have not carefully gone through the derivations--specifically Theorems 1 and 2).

The paper is generally clear and well written.

Major comments
-------------------------
I was a bit confused why 1000 SGD+mom steps pre-training steps were needed. As far as I can tell, pre-training is not typically done in the other meta-optimization literature? The authors suggest this is needed because ""the dynamics of training are different at the very start compared to later stages"", which is a bit vague. Perhaps the authors can expand upon  this point?

The conclusion suggests that the difference in greedy vs. fully optimized schedule is due to the curvature (poor scaling) of the objective--but Fig 2. and earlier discussion talked about the noise in the objective as introducing the bias (e.g. from earlier in the paper, ""The noise in the problem adds uncertainty to the objective, resulting in failures of greedy schedule""). Which is the real issue, noise or curvature? Would running the problem on quadratics with different condition numbers be insightful?

Minor comments
-------------------------
The stochastic gradient equation in Sec 2.2.2 is missing a subscript: ""h_i"" instead of ""h""

It would be nice to include the loss curve for a fixed learning rate and momentum for the noisy quadratic in Figure 2, just to get a sense of how that compares with the greedy and optimized curves.

It looks like there was an upper bound constraint placed on the optimized learning rate in Figure 2--is that correct? I couldn't find a mention of the constraint in the paper. (the optimized learning rate remains at 0.2 for the first ~60 steps)?

Figure 2 (and elsewhere): I would change 'optimal' to 'optimized' to distinguish it from an optimal curve that might result from an analytic derivation. 'Optimized' makes it more clear that the curve was obtained using an optimization process.

Figure 2: can you change the line style or thickness so that we can see both the red and blue curves for the deterministic case? I assume the red curve is hiding beneath the blue one--but it would be good to see this explicitly.

Figure 4 is fantastic--it succinctly and clearly demonstrates the problem of truncated unrolls. I would add a note in the caption to make it clear that the SMD trajectories are the red curves, e.g.: ""SMD trajectories (red) during meta-optimization of initial effective ..."". I would also change the caption to use ""meta-training losses"" instead of ""training losses"" (I believe those numbers are for the meta-loss, correct?). Finally, I would add a colorbar to indicate numerical values for the different grayscale values.

Some recent references that warrant a mention in the text:
- both of these learn optimizers using longer numbers of unrolled steps:
Learning gradient descent: better generalization and longer horizons, Lv et al, ICML 2017
Learned optimizers that scale and generalize, Wichrowska et al, ICML 2017
- another application of unrolled optimization:
Unrolled generative adversarial networks, Metz et al, ICLR 2017

In the text discussing Figure 4 (middle of pg. 8) , ""which is obtained by using..."" should be ""which are obtained by using...""

In the conclusion, ""optimal for deterministic objective"" should be ""deterministic objectives""","[8, 7, 6]","[' Top 50% of accepted papers, clear accept', ' Good paper, accept', ' Marginally above acceptance threshold']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is generally positive. The reviewer highlights the clarity and well-written nature of the paper, and finds the included figures (especially Figure 4) to be well-done and informative. While the reviewer raises some questions and suggests areas for improvement, these are presented constructively and with the aim of enhancing the paper. The language used is polite and professional throughout.",70.0,90.0
A Scalable Laplace Approximation for Neural Networks,"['Hippolyt Ritter', 'Aleksandar Botev', 'David Barber']",Accept,2018,"[2, 3, 24]","[6, 8, 29]","[14, 22, 131]","[7, 11, 64]","[6, 11, 41]","[1, 0, 26]","This paper proposes a Laplace approximation to approximate the posterior distribution over the parameters of deep networks. 

The idea is interesting and the realization of the paper is good. The idea builds upon previous work in scalable Gauss-Newton methods for optimization in deep networks, notably Botev et al., ICML 2017. In this respect, I think that the novelty in the current submission is limited, as the approximation is essentially what proposed in Botev et al., ICML 2017.  The Laplace approximation requires the Hessian of the posterior, so techniques developed for Gauss-Newton optimization can straightforwardly be applied to construct Laplace approximations.

Having said that, the experimental evaluation is quite interesting and in-depth. I think it would have been interesting to report comparisons with factorized variational inference (Graves, 2011) as it is a fairly standard and widely adopted in Bayesian deep learning. This would have been an interesting way to support the claims on the poor approximation offered by standard variational inference. 

I believe that the independence assumption across layers is a limiting factor of the proposed approximation strategy. Intuitively, changes in the weights in a given layer should affect the weights in other layers, so I would expect the posterior distribution over all the weights to reflect this through correlations across layers. I wonder how these results can be generalized to relax the independence assumption. 

","[6, 9, 6]","[' Marginally above acceptance threshold', ' Top 15% of accepted papers, strong accept', ' Marginally above acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer finds the paper interesting and well-executed, but notes a limited novelty due to its close connection to previous work. The reviewer praises the experimental evaluation but suggests additional comparisons. While acknowledging the limitations of the independence assumption, the reviewer maintains a generally positive tone and offers constructive criticism.",60.0,80.0
A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks,"['Behnam Neyshabur', 'Srinadh Bhojanapalli', 'Nathan Srebro']",Accept,2018,"[6, 6, 18]","[10, 11, 23]","[88, 61, 269]","[39, 24, 140]","[46, 35, 115]","[3, 2, 14]","The authors prove a generalization guarantee for deep
neural networks with ReLU activations, in terms of margins of the
classifications and norms of the weight matrices.  They compare this
bound with a similar recent bound proved by Bartlett, et al.  While,
strictly speaking, the bounds are incomparable in strength, the
authors of the submission make a convincing case that their new bound
makes stronger guarantees under some interesting conditions.

The analysis is elegant.  It uses some existing tools, but brings them
to bear in an important new context, with substantive new ideas needed.
The mathematical writing is excellent.

Very nice paper.

I guess that networks including convolutional layers are covered by
their analysis.  It feels to me that these tend to be sparse, but that
their analysis still my provides some additional leverage for such
layers.  Some explicit discussion of convolutional layers may be
helpful.  ","[9, 7, 6]","[' Top 15% of accepted papers, strong accept', ' Good paper, accept', ' Marginally above acceptance threshold']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review is overwhelmingly positive. The reviewer uses terms like ""convincing"", ""elegant"", ""excellent"", ""substantive"", and ""Very nice paper"" to describe the work. There are no negative remarks, only a suggestion for improvement. Therefore, the sentiment is very positive and the language is very polite.",90.0,100.0
Deep Learning as a Mixed Convex-Combinatorial Optimization Problem,"['Abram L. Friesen', 'Pedro Domingos']",Accept,2018,"[9, 25]","[14, 28]","[24, 169]","[14, 129]","[10, 16]","[0, 24]","The paper discusses the problem of optimizing neural networks with hard threshold and proposes a novel solution to it. The problem is of significance because in many applications one requires deep networks which uses reduced computation and limited energy. The authors frame the problem of optimizing such networks to fit the training data as a convex combinatorial problems. However since the complexity of such a problem is exponential, the authors propose a collection of heuristics/approximations to solve the problem. These include, a heuristic for setting the targets at each layer, using a soft hinge loss, mini-batch training and such. Using these modifications the authors propose an algorithm (Algorithm 2 in appendix) to train such models efficiently. They compare the performance of a bunch of models trained by their algorithm against the ones trained using straight-through-estimator (SSTE) on a couple of datasets, namely, CIFAR-10 and ImageNet. They show superiority of their algorithm over SSTE. 

I thought the paper is very well written and provides a really nice exposition of the problem of training deep networks with hard thresholds. The authors formulation of the problem as one of combinatorial optimization and proposing Algorithm 1 is also quite interesting. The results are moderately convincing in favor of the proposed approach. Though a disclaimer here is that I'm not 100% sure that SSTE is the state of the art for this problem. Overall i like the originality of the paper and feel that it has a potential of reasonable impact within the research community. 

There are a few flaws/weaknesses in the paper though, making it somewhat lose. 
- The authors start of by posing the problem as a clean combinatorial optimization problem and propose Algorithm 1. Realizing the limitations of the proposed algorithm, given the assumptions under which it was conceived in, the authors relax those assumptions in the couple of paragraphs before section 3.1 and pretty much throw away all the nice guarantees, such as checks for feasibility, discussed earlier. 
- The result of this is another algorithm (I guess the main result of the paper), which is strangely presented in the appendix as opposed to the main text, which has no such guarantees.  
- There is no theoretical proof that the heuristic for setting the target is a good one, other than a rough intuition
- The authors do not discuss at all the impact on generalization ability of the model trained using the proposed approach. The entire discussion revolves around fitting the training set and somehow magically everything seem to generalize and not overfit. 
","[7, 7, 7]","[' Good paper, accept', ' Good paper, accept', ' Good paper, accept']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer explicitly states liking the paper, its originality, and potential impact, indicating a positive sentiment. However, they also point out flaws and weaknesses, suggesting the sentiment is not overly positive. Therefore, a sentiment score of 50 seems appropriate. The language used is professional and respectful, with constructive criticism offered. Thus, a politeness score of 80 is justified.",50.0,80.0
Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis,"['Rudy Bunel', 'Matthew Hausknecht', 'Jacob Devlin', 'Rishabh Singh', 'Pushmeet Kohli']",Accept,2018,"[3, 10, 8, 10, 16]","[8, 15, 13, 14, 21]","[43, 53, 49, 133, 322]","[16, 25, 25, 62, 180]","[24, 24, 22, 50, 102]","[3, 4, 2, 21, 40]","This is a nice paper. It makes novel contributions to neural program synthesis by (a) using RL to tune neural program synthesizers such that they can generate a wider variety of correct programs and (b) using a syntax checker (or a learned approximation thereof) to prevent the synthesizer from outputting any syntactically-invalid programs, thus pruning the search space. In experiments, the proposed method synthesizes correct Karel programs (non-trivial programs involving loops and conditionals) more frequently than synthesizers trained using only maximum likelihood supervised training.

I have a few minor questions and requests for clarification, but overall the paper presents strong results and, I believe, should be accepted.


Specific comments/questions follow:


Figure 2 is too small. It would be much more helpful (and easier to read) if it were enlarged to take the full page width.

Page 7: ""In the supervised setting..."" This suggests that the syntaxLSTM can be trained without supervision in the form of known valid programs, a possibility which might not have occurred to me without this little aside. If that is indeed the case, that's a surprising and interesting result that deserves having more attention called to it (I appreciated the analysis in the results section to this effect, but you could call attention to this sooner, here on page 7).

Is the ""Karel DSL"" in your experiments the full Karel language, or a subset designed for the paper?

For the versions of the model that use beam search, what beam width was used? Do the results reported in e.g. Table 1 change as a function of beam width, and if so, how? 
","[7, 5, 6]","[' Good paper, accept', ' Marginally below acceptance threshold', ' Marginally above acceptance threshold']","[3, 3, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer explicitly states ""the paper presents strong results and, I believe, should be accepted."" This indicates a clear positive sentiment. The reviewer's questions seem to be aimed at clarifying and improving the paper rather than criticizing it, which points towards a polite and constructive tone.",85.0,90.0
Unbiased Online Recurrent Optimization,"['Corentin Tallec', 'Yann Ollivier']",Accept,2018,"[2, 16]","[6, 20]","[25, 47]","[10, 15]","[15, 28]","[0, 4]","The authors introduce a novel approach to online learning of the parameters of recurrent neural networks from long sequences that overcomes the limitation of truncated backpropagation through time (BPTT) of providing biased gradient estimates.

The idea is to use a forward computation of the gradient as in Williams and Zipser (1989) with an unbiased approximation of Delta s_t/Delta theta to reduce the memory and computational cost.

The proposed approach, called UORO, is tested on a few artificial datasets.

The approach is interesting and could potentially be very useful. However, the paper lacks in providing a substantial experimental evaluation and comparison with other methods.
Rather than with truncated BPTT with smaller truncation than required, which is easy to outperform, I would have expected a comparison with some of the other methods mentioned in the Related Work Section, such as NBT, ESNs, Decoupled Neural Interfaces, etc. Also the evaluation should be extended to other challenging tasks. 

I have increased the score to 6 based on the comments and revisions from the authors.","[6, 8, 7]","[' Marginally above acceptance threshold', ' Top 50% of accepted papers, clear accept', ' Good paper, accept']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer finds the approach interesting and potentially useful, which indicates a positive sentiment. However, they also point out significant limitations in the experimental evaluation and suggest comparisons with other methods. This suggests the sentiment is not overly positive. The language used is constructive and professional, suggesting a neutral to positive politeness level.",60.0,70.0
Learning Wasserstein Embeddings,"['Nicolas Courty', 'Rémi Flamary', 'Mélanie Ducoffe']",Accept,2018,"[13, 10, 12, 20]","[17, 15, 17, 25]","[49, 104, 66, 257]","[28, 46, 26, 120]","[2, 11, 6, 34]","[19, 47, 34, 103]","The paper proposes to use a deep neural network to embed probability distributions in a vector space, where the Euclidean distance in that space matches the Wasserstein distance in the original space of probability distributions. A dataset of pairs of probability distributions and their Wasserstein distance is collected, and serves as a target to be predicted by the deep network.

The method is straightforward, and clearly explained. Two analyses based on Wasserstein distances (computing barycenters, and performing geodesic analysis) are then performed directly in the embedded space.

The authors claim that the proposed method produces sharper barycenters than those learned using the standard (smooth) Wasserstein distance. It is unclear from the paper whether the advantage comes from the ability of the method to scale better and use more examples, or to be able to use the non-smooth Wasserstein distance, or finally, whether the learning of a deep embedding yields improved extrapolation properties. A short discussion could be added. It would also be interesting to provide some guidance on what is a good structure for the encoder (e.g. should it include spatial pooling layers?)

The term “Wasserstein deep learning” is probably too broad, “deep Wasserstein embedding” could be more appropriate.

The last line of future work in the conclusion seems to describe the experiment of Table 1.","[7, 7, 7]","[' Good paper, accept', ' Good paper, accept', ' Good paper, accept']","[3, 3, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is generally positive. The reviewer acknowledges the clarity and straightforwardness of the method. While they raise some questions and suggest improvements (e.g., further discussion on the advantages, clarification on terminology), these are constructive criticisms aimed at strengthening the paper. The tone throughout is professional and polite.",60.0,80.0
Quantitatively Evaluating GANs With Divergences Proposed for Training,"['Daniel Jiwoong Im', 'He Ma', 'Graham W. Taylor', 'Kristin Branson']",Accept,2018,"[5, 10, 15, 14]","[8, 15, 20, 18]","[26, 77, 174, 21]","[8, 40, 78, 8]","[17, 8, 77, 11]","[1, 29, 19, 2]","Through evaluation of current popular GAN variants. 
  * useful AIS figure
  * useful example of failure mode of inception scores
   * interesting to see that using a metric based on a model’s distance does not make the model better at that distance
the main criticism that can be given to the paper is that the proposed metrics are based on trained models which do not have an independent clear evaluation metrics (as classifiers do for inception scores). However, the authors do show that the results are consistent when changing the critic architecture. Would be nice to see if this also holds for changes in learning rates. 
 * nice to see an evaluation on how models scale with the increase in training data.

Using an Independent critic for evaluation has been proposed and used in practice before, see “Comparison of Maximum Likelihood and GAN-based training of Real NVPs”, Danihelka et all, as well as Variational Approaches for Auto-Encoding Generative Adversarial Networks, Rosca at all.

Improvements to be added to the paper:
   * How about overfitting? Would be nice to mention whether the proposed metrics are useful at detecting overfitting. From algorithm 1 one can see that the critic is trained on training data, but at evaluation time test data is used. However, if the generator completely memorizes the training set, the critic will not be able to learn anything useful. In that case, the test measure will not provide any information either. A way to go around this is to use validation data to train the critic, not training data. In that case, the critic can learn the difference between training and validation data and at test time the test set can be used. 
  * Using the WGAN with weight clipping is not a good baseline. The improved WGAN method is more robust to hyper parameters and is the one currently  used by the community. The WGAN with weight clipping is quite sensitive to the clipping hyperparameter, but the authors do not report having changed it from the original paper, both for the critic or for the discriminator used during training. 
  *  Is there a guidance for  which metric should be used? 

Figure 3 needs to be made a bit larger, it is quite hard to read in the current set up. ","[7, 7, 4]","[' Good paper, accept', ' Good paper, accept', ' Ok but not good enough - rejection']","[5, 4, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review starts with positive remarks, highlighting the usefulness and interesting aspects of the paper. While it points out a main criticism regarding the lack of independent evaluation metrics for the proposed method, it acknowledges the authors' efforts in demonstrating consistency. The reviewer further suggests improvements and additional experiments in a constructive manner. The tone throughout is professional and suggestive, not demanding, indicating a positive leaning overall.",60.0,80.0
Training Confidence-calibrated Classifiers for Detecting Out-of-Distribution Samples,"['Kimin Lee', 'Honglak Lee', 'Kibok Lee', 'Jinwoo Shin']",Accept,2018,"[3, 14, 4, 10]","[8, 19, 8, 15]","[79, 273, 26, 276]","[37, 145, 13, 128]","[41, 123, 13, 122]","[1, 5, 0, 26]","This paper proposes a new method of detecting in vs. out of distribution samples. Most existing approaches for this deal with detecting out of distributions at *test time* by augmenting input data and or temperature scaling the softmax and applying a simple classification rule based on the output. This paper proposes a different approach (with could be combined with these methods) based on a new training procedure. 

The authors propose to train a generator network in combination with the classifier and an adversarial discriminator. The generator is trained to produce images that (1) fools a standard GAN discriminator and (2) has high entropy (as enforced with the pull-away term from the EBGAN). Classifier is trained to not only maximize classification accuracy on the real training data but also to output a uniform distribution for the generated samples. 

The model is evaluated on CIFAR-10 and SVNH, where several out of distribution datasets are used in each case. Performance gains are clear with respect to the baseline methods.

This paper is clearly written, proposes a simple model and seems to outperform current methods. One thing missing is a discussion of how this approach is related to semi-supervised learning approaches using GANS where a generative model produces extra data points for the classifier/discriminator. 

 I have some clarifying questions below:
- Figure 4 is unclear: does ""Confidence loss with original GAN"" refer to the method where the classifier is pretrained and then ""Joint confidence loss"" is with joint training? What does ""Confidence loss (KL on SVHN/CIFAR-10)"" refer to?

- Why does the join training improve the ability of the model to generalize to out-of-distribution datasets not seen during training?

- Why is the pull away term necessary and how does the model perform without it? Most GAN models are able to stably train without such explicit terms such as the pull away or batch discrimination. Is the proposed model unstable without the pull-away term? 

- How does this compare with a method whereby instead of pushing the fake sample's softmax distribution to be uniform, the model is simply a trained to classify them as an additional ""out of distribution"" class? This exact approach has been used to do semi supervised learning with GANS [1][2]. More generally, could the authors comment on how this approach is related to these semi-supervised approaches? 

- Did you try combining the classifier and discriminator into one model as in [1][2]?

[1] Semi-Supervised Learning with Generative Adversarial Networks (https://arxiv.org/abs/1606.01583)
[2] Good Semi-supervised Learning that Requires a Bad GAN (https://arxiv.org/abs/1705.09783)","[6, 6, 7]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Good paper, accept']","[3, 4, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review starts with positive statements, highlighting the clarity, simplicity, and performance of the proposed method. The reviewer acknowledges the value of the work. However, the numerous clarifying questions, particularly those comparing the method to existing semi-supervised approaches, suggest areas for improvement and further analysis. Therefore, the sentiment leans towards positive, but with room for improvement.",60.0,100.0
Synthesizing realistic neural population activity patterns using Generative Adversarial Networks,"['Manuel Molano-Mazon', 'Arno Onken', 'Eugenio Piasini*', 'Stefano Panzeri*']",Accept,2018,"[1, 11, 5, 24]","[1, 16, 9, 29]","[2, 21, 11, 84]","[1, 5, 4, 21]","[1, 9, 4, 7]","[0, 7, 3, 56]","[Summary of paper] The paper presents a method for simulating spike trains from populations of neurons which match empirically measured multi-neuron recordings. They set up a Wasserstein-GAN and train it on both synthetic and real multi-neuron recordings, using data from the salamander retina. They find that their method (Spike-GAN) can produce spike trains that visually look like the original data, and which have low-order statistics (firing rates, correlations, time-lagged-correlations, total sum of population activity) which matches those of the original data. They emphasize that their network architecture is 'semi-convolutional', i.e. convolutional in time but not across neurons. Finally, they suggest a way to analyse the fitted networks in order to gain insights into what the 'relevant' neural features are, and illustrate it on synthetic data into which they embedded these features.

[Originality] This paper falls into the category of papers that do a next obvious thing (""GANs have not been applied to population spike trains yet""), and which do it pretty well: If one wants to create simulated neural activity data which matches experimentally observed one, then this method indeed seems to do that. As far as I know, this would be the first peer-reviewed application of GANs to multi-neuron recordings of neural data (but see https://arxiv.org/abs/1707.04582 for an arxiv paper, not cited here-- should be discussed at least).  On a technical level, there is very little to no innovation here -- while the authors emphasise their 'semi-convolutional' network architecture, this is obviously the right architecture to use for multivariate time-series data, and in itself not a big technical novel. Therefore, the paper should really be evaluate as an `application' paper, and be assessed in terms of i) how important the application is, ii) how clearly it is presented, and iii) how convincing the results are relative to state of the art. 

i) [Importance of problem, potential significance] Finding statistical models for modelling and simulating population spike trains is a topic which is extensively studied in computational neuroscience, predominantly using  model-based approaches using MaxEnt models, GLMs or latent variable models. These models are typically simple and restricted, and certainly fall short of capturing the full complexity of neural data. Thus, better, and more flexible solutions for this problem would certainly be very welcome, and have an immediate impact in this community.  However, I think that the approach based on GANs actually has two shortcomings which are not stated by the authors, and which possibly limit the impact of the method: First, statistical models of neural spike trains are often used to compute probabilities e.g. for decoding analyses— this is difficult or impossible for GANs. Second, one most often does not want to simulate data which match a specific recording, but rather which have specified statistics (e.g. firing rates and correlations)— the method here is based on fitting a particular data-set, and it is actually unclear to me when that will be useful.

ii) [Clarity] The methods are presented and explained clearly and cleanly. In my view, too much emphasis is given to highlighting the ‘semi-convolutional’ network, and, conversely, practical issues (exact architectures, cost of training) should be explained more clearly, possibly in an appendix. Similarly, the method would benefit from the authors releasing their code.

iii) [Quality, advance over previous methods] The authors discuss several methods for simulating spike trains in the introduction. In their empirical comparisons, however, they completely focus on a particular model-class (maximum entropy models, ME) which they label being the ‘state-of-the-art’. This label is misleading— ME models are but one of several approaches to modelling neural spike trains, with different models having different advantages and limitations (there is no benchmark which can be used to rank them...). In particular, the only ‘gain’ of the GAN over ME  models in the results comes from their ability of the GAN to match temporal statistics. Given that the ME models used by the authors are blind to temporal correlations, this is, of course (and as pointed out by the authors) hardly surprising. How does the GAN approach fair against alternative models which do take temporal statistics into account, e.g. GLMs, or simple moment-based method e.g. Krumin et al 2009, Lyamzin 2010, Gutnisky et al 2010— setting these up would be simple, and it would provide a non-trivial baseline for the ability of spike-GAN to outperform at least these models? While it true that GANs are much more expressive than the model-based approaches used in neuroscience, a clear demonstration would have been useful.

Minor comments: 
  - p.3: The abbreviation “1D-DCGAN” is never spelled out.
  - p.3: The architecture of Spike-GAN is never explicitely given.
  - p.3: (Sec. 2.2) Statistic 2) “average time course across activity patterns” is unclear to me -- how does one select the activity patterns over which to average? Moreover, later figures do not seem to use this statistic.
  - p.4: “introduced correlations between randomly selected pairs” -- How many such pairs were formed?
  - p.7 (just above Discussion) At the beginning of this section, and for Figs. 4A,B, the texts suggests that packets fire spontaneously with a given probability. For Figs. 4C-E, a particular packet responds to a particular input. Is then the neuron population used in these figures different from the one in Figs. 4A,B? How did the authors ensure that a particular set of neurons respond to their stimulus as a packet? What stimulus did they use?
  - p.8 (Fig. 4E) Are the eight neurons with higher importance those corresponding to the packet? This is insinuated but not stated.
  - p.12 (Appendix A) 
    + The authors do not mention how they produced their “ground truth” data. (What was its firing rate? Did it include correlations? A refractory period?)
    + Generating samples from the trained Spike-GAN is ostensibly cheap. Hence it is unclear why the authors did not  produce a large enough number of samples in order to obtain a 'numerical probability', just as they did for the ground truth data? 
    + Fig. S1B: The figure shows that every sample has the same empirical frequency. This indicates more a lack of statistical power rather than any correspondence between the theoretical and empirical probabilities. This undermines the argument in the second paragraph of p.12. In the other hand, if the authors did approximate numerical probabilities for the Spike-GAN, this argument would no longer be required.
  - p.13 Fig. S1A,B: the abscissas mention “frequency”, while the ordinates mention “probability”
  - p.25 Fig. S4: This figure suggests that the first layer of the Spike-GAN critic sometimes recognizes the packet patterns in the data. However, to know whether this is true, we would need to compare this to a representation of the neurons reordered in the same way and identified by packet. I.e. one expects something something like figure like Fig. 4A, with the packets lining up with the recovered filters when neurons are ordered the same way.
","[8, 4, 6]","[' Top 50% of accepted papers, clear accept', ' Ok but not good enough - rejection', ' Marginally above acceptance threshold']","[5, 4, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review acknowledges the paper's contribution (application of GANs to multi-neuron recordings) but highlights it as a ""next obvious thing"" with limited novelty. The reviewer is critical about the paper's impact due to the limitations of GANs in calculating probabilities and generating data with specific statistics. While acknowledging the clarity of the methods, the reviewer expresses concerns about the focus on the ""semi-convolutional"" aspect and the lack of practical details. The comparison with only Maximum Entropy models is deemed insufficient, urging for comparisons with other models like GLMs. The numerous minor comments further indicate areas for improvement. Overall, the sentiment leans towards the critical side, suggesting potential but with reservations.",20.0,70.0
Kernel Implicit Variational Inference,"['Jiaxin Shi', 'Shengyang Sun', 'Jun Zhu']",Accept,2018,"[5, 4, 14]","[10, 9, 19]","[87, 36, 468]","[40, 14, 204]","[35, 13, 211]","[12, 9, 53]","Thank you for the feedback, and I think many of my concerns have been addressed.

I think the paper should be accepted.

==== original review ====

Thank you for an interesting read. 

Approximate inference with implicit distribution has been a recent focus of the research since late 2016. I have seen several papers simultaneously proposing the density ratio estimation idea using GAN approach. This paper, although still doing density ratio estimation, uses kernel estimators instead and thus avoids the usage of discriminators. 

Furthermore, the paper proposed a new type of implicit posterior approximation which uses intuitions from matrix factorisation. I do think that another big challenge that we need to address is the construction of good implicit approximations, which is not well studied in previous literature (although this is a very new topic). This paper provides a good start in this direction.

However several points need to be clarified and improved:
1. There are other ways to do implicit posterior inference such as amortising deterministic/stochastic dynamics, and approximating the gradient updates of VI. Please check the literature.
2. For kernel based density ratio estimation methods, you probably need to cite a bunch of Sugiyama papers besides (Kanamori et al. 2009). 
3. Why do you need to introduce both regression under p and q (the reverse ratio trick)? I didn't see if you have comparisons between the two. From my perspective the reverse ratio trick version is naturally more suitable to VI.
4. Do you have any speed and numerical issues on differentiating through alpha (which requires differentiating K^{-1})?
5. For kernel methods, kernel parameters and lambda are key to performances. How did you tune them?
6. For the celebA part, can you compute some quantitative metric, e.g inception score?
","[7, 5, 7]","[' Good paper, accept', ' Marginally below acceptance threshold', ' Good paper, accept']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer explicitly recommends acceptance and praises the novelty of the paper's approach. While they list several points for clarification and improvement, these are presented constructively and aim to enhance the paper's quality. ",85.0,90.0
Residual Loss Prediction: Reinforcement Learning With No Incremental Feedback,"['Hal Daumé III', 'John Langford', 'Amr Sharaf']",Accept,2018,"[18, 21, 4]","[23, 26, 9]","[254, 222, 21]","[152, 112, 10]","[79, 83, 11]","[23, 27, 0]","The authors propose a new episodic reinforcement learning algorithm based on contextual bandit oracles.
The key specificity of this algorithm is its ability to deal with the credit assignment problem by learning automatically a progressive ""reward shaping"" (the residual losses) from a feedback that is only provided at the end of the epochs.

The paper is dense but well written. 

The theoretical grounding is a bit thin or hard to follow.
The authors provide a few regret theoretical results (that I did not check deeply) obtained by reduction to ""value-aware"" contextual bandits.

The experimental section is solid. The method is evaluated on several RL environments against state of the art RL algorithms. It is also evaluated on bandit structured prediction tasks.
An interesting synthetic experiment (Figure 4) is also proposed to study the ability of the algorithm to work on both decomposable and non-decomposable structured prediction tasks.


Question 1: The credit assignment approach you propose seems way more sophisticated than eligibility traces in TD learning. But sometimes old and simple methods are not that bad. Could you develop a bit on the relation between RESLOPE and eligibility traces ?

Question 2: RESLOPE is built upon contextual bandits which require a stationary environment. Does RESLOPE inherit from this assumption?


Typos:
page 1 
""scalar loss that output."" -> ""scalar loss.""
"", effectively a representation"" -> "". By effective we mean effective in term of credit assignment.""
page 5
""and MTR"" -> ""and DR""
page 6
""in simultaneously."" -> ???
"".In greedy"" -> "". In greedy""
","[7, 6, 7]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Good paper, accept']","[2, 4, 5]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The reviewer provides a generally positive assessment, praising the paper as ""well-written"" and the experimental section as ""solid."" While pointing out that the theoretical grounding could be clearer and asking for elaboration on certain points, the reviewer's tone remains constructive and inquisitive rather than dismissive. The use of phrases like ""a bit thin"" and posing questions like ""Could you develop a bit...?"" suggests a desire for improvement rather than outright criticism.",60.0,80.0
Distributed Distributional Deterministic Policy Gradients,"['Gabriel Barth-Maron', 'Matthew W. Hoffman', 'David Budden', 'Will Dabney', 'Dan Horgan', 'Dhruva TB', 'Alistair Muldal', 'Nicolas Heess', 'Timothy Lillicrap']",Accept,2018,"[5, 14, 1, 2, 4, 2, 1, 10, 11]","[9, 19, 5, 7, 5, 5, 5, 15, 16]","[17, 46, 35, 67, 11, 6, 16, 200, 127]","[5, 21, 14, 30, 5, 1, 4, 81, 47]","[10, 22, 20, 36, 5, 5, 11, 111, 70]","[2, 3, 1, 1, 1, 0, 1, 8, 10]","The paper investigates a number of additions to DDPG algorithm and their effect on performance. The additions investigated are distributional Bellman updates, N-step returns, and prioritized experience replay.

The paper does a good job of analyzing these effects on a wide range of continuous control tasks, from the standard benchmark suite, to hand manipulation, to complex terrain locomotion and I believe these results are valuable to the community.

However, I have a concern about the soundness of using N-step returns in DDPG setting. When a sequence of length N is sampled from the replay buffer and used to calculate N-step return, this sequence is generated according a particular policy. As a result, experience is non-stationary - for the same state-action pair, early iterations of the algorithm will produce structurally different (not just due to stochasticity) N-step returns because the policy to generate those N steps has changed between algorithm iterations. So it seems to me the authors are using off-policy updates where strictly on-policy updates should be used. I would like some clarification from the authors on this point, and if it is indeed the case to bring attention to this point in the final manuscript.

It would also be useful to evaluate the effect of N for values other than 1 and 5, especially given the significance this addition has on performance. I can believe N-step returns are useful, possibly due to effectively enlarging simulation timestep, but it would be good to know at which point it becomes detrimental.

I also believe ""Distributional Policy Gradients"" is an overly broad title for this submission as this work still relies on off-policy updates and does not tackle the problem of marrying distributional updates with on-policy methods. ""Distributional DDPG"" or ""Distributional Actor-Critic"" or variant perhaps could be more fair title choices?

Aside from these concerns, lack of originality of contributions makes it difficult to highly recommend the paper. Nonetheless, I do believe the experimental evaluation if well-conducted and would be of interest to the ICLR community. ","[6, 9, 5]","[' Marginally above acceptance threshold', ' Top 15% of accepted papers, strong accept', ' Marginally below acceptance threshold']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer acknowledges the paper's valuable contributions and well-conducted experiments. However, they raise a significant concern about the soundness of using N-step returns in the DDPG setting, questioning its validity. While not outright negative, this concern significantly impacts the overall positivity. The reviewer suggests a title change, indicating the original title might be slightly misleading. Overall, the tone is constructive and helpful, leaning towards the positive side due to the appreciation for the experimental work.",40.0,80.0
Debiasing Evidence Approximations: On Importance-weighted Autoencoders and Jackknife Variational Inference,['Sebastian Nowozin'],Accept,2018,[12],[17],[121],[69],[42],[10],"This paper provides an interesting analysis of the importance sampled estimate of the LL bound and proposes to use Jackknife to correct for the bias. The experiments show that the proposed method works for model evaluation and that computing the correction is archivable at a reasonable computational cost. It also contains an insightful analysis.

","[7, 7, 6]","[' Good paper, accept', ' Good paper, accept', ' Marginally above acceptance threshold']","[3, 4, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review starts with positive wording like ""interesting analysis"", ""proposed method works"", ""insightful analysis"".  It does not contain any overly negative or critical language. Therefore, the sentiment is positive and the language is polite.",75.0,75.0
Towards Reverse-Engineering Black-Box Neural Networks,"['Seong Joon Oh', 'Max Augustin', 'Mario Fritz', 'Bernt Schiele']",Accept,2018,"[4, 2, 17, 25]","[9, 6, 22, 30]","[69, 14, 256, 531]","[28, 6, 128, 303]","[38, 8, 114, 166]","[3, 0, 14, 62]","The basic idea is to train a neural network to predict various hyperparameters of a classifier from input-output pairs for that classifier (kennen-o approach). It is surprising that some of these hyperparameters can even be predicted with more than chance accuracy. As a simple example, it's possible that there are values of batch size for which the classifiers may become indistinguishable, yet Table 2 shows that batch size can be predicted with much higher accuracy than chance. It would be good to provide insights into under what conditions and why hyperparameters can be predicted accurately. That would make the results much more interesting, and may even turn out to be useful for other problems, such as hyperparameter optimization.

The selection of the queries for kennen-o is not explained. What is the procedure for selecting the queries? How sensitive is the performance of kennen-o to the choice of the queries? One would expect that there is significant sensitivity, in which case it may even make sense to consider learning to select a sequence of queries to maximize accuracy.

In table 3, it would be useful to show the results for kennen-o as well, because Split-E seems to be the more realistic problem setting and kennen-o seems to be a more realistic attack than kennen-i or kennen-io.

In the ImageNet classifier family prediction, how different are the various families from each other? Without going through all the references, it is difficult to get a sense of the difficulty of the prediction task for a non-computer-vision reader.

Overall the results seem interesting, but without more insights it's difficult to judge how generally useful they are.","[7, 5, 7]","[' Good paper, accept', ' Marginally below acceptance threshold', ' Good paper, accept']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is cautiously positive. It finds the results interesting and acknowledges the surprising nature of some findings. However, it also raises several questions and asks for clarifications, suggesting that the current work lacks depth and generalizability. The reviewer offers constructive criticism and suggestions for improvement rather than outright dismissing the work.",50.0,80.0
Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning,"['Rajarshi Das', 'Shehzaad Dhuliawala', 'Manzil Zaheer', 'Luke Vilnis', 'Ishan Durugkar', 'Akshay Krishnamurthy', 'Alex Smola', 'Andrew McCallum']",Accept,2018,"[30, 4, 8, 6, 6, 9, 23, 29]","[35, 9, 13, 10, 10, 14, 28, 34]","[94, 25, 143, 32, 23, 148, 341, 392]","[65, 13, 69, 16, 12, 69, 216, 258]","[23, 11, 73, 16, 11, 73, 83, 113]","[6, 1, 1, 0, 0, 6, 42, 21]","The paper proposes an approach for query answering/link prediction in KBs that uses RL to navigate the KB graph between a query entity and a potential answer entity. The main originality is that, unlike random walk models, the proposed approach learns to navigate the graph while being conditioned on the query relation type.

I find the method sound and efficient and the proposed experiments are solid and convincing; for what they test for.

Indeed, for each relation type that one wants to be testing on, this type of approach needs many training examples of pairs of entities (say e_1, e_2) connected both by this relation type (e_1 R e_2) and by alternative paths (e_1 R' R'' R''' e_2). Because the model needs to discover and learn that R <=> R ' R'' R''' .

The proposed model seems to be able to do that well when the number of relation types remains low (< 50). But things get interesting in KBs when the number of relation types gets pretty large (hundreds / thousands). Learning the kind of patterns described above gets much trickier then. The results on FB15k are a bit worrying in that respect. Maybe this is a matter of the dataset FB15k itself but then having experiments on another dataset with hundreds of relation types could be important. 

NELL has indeed 200 relations but if I'm not mistaken, the NELL dataset is used for fact prediction and not query answering. And as noted in the paper, fact prediction is much easier.

","[6, 5, 7]","[' Marginally above acceptance threshold', ' Marginally below acceptance threshold', ' Good paper, accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer finds the method sound, efficient, and the experiments convincing. However, they express concern about the model's performance with a large number of relation types, as indicated by the results on FB15k. The reviewer suggests additional experiments on datasets with hundreds of relation types to address this concern. The language used is constructive and professional.",60.0,80.0
Boosting the Actor with Dual Critic,"['Bo Dai', 'Albert Shaw', 'Niao He', 'Lihong Li', 'Le Song']",Accept,2018,"[16, 2, 7, 16, 15]","[21, 6, 12, 20, 20]","[315, 12, 83, 170, 343]","[147, 6, 36, 89, 185]","[135, 6, 42, 66, 122]","[33, 0, 5, 15, 36]","This paper studies a new architecture DualAC. The author give strong and convincing justifications based on the Lagrangian dual of the Bellman equation (although not new, introducing this as the justification for the architecture design is plausible).

There are several drawbacks of the current format of the paper:
1. The algorithm is vague. Alg 1 line 5: 'closed form': there is no closed form in Eq(14). It is just an MC approximation.
line 6: Decay O(1/t^\beta). This is indeed vague albeit easy to understand. The algorithm requires that every step is crystal clear.

2. Also, there are several format error which may be due to compiling, e.g., line 2 of Abstract,'Dual-AC ' (an extra space). There are many format errors like this throughout the paper. The author is suggested to do a careful format check.

3. The author is suggested to explain more about the necessity of introducing path regularization and SDA. The current justification is reasonable but too brief.

4. The experimental part is ok to me, but not very impressive.

Overall, this seems to be a nice paper to me.","[7, 6, 5]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Marginally below acceptance threshold']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer finds the paper's core idea plausible and interesting, stating ""this seems to be a nice paper to me."" However, they also point out several areas for improvement, particularly regarding clarity, formatting, and a somewhat underwhelming experimental section. This suggests an overall positive but reserved sentiment, as the reviewer sees potential but also significant room for improvement. The language used is generally constructive and polite, employing suggestions like ""The author is suggested to..."" rather than harsh criticisms.",50.0,75.0
Mastering the Dungeon: Grounded Language Learning by Mechanical Turker Descent,"['Zhilin Yang', 'Saizheng Zhang', 'Jack Urbanek', 'Will Feng', 'Alexander Miller', 'Arthur Szlam', 'Douwe Kiela', 'Jason Weston']",Accept,2018,"[16, 6, 2, 2, 3, 10, 6, 20]","[21, 6, 7, 6, 7, 15, 11, 25]","[82, 27, 24, 6, 24, 138, 158, 237]","[34, 13, 9, 4, 11, 54, 82, 126]","[35, 13, 15, 2, 13, 74, 72, 84]","[13, 1, 0, 0, 0, 10, 4, 27]","The paper provides an interesting data collection scheme that improves upon standard collection of static databases that have multiple shortcomings -- End of Section 3 clearly summarizes the advantages of the proposed algorithm. The paper is easy to follow and the evaluation is meaningful.

In MTD, both data collection and training the model are intertwined and so, the quality of the data can be limited by the learning capacity of the model. It is possible that after some iterations, the data distribution is similar to previous rounds in which case, the dataset becomes similar to static data collection (albeit at a much higher cost and effort). Is this observed ? Further, is it possible to construct MTD variants that lead to constantly improving datasets by being agnostic to the actual model choice ? For example, utilizing only the priors of the D_{train_all}, mixing model and other humans' predictions, etc.



","[8, 7, 7]","[' Top 50% of accepted papers, clear accept', ' Good paper, accept', ' Good paper, accept']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with positive statements, highlighting the interesting aspects and strengths of the paper. The reviewer acknowledges the value of the proposed algorithm and finds the paper well-written and the evaluation relevant. However, the reviewer also raises important questions and suggests potential improvements, indicating that there is room for the paper to be strengthened. Overall, the feedback is constructive and encouraging further development. Therefore, the sentiment is moderately positive. The language used is entirely professional and respectful, suggesting areas for improvement without resorting to any negativity or harsh criticism. Thus, the politeness is very high.",60.0,90.0
Policy Optimization by Genetic Distillation ,"['Tanmay Gangwani', 'Jian Peng']",Accept,2018,"[3, 10]","[8, 15]","[23, 166]","[11, 84]","[12, 62]","[0, 20]","The authors present an algorithm for training ensembles of policy networks that regularly mixes different policies in the ensemble together by distilling a mixture of two policies into a single policy network, adding it to the ensemble and selecting the strongest networks to remain (under certain definitions of a ""strong"" network). The experiments compare favorably against PPO and A2C baselines on a variety of MuJoCo tasks, although I would appreciate a wall-time comparison as well, as training the ""crossover"" network is presumably time-consuming.

It seems that for much of the paper, the authors could dispense with the genetic terminology altogether - and I mean that as a compliment. There are few if any valuable ideas in the field of evolutionary computing and I am glad to see the authors use sensible gradient-based learning for GPO, even if it makes it depart from what many in the field would consider ""evolutionary"" computing. Another point on terminology that is important to emphasize - the method for training the crossover network by direct supervised learning from expert trajectories is technically not imitation learning but behavioral cloning. I would perhaps even call this a distillation network rather than a crossover network. In many robotics tasks behavioral cloning is known for overfitting to expert trajectories, but that may not be a problem in this setting as ""expert"" trajectories can be generated in unlimited quantities.","[6, 8, 3]","[' Marginally above acceptance threshold', ' Top 50% of accepted papers, clear accept', ' Clear rejection']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer provides a generally positive overview of the paper, praising the algorithm and its performance compared to baselines. While they suggest areas for improvement (e.g., wall-time comparison) and offer constructive criticism on terminology, the overall tone is supportive and encouraging. The reviewer's preference for gradient-based learning over evolutionary approaches doesn't detract from their positive view of the paper's contribution.",65.0,75.0
Deep Neural Networks as Gaussian Processes,"['Jaehoon Lee', 'Yasaman Bahri', 'Roman Novak', 'Samuel S. Schoenholz', 'Jeffrey Pennington', 'Jascha Sohl-Dickstein']",Accept,2018,"[2, 7, 5, 12, 26, 6]","[7, 12, 10, 16, 30, 10]","[21, 62, 69, 93, 310, 34]","[8, 27, 26, 55, 184, 11]","[12, 27, 39, 30, 66, 17]","[1, 8, 4, 8, 60, 6]","This paper presents a new covariance function for Gaussian processes (GPs) that is equivalent to a Bayesian deep neural network with a Gaussian prior on the weights and an infinite width. As a result, exact Bayesian inference with a deep neural network can be solved with the standard GP machinery.


Pros:

The result highlights an interesting relationship between deep nets and Gaussian processes. (Although I am unsure about how much of the kernel design had already appeared outside of the GP literature.)

The paper is clear and very well written.

The analysis of the phases in the hyperparameter space is interesting and insightful. On the other hand, one of the great assets of GPs is the powerful way to tune their hyperparameters via maximisation of the marginal likelihood but the authors have left this for future work!


Cons:

Although the computational complexity of computing the covariance matrix is given, no actual computational times are reported in the article.

I suggest using the same axis limits for all subplots in Figure 3.","[7, 6, 4]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Ok but not good enough - rejection']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is overall positive. The reviewer highlights the pros of the paper, such as the interesting result, clear writing, and insightful analysis. While it points out some cons (missing computational times and a suggestion for improvement in figure presentation), these are presented constructively and do not overshadow the positive aspects. ",60.0,80.0
META LEARNING SHARED HIERARCHIES,"['Kevin Frans', 'Jonathan Ho', 'Xi Chen', 'Pieter Abbeel', 'John Schulman']",Accept,2018,"[2, 9, 4, 17, 8]","[6, 14, 8, 22, 13]","[10, 50, 47, 608, 66]","[3, 20, 21, 291, 27]","[7, 26, 24, 291, 37]","[0, 4, 2, 26, 2]","Please see my detailed comments in the ""official comment""

The extensive revisions addressed most of my concerns

Quality
======
The idea is interesting, the theory is hand-wavy at best (ADDRESSED but still a bit vague), the experiments show that it works but don't evaluate many interesting/relevant aspects (ADDRESSED). It is also unclear how much tuning is involved (ADDRESSED).

Clarity
=====
The paper reads OK. The general idea is clear but the algorithm is only provided in vague text form (and actually changing from sequential to asynchronous without any justification why this should work) (ADDRESSED) leaving many details up the the reader's best guess (ADDRESSED).

Originality
=========
The idea looks original.

Significance
==========
If it works as advertised this approach would mean a drastic speedup on previously unseen task from the same distribution.

Pros and Cons
============
+ interesting idea
- we do everything asynchronously and in parallel and it magically works (ADDRESSED)
- many open questions / missing details (ADDRESSED)","[6, 7, 4]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Ok but not good enough - rejection']","[3, 3, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer acknowledges the interesting idea and potential significance of the work. While they initially had concerns about the theory, clarity, and missing details, they state that these have been addressed through revisions. The use of ""magically works"" is somewhat informal but is counterbalanced by the generally constructive tone and the acknowledgment of the revisions.",60.0,70.0
PixelDefend: Leveraging Generative Models to Understand and Defend against Adversarial Examples,"['Yang Song', 'Taesup Kim', 'Sebastian Nowozin', 'Stefano Ermon', 'Nate Kushman']",Accept,2018,"[4, 10, 12, 10, 12]","[8, 15, 17, 15, 16]","[64, 34, 121, 406, 33]","[31, 16, 69, 199, 19]","[33, 17, 42, 200, 13]","[0, 1, 10, 7, 1]","The authors propose to use a generative model of images to detect and defend against adverarial examples. White-box attacks against standard models for image recognition (Resnet and VGG) are considered, and a generative model (a PixelCNN) is trained on the same data as the classifiers. The authors first show that adversarial examples created by the white-box attacks correspond to low likelihood region (according to the pixelCNN), which first gives a classification rule for detecting adversarial examples.

Then, to turn the genrative model into a defensive algorithm, the authors propose to preprocess test images by approximately maximizing the likelihood under similar constraints as the attacker of images, to ""project"" adversarial examples back to high-density regions (as estimated by the generative model). As a heuristic method, the authors propose to greedily maximize the likelihood of the incoming images pixel-by-pixel, which is possible because of the specific form of the PixelCNN likelihood in the context of l-infty attacks. An ""adaptive"" version of the algorithm, in which the preprocessing is used only when the likelihood of an example is below a certain threshold, is also proposed.

Experiments are carried out on Fashion MNIST and CIFAR-10. At a high level, the message is that projecting the image into a high density region is sufficient to correct for a significant portions of the mistakes made on adversarial examples. The main result is that this approach based on generative models seems to work even on against the strongest attacks.

Overall, the idea proposed in the paper, using a generative model to detect and filter out spurious patterns that can appear in adversarial examples, is rather intuitive. The experimental result that adversarial examples can somehow be corrected by a generative model is also interesting. The design choice of PixelCNN, which allows for a greedy optimization seems reasonable in that setting.

Whereas the paper is an interesting step forward, the paper still doesn't provide definitive arguments in favor of using such approaches in practice. There is a significant loss in accuracy on clean examples (2% on CIFAR-10 for a resnet), and more generally against weaker opponents such as the fast gradient sign. Thus, in reality, the experiments show that the pipeline generative model + classifier is robust against the strongest white box methods for this classifier, but on the other hand these methods do not transfer well to new models. This somewhat weakens the result, since robustness against these methods that do not transfer well is achieved by changing the model. 
","[7, 7, 7]","[' Good paper, accept', ' Good paper, accept', ' Good paper, accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with a neutral tone, presenting the authors' method and findings objectively. While acknowledging the novelty and interesting results, the reviewer raises concerns about the practicality and limitations of the approach. They point out a significant accuracy loss on clean examples and against weaker attacks, suggesting the method might not be universally applicable. The tone remains professional and respectful throughout, but the concluding remarks highlighting the weaknesses temper the initial positivity, leading to a more cautious and less enthusiastic overall sentiment.",40.0,80.0
Ensemble Adversarial Training: Attacks and Defenses,"['Florian Tramèr', 'Alexey Kurakin', 'Nicolas Papernot', 'Ian Goodfellow', 'Dan Boneh', 'Patrick McDaniel']",Accept,2018,"[4, 8, 5, 10, 26, 20]","[9, 13, 10, 12, 31, 25]","[87, 37, 150, 107, 363, 252]","[33, 13, 56, 48, 207, 154]","[49, 22, 88, 55, 121, 46]","[5, 2, 6, 4, 35, 52]","This paper describes computationally efficient methods for training adversarially robust deep neural networks for image classification. (These methods may extend to other machine learning models and domains as well, but that's beyond the scope of this paper.) 

The former standard method for generating adversarially images quickly and using them in training was to do a single gradient step to increase the loss of the true label or decrease the loss of an alternate label. This paper shows that such training methods only lead to robustness against these ""weak"" adversarial examples, leaving the adversarially-trained models vulnerable to multi-step white-box attacks and black-box attacks (adversarial examples generated to attack alternate models).

There are two proposed solutions. The first is to generate additional adversarial examples from other models and use them in training. This seems to yield robustness against black-box attacks from held-out models as well.  Of course, it requires that you have a somewhat diverse group of models to choose from. If that's the case, why not directly build an ensemble of all the models? An ensemble of neural networks can still be represented as a neural network, although a more computationally costly one. Thus, while this heuristic appears to be useful with current models against current attacks, I don't know how well it will hold up in the future.

The second solution is to add random noise before taking the gradient step.  This yields more effective adversarial examples, both for attacking models and for training, because it relies less on the local gradient. This is another simple idea that appears to be effective. However, I would be interested to see a comparison to a 2-step gradient-based attack.  R+Step-LL can be viewed as a 2-step attack: a random step followed by a gradient step. What if both steps were gradient steps instead? This interpolates between Step-LL and I-Step-LL, with an intermediate computational cost. It would be very interesting to know if R+Step-LL is more or less effective than 2+Step-LL, and how large the difference is.

I like that this paper demonstrates the weakness of previous methods, including extensive experiments and a very nice visualization of the loss landscape in two adversarial dimensions. The proposed heuristics seem effective in practice, but they're somewhat ad hoc and there is no analysis of how these heuristics might or might not be vulnerable to future attacks.","[6, 6, 6]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[4, 2, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer acknowledges the value of the paper's findings, particularly praising the visualization of the loss landscape and the demonstration of weaknesses in previous methods. This suggests a positive sentiment. However, they also raise concerns about the ad-hoc nature of the proposed solutions and their long-term effectiveness, which tempers the positivity. Therefore, the sentiment is likely positive but with reservations. The language used is analytical and objective, employing terms like ""ad hoc"" and ""heuristics"" without being disrespectful. The reviewer also offers constructive criticism and suggestions for further research, indicating a polite and professional tone.",60.0,80.0
DCN+: Mixed Objective And Deep Residual Coattention for Question Answering,"['Caiming Xiong', 'Victor Zhong', 'Richard Socher']",Accept,2018,"[10, 4, 12]","[15, 8, 17]","[383, 37, 229]","[165, 18, 111]","[208, 19, 111]","[10, 0, 7]","This paper proposed an improved version of dynamic coattention networks, which is used for question answering tasks. Specifically, there are 2 aspects to improve DCN: one is to use a mixed objective that combines cross entropy with self-critical policy learning, the other one is to imporve DCN with deep residual coattention encoder. The proposed model achieved STOA performance on Stanford Question Asnwering Dataset and several ablation experiments show the effectiveness of these two improvements. Although DCN+ is an improvement of DCN, I think the improvement is not incremental. 

One question is that since the model is compicated, will the authors release the source code to repeat all the experimental results?","[6, 8, 7]","[' Marginally above acceptance threshold', ' Top 50% of accepted papers, clear accept', ' Good paper, accept']","[4, 2, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is confident but not absolutely certain that the evaluation is correct']",,,
All-but-the-Top: Simple and Effective Postprocessing for Word Representations,"['Jiaqi Mu', 'Pramod Viswanath']",Accept,2018,"[4, 23]","[4, 28]","[10, 262]","[5, 104]","[4, 99]","[1, 59]","This paper proposes a simple post-processing technique for word representations designed to improve representational quality and performance on downstream tasks. The procedure involves mean subtraction followed by projecting out the first D principle directions and is motivated by improving isotropy of the partition function. Extensive empirical analysis supports the efficacy of the approach.

The idea of post-processing word embeddings to improve their performance is not new, but I believe the specific procedure and its connection to the concept of isotropy has not been investigated previously. Relative to other post-processing techniques, this method has a fair amount of theoretical justification, particularly as described in Appendix A. I think the experiments are reasonably comprehensive. All told, I think this is a good paper, but I do have some comments and questions that I think should be addressed before publication.

1) I think it is useful to analyze the distribution of singular values of the matrix of word vectors. However, I did not find the heuristic analysis based on the visual appearance of these distributions to be convincing. For example, in Fig. 1, it is not clear to me that there exists a separation between regimes of exponential decay and rough constancy. It would be ideal if a more quantitative metric is established that captures the main qualitative behavior alluded to here.

Furthermore, the vocabulary size is likely to have a strong effect on the shape of the distributions. Are the plots in Fig. 4 for the same vocabulary size? Related to this, the dimensionality of the representation will have a strong effect on the shape, and this should be controlled for in Fig. 8. One way to do this would be to instead plot the density of singular values. Finally, for the Gaussian matrix simulations, in the asymptotic limit, the density of singular values depends only on the ratio of dimensions, i.e. the vector dimension to the vocabulary size. Fig. 4/8 might be more revealing if this ratio were controlled for.

2) It would be useful to describe why isotropy of the partition function is the goal, as opposed to isotropy of the vectors themselves. This may be argued in Arora et al. (2016), but summarizing that argument in this paper would be helpful. In fact, an additional experiment that would be very valuable would be to investigate empirically which form of isotropy is more effective in governing performance. One way to do this would be to enforce approximate isotropy of the partition function without also enforcing isotropy of the vectors themselves. Practically speaking, one might imagine doing this by requiring I = 1 to second order without also requiring that the mean vanish. I think this would allow for \sigma_max > \sigma_min while still satisfying I = 1 to second order. (But this is just off the top of my head -- there may be better ways to conduct this experiment).

It is not clear to me why the experiment leading to Table 2 is a good proxy for the exact computation of I. It would be great if there were some mathematical justification for this approximation.

Why does Fig. 3 use D=10, 20 when much smaller D are considered elsewhere? Also I think a log scale on the x-axis might be more informative.

3) It would be good to mention other forms of post-processing, especially in the context of word similarity. For example, in the original paper, GloVe advocates averaging the target and context vector representations, and normalizing across the feature dimension before computing cosine similarity.

4) I think it's likely that there is a strong connection between the optimal value of D and the frequency distribution of words in the evaluation dataset. While the paper does mention that D may depend on specifics of the dataset, etc., I would expect frequency-dependence to be the main factor, and it might be worth exploring this effect explicitly.
","[7, 7, 6]","[' Good paper, accept', ' Good paper, accept', ' Marginally above acceptance threshold']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The reviewer provides constructive criticism, suggesting improvements and further investigations. They express positive sentiments like ""good paper"", ""reasonably comprehensive"", and ""fair amount of theoretical justification."" However, they also point out areas that need clarification or stronger evidence. Overall, the tone is balanced and objective.",60.0,80.0
Deep Rewiring: Training very sparse deep networks,"['Guillaume Bellec', 'David Kappel', 'Wolfgang Maass', 'Robert Legenstein']",Accept,2018,"[5, 5, 42, 19]","[9, 10, 46, 23]","[18, 22, 214, 61]","[8, 4, 76, 23]","[9, 12, 46, 20]","[1, 6, 92, 18]","This paper presents an iterative approach to sparsify a network already during training. During the training process, the amount of connections in the network is guaranteed to stay under a specific threshold. This is a big advantage when training is performed on hardware with computational limitations, in comparison to ""post-hoc"" sparsification methods, that compress the network after training.
The method is derived by considering the ""rewiring"" of an (artificial) neural network as a stochastic process. This perspective is based on a recent model in computational biology but also can be interpreted as a (sequential) monte carlo sampling based stochastic gradient descent approach. References to previous work in this area are missing, e.g.

[1] de Freitas et al., Sequential Monte Carlo Methods to Train Neural Network
Models, Neural Computation 2000
[2] Welling et al., Bayesian Learning via Stochastic Gradient Langevin Dynamics, ICML 2011

Especially the stochastic gradient method in [2] is strongly related to the existing approach.

Positive aspects

- The presented approach is well grounded in the theory of stochastic processes. The authors provide proofs of convergence by showing that the iterative updates converge to a fixpoint of the stochastic process

- By keeping the temperature parameter of the stochastic process high, it can be directly applied to online transfer learning.

- The method is specifically designed for online learning with limited hardware ressources.

Negative aspects

- The presented approach is outperformed for moderate compression levels (by Han's pruning method for >5% connectivity on MNIST, Fig. 3 A, and by l1-shrinkage for >40% connectivity on CIFAR-10 and TIMIT, Fig. 3 B&C). Especially the results on MNIST suggest that this method is most advantageous for very high compression levels. However in these cases the overall classification accuracy has already dropped significantly which could limit the practical applicability.

- A detailled discussion of the relation to previously existing very similar work is missing (see above)


Technical Remarks

Fig. 1, 2 and 3 are referenced on the pages following the page containing the figure. Readibility could be slightly increased by putting the figures on the respective pages.
","[6, 8, 5]","[' Marginally above acceptance threshold', ' Top 50% of accepted papers, clear accept', ' Marginally below acceptance threshold']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review acknowledges the novelty and theoretical grounding of the paper, highlighting its advantages for online learning and limited hardware. This suggests a positive sentiment. However, it also points out that the method is outperformed by existing techniques in certain scenarios and criticizes the lack of discussion on related work, indicating some negative aspects. The tone remains professional and polite throughout, suggesting a neutral-to-positive politeness level. Overall, the balance leans slightly towards the positive side due to the acknowledgment of the paper's merits.",20.0,50.0
Communication Algorithms via Deep Learning,"['Hyeji Kim', 'Yihan Jiang', 'Ranvir B. Rana', 'Sreeram Kannan', 'Sewoong Oh', 'Pramod Viswanath']",Accept,2018,"[9, 1, 1, 9, 11, 23]","[14, 6, 5, 14, 16, 28]","[73, 25, 10, 132, 196, 262]","[35, 12, 5, 53, 80, 104]","[27, 10, 4, 62, 89, 99]","[11, 3, 1, 17, 27, 59]","Error-correcting codes constitute a well-researched area of study within communication engineering. In communication, messages that are to be transmitted are encoded into binary vector called codewords that contained some redundancy. The codewords are then transmitted over a channel that has some random noise. At the receiving end the noisy codewords are then decoded to recover the messages. Many well known families of codes exist, notably convolutional codes and Turbo codes, two code families that are central to this paper, that achieve the near optimal possible performance with efficient algorithms. For Turbo and convolutional codes the efficient MAP decodings are known as Viterbi decoder and the BCJR decoder. For drawing baselines, it is assumed that the random noise in channel is additive Gaussian (AWGN).

This paper makes two contributions. First, recurrent neural networks (RNN) are proposed to replace the Viterbi and BCJR algorithms for decoding of convolutional and Turbo decoders. These decoders are robust to changes in noise model and blocklength - and shows near optimal performance.

It is unclear to me what is the advantage of using RNNs instead of Viterbi or BCJR, both of which are optimal, iterative and runs in linear time. Moreover the authors point out that RNNs are shown to emulate BCJR and Viterbi decodings in prior works - in light of that, why their good performance surprising?

The second contribution of the paper constitutes the design and decoding of codes based on RNNs for a Gaussian channel with noisy feedback. For this channel the optimal codes are unknown. The authors propose an architecture to design codes for this channel. This is a nice step. However, in the performance plot (figure 8), the RNN based code-decoder does not seem to be outperforming the existing codes except for two points. For both in high and low SNR the performance is suboptimal to  Turbo codes and a code by Schalkwijk & Kailath. The section is also super-concise to follow. I think it was necessary to introduce an LSTM encoder - it was hard to understand the overall encoder. This is an issue with the paper - the authors previously mentioned (8,16) polar code without mentioning what the numbers mean. 

However, I overall liked the idea of using neural nets to design codes for some non-standard channels. While at the decoding end it does not bring in anything new (modern coding theory already relies on iterative decoders, that are super fast), at the designing-end the Gaussian feedback channel part can be a new direction. This paper lacks theoretical aspect, as to no indication is given why RNN based design/decoders can be good. I am mostly satisfied with the experiments, barring Fig 8, which does not show the results that the authors claim.
","[6, 2, 9]","[' Marginally above acceptance threshold', ' Strong rejection', ' Top 15% of accepted papers, strong accept']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The reviewer acknowledges the interesting application of RNNs for code design in non-standard channels, particularly the Gaussian feedback channel. However, they express concerns about the lack of novelty in using RNNs for decoding, as existing methods are already efficient. The reviewer also finds the performance in Figure 8 unconvincing and criticizes the paper's conciseness, making it difficult to understand certain aspects. Overall, the reviewer leans towards the positive side due to the potential of RNN-based code design but highlights significant areas for improvement.",40.0,50.0
Demystifying MMD GANs,"['Mikołaj Bińkowski', 'Danica J. Sutherland', 'Michael Arbel', 'Arthur Gretton']",Accept,2018,"[2, 7, 2, 17]","[6, 12, 7, 22]","[18, 59, 37, 201]","[9, 24, 18, 102]","[9, 33, 19, 74]","[0, 2, 0, 25]","The quality and clarity of this work are very good. The introduction of the kernel inception metric is well-motivated and novel, to my knowledge. With the mention of a bit more related work (although this is already quite good), I believe that this could be a significant resource for understanding MMD GANs and how they fit into the larger model zoo.

Pros
 - best description of MMD GANs that I have encountered
 - good contextualization of related work and descriptions of relationships, at least among the works surveyed
 - reasonable proposed metric (KID) and comparison with other scores
 - proof of unbiased gradient estimates is a solid contribution

Cons
 - although the review of related work is very good, it does focus on ~3 recent papers. As a review, it would be nice to see mention (even just in a list with citations) of how other models in the zoo fit in
 - connection between IPMs and MMD gets a bit lost; a figure (e.g.  flow chart) would help
 - wavers a bit between proposing/proving novel things vs. reviewing and lacks some overall structure/storyline
 - Figure 1 is a bit confusing; why is KID tested without replacement, and FID with? Why 100 vs 10 samples? The comparison is good to have, but it's hard to draw any insight with these differences in the subfigures. The figure caption should also explain what we are supposed to get out of looking at this figure.

Specific comments:
 - I suggest bolding terms where they are defined; this makes it easy for people to scan/find (e.g. Jensen-Shannon divergence, Integral Probability Metrics, witness functions, Wasserstein distance, etc.) 
 - Although they are common knowledge in the field, because this is a review it could be helpful to provide references or brief explanations of e.g. JSD, KL, Wasserstein distance, RKHS, etc.
 - a flow chart (of GANs, IPMs, MMD, etc., mentioning a few more models than are discussed in depth here, would be *very* helpful.
 - page 2, middle paragraph, you mention ""...constraints to ensure the kernel distribution embeddings remained injective""; it would be helpful to add a sentence here to explain why that's a good thing.
","[7, 6, 4]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Ok but not good enough - rejection']","[4, 2, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with a very positive statement about the quality and clarity of the work. It praises the novelty and potential impact of the research. The reviewer also lists several positive aspects (pros) before mentioning any areas for improvement (cons). While the cons section does point out some weaknesses, the suggestions are constructive and framed as opportunities for enhancement rather than harsh criticisms. The language throughout is professional and respectful.",75.0,90.0
"Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks","['Pratik Chaudhari', 'Stefano Soatto']",Accept,2018,"[10, 26]","[15, 31]","[77, 445]","[29, 241]","[42, 138]","[6, 66]","This paper develop theory to study the impact of stochastic gradient noise for SGD, especially for deep neural network models. It is shown that when the gradient noise is isotropic normal, SGD converges to a distribution tilted by the original objective function. However, when the gradient noise is non isotropic normal, which is shown common in many models especially in deep neural network models, the behavior of SGD is intriguing, which will not converge to the tilted distribution by the original objective function, sometimes more interestingly, will converge to limit cycles around some critical points of the original objective function. The paper also provides some hints on why using SGD can get good generalization ability than gradient descend.

I think the finding of this paper is interesting, and the technical details are correct. I still have the following comments.

First, Assumption 4 seems a bit too abstract. It is not easy to see what the assumption means. It would be better if an example is given, which is verified to satisfy the assumption.

Another comment is related to the overall content of this paper. Thought the paper point out that SGD will have the out-of-equilibrium behavior when the gradient noise is non isotropic normal, it remains to show how far away this stationary distribution is from the original distribution defined by the objective function.","[6, 8, 5]","[' Marginally above acceptance threshold', ' Top 50% of accepted papers, clear accept', ' Marginally below acceptance threshold']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer explicitly states ""I think the finding of this paper is interesting, and the technical details are correct."", which points towards a positive sentiment. However, they also provide constructive criticism, indicating there's room for improvement. Therefore, the sentiment is positive but not overly enthusiastic. The language used is polite and professional throughout, suggesting a respectful and constructive approach.",60.0,80.0
Distributed Fine-tuning of Language Models on Private Data,"['Vadim Popov', 'Mikhail Kudinov', 'Irina Piontkovskaya', 'Petr Vytovtov', 'Alex Nevidomsky']",Accept,2018,"[2, 4, 2, 2, 2]","[6, 8, 7, 3, 3]","[11, 22, 19, 4, 3]","[7, 10, 9, 3, 2]","[4, 11, 10, 1, 1]","[0, 1, 0, 0, 0]","This paper discusses the application of word prediction for software keyboards. The goal is to customize the predictions for each user to account for member specific information while adhering to the strict compute constraints and privacy requirements. 

The authors propose a simple method of mixing the global model with user specific data. Collecting the user specific models and averaging them to form the next global model. 

The proposal is practical. However, I am not convinced that this is novel enough for publication at ICLR. 

One major question. The authors assume that the global model will depict general english. However, it is not necessary that the population of users will adhere to general English and hence the averaged model at the next time step t+1 might be significantly different from general English. It is not clear to me as how this mechanism guarantees that it will not over-fit or that there will be no catastrophic forgetting.","[4, 5, 4]","[' Ok but not good enough - rejection', ' Marginally below acceptance threshold', ' Ok but not good enough - rejection']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer acknowledges the practicality of the proposal but expresses concerns about its novelty and raises a significant question about the model's potential for overfitting and catastrophic forgetting. The use of phrases like ""I am not convinced"" and ""It is not clear to me"" suggests skepticism rather than outright negativity. The language is professional and avoids harsh criticism.",-20.0,70.0
Noisy Networks For Exploration,"['Meire Fortunato', 'Mohammad Gheshlaghi Azar', 'Bilal Piot', 'Jacob Menick', 'Matteo Hessel', 'Ian Osband', 'Alex Graves', 'Volodymyr Mnih', 'Remi Munos', 'Demis Hassabis', 'Olivier Pietquin', 'Charles Blundell', 'Shane Legg']",Accept,2018,"[4, 9, 7, 2, 5, 6, 15, 13, 23, 10, 17, 9, 19]","[8, 14, 12, 6, 9, 11, 18, 17, 28, 15, 22, 14, 24]","[14, 47, 60, 21, 51, 53, 77, 54, 266, 54, 231, 88, 78]","[5, 19, 31, 9, 25, 18, 42, 27, 143, 10, 143, 35, 23]","[8, 26, 27, 12, 26, 32, 27, 24, 96, 26, 74, 48, 48]","[1, 2, 2, 0, 0, 3, 8, 3, 27, 18, 14, 5, 7]","A new exploration method for deep RL is presented, based on the idea of injecting noise into the deep networks’ weights. The noise may take various forms (either uncorrelated or factored) and its magnitude is trained by gradient descent along other parameters. It is shown how to implement this idea both in DQN (and its dueling variant) and A3C, with experiments on Atari games showing a significant improvement on average compared to these baseline algorithms.

This definitely looks like a worthy direction of research, and experiments are convincing enough to show that the proposed algorithms indeed improve on their baseline version. The specific proposed algorithm is close in spirit to the one from “Parameter space noise for exploration”, but there are significant differences. It is also interesting to see (Section 4.1) that the noise evolves in non-obvious ways across different games.

I have two main concerns about this submission. The first one is the absence of a comparison to the method from “Parameter space noise for exploration”, which shares similar key ideas (and was published in early June, so there was enough time to add this comparison by the ICLR deadline). A comparison to the paper(s) by Osband et al (2016, 2017) would have also been worth adding. My second concern is that I find the title and overall discussion in the paper potentially misleading, by focusing only on the “exploration” part of the proposed algorithm(s). Although the noise injected in the parameters is indeed responsible for the exploration behavior of the agent, it may also have an important effect on the optimization process: in both DQN and A3C it modifies the cost function being optimized, both through the “target” values (respectively Q_hat and advantage) and the parameters of the policy (respectively Q and pi). Since there is no attempt to disentangle these exploration and optimization effects, it is unclear if one is more important than the other to explain the success of the approach. It also sheds doubt on the interpretation that the agent somehow learns some kind of optimal exploration behavior through gradient descent (something I believe is far from obvious).

Estimating the impact of a paper on future research is an important factor in evaluating it. Here, I find myself in the akward (and unusual to me) situation where I know the proposed approach has been shown to bring a meaningful improvement, more precisely in Rainbow (“Rainbow: Combining Improvements in Deep Reinforcement Learning”). I am unsure whether I should take it into account in this review, but in doubt I am choosing to, which is why I am advocating for acceptance in spite of the above-mentioned concerns.

A few small remarks / questions / typos:
- In eq. 3 A(...) is missing the action a as input
- Just below: “the the”
- Last sentence of p. 3 can be misleading because the gradient is not back-propagated through all paths in the defined cost
- “In our experiments we used f(x) = sgn(x) p |x|”: this makes sense to me for eq. 9 but why not use f(x) = x in eq. 10?
- Why use factored noise in DQN and independent noise in A3C? This is presented like an arbitrary choice here.
- What is the justification for using epsilon’ instead of epsilon in eq. 15? My interpretation of double DQN is that we want to evaluate (with the target network) the action chosen by the Q network, which here is perturbed with epsilon (NB: eq. 15 should have b in the argmax, not b*)
- Section 4 should say explicitly that results are over 200M frames
- Assuming the noise is sampled similarly doing evaluation (= as in training), please mention it clearly.
- In paragraph below eq. 18: “superior performance compare to their corresponding baselines”: compared
- There is a Section 4.1 but no 4.2
- Appendix has a lot of redundant material with the main text, for instance it seems to me that A.1 is useless.
- In appendix B: “σi,j is simply set to 0.017 for all parameters” => where does this magic value come from?
- List x seems useless in C.1 and C.2
- C.1 and C.2 should be combined in a single algorithm with a simple “if dueling” on l. 24
- In C.3: (1) missing pi subscript for zeta in the “Output:” line, (2) it is not clear what the zeta’ parameters are for, in particular should they be used in l. 12 and 22?
- The paper “Dropout as a Bayesian approximation” seems worth at least  adding to the list of related work in the introduction.","[6, 5, 7]","[' Marginally above acceptance threshold', ' Marginally below acceptance threshold', ' Good paper, accept']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with positive sentiment, acknowledging the worthiness of the research direction and the convincing experiments. However, it also raises significant concerns, such as the lack of comparison with a key related work and the potential misleading focus on ""exploration"" without disentangling its effects from optimization. The reviewer acknowledges the impact of the paper based on its inclusion in a subsequent work (""Rainbow""). The review concludes with numerous detailed remarks, questions, and typo observations, indicating a thorough and critical analysis. Overall, the sentiment leans towards the positive side due to the paper's impact and the constructive nature of the criticism, but the numerous concerns prevent it from being overwhelmingly positive. The language used is polite and professional throughout, maintaining a respectful and constructive tone even when pointing out flaws or suggesting improvements.",60.0,90.0
Neural-Guided Deductive Search for Real-Time Program Synthesis from Examples,"['Ashwin Kalyan', 'Abhishek Mohta', 'Oleksandr Polozov', 'Dhruv Batra', 'Prateek Jain', 'Sumit Gulwani']",Accept,2018,"[1, 1, 5, 11, 15, 18]","[6, 4, 9, 16, 20, 23]","[24, 3, 37, 292, 216, 182]","[8, 1, 16, 134, 109, 120]","[15, 2, 18, 142, 93, 36]","[1, 0, 3, 16, 14, 26]","This is a strong paper. It focuses on an important problem (speeding up program synthesis), it’s generally very well-written, and it features thorough evaluation. The results are impressive: the proposed system synthesizes programs from a single example that generalize better than prior state-of-the-art, and it does so ~50% faster on average.

In Appendix C, for over half of the tasks, NGDS is slower than PROSE (by up to a factor of 20, in the worst case). What types of tasks are these? In the results, you highlight a couple of specific cases where NGDS is significantly *faster* than PROSE—I would like to see some analysis of the cases were it is slower, as well. I do recognize that in all of these cases, PROSE is already quite fast (less than 1 second, often much less) so these large relative slowdowns likely don’t lead to a noticeable absolute difference in speed. Still, it would be nice to know what is going on here.

Overall, this is a strong paper, and I would advocate for accepting it.


A few more specific comments:


Page 2, “Neural-Guided Deductive Search” paragraph: use of the word “imbibes” - while technically accurate, this use doesn’t reflect the most common usage of the word (“to drink”). I found it very jarring.

The paper is very well-written overall, but I found the introduction to be unsatisfyingly vague—it was hard for me to evaluate your “key observations” when I couldn’t quite yet tell what the system you’re proposing actually does. The paragraph about “key observation III” finally reveals some of these details—I would suggest moving this much earlier in the introduction.

Page 4, “Appendix A shows the resulting search DAG” - As this is a figure accompanying a specific illustrative example, it belongs in this section, rather than forcing the reader to hunt for it in the Appendix.

","[8, 6, 6]","[' Top 50% of accepted papers, clear accept', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[3, 3, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer explicitly states ""This is a strong paper"" and advocates for accepting it. They also point out the impressive results and thorough evaluation. While they do raise some questions and suggest improvements, these are presented constructively and aim to further strengthen the paper. The language used is formal and objective, typical of scientific peer reviews. There are no instances of harsh criticism or personal attacks.",85.0,90.0
Hyperparameter optimization: a spectral approach,"['Elad Hazan', 'Adam Klivans', 'Yang Yuan']",Accept,2018,"[16, 21, 16]","[21, 26, 21]","[219, 123, 104]","[103, 60, 47]","[93, 47, 34]","[23, 16, 23]","- algorithm 1 has a lot of problem specific hyperparametes that may be difficult to get right. Not clear how important they are
- they analyze the simpler (analytically and likely computationally) Boolean hyperparameter case (each hyperparameter is binary). Not a realistic setting. In their experiments they use these binary parameter spaces so I'm not sure how much I buy that it is straightforward to use continuous valued polynomials. 
- interesting idea but I think it's more theoretical than practical. Feels like a hammer in need of a nail. ","[6, 9, 6]","[' Marginally above acceptance threshold', ' Top 15% of accepted papers, strong accept', ' Marginally above acceptance threshold']","[3, 5, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer finds the idea interesting, but has several concerns regarding the practicality and realism of the approach. They point out limitations and express skepticism, which indicates a rather negative sentiment. However, the language remains technical and avoids direct negative adjectives, suggesting a neutral politeness level.",-50.0,0.0
Deep Learning with Logged Bandit Feedback,"['Thorsten Joachims', 'Adith Swaminathan', 'Maarten de Rijke']",Accept,2018,"[22, 7, 28]","[27, 11, 33]","[220, 44, 826]","[125, 25, 522]","[56, 17, 172]","[39, 2, 132]","Learning better policies from logged bandit feedback is a very important problem, with wide applications in internet, e-commerce and anywhere it is possible to incorporate controlled exploration. The authors study the problem of learning the best policy from logged bandit data. While this is not a brand new problem, the important and relevant contribution that the authors make is to do this using policies that can be learnt via neural networks. The authors are motivated by two main applications: (i) multi-class classification problems with bandit feedback (ii) ad placements problem in the contextual bandit setting. 

The main contributions of the authors is to design an output layer that allows training on logged bandit feedback data. Traditionally in the full feedback setting (setting where one gets to see the actual label and not just if our prediction is correct or incorrect) one uses cross-entropy loss function to optimize the parameters of a deep neural network. This does not work in a bandit setting, and previous work has developed various methods such as inverse-propensity scoring, self-normalized inverse propensity scoring, doubly robust estimators to handle the bandit setting. The authors in this paper work with self-normalized inverse propensity scoring as the technique to deal with bandit feedback data. the self normalized inverse propensity estimator (SNIPS) that the authors use is not a new estimator and has been previously studied in the work of Adith Swaminathan and co-authors. However, this estimator being a ratio is not an easy optimization problem to work with. The authors use a fairly standard reduction of converting ratio problems to a series of constrained optimization problems. This conversion of ratio problems to a series of constrained optimization problems is a standard textbook problem, and therefore not new. But, i like the authors handling of the constrained optimization problems via the use of Lagrangian constraints. It would have been great if the authors connected this to the REINFORCE algorithm of Williams. Unfortunately, the authors do not do a great job in establishing this connection, and I hope they do this in the full version of the paper.  The experimental results are fairly convincing and i really do not have any major comments. Here are my ""minor"" comments.

1. It would be great if the authors can establish connections to the REINFORCE algorithm in a more elaborate manner. It would be really instructive to the reader.

2.  On page 6,  the authors talk about lowering the learning rate and the learning rate schedule. I am guessing this is because of the intrinsic high variance of the problem. It would be great if the authors can explain in more detail why they did so.","[7, 6, 8]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Top 50% of accepted papers, clear accept']","[4, 3, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review is positive overall. The reviewer finds the problem interesting and the solution technically sound. They acknowledge the contribution of the authors, even if not entirely novel. The reviewer also praises the experimental results. While they have some suggestions for improvement, these are presented constructively and are aimed at further strengthening the paper. There are no negative remarks about the authors' work or approach. The language used is polite and professional throughout the review.",65.0,80.0
Few-shot Autoregressive Density Estimation: Towards Learning to Learn Distributions,"['Scott Reed', 'Yutian Chen', 'Thomas Paine', 'Aäron van den Oord', 'S. M. Ali Eslami', 'Danilo Rezende', 'Oriol Vinyals', 'Nando de Freitas']",Accept,2018,"[5, 10, 5, 7, 7, 8, 12, 19]","[9, 15, 9, 11, 11, 13, 17, 24]","[42, 84, 16, 74, 54, 90, 209, 199]","[20, 38, 10, 34, 18, 28, 101, 101]","[21, 32, 5, 36, 32, 58, 98, 84]","[1, 14, 1, 4, 4, 4, 10, 14]","This paper considers the problem of one/few-shot density estimation, using metalearning techniques that have been applied to one/few-shot supervised learning. The application is an obvious target for research and some relevant citations are missing, e.g. ""Towards a Neural Statistician"" (Edwards et al., ICLR 2017). Nonetheless, I think the current paper seems interesting enough to merit publication.

The paper is well-produced, i.e. the overall writing, visuals, and narrative flow are good. It was easy to read the paper straight through while understanding both the technical details and more intuitive motivations.

I have some concerns about the architectures and experiments presented in the paper. For architectures: the attention-based model seems powerful but difficult to scale to problems with more inputs for conditioning, and the meta PixelCNN model is a standard PixelCNN trained with the MAML approach by Finn et al. For experiments: the ImageNet flipping task is clearly tailored to the strengths of the attention-based model, and the presentation of the general Omniglot results could be improved. The image flipping experiment is neat, but the attention-based model's strong performance is unsurprising. I think the results in Tables 1/2 should be merged into a single table. It would make it clear that the MAML-based and attention-based models achieve similar performance on this task.

Overall, I think the paper makes a nice contribution. The paper could be improved significantly, e.g., by showing how to scale the attention-based architecture to problems with more data or by designing an architecture specifically for use with MAML-based inference.","[7, 6, 6]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer states that the paper is ""interesting enough to merit publication"" and makes a ""nice contribution."" This suggests a positive sentiment. However, the reviewer also lists several concerns, indicating that the paper is not perfect. Therefore, the sentiment is likely moderately positive. The language used is generally constructive and professional, suggesting a polite tone. While the reviewer points out flaws, they do so in a respectful and helpful manner.",60.0,70.0
TreeQN and ATreeC: Differentiable Tree-Structured Models for Deep Reinforcement Learning,"['Gregory Farquhar', 'Tim Rocktäschel', 'Maximilian Igl', 'Shimon Whiteson']",Accept,2018,"[2, 7, 2, 16]","[6, 12, 6, 21]","[37, 129, 29, 274]","[18, 61, 15, 142]","[18, 63, 13, 103]","[1, 5, 1, 29]","The authors propose a new network architecture for RL that contains some relevant inductive biases about planning. This fits into the recent line of work on implicit planning where forms of models are learned to be useful for a prediction/planning task. The proposed architecture performs something analogous to a full-width tree search using an abstract model (learned end-to-end). This is done by expanding all possible transitions to a fixed depth before performing a max backup on all expanded nodes. The final backup value is the Q-value prediction for a given state, or can represent a policy through a softmax.

I thought the paper was clear and well-motivated. The architecture (and various associated tricks like state vector normalization) are well-described for reproducibility. 

Experimental results seem promising but I wasn’t fully convinced of its conclusions. In both domains, TreeQN and AtreeC are compared to a DQN architecture, but it wasn’t clear to me that this is the right baseline. Indeed TreeQN and AtreeC share the same conv stack in the encoder (I think?), but also have the extra capacity of the tree on top. Can the performance gain we see in the Push task as a function of tree depth be explained by the added network capacity? Same comment in Atari, but there it’s not really obvious that the proposed architecture is helping. Baselines could include unsharing the weights in the tree, removing the max backup, having a regular MLP with similar capacity, etc.

Page 5, the auxiliary loss on reward prediction seems appropriate, but it’s not clear from the text and experiments whether it actually was necessary. Is it that makes interpretability of the model easier (like we see in Fig 5c)? Or does it actually lead to better performance?  

Despite some shortcomings in the result section, I believe this is good work and worth communicating as is.","[8, 5, 4]","[' Top 50% of accepted papers, clear accept', ' Marginally below acceptance threshold', ' Ok but not good enough - rejection']","[5, 3, 5]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The reviewer starts by praising the clarity, motivation, and description of the paper. Words like ""well-motivated"", ""well-described"", and ""promising"" indicate a positive sentiment. However, the reviewer raises several concerns and questions about the experimental setup and conclusions, suggesting the paper could be improved. This indicates a sentiment that is positive but with reservations. Therefore, the sentiment score is closer to slightly positive. The language used throughout is constructive, professional, and polite, suggesting a high politeness score.",60.0,80.0
LEARNING TO SHARE: SIMULTANEOUS PARAMETER TYING AND SPARSIFICATION IN DEEP LEARNING,"['Dejiao Zhang', 'Haozhu Wang', 'Mario Figueiredo', 'Laura Balzano']",Accept,2018,"[6, 10, 27, 17]","[10, 15, 32, 21]","[33, 9, 283, 101]","[16, 3, 159, 50]","[16, 3, 51, 33]","[1, 3, 73, 18]","SUMMARY
The paper proposes to apply GrOWL regularization to the tensors of parameters between each pair of layers. The groups are composed of all coefficients associated to inputs coming from the same neuron in the previous layer. The proposed algorithm is a simple proximal gradient algorithm using the proximal operator of the GrOWL norm. Given that the GrOWL norm tend to empirically reinforce a natural clustering of the vectors of coefficients which occurs in some layers, the paper proposes to cluster the corresponding parameter vectors, to replace them with their centroid and to retrain with the constrain that some vectors are now equal. Experiments show that some sparsity is obtained by the model and that together with the clustering and high compression of the model is obtained which maintaining or improving over a good level of generalization accuracy. In comparison, plain group Lasso yields compressed versions that are too sparse, and tend to degrade performance. The method is also competitive with weight decay with much better compression.

REVIEW
Given the well known issue that the Lasso tends to select arbitrarily and in a non stable way variables
that are correlated *but* given that the well known elastic-net (and conceptually simpler than GrOWL) was proposed to address that issue already more than 10 years ago, it would seem relevant to compare the proposed method with the group elastic-net.

The proposed algorithm is a simple proximal gradient algorithm, but since the objective is non-convex it would be relevant to provide references for convergence guarantees of the algorithm.

How should the step size eta be chosen? I don't see that this is discussed in the paper.

In the clustering algorithm how is the threshold value chosen?

Is it chosen by cross validation?

Is the performance better with clustering or without?

Is the same threshold chosen for GrOWL and the Lasso?

It would be useful to know which values of p, Lambda_1 and Lambda_2 are selected in the experiments?

For Figures 5,7,8,9 given that the matrices do not have particular structures that need to be visualized but that the important thing to compare is the distribution of correlation between pairs, these figures that are hard to read and compare would be advantageously replaced by histograms of the values of the correlations between pairs (of different variables). Indeed, right now one must rely on comparison of shades of colors in the thin lines that display correlation and it is really difficult to appreciate how much of correlation of what level are present in each Figure. Histograms would extract exactly the relevant information...

A brief description of affinity propagation, if only in the appendix, would be relevant.
Why this method as opposed to more classical agglomerative clustering?

A brief reminder of what the principle of weight decay is would also be relevant for the paper to more self contained.

The proposed experiments are compelling, except for the fact that it would be nice to have a comparison with the group elastic-net. 

I liked figure 6.d and would vote for inclusion in the main paper.


TYPOS etc 

3rd last line of sec. 3.2 can fail at selecting -> fail to select

In eq. (5) theta^t should be theta^{(t)}

In section 4.1 you that the network has a single fully connected layer of hidden units -> what you mean is that the network has a single hidden layer, which is furthermore fully connected.

You cite several times Sergey (2015) in section 4.2. It seems you have exchanged first name and last name plus the corresponding reference is quite strange.

Appendix B line 5 "", while."" -> incomplete sentence.

","[8, 6, 7]","[' Top 50% of accepted papers, clear accept', ' Marginally above acceptance threshold', ' Good paper, accept']","[5, 3, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is mostly positive. The reviewer acknowledges the merits of the paper and finds the experiments compelling. However, they also raise several important questions and suggest comparisons with other methods, indicating that the paper could be improved. The language used is polite and professional, with constructive criticism and suggestions for improvement.",60.0,80.0
A New Method of Region Embedding for Text Classification,"['chao qiao', 'bo huang', 'guocheng niu', 'daren li', 'daxiang dong', 'wei he', 'dianhai yu', 'hua wu']",Accept,2018,"[1, 15, 8, 9, 8, 14, 13, 15]","[5, 20, 12, 10, 13, 15, 18, 20]","[6, 211, 14, 4, 17, 23, 51, 243]","[2, 76, 8, 3, 13, 17, 19, 128]","[3, 19, 5, 0, 4, 6, 31, 102]","[1, 116, 1, 1, 0, 0, 1, 13]","The authors present a model for text classification. The parameters of the model are an embedding for each word and a local context unit. The local context unit can be seen as a filter for a convolutional layer, but which filter is used at location i depends on the word at location i (i.e. there is one filter per vocabulary word). After the filter is applied to the embeddings and after max pooling, the word-context region embeddings are summed and fed into a neural network for the classification task. The embeddings, the context units and the neural net parameters are trained jointly on a supervised text classification task. The authors also offer an alternative model, which changes the role of the embedding an the context unit, and results in context-word region embeddings. Here the embedding of word i is combined with the elements of the context units of words in the context. To get the region embeddings both model (word-context and context-word) combine attributes of the words (embeddings) with how their attributes should be emphasized or deemphasized based on nearby words (local context units and max pooling) while taking into account the relative position of the words in the context (columns of the context units). 

The method beats existing methods for text classification including d-LSTMs , BoWs, and ngram TFIDFs on held out classification accuracy. the choice of baselines is convincing. What is the performance of the proposed method if the embeddings are initialized to pretrained word embeddings and a) trained for the classification task together with randomly initialized context units b) frozen to pretrained embeddings and only the context units are trained for the classification task?

The introduction was fine. Until page 3 the authors refer to the context units a couple of times without giving some simple explanation of what it could be. A simple explanation in the introduction would improve the writing.
The related work section only makes sense *after* there is at least a minimal explanation of what the local context units do. A simple explanation of the method, for example in the introduction, would then make the connections to CNNs more clear. Also, in the related work, the authors could include more citations (e.g. the d-LSTM and the CNN based methods from Table 2) and explain the qualitative differences between their method and existing ones.

The authors should consider adding equation numbers. The equation on the bottom of page 3 is fine, but the expressions in 3.2 and 3.3 are weird. A more concise explanation of the context-word region embeddings and the word-context region embeddings would be to instead give the equation for r_{i,c}.  

The included baselines are extensive and the proposed method outperforms existing methods on most datasets. In section 4.5 the authors analyze region and embedding size, which are good analyses to include in the paper. Figure 2 and 3 could be next to each other to save space. 
I found the idea of multi region sizes interesting, but no description is given on how exactly they are combined. Since it works so well, maybe it could be promoted into the method section? Also, for each data set, which region size worked best?

Qualitative analysis: It would have been nice to see some analysis of whether the learned embeddings capture semantic similarities, both at the embedding level and at the region level. It would also be interesting to investigate the columns of the context units, with different columns somehow capturing the importance of relative position. Are there some words for which all columns are similar meaning that their position is less relevant in how they affect nearby words? And then for other words with variation along the columns of the context units, do their context units modulate the embedding more when they are closer or further away? 

Pros:
 + simple model
 + strong quantitative results

Cons:
 - notation (i.e. precise definition of r_{i,c})
 - qualitative analysis could be extended
 - writing could be improved  ","[6, 6, 6]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[5, 3, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is overall positive, highlighting the simplicity and strong results of the proposed model. While it acknowledges the good choice of baselines and interesting analyses, it also suggests several areas for improvement. These include clarifying the notation, expanding the qualitative analysis, and improving the writing, particularly in the introduction and related work sections. The reviewer uses constructive language and provides specific suggestions, indicating a polite and professional tone.",60.0,80.0
Deep Voice 3: Scaling Text-to-Speech with Convolutional Sequence Learning,"['Wei Ping', 'Kainan Peng', 'Andrew Gibiansky', 'Sercan O. Arik', 'Ajay Kannan', 'Sharan Narang', 'Jonathan Raiman', 'John Miller']",Accept,2018,"[9, 2, 2, 8, 5, 4, 4, 4]","[14, 4, 6, 13, 10, 9, 8, 8]","[56, 15, 10, 76, 11, 33, 21, 31]","[26, 7, 5, 26, 3, 8, 9, 16]","[30, 8, 5, 41, 7, 23, 12, 15]","[0, 0, 0, 9, 1, 2, 0, 0]","This paper provides an overview of the Deep Voice 3 text-to-speech system. It describes the system in a fair amount of detail and discusses some trade-offs w.r.t. audio quality and computational constraints. Some experimental validation of certain architectural choices is also provided.

My main concern with this work is that it reads more like a tech report: it describes the workings and design choices behind one particular system in great detail, but often these choices are simply stated as fact and not really motivated, or compared to alternatives. This makes it difficult to tell which of these aspects are crucial to get good performance, and which are just arbitrary choices that happen to work okay.

As this system was clearly developed with actual deployment in mind (and not purely as an academic pursuit), all of these choices must have been well-deliberated. It is unfortunate that the paper doesn't demonstrate this. I think this makes the work less interesting overall to an ICLR audience. That said, it is perhaps useful to get some insight into what types of models are actually used in practice.

An exception to this is the comparison of ""converters"", model components that convert the model's internal representation of speech into waveforms. This comparison is particularly interesting because some of the results are remarkable, i.e. Griffin-Lim spectrogram inversion and the WORLD vocoder achieving very similar MOS scores in some cases (Table 2). I wish there would be more of that kind of thing in the paper. The comparison of attention mechanisms is also useful.

I'm on the fence as I think it is nice to get some insight into a practical pipeline which benefits from many current trends in deep learning research (autoregressive models, monotonic attention, ...), but I also feel that the paper is a bit meager when it comes to motivating all the architectural aspects. I think the paper is well written so I've tentatively recommended acceptance.


Other comments:

- The separation of the ""decoder"" and ""converter"" stage is not entirely clear to me. It seems that the decoder is trained to predict spectrograms autoregressively, but its final layer is then discarded and its hidden representation is then used as input to the converter stage instead? The motivation for doing this is unclear to me, surely it would be better to train everything end-to-end, including the converter? This seems like an unnecessary detour, what's the reasoning behind this?

- At the bottom of page 2 it is said that ""the whole model is trained end-to-end, excluding the vocoder"", which I think is an unfortunate turn of phrase. It's either end-to-end, or it isn't.

- In Section 3.3, the point of mixing of h_k and h_e is unclear to me. Why is this done?

- The gated linear unit in Figure 2a shows that speaker embedding information is only injected in the linear part. Has this been experimentally validated to work better than simpler mechanisms such as adding conditioning-dependent biases/gains?

- When the decoder is trained to do autoregressive prediction of spectrograms, is it autoregressive only in time, or also in frequency? I'm guessing it's the former, but this means there is an implicit independence assumption (the intensities in different frequency bins are conditionally independent, given all past timesteps). Has this been taken into consideration? Maybe it doesn't matter because the decoder is never used directly anyway, and this is only a ""feature learning"" stage of sorts?

- Why use the L1 loss on spectrograms?

- The recent work on Parallel WaveNet may allow for speeding up WaveNet when used as a vocoder, this could be worth looking into seeing as inference speed is used as an argument to choose different vocoder strategies (with poorer audio quality as a result).

- The title heavily emphasizes that this model can do multi-speaker TTS with many (2000) speakers, but that seems to be only a minor aspect that is only discussed briefly in the paper. And it is also something that preceding systems were already capable of (although maybe it hasn't been tested with a dataset of this size before). It might make sense to rethink the title to emphasize some of the more relevant and novel aspects of this work.


----

Revision: the authors have adequately addressed quite a few instances where I feel motivations / explanations were lacking, so I'm happy to increase my rating from 6 to 7. I think the proposed title change would also be a good idea.","[7, 6, 6]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[4, 3, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The reviewer provides a lot of constructive criticism, but ultimately recommends acceptance and increases their rating after revision. They acknowledge the value of the work, even though they find it lacking in some aspects. The language used is formal and professional, without resorting to harsh or negative wording.",50.0,70.0
HexaConv,"['Emiel Hoogeboom', 'Jorn W.T. Peters', 'Taco S. Cohen', 'Max Welling']",Accept,2018,"[1, 1, 5, 19]","[6, 4, 10, 24]","[31, 7, 86, 390]","[12, 3, 32, 190]","[19, 4, 49, 166]","[0, 0, 5, 34]","The paper presents an approach to efficiently implement planar and group convolutions over hexagonal lattices to leverage better accuracy of these operations due to reduced anisotropy. They show that convolutional neural networks thus built lead to better performance - reduced inductive bias - for the same parameter budget.

G-CNNs were introduced by Cohen and Welling in ICML, 2016. They proposed DNN layers that implemented equivariance to symmetry groups. They showed that group equivariant networks can lead to more effective weight sharing and hence more efficient networks as evinced by better performance on CIFAR10 & CIFAR10+ for the same parameter budget. This paper shows G-equivariance implemented on hexagonal lattices can lead to even more efficient networks. 

The benefits of using hexagonal lattices over rectangular lattices is well known in the signal processing as well as in computer vision. For example, see   

Golay M. Hexagonal parallel pattern transformation. IEEE Transactions on Computers 1969. 18(8): p. 733-740.

Staunton R. The design of hexagonal sampling structures for image digitization and their use with local operators. Image and Vision Computing 1989. 7(3): p. 162-166. 

L. Middleton and J. Sivaswamy, Hexagonal Image Processing, Springer Verlag, London, 2005

The originality of the paper lies in the practical and efficient implementation of G-Conv layers. Group-equivariant DNNs could lead to more robust, efficient and (arguably) better performing neural networks.

Pros

- A good paper that systematically pushes the state of the art towards the design of invariant, efficient and better performing  DNNs with G-equivariant representations.

- It leverages upon the existing theory in a variety of areas - signal & image processing and machine learning, to design better DNNs.

 - Experimental evaluation suffices for a proof of concept validation of the presented ideas.   

 
Cons

- The authors should relate the paper better to existing works in the signal processing and vision literature.

- The results are on simple benchmarks like CIFAR-10. It is likely but not immediately apparent if the benefits scale to more complex problems.

- Clarity could be improved in a few places

: Since * is used for a standard convolution operator, it might be useful to use *_g as a G-convolution operator.

: Strictly speaking, for translation equivariance, the shift should be cyclic etc.

: Spelling mistakes - authors should run a spellchecker.
","[7, 7, 7]","[' Good paper, accept', ' Good paper, accept', ' Good paper, accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with positive statements, highlighting the merits of the paper and its contribution. The reviewer acknowledges the originality and practical implications of the work. While it points out areas for improvement, these are presented constructively and focus on enhancing the paper's impact. The language is professional and encouraging throughout.",75.0,90.0
Coulomb GANs: Provably Optimal Nash Equilibria via Potential Fields,"['Thomas Unterthiner', 'Bernhard Nessler', 'Calvin Seward', 'Günter Klambauer', 'Martin Heusel', 'Hubert Ramsauer', 'Sepp Hochreiter']",Accept,2018,"[8, 11, 2, 7, 12, 4, 25]","[12, 14, 5, 12, 12, 9, 30]","[38, 15, 7, 40, 6, 12, 114]","[15, 6, 3, 8, 2, 5, 38]","[19, 4, 4, 19, 2, 7, 48]","[4, 5, 0, 13, 2, 0, 28]","The paper takes an interesting approach to solve the existing problems of GAN training, using Coulomb potential for addressing the learning problem. It is also well written with a clear presentation of the motivation of the problems it is trying to address, the background and proves the optimality of the suggested solution. My understanding and validity of the proof is still an educated guess. I have been through section A.2 , but I'm unfamiliar with the earlier literature on the similar topics so I would not be able to comment on it. 

Overall, I think this is a good paper that provides a novel way of looking at and solving problems in GANs. I just had a couple of points in the paper that I would like some clarification on : 

* In section 2.2.1 : The notion of the generated a_i not disappearing is something I did not follow. What does it mean for a generated sample to ""not disappear"" ? and this directly extends to the continuity equation in (2). 

* In section 1 : in the explanation of the 3rd problem that GANs exhibit i.e.  the generator not being able to generalize the distribution of the input samples, I was hoping if you could give a bit more motivation as to why this happens. I don't think this needs to be included in the paper, but would like to have it for a personal clarification. ","[7, 7, 5]","[' Good paper, accept', ' Good paper, accept', ' Marginally below acceptance threshold']","[2, 3, 4]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer explicitly states ""Overall, I think this is a good paper that provides a novel way of looking at and solving problems in GANs."" This indicates a positive sentiment. The reviewer also provides constructive criticism and requests clarification, which further suggests a positive view of the paper's potential. There is no use of negative or impolite language. The reviewer maintains a respectful and professional tone throughout.",75.0,100.0
Learn to Pay Attention,"['Saumya Jetley', 'Nicholas A. Lord', 'Namhoon Lee', 'Philip H. S. Torr']",Accept,2018,"[0, -4, -3, 31, 3, -3]","[5, 1, 2, 36, 8, 2]","[16, 1, 3, 104, 4, 2]","[13, 1, 3, 87, 3, 1]","[2, 0, 0, 4, 0, 0]","[1, 0, 0, 13, 1, 1]","This paper proposes a network with the standard soft-attention mechanism for classification tasks, where the global feature is used to attend on multiple feature maps of local features at different intermediate layers of CNN. The attended features at different feature maps are then used to predict the final classes by either concatenating features or ensembling results from individual attended features. The paper shows that the proposed model outperforms the baseline models in classification and weakly supervised segmentation.

Strength:
- It is interesting idea to use the global feature as a query in the attention mechanism while classification tasks do not naturally involve a query unlike other tasks such as visual question answering and image captioning.

- The proposed model shows superior performances over GAP in multiple tasks.

Weakness:
- There are a lot of missing references. There have been a bunch of works using the soft-attention mechanism in many different applications including visual question answering [A-C], attribute prediction [D], image captioning [E,F] and image segmentation [G]. Only two previous works using the soft-attention (Bahdanau et al., 2014; Xu et al., 2015) are mentioned in Introduction but they are not discussed while other types of attention models (Mnih et al., 2014; Jaderberg et al., 2015) are discussed more.

- Section 2 lacks discussions about related work but is more dedicated to emphasizing the contribution of the paper.

- The global feature is used as the query vector for the attention calculation. Thus, if the global feature contains information for a wrong class, the attention quality should be poor too. Justification on this issue can improve the paper.

- [H] reports the performance on the fine-grained bird classification using different type of attention mechanism. Comparison and justification with this method can improve the paper. The performance in [H] is almost 10 % point higher accuracy than the proposed model.

- In the segmentation experiments, the models are trained on extremely small images, which is unnatural in segmentation scenarios. Experiments on realistic settings should be included. Moreover, [G] introduces a method of using an attention model for segmentation, while the paper does not contain any discussion about it.


Overall, I am concerned that the proposed model is not well discussed with important previous works. I believe that the comparisons and discussions with these works can greatly improve the paper.

I also have some questions about the experiments:
- Is there any reasoning why we have to simplify the concatenation into an addition in Section 3.2? They are not equivalent.

- When generating the fooling images of VGG-att, is the attention module involved, or do you use the same fooling images for both VGG and VGG-att?

Minor comments:
- Fig. 1 -> Fig. 2 in Section 3.1. If not, Fig. 2 is never referred.

References
[A] Huijuan Xu and Kate Saenko. Ask, attend and answer: Exploring question-guided spatial attention for visual question answering. In ECCV, 2016.
[B] Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and Alex Smola. Stacked attention networks for image question answering. In CVPR, 2016.
[C] Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Deep compositional question answering with neural module networks. In CVPR, 2016.
[D] Paul Hongsuck Seo, Zhe Lin, Scott Cohen, Xiaohui Shen, and Bohyung Han. Hierarchical attention networks. arXiv preprint arXiv:1606.02393, 2016.
[E] Quanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang, and Jiebo Luo. Image captioning with semantic attention. In CVPR, 2016.
[F] Jonghwan Mun, Minsu Cho, and Bohyung Han. Text-Guided Attention Model for Image Captioning. AAAI, 2017.
[G] Seunghoon Hong, Junhyuk Oh, Honglak Lee and Bohyung Han, Learning Transferrable Knowledge for Semantic Segmentation with Deep Convolutional Neural Network, In CVPR, 2016.
[H] Max Jaderberg, Karen Simonyan, Andrew Zisserman, Koray Kavukcuoglu, Spatial Transformer Networks, NIPS, 2015


","[5, 6, 6]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review presents a mixed sentiment. While acknowledging the interesting idea and superior performance of the proposed model, the reviewer raises significant concerns about missing references, lack of discussion on related work, and potential limitations of the approach. The reviewer also points out issues with experimental settings and provides specific questions and minor comments for improvement. The language used is generally polite and professional, providing constructive criticism with specific examples and suggestions for improvement. Therefore, the sentiment leans towards the negative side due to the substantial concerns raised, but the politeness remains positive due to the constructive and professional tone.",-25.0,75.0
SMASH: One-Shot Model Architecture Search through HyperNetworks,"['Andrew Brock', 'Theo Lim', 'J.M. Ritchie', 'Nick Weston']",Accept,2018,"[3, 18, 15, 3]","[8, 22, 18, 3]","[32, 62, 38, 6]","[12, 28, 12, 2]","[19, 9, 4, 4]","[1, 25, 22, 0]","This paper tackles the problem of finding an optimal architecture for deep neural nets . They propose to solve it by training an auxiliary HyperNet to generate the main model. The authors propose the so called ""SMASH"" algorithm that ranks the neural net architectures based on their validation error. The authors adopt a memory-bank view of the network configurations for exploring a varied collection of network configurations. It is not clear whether this is a new contribution of this paper or whether the authors merely adopt this idea.  A clearer note on this would be welcome. My key concern is with the results as described in 4.1.; the correlation structure breaks down completely for ""low-budget"" SMASH in Figure 5(a) as compared Figure (4). Doesn't this then entail an investigation of what is the optimal size of the hyper network? Also I couldn't quite follow the importance of figure 5(b) - is it referenced in the text? The authors also note that SMASH is saves a lot of computation time; some time-comparison numbers would probably be more helpful to drive home the point especially when other methods out-perform SMASH. 
One final point, for the uninitiated reader- sections 3.1 and 3.2 could probably be written somewhat more lucidly for better access.","[6, 7, 7]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Good paper, accept']","[2, 4, 3]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review starts with a neutral tone, outlining the paper's focus and approach. However, it quickly raises concerns about the clarity and novelty of the contribution. The reviewer then expresses a 'key concern' regarding the results, particularly questioning the effectiveness of the proposed method in certain scenarios. While the reviewer offers constructive criticism and suggestions for improvement, the overall tone leans towards the critical side, especially with phrases like 'key concern' and 'doesn't this then entail.' The language, however, remains professional and polite throughout, employing constructive criticism rather than harsh language.",-20.0,60.0
On the regularization of Wasserstein GANs,"['Henning Petzka', 'Asja Fischer', 'Denis Lukovnikov']",Accept,2018,"[2, 9, 5]","[6, 14, 9]","[14, 79, 21]","[8, 30, 12]","[5, 36, 8]","[1, 13, 1]","This paper proposes a novel regularization scheme for Wasserstein GAN based on a relaxation of the constraints on the Lipschitz constant of 1. The proposed regularization penalize the critic function only when its gradient has a norm larger than one using some kind of squared hinge loss. The reasons for this choice are discussed and linked to theoretical properties of OT. Numerical experiments suggests that the proposed regularization leads to better posed optimization problem and even a slight advantage in terms of inception score on the CIFAR-10 dataset.

The paper is interesting and well written, the proposed regularization makes sens since it is basically a relaxation of the constraints and the numerical experiments also suggest it's a good idea. Still as discussed below the justification do not address a lots of interesting developments and implications of the method and should better discuss the relation with regularized optimal transport.

Discussion:

+ The paper spends a lot of time justifying the proposed method by discussing the limits of the ""Improved training of Wasserstein GAN"" from Gulrajani et al. (2017). The two limits (sampling from marginals instead of optimal coupling and differentiability of the critic) are interesting and indeed suggest that one can do better but the examples and observations are well known in OT and do not require proof in appendix. The reviewer believes that this space could be better spend discussing the theoretical implication of the proposed regularization (see next).

+ The proposed approach is a relaxation of the constraints on the dual variable for the OT problem. As a matter of fact we can clearly recognize a squared hinge loss is the proposed loss. This approach (relaxing a strong constraint) has been used for years when learning support vector machines and ranking and a small discussion or at least reference to those venerable methods would position the paper on a bigger picture.

+ The paper is rather vague on the reason to go from Eq. (6) to Eq. (7). (gradient approximation between samples to gradient on samples). Does it lead to better stability to choose one or the other? 
 How is it implemented in practice? recent NN toolbox can easily compute the exact gradient and use it for the penalization but this is not clearly discussed even in appendix. Numerical experiments comparing the two implementation or at least a discussion is necessary.

+ The proposed approach has a very strong relations to the recently proposed regularized OT (see [1] for a long list of regularizations) and more precisely to the euclidean regularization. I understand that GANS (and Wasserstein GAN) is a relatively young community and that references list can be short but their is a large number of papers discussing regularized optimal transport and how the resulting problems are easier to solve. A discussion of the links is necessary and will clearly bring more theoretical ground to the method. Note that a square euclidean regularization leads to a regularization term in the dual of the form max(0,f(x)+f(y)-|x-y|)^2 that is very similar to the proposed regularization. In other words the authors propose to do regularized OT (possibly with a new regularization term) and should discuss that.

+ The numerical experiments are encouraging but a bit short. The 2D example seem to work very well and the convergence curves are far better with the proposed regularization. But the real data CIFAR experiments are much less detailed with only a final inception score (very similar to the competing method) and no images even in appendix. The authors should also define (maybe in appendix) the conditional and unconditional inception scores and why they are important (and why only some of them are computed in Table 1).

+ This is more of a suggestion. The comparison of the dual critic to the true Wasserstein distance is very interesting. It would be nice to see the behavior for different values of lambda.


[1] Dessein, A., Papadakis, N., & Rouas, J. L. (2016). Regularized Optimal Transport and the Rot Mover's Distance. arXiv preprint arXiv:1610.06447.


Review update after reply:

The authors have responded to most of my concerns and I think the paper is much stronger now and discuss the relation with regularized OT. I change the rating to Accept. 
","[7, 6, 2]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Strong rejection']","[4, 5, 2]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']","The review is positive overall. The reviewer finds the paper interesting, well-written, and the proposed method sensible. They acknowledge the value of the proposed regularization and its positive results in numerical experiments. While the reviewer provides constructive criticism and suggestions for improvement, they ultimately upgrade the paper to ""Accept"" after the authors addressed their concerns.",75.0,75.0
Interactive Grounded Language Acquisition and Generalization in a 2D World,"['Haonan Yu', 'Haichao Zhang', 'Wei Xu']",Accept,2018,"[9, 12, 19]","[14, 17, 23]","[46, 55, 71]","[15, 24, 32]","[24, 24, 37]","[7, 7, 2]","The paper introduces XWORLD, a 2D virtual environment with which an agent can constantly interact via navigation commands and question answering tasks. Agents working in this setting therefore, learn the language of the ""teacher"" and efficiently ground words to their respective concepts in the environment. The work also propose a neat model motivated by the environment and outperform various baselines. 

Further, the paper evaluates the language acquisition aspect via two zero-shot learning tasks -- ZS1) A setting consisting of previously seen concepts in unseen configurations ZS2) Contains new words that did not appear in the training phase. 

The robustness to navigation commands in Section 4.5 is very forced and incorrect -- randomly inserting unseen words at crucial points might lead to totally different original navigation commands right? As the paper says, a difference of one word can lead to completely different goals and so, the noise robustness experiments seem to test for the biases learned by the agent in some sense (which is not desirable). Is there any justification for why this method of injecting noise was chosen ? Is it possible to use hard negatives as noisy / trick commands and evaluate against them for robustness ?  

Overall, I think the paper proposes an interesting environment and task that is of interest to the community in general. The modes and its evaluation are relevant and intuitions can be made use for evaluating other similar tasks (in 3D, say). ","[6, 7, 6]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Marginally above acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer provides a positive sentiment by praising the environment, task, and model proposed in the paper. They find it interesting and relevant to the community. While they have concerns about the robustness evaluation, they do not deem it a deal-breaker. The language used is polite and professional, asking for clarifications and suggesting alternative approaches.",70.0,80.0
On the Information Bottleneck Theory of Deep Learning,"['Andrew Michael Saxe', 'Yamini Bansal', 'Joel Dapello', 'Madhu Advani', 'Artemy Kolchinsky', 'Brendan Daniel Tracey', 'David Daniel Cox']",Accept,2018,"[13, 1, 1, 3, 9, 8, 16]","[18, 6, 5, 5, 14, 12, 21]","[43, 14, 7, 11, 34, 24, 92]","[20, 5, 4, 3, 4, 4, 44]","[19, 9, 3, 7, 22, 13, 35]","[4, 0, 0, 1, 8, 7, 13]","This paper presents a study on the Information Bottleneck (IB) theory of deep learning, providing results in contrasts to the main theory claims. According to the authors, the IB theory suggests that the network generalization is mainly due to a ‘compression phase’ in the information plane occurring after a ‘fitting phase’ and that the ‘compression phase’ is due to the stochastic gradient decent (SDG). Instead, the results provided by this paper show that: the generalization can happen even without compression; that SDG is not the primary factor in compression; and that the compression does not necessarily occur after the ‘fitting phase’. Overall, the paper tackles the IB theory claims with consistent methodology, thus providing substantial arguments against the IB theory. 

The main concern is that the paper is built to argue against another theoretical work, raising a substantial discussion with the authors of the IB theory. This paper should carefully address all the raised arguments in the main text. 

There are, moreover, some open questions that are not fully clear in this contribution:
1)	To evaluate the mutual information in the ReLu networks (sec. 2) the authors discretize the output activity in their range. Should the non-linearity of ReLu be considered as a form of compression? Do you check the ratio of ReLus that are not active during training or the ratio of inputs that fall into the negative domain of each ReLu? 
2)	Since one of today common topics is the training of deep neural networks with lower representational precision, could the quantization error due to the low precision be considered as a form of noise inserted in the network layers that influences the generalization performance in deep neural networks? 
3)	What are the main conclusions or impact of the present study in the theory of neural networks? Is it the authors aim to just demonstrate that the IB theory is not correct? Perhaps, the paper should empathize the obtained results not just in contrast to the other theory, but proactively in agreement with a new proposal. 

Finally, a small issue comes from the Figures that need some improvement. In most of the cases (Figure 3 C, D; Figure 4 A, B, C; Figure 5 C, D; Figure 6) the axes font is too small to be read. Figure 3C is also very unclear.
","[6, 7, 7]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Good paper, accept']","[2, 3, 3]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review acknowledges the paper's contribution (""Overall, the paper tackles the IB theory claims with consistent methodology, thus providing substantial arguments against the IB theory.""), which points towards a positive sentiment. However, the reviewer also raises a ""main concern"" and presents ""open questions,"" indicating that the paper is not perfect and requires further work. Therefore, the sentiment is positive but moderate. The language used is formal, respectful, and constructive, suggesting a high level of politeness.",60.0,80.0
Adversarial Dropout Regularization,"['Kuniaki Saito', 'Yoshitaka Ushiku', 'Tatsuya Harada', 'Kate Saenko']",Accept,2018,"[3, 4, 14]","[8, 9, 19]","[60, 35, 379]","[29, 21, 210]","[25, 12, 112]","[6, 2, 57]","(Summary)
This paper is about learning discriminative features for the target domain in unsupervised DA problem. The key idea is to use a critic which randomly drops the activations in the logit and maximizes the sensitivity between two versions of discriminators.

(Pros)
The approach proposed in section 3.2 uses dropout logits and the sensitivity criterion between two softmax probability distributions which seems novel.

(Cons)
1. By biggest concern is that the authors avoid comparing the method to the most recent state of the art approaches in unsupervised domain adaptation and yet claims ""achieved state of the art results on three datasets."" in sec5. 1) Unsupervised Pixel-Level Domain Adaptation with Generative Adversarial Networks, Bousmalis et al. CVPR17, and 2) Learning Transferrable Representations for Unsupervised Domain Adaptation, Sener et al. NIPS16. Does the proposed method outperform these state of the art methods using the same network architectures?
2. I suggest the authors to rewrite the method section 3.2 so that the loss function depends on the optimization variables G,C. In the current draft, it's not immediately clear how the loss functions depend on the optimization variables. For example, in eqns 2,3,5, the minimization is over G,C but G,C do not appear anywhere in the equation. 
3. For the digits experiments, appendix B states ""we used exactly the same network architecture"". Well, which architecture was it?
4. It's not clear what exactly the ""ENT"" baseline is. The text says ""(ENT) obtained by modifying (Springenberg 2015)"". I'd encourage the authors to make this part more explicit and self-explanatory.

(Assessment)
Borderline. The method section is not very well written and the authors avoid comparing the method against the state of the art methods in unsupervised DA.","[5, 7, 8]","[' Marginally below acceptance threshold', ' Good paper, accept', ' Top 50% of accepted papers, clear accept']","[4, 3, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review acknowledges the novelty of the paper's approach but raises significant concerns about the clarity of the method section and the lack of comparison with state-of-the-art methods. The use of ""borderline"" suggests a somewhat negative sentiment, as it implies the paper is on the verge of rejection. The language is direct and professional, but not overly positive.",-30.0,50.0
Detecting Statistical Interactions from Neural Network Weights,"['Michael Tsang', 'Dehua Cheng', 'Yan Liu']",Accept,2018,"[17, 5, 17]","[21, 10, 22]","[25, 20, 219]","[12, 12, 132]","[9, 8, 57]","[4, 0, 30]","This paper develops a novel method to use a neural network to infer statistical interactions between input variables without assuming any explicit interaction form or order. First the paper describes that an 'interaction strength' would be captured through a simple multiplication of the aggregated weight and the weights of the first hidden layers. Then, two simple networks for the main and interaction effects are modeled separately, and learned jointly with posing L1-regularization only on the interaction part to cancel out the main effect as much as possible. The automatic cutoff determination is also proposed by using a GAM fitting based on these two networks. A nice series of experimental validations demonstrate the various types of interactions can be detected, while it also fairly clarifies the limitations.

In addition to the related work mentioned in the manuscript, interaction detection is also originated from so-called AID, literally intended for 'automatic interaction detector' (Morgan & Sonquist, 1963), which is also the origin of CHAID and CART, thus the tree-based methods like Additive Groves would be the one of main methods for this. But given the flexibility of function representations, the use of neural networks would be worth rethinking, and this work would give one clear example.

I liked the overall ideas which is clean and simple, but also found several points still confusing and unclear.

1) One of the keys behind this method is the architecture described in 4.1. But this part sounds quite heuristic, and it is unclear to me how this can affect to the facts such as Theorem 4 and Algorithm 1. Absorbing the main effect is not critical to these facts? In a standard sense of statistics, interaction would be something like residuals after removing the main (additive) effect. (like a standard test by a likelihood ratio test for models with vs without interactions)

2) the description about the neural network for the main effect is a bit unclear. For example, what does exactly mean the 'networks with univariate inputs for each input variable'? Is my guessing that it is a 1-10-10-10-1 network (in the experiments) correct...? Also, do g_i and g_i' in the GAM model (sec 4.3) correspond to the two networks for the main and interaction effects respectively?

3) mu is finally fixed at min function, and I'm not sure why this is abstracted throughout the manuscript. Is it for considering the requirements for any possible criteria?

Pros:
- detecting (any order / any form of) statistical interactions by neural networks is provided.
- nice experimental setup and evaluations with comparisons to relevant baselines by ANOVA, HierLasso, and Additive Groves.

Cons:
- some parts of explanations to support the idea has unclear relationship to what was actually done, in particular, for how to cancel out the main effect.
- the neural network architecture with L1 regularization is a bit heuristic, and I'm not surely confident that this architecture can capture only the interaction effect by cancelling out the main effect.

","[7, 7, 7]","[' Good paper, accept', ' Good paper, accept', ' Good paper, accept']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The reviewer provides constructive criticism, acknowledges the novelty and potential of the work, and highlights both its strengths and weaknesses. They praise the experimental setup and the clarity of the limitations. While they point out areas of confusion and potential improvements, the language remains respectful and focused on improving the paper. ",60.0,80.0
SCAN: Learning Hierarchical Compositional Visual Concepts,"['Irina Higgins', 'Nicolas Sonnerat', 'Loic Matthey', 'Arka Pal', 'Christopher P Burgess', 'Matko Bošnjak', 'Murray Shanahan', 'Matthew Botvinick', 'Demis Hassabis', 'Alexander Lerchner']",Accept,2018,"[7, 10, 11, 3, 2, 11, 32, 18, 10, 16]","[11, 13, 14, 3, 5, 16, 37, 23, 15, 20]","[36, 11, 28, 7, 22, 30, 98, 110, 54, 30]","[11, 3, 12, 3, 9, 15, 50, 40, 10, 11]","[20, 6, 15, 4, 13, 12, 27, 54, 26, 16]","[5, 2, 1, 0, 0, 3, 21, 16, 18, 3]","This paper introduces a VAE-based model for translating between images and text. The main way that their model differs from other multimodal methods is that their latent representation is well-suited to applying symbolic operations, such as AND and IGNORE, to the text. This gives them a more expressive language for sampling images from text.

Pros:
- The paper is well written, and it provides useful visualizations and implementation details in the appendix.

- The idea of learning compositional representations inside of a VAE framework is very appealing.

- They provide a modular way of learning recombination operations.

Cons:
- The experimental evaluation is limited. They test their model only on a simple, artificial dataset. It would also be helpful to see a more extensive evaluation of the model's ability to learn logical recombination operators, since this is their main contribution.

- The approach relies on first learning a pretrained visual VAE model, but it is unclear how robust this is. Should we expect visual VAEs to learn features that map closely to the visual concepts that appear in the text? What happens if the visual model doesn't learn such a representation? This again could be addressed with experiments on more challenging datasets.

- The paper should explain the differences and trade offs between other multimodal VAE models (such as their baselines, JMVAE and TrELBO) more clearly. It should also clarify differences between the SCAN_U baseline and SCAN in the main text.

- The paper suggests that using the forward KL-divergence is important, but this does not seem to be tested with experiments.

- The three operators (AND, IN COMMON, and IGNORE) can easily be implemented as simple transformations of a (binary) bag-of-words representation. What about more complex operations, such as OR, which seemingly cannot be encoded this way?

Overall, I am borderline on this paper, due to the limited experimental evaluation, but lean slightly towards acceptance.
","[6, 7, 5]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Marginally below acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer provides both positive and negative points regarding the paper. While they find the idea appealing and the paper well-written, they point out the limitations in experimental validation as a major concern. The reviewer leans slightly towards acceptance due to the novelty, but the ""borderline"" stance and call for more robust experiments indicate a not overly positive sentiment. The language used is constructive and professional throughout the review.",10.0,80.0
Efficient Sparse-Winograd Convolutional Neural Networks,"['Xingyu Liu', 'Jeff Pool', 'Song Han', 'William J. Dally']",Accept,2018,"[2, 11]","[7, 16]","[22, 151]","[12, 97]","[2, 16]","[8, 38]","Summary: 
The paper presents a modification of the Winograd convolution algorithm that enables a reduction of multiplications in a forward pass of 10.8x almost without loss of accuracy. 
This modification combines the reduction of multiplications achieved by the Winograd convolution algorithm with weight pruning in the following way:
- weights are pruned after the Winograd transformation, to prevent the transformation from filling in zeros, thus preserving weight sparsity
- the ReLU activation function associated with the previous layer is applied to the Winograd transform of the input activations, not directly to the spatial-domain activations, also yielding sparse activations

This way sparse multiplication can be performed. Because this yields a network, which is not mathematically equivalent to a vanilla or Winograd CNN, the method goes through three stages: dense training, pruning and retraining. The authors highlight that a dimension increase in weights and ReLU activations provide a more powerful representation and that stable dynamic activation densities over layer depths benefit the representational power of ReLU layers.

Review:
The paper shows good results using the proposed method and the description is easy to follow. I particularly like Figure 1. 
I only have a couple of questions/comments:
1) I’m not familiar with the term m-specific (“Matrices B, G and A are m-specific.”) and didn’t find anything that seemed related in a very quick google search. Maybe it would make sense to add at least an informal description.
2) Although small filters are the norm, you could add a note, describing up to what filter sizes this method is applicable. Or is it almost exactly the same as for general Winograd CNNs?
3) I think it would make sense to mention weight and activation quantization in the intro as well (even if you leave a combination with quantization for future work), e.g. Rastegari et al. (2016), Courbariaux et al. (2015) and Lin et al. (2015)
4) Figure 5 caption has a typo: “acrruacy”

References:
Courbariaux, Matthieu, Yoshua Bengio, and Jean-Pierre David. ""Binaryconnect: Training deep neural networks with binary weights during propagations."" In Advances in Neural Information Processing Systems, pp. 3123-3131. 2015.
Lin, Zhouhan, Matthieu Courbariaux, Roland Memisevic, and Yoshua Bengio. ""Neural networks with few multiplications."" arXiv preprint arXiv:1510.03009 (2015).
Rastegari, Mohammad, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. ""Xnor-net: Imagenet classification using binary convolutional neural networks."" In European Conference on Computer Vision, pp. 525-542. Springer International Publishing, 2016.","[8, 7, 7]","[' Top 50% of accepted papers, clear accept', ' Good paper, accept', ' Good paper, accept']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer provides positive feedback, stating that the paper ""shows good results"" and is ""easy to follow."" They also praise Figure 1. While they do pose some questions and suggestions, these are constructive and aim to improve the paper rather than criticize it. The language used is polite and professional throughout.",75.0,100.0
mixup: Beyond Empirical Risk Minimization,"['Hongyi Zhang', 'Moustapha Cisse', 'Yann N. Dauphin', 'David Lopez-Paz']",Accept,2018,"[22, 9, 8, 8]","[27, 13, 13, 13]","[84, 35, 67, 60]","[48, 18, 32, 28]","[14, 15, 32, 27]","[22, 2, 3, 5]","Theoretical contributions: None. Moreover, there is no clear theoretical explanation for why this approach ought to work. The authors cite (Chapelle et al., 2000) and actually most of the equations are taken from there, but the authors do not justify why the proposed distribution is a good approximation for the true p(x, y). 

Practical contributions: The paper introduces a new technique for training DNNs by forming a convex combination between two training data instances, as well as changing the associated label to the corresponding convex combination of the original 2 labels. 

Experimental results. The authors show mixup provides improvement over baselines in the following settings:
  * Image Classification on Imagenet. CIFAR-10 and CIFAR-100, across architectures.
  * Speech data 
  * Memorization of corrupted labels
  * Adversarial robustness (white box and black box attacks)
  * GANs (though quite a limited example, it is hard to generalize from this setting to the standard problems that GANs are used for).
  * Tabular data.

Reproducibility: The provided website to access the source code is currently not loading. However, experiment hyperparameters are meticulously recorded in the paper. 

Key selling points:
  * Good results across the board.
  * Easy to implement.
  * Not computationally expensive. 

What is missing:
  * Convincing theoretical arguments for why combining data and labels this way is a good approach. Convex combinations of natural images does not result in natural images. 
 * Baseline in which the labels are not mixed, in order to ensure that the gains are not coming from the data augmentation only. Combining the proposed data augmentation with label smoothing should be another baseline.
  * A thorough discussion on mixing in feature space, as well as a baseline which mizes in feature space. 
  * A concrete strategy for obtaining good results using the proposed method. For example, for speech data the authors say that “For mixup, we use a warm-up period of five epochs where we train the network on original training examples, since we find it speeds up initial convergence.“ Would be good to see how this affects results and convergence speed. Apart from having to tune the lambda hyperparameter, one might also have to tune when to start mixup. 
  * Figure 2 seems like a test made to work for this method and does not add much to the paper. Yes, if one trains on convex combination between data, one expects the model to do better in that regime. 
  * Label smoothing baseline to put numbers into perspective, for example in Figure 4. 



","[6, 6, 7]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Good paper, accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review acknowledges the strong empirical performance of the paper ('Good results across the board', 'Easy to implement', 'Not computationally expensive') but points out a lack of theoretical grounding as a significant weakness. The reviewer also provides several concrete suggestions for improvement, indicating a desire to see the work strengthened.  The language is direct and critical in places ('Figure 2 seems like a test made to work for this method and does not add much to the paper') but maintains a professional tone overall.",40.0,60.0
Towards Neural Phrase-based Machine Translation,"['Po-Sen Huang', 'Chong Wang', 'Sitao Huang', 'Dengyong Zhou', 'Li Deng']",Accept,2018,"[8, 15, 6, 18, 28]","[12, 20, 11, 21, 30]","[75, 269, 35, 62, 329]","[44, 114, 22, 37, 188]","[26, 38, 11, 17, 36]","[5, 117, 2, 8, 105]","This paper introduces a new architecture for end to end neural machine translation. Inspired by the phrase based approach, the translation process is decomposed as follows : source words are embedded and then reordered; a bilstm then encodes the reordered source; a sleep wake network finally generates the target sequence as a phrase sequence built from left to right. 

This kind of approach is more related to ngram based machine translation than conventional phrase based one.  

The idea is nice. The proposed approach does not rely on attention based model. This opens nice perpectives for better and faster inference. 

My first concern is about the architecture description. For instance, the swan part is not really stand alone. For reader who does not already know this net, I'm not sure this is really clear. Moreover, there is no link between notations used for the swan part and the ones used in the reordering part. 

Then, one question arises. Why don't you consider the reordering of the whole source sentence. Maybe you could motivate your choice at this point. This is the main contribution of the paper, since swan already exists.

Finally, the experimental part shows nice improvements but: 1/ you must provide baseline results with a well tuned phrase based mt system; 2/ the datasets are small ones, as well as the vocabularies, you should try with larger datasets and bpe for sake of comparison. ","[8, 6, 6]","[' Top 50% of accepted papers, clear accept', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[5, 3, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer provides a generally positive overview, acknowledging the novelty and potential of the approach. While they point out areas for improvement, they are presented constructively with suggestions rather than harsh criticisms. The language avoids informal or negative phrasing, maintaining a professional tone.",60.0,70.0
cGANs with Projection Discriminator,"['Takeru Miyato', 'Masanori Koyama']",Accept,2018,"[3, 7]","[8, 12]","[24, 38]","[9, 14]","[14, 19]","[1, 5]","This manuscript makes the case for a particular parameterization of conditional GANs, specifically how to add conditioning information into the network.  It motivates the method by examining the form of the log density ratio in the continuous and discrete cases.

This paper's empirical work is quite strong, bringing to bare nearly all of the established tools we currently have for evaluating implicit image models (MS-SSIM, FID, Inception scores). 

What bothers me is mostly that, while hyperparameters are stated (and thank you for that), they seem to be optimized for the candidate method rather than the baseline. In particular, Beta1 = 0 for the Adam momentum coefficient seems like a bold choice based on my experience. It would be an easier sell if hyperparameter search details were included and a separate hyperparameter search were conducted for the candidate and control, allowing the baseline to put its best foot forward.

The sentence containing ""assume that the network model can be shared"" had me puzzled for a few minutes. I think what is meant here is just that we can parameterize the log density ratio directly (including some terms that belong to the data distribution to which we do not have explicit access). This could be clearer.","[6, 7, 6]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Marginally above acceptance threshold']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with positive remarks, highlighting the strong empirical work and use of established evaluation tools. However, it then expresses concerns about potential bias in hyperparameter optimization, which slightly dampens the initial positivity.  The reviewer also points out a confusing sentence but doesn't frame it as a major flaw. Overall, the feedback is constructive and suggestive of improvements rather than outright rejection. ",60.0,70.0
Multi-Task Learning for Document Ranking and Query Suggestion,"['Wasi Uddin Ahmad', 'Kai-Wei Chang', 'Hongning Wang']",Accept,2018,"[3, 11, 11]","[8, 16, 16]","[55, 317, 170]","[24, 146, 107]","[30, 157, 53]","[1, 14, 10]","Novelty: It looks quite straightforward to combine document ranking and query suggestion.  For the model architecture, it is a standard multi-task learning framework. For the “session encoder”, it is also proposed (at least, used) in (Sordoni et al., CIKM 2015). Therefore, I think the technical novelty of the work is limited. 

Clarify: The paper is in general well written. One minor suggestion is to replace Figure 1 with Figure 3, which is more intuitive. 

Experiments: 
1.	Why don’t you try deep LSTM models and attention mechanisms (although you mentioned them as future work)? There are many open-source tools for deep LSTM/GRU and attention models, and I see no obstacle to implement your algorithms on their top. 
2.	In Table 2, M-NSRF with regularization significantly outperforms the version without regularization. This indicates that it might be the regularization that works rather than multi-task learning. For fair comparison, the regularization trick should also be applied to the baselines. 
3.	For the evaluation metric of query suggestion, why not using BLEU score? At least, you should compare with the metrics used in (Sordoni et al., 2015) for fairness. 
4.	The experiments are not very comprehensive – currently, there is only one experiment in the paper, from which one can hardly draw convincing conclusions.
5.	How many words are there in your documents? What is the average length of each document? You only mention that “our goal is to rank candidate documents titles……” in Page 6, 2nd paragraph. It might be quite different for long document retrieval vs. short document retrievel. 
6.	How did you split the dataset into training, validation and test sets?  It seems that you used a different splitting rule from (Sordoni et al., 2015), why? ","[4, 7, 6]","[' Ok but not good enough - rejection', ' Good paper, accept', ' Marginally above acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer highlights a lack of novelty and provides several concrete suggestions for improvement, indicating a critical but not dismissive stance. The language is direct and focused on technical aspects, which is typical for scientific reviews, but maintains a professional and respectful tone.",-20.0,50.0
Flipout: Efficient Pseudo-Independent Weight Perturbations on Mini-Batches,"['Yeming Wen', 'Paul Vicol', 'Jimmy Ba', 'Dustin Tran', 'Roger Grosse']",Accept,2018,"[1, 4, 8, 4, 12]","[5, 9, 13, 9, 17]","[16, 30, 96, 76, 123]","[6, 16, 46, 31, 60]","[10, 14, 49, 40, 61]","[0, 0, 1, 5, 2]","The paper is well written. The proposal is explained clearly. 
Although the technical contribution of this work is relevant for network learning, several key aspects are yet to be addressed thoroughly, particularly the experiments. 

Will there be any values of alpha, beta and gamma where eq(8) and eq(9) are equivalent. In other words, will it be the case that SharedPerturbation(alpha, beta, gamma, N) = Flipout(alpha1, beta1, gamma1, N1) for some choices of alpha, alpha1, beta, beta1, ...? This needs to be analyzed very thoroughly because some experiments seem to imply that Flip and NoFlip are giving same performance (Fig 2(b)). 
It seems like small batch with shared perturbation should be similar to large batch with flipout? 
Will alpha and gamma depend on the depth of the network? Can we say anything about which networks are better? 
It is clear that the perturbations E1 and E2 are to be uniform +/-1. Are there any benefits for choosing non-uniform sampling, and does the computational overhead of sampling them depend on the network depth/size. 

The experiments seem to be inconclusive. 
Firstly, how would the proposed strategy work on standard vision problems including learning imagenet and cifar datasets (such experiments would put the proposal into perspective compared to dropout and residual net type procedures) ?
Secondly, without confidence intervals (or significance tests of any kind), it is difficult to evaluate the goodness of Flipout vs. baselines, specifically in Figures 2(b,d). 
Thirdly, it is known that small batch sizes give better performance guarantees than large ones, and so, what does Figure 1 really imply? (Needs more explanation here, relating back to description of alpha, beta and gamma; see above). 
","[6, 8, 6]","[' Marginally above acceptance threshold', ' Top 50% of accepted papers, clear accept', ' Marginally above acceptance threshold']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with positive remarks, highlighting the clear writing and relevant contribution. However, it quickly dives into a series of in-depth technical questions and critiques the experiments as ""inconclusive."" The numerous pointed questions and desire for more comprehensive analysis suggest the reviewer sees potential but finds the current work lacking in several key areas. The language, while direct and critical, maintains a professional and respectful tone throughout, focusing on the scientific content and areas for improvement.",20.0,70.0
Improving GAN Training via Binarized Representation Entropy (BRE) Regularization,"['Yanshuai Cao', 'Gavin Weiguang Ding', 'Kry Yik-Chau Lui', 'Ruitong Huang']",Accept,2018,"[6, 6, 1, 8]","[10, 9, 4, 11]","[32, 25, 10, 27]","[12, 11, 5, 15]","[19, 10, 5, 11]","[1, 4, 0, 1]","The paper proposed a novel regularizer that is to be applied to the (rectifier) discriminators in GAN in order to encourage a better allocation of the ""model capacity"" of the discriminators over the (potentially multi-modal) generated / real data points, which might in turn helps with learning a more faithful generator.

The paper is in general very well written, with intuitions and technical details well explained and empirical studies carefully designed and executed.

Some detailed comments / questions:

1. It seems the concept of ""binarized activation patterns"", which the proposed regularizer is designed upon, is closely coupled with rectifier nets. I would therefore suggest the authors to highlight this assumption / constraint more clearly e.g. in the abstract.

2. In order for the paper to be more self-contained, maybe list at least once the formula for ""rectifier net"" (sth. like ""a^T max(0, wx + b) + c"") ? This might also help the readers better understand where the polytopes in Figure 1 come from.

3. In section 3.1, when presenting random variables (U_1, ..., U_d), I find the word ""Bernourlli"" a bit misleading because typically people would expect U_i to take values from {0, 1} whereas here you assume {-1, +1}. This can be made clear with just one sentence yet would greatly help with clearing away confusions for subsequent derivations.
Also, ""K"" is already used to denote the mini-batch size, so it's a slight abuse to reuse ""k"" to denote the ""kth marginal"".

4. In section 3.2, it may be clearer to explicitly point out the use of the ""3-sigma"" rule for Gaussian distributions here. But I don't find it justified anywhere why ""leave 99.7% of i, j pairs unpenalized"" is sth. to be sought for here?

5. In section 3.3, when presenting Corollary 3.3 of Gavinsky & Pudlak (2015), ""n"" abruptly appears without proper introduction / context.

6. For the empirical study with 2D MoG, would an imbalanced mixture make it harder for the BRE-regularized GAN to escape from modal collapse?

7. Figure 3 is missing the sub-labels (a), (b), (c), (d).","[7, 6, 4]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Ok but not good enough - rejection']","[4, 3, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer provides generally positive feedback, praising the paper's clarity, intuition explanation, and empirical studies. They offer constructive criticism in the form of specific, actionable suggestions for improvement rather than harsh critiques. The language is formal, objective, and respectful throughout.",75.0,90.0
Generalizing Hamiltonian Monte Carlo with Neural Networks,"['Daniel Levy', 'Matt D. Hoffman', 'Jascha Sohl-Dickstein']",Accept,2018,"[13, 13, 10]","[16, 17, 15]","[21, 85, 130]","[10, 49, 52]","[10, 28, 74]","[1, 8, 4]","The paper proposed a generalized HMC by modifying the leapfrog integrator using neural networks to make the sampler to converge and mix quickly. Mixing is one of the most challenge problems for a MCMC sampler, particularly when there are many modes in a distribution. The derivations look correct to me. In the experiments, the proposed algorithm was compared to other methods, e.g., A-NICE-MC and HMC. It showed that the proposed method could mix between the modes in the posterior. Although the method could mix well when applied to those particular experiments, it lacks theoretical justifications why the method could mix well. ","[6, 7, 8]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Top 50% of accepted papers, clear accept']","[3, 4, 2]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']","The review acknowledges the paper's contribution (""The paper proposed a generalized HMC...""), finds the derivations correct, and points out the positive results in the experiments (""It showed that the proposed method could mix between the modes in the posterior.""). However, it also highlights a significant weakness regarding the lack of theoretical justification. Overall, the feedback is balanced, suggesting a slightly positive sentiment. The language used is formal, polite, and maintains a professional tone throughout.",60.0,80.0
Improving the Universality and Learnability of Neural Programmer-Interpreters with Combinator Abstraction,"['Da Xiao', 'Jo-Yu Liao', 'Xingyuan Yuan']",Accept,2018,"[14, 1, 1]","[18, 1, 1]","[23, 2, 2]","[8, 1, 1]","[2, 1, 1]","[13, 0, 0]","The paper is interesting to read and gives valuable insights. 

However, the paper clearly breaks the submission guidelines. The paper is far too long, 14 pages (+refs and appendix, in total 19 pages), while the page limit is 8 pages (+refs and appendix). Therefore, the paper should be rejected. I can not foresee how the authors should be able to squeeze to content into 8 pages. The paper is more suitable for a journal, where page limit is less of an issue.","[3, 7, 7]","[' Clear rejection', ' Good paper, accept', ' Good paper, accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with a positive note, acknowledging the paper's value ('interesting', 'valuable insights'). However, it strongly emphasizes a negative aspect - the paper's length exceeding the limit, leading to a recommendation for rejection. Despite the initial positivity, the rejection based on a strict rule and the inability to envision a solution ('I cannot foresee...') make the overall sentiment negative. 

The language used is formal and professional. While direct, it doesn't contain personal attacks or disrespectful phrasing. The reviewer expresses their opinion without being aggressive or demeaning.",-60.0,50.0
Minimax Curriculum Learning: Machine Teaching with Desirable Difficulties and Scheduled Diversity,"['Tianyi Zhou', 'Jeff Bilmes']",Accept,2018,"[10, 30]","[15, 35]","[112, 299]","[55, 211]","[48, 49]","[9, 39]","The main strength of this paper, I think, is the theoretical result in Theorem 1. This result is quite nice. I wish the authors actually concluded with the following minor improvement to the proof that actually strengthens the result further.

The authors ended the discussion on thm 1 on page 7 (just above Sec 2.3) by saying what is sufficiently close to w*. If one goes back to (10), it is easy to see that what converges to w* when one of three things happen (assuming beta is fixed once loss L is selected).

1) k goes to infinity
2) alpha goes to 1
3) g(w*) goes to 0

The authors discussed how alpha is close to 1 by virtue of submodular optimization lower bounds there for what is close to w*. In fact this proof shows the situation is much better than that. 

If we are really concerned about making what converge to w*, and if we are willing to tolerate the increasing computational complexity associated solving submodular problems with larger k, we can schedule k to increase over time which guarantees that both alpha goes to 1 and g(w*) goes to zero. 

There is also a remark that G(A) tends to be modular when lambda is small which is useful.
From the algorithm, it seems clear that the authors recognized these two useful aspects of the objective and scheduled lambda to decrease exponentially and k to increase linearly.

It would be really nice to complete the analysis of Thm1 with a formal analysis of convergence speed for ||what-w*|| as lambda and k are scheduled in this fashion. Such an analysis would help practitioners make better choices for the hyper parameters gamma and Delta.","[6, 6, 5]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally below acceptance threshold']","[3, 4, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review starts by highlighting a strength of the paper, particularly praising Theorem 1. The reviewer suggests an improvement to the proof, but this is framed as a suggestion rather than a flaw. The language throughout is constructive and aims to enhance the paper. While the reviewer identifies areas for further analysis, the overall tone remains positive and encouraging.",75.0,80.0
Recasting Gradient-Based Meta-Learning as Hierarchical Bayes,"['Erin Grant', 'Chelsea Finn', 'Sergey Levine', 'Trevor Darrell', 'Thomas Griffiths']",Accept,2018,"[4, 6, 10, 32, 19]","[8, 11, 15, 37, 24]","[24, 306, 743, 653, 317]","[13, 128, 326, 376, 190]","[10, 172, 396, 230, 87]","[1, 6, 21, 47, 40]","MAML (Finn+ 2017) is recast as a hierarchical Bayesian learning procedure. In particular the inner (task) training is initially cast as point-wise max likelihood estimation, and then (sec4) improved upon by making use of the Laplace approximation. Experimental evidence of the relevance of the method is provided on a toy task involving a NIW prior of Gaussians, and the (benchmark) MiniImageNet task.

Casting MAML as HB seems a good idea. The paper does a good job of explaining the connection, but I think the presentation could be clarified. The role of the task prior and how it emerges from early stopping (ie a finite number of gradient descent steps) (sec 3.2) is original and technically non-trivial, and is a contribution of this paper. 
The synthetic data experiment sec5.1 and fig5 is clearly explained and serves to additionally clarify the proposed method. 
Regarding the MiniImageNet experiments, I read the exchange on TCML and agree with the authors of the paper under review. However, I recommend including the references to Mukhdalai 2017 and Sung 2017 in the footnote on TCML to strengthen the point more generically, and show that not just TCML but other non-shallow architectures are not considered for comparison here. In addition, the point made by the TCML authors is fair (""nothing prevented you from..."") and I would also recommend mentioning the reviewed paper's authors' decision (not to test deeper architectures) in the footnote. This decision is in order but needs to be stated in order for the reader to form a balanced view of methods at her disposal.
The experimental performance reported Table 1 remains small and largely within one standard deviation of competitor methods.

I am assessing this paper as ""7"" because despite the merit of the paper, the relevance of the reformulation of MAML, and the technical steps involved in the reformulation, the paper does not eg address other forms (than L-MAML) of the task-specific subroutine ML-..., and the benchmark improvements are quite small. I think the approach is good and fruitful. 


# Suggestions on readability

* I have the feeling the paper inverts $\alpha, \beta$ from their use in Finn 2017 (step size for meta- vs task-training). This is unfortunate and will certainly confuse readers; I advise carefully changing this throughout the entire paper (eg Algo 2,3,4, eq 1, last eq in sec3.1, eq in text below eq3, etc)

* I advise avoiding the use of the symbol f, which appears in only two places in Algo 2 and the end of sec 3.1. This is in part because f is given another meaning in Finn 2017, but also out of general parsimony in symbol use. (could leave the output of ML-... implicit by writing ML-...(\theta, T)_j in the $sum_j$; if absolutely needed, use another symbol than f)

* Maybe sec3 can be clarified in its structure by re-ordering points on the quadratic error function and early stopping (eg avoiding to split them between end of 3.1 and 3.2).

* sec6 ""Machine learning and deep learning"": I would definitely avoid this formulation, seems to tail in with all the media nonsense on ""what's the difference between ML and DL ?"". In addition the formulation seems to contrast ML with hierarchical Bayesian modeling, which does not make sense/ is wrong and confusing.

# Typos

* sec1 second parag: did you really mean ""in the architecture or loss function""? unclear.
* sec2: over a family
* ""common structure, so that"" (not such that)
* orthgonal
* sec2.1 suggestion: clarify that \theta and \phi are in the same space
* sec2.2 suggestion: task-specific parameter $\phi_j$ is distinct from ... parameters $\phi_{j'}, j' \neq j}
* ""unless an approximate ... is provided"" (the use of the subjunctive here is definitely dated :-) )
* sec3.1 task-specific parameters $\phi_j$ (I would avoid writing just \phi altogether to distinguish in usage from \theta)
* Gaussian-noised
* approximation of the it objective
* before eq9: ""that solves"": well, it doesn't really ""solve"" the minimisation, in that it is not a minimum; reformulate this?
* sec4.1 innaccurate
* well approximated
* sec4.2 an curvature
* (Amari 1989)
* For the the Laplace
* O(n^3) : what is n ?
* sec5.2 (Ravi and L 2017)
* for the the 
","[7, 7, 6]","[' Good paper, accept', ' Good paper, accept', ' Marginally above acceptance threshold']","[3, 3, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer finds the paper's core idea good and technically sound, highlighting the originality and non-triviality of connecting early stopping to the task prior. They see merit in the reformulation of MAML and appreciate the clear explanation provided, particularly in the synthetic data experiment. However, they also point out limitations, such as not addressing other forms of the task-specific subroutine and the marginal improvements in benchmark performance. The numerous suggestions for improvement, while constructive, also indicate areas where the paper falls short. The tone is professional and helpful, not overly negative despite the criticisms. ",60.0,80.0
Decoupling the Layers in Residual Networks,"['Ricky Fok', 'Aijun An', 'Zana Rashidi', 'Xiaogang Wang']",Accept,2018,"[2, 25, 1, 14]","[2, 30, 4, 18]","[6, 174, 5, 22]","[1, 117, 4, 7]","[2, 14, 0, 2]","[3, 43, 1, 13]","Motivated via Talor approximation of the Residual network on a local minima, this paper proposed a warp operator that can replace a block of a consecutive number of residual layers. While having the same number of parameters as the original residual network, the new operator has the property that the computation can be parallelized. As demonstrated in the paper, this improves the training time with multi-GPU parallelization, while maintaining similar performance on CIFAR-10 and CIFAR-100.

One thing that is currently not very clear to me is about the rotational symmetry. The paper mentioned rotated filters, but continue to talk about the rotation in the sense of an orthogonal matrix applying to the weight matrix of a convolution layer. The rotation of the filters (as 2D images or images with depth) seem to be quite different from ""rotating"" a general N-dim vectors in an abstract Euclidean space. It would be helpful to make the description here more explicit and clear.","[6, 7, 6]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Marginally above acceptance threshold']","[3, 3, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review starts with a positive sentiment, highlighting the motivation and contribution of the paper. It acknowledges the demonstrated benefits of the proposed method. The reviewer then raises a point that is unclear, but the language remains constructive and suggests an improvement rather than stating a flaw. Overall, the tone is neutral, leaning slightly towards positive due to the initial appreciation.",20.0,60.0
Proximal Backpropagation,"['Thomas Frerix', 'Thomas Möllenhoff', 'Michael Moeller', 'Daniel Cremers']",Accept,2018,"[2, 6, 9, 19]","[5, 11, 14, 24]","[9, 31, 113, 602]","[5, 14, 40, 327]","[4, 14, 48, 186]","[0, 3, 25, 89]","The paper uses a lesser-known interpretation of the gradient step of a composite function (i.e., via reverse mode automatic differentiation or backpropagation), and then replaces one of the steps with a proximal step. The proximal step requries the solution of a positive-definite linear system, so it is approximated using a few iterations of CG. The paper provides theory to show that their proximal variant (even with the CG approximations) can lead to convergent algorithms (and since practical algorithms are not necessarily globally convergent, most of the theory shows that the proximal variant has similar guarantees to a standard gradient step).

On reading the abstract and knowing quite a bit about proximal methods, I was initially skeptical, but I think the authors have done a good job of making their case. It is a well-written, very clear paper, and it has a good understanding of the literature, and does not overstate the results. The experiments are serious, and done using standard state-of-the-art tools and architectures. Overall, it is an interesting idea, and due to the current focus on neural nets, it is of interest even though it is not yet providing substantial improvements.

The main drawback of this paper is that there is no theory to suggest the ProxProp algorithm has better worst-case convergence guarantees, and that the experiments do not show a consistent benefit (in terms of time) of the method. On the one hand, I somewhat agree with the authors that ""while the running time is higher... we expect that it can be improved through further engineering efforts"", but on the other hand, the idea of nested algorithms (""matrix-free"" or ""truncated Newton"") always has this issue. A very similar type of ideas comes up in constrained or proximal quasi-Newton methods, and I have seen many papers (or paper submissions) on this style of method (e.g., see the 2017 SIAM Review paper on FWI by Metivier et al. at https://doi.org/10.1137/16M1093239). In every case, the answer seems to be that it can work on *some problems* and for a few well chosen parameters, so I don't yet buy that ProxProp is going to make a huge savings on a wide-range of problems.

In brief: quality is high, clarity is high, originality is high, and significance is medium.
Pros: interesting idea, relevant theory provided, high-quality experiments
Cons: no evidence that this is a ""break-through"" idea

Minor comments:

- Theorems seemed reasonable and I have no reason to doubt their accuracy

- No typos at all, which I find very unusual. Nice job!

- In Algo 1, it would help to be more explicit about the updates (a), (b), (c), e.g., for (a), give a reference to eq (8), and for (b), reference equations (9,10).  It's nice to have it very clear, since ""gradient step"" doesn't make it clear what the stepsize is, and if this is done in a ""Jacob-like"" or ""Gauss-Seidel-like"" fashion. (c) has no reference equation, does it?

- Similarly, for Algo 2, add references. In particular, tie in the stepsizes tau and tau_theta here.

- Motivation in section 4.1 was a bit iffy. A larger stepsize is not always better, and smaller is not worse. Minimizing a quadratic f(x) = .5||x||^2 will converge in one step with a step-size of 1 because this is well-conditioned; on the flip side, slow convergence comes from lack of strong convexity, or with strong convexity, ill-conditioning of the Hessian (like a stiff ODE).

- The form of equation (6) was very nice, and you could also point out the connection with backward Euler for finite-difference methods. This was the initial setting of analysis for most of original results that rely on the proximal operator (e.g., Lions and Mercier 1970s).

- Eq (9), this is done component-wise, i.e., Hadamard product, right?

- About eq (12), even if softmax cross-entropy doesn't have a closed-form prox (and check the tables of Combettes and Pesquet), because it is separable (if I understand correctly) then it ought to be amenable to solving with a handful of Newton iterations which would be quite cheap.

Prox tables (see also the new edition of Bauschke and Combettes' book): P. L. Combettes and J.-C. Pesquet, ""Proximal splitting methods in signal processing,"" in: Fixed-Point Algorithms for Inverse Problems in Science and Engineering (2011) http://www4.ncsu.edu/~pcombet/prox.pdf

- Below prop 4, discussing why not to make step (b) proximal, this was a bit vague to me. It would be nice to expand this.

- Page 6 near the top, to apply the operator, in the fully-connected case, this is just a matrix multiply, right? and in a conv net, just a convolution? It would help the reader to be more explicit here.

- Section 5.1, 2nd paragraph, did you swap tau_theta and tau, or am I just confused? The wording here was confusing.

- Fig 2 was not that convincing since the figure with time showed that either usual BackProp or the exact ProxProp were best, so why care about the approximate ProxProp with a few CG iterations? The argument of better generalization is based on very limited experiments and without any explanation, so I find that a weak argument (and it just seems weird that inexact CG gives better generalization).  The right figure would be nice to see with time on the x-axis as well.

- Section 5.2, this was nice and contributed to my favorable opinion about the work. However, any kind of standard convergence theory for usual SGD requires the stepsize to change per iteration and decrease toward zero. I've heard of heuristics saying that a fixed stepsize is best and then you just make sure to stop the algorithm a bit early before it diverges or behaves wildly -- is that true here?

- Final section of 5.3, about the validation accuracy, and the accuracy on the test set after 50 epochs. I am confused why these are different numbers. Is it just because 50 epochs wasn't enough to reach convergence, while 300 seconds was? And why limit to 50 epochs then? Basically, what's the difference between the bottom two plots in Fig 3 (other than scaling the x-axis by time/epoch), and why does ProxProp achieve better accuracy only in the right figure?","[7, 5, 6]","[' Good paper, accept', ' Marginally below acceptance threshold', ' Marginally above acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is positive overall. The reviewer finds the paper well-written, clear, and with a good understanding of the literature. They praise the serious experiments and the interesting idea. However, they point out the lack of theoretical or consistent empirical evidence for significant benefits of the proposed method compared to existing approaches. The reviewer also provides constructive criticism and suggestions for improvement, indicating a willingness to see the work succeed.  The language is polite and professional, with a focus on constructive feedback and suggestions for improvement.",60.0,80.0
An Online Learning Approach to Generative Adversarial Networks,"['Paulina Grnarova', 'Kfir Y Levy', 'Aurelien Lucchi', 'Thomas Hofmann', 'Andreas Krause']",Accept,2018,"[5, 8, 10, 26, 16]","[8, 13, 15, 31, 21]","[11, 60, 110, 205, 476]","[3, 29, 51, 112, 250]","[6, 29, 52, 69, 192]","[2, 2, 7, 24, 34]","It is well known that the original GAN (Goodfellow et al.) suffers from instability and mode collapsing. Indeed, existing work has pointed out that the standard GAN training process may not converge if we insist on obtaining pure strategies (for the minmax game). The present paper proposes to obtain mixed strategy through an online learning approach. Online learning (no regret) algorithms have been used in finding an equilibrium for zero sum game. However, most theoretical convergence results are known for convex-concave loss. One interesting theoretical contribution of the paper is to show that convergence result can be proved if one player is a shallow network (and concave in M).In particular, the concave player plays the FTRL algorithm with standard L2 regularization term. The regret of concave player can be bounded using existing result for FTRL. The regret for the other player is more interesting: it uses the fact the adversary's strategy doesn't change too drastically. Then a lemma by Kalai and Vempala can be used. The theory part of the paper is reasonable and quite well written. 

Based on the theory developed, the paper presents a practical algorithm. Compared to the standard GAN training, the new algorithm returns mixed strategy and examine several previous models (instead of the latest) in each iteration. The paper claims that this may help to prevent model collapsing.

However, the experimental part is less satisfying. From figure 2, I don't see much advantage of Checkhov GAN. In other experiments, I don't see much improvement neither (CIFAR10 and CELEBA).The paper didn't really compare other popular GAN models, especially WGAN and its improved version, which is already quite popular by now and should be compared with.

Overall, I think it is a borderline paper.

-------------------------
I read the response and the new experimental results regarding WGAN.
The experimental results make more sense now.
It would be interesting to see whether the idea can be applied to more recent GAN models and still perform better.
I raised my score to 7.

","[7, 8, 5]","[' Good paper, accept', ' Top 50% of accepted papers, clear accept', ' Marginally below acceptance threshold']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer acknowledges the theoretical contribution of the paper and finds it well-written. However, they express dissatisfaction with the initial experimental results, describing them as ""less satisfying"" and lacking comparison with popular models like WGAN. This suggests a somewhat negative sentiment initially. However, after the authors provide additional results including a comparison with WGAN, the reviewer's opinion improves. They raise their score and express interest in further applications of the idea. This suggests the sentiment shifted towards the positive end of the spectrum, but not overwhelmingly so, as they still consider it ""borderline.""",40.0,70.0
Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training,"['Yujun Lin', 'Song Han', 'Huizi Mao', 'Yu Wang', 'Bill Dally']",Accept,2018,"[3, 4, 4, 13, 34]","[7, 9, 8, 18, 39]","[34, 150, 24, 404, 274]","[16, 66, 13, 255, 174]","[13, 67, 10, 58, 23]","[5, 17, 1, 91, 77]","The paper is thorough and on the whole clearly presented. However, I think it could be improved by giving the reader more of a road map w.r.t. the guiding principle. The methods proposed are heuristic in nature, and it's not clear what the guiding principle is. E.g., ""momentum correction"". What exactly is the problem without this correction? The authors describe it qualitatively, ""When the gradient sparsity is high, the interval dramatically increases, and thus the significant momentum effect will harm the model performance"". Can the issue be described more precisely? Similarly for gradient clipping, ""The method proposed by Pascanu et al. (2013) rescales the gradients whenever the sum of their L2-norms exceeds a threshold. This step is conventionally executed after gradient aggregation from all nodes. Because we accumulate gradients over iterations on each node independently, we perform the gradient clipping locally before adding the current gradient... "" What exactly is the issue here? It reads like a story of what the authors did, but it's not really clear why they did it.

The experiments seem quite thorough, with several methods being compared. What is the expected performance of the 1-bit SGD method proposed by Seide et al.?

re. page 2: What exactly is ""layer normalization""?

re. page 4: What are ""drastic gradients""?","[7, 6, 7]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Good paper, accept']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer acknowledges the paper's thoroughness and clarity but expresses concerns about the lack of clarity regarding the guiding principles and motivations behind certain methodological choices. The reviewer uses phrases like ""it's not clear"" and points out descriptive gaps, suggesting a need for improvement rather than outright negativity. The language remains constructive and focused on improving the paper, indicating politeness. ",50.0,70.0
Mixed Precision Training of Convolutional Neural Networks using Integer Operations,"['Dipankar Das', 'Naveen Mellempudi', 'Dheevatsa Mudigere', 'Dhiraj Kalamkar', 'Sasikanth Avancha', 'Kunal Banerjee', 'Srinivas Sridharan', 'Karthik Vaidyanathan', 'Bharat Kaul', 'Evangelos Georganas', 'Alexander Heinecke', 'Pradeep Dubey', 'Jesus Corbal', 'Nikita Shustrov', 'Roma Dubtsov', 'Evarist Fomenko', 'Vadim Pirogov']",Accept,2018,"[14, 2, 9, 12, 18, 8, 12, 16, 7, 7, 12, 40, 21, 1, 6, 1, 4]","[17, 7, 14, 17, 22, 13, 17, 21, 12, 12, 17, 45, 21, 1, 6, 1, 4]","[40, 12, 55, 34, 48, 55, 31, 53, 37, 39, 76, 123, 20, 2, 3, 4, 4]","[20, 1, 22, 19, 18, 35, 17, 42, 14, 20, 43, 57, 13, 1, 2, 2, 2]","[15, 11, 31, 12, 19, 12, 12, 5, 21, 15, 18, 24, 1, 1, 1, 2, 1]","[5, 0, 2, 3, 11, 8, 2, 6, 2, 4, 15, 42, 6, 0, 0, 0, 1]","This paper describes an implementation of reduced precision deep learning using a 16 bit integer representation. This field has recently seen a lot of publications proposing various methods to reduce the precision of weights and activations. These schemes have generally achieved close-to-SOTA accuracy for small networks on datasets such as MNIST and CIFAR-10. However, for larger networks (ResNET, Vgg, etc) on large dataset such as ImageNET, a significant accuracy drop are reported. In this work, the authors show that a careful implementation of mixed-precision dynamic fixed point computation can achieve SOTA on 4 large networks on the ImageNET-1K datasets. Using a INT16 (as opposed to FP16) has the advantage of enabling the use of new SIMD mul-acc instructions such as QVNNI16. 

The reported accuracy numbers show convincingly that INT16 weights and activations can be used without loss of accuracy in large CNNs. However, I was hoping to see a direct comparison between FP16 and INT16.  

The paper is written clearly and the English is fine.","[7, 6, 7]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Good paper, accept']","[4, 3, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review starts by acknowledging the relevance of the paper's topic and its contribution to the field. It highlights the positive achievement of the paper, namely that the proposed method achieves state-of-the-art accuracy. While the reviewer expresses a desire for further comparison with FP16, this is presented as a suggestion rather than a criticism. The reviewer also explicitly mentions that the paper is well-written. All of this points to a positive sentiment. The language used is constructive and professional throughout, indicating a polite tone.",75.0,75.0
Gradient Estimators for Implicit Models,"['Yingzhen Li', 'Richard E. Turner']",Accept,2018,"[7, 12]","[12, 17]","[66, 147]","[28, 67]","[38, 70]","[0, 10]","This paper deals with the estimation of the score function, i.e., the derivative of the log likelihood. Some methods were introduced and a new method using Stein identity was proposed. The setup of the trasnductive learning was introduced to add the prediction power to the proposed method. The method was used to several applications.

This is an interesting approach to estimate the score function for location models in a non-parametric way. I have a couple of minor comments below. 

- Stein identity is the formula that holds for the class of ellipsoidal distribution including Gaussian distribution. I'm not sure the term ""Stein identity"" is appropriate to express the equation (8). 
- Some boundary condition should be assumed to assure that integration by parts works properly. Describing an explicit boundary condition to guarantee the proper estimation would be nice. ","[6, 7, 7]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Good paper, accept']","[2, 4, 2]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']","The review starts with a neutral summary of the paper's content. It then uses the phrase ""This is an interesting approach..."" which reveals a positive sentiment towards the research. The comments are framed as suggestions rather than harsh criticisms, contributing to the polite tone. Therefore, the sentiment is slightly positive and the politeness is very polite.",60.0,80.0
Deep Autoencoding Gaussian Mixture Model for Unsupervised Anomaly Detection,"['Bo Zong', 'Qi Song', 'Martin Renqiang Min', 'Wei Cheng', 'Cristian Lumezanu', 'Daeki Cho', 'Haifeng Chen']",Accept,2018,"[10, 13, 12, 10, 13, 9, 18]","[15, 18, 17, 15, 17, 12, 23]","[54, 179, 79, 112, 50, 5, 234]","[37, 94, 37, 64, 44, 4, 145]","[13, 28, 32, 24, 4, 1, 42]","[4, 57, 10, 24, 2, 0, 47]","1. This is a good paper, makes an interesting algorithmic contribution in the sense of joint clustering-dimension reduction for unsupervised anomaly detection
2. It demonstrates clear performance improvement via comprehensive comparison with state-of-the-art methods
3. Is the number of Gaussian Mixtures 'K' a hyper-parameter in the training process? can it be a trainable parameter?
4. Also, it will be interesting to get some insights or anecdotal evidence on how the joint learning helps beyond the decoupled learning framework, such as what kind of data points (normal and anomalous) are moving apart due to the joint learning  ","[8, 8, 8]","[' Top 50% of accepted papers, clear accept', ' Top 50% of accepted papers, clear accept', ' Top 50% of accepted papers, clear accept']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with positive statements, highlighting the paper's strengths and contribution. The questions asked are constructive and aim to further improve the paper, rather than criticize it. The language is formal and respectful throughout.",75.0,100.0
Robustness of Classifiers to Universal Perturbations: A Geometric Perspective,"['Seyed-Mohsen Moosavi-Dezfooli', 'Alhussein Fawzi', 'Omar Fawzi', 'Pascal Frossard', 'Stefano Soatto']",Accept,2018,"[4, 8, 9, 20, 26]","[9, 12, 14, 25, 31]","[50, 46, 86, 586, 445]","[21, 19, 33, 262, 241]","[27, 20, 35, 160, 138]","[2, 7, 18, 164, 66]","The paper is written well and clear.   The core contribution of the paper is the illustration that: under the assumption of flat, or curved decision boundaries with positive curvature small universal adversarial perturbations exist.  

Pros: the intuition and geometry is rather clearly presented.  

Cons: 
References to ""CaffeNet""  and ""LeNet"" (even though the latter is well-known) are missing.  In the experimental section used to validate the main hypothesis that the deep networks have positive curvature decision boundaries, there is no description of how these networks were trained. 

It is not clear why the authors have decided to use out-dated 5-layer ""LeNet""  and NiN (Network in network) architectures instead of more recent and much better performing architectures (and less complex than NiN architectures). It would be nice to see how the behavior and boundaries look in these cases.  

The conclusion is speculative:
""Our analysis hence shows that to construct classifiers that are robust to universal perturbations, it
is key to suppress this subspace of shared positive directions, which can possibly be done through
regularization of the objective function. This will be the subject of future works."" 

It is clear that regularization should play a significant role in shaping the decision boundaries. Unfortunately, the paper does not provide details at the basic level, which algorithms,  architectures, hyper-parameters or regularization terms are used. All these factors should play a very significant role in the experimental validation of their hypothesis.

Notes: I did not check the proofs of the theorems in detail. 
","[6, 7, 5]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Marginally below acceptance threshold']","[4, 3, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review starts with positive remarks, highlighting the well-written nature and clear presentation of the paper. However, it then delves into a series of ""Cons,"" pointing out several limitations and suggesting potential improvements. While the criticisms are specific and constructive, the overall tone leans towards the critical side. The language used is professional and polite throughout, employing a neutral tone without resorting to harsh or disrespectful language.",20.0,70.0
Sparse Persistent RNNs: Squeezing Large Recurrent Networks On-Chip,"['Feiwen Zhu', 'Jeff Pool', 'Michael Andersch', 'Jeremy Appleyard', 'Fung Xie']",Accept,2018,"[8, 11, 7, 5, 1]","[12, 14, 11, 9, 1]","[7, 24, 9, 9, 2]","[4, 13, 6, 5, 1]","[3, 10, 2, 3, 1]","[0, 1, 1, 1, 0]","The paper devises a sparse kernel for RNNs which is urgently needed because current GPU deep learning libraries (e.g., CuDNN) cannot exploit sparsity when it is presented and because a number of works have proposed to sparsify/prune RNNs so as to be able to run on devices with limited compute power (e.g., smartphones). Unfortunately, due to the low-level and GPU specific nature of the work, I would think that this work will be better critiqued in a more GPU-centric conference. Another concern is that while experiments are provided to demonstrate the speedups achieved by exploiting sparsity, these are not contrasted by presenting the loss in accuracy caused by introducing sparsity (in the main portion of the paper). It may be the case by reducing density to 1% we can speedup by N fold but this observation may not have any value if the accuracy becomes  abysmal.

Pros:
- Addresses an urgent and timely issue of devising sparse kernels for RNNs on GPUs
- Experiments show that the kernel can effectively exploit sparsity while utilizing GPU resources well

Cons:
- This work may be better reviewed at a more GPU-centric conference
- Experiments (in main paper) only show speedups and do not show loss of accuracy due to sparsity","[6, 6, 6]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[2, 2, 4]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts off fairly positive, highlighting the relevance and timeliness of the work. However, it then suggests a more suitable venue and raises a significant concern about the lack of accuracy evaluation against the speed gains. The language, while direct, remains professional and avoids harsh criticism. Therefore, the sentiment leans slightly positive due to the acknowledgment of the paper's merits, but the overall tone is cautious due to the highlighted concerns.",20.0,60.0
Not-So-Random Features,"['Brian Bullins', 'Cyril Zhang', 'Yi Zhang']",Accept,2018,"[3, 2, 26]","[8, 7, 31]","[34, 42, 1300]","[15, 18, 574]","[16, 24, 160]","[3, 0, 566]","
In this paper, the authors proposed an interesting algorithm for learning the l1-SVM and the Fourier represented kernel together. The model extends kernel alignment with random feature dual representation and incorporates it into l1-SVM optimization problem. They proposed algorithms based on online learning in which the Langevin dynamics is utilized to handle the nonconvexity. Under some conditions about the quality of the solution to the nonconvex optimization, they provide the convergence and the sample complexity. Empirically, they show the performances are better than random feature and the LKRF. 

I like the way they handle the nonconvexity component of the model. However, there are several issues need to be addressed. 

1, In Eq. (6), although due to the convex-concave either min-max or max-min are equivalent, such claim should be explained explicitly. 

2, In the paper, there is an assumption about the peak of random feature ""it is a natural assumption on realistic data that the largest peaks are close to the origin"". I was wondering where this assumption is used? Could you please provide more justification for such assumption?

3, Although the proof of the algorithm relies on the online learning regret bound, the algorithm itself requires visit all the data in each update, and thus, it is not suitable for online learning. Please clarify this in the paper explicitly. 

4, The experiment is weak. The algorithm is closely related to boosting and MKL, while there is no such comparison. Meanwhile, Since the proposed algorithm requires extra optimization w.r.t. random feature, it is more convincing to include the empirical runtime comparison. 

Suggestion: it will be better if the author discusses some other model besides l1-SVM with such kernel learning. 
","[6, 7, 4]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Ok but not good enough - rejection']","[5, 3, 5]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The reviewer starts with positive remarks, highlighting the interesting aspects of the paper and praising the handling of nonconvexity. However, they then list several issues that need addressing, indicating a need for significant revisions. While the issues raised are constructive and the language is polite, the overall sentiment leans towards the critical side due to the volume of revisions suggested.",50.0,70.0
Mitigating Adversarial Effects Through Randomization,"['Cihang Xie', 'Jianyu Wang', 'Zhishuai Zhang', 'Zhou Ren', 'Alan Yuille']",Accept,2018,"[3, 20, 4, 9, 36]","[8, 25, 9, 14, 41]","[69, 181, 40, 48, 702]","[26, 63, 18, 26, 337]","[42, 48, 21, 17, 263]","[1, 70, 1, 5, 102]","The paper basically propose keep using the typical data-augmentation transformations done during training also in evaluation time, to prevent adversarial attacks. In the paper they analyze only 2 random resizing and random padding, but I suppose others like random contrast, random relighting, random colorization, ... could be applicable.

Some of the pros of the proposed tricks is that it doesn't require re-training existing models, although as the authors pointed out re-training for adversarial images is necessary to obtain good results.


Typically images have different sizes, however in the Dataset are described as having 299x299x3 size, are all the test images resized before hand? How would this method work with variable size images?

The proposed defense requires increasing the size of the input images, have you analyzed the impact in performance? Also it would be good to know how robust is the method for smaller sizes.

Section 4.6.2 seems to indicate that 1 pixel padding or just resizing 1 pixel is enough to get most of the benefit, please provide an analysis of how results improve as the padding or size increase. 

In section 5 for the challenge authors used a lot more evaluations per image, could you provide how much extra computation is needed for that model?

","[6, 7, 6]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Marginally above acceptance threshold']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is quite neutral in its sentiment. It raises valid concerns and asks for clarifications and further analysis, which is typical for a peer review. The reviewer points out potential limitations and areas for improvement, but doesn't express strong negativity towards the paper. The language used is polite and professional, adhering to academic writing norms. There are no personal attacks or disrespectful remarks. The reviewer phrases their suggestions and questions constructively.",0.0,50.0
Learning an Embedding Space for Transferable Robot Skills,"['Karol Hausman', 'Jost Tobias Springenberg', 'Ziyu Wang', 'Nicolas Heess', 'Martin Riedmiller']",Accept,2018,"[6, 7, 7, 10, 26]","[11, 12, 10, 15, 31]","[83, 76, 53, 200, 168]","[36, 37, 20, 81, 100]","[40, 35, 29, 111, 47]","[7, 4, 4, 8, 21]","The paper presents a new approach for hierarchical reinforcement learning which aims at learning a versatile set of skills. The paper uses a variational bound for entropy regularized RL to learn a versatile latent space which represents the skill to execute. The variational bound is used to diversify the learned skills as well as to make the skills identifyable from their state trajectories. The algorithm is tested on a simple point mass task and on simulated robot manipulation tasks.

This is a very intersting paper which is also very well written. I like the presented approach of learning the skill embeddings using the variational lower bound. It represents one of the most principled approches for hierarchical RL. 

Pros: 
- Interesting new approach for hiearchical reinforcement learning that focuses on skill versatility
- The variational lower bound is one of the most principled formulations for hierarchical RL that I have seen so far
- The results are convincing

Cons:
- More comparisons against other DRL algorithms such as TRPO and PPO would be useful

Summary: This is an interesting deep reinforcement learning paper that introduces a new principled framework for learning versatile skills. This is a good paper.

More comments:
- There are several papers that focus on learning versatile skills in the context of movement primitive libraries, see [1],[2],[3]. These papers should be discussed.

[1] Daniel, C.; Neumann, G.; Kroemer, O.; Peters, J. (2016). Hierarchical Relative Entropy Policy Search, Journal of Machine Learning Research (JMLR),
[2] End, F.; Akrour, R.; Peters, J.; Neumann, G. (2017). Layered Direct Policy Search for Learning Hierarchical Skills, Proceedings of the International Conference on Robotics and Automation (ICRA).
[3] Gabriel, A.; Akrour, R.; Peters, J.; Neumann, G. (2017). Empowered Skills, Proceedings of the International Conference on Robotics and Automation (ICRA).
","[7, 7, 7]","[' Good paper, accept', ' Good paper, accept', ' Good paper, accept']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer explicitly states ""This is a very interesting paper which is also very well written"" and ""This is an interesting deep reinforcement learning paper that introduces a new principled framework for learning versatile skills. This is a good paper."" They also mention liking the approach and finding the results convincing. All of this points to a positive sentiment. The language used is polite and professional throughout, without any negative or harsh wording.",85.0,90.0
Learning to cluster in order to transfer across domains and tasks,"['Yen-Chang Hsu', 'Zhaoyang Lv', 'Zsolt Kira']",Accept,2018,"[4, 3, 15]","[9, 7, 20]","[38, 24, 130]","[16, 11, 54]","[22, 13, 71]","[0, 0, 5]","The authors propose a method for performing transfer learning and domain adaptation via a clustering approach. The primary contribution is the introduction of a Learnable Clustering Objective (LCO) that is trained on an auxiliary set of labeled data to correctly identify whether pairs of data belong to the same class. Once the LCO is trained, it is applied to the unlabeled target data and effectively serves to provide ""soft labels"" for whether or not pairs of target data belong to the same class. A separate model can then be trained to assign target data to clusters while satisfying these soft labels, thereby ensuring that clusters are made up of similar data points. 

The proposed LCO is novel and seems sound, serving as a way to transfer the general knowledge of what a cluster is without requiring advance knowledge of the specific clusters of interest. The authors also demonstrate a variety of extensions, such as how to handle the case when the number of target categories is unknown, as well as how the model can make use of labeled source data in the setting where the source and target share the same task.

The way the method is presented is quite confusing, and required many more reads than normal to understand exactly what is going on. To point out one such problem point, Section 4 introduces f, a network that classifies each data instance into one of k clusters. However, f seems to be mentioned only in a few times by name, despite seeming like a crucial part of the method. Explaining how f is used to construct the CCN could help in clarifying exactly what role f plays in the final model. Likewise, the introduction of G during the explanation of the LCO is rather abrupt, and the intuition of what purpose G serves and why it must be learned from data is unclear. Additionally, because G is introduced alongside the LCO, I was initially misled into understanding was that G was optimized to minimize the LCO. Further text explaining intuitively what G accomplishes (soft labels transferred from the auxiliary dataset to the target dataset) and perhaps a general diagram of what portions of the model are trained on what datasets (G is trained on A, CCN is trained on T and optionally S') would serve the method section greatly and provide a better overview of how the model works.

The experimental evaluation is very thorough, spanning a variety of tasks and settings. Strong results in multiple settings indicate that the proposed method is effective and generalizable. Further details are provided in a very comprehensive appendix, which provides a mix of discussion and analysis of the provided results. It would be nice to see some examples of the types of predictions and mistakes the model makes to further develop an intuition for how the model works. I'm also curious how well the model works if, you do not make use of the labeled source data in the cross-domain setting, thereby mimicking the cross-task setup.

At times, the experimental details are a little unclear. Consistent use of the A, T, and S' dataset abbreviations would help. Also, the results section seems to switch off between calling the method CCN and LCO interchangeably. Finally, a few of the experimental settings differ from their baselines in nontrivial ways. For the Office experiment, the LCO appears to be trained on ImageNet data. While this seems similar in nature to initializing from a network pre-trained on ImageNet, it's worth noting that this requires one to have the entire ImageNet dataset on hand when training such a model, as opposed to other baselines which merely initialize weights and then fine-tune exclusively on the Office data. Similarly, the evaluation on SVHN-MNIST makes use of auxiliary Omniglot data, which makes the results hard to compare to the existing literature, since they generally do not use additional training data in this setting. In addition to the existing comparison, perhaps the authors can also validate a variant in which the auxiliary data is also drawn from the source so as to serve as a more direct comparison to the existing literature.

Overall, the paper seems to have both a novel contribution and strong technical merit. However, the presentation of the method is lacking, and makes it unnecessarily difficult to understand how the model is composed of its parts and how it is trained. I think a more careful presentation of the intuition behind the method and more consistent use of notation would greatly improve the quality of this submission.

=========================
Update after author rebuttal:
=========================
I have read the author's response and have looked at the changes to the manuscript. I am satisfied with the improvements to the paper and have changed my review to 'accept'. ","[7, 9, 5]","[' Good paper, accept', ' Top 15% of accepted papers, strong accept', ' Marginally below acceptance threshold']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer acknowledges the novelty and effectiveness of the proposed method, highlighting the strong experimental results and comprehensive evaluation. They find the core idea sound and the contributions valuable. This points towards a positive sentiment. However, the reviewer expresses concerns about the clarity and presentation of the method, finding it confusing and difficult to follow. They point out specific areas where the explanation is lacking and suggest improvements. This indicates a somewhat less positive sentiment due to the presentation issues.  The language used in the review is constructive and professional, focusing on areas for improvement without resorting to harsh or disrespectful language. The reviewer maintains a polite and objective tone throughout.",60.0,80.0
Spatially Transformed Adversarial Examples,"['Chaowei Xiao', 'Jun-Yan Zhu', 'Bo Li', 'Warren He', 'Mingyan Liu', 'Dawn Song']",Accept,2018,"[5, 7, 10, 7, 20, 20]","[10, 12, 15, 11, 25, 25]","[83, 121, 300, 27, 261, 440]","[34, 47, 124, 16, 130, 254]","[46, 59, 148, 9, 59, 154]","[3, 15, 28, 2, 72, 32]","This paper proposes a new way to create adversarial examples. Instead of changing pixel values they perform spatial transformations. 

The authors obtain a flow field that is optimized to fool a target classifier. A regularization term controlled by a parameter tau is ensuring very small visual difference between the adversarial and the original image. 

The used spatial transformations are differentiable with respect to the flow field (as was already known from previous work on spatial transformations) it is easy to perform gradient descent to optimize the flow that fools classifiers for targeted and untargeted attacks. 

The obtained adversarial examples seem almost imperceivable (at least for ImageNet). 
This is a new direction of attacks that opens a whole new dimension of things to consider. 

It is hard to evaluate this paper since it opens a new direction but the authors do a good job using numerous datasets, CAM attention visualization and also additional materials with high-res attacks. 

This is a very creative new and important idea in the space of adversarial attacks. 

Edit: After reading the other reviews , the replies to the reviews and the revision of the paper with the human study on perception, I increase my score to 9. This is definitely in the top 15% of ICLR accepted papers, in my opinion.   

Also a remark: As far as I understand, a lot of people writing comments here have a misconception about what this paper is trying to do: This is not about improving attack rates, or comparing with other attacks for different epsilons, etc. 
This is a new *dimension* of attacks. It shows that limiting l_inf of l_2 is not sufficient and we have to think of human perception to get the right attack model. Therefore, it is opening a new direction of research and hence it is important scholarship. It is asking a new question, which is frequently more important than improving performance on previous benchmarks. 

","[9, 7, 7]","[' Top 15% of accepted papers, strong accept', ' Good paper, accept', ' Good paper, accept']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer clearly states that this is ""a very creative new and important idea"" and that the authors ""do a good job"" in their research. They find the work to be of high quality, increasing their score after reading the authors' responses to previous reviewers. They also defend the paper against criticisms, highlighting its importance in opening a ""new dimension of attacks"". All of this points to a very positive sentiment. The language used throughout is formal and respectful.",90.0,100.0
GANITE: Estimation of Individualized Treatment Effects using Generative Adversarial Nets,"['Jinsung Yoon', 'James Jordon', 'Mihaela van der Schaar']",Accept,2018,"[4, 1, 19]","[9, 5, 24]","[75, 29, 829]","[27, 16, 344]","[33, 13, 230]","[15, 0, 255]","This paper introduces a generative adversarial network (GAN) for estimating individualized treatment effects (ITEs) by (1) learning a generator that tries to fool a discriminator with feature, treatment, potential outcome- vectors, and (2) by learning a GAN for the treatment effect. In my view, the counterfactual component is the interesting and original component, and the results show that the ITE GAN component further improves performance (marginally but not significant). The analysis is conducted on semi-synthetic data sets created to match real data distributions with synthetically introduced selection bias and conducts extensive experimentation. While the results show worse performance compared to existing literature in the experiment with small data sizes, the work does show improvements in larger data sets. However, Table 5 in the appendix suggests these results are not significant when considering average treatment effect estimation (eATE and eATE).

Quality: good. Clarity: acceptable. Originality: original. Significance: marginal.

The ITE GAN does not significantly outperform the counterfactual GAN alone (in the S and GAN loss regime), and in my understanding the counterfactual GAN is the particularly innovative component here, i.e., can the algorithm effectively enough generate indistinguishable counterfactual outcomes from x and noise. I wonder if the paper should focus on this in isolation to better understand and characterize this contribution.

What is the significance of bold in the tables? I'd remove it if it's just to highlight which method is yours.

Discussion section should be called ""Conclusion"" and a space permitting a Discussion section should be written.
E.g. exploration of the form of the loss when k>2, or when k is exponential e.g. a {0,1}^c hypercube for c potentially related treatment options in an order set. 
E.g. implications of underperformance in settings with small data sets. We have lots of large data sets where ground truth is unknown, and relatively more small data sets where we can identify ground truth at some cost.
E.g. discussion of Table 2 (ITEs) where GANITE is outperforming the methods (at least on large data sets) and Table 5 (ATEs) which does not show the same result is warranted. Why might we expect this to the case?","[6, 6, 6]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[3, 4, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer acknowledges the originality of the work, particularly the counterfactual component. However, they point out that the ITE GAN's improvement is marginal and not significant. The reviewer suggests focusing on the counterfactual GAN alone. The language is direct and professional, but not overly positive.",20.0,50.0
Overcoming Catastrophic Interference using Conceptor-Aided Backpropagation,"['Xu He', 'Herbert Jaeger']",Accept,2018,"[15, 29]","[20, 34]","[142, 43]","[64, 12]","[18, 7]","[60, 24]","This paper introduces a method for learning new tasks, without interfering previous tasks, using conceptors. This method originates from linear algebra, where a the network tries to algebraically infer the main subspace where previous tasks were learned, and make the network learn the new task in a new sub-space which is ""unused"" until the present task in hand.

The paper starts with describing the method and giving some context for the method and previous methods that deal with the same problem. In Section 2 the authors review conceptors. This method is algebraic method closely related to spanning sub spaces and SVD. The main advantage of using conceptors is their trait of boolean logics: i.e., their ability to be added and multiplied naturally. In section 3 the authors elaborate on reviewed ocnceptors method and show how to adapt this algorithm to SGD with back-propagation. The authors provide a version with batch SGD as well.

In Section 4, the authors show their method on permuted MNIST. They compare the method to EWC with the same architecture. They show that their method more efficiently suffers on permuted MNIST from less degradation. Also, they compared the method to EWC and IMM on disjoint MNIST and again got the best performance.

In general, unlike what the authors suggest, I do not believe this method is how biological agents perform their tasks in real life. Nevertheless, the authors show that their method indeed reduce the interference generated by a new task on the old learned tasks.

I think that this work might interest the community since such methods might be part of the tools that practitioners have in order to cope with learning new tasks without destroying the previous ones.  What is missing is the following: I think that without any additional effort, a network can learn a new task in parallel to other task, or some other techniques may be used which are not bound to any algebraic methods. Therefore, my only concern is that in this comparison the work bounded to very specific group of methods, and the question of what is the best method for continual learning remained open.   ","[7, 7, 7]","[' Good paper, accept', ' Good paper, accept', ' Good paper, accept']","[3, 3, 5]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The reviewer acknowledges the paper's contribution and potential interest to the community, but expresses some reservations. While they find the method effective in reducing interference, they question its biological plausibility and suggest that simpler methods might achieve similar results. The reviewer's main concern is the limited scope of the comparison, leaving the question of the best method for continual learning open. This suggests a somewhat positive but cautious sentiment.",50.0,70.0
Initialization matters: Orthogonal Predictive State Recurrent Neural Networks,"['Krzysztof Choromanski', 'Carlton Downey', 'Byron Boots']",Accept,2018,"[7, 9, 14]","[12, 14, 19]","[129, 22, 221]","[49, 13, 112]","[68, 7, 98]","[12, 2, 11]","This paper investigates the Predictive State Recurrent Neural Networks (PSRNN) model that embed the predictive states in a Reproducible Hilbert Kernel Space and then update the predictive states given new observation in this space.
While PSRNN usually uses random features to project the map the states in a new space where dot product approximates the kernel well, the authors proposes to leverage orthogonal random features.

In particular, authors provide theoretical guarantee and show that the model using orthogonal features has a smaller upper bound on the failure probability regarding the empirical risk than the model using unstructured randomness. 

Authors then empirically validate their model on several small-scale datasets where they compare their model with PSRNN and LSTM. They observe that PSRNN with orthogonal random features leads to lower MSE on test set than both PSRNN and LSTM and seem to reach lower value earlier in training.

Question:
-	What is the cost of constructing orthogonal random features compared to RF?
-	What is the definition of H the Hadamard matrix in the discrete orthogonal joint definition?
-	What are the hyperparameters values use for the LSTM
-	Empirical evaluations seem to use relatively small datasets composed by few dozens of temporal trajectories. Did you consider larger dataset for evaluation? 
-	How did you select the maximum number of epochs in Figure 5? It seems that the validation error is still decreasing after 25 epochs?

Pros:
-	Provide theoretical guarantee for the use of orthogonal random features in the context of PSRNN
Cons:
-	Empirical evaluation only on small scale datasets.
","[7, 4, 8]","[' Good paper, accept', ' Ok but not good enough - rejection', ' Top 50% of accepted papers, clear accept']","[2, 5, 4]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is mostly positive. The reviewer acknowledges the theoretical contributions of the paper and the empirical results. However, they raise several important questions about the experiments and suggest further evaluation on larger datasets. The language used is polite and professional, posing questions and suggestions constructively.",60.0,80.0
Fidelity-Weighted Learning,"['Mostafa Dehghani', 'Arash Mehrjou', 'Stephan Gouws', 'Jaap Kamps', 'Bernhard Schölkopf']",Accept,2018,"[7, 3, 9, 21, 24]","[12, 8, 11, 26, 29]","[117, 41, 16, 281, 777]","[50, 11, 9, 217, 380]","[56, 27, 7, 24, 286]","[11, 3, 0, 40, 111]","The authors propose an approach for training deep learning models for situation where there is not enough reliable annotated data.  This algorithm can be useful because correct annotation of enough cases to train a deep model in many domains is not affordable.  The authors propose to combine a huge number of weakly annotated data with a small set of strongly annotated cases to train a model in a student-teacher framework. The authors evaluate their proposed methods on one toy problem and two real-world problems. The paper is well written, easy to follow, and have good experimental study.  My main problem with the paper is the lack of enough motivation and justification for the proposed method; the methodology seems pretty ad-hoc to me and there is a need for more experimental study to show how the methodology work. Here are some questions that comes to my mind:  (1) Why first building a student model only using the weak data and why not all the data together to train the student model? To me, it seems that the algorithm first tries to learn a good representation for which lots of data is needed and the weak training data can be useful but why not combing with the strong data? (2) What are the sensitivity of the procedure to how weakly the weak data are annotated (this could be studied using both toy example and real-world examples)? (3) The authors explicitly suggest using an unsupervised method (check Baseline no.1) to annotate data weakly? Why not learning the representation using an unsupervised learning method (unsupervised pre training)? This should be at least one of the baselines.
(4) the idea of using surrogate labels to learn representation is also not new. One example work is ""Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks"". The authors didn't compare their method with this one.","[6, 7, 5]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Marginally below acceptance threshold']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer starts with positive remarks, praising the paper's clarity, writing, and experimental study. They find the topic important and the approach potentially useful. However, they express significant concerns about the method's justification and lack of thoroughness in exploration (e.g., sensitivity analysis, comparison with a standard technique like unsupervised pre-training). The reviewer poses multiple pointed questions indicating areas needing improvement.  The tone is critical but professional and aims to improve the paper. Therefore, the sentiment is moderately positive due to the initial praise, but tempered by the numerous concerns. The politeness remains consistently professional throughout.",40.0,60.0
Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning,"['Sandeep Subramanian', 'Adam Trischler', 'Yoshua Bengio', 'Christopher J Pal']",Accept,2018,"[6, 3, 31, 22]","[10, 8, 36, 27]","[36, 82, 975, 230]","[16, 39, 405, 98]","[18, 43, 454, 109]","[2, 0, 116, 23]","This paper shows that learning sentence representations from a diverse set of tasks (skip-thought objective, MT, constituency parsing, and natural language inference) produces .
The main contribution of the paper is to show learning from multiple tasks improves the quality of the learned representations.
Experiments on various text classification and sentiment analysis datasets show that the proposed method is competitive with existing approaches.
There is an impressive number of experiments presented in the paper, but the results are a bit mixed, and it is not always clear that adding more tasks help.

I think this paper addresses an important problem of learning general purpose sentence representations. 
However, I am unable to draw a definitive conclusion from the paper. 
From Table 2, the best performing model is not always the one with more tasks. 
For example, adding a parsing objective can either improve or lower the performance quite significantly.
Could it be that datasets such as MRPC, SICK, and STSB require more understanding of syntax?
Even if this is the case, why adding this objective hurt performance for other datasets?
Importantly, it is also not clear whether the performance improvement comes from having more unlabeled data (even if it is trained with the same training objective) or having multiple training objectives.
Another question I have is that if there is any specific reason that language modeling is not included as one of the training objectives to learn sentence representations, given that it seems to be the easiest one to collect training data for.

The results for transfer learning and low resource settings are more positive.
However, it is not surprising that pretraining parts of the model on a large amount of unlabeled data helps when there is not a lot of labeled examples.

Overall, while the main contribution of the paper is that having multiple training objectives help learning better sentence, I am not yet convinced by the experiments that this is indeed the case.","[4, 8, 8]","[' Ok but not good enough - rejection', ' Top 50% of accepted papers, clear accept', ' Top 50% of accepted papers, clear accept']","[5, 5, 5]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The reviewer acknowledges the importance of the paper's topic and finds the number of experiments impressive. However, they express uncertainty about the paper's conclusions, pointing out inconsistencies in the results and raising valid questions about the method's effectiveness. The reviewer also notes the positive results in transfer learning and low-resource settings but doesn't find them surprising. Overall, the tone is critical but not dismissive, suggesting potential for improvement.",20.0,70.0
Understanding Deep Neural Networks with Rectified Linear Units,"['Raman Arora', 'Amitabh Basu', 'Poorya Mianjy', 'Anirbit Mukherjee']",Accept,2018,"[12, 15, 3, 3]","[17, 20, 7, 8]","[118, 86, 21, 17]","[66, 21, 12, 2]","[39, 21, 9, 13]","[13, 44, 0, 2]","This paper presents several theoretical results regarding the expressiveness and learnability of ReLU-activated deep neural networks. I summarize the main results as below:

(1) Any piece-wise linear function can be represented by a ReLU-acteivated DNN. Any smooth function can be approximated by such networks.

(2) The expressiveness of 3-layer DNN is stronger than any 2-layer DNN.

(3) Using a polynomial number of neurons, the ReLU-acteivated DNN can represent a piece-wise linear function with exponentially many pieces

(4) The ReLU-activated DNN can be learnt to global optimum with an exponential-time algorithm.

Among these results (1), (2), (4) are sort of known in the literature. This paper extends the existing results in some subtle ways. For (1), the authors show that the DNN has a tighter bound on the depth. For (2), the ""hard"" functions has a better parameterization, and the gap between 3-layer and 2-layer is proved bigger. For (4), although the algorithm is exponential-time, it guarantees to compute the global optimum.

The stronger results of (1), (2), (4) all rely on the specific piece-wise linear nature of ReLU. Other than that, I don't get much more insight from the theoretical result. When the input dimension is n, the representability result of (1) fails to show that a polynomial number of neurons is sufficient. Perhaps an exponential number of neurons is necessary in the worst case, but it will be more interesting if the authors show that under certain conditions a polynomial-size network is good enough.

Result (3) is more interesting as it is a new result. The authors present a constructive proof to show that ReLU-activated DNN can represent many linear pieces.  However, the construction seems artificial and these functions don't seem to be visually very complex.

Overall, this is an incremental work in the direction of studying the representation power of neural networks. The results might be of theoretical interest, but I doubt if a pragmatic ReLU network user will learn anything by reading this paper.","[6, 7, 6]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Marginally above acceptance threshold']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The reviewer acknowledges the paper's contributions as incremental improvements on existing knowledge, using phrases like ""sort of known,"" ""extends existing results in some subtle ways,"" and ""stronger results."" They find some results more interesting than others, but ultimately question the practical implications for those using ReLU networks. This suggests a lukewarm reception overall.",20.0,60.0
Improving GANs Using Optimal Transport,"['Tim Salimans', 'Han Zhang', 'Alec Radford', 'Dimitris Metaxas']",Accept,2018,"[7, 8, 3, 30]","[12, 13, 7, 35]","[60, 79, 28, 683]","[21, 33, 9, 428]","[36, 41, 19, 109]","[3, 5, 0, 146]","The paper introduces a new algorithm for training GANs based on the Earth Mover’s distance. In order to avoid biased gradients, the authors use the dual form of the distance on mini-batches, to make it more robust. To compute the distance between mini batches, they use the Sinkhorn distance. Unlike the original Sinkhorn distance paper, they use the dual form of the distance and do not have biased gradients. Unlike the Cramer GAN formulation, they use a mini-batch distance allowing for a better leverage of the two distributions, and potentially decrease variance in gradients.

Evaluation: the paper shows good results a battery of tasks, including a standard toy example, CIFAR-10 and conditional image generation, where they obtain better results than StackGAN. 

The paper is honest about its shortcomings, in the current set up the model requires a lot of computation, with best results obtained using a high batch size.

Would like to see: 
  * a numerical comparison with Cramer GAN, to see whether the additional  computational cost is worth the gains. 
  * Cramer GAN shows an increase in diversity, would like to see an analog experiment for conditional generation, like figure 3 in the Cramer GAN paper.
","[8, 6, 6]","[' Top 50% of accepted papers, clear accept', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[4, 3, 2]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']","The review is positive about the paper, highlighting its novelty, good results, and honesty about limitations. The language is constructive and suggestive, asking for additional experiments rather than framing them as flaws. There are no negative remarks, and the suggestions are presented politely. Therefore, the sentiment is positive, and the politeness is high.",75.0,80.0
Routing Networks: Adaptive Selection of Non-Linear Functions for Multi-Task Learning,"['Clemens Rosenbaum', 'Tim Klinger', 'Matthew Riemer']",Accept,2018,"[3, 15, 4]","[7, 20, 9]","[12, 38, 46]","[5, 22, 20]","[7, 15, 25]","[0, 1, 1]","Summary:
The paper suggests to use a modular network with a controller which makes decisions, at each time step, regarding the next nodule to apply. This network is suggested a tool for solving multi-task scenarios, where certain modules may be shared and others may be trained independently for each task. It is proposed to learn the modules with standard back propagation and the controller with reinforcement learning techniques, mostly tabular. 

-	page 4: 
In algorithm 2, line 6, I do not understand the reward computation. It seems that either a _{k+1} subscript index is missing for the right hand side R, or an exponent of n-k is missing on \gamma. In the current formula, the final reward affects all decisions without a decay based on the distance between action and reward gain. This issue should be corrected or explicitly stated.

The ‘collaboration reward’ is not clearly justified: If I understand correctly, It is stated that actions which were chosen often in the past get higher reward when chosen again. This may create a ‘winner takes all’ effect, but it is not clear why this is beneficial for good routing. Specifically, this term is optimized when a single action is always chosen with high probability – but such a single winner does not seem to be the behavior we want to encourage.

-	Page 5: It is not described clearly (and better: defined formally) what exactly is the state representation. It is said to include the current network output (which is a vector in R^d), the task label and the depth, but it is not stated how this information is condensed into a single integer index for the tabular methods. If I understand correctly, the state representation used in the tabular algorithms includes only the current depth. If this is true, this constitutes a highly restricted controller, making decisions only based on depth without considering the current output. 
-	The functional approximation versions are even less clear: Again it is not clear what information is contained in the state and how it is represented. In addition it is not clear in this case what network architecture is used for computation of the policy (PG) or valkue (Q-learning), and how exactly they are optimized.
-	The WPL algorithm is not clear to me
o	 In algorithm box 3, what is R_k? I do not see it defined anywhere. Is it related to \hat{R}? how?
o	Is it assumed that the actions are binary? 
o	I do not understand why positive gradients are multiplied with the action probability and negative gradients with 1 minus this probability. What is the source of a-symmetry between positive and negative gradients?  

-	Page 6:
o	It is not clear why MNist is tested over 200 examples, where there is a much larger test set available
o	In MIN-MTL I do not understand the motivation from creating superclasses composed of 5 random classes each: why do we need such arbitrary and un-natural class definitions? 

-	Page 7: 
The results on Cifar-100 are compared to several baselines, but not to the standard non-MTL solution: Solve the multi-class classification problem using a softmax loss and a unified, non routing architecture in which all the layers are shared by all classes, with the only distinction in the last classification layer. If the routing solution does not beat this standard baseline, there is no justification for its more complex structure and optimization.

-	Page 8: The author report that when training the controller with single agent methods the policy collapses into choosing a single module for most tasks. However, this is not-surprising, given that the action-based reward (whos strength is unclear) seems to promote such winner-takes-all behavior.

Overall:
-	The paper is highly unclear in its method representation
o	There is no unified clear notation. The essential symbols (states, actions, rewards) are not formally defined, and often it is not clear even if they are integers, scalars, or vectors. In notation existing, there are occasional notation errors. 
o	The reward is a) not clear, and b) not well motivated when it is explained, and c) not explicitly stated anywhere: it is said that the action-specific reward may be up to 10 times larger than the final reward, but the actual tradeoff parameter between them is not stated. Note that this parameter is important, as using a 10-times larger action-related reward means that the classification-related reward becomes insignificant.
o	The state representation used is not clear, and if I understand correctly, it includes only the current depth. This is a severely limited state representation, which does not enable to learn actions based on intermediate results
o	The continuous versions of the RL algorithms are not explained at all: no state representation, nor optimization is described.
o	The presentation suffers from severe over-generalization and lack of clarity, which disabled my ability to understand the network and algorithms for a specific case. Instead, I would recommend that in future versions of this document a single network, with a specific router  and set of decisions, and  with a single algorithm, will be explained with clear notation end-to-end

Beyond the clarity issues, I suspect also that the novelty is minor (if the state does not include any information about the current output) and that the empirical baseline is lacking. However, it is hard to judge these due to lack of clarity.


After revision:
- Most of the clarity issues were handled well, and the paper now read nicely
- It is now clear that routing is not done based on the current input (an example is not dynamically routed based on its current representation). Instead routing depends on the task and depth only.  This is still interesting, but is far from reaching context-dependent routing.
- The results presented are nice and show that task-dependent routing may be better than plain baseline or the stiching alternative.  However, since this is a task transfer issue, I believe several data size points should be tested. For example, as data size rises, the task-specific-all-fc alternative is expected to get stronger (as with more data, related task are less required for good performance).
 

- ","[6, 7, 8]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Top 50% of accepted papers, clear accept']","[3, 3, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is quite critical of the paper, pointing out numerous flaws in clarity, methodology, and novelty. The reviewer finds the paper difficult to follow due to unclear notation, poorly explained concepts, and a lack of specific examples. While the reviewer acknowledges some improvements after revision, they still have concerns about the limited scope of the routing mechanism and suggest further experiments. The tone, however, remains professional and focused on improving the paper. ",-50.0,70.0
On Unifying Deep Generative Models,"['Zhiting Hu', 'Zichao Yang', 'Ruslan Salakhutdinov', 'Eric P. Xing']",Accept,2018,"[6, 9, 16, 18]","[11, 14, 21, 23]","[112, 60, 419, 625]","[51, 24, 207, 339]","[54, 23, 201, 218]","[7, 13, 11, 68]","The authors develops a framework interpreting GAN algorithms as performing a form of variational inference on a generative model reconstructing an indicator variable of whether a sample is from the true of generative data distributions. Starting from the ‘non-saturated’ GAN loss the key result (lemma 1) shows that GANs minimizes the KL divergence between the generator(inference) distribution and a posterior distribution implicitly defined by the discriminator. I found the paper IWGAN and especially the AAVAE experiments quite interesting.  However the paper is also very dense and quite hard to follow at times - In general I think the paper would benefit from moving some content (like the wake-sleep part of the paper) to the appendix and concentrating more on the key results and a few more experiments as detailed in the comments / questions below.

Q1) What would happen if the KL-divergence minimizing loss proposed by Huszar (see e.g http://www.inference.vc/an-alternative-update-rule-for-generative-adversarial-networks/) was used instead of the “non-saturated” GAN loss - would the residial JSD terms in Lemma 1 cancel out then?

Q2) In Lemma 1 the negative JSD term looks a bit nasty to me e.g. in addition to KL divergence the GAN loss also maximises the JSD between the data and generative distributions. This JSD term acts in a somewhat opposite direction of the KL-divergence that we are interested in minimizing. Can the authors provide some more detailed comments / analysis on these two somewhat opposed terms - I find this quite important to include given the opposed direction of the JSD versus the KL term and that the JSD is ignored in e.g. section 4.1? secondly did the authors do any experiments on the the relative sizes of these two terms? I imagine it would be possible to perform some low-dimensional toy experiments where both terms were tractable to compute numerically?

Q3) I think the paper could benefit from some intuition / discussion of the posterior term q^r(x|y) in lemma 1 composed on the prior p_theta0(x) and discriminator q^r(y|x). The terms drops out nicely in math however i had a bit of a hard time wrapping my head around what minimizing the KL-divergence between this term and the inference distribution p(xIy). I know this is a kind of open ended question but i think it would greatly aid the reader in understanding the paper if more ‘guidance’ is provided instead of just writing “..by definition this is the posterior.’

Q4) In a similar vein to the above. It would be nice with some more discussion / definitions of the terms in Lemma 2. e.g what does “Here most of the components have exact correspondences (and the same definitions) in GANs and InfoGAN (see Table 1)” mean? 

Q5) The authors state that there is ‘strong connections’ between VAEs and GANs. I agree that both (after some assumptions) both minimize a KL-divergence (table 1) however to me it is not obvious how strong this relation is. Could the authors provide some discussion / thoughts on this topic?

Overall i like this work but also feel that some aspects could be improved: My main concern is that a lot of the analysis hinges on the JSD term being insignificant, but the authors to my knowledge does but provide any prof / indications that this is actually true. Secondly I think the paper would greatly benefit from concentration on fewer topics (e.g. maybe drop the RW topic as it feels a bit like an appendix) and instead provide a more throughout discussion of the theory (lemma 1 + lemma 2) as well as some more experiments wrt JSD term.
","[6, 7, 7]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Good paper, accept']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer finds the paper interesting, particularly the IWGAN and AAVAE experiments. They see value in the core idea but find the paper dense and hard to follow at times. The reviewer suggests moving some content to the appendix and expanding on key results and experiments. The numerous questions and suggestions for improvement indicate a desire to see the paper strengthened rather than rejected. The language is polite, using professional academic discourse and framing suggestions constructively.",50.0,75.0
Improving the Improved Training of Wasserstein GANs: A Consistency Term and Its Dual Effect,"['Xiang Wei', 'Boqing Gong', 'Zixia Liu', 'Wei Lu', 'Liqiang Wang']",Accept,2018,"[23, 10, 4, 5, 16]","[28, 15, 7, 10, 21]","[56, 154, 11, 59, 181]","[18, 75, 9, 18, 98]","[4, 72, 2, 3, 31]","[34, 7, 0, 38, 52]","This paper continues a trend of incremental improvements to Wasserstein GANs (WGAN), where the latter were proposed in order to alleviate the difficulties encountered in training GANs. Originally, Arjovsky et al.  [1] argued that the Wasserstein distance was superior to many others typically used for GANs. An important feature of WGANs is the requirement for the discriminator to be 1-Lipschitz, which [1] achieved simply by clipping the network weights. Recently, Gulrajani et al. [2] proposed a gradient penalty ""encouraging"" the discriminator to be 1-Lipschitz. However, their approach estimated continuity on points between the generated and the real samples, and thus could fail to guarantee Lipschitz-ness at the early training stages. The paper under review overcomes this drawback by estimating the continuity on perturbations of the real samples. Together with various technical improvements, this leads to state-of-the-art practical performance both in terms of generated images and in semi-supervised learning.  

In terms of novelty, the paper provides one core conceptual idea followed by several tweaks aimed at improving the practical performance of GANs. The key conceptual idea is to perturb each data point twice and use a Lipschitz constant to bound the difference in the discriminator’s response on the perturbed points.  The proposed method is used in eq. (6) together with the gradient penalty from [2]. The authors found that directly perturbing the data with Gaussian noise led to inferior results and therefore propose to perturb the hidden layers using dropout. For supervised learning they demonstrate less overfitting for both MNIST and CIFAR 10.  They also extend their framework to the semi-supervised setting of Salismans et al 2016 and report improved image generation. 

The authors do an excellent comparative job in presenting their experiments. They compare numerous techniques (e.g., Gaussian noise, dropout) and demonstrates the applicability of the approach for a wide range of tasks. They use several criteria to evaluate their performance (images, inception score, semi-supervised learning, overfitting, weight histogram) and compare against a wide range of competing papers. 

Where the paper could perhaps be slightly improved is writing clarity. In particular, the discussion of M and M' is vital to the point of the paper, but could be written in a more transparent manner. The same goes for the semi-supervised experiment details and the CIFAR-10 augmentation process. Finally, the title seems uninformative. Almost all progress is incremental, and the authors modestly give credit to both [1] and [2], but the title is neither memorable nor useful in expressing the novel idea. 
[1] Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein gan.

[2] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. Improved training of wasserstein gans. 

","[7, 4, 6]","[' Good paper, accept', ' Ok but not good enough - rejection', ' Marginally above acceptance threshold']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review starts with praising the paper for improving the state-of-the-art performance and addressing a drawback of previous methods. It acknowledges the novelty of the core idea and praises the experimental evaluation. While it suggests minor improvements in writing clarity and title, these are framed as suggestions rather than strong criticisms. Overall, the tone is positive and encouraging.",75.0,80.0
Unsupervised Neural Machine Translation,"['Mikel Artetxe', 'Gorka Labaka', 'Eneko Agirre', 'Kyunghyun Cho']",Accept,2018,"[2, 17]","[6, 22]","[3, 149]","[3, 84]","[0, 4]","[0, 61]","The authors present a model for unsupervised NMT which requires no parallel corpora between the two languages of interest. While the results are interesting I find very few original ideas in this paper. Please find my comments/questions/suggestions below:

1) The authors mention that there are 3 important aspects in which their model differs from a standard NMT architecture. All the 3 differences have been adapted from existing works. The authors clearly acknowledge and cite the sources. Even sharing the encoder using cross lingual embeddings has been explored in the context of multilingual NER (please see https://arxiv.org/abs/1607.00198). Because of this I find the paper to be a bit lacking on the novelty quotient. Even backtranslation has been used successfully in the past (as acknowledged by the authors). Unsupervised MT in itself is not a new idea (again clearly acknowledged by the authors).

2) I am not very convinced about the idea of denoising. Specifically, I am not sure if it will work for arbitrary language pairs. In fact, I think there is a contradiction even in the way the authors write this. On one hand, they want to ""learn the internal structure of the languages involved"" and on the other hand they deliberately corrupt this structure by adding noise. This seems very counter-intuitive and in fact the results in Table 1 suggest that it leads to a drop in performance. I am not very sure that the analogy with autoencoders holds in this case.

3) Following up on the above question, the authors mention that ""We emphasize, however, that it is not possible to use backtranslation alone without denoising"". Again, if denoising itself leads to a drop in the performance as compared to the nearest neighbor baseline then why use backtranslation in conjunction with denoising and not in conjunction with the baseline itself. 

4) This point is more of a clarification and perhaps due to my lack of understanding. Backtranslation to generate a pseudo corpus makes sense only after the model has achieved a certain (good) performance. Can you please provide details of how long did you train the model (with denoising?) before producing the backtranslations ?

5) The authors mention that 100K parallel sentences may be insufficient for training a NMT system. However, this size may be decent enough for  a PBSMT system. It would be interesting to see the performance of a PBSMT system trained on 100K parallel sentences. 

6) How did you arrive at the beam size of 12 ? Was this a hyperparameter? Just curious.

7) The comparable NMT set up is not very clear. Can you please explain it in detail ? In the same paragraph, what exactly do you mean by ""the supervised system in this paper is relatively small?""","[6, 5, 7]","[' Marginally above acceptance threshold', ' Marginally below acceptance threshold', ' Good paper, accept']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The reviewer raises several concerns and questions, indicating skepticism towards the novelty and effectiveness of the proposed method. They find the lack of original ideas and question the denoising approach, suggesting potential contradictions and limitations. While the reviewer maintains a professional tone, their pointed questions and critical analysis reflect a rather negative sentiment.",-30.0,60.0
Deep Gaussian Embedding of Graphs: Unsupervised Inductive Learning via Ranking,"['Aleksandar Bojchevski', 'Stephan Günnemann']",Accept,2018,"[2, 10]","[7, 15]","[38, 274]","[17, 145]","[19, 104]","[2, 25]","This paper proposes Graph2Gauss (G2G), a node embedding method that embeds nodes in attributed graphs (can work w/o attributes as well) into Gaussian distributions rather than conventionally latent vectors. By doing so, G2G can reflect the uncertainty of a node's embedding. The authors then use these Gaussian distributions and neighborhood ranking constraints to obtain the final node embeddings. Experiments on link prediction and node classification showed improved performance over several strong embedding methods. Overall, the paper is well-written and the contributions are remarkable. The reason I am giving a less possible rating is that some statements are questionable and can severely affect the conclusions claimed in this paper, which therefore requires the authors' detailed response. I am certainly willing to change my rating if the authors clarify my questions.

Major concern 1: Is the latent vector dimension L really the same for G2G and other compared methods? 
In the first paragraph of Section 4, it is stated that ""in all experiments if the competing techniques use an embedding of
dimensionality L, G2G’s embedding is actually only half of this dimensionality so that the overall number of ’parameters’ per node (mean vector + variance terms) matches L.""  This setting can be wrong since the degree of freedom of a L-dim Gaussian distribution should be L+L(L-1)/2, where the first term corresponds to the mean and the second term corresponds to the covariance. If I understand it correctly, when any compared embedding method used an L-dim vector, the authors used the dimension of L/2. But this setting is wrong if one wants the overall number of ’parameters’ per node (mean vector + variance terms) matches L, as stated by the authors. Fixing L, the equivalent dimension L_G2G for G2G should be set such that L_G2G +L_G2G (L_G2G -1)/2=L, not 2*L_G2G=L.  Since this setting is universal to the follow-up analysis and may severely degrade the performance of GSG due to less embedding dimensions, I hope the authors can clarify this point.

Major concern 2: The claim on inductive learning
Inductive learning is one of the major contributions claimed in this paper. The authors claim G2G can learn an embedding of an unseen node solely based on their attributes. However, is it not clear why this can be done. In the learning stage of Sec. 3.3, the attributes do not seem to play a role in the energy function. Also, since no algorithm descriptions are available, it's not clear how using only an unseen node's attributes can yield a good embedding under G2G work (so does Sec. 4.5). 
Moreover, how does it compare to directly using raw user attributes for these tasks?

Minor concern/suggestions: The ""similarity"" measure in section 3.1 using KL divergence should be better rephased by ""dissimilarity"" measure. Otherwise, one has a similarity measure $Delta$ and wants it to increase as the hop distance k decreases (closer nodes are more similar). But the ranking constraints are somewhat counter-intuitive because you want $Delta$ to be small if nodes are closer. There is nothing wrong with the ranking condition, but rather an inconsistency between the use of ""similarity"" measure for KL divergence. 
","[7, 7, 6]","[' Good paper, accept', ' Good paper, accept', ' Marginally above acceptance threshold']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer acknowledges the paper's merits, praising its writing and contributions as remarkable. This suggests a positive sentiment. However, they raise major concerns that could significantly impact the paper's conclusions, leading to a more cautious overall sentiment. While eager to revise their assessment based on the authors' clarifications, the current presence of potential flaws prevents a fully positive rating. The language used is polite and professional throughout, focusing on constructive criticism and suggestions for improvement rather than harsh negativity.",60.0,80.0
Parametrized Hierarchical Procedures for Neural Programming,"['Roy Fox', 'Richard Shin', 'Sanjay Krishnan', 'Ken Goldberg', 'Dawn Song', 'Ion Stoica']",Accept,2018,"[12, 9, 5, 32, 20, 24]","[17, 13, 10, 37, 25, 29]","[51, 38, 96, 498, 440, 431]","[21, 21, 47, 282, 254, 276]","[29, 16, 33, 114, 154, 97]","[1, 1, 16, 102, 32, 58]","In the paper titled ""Parameterized Hierarchical Procedures for Neural Programming"", the authors proposed ""Parametrized Hierarchical Procedure (PHP)"", which is a representation of a hierarchical procedure by differentiable parametrization. Each PHP is represented with two multi-layer perceptrons with ReLU activation, one for its operation statement and one for its termination statement. With two benchmark tasks (NanoCraft and long-hand addition), the authors demonstrated that PHPs are able to learn neural programs accurately from smaller amounts of strong/weak supervision. 

Overall the paper is well-written with clear logic and accurate narratives. The methodology within the paper appears to be reasonable to me. Because this is not my research area, I cannot judge its technical contribution. ","[6, 6, 6]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[1, 2, 3]","["" The reviewer's evaluation is an educated guess"", ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer explicitly states the paper is ""well-written with clear logic and accurate narratives"" and that the methodology ""appears to be reasonable"".  They are slightly hesitant as it's not their area, but overall the tone is positive. Therefore, the sentiment is positive, but not overly so due to the reviewer's lack of expertise in the specific area. The language used is entirely professional and polite.",60.0,100.0
Eigenoption Discovery through the Deep Successor Representation,"['Marlos C. Machado', 'Clemens Rosenbaum', 'Xiaoxiao Guo', 'Miao Liu', 'Gerald Tesauro', 'Murray Campbell']",Accept,2018,"[8, 3, 6, 8, 32, 38]","[13, 7, 11, 13, 37, 42]","[51, 12, 86, 50, 118, 78]","[19, 5, 39, 24, 67, 36]","[28, 7, 38, 23, 28, 23]","[4, 0, 9, 3, 23, 19]","Eigenoption Discovery Through the Deep Successor Representation

The paper is a follow up on previous work by Machado et al. (2017) showing how proto-value functions (PVFs) can be used to define options called “eigenoptions”. In essence, Machado et al. (2017) showed that, in the tabular case, if you interpret the difference between PVFs as pseudo-rewards you end up with useful options. They also showed how to extend this idea to the linear case: one replaces the Laplacian normally used to build PVFs with a matrix formed by sampling differences phi(s') - phi(s), where phi are features. The authors of the current submission extend the approach above in two ways: they show how to deal with stochastic dynamics and how to replace a linear model with a nonlinear one. Interestingly, the way they do so is through the successor representation (SR). Stachenfeld et al. (2014) have showed that PVFs can be obtained as a linear transformation of the eigenvectors of the matrix formed by stacking all SRs of an MDP. Thus, if we have the SR matrix we can replace the Laplacian mentioned above. This provides benefits already in the tabular case, since SRs naturally extend to domains with stochastic dynamics. On top of that, one can apply a trick similar to the one used in the linear case --that is,  construct the matrix representing the diffusion model by simply stacking samples of the SRs. Thus, if we can learn the SRs, we can extend the proposed approach to the nonlinear case. The authors propose to do so by having a deep neural network similar to Kulkarni et al. (2016)'s Deep Successor Representation. The main difference is that, instead of using an auto-encoder, they learn features phi(s) such that the next state s' can be recovered from it (they argue that this way psi(s) will retain information about aspects of the environment the agent has control over).

This is a well-written paper with interesting (and potentially useful) insights. I only have a few comments regarding some aspects of the paper that could perhaps be improved, such as the way eigenoptions are evaluated.

One question left open by the paper is the strategy used to collect data in order to compute the diffusion model (and thus the options). In order to populate the matrix that will eventually give rise to the PVFs the agent must collect transitions. The way the authors propose to do it is to have the agent follow a random policy. So, in order to have options that lead to more direct, ""purposeful"" behaviour, the agent must first wander around in a random, purposeless, way, and hope that this will lead to a reasonable exploration of the state space. 

This problem is not specific to the proposed approach, though: in fact, any method to build options will have to resolve the same issue. One related point that is perhaps more specific to this particular work is the strategy used to evaluate the options built: the diffusion time, or the expected number of steps between any two states of an MDP when following a random walk. First, although this metric makes intuitive sense, it is unclear to me how much it reflects control performance, which is what we ultimately care about. Perhaps more important, measuring performance using the same policy used to build the options (the random policy) seems somewhat unsatisfactory to me. To see why, suppose that the options were constructed based on data collected by a non-random policy that only visits a subspace of the state space. In this case it seems likely that the decrease in the diffusion time would not be as apparent as in the experiments of the paper. Conversely, if the diffusion time were measured under another policy, it also seems likely that options built with a random policy would not perform so well (assuming that the state space is reasonably large to make an exhaustive exploration infeasible). More generally, we want options built under a given policy to reduce the diffusion time of other policies (preferably ones that lead to good control performance).

Another point associated with the evaluation of the proposed approach is the method used to qualitatively assess options in the Atari experiments described in Section 4.2. In the last paragraph of page 7 the authors mention that eigenoptions are more effective in reducing the diffusion time than “random options” built based on randomly selected sub-goals. However, looking at Figure 4, the terminal states of the eigenoptions look a bit like randomly-selected  sub-goals. This is especially true when we note that only a subset of the options are shown: given enough random options, it should be possible to select a subset of them that are reasonably spread across the state space as well. 

Interestingly, one aspect of the proposed approach that seems to indeed be an improvement over random options is made visible by a strategy used by the authors to circumvent computational constraints. As explained in the second paragraph of page 8, instead of learning policies to maximize the pseudo-rewards associated with eigenoptions the authors used a myopic policy that only looks one step ahead (which is the same as having a policy learned with a discount factor of zero). The fact that these myopic policies are able to navigate to specific locations and stay there suggests that the proposed approach gives rise to dense pseudo-rewards that are very informative. As a comparison, when we define a random sub-goal the resulting reward is a very sparse signal that would almost certainly not give rise to useful myopic policies. Therefore, one could argue that the proposed approach not only generate useful options, it also gives rise to dense pseudo-rewards that make it easier to build the policies associated with them.","[7, 6, 9]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Top 15% of accepted papers, strong accept']","[4, 3, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review is positive overall. The reviewer finds the paper well-written, the insights interesting and potentially useful.  The reviewer does not express any negative sentiment towards the authors or the paper. They offer constructive criticism and suggestions for improvement, but the tone remains positive and encouraging. The reviewer acknowledges the challenges and limitations of the approach but frames them as opportunities for further exploration.",60.0,90.0
Skip Connections Eliminate Singularities,"['Emin Orhan', 'Xaq Pitkow']",Accept,2018,"[2, 8]","[7, 13]","[17, 33]","[4, 12]","[12, 17]","[1, 4]","This paper proposes to explain the benefits of skip connections in terms of eliminating the singularities of the loss function. The discussion is largely based on a sequence of experiments, some of which are interesting and insightful. The discussion here can be useful for other researchers. 

My main concern is that the result here is purely empirical, with no concrete theoretical justification. What the experiments reveal is an empirical correlation between the Eigval index and training accuracy, which can be caused by lots of reasons (and cofounders), and does not necessarily establish a causal relation. Therefore, i found many of the discussion to be questionable. I would love to see more solid theoretical discussion to justify the hypothesis proposed in this paper.
 
Do you have a sense how accurate is the estimation of the tail probabilities of the eigenvalues? Because the whole paper is based on the approximation of the eigval indexes, it is critical to exam the estimation is accurate enough to draw the conclusions in the paper. 

All the conclusions are based on one or two datasets. Could you consider testing the result on more different datasets to verify if the results are generalizable? ","[6, 8, 8]","[' Marginally above acceptance threshold', ' Top 50% of accepted papers, clear accept', ' Top 50% of accepted papers, clear accept']","[4, 3, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review starts with a positive note, acknowledging the interesting and insightful experiments. However, it then expresses a ""main concern,"" which is a lack of theoretical justification. The reviewer finds many discussions questionable and desires more solid theoretical backing. The questions posed are also probing potential weaknesses in the paper's core arguments. While the language is critical, it maintains a professional and constructive tone. Therefore, the sentiment leans slightly negative due to the significant concerns raised, but the politeness remains positive due to the constructive phrasing and suggestions for improvement.",-20.0,70.0
Distributed Prioritized Experience Replay,"['Dan Horgan', 'John Quan', 'David Budden', 'Gabriel Barth-Maron', 'Matteo Hessel', 'Hado van Hasselt', 'David Silver']",Accept,2018,"[4, 2, 9, 20]","[9, 7, 14, 25]","[40, 68, 499, 631]","[6, 18, 124, 206]","[4, 28, 170, 111]","[30, 22, 205, 314]","This paper examines a distributed Deep RL system in which experiences, rather than gradients, are shared between the parallel workers and the centralized learner. The experiences are accumulated into a central replay memory and prioritized replay is used to update the policy based on the diverse experience accumulated by all the of the workers. Using this system, the authors are able to harness much more compute to learn very high quality policies in little time. The results very convincingly show that Ape-X far outperforms competing algorithms such as recently published Rainbow. 

It’s hard to take issue with a paper that has such overwhelmingly convincing experimental results. However, there are a couple additional experiments that would be quite nice:
•	In order to understand the best way for training a distributed RL agent, it would be nice to see a side-by-side comparison of systems for distributed gradient sharing (e.g. Gorila) versus experience sharing (e.g. Ape-X). 
•	It would be interesting to get a sense of how Ape-X performs as a function of the number of frames it has seen, rather than just wall-clock time. For example, in Table 1, is Ape-X at 200M frames doing better than Rainbow at 200M frames?

Pros:
•	Well written and clear.
•	Very impressive results.
•	It’s remarkable that Ape-X preforms as well as it does given the simplicity of the algorithm.

Cons:
•	Hard to replicate experiments without the deep computational pockets of DeepMind.
","[9, 6, 7]","[' Top 15% of accepted papers, strong accept', ' Marginally above acceptance threshold', ' Good paper, accept']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer clearly states the results are ""overwhelmingly convincing"" and ""impressive"", which points to a positive sentiment. They provide constructive suggestions for further experiments but acknowledge the difficulty of replicating the research due to its computational demands. The language used is respectful and professional throughout.",85.0,90.0
Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks,"['Shiyu Liang', 'Yixuan Li', 'R. Srikant']",Accept,2018,"[5, 4, 29]","[10, 9, 34]","[20, 55, 378]","[9, 23, 165]","[7, 29, 75]","[4, 3, 138]","
-----UPDATE------

The authors addressed my concerns satisfactorily. Given this and the other reviews I have bumped up my score from a 5 to a 6.

----------------------


This paper introduces two modifications that allow neural networks to be better at distinguishing between in- and out- of distribution examples: (i) adding a high temperature to the softmax, and (ii) adding adversarial perturbations to the inputs. This is a novel use of existing methods.

Some roughly chronological comments follow:

In the abstract you don't mention that the result given is when CIFAR-10 is mixed with TinyImageNet.

The paper is quite well written aside from some grammatical issues. In particular, articles are frequently missing from nouns. Some sentences need rewriting (e.g. in 4.1 ""which is as well used by Hendrycks..."", in 5.2 ""performance becomes unchanged"").

 It is perhaps slightly unnecessary to give a name to your approach (ODIN) but in a world where there are hundreds of different kinds of GANs you could be forgiven.

I'm not convinced that the performance of the network for in-distribution images is unchanged, as this would require you to be able to isolate 100% of the in-distribution images. I'm curious as to what would happen to the overall accuracy if you ignored the results for in-distribution images that appear to be out-of-distribution (e.g. by simply counting them as incorrect classifications). Would there be a correlation between difficult-to-classify images, and those that don't appear to be in distribution?

When you describe the method it relies on a threshold delta which does not appear to be explicitly mentioned again.

In terms of experimentation it would be interesting to see the reciprocal of the results between two datasets. For instance, how would a network trained on TinyImageNet cope with out-of-distribution images from CIFAR 10?

Section 4.5 felt out of place, as to me, the discussion section flowed more naturally from the experimental results. This may just be a matter of taste.

I did like the observations in 5.1 about class deviation, although then, what would happen if the out-of-distribution dataset had a similar class distribution to the in-distribution one? (This is in part, addressed in the CIFAR80 20 experiments in the appendices).

This appears to be a borderline paper, as I am concerned that the method isn't sufficiently novel (although it is a novel use of existing methods).

Pros:
- Baseline performance is exceeded by a large margin
- Novel use of adversarial perturbation and temperature
- Interesting analysis

Cons:
- Doesn't introduce and novel methods of its own
- Could do with additional experiments (as mentioned above)
- Minor grammatical errors
","[6, 6, 9]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Top 15% of accepted papers, strong accept']","[4, 3, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer acknowledges the novelty of the paper's approach and praises its performance, analysis, and writing. However, they also express concerns about the lack of novel methods and suggest additional experiments. The update at the beginning, where the reviewer increased their score based on the authors' response, indicates a positive shift in their opinion. The language used is constructive and professional throughout.",60.0,80.0
FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling,"['Jie Chen', 'Tengfei Ma', 'Cao Xiao']",Accept,2018,"[11, 3, 4]","[16, 8, 9]","[79, 81, 149]","[23, 38, 66]","[33, 37, 62]","[23, 6, 21]","The paper presents a novel view of GCN that interprets graph convolutions as integral transforms of embedding functions. This addresses the issue of lack of sample independence in training and allows for the use of Monte Carlo methods. It further explores variance reduction to speed up training via importance sampling.  The idea comes with theoretical support and experimental studies.

Some questions are as follows:

1) could you elaborate on n/t_l  in (5) that accounts for the normalization difference between matrix form (1) and the integral form (2) ?

2) In Prop.2., there seems no essential difference between the two parts, as e(v) also depends on how the u_j's are sampled.

3) what loss g is used in experiments?","[7, 6, 8, 7]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Top 50% of accepted papers, clear accept', ' Good paper, accept']","[2, 4, 4, 4]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with positive statements, highlighting the novelty and contribution of the paper. It acknowledges the theoretical support and experimental validation. While it raises some questions, these are presented constructively as requests for clarification or elaboration, rather than criticisms. The language is formal and objective.",60.0,80.0
Model-Ensemble Trust-Region Policy Optimization,"['Thanard Kurutach', 'Ignasi Clavera', 'Yan Duan', 'Aviv Tamar', 'Pieter Abbeel']",Accept,2018,"[4, 2, 11, 8, 17]","[7, 4, 15, 13, 22]","[19, 18, 52, 108, 608]","[9, 8, 28, 47, 291]","[10, 10, 19, 57, 291]","[0, 0, 5, 4, 26]","This paper presents a simple model-based RL approach, and shows that with a few small tweaks to more ""typical"" model-based procedures, the methods can substantially outperform model-free methods on continuous control tasks.  In particular, the authors show that by 1) using an ensemble of models instead of a single models, 2) using TRPO to optimize the policy based upon these models (rather that analytical gradients), and 3) using the model ensemble to validate when to stop policy optimization, then a simple model-based approach actually can outperform model-free methods.

Overall, I think this is a nice paper, and worth accepting.  There is very little actually new here, of course: the actual model-based method is entirely standard except with the additions above (which are also all fairly standard approaches in isolation).  But at a higher level, the fact that such simple model-based approaches work better than somewhat complex model free approaches actually is the point of the paper to me.  While the general theme of model-based RL outperforming model-free RL is not new (Atkeson and Santamaria (1997) comes to a similar conclusion) its good to see this same pattern demonstrated ""officially"" on modern RL benchmarks, especially since the _completely_ naive strategy of using a single model and more standard policy optimization doesn't perform as well.

Naturally, there is some question as to whether the work here is novel enough to warrant publication, but I think the overall message of the paper is strong enough to overcome fairly minimal contribution from an algorithmic perspective.  I did also have a few general concerns that I think could be discussed with a bit more detail in the paper:
1) The choice of this particular model ensemble to represent uncertainty seems rather ad-how.  Why is it sufficient to simply learn N models with different initial weights?  It seems that the likely cause for this is that the random initial weights may lead to very different behavior in the unobserved parts of the space (i.e., portions of the state space where we have no samples), and thus.  But it seems like there are much more principled ways of overcoming this same problem, e.g. by using an actual Bayesian neural net, directly modeling uncertainty in the forward model, or using generative model approaches.  There's some discussion of this point in the introduction, but I think a bit more explanation about why the model ensemble is expected to work well for this purpose.
2) Likewise, the fact the TRPO outperforms more standard gradient methods is somewhat surprising.  How is the model ensemble being treated during BPTT?  In the described TRPO method, the authors use a different model at each time step, sampling uniformly.  But it seems like a single model is used for each rollout in the proposed BPTT method?  If so, it's not surprising that this approach performs worse.  But it seems like one could backprop through the different per-timestep models just as easily, and it would remove one additional source of difference between the two settings.","[7, 6, 7]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Good paper, accept']","[5, 3, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer clearly states that this is ""a nice paper, and worth accepting"", which indicates positive sentiment. They acknowledge the limitations of the work but ultimately find the results compelling. The language used is constructive and professional throughout, suggesting a polite and respectful tone.",60.0,80.0
Deep Sensing: Active Sensing using Multi-directional Recurrent Neural Networks,"['Jinsung Yoon', 'William R. Zame', 'Mihaela van der Schaar']",Accept,2018,"[4, 22, 19]","[9, 26, 24]","[75, 45, 829]","[27, 16, 344]","[33, 17, 230]","[15, 12, 255]","This paper proposes a novel method to solve the problem of active sensing from a new angle (Essentially, the active sensing is a kind of method that decides when (or where) to take new measurements and what measurements we should conduct at that time or (place)). By taking advantage of the characteristics of long-term memory and Bi-directionality of Bi-RNN and M-RNN, deep sensing can model multivariate time-series signals for predicting future labels and estimating the values of new measurements. The architecture of Deep Sensing basically consists of three components: 
1. Interpolation and imputation for each of channels where missing points exist;
2. Prediction for the future labels in terms of the whole multivariate signals (The signal is a time-series data and made up of multiple channels, there is supposed to be a measured label for each moment of the signal); 
3. Active sensing for the future moments of each of the channels. 

Pros

The novelty of this paper lies in using a neural network structure to solve a traditional statistical problem which was usually done by a Bayesian approach or using the idea of the stochastic process. 

A detailed description of the network architecture is provided and each of the configurations has been fully illustrated.  The explanation of the structure of the combined RNNs is rigorous but clear enough of understanding. 

The method was tested on a large real dataset and got a really promising result based several rational assumptions (such as assuming some of the points are missing for evaluating the error of the interpolation & imputation).

Cons

How and why the architecture is designed in this way should be further discussed or explained. Some of the details of the design could be inferred indirectly. But somewhere like the structure of the interpolation in Fig.3 doesn't have any further discussion. For example, why using GRU based RNN, and how Bi-RNN benefits here. 
","[7, 8, 6]","[' Good paper, accept', ' Top 50% of accepted papers, clear accept', ' Marginally above acceptance threshold']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review acknowledges the novelty of the paper and praises its clarity, detailed descriptions, and promising results. However, it also points out areas for improvement, specifically regarding the justification and explanation of certain design choices. The language used is constructive and objective, suggesting a positive overall sentiment with room for improvement.",60.0,70.0
Graph Attention Networks,"['Petar Veličković', 'Guillem Cucurull', 'Arantxa Casanova', 'Adriana Romero', 'Pietro Liò', 'Yoshua Bengio']",Accept,2018,['only informal'],['only informal'],[],[],[],[],"This paper has proposed a new method for classifying nodes of a graph. Their method can be used in both semi-supervised scenarios where the label of some of the nodes of the same graph as the graph in training is missing (Transductive) and in the scenario that the test is on a completely new graph (Inductive).
Each layer of the network consists of feature representations for all of the nodes in the Graph. A linear transformation is applied to all the features in one layer and the output of the layer is the weighted sum of the transformed neighbours (including the node). The attention logit between node i and its neighbour k is calculated by a one layer fully connected network on top of the concatenation of the transformed representation of node i and transformed representation of the neighbour k. They also can incorporate the multi-head attention mechanism and average/concatenate the output of each head.

Originality:
Authors improve upon GraphSAGE by replacing the aggregate and sampling function at each layer with an attention mechanism. However, the significance of the attention mechanism has not been studied in the experiments. For example by reporting the results when attention is turned off (1/|N_i| for every node) and only a 0-1 mask for neighbours is used. They have compared with GraphSAGE only on PPI dataset. I would change my rating if they show that the 33% gain is mainly due to the attention in compare to other hyper-parameters. [The experiments are now more informative. Thanks]
Also, in page 4 authors claim that GraphSAGE is limited because it samples a neighbourhood of each node and doesn't aggregate over all the neighbours in order to keep its computational footprint consistent. However, the current implementation of the proposed method is computationally equal to using all the vertices in GraphSAGE.

Pros:
- Interesting combination of attention and local graph representation learning. 
- Well written paper. It conveys the idea clearly.
- State-of-the-art results on three datasets.

Cons:
- When comparing with spectral methods it would be better to mention that the depth of embedding propagation in this method is upper-bounded by the depth of the network. Therefore, limiting its adaptability to broader class of graph datasets. 
- Explaining how attention relates to previous body of work in embedding propagation and when it would be more powerful.","[6, 5, 7]","[' Marginally above acceptance threshold', ' Marginally below acceptance threshold', ' Good paper, accept']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The reviewer provides constructive criticism, acknowledges the paper's strengths (well-written, interesting combination of techniques, state-of-the-art results), and suggests specific improvements (further experiments, discussion of limitations). This suggests a positive overall sentiment, aiming to guide the authors towards a stronger publication. The language remains polite throughout, employing professional and objective phrasing.",60.0,80.0
Unsupervised Representation Learning by Predicting Image Rotations,"['Spyros Gidaris', 'Praveer Singh', 'Nikos Komodakis']",Accept,2018,"[4, 5, 15]","[9, 10, 19]","[38, 30, 130]","[18, 8, 71]","[19, 17, 28]","[1, 5, 31]","Strengths:
* Very simple strategy for unsupervised learning of deep image features. Simplicity of approach is a good quality in my view.
* The rationale for the effectiveness of the approach is explained well.
* The representation learned from unlabeled data is shown to yield strong results on image categorization (albeit mostly in scenarios where the unsupervised features have been learned from the *same* dataset where classification is performed -- more on this below).
* The image rotations are implemented in terms of flipping and transposition, which do not create visual artifacts easily recognizable by deep models.

Weaknesses:
* There are several obvious additional experiments that, in my view, would greatly strengthen this work:
1. Nearly all of the image categorization results (with the exception of those in Table 4) are presented for the contrived scenario where the unsupervised representation is learned from the same training set as the one used for the final supervised training of the categorization model. This is a useless application scenario. If labels for the training examples are available, why not using them for feature learning given that this leads to improved performance (see results in Tables)? More importantly, this setup does not allow us to understand how general the unsupervised features are. Maybe they are effective  precisely because they have been learned from images of the 10 classes that the final classifier needs to distinguish... I would have liked to see some results involving unsupervised learning from a dataset that may contain classes different from those of the final test classification or, even better, from a dataset of randomly selected images that lack categorical coherence (e.g., photos randomly picked from the Web, such as Flickr pics).
2. In nearly all the experiments, the classifier is built on top of the frozen unsupervised features. This is in contrast with the common practice of finetuning the entire pretrained unsupervised net on the supervised task. It'd be good to know why the authors opted for the different setup and to see in any case some supervised finetuning results.
3. It would be useful to see the accuracy per class both when using unsupervised features as well as fully-supervised features. There are many objects that have a canonical pose/rotation in the world. Forcing the unsupervised features to distinguish rotations of such objects may affect the recognition accuracy for these classes. Thus, my request for seeing how the unsupervised learning affects class-specific accuracy.
4. While the results in Table 2 are impressive, it appears that the different unsupervised learning methods reported in this table are based on different architectures. This raises the question of whether performance gains are due to the better mechanism for unsupervised learning or rather the better network architecture.
5. I do understand that using only 0, 90, 180 and 270 degree rotations eliminates the issue of potentially recognizable artifacts. Nevertheless, it'd be interesting to see what happens empirically when the number of discrete rotations is increased, e.g., by including 45, 135, 225 and 315 degree rotations. And what happens if you use only 0 and 180? Or only 90 and 270?
* While the paper is easy to understand, at times the writing is poor and awkward (e.g., opening sentence of intro, first sentence in section 2.2).","[6, 6, 6]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[4, 3, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The reviewer provides a balanced perspective, outlining both strengths and weaknesses. They acknowledge the simplicity and effectiveness of the approach, demonstrating a positive sentiment. However, they also point out significant limitations and suggest several experiments to strengthen the work. The language, while direct and critical in places, maintains a professional and constructive tone. The criticism is specific and aimed at improving the paper, not attacking the authors.",40.0,60.0
The Role of Minimal Complexity Functions in Unsupervised Learning of Semantic Mappings,"['Tomer Galanti', 'Lior Wolf', 'Sagie Benaim']",Accept,2018,"[2, 19, 6]","[7, 24, 11]","[37, 413, 53]","[13, 213, 20]","[22, 165, 28]","[2, 35, 5]","Quality:
The paper appears to be correct

Clarity:
the paper is clear, although more formalization would help sometimes

Originality
The paper presents an analysis for unsupervised learning of mapping between 2 domains that is totally new as far as I know.

Significance
The points of view defended in this paper can be a basis for founding a general theory for unsupervised learning of mappings between domains.

Pros/cons
Pros
-Adresses an important problem in representation learning
-The paper proposes interesting assumptions and results for measuring the complexity of semantic mappings
-A new cross domain mapping is proposed
-Large set of experiments
Cons
-Some parts deserve more formalization/justification
-Too many materials for a conference paper
-The cost of the algorithm seems high 

Summary:
This paper studies the problem of unsupervised learning of semantic mappings. It proposes a notion of low complexity networks in this context used for identifying  minimal complexity mappings which is assumed to be central for recovering the best cross domain mapping. A theoretical result shows that the number of low-discrepancy (between cross-domains) mappings of low complexity is rather small.
A large set of experiments are provided to support the claims of the paper.


Comments:

-The work is interesting, for an important problemin representation learning, while in machine learning in general with the unsupervised aspect.

-In a sense, I find that the approach suggested by algorithm 1 has some connections with structural risk minimization: by increasing k1 and k2 - when looking for the mapping - you increase the complexity of the model searched while trying to optimize the risk which is measured by the discrepancies and loss.
The approach seems costly anyway and I wonder if the authors could think of a smoother version of the algorithm to make it more efficient.

-For counting the minimal complexity mappings, I wonder if one can make a connection with Algorithm robustness of Xu&Mannor(COLT,2012) where instead of comparing losses, you work with discrepancies.

Typo:
Section 5.1 is build of -> is built of
","[7, 6, 7]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Good paper, accept']","[2, 4, 4]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer finds the paper interesting, addressing an important problem with a novel approach. They praise the clarity, originality, and significance of the work. While they list several areas for improvement, these are presented as constructive suggestions rather than harsh criticisms. The reviewer also acknowledges the strengths of the paper, such as the large set of experiments and the interesting assumptions and results. Overall, the tone is positive and encouraging.",75.0,85.0
Lifelong Learning with Dynamically Expandable Networks,"['Jaehong Yoon', 'Eunho Yang', 'Jeongtae Lee', 'Sung Ju Hwang']",Accept,2018,"[2, 13, 2, 9]","[7, 18, 2, 14]","[26, 122, 2, 207]","[10, 68, 1, 95]","[15, 49, 1, 108]","[1, 5, 0, 4]","The topic is of great interest to the community, and the ideas explored by the authors are reasonable, but I found the conclusion less-than-clear. Mainly, I was not sure how to interpret the experimental findings, and did not have a clear picture of the various models being investigated (e.g. ""base DNN regularized with l2""), or even of the criteria being examined. What is ""learning capacity""? (If it's number of model parameters, the authors should just say, ""number of parameters""). The relative performance of the different models examined, plotted in the top row of Figure 3, is quite different, and though the authors do devote a paragraph to interpreting the results, I found it slightly hard to follow, and was not sure what the bottom line was.

What does the ""batch model"" refer to?

re. "" 11.9%p − 51.8%p""; remove ""p""?

Reference for CIFAR-100? Explain abbreviation for both CIFAR-100 and AWA-Class?

re. ""... but when the number of tasks is large, STL works better since it has larger learning capacity than MTL"": isn't the number of parameters matched? If so, why is the ""learning capacity"" different? What do the authors mean exactly by ""learning capacity""?

re. Figure 3, e.g. ""Average per-task performance of the models over number of task t"": this is a general point, but usually the expression ""<f(x)> vs. <x>"" is used rather than ""<f(x)> over <x>"" when describing a plot.

""DNN: dase (sic) DNN"": how is this trained?


","[6, 7, 8]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Top 50% of accepted papers, clear accept']","[3, 3, 2]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']","The reviewer expresses interest in the topic and finds the ideas reasonable. However, they also express significant concerns about the clarity of the paper, particularly in understanding the experimental findings and model descriptions. The reviewer asks for several clarifications and points out areas of potential improvement. The tone is critical but professional and aims to improve the paper. Therefore, the sentiment leans slightly towards the negative side due to the need for major clarifications, and the politeness is neutral as the reviewer maintains a professional and objective tone.",-20.0,0.0
Parallelizing Linear Recurrent Neural Nets Over Sequence Length,"['Eric Martin', 'Chris Cundy']",Accept,2018,"[24, 2]","[29, 7]","[17, 13]","[12, 5]","[1, 8]","[4, 0]","This paper abstracts two recently-proposed RNN variants into a family of RNNs called the Linear Surrogate RNNs which satisfy  Blelloch's criteria for parallelizable sequential computation. The authors then propose an efficient parallel algorithm for this class of RNNs, which produces speedups over the existing implements of Quasi-RNN, SRU, and LSTM. Apart from efficiency results, the paper also contributes a comparison of model convergence on a long-term dependency task due to (Hochreiter and Schmidhuber, 1997). A novel linearized version of the LSTM outperforms traditional LSTM on this long-term dependency task, and raises questions about whether RNNs and LSTMs truly need the nonlinear structure.

The paper is written very well, with explanation (as opposed to obfuscation) as the goal. Linear Surrogate RNNs is an important concept that is useful to understand RNN variants today, and potentially other future novel architectures.

The paper provides argument and experimental evidence against the rotation used typically in RNNs. While this is an interesting insight, and worthy of further discussion, such a claim needs backing up with more large-scale experiments on real datasets.

While the experiments on toy tasks is clearly useful, the paper could be significantly improved by adding experiments on real tasks such as language modelling.","[7, 7, 6]","[' Good paper, accept', ' Good paper, accept', ' Marginally above acceptance threshold']","[4, 2, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is fairly confident that the evaluation is correct']","The review is positive about the paper's contribution, praising its clarity, the introduction of the ""Linear Surrogate RNNs"" concept, and the efficiency results. The reviewer finds the argument against typical RNN rotations ""interesting"" but suggests more extensive experiments. The suggestion to include real-world tasks and language modeling further indicates a positive stance but with room for improvement.",75.0,90.0
Learning Robust Rewards with Adverserial Inverse Reinforcement Learning,"['Justin Fu', 'Katie Luo', 'Sergey Levine']",Accept,2018,"[4, 2, 10]","[8, 7, 15]","[34, 17, 743]","[15, 8, 326]","[18, 9, 396]","[1, 0, 21]","The paper provides an approach to learning reward functions in high-dimensional domains, showing that it performs comparably to other recent approaches to this problem in the imitation-learning setting. It also argues that a key property to learning generalizable reward functions is for them to depend on state, but not state-action or state-action-state. It uses this property to produce ""disentangled rewards"", demonstrating that they transfer well to the same task under different transition dynamics.

The need for ""state-only"" rewards is a useful insight and is covered fairly well in the paper. The need for an ""adversarial"" approach is not justified as fully, but perhaps is a consequence of recent work. The experiments are thorough, although the connection to the motivation in the abstract (wanting to avoid reward engineering) is weak.

Detailed feedback:

""deployed in at test-time on environments"" -> ""deployed at test time in environments""?

""which can effectively recover disentangle the goals"" -> ""which can effectively disentangle the goals""?

""it allows for sub-optimality in demonstrations, and removes ambiguity between demonstrations and the expert policy"": I am not certain what is being described here and it doesn't appear to come up again in the paper. Perhaps remove it?

""r high-dimensional (Finn et al., 2016b) Wulfmeier"" -> ""r high-dimensional (Finn et al., 2016b). Wulfmeier"".

""also consider learning cost function with"" -> ""also consider learning cost functions with""?

""o learn nonlinear cost function have"" -> ""o learn nonlinear cost functions have"".

"" are not robust the environment changes"" -> "" are not robust to environment changes""?

""We present a short proof sketch"": It is unclear to me what is being proven here. Please state the theorem.

""In the method presented in Section 4, we cannot learn a state-only reward function"": I'm not seeing that. Or, maybe I'm confused between rewards depending on s vs. s,a vs. s,a,s'. Again, an explicit theorem statement might remove some confusion here.

""AIRLperforms"" -> ""AIRL performs"".

Figure 2: The blue and green colors look very similar to me. I'd recommend reordering the legend to match the order of the lines (random on the bottom) to make it easier to interpret.

""must reach to goal"" -> ""must reach the goal""?

""pointmass"" -> ""point mass"". (Multiple times.)

Amin, Jiang, and Singh's work on efficiently learning a transferable reward function seems relevant here. (Although, it might not be published yet: https://arxiv.org/pdf/1705.05427.pdf.)

Perhaps the final experiment should have included state-only runs. I'm guessing that they didn't work out too well, but it would still be good to know how they compare.
","[6, 6, 7]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Good paper, accept']","[4, 2, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is fairly confident that the evaluation is correct']","The review is overall positive. The reviewer states that the paper provides a useful insight, the experiments are thorough, and the need for ""state-only"" rewards is well-covered. While there are points of improvement and clarification needed, the reviewer does not harshly criticize the paper. The language used is constructive and polite, suggesting specific improvements and asking for clarifications. ",60.0,80.0
Reinforcement Learning Algorithm Selection,"['Romain Laroche', 'Raphael Feraud']",Accept,2018,"[1, -3, 14, 0, 0]","[5, 1, 18, 4, 4]","[23, 1, 2, 4, 4]","[23, 1, 1, 4, 4]","[0, 0, 0, 0, 0]","[0, 0, 1, 0, 0]","The authors consider the problem of dynamically choosing between several reinforcement learning algorithms for solving a reinforcement learning with discounted rewards and episodic tasks. The authors propose the following solution to the problem:
- During epochs of exponentially increasing size (this technique is well known in the bandit litterature and is called a ""doubling trick""), the various reinforcement learning algorithms are ""frozen"" (i.e. they do not adapt their policy) and the K available algorithms are sampled using the UCB1 algorithm  in order to discover the one which yields the highest mean reward.

Overall the paper is well written, and presents some interesting novel ideas on aggregating reinforcement learning algorithms. Below are some remarks:

- An alternative and perhaps simpler formalization of the problem would be learning with expert advice (using algorithms such as ""follow the perturbed leader""), where each of the available reinforcement learning algorithms acts as an expert. What is more, these algorithms usually yield O(sqrt(T)log(T)), which is the regret obtained by the authors in the worse case (where all the learning algorithms do converge to the optimal policy at the optimal speed O(1/sqrt(T)). It would have been good to see how those approaches perform against the proposed algorithms. 
- The authors use UCB1, but they did not try KL-UCB, which is stricly better (in fact it is optimal for bounded rewards). In particular the numerical performance of the latter is usually vastly better than the former, especially when rewards have a small variance.
- The performance measure used by the authors is rather misleading (""short sighted regret""): they compare what they obtain to what the policy discovered by the best reainforcement learning algorithm \underline{based on the trajectories they have seen}, and the trajectories themselves are generated by the choices made by the algorthms at previous time. Ie in general, there might be cases in which one does not explore enough with this approach (i.e one does not try all state-action pairs enough), so that while this performance measure is low, the actual regret is very high and the algorithm does not learn the optimal policy at all (while this could be done by simply exploring at random log(T) times ...).
","[6, 7, 6]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Marginally above acceptance threshold']","[3, 4, 5]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The reviewer starts by acknowledging the novelty of the paper's ideas and praises the writing style. While they raise valid concerns and suggestions for improvement, these are presented constructively and aim to guide the authors towards a stronger publication. The reviewer suggests alternative approaches and points out potential weaknesses in the chosen methodology, but does so without resorting to harsh criticism. Overall, the tone is one of constructive feedback aimed at improving the paper.",60.0,80.0
Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models,"['Wieland Brendel *', 'Jonas Rauber *', 'Matthias Bethge']",Accept,2018,"[8, 2, 20]","[13, 5, 25]","[63, 18, 142]","[22, 4, 52]","[34, 12, 58]","[7, 2, 32]","The authors identify a new security threat for deep learning: Decision-based adversarial attacks. This new class of attacks on deep learning systems requires from an attacker only the knowledge of class labels (previous attacks required more information, e.g., access to a gradient oracle). Unsurprisingly, since the attacker has so few information, such kind of attacks involves quite a lot trial and error. The authors propose one specific attack instance out of this class of attacks. It works as follows.

First, an initial point outside of the benign region is guessed. Then multiple steps towards the decision boundary is taken, finally reaching the boundary (I am not sure about the precise implementation, but it seems not crucial; the author may please check whether their description of the algorithm is really reproducable). Then, in a nutshell, a random walk on a sphere centered around the original, benign point is performed, where after each step, the radius of the sphere is slightly reduced (drawing the point closer to the original point), if and only if the resulting point still is outside of the benign region.

The algorithm is evaluated on the following datasets: MNIST, CIFAR, VGG19, ResNet50, and InceptionV3.

The paper is rather well written and structured. The text was easy to follow. I suggest that a self-contained description of the problem setting (assumptions on attacker and defender; aim?) shall be added to the camera-ready version (being not familiar with the area, I had to read a couple of papers to get a feeling for the setting, before reviewing this paper). As in many DL papers these days, there really isn't any math in it worth a mention; so no reason here to say anything about mathematical soundness. The authors employ a reasonable evaluation criterion in their experiments: the median squared Euclidean distance between the original and adversarially modified data point. The results show consistent improvement for most data sets. 

In summary, this is an innovative paper, proposing a new class of attacks that totally makes sense in my opinion. Apart from some minor weaknesses in the presentation that can be easily fixed for the camera ready, this is a nice, fresh paper, that might spur more attacks (and of course new defenses) from the new class of decision-based attacks. It is worth to note that the authors show that distillation is not a useful defense against such attacks, so we may expect follow-up proposing useful defenses against the new attack (which BTW is shown to be about a factor of 10 in terms of iterations more costly than the SOTA).","[7, 7, 8]","[' Good paper, accept', ' Good paper, accept', ' Top 50% of accepted papers, clear accept']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer explicitly states that this is ""an innovative paper"" and that the proposed attack ""totally makes sense"". They also mention that the paper is ""well written and structured"" and ""easy to follow."" These are all strong indicators of a positive sentiment. While the reviewer provides constructive criticism, it is presented in a suggestive rather than demanding tone, indicating politeness. The score is not a perfect 100 because there are minor suggestions for improvement.",85.0,90.0
Generating Wikipedia by Summarizing Long Sequences,"['Peter J. Liu*', 'Mohammad Saleh*', 'Etienne Pot', 'Ben Goodrich', 'Ryan Sepassi', 'Lukasz Kaiser', 'Noam Shazeer']",Accept,2018,"[3, 14, 4, 10, 1, 14, 9]","[8, 19, 8, 13, 5, 18, 13]","[32, 31, 11, 14, 12, 84, 62]","[9, 11, 4, 7, 5, 44, 23]","[21, 11, 6, 6, 7, 32, 36]","[2, 9, 1, 1, 0, 8, 3]","The main significance of this paper is to propose the task of generating the lead section of Wikipedia articles by viewing it as a multi-document summarization problem. Linked articles as well as the results of an external web search query are used as input documents, from which the Wikipedia lead section must be generated. Further preprocessing of the input articles is required, using simple heuristics to extract the most relevant sections to feed to a neural abstractive summarizer. A number of variants of attention mechanisms are compared, including the transofer-decoder, and a variant with memory-compressed attention in order to handle longer sequences. The outputs are evaluated by ROUGE-L and test perplexity. There is also a A-B testing setup by human evaluators to show that ROUGE-L rankings correspond to human preferences of systems, at least for large ROUGE differences.

This paper is quite original and clearly written. The main strength is in the task setup with the dataset and the proposed input sources for generating Wikipedia articles. The main weakness is that I would have liked to see more analysis and comparisons in the evaluation.

Evaluation:
Currently, only neural abstractive methods are compared. I would have liked to see the ROUGE performance of some current unsupervised multi-document extractive summarization methods, as well as some simple multi-document selection algorithms such as SumBasic. Do redundancy cues which work for multi-document news summarization still work for this task?

Extractiveness analysis:
I would also have liked to see more analysis of how extractive the Wikipedia articles actually are, as well as how extractive the system outputs are. Does higher extractiveness correspond to higher or lower system ROUGE scores? This would help us understand the difficulty of the problem, and how much abstractive methods could be expected to help. 

A further analysis which would be nice to do (though I have less clear ideas how to do it), would be to have some way to figure out which article types or which section types are amenable to this setup, and which are not. 

I have some concern that extraction could do very well if you happen to find a related article in another website which contains encyclopedia-like or definition-like entries (e.g., Baidu, Wiktionary) which is not caught by clone detection. In this case, the problem could become less interesting, as no real analysis is required to do well here.

Overall, I quite like this line of work, but I think the paper would be a lot stronger and more convincing with some additional work.

----
After reading the authors' response and the updated submission, I am satisfied that my concerns above have been adequately addressed in the new version of the paper. This is a very nice contribution.
","[7, 7, 8]","[' Good paper, accept', ' Good paper, accept', ' Top 50% of accepted papers, clear accept']","[4, 5, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']","The review starts with highlighting the significance and clarity of the paper, indicating a positive sentiment. However, the reviewer then expresses a desire for more analysis and comparisons, suggesting room for improvement. Despite the constructive criticism, the overall tone remains encouraging and ends on a positive note by acknowledging the value of the work. The language used is formal, respectful, and constructive throughout the review. There's no use of harsh language or personal attacks, maintaining a professional and polite tone.",60.0,80.0
Variational Message Passing with Structured Inference Networks,"['Wu Lin', 'Nicolas Hubacher', 'Mohammad Emtiyaz Khan']",Accept,2018,"[9, 1, 15]","[14, 1, 20]","[37, 1, 72]","[19, 1, 34]","[10, 0, 36]","[8, 0, 2]","This paper presents a variational inference algorithm for models that contain
deep neural network components and probabilistic graphical model (PGM)
components.
The algorithm implements natural-gradient message-passing where the messages
automatically reduce to stochastic gradients for the non-conjugate neural
network components. The authors demonstrate the algorithm on a Gaussian mixture
model and linear dynamical system where they show that the proposed algorithm
outperforms previous algorithms. Overall, I think that the paper proposes some
interesting ideas, however, in its current form I do not think that the novelty
of the contributions are clearly presented and that they are not thoroughly
evaluated in the experiments.

The authors propose a new variational inference algorithm that handles models
with deep neural networks and PGM components. However, it appears that the
authors rely heavily on the work of (Khan & Lin, 2017) that actually provides
the algorithm. As far as I can tell this paper fits inference networks into
the algorithm proposed in (Khan & Lin, 2017) which boils down to i) using an
inference network to generate potentials for a conditionally-conjugate
distribution and ii) introducing new PGM parameters to decouple the inference
network from the model parameters. These ideas are a clever solution to work
inference networks into the message-passing algorithm of (Khan & Lin, 2017),
but I think the authors may be overselling these ideas as a brand new algorithm.
I think if the authors sold the paper as an alternative to (Johnson, et al., 2016)
that doesn't suffer from the implicit gradient problem the paper would fit into
the existing literature better.

Another concern that I have is that there are a lot of conditiona-conjugacy
assumptions baked into the algorithm that the authors only mention at the end
of the presentation of their algorithm. Additionally, the authors briefly state
that they can handle non-conjugate distributions in the model by just using
conjugate distributions in the variational approximation. Though one could do
this, the authors do not adequately show that one should, or that one can do this
without suffering a lot of error in the posterior approximation. I think that
without an experiment the small section on non-conjugacy should be removed.

Finally, I found the experimental evaluation to not thoroughly demonstrate the
advantages and disadvantages of the proposed algorithm. The algorithm was applied
to the two models originally considered in (Johnson, et al., 2016) and the
proposed algorithm was shown to attain lower mean-square errors for the two
models. The experiments do not however demonstrate why the algorithm is
performing better. For instance, is the (Johnson, et al., 2016) algorithm
suffering from the implicit gradient? It also would have been great to have
considered a model that the (Johnson, et. al., 2016) algorithm would not work
well on or could not be applied to show the added applicability of the proposed
algorithm.

I also have some minor comments on the paper:
- There are a lot of typos.
- The first two sentences of the abstract do not really contribute anything
  to the paper. What is a powerful model? What is a powerful algorithm?
- DNN was used in Section 2 without being defined.
- Using p() as an approximate distribution in Section 3 is confusing notation
  because p() was used for the distributions in the model.
- How is the covariance matrix parameterized that the inference network produces?
- The phrases ""first term of the inference network"" are not clear. Just use The
  DNN term and the PGM term of the inference networks, and better still throw
  in a reference to Eq. (4).
- The term ""deterministic parameters"" was used and never introduced.
- At the bottom of page 5 the extension to the non-conjugate case should be
  presented somewhere (probably the appendix) since the fact that you can do
  this is a part of your algorithm that's important.
","[7, 7, 7]","[' Good paper, accept', ' Good paper, accept', ' Good paper, accept']","[4, 3, 2]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']","The reviewer finds the paper interesting but believes the novelty and evaluation of the contributions are not clearly presented. They find the authors may be overselling their ideas and suggest reframing the paper as an alternative to existing work. The reviewer also criticizes the experimental evaluation for not thoroughly demonstrating the advantages and disadvantages of the proposed algorithm and for not considering a wider range of models. The reviewer also points out several minor issues like typos and unclear phrasing. While the reviewer provides constructive criticism and suggestions for improvement, their overall tone suggests that the paper needs significant revisions before publication.",20.0,50.0
Sobolev GAN,"['Youssef Mroueh', 'Chun-Liang Li', 'Tom Sercu', 'Anant Raj', 'Yu Cheng']",Accept,2018,"[1, 5]","[2, 10]","[4, 86]","[2, 39]","[2, 40]","[0, 7]","Summary: The authors provide another type of GAN--the Sobolev GAN--which is the typical setup of a GAN but using a function class F for which f belongs to F iff \grad f belongs to L^2(mu). They relate this MMD to the Cramer and Fisher distance and then produce a recipe for training GANs with this sort of function class. In their empirical examples, they show it has similar performance to the WGAN-GP.

Overall, the paper has some interesting mathematical relationships to other MMDs. However, I finished reading the paper wondering why one would want to trust this GAN over any of the other GANs. I may have missed it, but I didn't see any compelling theoretical reason the gradients from this method would prove superior to many of the other GANs in existence today. The authors argue ""from [equation 5] we see that we are comparing CDFs, which are better behaved on discrete distributions,"" but I wasn't sure what exactly to make of this comment.

Nits:
* The ""Stein metric"" is actually called the Stein discrepancy [see Gorham & Mackey (2015) Measuring Sample Quality using Stein's Method].","[6, 7, 8, 6]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Top 50% of accepted papers, clear accept', ' Marginally above acceptance threshold']","[3, 3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is slightly positive in the beginning, acknowledging interesting mathematical relationships. However, it becomes increasingly critical, questioning the GAN's practical advantage and pointing out the lack of compelling theoretical justification. The reviewer also identifies a factual error in the paper. While the criticism is direct, the language remains professional and not personally offensive.",20.0,60.0
Hierarchical and Interpretable Skill Acquisition in Multi-task Reinforcement Learning,"['Tianmin Shu', 'Caiming Xiong', 'Richard Socher']",Accept,2018,"[4, 10, 12]","[9, 15, 17]","[47, 383, 229]","[21, 165, 111]","[22, 208, 111]","[4, 10, 7]","This paper aims to learn hierarchical policies by using a recursive policy structure regulated by a stochastic temporal grammar. The experiments show that the method is better than a flat policy for learning a simple set of block-related skills in minecraft (find, get, put, stack) and generalizes better to a modification of the environment (size of room). The sequence of subtasks generated by the policy are interpretable.

Strengths:
- The grammar and policies are trained using a sparse reward upon task completion. 
- The method is well ablated; Figures 4 and 5 answered most questions I had while reading.
- Theoretically, the method makes few assumptions about the environment and the relationships between tasks.
- The interpretability of the final behaviors is a good result. 

Weaknesses:
- The implementation gives the agent a -0.5 reward if it generates a currently unexecutable goal g’. Providing this reward requires knowing the full state of the world. If this hack is required, then this method would not be useful in a real world setting, defeating the purpose of the sparse reward mentioned above. I would really like to see how the method performs without this hack. 
- There are no comparisons to other multitask or hierarchical methods. Progressive Networks or Zero-Shot Task Generalization with Multi-Task Deep Reinforcement Learning seem like natural comparisons.
- A video to show what the environments and tasks look like during execution would be helpful.
- The performances of the different ablations are rather close. Please a standard deviation over multiple training runs. Also, why does figure 4.b not include a flat policy?
- The stages are ordered in a semantically meaningful order (find is the first stage), but the authors claim that the order is arbitrary. If this claim is going to be included in the paper, it needs to be proven (results shown for random orderings) because right now I do not believe it. 

Quality:
The method does provide hierarchical and interpretable policies for executing instructions, this is a meaningful direction to work on.

Clarity:
Although the method is complicated, the paper was understandable.

Originality and significance:
Although the method is interesting, I am worried that the environment has been too tailored for the method, and that it would fail in realistic scenarios. The results would be more significant if the tasks had an additional degree of complexity, e.g. “put blue block next to the green block” “get the blue block in room 2”. Then the sequences of subtasks would be a bit less linear (e.g., first need to find blue, then get, then find green, then put). At the moment the tasks are barely more than the actions provided in the environment.

Another impedance to the paper’s significance is the number of hacks to make the method work (ordering of stages, alternating policy optimization, first training each stage on only tasks of previous stage). Because the method is only evaluated on one simple environment, it unclear which hacks are for the method generally, and which hacks are for the method to work on the environment.","[6, 6, 6]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[4, 3, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer acknowledges the strengths of the paper, such as the use of sparse rewards, ablation studies, and interpretability of results. However, they also raise significant concerns about the method's reliance on unrealistic assumptions and potential hacks, limiting its applicability to real-world scenarios. The reviewer suggests comparisons with other methods and improvements in result presentation. The language used is critical but professional and polite.",40.0,60.0
Memory Architectures in Recurrent Neural Network Language Models,"['Dani Yogatama', 'Yishu Miao', 'Gabor Melis', 'Wang Ling', 'Adhiguna Kuncoro', 'Chris Dyer', 'Phil Blunsom']",Accept,2018,"[10, 7, 5, 9, 3, 20, 15]","[14, 11, 9, 13, 7, 24, 20]","[74, 45, 22, 68, 29, 268, 178]","[33, 25, 7, 41, 12, 161, 91]","[34, 19, 13, 21, 15, 90, 70]","[7, 1, 2, 6, 2, 17, 17]","The main contribution of this paper are:
(a) a proposed extension to continuous stack model to allow multiple pop operation,
(b) on a language model task, they demonstrate that their model gives better perplexity than comparable LSTM and attention model, and 
(c) on a syntactic task (non-local subject-verb agreement), again, they demonstrate better performance than comparable LSTM and attention model.

Additionally, the paper provides a nice introduction to the topic and casts the current models into three categories -- the sequential memory access, the random memory access and the stack memory access models. 

Their analysis in section (3.4) using the Venn diagram and illustrative figures in (3), (4) and (5) provide useful insight into the performance of the model.","[8, 6, 5]","[' Top 50% of accepted papers, clear accept', ' Marginally above acceptance threshold', ' Marginally below acceptance threshold']","[5, 3, 5]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review explicitly points out the strengths of the paper, such as the clear categorization of models, the insightful analysis using Venn diagrams, and the effective use of figures for illustration. Moreover, the reviewer emphasizes the paper's key contributions, including the proposed extension to the continuous stack model and its superior performance in language modeling and syntactic tasks. The language used is positive and appreciative, highlighting the value and novelty of the work. There's no negative feedback present. ",85.0,90.0
Wavelet Pooling for Convolutional Neural Networks,"['Travis Williams', 'Robert Li']",Accept,2018,"[1, 18]","[6, 23]","[20, 55]","[8, 24]","[9, 15]","[3, 16]","I think this paper presents an interesting take on feature pooling. In particular, the idea is to look at pooling as some form of a lossy process, and try to find such a process such that it discards less information given some decimation criterion. Once formulating the problem like this, it becomes obvious that wavelets are a very good candidate.

Pros:
- The nice thing about this method is that average pooling is in some sense a special case of this method, so we can see a clear connection.
- Lots of experiments, and results, which show the method both performing the best in some cases, and not the best in others. I applaud the authors for publishing all the experiments they ran because some may have been tempted to ""forget"" about the experiments in which the proposed method did not perform the best.

Cons:
- No comparison to non-wavelet methods. For example, one obvious comparison would have been to look at using a DCT or FFT transform where the output would discard high frequency components (this can get very close to the wavelet idea!).
- This method has the potential to show its potential on larger pooling windows than 2x2. I would have loved to see some experiments that prove/disprove this.

Other comments:
- Given that this method's flexibility, I could imagine this generate a new class of pooling methods based on lossy transforms. For example, given a MxNxK input, the wavelet idea can be made to output (M/D)x(N/D)x(K/D) (where D is decimation factor). Of interest is the fact that channels can be treated just like any other dimension, since information will be preserved!

Final comments:
- I like the idea and it seems novel it may lead to some promising research directions related to lossy pooling methods/channel aggregation. As such, I think it will be a nice addition to ICLR, especially if the authors decide to run some of the experiments I was suggesting, namely: show what happens when larger pooling windows are used (say 4x4 instead of 2x2), and compare to other lossy techniques (such as Fourier or cosine-transforms).","[9, 7, 4]","[' Top 15% of accepted papers, strong accept', ' Good paper, accept', ' Ok but not good enough - rejection']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer explicitly states they ""like the idea"" and find it ""novel"", suggesting a positive sentiment. They find the paper interesting and potentially leading to promising research. While they list cons and suggestions for improvement, these are constructive and aim to guide the authors towards a stronger paper, rather than being outright rejections. The language used is professional and polite throughout, with no personal attacks or disrespectful phrasing.",75.0,90.0
Deep Learning and Quantum Entanglement: Fundamental Connections with Implications to Network Design,"['Yoav Levine', 'David Yakira', 'Nadav Cohen', 'Amnon Shashua']",Accept,2018,"[2, 2, 7, 31]","[7, 5, 12, 36]","[25, 8, 55, 142]","[7, 2, 23, 83]","[18, 5, 27, 39]","[0, 1, 5, 20]","The authors try to bring in two seemingly different areas and try
to leverage the results in one for another.
First authors show that the equivalence of the function realized(in
tensor form, given in earlier work) by a ConvAC and
the function used to model n-body quantum system. After establishing
the equivalence of two, the authors argue that
quantum entanglement measures used to measure correlations in n-body
quantum systems can be used as an expressive measure
(how much correlation in input they can handle) of the function
realized by a ConvAC. Separation Rank analysis, which was done
earlier, becomes a special case. As the functional equivalence is
established, authors adopt Tensor Network framework,
to analyze the properties of the ConvAC. The main result being able
to quantify the expressiveness to some extend to the min
cut of the underlying Tensor Network graph corresponding to ConvAC.
This is further used to argue about guide-lining the
width of various parts of ConvAC, if some prior correlation
structure is known about the input. This is also validated
experimentally.

Although I do not see major results at this moment, this work can be
of great significance. The attempt to bring in two areas
have to be appreciated. This work opens up a footing to do graph
theoretical analysis of deep learning architectures and from
the perspective of Quantum entanglement, this could lead to open up new directions. 
The paper is lucidly written, comprehensively covering the
preliminaries. I thoroughly enjoyed reading it, and I think the
paper and the work would be of great contribution to the community.

(There are some typos  (preform --> perform ))","[8, 6, 7, 6]","[' Top 50% of accepted papers, clear accept', ' Marginally above acceptance threshold', ' Good paper, accept', ' Marginally above acceptance threshold']","[5, 2, 3, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer explicitly states ""this work can be of great significance"" and ""I think the paper and the work would be of great contribution to the community.""  They also mention enjoying reading the paper, which points to a positive sentiment. While they note there are no ""major results,"" the overall tone suggests optimism about the work's potential. Therefore, the sentiment is rather positive, but not overly enthusiastic. The language used is respectful and constructive throughout, indicating a polite tone.",70.0,80.0
Variational Inference of Disentangled Latent Concepts from Unlabeled Observations,"['Abhishek Kumar', 'Prasanna Sattigeri', 'Avinash Balakrishnan']",Accept,2018,"[9, 10, 2]","[13, 15, 4]","[44, 102, 13]","[24, 44, 6]","[18, 47, 6]","[2, 11, 1]","******
Update: revising reviewer score to 6 after acknowledging revisions and improved manuscript
******

The authors propose a new regularization term modifying the VAE (Kingma et al 2013) objective to encourage learning disentangling representations.
Specifically, the authors suggest to add penalization to ELBO in the form of -KL(q(z)||p(z)) , which encourages a more global criterion than the local ELBOs.
In practice, the authors decide that the objective they want to optimize is unwieldy and resort to moment matching of covariances of q(z) and p(z) via gradient descent.
The final objective uses a persistent estimate of the covariance matrix of q and upgrades it at each mini-batch to perform learning.

The authors use this objective function to perform experiments measuring disentanglement and find minor benefits compared to other objectives in quantitative terms.

Comments:
1. The originally proposed modification in Equation (4) appears to be rigorous and as far as I can tell still poses a lower bound to log(p(x)). The proof could use the result posed earlier: KL(q(z)||p(z)) is smaller than E_x KL(q(z|x)||p(z|x)).
2. The proposed moment matching scheme performing decorrelation resembles approaches for variational PCA and especially independent component analysis. The relationship to these techniques is not discussed adequately. In addition, this paper could really benefit from an empirical figure of the marginal statistics of z under the different regularizers in order to establish what type of structure is being imposed here and what it results in.
3. The resulting regularizer with the decorrelation terms could be studied as a modeling choice. In the probabilistic sense, regularizers can be seen as structural and prior assumptions on variables. As it stands, it is unnecessarily vague which assumptions this extra regularizer is making on variables.
4. Why is using the objective in Equation (4) not tried and tested and compared to? It could be thought that subsampling would be enough to evaluate this extra KL term without any need for additional variational parameters \psi. The reason for switching to the moment matching scheme seems not well motivated here without showing explicitly that Eq (4) has problems.
5. The model seems to be making on minor progress in its stated goal, disentanglement. It would be more convincing to clarify the structural properties of this regularizer in a statistical sense more clearly given that experimentally it seems to only have a minor effect.
6. Is there a relationship to NICE (Laurent Dinh et al)?
7. The infogan is also an obvious point of reference and comparison here.
8. The authors claim that there are no models which can combine GANs with inference in a satisfactory way, which is obviously not accurate nowadays given the progress on literature combining GANs and variational inference.

All in all I find this paper interesting but would hope that a more careful technical justification and derivation of the model would be presented given that it seems to not be an empirically overwhelming change.","[6, 7, 7]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Good paper, accept']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer acknowledges improvements in the revised manuscript, indicating a positive sentiment. However, the reviewer raises several concerns and suggests areas for improvement, suggesting the sentiment is not overly positive. The language used is formal, professional, and polite, without resorting to harsh criticism.",50.0,70.0
Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting,"['Yaguang Li', 'Rose Yu', 'Cyrus Shahabi', 'Yan Liu']",Accept,2018,"[7, 8, 26, 17]","[12, 13, 31, 22]","[46, 88, 399, 219]","[35, 35, 248, 132]","[3, 50, 41, 57]","[8, 3, 110, 30]","The paper proposes to build a graph where the edge weight is defined using the road network distance which is shown to be more realistic than the Euclidean distance. The defined diffusion convolution operation is essentially conducting random walks over the road segment graph. To avoid the expensive matrix operation for the random walk, it empirically shows that K = 3 hops of the random walk can give a good performance. The outputs of the graph convolutionary operation are then fed into the sequence to sequence architecture with the GRU cell to model the temporal dependency. Experiments show that the proposed architecture can achieve good performance compared to classic time series baselines and several simplified variants of the proposed model. 

Although the paper argues that several existing deep-learning based approaches may not be directly applied in the current setting either due to using Euclidean distance or undirected graph structure, the comparisons are not persuasive. For example, the approach in the paper ""DeepTransport: Learning Spatial-Temporal Dependency for Traffic Condition Forecasting"" also consider directed graph and a diffusion effect from 2 or 3 hops away in the neighboring subgraph of a target road segment. 

Furthermore, the paper proposes to use two convolution components in Equation 2, each of which corresponds to out-degree and in-degree direction, respectively. This effectively increase the number of model parameters to learn. Compared to the existing spectral graph convolution approach, it is still not clear how its performance will be by using the same number of parameters. The experiments will be improved if it can compare with ""Spatio-temporal graph convolutional neural network: A deep learning framework for traffic forecasting"" using roughly the same number of parameters.","[5, 9, 4]","[' Marginally below acceptance threshold', ' Top 15% of accepted papers, strong accept', ' Ok but not good enough - rejection']","[3, 5, 5]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review acknowledges the merits of the paper's approach (using road network distance, considering diffusion effect) but raises concerns about the novelty and experimental comparison. It points out similarities with existing approaches and suggests more rigorous comparisons with existing spectral graph convolution methods. The language is critical but professional and not disrespectful.",20.0,60.0
i-RevNet: Deep Invertible Networks,"['Jörn-Henrik Jacobsen', 'Arnold W.M. Smeulders', 'Edouard Oyallon']",Accept,2018,"[3, 37, 5]","[8, 42, 9]","[40, 295, 40]","[17, 161, 15]","[21, 45, 22]","[2, 89, 3]","ICLR I-Revnet


This paper build on top of ReVNets (Gomez et al., 2017)  and introduce a variant that is fully 
invertible. The model performs comparable to its variants without any loss of information.
They analyze the model and its learned representations from multiple perspectives in detail. 
 
It is indeed very interesting an thought provoking to see that contrary to popular belief in the community no information loss is necessary to learn good generalizable features. What is missing, is more motivation for why such a property is desirable. As the authors mentions the model size has almost doubled compared to comparable ResNet. And the study of the property of the learned futures might probably limited to this i-RevNet only. It would be good to see more motivation, beside the valuable insight of knowing it’s possible.

Generally the paper is well written and readable, but few minor comments:
1-Better formatting such as putting results in model sizes, etc in tables will make them easier to find.
2-Writing down more in detail 3.1, ideally in algorithm or equation than all in text as makes it hard to read in current format.","[8, 9, 8]","[' Top 50% of accepted papers, clear accept', ' Top 15% of accepted papers, strong accept', ' Top 50% of accepted papers, clear accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer provides a positive sentiment by acknowledging the paper's interesting findings and detailed analysis. However, they also express a desire for more motivation regarding the practical implications of the research. The reviewer suggests improvements in formatting and clarity, but the overall tone is constructive and helpful rather than overly critical. Therefore, the sentiment is positive, but not overly enthusiastic, and the politeness is high.",60.0,80.0
Unsupervised Cipher Cracking Using Discrete GANs,"['Aidan N. Gomez', 'Sicong Huang', 'Ivan Zhang', 'Bryan M. Li', 'Muhammad Osama', 'Lukasz Kaiser']",Accept,2018,"[2, 1, 1, 1, 4, 14]","[6, 6, 4, 6, 9, 18]","[29, 14, 5, 7, 44, 84]","[9, 6, 2, 1, 20, 44]","[18, 6, 3, 4, 15, 32]","[2, 2, 0, 2, 9, 8]","The paper shows an application of GANs to deciphering text. The goal is to arrive at a ```""hands free"" approach to this problem; i.e., an approach that does not require any knowledge of the language being deciphered such as letter frequency and such. The authors start from a CycleGAN architecture, which may be used to learn mapping between two probability spaces. They point out that using GANs for discrete distributions is a challenging problem since it can lead to uninformative discriminants. They propose to  resolve this issue by using a continuous embedding space to approximate (or convert) the discrete random variables into continuous random variables. The new proposed algorithm, called CipherGAN, is then shown to be stable and achieve deciphering of substitution ciphers and Vigenere ciphers.

I did not completely understand how the embedding was performed, so perhaps the authors could elaborate on that a bit more. Apart from that, the paper is well written and well motivated. It used some recent ideas in deep learning such as Cycle GANs and shows how to tweak them to make them work for discrete problems and also make them more stable. One comment would be that the paper is decidedly an applied paper (and not much theory) since certain steps in the algorithm (such as training the discriminator loss along with the Lipschitz conditioning term) are included because it was experimentally  observed to lead to stability. ","[7, 8, 7]","[' Good paper, accept', ' Top 50% of accepted papers, clear accept', ' Good paper, accept']","[1, 4, 4]","["" The reviewer's evaluation is an educated guess"", ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer provides a summary of the paper's contributions and praises its clarity (""well written and well motivated""), novelty (""used some recent ideas""), and effectiveness (""stable and achieve deciphering""). While they point out a need for clarification, the overall tone is positive and encouraging. There's no harsh criticism, only constructive feedback.",75.0,90.0
Learning Parametric Closed-Loop Policies for Markov Potential Games,"['Sergio Valcarcel Macua', 'Javier Zazo', 'Santiago Zazo']",Accept,2018,"[8, 5, 25]","[13, 10, 29]","[35, 23, 115]","[16, 11, 60]","[12, 8, 9]","[7, 4, 46]","While it is not very surprising that in a potential game it is easy to find Nash equilibria (compare to normal form static games, in which local maxima of the potential are pure Nash equilibria), the idea of approaching these stochastic games from this direction is novel and potentially (no pun intended) fruitful. The paper is well written, the motivation is clear, and some of the ideas are non-trivial. However, the connection to learning representations is a little tenuous. ","[7, 6, 6]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[2, 3, 1]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is fairly confident that the evaluation is correct', "" The reviewer's evaluation is an educated guess""]","The review starts with positive statements, highlighting novelty and good writing. While it points out a tenuous connection as a drawback, the overall tone remains constructive and encouraging. Therefore, the sentiment leans positive. The language used is formal, respectful, and avoids harsh criticism, indicating politeness.",60.0,80.0
Temporal Difference Models: Model-Free Deep RL for Model-Based Control,"['Vitchyr Pong*', 'Shixiang Gu*', 'Murtaza Dalal', 'Sergey Levine']",Accept,2018,"[3, 7, 1, 10]","[7, 12, 6, 15]","[23, 89, 17, 743]","[11, 38, 7, 326]","[12, 49, 10, 396]","[0, 2, 0, 21]","
This paper proposes a ""temporal difference model learning"", a method that aims to combine the benefits of model-based and model-free RL.  The proposed method essentially learns a time-varying goal-conditional value function for a specific reward formulation, which acts as a surrogate for a model in an MPC-like setting.  The authors show that the method outperforms some alternatives on three continuous control domains and real robot system.

I believe this paper to be borderline, but ultimately below the threshold for acceptance.  On the positive side, there are certainly some interesting ideas here: the notion of goal-conditioned value functions as proxies for a model, and as a means of merging model-free and model-based approaches is very really interesting, and hints at a deeper structure to goal-conditioned value functions in general.  Ultimately, though, I feel that there are two main issues that make this research feel as though it is still ultimately in the earlier stages: 1) the very large focus on the perspective that this approach is unifying model-based and model-free RL, when it fact this connection seems a bit tenuous; and 2) the rather lackluster experimental results, which show only marginal improvement over purely model-based methods (at the cost of much additional complexity), and which make me wonder if there's an issue with their implementation of prior work (namely the Highsight Experience Replay algorithm).

To address the first point, although the paper stresses it to a very high degree, I can't help but feel that the connection that the claimed advance of ""unifying model-based and model-free RL"" is overstated.  As far as I can tell, the connection is as follows: the learned quantity here is a time-varying goal-conditioned value function, and under some specific definition of reward, we can interpret the constraint that this value function equal zero as a proxy for the dynamics constraint in MPC.  But the exact correspondence between this and the MPC formulation only occurs for a horizon of size zero: longer horizons require a multi-step MPC for the definition of the model-free and model-based correspondence.  The fact that the action selection of a model-based method and this approach have some function which looks similar (but only under certain conditions), just seems like a fairly odd connection to highlight so heavily.

Rather, it seems to me that what's happening here is really quite simple: the authors are extending goal-conditioned value functions to the case of non-stationary finite horizon value functions (the claimed ""key insight"" in eq (5) is a completely standard finite-horizon MDP formulation).  This seems to describe perfectly well what is happening here, and it does also seem intuitive that this provides an advantage over stationary goal-conditioned value functions: just as goal conditioned value functions offer the advantage of considering ""every state as a goal"", this method can consider ""every state as a goal for every time horizon"".  This seems interesting enough on its own, and I admit I don't see the need for the method to be yet another claimed unification of model-free and model-based RL.

I would also suggest that the authors look into the literature on how TD methods implicitly learn models (see e.g. Boyan 1997 ""Least-squares temporal difference learning"", and Parr et al., 2007 ""An analysis of linear models..."").  In these works it has been shown that least squares TD methods (at least in the linear feature setting), implicitly learn a dynamics model in feature space, but only the ""projection"" of the reward function is actually needed to learn the TD weights.  In building the proposed value functions, it seems like the authors are effectively solving for multiple rewards simultaneously, which would effectively preserve the learned dynamics model.  I feel like this may be an interesting line of analysis for the paper if the authors _do_ want to stick with the notion of the method as unifying model-free and model-based RL.

All these points may ultimately just be a matter of interpretation, though, if not for the second issue with the paper, which is that the results seem quite lackluster, and the claimed performance of HER seems rather suspicious.  But instead, the authors evaluate the algorithm on just three continuous control tasks (and a real robot, which is more impressive, but the task here is still so extremely simple for a real robot system that it really just qualifies as a real-world demonstration rather than an actual application).  And in these three settings, a model-based approach seems to work just as well on two of the tasks, and may soon perform just as well after a few more episodes on the last task (it doesn't appear to have converged yet).  And despite the HER paper showing improvement over traditional policy approaches, in these experiments plain DDPG consistently performs as well or better than HER.  ","[4, 7, 7]","[' Ok but not good enough - rejection', ' Good paper, accept', ' Good paper, accept']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer finds the paper ""borderline"" and ultimately suggests rejection. While they find the core idea interesting, they see the paper's framing as a unification of model-based and model-free RL as tenuous and overstated. The reviewer also finds the experimental results unconvincing, describing them as ""lackluster"" and expressing skepticism about the implementation of prior work. The language used is critical but professional and not overtly negative.",-25.0,50.0
Model compression via distillation and quantization,"['Antonio Polino', 'Razvan Pascanu', 'Dan Alistarh']",Accept,2018,"[8, 2, 20]","[13, 7, 25]","[33, 39, 207]","[15, 19, 86]","[16, 20, 86]","[2, 0, 35]","The paper proposes to combine two approaches to compress deep neural networks - distillation and quantization. The authors proposed two methods, one largely relying on the distillation loss idea then followed by a quantization step, and another one that also learns the location of the quantization points. Somewhat surprisingly, nobody has combined the two approaches before, which makes this paper interesting. Experiments show that both methods work well in compressing large deep neural network models for applications where resources are limited, like on mobile devices. 

Overall I am mostly OK with this paper but not impressed by it.  Detailed comments below.

1. Quantizing with respect to the distillation loss seems to do better than with the normal loss - this needs more discussion. 
2. The idea of using the gradient with respect to the quantization points to learn them is interesting but not entirely new (see, e.g., ""Matrix Recovery from Quantized and Corrupted Measurements"", ICASSP 2014 and ""OrdRec: An Ordinal Model for Predicting Personalized Item Rating Distributions"", RecSys 2011, although in a different context). I also wonder if it would work better if you can also allow the weights to move a little bit (it seems to me from Algorithm 2 that you only update the quantization points). How about learning them altogether? Also this differentiable quantization method does not really depend on distillation, which is kind of confusing given the title.
3. I am a little bit confused by how the bits are redistributed in the second method, as in the end it seems to use more than the proposed number of bits shown in the table (as recognized in section 4.2). This makes the comparison a little bit unfair (especially for the CIFAR 100 case, where the ""2 bits"" differentiable quantization is actually using 3.23 bits). This needs more clarification.
4. The writing can be improved. For example, the concepts of ""teacher"" and ""student"" is not clear at all in the abstract - consider putting the first sentence of Section 3 in there instead. Also, the first sentence of the paper reads as ""... have showed tremendous performance"", which is not proper English. At the top of page 3 I found ""we will restrict our attention to uniform and non-uniform quantization"". What are you not restricting to, then?

Slightly increased my rating after reading the rebuttal and the revision. ","[7, 8, 6]","[' Good paper, accept', ' Top 50% of accepted papers, clear accept', ' Marginally above acceptance threshold']","[2, 5, 4]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer finds the paper interesting as it tackles a novel idea. However, they are not impressed by the novelty or significance of the findings. They point out several areas for improvement and clarification, indicating a lukewarm reception overall. While the language is direct and critical in places, it maintains a professional and constructive tone. The reviewer also acknowledges improvements made after reading the author's rebuttal, suggesting a slightly more positive final impression.",20.0,60.0
Auto-Conditioned Recurrent Networks for Extended Complex Human Motion Synthesis,"['Yi Zhou', 'Zimo Li', 'Shuangjiu Xiao', 'Chong He', 'Zeng Huang', 'Hao Li']",Accept,2018,"[23, 5, 14, 5, 10, 11]","[28, 9, 19, 10, 15, 16]","[374, 17, 51, 34, 24, 113]","[177, 10, 46, 12, 12, 49]","[61, 5, 1, 4, 10, 33]","[136, 2, 4, 18, 2, 31]","The problem of learning auto-regressive (data-driven) human motion models that have long-term stability
is of ongoing interest. Steady progress is being made on this problem, and this paper adds to that.
The paper is clearly written. The specific form of training (a fixed number of self-conditioned predictions,
followed by a fixed number of ground-truth conditioned steps) is interesting for simplicity and its efficacy.
The biggest open question for me is how it would compare to the equally simple stochastic version proposed
by the scheduled sampling approach of [Bengio et al. 2015].

PROS:  The paper provides a simple solution to a problem of interest to many.
CONS:  It is not clear if it improves over something like scheduled sampling, which is a stochastic predecessor
       of the main idea introduced here. The ""duration of stability"" is a less interesting goal than
       actually matching the distribution of the input data.

The need to pay attention to the distribution-mismatch problem for sequence prediction problems
has been known for a while. In particular, the DAGGER (see below) and scheduled sampling algorithms (already cited) 
target this issue, in addition to the addition of progressively increasing amounts of noise during training
(Fragkiadaki et al). Also see papers below on Professor Forcing, as well as ""Learning Human Motion Models
for Long-term Predictions"" (concurrent work?), which uses annealing over dropout rates to achieve stable long-term predictions.

  DAGGER algorithm (2011):  http://www.jmlr.org/proceedings/papers/v15/ross11a/ross11a.pdf
  ""A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning""

  Professor Forcing (NIPS 2016)
  http://papers.nips.cc/paper/6099-professor-forcing-a-new-algorithm-for-training-recurrent-networks.pdf

  Learning Human Motion Models for Long-term Predictions (2017)
  https://arxiv.org/abs/1704.02827
  https://www.youtube.com/watch?v=PgJ2kZR9V5w
  
While the motions do not freeze, do the synthesized motion distributions match the actual data distributions?
This is not clear, and would be relatively simple to evaluate.  Is the motion generation fully deterministic?
It would be useful to have probabilistic transition distributions that match those seen in the data.
An interesting open issue (in motion, but also of course NLP domains) is that of how to best evaulate
sequence-prediction models.  The duration of ""stable prediction"" does not directly capture the motion quality. 

Figure 1:  Suggest to make u != v for the purposes of clarity, so that they can be more easily distinguished.

Data representation:
Why not factor out the facing angle, i.e., rotation about the vertical axis, as done by Holden et al, and in a variety of
previous work in general?
The representation is already made translation invariant. Relatedly, in the Training section,
data augmentation includes translating the sequence: ""rotate and translate the sequence randomly"".
Why bother with the translation if the representation itself is already translation invariant?

The video illustrates motions with and without ""foot alignment"".
However, no motivation or description of ""foot alignment"" is given in the paper.

The following comment need not be given much weight in terms of evaluation of the paper, given that the
current paper does not use simulation-based methods. However, it is included for completeness.
The survey of simulation-based methods for modeling human motions is not representative of the body of work in this area
over the past 25 years.  It may be more useful to reference a survey, such as 
""Interactive Character Animation Using Simulated Physics: A State‐of‐the‐Art Review"" (2012)
An example of recent SOTA work for modeling dynamic motions from motion capture, including many
highly dynamic motions, is ""Guided Learning of Control Graphs for Physics-Based Characters"" (2016)
More recent work includes ""Learning human behaviors from motion capture by adversarial imitation"", 
""Robust Imitation of Diverse Behaviors"", and ""Deeploco: Dynamic locomotion skills using hierarchical deep reinforcement learning"", all of which demonstrate imitation of various motion styles to various degrees.

It is worthwhile acknowledging that the synthesized motions are still low quality, particular when rendered with more human-like looking models, and readily distinguishable from the original motions.  In this sense, they are not comparable to the quality of results demonstrated in recent works by Holden et al. or some other recent works.  However, the authors should be given credit for including some results with fully rendered characters, which much more readily exposes motion flaws.

The followup work on [Lee et al 2010 ""Motion Fields""] is quite relevant:
""Continuous character control with low-dimensional embeddings""
In terms of usefulness, being able to provide some control over the motion output is a more interesting problem than
being able to generate long uncontrolled sequences.  A caveat is that the methods are not applied to large datasets.
","[7, 6, 7]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Good paper, accept']","[5, 5, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']","The review starts with positive remarks, highlighting the relevance of the problem tackled and the clarity of the paper. However, it then raises significant concerns and questions about the novelty and evaluation of the proposed method. The reviewer provides numerous references to relevant prior work, suggesting the authors might have missed some key comparisons. While the tone remains largely professional and polite, the numerous critical points and suggestions for improvement indicate a somewhat skeptical stance towards the paper's contributions.",20.0,60.0
On the Expressive Power of Overlapping Architectures of Deep Learning,"['Or Sharir', 'Amnon Shashua']",Accept,2018,"[4, 31]","[8, 36]","[22, 142]","[7, 83]","[15, 39]","[0, 20]","The paper studies convolutional neural networks where the stride is smaller than the convolutional filter size; the so called overlapping convolutional architectures. The main object of study is to quantify the benefits of overlap in convolutional architectures.

The main claim of the paper is Theorem 1, which is that overlapping convolutional architectures are efficient with respect to non-overlapping architectures, i.e., there exists functions in the overlapping architecture which require an exponential increase in size to be represented in the non-overlapping architecture; whereas overlapping architecture can capture within a linear size the functions represented by the non-overlapping architectures. The main workhorse behind the paper is the notion of rank of matricized grid tensors following a paper of Cohen and Shashua which capture the relationship between the inputs and the outputs, the function implemented by the neural network. 

(1) The results of the paper hold only for product pooling and linear activation function except for the representation layer, which allows general functions. It is unclear why the generalized convolutional networks are stated with such generality when the results apply only to this special case. That this is the case should be made clear in the title and abstract. The paper makes a point that generalized tensor decompositions can be potentially applied to solve the more general case, but since it is left as future work, the paper should make it clear throughout.

(2) The experiment is minimal and even the given experiment is not described well. What data augmentation was used for the CIFAR-10 dataset? It is only mentioned that the data is augmented with translations and horizontal flips. What is the factor of augmentation? How much translation? These are important because there maybe a much simpler explanation to the benefit of overlap: it is able to detect these translated patterns easily. Indeed, this simple intuition seems to be why the authors chose to make the problem by introducing translations and flips. 

(3) It is unclear if the paper resolves the mystery that they set out to solve, which is a reconciliation of the following two observations (a) why are non-overlapping architectures so common? (b) why only slight overlap is used in practice?  The paper seems to claim that since overlapping architectures have higher expressivity that answers (a). It appears that the paper does not answer (b) well: it points out that since there is exponential increase, there is no reason to increase it beyond a particular point. It seems the right resolution will be to show that after the overlap is set to a certain small value, there will be *only* linear increase with increasing overlap; i.e., the paper should show that small overlap networks are efficient with respect to *large* overlap networks; a comparison that does not seem to be made in the paper. 

(4) Small typo: the dimensions seem to be wrong in the line below the equation in page 3. 

The paper makes important progress on a highly relevant problem using a new methodology (borrowed from a previous paper). However, the writing is hurried and the high-level conclusions are not fully supported by theory and experiments. ","[6, 8, 6]","[' Marginally above acceptance threshold', ' Top 50% of accepted papers, clear accept', ' Marginally above acceptance threshold']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review acknowledges the importance of the paper's subject and the novelty of the approach but raises several significant concerns. The reviewer points out limitations in the scope of the results, lack of clarity and details in the experimental setup, and questions whether the paper truly answers the questions it poses. The tone, while direct and critical, maintains a professional and academic demeanor. There are no personal attacks or disrespectful language.",20.0,60.0
Regularizing and Optimizing LSTM Language Models,"['Stephen Merity', 'Nitish Shirish Keskar', 'Richard Socher']",Accept,2018,"[8, 4, 12]","[9, 8, 17]","[14, 46, 229]","[6, 16, 111]","[8, 27, 111]","[0, 3, 7]","Clearly presented paper, including a number of reasonable techniques to improve LSTM-LMs. The proposed techniques are heuristic, but are reasonable and appear to yield improvements in perplexity. Some specific comments follow.

re. ""ASGD"" for Averaged SGD: ASGD usually stands for Asynchronous SGD, have the authors considered an alternative acronym? AvSGD?

re. Optimization criterion on page 2, note that SGD is usually taken to minimizing expected loss, not just empirical loss (Bottou thesis 1991).

Is there any theoretical analysis of convergence for Averaged SGD?

re. paragraph starting with ""To prevent such inefficient data usage, we randomly select the sequence length for the forward and backward pass in two steps"": the explanation is a bit unclear. What is the ""base sequence length"" exactly? Also, re. the motivation above this paragraph, I'm not sure what ""elements"" really refers to, though I can guess.

What is the number of training tokens of the datasets used, PTB and WT2?

Can the authors provide more explanation for what ""neural cache models"" are, and how they relate to ""pointer models""?

Why do the sections ""Pointer models"", ""Ablation analysis"", and ""AWD-QRNN"" come after the Experiments section?","[7, 7, 7]","[' Good paper, accept', ' Good paper, accept', ' Good paper, accept']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review starts with a positive statement about the paper's clarity and the reasonableness of the proposed techniques. The reviewer acknowledges the improvements in perplexity achieved. While the reviewer poses many questions and requests for clarification, these are presented constructively and suggest a desire to understand the work better rather than outright criticism. The tone throughout is professional and objective.",60.0,80.0
Memorization Precedes Generation: Learning Unsupervised GANs with Memory Networks,"['Youngjin Kim', 'Minjung Kim', 'Gunhee Kim']",Accept,2018,"[24, 18, 16]","[29, 23, 21]","[94, 39, 150]","[41, 22, 95]","[14, 9, 45]","[39, 8, 10]","MemoryGAN is proposed to handle structural discontinuity (avoid unrealistic samples) for the generator, and the forgetting behavior of the discriminator. The idea to incorporate memory mechanism into GAN is interesting, and the authors make nice interpretation why this needed, and clearly demonstrate which component helps (including the connections to previous methods).   

My major concerns:

Figure 1 is questionable in demonstrating the advantage of proposed MemoryGAN. My understanding is that four z's used in DCGAN and MemoryGAN are ""randomly sampled"" and fixed, interpolation is done in latent space, and propagate to x to show the samples.  Take MNIST for example, It can be seen that the DCGAN has to (1) transit among digits in different classes, while MemoryGAN only (2) transit among digits in the same class. Task 1 is significantly harder than task 2, it is not surprise that DCGAN generate unrealistic images. A better experiment is to fix four digits from different class at first, find their corresponding latent codes, do interpolation, and propagate back to sample space to visualize results. If the proposed technique can truly handle structural discontinuity, it will ""jump"" over the sample manifold from one class to another, and thus avoid unrealistic samples. Also, the current illustration also indicates that the generated samples by MemoryGAN is not diverse.

It seems the memory mechanism can bring major computational overhead, is it possible to provide the comparison on running time?

To what degree the MemoryGAN can handle structural discontinuity? It can be seen from Table 2 that larger improvement is observed when tested on a more diverse dataset. For example, the improvement gap from MNIST to CIFAR is larger. If the MemoryGAN can truly deal with structural discontinuity, the results on generating a wide range of different images for ImageNet may endow the paper with higher impact.

The authors should consider to make their code reproducible and public. 


Minor comments:

In Section 4.3, Please fix ""Results in 2"" as ""Results in Table 2"".


","[6, 6, 7]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Good paper, accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with an acknowledgment of the interesting idea and clear demonstration of the proposed method. However, it raises major concerns about the experimental validation and potential limitations. While acknowledging the potential, the reviewer's tone leans towards skepticism and suggests further experiments and analysis are needed to support the claims. The language used is formal and professional, directly addressing the paper's shortcomings but maintaining a respectful tone.",40.0,70.0
An image representation based convolutional network for DNA classification,"['Bojian Yin', 'Marleen Balvert', 'Davide Zambrano', 'Alexander Schoenhuth', 'Sander Bohte']",Accept,2018,"[1, 1, 10, 17, 19]","[6, 4, 15, 22, 24]","[14, 10, 23, 66, 85]","[5, 2, 11, 15, 37]","[7, 5, 7, 18, 23]","[2, 3, 5, 33, 25]","The authors of this manuscript transformed the k-mer representation of DNA fragments to a 2D image representation using the space-filling Hilbert curves for the classification of chromatin occupancy. In generally, this paper is easy to read. The components of the proposed model mainly include Hilbert curve theory and CNN which are existing technologies. But the authors make their combination useful in applications. Some specific comments are:

1. In page 5, I could not understand the formula d_kink < d_out. d_link ;
2. There may exist some new histone modification data that were captured by the next-generation sequencing (e.g. ChIP-seq) and are more accurate;  
3. It seems that the authors treat it as a two-class problem for each data set. It would be more useful in real applications if all the data sets are combined to form a multi-class problem.
","[7, 7, 7]","[' Good paper, accept', ' Good paper, accept', ' Good paper, accept']","[5, 3, 5]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review starts with a neutral summary of the work and acknowledges its usefulness despite using existing technologies. The reviewer then provides constructive feedback with specific points for improvement, suggesting a positive attitude towards the paper's potential. The language used is objective and professional throughout.",50.0,75.0
Reinforcement Learning on Web Interfaces using Workflow-Guided Exploration,"['Evan Zheran Liu', 'Kelvin Guu', 'Panupong Pasupat', 'Tianlin Shi', 'Percy Liang']",Accept,2018,"[2, 4, 7, 6, 14]","[7, 9, 12, 6, 19]","[19, 37, 50, 17, 346]","[8, 13, 26, 9, 166]","[11, 23, 24, 6, 170]","[0, 1, 0, 2, 10]","This paper introduces a new exploration policy for Reinforcement Learning for agents on the web called ""Workflow Guided Exploration"". Workflows are defined through a DSL unique to the domain.

The paper is clear, very well written, and well-motivated. Exploration is still a challenging problem for RL. The workflows remind me of options though in this paper they appear to be hand-crafted. In that sense, I wonder if this has been done before in another domain. The results suggest that WGE sometimes helps but not consistently. While the experiments show that DOMNET improves over Shi et al, that could be explained as not having to train on raw pixels or not enough episodes.","[7, 6, 7]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Good paper, accept']","[3, 3, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer starts with positive statements: ""clear, very well written, and well-motivated."" and acknowledges the importance of the topic. However, they also pose questions and express slight reservation (""I wonder if this has been done before"", ""results suggest that WGE sometimes helps but not consistently""). Overall, this suggests a slightly positive but cautious sentiment. The language used is polite and professional throughout.",60.0,80.0
Thermometer Encoding: One Hot Way To Resist Adversarial Examples,"['Jacob Buckman', 'Aurko Roy', 'Colin Raffel', 'Ian Goodfellow']",Accept,2018,"[3, 5, 9, 10]","[7, 9, 14, 12]","[16, 28, 112, 107]","[7, 10, 49, 48]","[8, 14, 58, 55]","[1, 4, 5, 4]","The authors present an in-depth study of discretizing / quantizing the input as a defense against adversarial examples. The idea is that the threshold effects of discretization make it harder to find adversarial examples that only make small alterations of the image, but also that it introduces more non-linearities, which might increase robustness. In addition, discretization has little negative impact on the performance on clean data. The authors also propose a version of single-step or multi-step attacks against models that use discretized inputs, and present extensive experiments on MNIST, CIFAR-10, CIFAR-100 and SVHN, against standard baselines and, on MNIST and CIFAR-10, against a version of quantization in which the values are represented by a small number of bits.

The merits of the paper is that the study is rather comprehensive: a large number of datasets were used, two types of discretization were tried, and the authors propose an attack mechanism better that seems reasonable considering the defense they consider. The two main claims of the paper, namely that discretization doesn't hurt performance on natural test examples and that better robustness (in the author's experimental setup) is achieved through the discretized encoding, are properly backed up by the experiments.

Yet, the applicability of the method in practice is still to be demonstrated. The threshold effects might imply that small perturbations of the input (in the l_infty sense) will not have a large effect on their discritized version, but it may also go the other way: an opponent might be able to greatly change the discretized input without drastically changing the input. Figure 8 in the appendix is a bit worrysome on that point, as the performance of the discretized version drops rapidly to 0 when the opponents gets a bit stronger. Did the authors observe the same kind of bahavior on other datasets? What would the authors propose to mitigate this issue? To what extend the good results that are exhibited in the paper are valid over the wide range of opponent's strengths?

minor comment:
- the experiments on CIFAR-100 in Appendix E are carried out by mixing adversarial / clean examples while training, whereas those on SVHN in Appendix F use adversarial examples only.
","[6, 6, 6]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[4, 4, 2]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']","The review is positive overall. The reviewer acknowledges the merits of the paper, such as its comprehensive study and well-supported claims. However, they also raise valid concerns about the method's practical applicability and point out a potential weakness. The language used is polite and professional, employing constructive criticism and asking clarifying questions rather than making harsh statements.",60.0,80.0
On the State of the Art of Evaluation in Neural Language Models,"['Gábor Melis', 'Chris Dyer', 'Phil Blunsom']",Accept,2018,"[5, 20, 15]","[9, 24, 20]","[22, 268, 178]","[7, 161, 91]","[13, 90, 70]","[2, 17, 17]","The submitted manuscript describes an exercise in performance comparison for neural language models under standardization of the hyperparameter tuning and model selection strategies and costs.  This type of study is important to give perspective to non-standardized performance scores reported across separate publications, and indeed the results here are interesting as they favour relatively simpler structures.

I have a favourable impression of this paper but would hope another reviewer is more familiar with the specific application domain than I am.","[7, 5, 8]","[' Good paper, accept', ' Marginally below acceptance threshold', ' Top 50% of accepted papers, clear accept']","[2, 5, 3]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer explicitly states a ""favourable impression"", uses positive language like ""important"" and ""interesting"", and expresses only a mild concern about their own domain expertise. All of this points to a positive but not overwhelmingly positive sentiment. The language is entirely neutral and professional throughout.",60.0,100.0
Sensitivity and Generalization in Neural Networks: an Empirical Study,"['Roman Novak', 'Yasaman Bahri', 'Daniel A. Abolafia', 'Jeffrey Pennington', 'Jascha Sohl-Dickstein']",Accept,2018,"[25, 2, 1, 8, 10]","[29, 6, 3, 13, 15]","[46, 18, 6, 61, 130]","[21, 7, 2, 33, 52]","[15, 10, 4, 27, 74]","[10, 1, 0, 1, 4]","This work investigates sensitivity and generalisation properties of neural networks with respect to a number of metrics aimed at quantifying the robustness with respect to data variability, varying parameters and representativity of training/testing data. 
The validation is based on the Jacobian of the network, and in the detection of the “transitions” associated to the data space. These measures are linked, as the former quantifies the sensitivity of the network respect to infinitesimal data variations, while the latter quantifies the complexity of the modelled data space. 
The study explores a number of experimental setting, where the behaviour of the network is analysed on synthetic paths around training data, from pure random data points, to curves interpolating different/same data classes.
The experimental results are performed on CIFAR10,CIFAR100, and MNIST. Highly-parameterised networks seem to offer a better generalisation, while lower Jacobian norm are usually associated to better generalisation and fewer transitions, and can be obtained with data augmentation.

The paper proposes an interesting analysis aimed at the empirical exploration of neural network properties, the proposed metrics provide relevant insights to understand the behaviour of a network under varying data points. 

Major remarks.

The proposed investigation is to my opinion quite controversial. Interesting data variation does not usually corresponds to linear data change. When considering the linear interpolation of training data, the authors are actually creating data instances not compatible with the original data source: for example, the pixel-wise intensity average of digits is not a digit anymore. For this reason, the conclusions drawn about the model sensitivity are to my opinion based a potentially uninteresting experimental context. Meaningful data variation can be way more complex and high-dimensional, for example by considering spatial warps of digits, or occlusions and superpositions of natural images. This kind of variability is likely to correspond to real data changes, and may lead to more reliable conclusions. For this reason, the proposed results may provide little indications of the true behaviour of the models data in case of meaningful  data variations. 

Moreover, although performed within a cross-validation setting, training and testing are still applied to the same dataset. Cross-validation doesn’t rule out validation bias, while it is also known that the classification performance significantly drops when applied to independent “unseen” data, provided for example in different cohorts. I would expect that highly parameterised models would lead to worse performance when applied to genuinely independent cohorts, and I believe that this work should extend the investigation to this experimental setting.

Minor remarks.

The authors should revise the presentation of the proposed work. The 14 figures(!) of main text are not presented in the order of appearance. The main one (figure 1) is provided in the first paragraph of the introduction and never discussed in the rest of the paper. 
","[5, 4, 8]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Top 50% of accepted papers, clear accept']","[3, 5, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer provides a mixed opinion, acknowledging the interesting aspects of the paper's analysis but also raising significant concerns about the experimental methodology and conclusions. The reviewer finds the choice of linear data changes for analyzing model sensitivity controversial and suggests exploring more realistic data variations. Additionally, the reviewer points out limitations in the cross-validation approach and suggests evaluating the models on genuinely independent datasets. The language used is critical but professional and maintains a respectful tone throughout.",10.0,60.0
Simulating Action Dynamics with Neural Process Networks,"['Antoine Bosselut', 'Omer Levy', 'Ari Holtzman', 'Corin Ennis', 'Dieter Fox', 'Yejin Choi']",Accept,2018,"[3, 7, 2, 2, 24, 14]","[8, 12, 7, 2, 29, 19]","[77, 122, 47, 2, 449, 344]","[35, 55, 22, 1, 244, 175]","[41, 62, 25, 1, 146, 160]","[1, 5, 0, 0, 59, 9]","SUMMARY.

The paper presents a novel approach to procedural language understanding.
The proposed model reads food recipes and updates the representation of the entities mentioned in the text in order to reflect the physical changes of the entities in the recipe.
The authors also propose a manually annotated dataset where each passage of a recipe is annotated with entities, actions performed over the entities, and the change in state of the entities after the action.
The authors tested their model on the proposed dataset and compared it with several baselines.


----------

OVERALL JUDGMENT
The paper is very well written and easy to read.
I enjoyed reading this paper, I found the proposed architecture very well thought for the proposed task.
I would have liked to see a little bit more of analysis on the results, it would be interesting to see what are the cases the model struggles the most.

I am wondering how the model would perform without intermediate losses i.e., entity selection loss and action selection loss.
It would also be interesting to see the impact of the amount of 'intermediate' supervision on the state change prediction.

The setup for generation is a bit unclear to me.
The authors mentioned to encode entity vectors with a biGRU, do the authors encode it in order of appearance in the text? would not it be better to encode the entities with some structure-agnostic model like Deep Sets?

","[9, 6, 8]","[' Top 15% of accepted papers, strong accept', ' Marginally above acceptance threshold', ' Top 50% of accepted papers, clear accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer explicitly states enjoying the paper and finding value in the proposed architecture. They also provide constructive feedback and suggestions for improvement, indicating a positive stance overall. The language used is polite and professional throughout.",75.0,100.0
CausalGAN: Learning Causal Implicit Generative Models with Adversarial Training,"['Murat Kocaoglu', 'Christopher Snyder', 'Alexandros G. Dimakis', 'Sriram Vishwanath']",Accept,2018,"[7, 19, 17, 19]","[12, 21, 22, 24]","[41, 10, 297, 333]","[22, 4, 123, 158]","[14, 4, 132, 116]","[5, 2, 42, 59]","In their paper ""CausalGAN: Learning Causal implicit Generative Models with adv. training"" the authors address the following issue: Given a causal structure between ""labels"" of an image (e.g. gender, mustache, smiling, etc.), one tries to learn a causal model between these variables and the image itself from observational data. Here, the image is considered to be an effect of all the labels. Such a causal model allows us to not only sample from conditional observational distributions, but also from intervention distributions. These tasks are clearly different, as nicely shown by the authors' example of ""do(mustache = 1)"" versus ""given mustache = 1"" (a sample from the latter distribution contains only men). The paper does not aim at learning causal structure from data (as clearly stated by the authors). The example images look convincing to me.

I like the idea of this paper. IMO, it is a very nice, clean, and useful approach of combining causality and the expressive power of neural networks. The paper has the potential of conveying the message of causality into the ICLR community and thereby trigger other ideas in that area. For me, it is not easy to judge the novelty of the approach, but the authors list related works, none of which seems to solve the same task. The presentation of the paper, however, should be improved significantly before publication. (In fact, because of the presentation of the paper, I was hesitating whether I should suggest acceptance.) Below, I give some examples (and suggest improvements), but there are many others. There is a risk that in its current state the paper will not generate much impact, and that would be a pity. I would therefore like to ask the authors to put a lot of effort into improving the presentation of the paper. 


- I believe that I understand the authors' intention of the caption of Fig. 1, but ""samples outside the dataset"" is a misleading formulation. Any reasonable model does more than just reproducing the data points. I find the argumentation the authors give in Figure 6 much sharper. Even better: add the expression ""P(male = 1 | mustache = 1) = 1"". Then, the difference is crystal clear.
- The difference between Figures 1, 4, and 6 could be clarified.    
- The list of ""prior work on learning causal graphs"" seems a bit random. I would add Spirtes et al 2000, Heckermann et al 1999, Peters et al 2016, and Chickering et al 2002. 
- Male -> Bald does not make much sense causally (it should be Gender -> Baldness)... Aha, now I understand: The authors seem to switch between ""Gender"" and ""Male"" being random variables. Make this consistent, please. 
- There are many typos and comma mistakes. 
- I would introduce the do-notation much earlier. The paragraph on p. 2 is now written without do-notation (""intervening Mustache = 1 would not change the distribution""). But this way, the statements are at least very confusing (which one is ""the distribution""?).
- I would get rid of the concept of CiGM. To me, it seems that this is a causal model with a neural network (NN) modeling the functions that appear in the SCM. This means, it's ""just"" using NNs as a model class. Instead, one could just say that one wants to learn a causal model and the proposed procedure is called CausalGAN? (This would also clarify the paper's contribution.)
- many realizations = one sample (not samples), I think. 
- Fig 1: which model is used to generate the conditional sample?  
- The notation changes between E and N and Z for the noises. I believe that N is supposed to be the noise in the SCM, but then maybe it should not be called E at the beginning. 
- I believe Prop 1 (as it is stated) is wrong. For a reference, see Peters, Janzing, Scholkopf: Elements of Causal Inference: Foundations and Learning Algorithms (available as pdf), Definition 6.32. One requires the strict positivity of the densities (to properly define conditionals). Also, I believe the Z should be a vector, not a set. 
- Below eq. (1), I am not sure what the V in P_V refers to.
- The concept of data probability density function seems weird to me. Either it is referring to the fitted model, then it's a bad name, or it's an empirical distribution, then there is no pdf, but a pmf.
- Many subscripts are used without explanation. r -> real? g -> generating? G -> generating? Sometimes, no subscripts are used (e.g., Fig 4 or figures in Sec. 8.13)
- I would get rid of Theorem 1 and explain it in words for the following reasons. (1) What is an ""informal"" theorem? (2) It refers to equations appearing much later. (3) It is stated again later as Theorem 2. 
- Also: the name P_g does not appear anywhere else in the theorem, I think. 
- Furthermore, I would reformulate the theorem. The main point is that the intervention distributions are correct (this fact seems to be there, but is ""hidden"" in the CIGN notation in the corollary).
- Re. the formulation in Thm 2: is it clear that there is a unique global optimum (my intuition would say there could be several), thus: better write ""_a_ global minimum""?
- Fig. 3 was not very clear to me. I suggest to put more information into its caption. 
- In particular, why is the dataset not used for the causal controller? I thought, that it should model the joint (empirical) distribution over the labels, and this is part of the dataset. Am I missing sth?
- IMO, the structure of the paper can be improved. Currently, Section 3 is called ""Background"" which does not say much. Section 4 contains CIGMs, Section 5 Causal GANs, 5.1. Causal Controller, 5.2. CausalGAN, 5.2.1. Architecture (which the causal controller is part of) etc. An alternative could be: 
Sec 1: Introduction 
Sec 1.1: Related Work
Sec 2: Causal Models
Sec 2.1: Causal Models using Generative Models (old: CIGM)
Sec 3: Causal GANs
Sec 3.1: Architecture (including controller)
Sec 3.2: loss functions 
...
Sec 4: Empricial Results (old: Sec. 6: Results)
- ""Causal Graph 1"" is not a proper reference (it's Fig 23 I guess). Also, it is quite important for the paper, I think it should be in the main part. 
- There are different references to the ""Appendix"", ""Suppl. Material"", or ""Sec. 8"" -- please be consistent (and try to avoid ambiguity by being more specific -- the appendix contains ~20 pages). Have I missed the reference to the proof of Thm 2?
- 8.1. contains copy-paste from the main text.
- ""proposition from Goodfellow"" -> please be more precise
- What is Fig 8 used for? Is it not sufficient to have and discuss Fig 23? 
- IMO, Section 5.3. should be rewritten (also, maybe include another reference for BEGAN).
- There is a reference to Lemma 15. However, I have not found that lemma.
- I think it's quite interesting that the framework seems to also allow answering counterfactual questions for realizations that have been sampled from the model, see Fig 16. This is the case since for the generated realizations, the noise values are known. The authors may think about including a comment on that issue.
- Since this paper's main proposal is a methodological one, I would make the publication conditional on the fact that code is released. 


","[7, 6, 9]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Top 15% of accepted papers, strong accept']","[3, 3, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer explicitly states they ""like the idea"" and find it ""very nice, clean, and useful"", which points towards a positive sentiment. They also encourage the authors to improve the paper to increase its impact, indicating they see value in the work. However, the numerous suggestions for improvement and the initial hesitation about acceptance prevent the sentiment from being overly positive. Therefore, a sentiment score of 60 is assigned. 

The language used is quite direct and contains some critical feedback, but it is always constructive and delivered politely. The reviewer uses phrases like ""I believe"", ""it seems to me"", and ""I would suggest"" to soften their criticism and maintain a respectful tone. Thus, a politeness score of 70 is assigned.",60.0,70.0
Espresso: Efficient Forward Propagation for Binary Deep Neural Networks,"['Fabrizio Pedersoli', 'George Tzanetakis', 'Andrea Tagliasacchi']",Accept,2018,"[7, 20, 11]","[11, 24, 16]","[10, 150, 116]","[6, 114, 36]","[2, 6, 53]","[2, 30, 27]","This paper builds on Binary-NET [Hubara et al. 2016] and expands it to CNN architectures. It also provides optimizations that substantially improve the speed of the forward pass: packing layer bits along the channel dimension, pre-allocation of CUDA resources and binary-optimized CUDA kernels for matrix multiplications. The authors compare their framework to BinaryNET and Nervana/Neon and show a 8x speedup for 8092 matrix-matrix multiplication and a 68x speedup for MLP networks. For CNN, they a speedup of 5x is obtained from the GPU to binary-optimizimed-GPU. A gain in memory size of 32x is also achieved by using binary weight and activation during the forward pass.

The main contribution of this paper is an optimized code for Binary CNN. The authors provide the code with permissive licensing. As is often the case with such comparisons, it is hard to disentangle from where exactly come the speedups. The authors should provide a table with actual numbers instead of the hard-to-read bar graphs. Otherwise the paper is well written and relatively clear, although the flow is somewhat unwieldy. 

Overall, i think it makes a good contribution to a field that is gaining importance for mobile and embedded applications of deep convnets. I think it is a good fit for a poster.","[7, 6, 7]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Good paper, accept']","[4, 3, 1]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', "" The reviewer's evaluation is an educated guess""]","The reviewer acknowledges the contribution of the paper, stating its value for mobile and embedded applications. While they have some critique (e.g., hard-to-read graphs, disentangling speedup sources), they recommend it as a good fit for a poster, which indicates positive sentiment. The language used is professional and constructive, suggesting a neutral to polite tone.",60.0,50.0
Towards Deep Learning Models Resistant to Adversarial Attacks,"['Aleksander Madry', 'Aleksandar Makelov', 'Ludwig Schmidt', 'Dimitris Tsipras', 'Adrian Vladu']",Accept,2018,"[14, 2, 5, 4, 9]","[19, 2, 10, 9, 14]","[127, 2, 104, 40, 42]","[55, 1, 50, 17, 20]","[64, 1, 50, 21, 21]","[8, 0, 4, 2, 1]","This paper consolidates and builds on recent work on adversarial examples and adversarial training for image classification. Its contributions:

 - Making the connection between adversarial training and robust optimization more explicit.

 - Empirical evidence that:
   * Projected gradient descent (PGD) (as proposed by Kurakin et al. (2016)) reasonably approximates the optimal attack against deep convolutional neural networks
   * PGD finds better adversarial examples, and training with it yields more robust models, compared to FGSM 

 - Additional empirical analysis:
   * Comparison of weights in robust and non-robust MNIST classifiers
   * Vulnerability of L_infty-robust models to to L_2-bounded attacks

The evidence that PGD consistently finds good examples is fairly compelling -- when initialized from 10,000 random points near the example to be disguised, it usually finds examples of similar quality. The remaining variance that's present in those distributions shouldn't hurt learning much, as long as a significant fraction of the adversarial examples are close enough to optimal.

Given the consistent effectiveness of PGD, using PGD for adversarial training should yield models that are reliably robust (for a specific definition of robustness, such as bounded L_infinity norm). This is an improvement over purely heuristic approaches, which are often less robust than claimed.

The comparison to R+FGSM is interesting, and could be extended in a few small ways. What would R+FGSM look like with 10,000 restarts? The distribution should be much broader, which would further demonstrate how PGD works better on these models. Also, when generating adversarial examples for testing, how well would R+FGSM work if you took the best of 2,000 random restarts? This would match the number of gradient computations required by PGD with 100 steps and 20 restarts. Again, I expect that PGD would be better, but this would make that point clearer. I think this analysis would make the paper stronger, but I don't think it's required for acceptance, especially since R+FGSM itself is such a recent development.

One thing not discussed is the high computational cost: performing a 40-step optimization of each training example will be ~40 times slower than standard stochastic gradient descent. I suspect this is the reason why there are results on MNIST and CIFAR, but not ImageNet. It would be very helpful to add some discussion of this.

The title seems unnecessarily vague, since many papers have been written with the same goal -- make deep learning models resistant to adversarial attacks. (This comment does not affect my opinion about whether or not the paper should be accepted, and is merely a suggestion for the authors.)

Also, much of the paper's content is in the appendices. This reads like a journal article where the references were put in the middle. I don't know if that's fixable, given conference constraints.","[7, 7, 6]","[' Good paper, accept', ' Good paper, accept', ' Marginally above acceptance threshold']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review is positive overall. The reviewer finds the evidence presented to be ""fairly compelling"" and the work to be a valuable contribution. While they suggest some additional analysis and improvements, they don't consider them critical for acceptance. The tone is constructive and professional throughout.",75.0,100.0
Learning to Teach,"['Yang Fan', 'Fei Tian', 'Tao Qin', 'Xiang-Yang Li', 'Tie-Yan Liu']",Accept,2018,"[-2, 6, 0, -4, 3, 14]","[3, 11, 5, 1, 8, 19]","[5, 36, 9, 1, 11, 198]","[3, 19, 4, 1, 8, 112]","[2, 15, 3, 0, 3, 49]","[0, 2, 2, 0, 0, 37]","This paper focuses on the problem of ""machine teaching"", i.e., how to select a good strategy to select training data points to pass to a machine learning algorithm, for faster learning. The proposed approach leverages reinforcement learning by defining the reward as how fast the learner learns, and use policy gradient to update the teacher parameters. I find the definition of the ""state"" in this case very interesting. The experimental results seem to show that such a learned teacher strategy makes machine learning algorithms learn faster. 

Overall I think that this paper is decent. The angle the authors took is interesting (essentially replacing one level of the bi-level optimization problem in machine teaching works with a reinforcement learning setup). The problem formulation is mostly reasonable, and the evaluation seems quite convincing. The paper is well-written: I enjoyed the mathematical formulation (Section 3). The authors did a good job of using different experiments (filtration number analysis, and teaching both the same architecture and a different architecture) to intuitively explain what their method actually does. 

At the same time, though, I see several important issues that need to be addressed if this paper is to be accepted. Details below. 

1. As much as I enjoyed reading Section 3, it is very redundant. In some cases it is good to outline a powerful and generic framework (like the authors did here with defining ""teaching"" in a very broad sense, including selecting good loss functions and hypothesis spaces) and then explain that the current work focuses on one aspect (selecting training data points). However, I do not see it being the case here. In my opinion, selecting good loss functions and hypothesis spaces are much harder problems than data teaching - except maybe when one use a pre-defined set of possible loss functions and select from it. But that is not very interesting (if you can propose new loss functions, that would be way cooler). I also do not see how to define an intuitive set of ""states"" in that case. Therefore, I think this section should be shortened. I also think that the authors should not discuss the general framework and rather focus on ""data teaching"", which is the only focus of the current paper. The abstract and introduction should also be modified accordingly to more honestly reflect the current contributions. 
2. The authors should do a better job at explaining the details of the state definition, especially the student model features and the combination of data and current learner model. 
3. There is only one definition of the reward - related to batch number when the accuracy first exceeds a threshold. Is accuracy stable, can it drop back down below the threshold in the next epoch? The accuracy on a held-out test set is not guaranteed to be monotonically increasing, right? Is this a problem in practice (it seems to happen on your curves)? What about other potential reward definitions? And what would they potentially lead to? 
4. Experimental results are averaged over 5 repeated runs - a bit too small in my opinion. 
5. Can the authors show convergence of the teacher parameter \theta? I think it is important to see how fast the teacher model converges, too. 
6. In some of your experiments, every training method converges to the same accuracy after enough training (Fig.2b), while in others, not quite (Fig. 2a and 2c). Why is this the case? Does it mean that you have not run enough iterations for the baseline methods? My intuition is that if the learner algorithm is convex, then ultimately they will all get to the same accuracy level, so the task is just to get there quicker. I understand that since the learner algorithm is an NN, this is not the case - but more explanation is necessary here - does your method also reduces the empirical possibility to get stuck in local minima? 
7. More explanation is needed towards Fig.4c. In this case, using a teacher model trained on a harder task (CIFAR10) leads to much improved student training on a simpler task (MNIST). Why?
8. Although in terms of ""effective training data points"" the proposed method outperforms the other methods, in terms of time (Fig.5) the difference between it and say, NoTeach, is not that significant (especially at very high desired accuracy). More explanation needed here. 

Read the rebuttal and revision and slightly increased my rating.","[8, 5, 9]","[' Top 50% of accepted papers, clear accept', ' Marginally below acceptance threshold', ' Top 15% of accepted papers, strong accept']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer provides a generally positive overview of the paper, stating that it is 'decent,' the approach is 'interesting,' and the evaluation is 'quite convincing.' They also praise the writing and mathematical formulation. However, they also list eight specific issues that need to be addressed, some of which are quite significant. The reviewer's statement that they 'slightly increased [their] rating' after reading the rebuttal and revision suggests that the authors were able to address some, but not all, of their concerns. Overall, this suggests a sentiment that is positive but with reservations. The language used is polite and professional throughout, with constructive criticism and suggestions for improvement.",60.0,80.0
Learning from Between-class Examples for Deep Sound Recognition,"['Yuji Tokozume', 'Yoshitaka Ushiku', 'Tatsuya Harada']",Accept,2018,"[2, 9, 22]","[2, 14, 27]","[5, 88, 294]","[3, 47, 171]","[2, 34, 106]","[0, 7, 17]","Overall: Authors defined a new learning task that requires a DNN to predict mixing ratio between sounds from two different classes. Previous approaches to training data mixing are (1) from random classes, or (2) from the same class. The presented approach mixes sounds from specific pairs of classes to increase discriminative power of the final learned network. Results look like significant improvements over standard learning setups.

Detailed Evaluation: The approach presented is simple, clearly presented, and looks effective on benchmarks. In terms of originality, it is different from warping training example for the same task and it is a good extension of previously suggested example mixing procedures with a targeted benefit for improved discriminative power. The authors have also provided extensive analysis from the point of views (1) network architecture, (2) mixing method, (3) number of labels / classes in mix, (4) mixing layers -- really well done due-diligence across different model and task parameters.

Minor Asks:
(1) Clarification on how the error rates are defined. Especially since the standard learning task could be 0-1 loss and this new BC learning task could be based on distribution divergence (if we're not using argmax as class label).
(2) #class_pairs targets as analysis - The number of epochs needed is naturally going to be higher since the BC-DNN has to train to predict mixing ratios between pairs of classes. Since pairs of classes could be huge if the total number of classes is large, it'll be nice to see how this scales. I.e. are we talking about a space of 10 total classes or 10000 total classes? How does num required epochs get impacted as we increase this class space?
(3) Clarify how G_1/20 and G_2/20 is important / derived - I assume it's unit conversion from decibels.
(4) Please explain why it is important to use the smoothed average of 10 softmax predictions in evaluation... what happens if you just randomly pick one of the 10 crops for prediction?","[9, 4, 8]","[' Top 15% of accepted papers, strong accept', ' Ok but not good enough - rejection', ' Top 50% of accepted papers, clear accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer explicitly states that ""Results look like significant improvements over standard learning setups."" and that the approach is ""simple, clearly presented, and looks effective on benchmarks."" They also praise the authors for their ""extensive analysis."" All of this points to a positive sentiment. 

The reviewer uses constructive language throughout, framing their suggestions as ""minor asks"" and phrasing their questions politely (e.g., ""It'll be nice to see..."", ""Please explain...""). This indicates a high level of politeness.",85.0,90.0
Hierarchical Representations for Efficient Architecture Search,"['Hanxiao Liu', 'Karen Simonyan', 'Oriol Vinyals', 'Chrisantha Fernando', 'Koray Kavukcuoglu']",Accept,2018,"[4, 11, 12, 16, 10]","[9, 15, 17, 20, 14]","[74, 102, 209, 51, 101]","[35, 44, 101, 27, 46]","[33, 51, 98, 13, 46]","[6, 7, 10, 11, 9]","The fundamental contribution of the article is the explicit use of compositionality in the definition of the search space. Instead of merely defining an architecture as a Directed Acyclic Graph (DAG), with nodes corresponding to feature maps and edges to primitive operations, the approach in this paper introduces a hierarchy of architectures of this form. Each level of the hierarchy utilises the existing architectures in the preceding level as candidate operations to be applied in the edges of the DAG. As a result, this would allow the evolutionary search algorithm to design modules which might be then reused in different edges of the DAG corresponding to the final architecture, which is located at the top level in the hierarchy.

Manually designing novel neural architectures is a laborious, time-consuming process. Therefore, exploring new approaches to automatise this task is a problem of great relevance for the field. 

Overall, the paper is well-written, clear in its exposition and technically sound. While some hyperparameter and design choices could perhaps have been justified in greater detail, the paper is mostly self-contained and provides enough information to be reproducible. 

The fundamental contribution of this article, when put into the context of the many recent publications on the topic of automatic neural architecture search, is the introduction of a hierarchy of architectures as a way to build the search space. Compared to existing work, this approach should emphasise modularity, making it easier for the evolutionary search algorithm to discover architectures that extensively reuse simpler blocks as part of the model. Exploiting compositionality in model design is not novel per se (e.g. [1,2]), but it is to the best of my knowledge the first explicit application of this idea in neural architecture search. 

Nevertheless, while the idea behind the proposed approach is definitely interesting, I believe that the experimental results do not provide sufficiently compelling evidence that the resulting method substantially outperforms the non-hierarchical, flat representation of architectures used in other publications. In particular, the results highlighted in Figure 3 and Table 1 seem to indicate that the difference in performance between both paradigms is rather small. Moreover, the performance gap between the flat and hierarchical representations of the search space, as reported in Table 1, remains smaller than the performance gap between the best performing of the approaches proposed in this article and NASNet-A (Zoph et al., 2017), as reported in Tables 2 and 3.

Another concern I have is regarding the definition of the mutation operators in Section 3.1. While not explicitly stated, I assume that all sampling steps are performed uniformly at random (otherwise please clarify it). If that was indeed the case, there is a systematic asymmetry between the probability to add and remove an edge, making the former considerably more likely. This could bias the architectures towards fully-connected DAGs, as indeed seems to occur based on the motifs reported in Appendix A.

Finally, while the main motivation behind neural architecture search is to automatise the design of new models, the approach here presented introduces a non-negligible number of hyperparameters that could potentially have a considerable impact and need to be selected somehow. This includes, for instance, the number of levels in the hierarchy (L), the number of motifs at each level in the hierarchy (M_l), the number of nodes in each graph at each level in the hierarchy (| G^{(l)} |), as well as the set of primitive operations. I believe the paper would be substantially strengthened if the authors explored how robust the resulting approach is with respect to perturbations of these hyperparameters, and/or provided users with a principled approach to select reasonable values.

References:

[1] Grosse, Roger, et al. ""Exploiting compositionality to explore a large space of model structures."" UAI (2012).
[2] Duvenaud, David, et al. ""Structure discovery in nonparametric regression through compositional kernel search."" ICML (2013).
","[6, 6, 8]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Top 50% of accepted papers, clear accept']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with positive remarks, highlighting the significance of the paper's contribution and praising its clarity and technical soundness. However, it then transitions into a more critical analysis, expressing concerns about the experimental validation and certain design choices. While acknowledging the interesting approach, the reviewer questions the conclusiveness of the results and points out potential limitations. The language remains professional and respectful throughout, employing constructive criticism rather than harsh negativity. Therefore, the sentiment leans slightly towards the positive side due to the initial praise and the generally constructive tone, while the politeness score remains high due to the absence of any disrespectful or unprofessional language.",60.0,80.0
Learning Latent Representations in Neural Networks for Clustering through Pseudo Supervision and Graph-based Activity Regularization,"['Ozsel Kilinc', 'Ismail Uysal']",Accept,2018,"[4, 14]","[8, 19]","[12, 37]","[3, 18]","[7, 5]","[2, 14]","This paper utilizes ACOL algorithm for unsupervised learning. ACOL can be considered a type of semi-supervised learning where the learner has access only to parent-class information (for example in digit recognition whether a digit is bigger than 5 or not) and not the sub-class information (number between 0-9). Given that in many applications such parent-class supervised information is not available, the authors of this paper propose domain specific pseudo parent-class labels (for example transformed images of digits) to adapt ACOL for unsupervised learning. The authors also modified affinity and balance term utilized in GAR (as part of ACOL algorithm) to improve it. The authors use multiple data sets to study different aspects of the proposed approach.

I updated my scores based on the reviewers responses. It turned out that ACOL and GAR are also originally proposed by the same authors and was only published in arxiv! Because of the double-blind review nature of ICLR, I didn't know these ideas came from the same authors and is being published for the first time in a peer-reviewed venue (ICLR). So my main problem with this paper, lack of novelty, is addressed and my score has changed. Thanks to the reviewer for clarifying this.
","[7, 6, 7]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Good paper, accept']","[4, 5, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']","The review is positive because the reviewer states that their main concern (lack of novelty) has been addressed.  They acknowledge the authors' response clarified that the algorithms being used were, in fact, also their own work and this is the first peer-reviewed publication. The language used is neutral and objective throughout.",60.0,0.0
Divide-and-Conquer Reinforcement Learning,"['Dibya Ghosh', 'Avi Singh', 'Aravind Rajeswaran', 'Vikash Kumar', 'Sergey Levine']",Accept,2018,"[2, 4, 4, 17, 10]","[7, 8, 9, 22, 15]","[22, 29, 60, 111, 743]","[10, 13, 25, 51, 326]","[12, 15, 33, 42, 396]","[0, 1, 2, 18, 21]","This paper presents a reinforcement learning method for learning complex tasks by dividing the state space into slices, learning local policies within each slice, while ensuring that they don't deviate too far from each other, while simultaneously learning a central policy that works across the entire state space in the process. The most closely related works to this one are Guided Policy Search (GPS) and ""Distral"", and the authors compare and contrast their work with the prior work suitably.

The paper is written well, has good insights, is technically sound, and has all the relevant references. The authors show through several experiments that the divide and conquer (DnC) technique can solve more complex tasks than can be solved with conventional policy gradient methods (TRPO is used as the baseline). The paper and included experiments are a valuable contribution to the community interested in solving harder and harder tasks using reinforcement learning.

For completeness, it would be great to include one more algorithm in the evaluation: an ablation of DnC which does not involve a central policy at all. If the local policies are trained to convergence, (and the context omega is provided by an oracle), how well does this mixture of local policies perform? This result would be instructive to see for each of the tasks.

The partitioning of each task must currently be designed by hand. It would be interesting (in future work) to explore how the partitioning could perhaps be discovered automatically.","[7, 4, 7]","[' Good paper, accept', ' Ok but not good enough - rejection', ' Good paper, accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is overwhelmingly positive. The reviewer finds the paper well-written, insightful, technically sound, and a valuable contribution. The suggestions made are constructive and aimed at further strengthening the paper, not criticizing it. There's no negative language used.",90.0,100.0
N2N learning: Network to Network Compression via Policy Gradient Reinforcement Learning,"['Anubhav Ashok', 'Nicholas Rhinehart', 'Fares Beainy', 'Kris M. Kitani']",Accept,2018,"[2, 5, 9, 11]","[3, 10, 9, 16]","[3, 48, 10, 287]","[1, 24, 6, 155]","[2, 23, 3, 109]","[0, 1, 1, 23]","Summary:
The manuscript introduces a principled way of network to network compression, which uses policy gradients for optimizing two policies which compress a strong teacher into a strong but smaller student model. The first policy, specialized on architecture selection, iteratively removes layers, starting with architecture of the teacher model. After the first policy is finished, the second policy reduces the size of each layer by iteratively outputting shrinkage ratios for hyperparameters such as kernel size or padding. This organization of the action space, together with a smart reward design achieves impressive compression results, given that this approach automates tedious architecture selection. The reward design favors low compression/high accuracy over high compression/low performance while the reward still monotonically increases with both compression and accuracy. As a bonus, the authors also demonstrate how to include hard constraints such as parameter count limitations into the reward model and show that policies trained on small teachers generalize to larger teacher models.

Review:
The manuscript describes the proposed algorithm in great detail and the description is easy to follow. The experimental analysis of the approach is very convincing and confirms the author’s claims. 
Using the teacher network as starting point for the architecture search is a good choice, as initialization strategies are a critical component in knowledge distillation. I am looking forward to seeing work on the research goals outlined in the Future Directions section.

A few questions/comments:
1) I understand that L_{1,2} in Algorithm 1 correspond to the number of layers in the network, but what do N_{1,2} correspond to? Are these multiple rollouts of the policies? If so, shouldn’t the parameter update theta_{{shrink,remove},i} be outside the loop over N and apply the average over rollouts according to Equation (2)? I think I might have missed something here.
2) Minor: some of the citations are a bit awkward, e.g. on page 7: “algorithm from Williams Williams (1992). I would use the \citet command from natbib for such citations and \citep for parenthesized citations, e.g. “... incorporate dark knowledge (Hinton et al., 2015)” or “The MNIST (LeCun et al., 1998) dataset...” 
3) In Section 4.6 (the transfer learning experiment), it would be interesting to compare the performance measures for different numbers of policy update iterations.
4) Appendix: Section 8 states “Below are the results”, but the figure landed on the next page. I would either try to force the figures to be output at that position (not in or after Section 9) or write ""Figures X-Y show the results"". Also in Section 11, Figure 13 should be referenced with the \ref command
5) Just to get a rough idea of training time: Could you share how long some of the experiments took with the setup you described (using 4 TitanX GPUs)?
6) Did you use data augmentation for both teacher and student models in the CIFAR10/100 and Caltech256 experiments?
7) What is the threshold you used to decide if the size of the FC layer input yields a degenerate solution?

Overall, this manuscript is a submission of exceptional quality and if minor details of the experimental setup are added to the manuscript, I would consider giving it the full score.","[9, 4, 5]","[' Top 15% of accepted papers, strong accept', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer provides overwhelmingly positive feedback, stating that the manuscript is ""a submission of exceptional quality"" and that the experimental analysis is ""very convincing."" The reviewer also states that they would consider giving the manuscript ""the full score"" with minor clarifications. The questions and comments are constructive and geared towards improving the manuscript. The language used is polite and professional throughout.",90.0,100.0
Gaussian Process Behaviour in Wide Deep Neural Networks,"['Alexander G. de G. Matthews', 'Jiri Hron', 'Mark Rowland', 'Richard E. Turner', 'Zoubin Ghahramani']",Accept,2018,"[4, 2, 19, 12, 26]","[9, 7, 24, 17, 30]","[19, 23, 79, 147, 310]","[9, 10, 38, 67, 184]","[9, 13, 38, 70, 66]","[1, 0, 3, 10, 60]","- Summary

The paper is well written and proves how deep, wide, fully connected NNs are equivalent to GPs in the limit. This result, which was well known for single-layer NNs, is now extended to the multilayer case. Although there was already previous work suggesting GP this behavior, there was no formal proof under the specific conditions presented here.

The convergence to a GP is also verified experimentally on some toy examples.


- Relevance

The result itself does not feel very novel because variants of it were already available.

Unfortunately, although making other researchers aware of this is worthy, the application of this result seems limited, since in fact it describes and lets us know more about a regime that we would rather avoid, rather than one we want to exploit. Most of the applications of deep learning benefit from strong structured priors that cannot be represented as a GP. This is properly acknowledged in the paper.

The lack of practical relevance combined with the not-groundbreaking novelty of the result makes this paper less appealing.


- Other comments

Page 6: ""It does mean however that our empirical study does not extend to larger datasets where such inference is prohibitively expensive (...) prior dominated problems are generally regarded as an area of strength for Bayesian approaches and in this context our results are directly relevant.""

Although that argument can hold for datasets that are large in terms of amount of data points, it doesn't for datasets that are large in terms of number of dimensions. The empirical study could have used very high-dimensional datasets with comparatively low amounts of training data. That would maintain a regime were the prior does matter but and better show the generality of the results.

Page 6: ""We use rectified linear units and correct the variances to avoid a loss of prior variance as depth is increased as discussed in Section 3"" 

Are you sure this is discussed in Section 3?

Page 4: ""This is because for finite H the input activations do not have a multivariate normal distribution"". 

Can you elaborate on this? Since we are interested in the infinite limit, why is this a problem?","[6, 6, 6]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer acknowledges the paper is well-written and delivers a formal proof for a known concept, but expresses reservations about its novelty and practical relevance. They find the empirical study limited and suggest improvements. The language used is professional and constructive, suggesting areas for improvement without resorting to harsh criticism.",20.0,70.0
Alternating Multi-bit Quantization for Recurrent Neural Networks,"['Chen Xu', 'Jianqiang Yao', 'Zhouchen Lin', 'Wenwu Ou', 'Yuanbin Cao', 'Zhirong Wang', 'Hongbin Zha']",Accept,2018,"[18, 1, 20, 2, 1, 19, 31]","[23, 1, 25, 6, 1, 23, 36]","[221, 2, 373, 62, 2, 25, 386]","[93, 1, 135, 30, 1, 13, 282]","[32, 1, 116, 30, 1, 5, 22]","[96, 0, 122, 2, 0, 7, 82]","I have read the comments and clarifications from the authors. They have added extra experiments, and clarified the speed-ups concern raised by others. I keep my original rating of the paper.

---------------
ORIGINAL REVIEW:

This paper introduces a multi-bit quantization method for recurrent neural networks, which is built on alternating the minimization formulated by Guo et al. 2017 by first fixing the \alpha values and then finding the optimal binary codes b_i with a BST, to then estimate \alpha with the refined approximation by Guo et al. 2017, iteratively. The observation that the optimal binary code can be computed with a BST is simple and elegant.

The paper is easy to follow and the topic of reducing memory and speeding up computations for RNN and DNN is interesting and relevant to the community.

The overall contribution on model quantization is based on existing methods, which makes the novelty of the paper suffer a bit. Said that, applying it to RNN is a convincing and a strong motivation. Also, in the paper it is shown how the matrix multiplications of the quantized model can be speeded up using 64 bits operation in CPU. This is, not only saves memory storage and usage, but also on runtime calculation using CPU, which is an important characteristic when there are limited computational resources.

Results on language models show that the models with quantized weights with 3 bits obtain the same or even slightly better performance on the tested datasets with impressive speed-ups and memory savings.

For completeness, it would be interesting, and I would strongly encourage to add a discussion or even an experiment using feedforward DNN with a simple dataset as MNIST, as most of previous work discussed in the paper report experiments on DNN that are feedforward. Would the speed-ups and memory savings obtained for RNN hold also for feedforward networks?




","[8, 7, 7]","[' Top 50% of accepted papers, clear accept', ' Good paper, accept', ' Good paper, accept']","[4, 4, 2]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']","The reviewer keeps their original rating after the authors addressed their comments, implying a positive sentiment. They find the method elegant, the paper easy to follow, and the topic relevant. While noting a slight lack of novelty, they acknowledge the strong motivation and application to RNNs. The reviewer also points out the impressive results and suggests an additional experiment, which is a recommendation for improvement rather than a criticism. The language used is constructive and polite throughout.",75.0,90.0
Divide and Conquer Networks,"['Alex Nowak', 'David Folqué', 'Joan Bruna']",Accept,2018,"[-2, 23, 23]","[1, 27, 27]","[1, 48, 91]","[1, 38, 72]","[0, 0, 0]","[0, 10, 19]","This paper proposes to add new inductive bias to neural network architecture - namely a divide and conquer strategy know from algorithmics. Since introduced model has to split data into subsets, it leads to non-differentiable paths in the graph, which authors propose to tackle with RL and policy gradients. The whole model can be seen as an RL agent, trained to do splitting action on a set of instances in such a way, that jointly trained predictor T quality is maximised (and thus its current log prob: log p(Y|P(X)) becomes a reward for an RL agent). Authors claim that model like this (strengthened with pointer networks/graph nets etc. depending on the application) leads to empirical improvement on three tasks - convex hull finding, k-means clustering and on TSP.  However, while results on convex hull task are good, k-means ones use a single, artificial problem (and do not test DCN, but rather a part of it), and on TSP DCN performs significantly worse than baselines in-distribution, and is better when tested on bigger problems than it is trained on. However the generalisation scores themselves are pretty bad thus it is not clear if this can be called a success story.

I will be happy to revisit the rating if the experimental section is enriched.

Pros:
- very easy to follow idea and model
- simple merge or RL and SL in an end-to-end trainable model
- improvements over previous solutions

Cons:
- K-means experiments should not be run on artificial dataset, there are plenty of benchmarking datasets out there. In current form it is just a proof of concept experiment rather than evaluation (+ if is only for splitting, not for the entire architecture proposed). It would be also beneficial to see the score normalised by the cost found by k-means itself (say using Lloyd's method), as otherwise numbers are impossible to interpret. With normalisation, claiming that it finds 20% worse solution than k-means is indeed meaningful. 
- TSP experiments show that ""in distribution"" DCN perform worse than baselines, and when generalising to bigger problems they fail more gracefully, however the accuracies on higher problem are pretty bad, thus it is not clear if they are significant enough to claim success. Maybe TSP is not the best application of this kind of approach (as authors state in the paper - it is not clear how merging would be applied in the first place). 
- in general - experimental section should be extended, as currently the only convincing success story lies in convex hull experiments

Side notes:
- DCN is already quite commonly used abbreviation for ""Deep Classifier Network"" as well as ""Dynamic Capacity Network"", thus might be a good idea to find different name.
- please fix \cite calls to \citep, when authors name is not used as part of the sentence, for example:
Graph Neural Network Nowak et al. (2017) 
should be
Graph Neural Network (Nowak et al. (2017))

# After the update

Evaluation section has been updated threefold:
- TSP experiments are now in the appendix rather than main part of the paper
- k-means experiments are Lloyd-score normalised and involve one Cifar10 clustering
- Knapsack problem has been added

Paper significantly benefited from these changes, however experimental section is still based purely on toy datasets (clustering cifar10 patches is the least toy problem, but if one claims that proposed method is a good clusterer one would have to beat actual clustering techniques to show that), and in both cases simple problem-specific baseline (Lloyd for k-means, greedy knapsack solver) beats proposed method. I can see the benefit of trainable approach here, the fact that one could in principle move towards other objectives, where deriving Lloyd alternative might be hard; however current version of the paper still does not show that.

I increased rating for the paper, however in order to put the ""clear accept"" mark I would expect to see at least one problem where proposed method beats all basic baselines (thus it has to either be the problem where we do not have simple algorithms for it, and then beating ML baseline is fine; or a problem where one can beat the typical heuristic approaches).

","[6, 7, 7]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Good paper, accept']","[3, 3, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review is mostly critical, highlighting limitations in the experimental section and suggesting improvements. While acknowledging the paper's merits (easy to follow, simple model, some improvements), the reviewer emphasizes the need for stronger empirical evidence. The reviewer's willingness to revisit the rating after improvements and the specific suggestions indicate a desire to see the paper succeed, making the overall tone constructive but leaning towards the negative due to the experimental shortcomings.",-20.0,70.0
Backpropagation through the Void: Optimizing control variates for black-box gradient estimation,"['Will Grathwohl', 'Dami Choi', 'Yuhuai Wu', 'Geoff Roeder', 'David Duvenaud']",Accept,2018,"[3, 2, 3, 2, 9]","[8, 4, 8, 6, 13]","[28, 9, 69, 13, 103]","[12, 4, 28, 5, 52]","[15, 5, 38, 8, 49]","[1, 0, 3, 0, 2]","This paper introduces LAX/RELAX, a method to reduce the variance of the REINFORCE gradient estimator. The method builds on and is directly inspired by REBAR. Similarly to REBAR, RELAX is an unbiased estimator, and the idea is to introduce a control variate that leverages the reparameterization gradient. In contrast to REBAR, RELAX learns a free-from control variate, which allows for low-variance gradient estimates for both discrete and continuous random variables. The method is evaluated on a toy experiment, as well as the discrete VAE and reinforcement learning. It effectively reduces the variance of state-of-the-art methods (namely, REBAR and actor-critic).

Overall, I enjoyed reading the paper. I think it is a neat idea that can be of interest for researchers in the field. The paper is clearly explained, and I found the experiments convincing. I have minor comments only.

+ Is there a good way to initialize c_phi prior to optimization? Given that c_phi must be a proxy for f(), maybe you can take advantage of this observation to find a good initialization for phi?

+ I was confused with the Bernoulli example in Appendix B. Consider the case theta=0.5. Then, b=H(z) takes value 1 if z>0, and 0 otherwise. Thus, p(z|b,theta) should assign mass zero to values z>0 when b=0, which does not seem to be the case with the proposed sampling scheme in page 11, since v*theta=0.5*v, which gives values in [0,0.5]. And similarly for the case b=1.

+ Why is the method called LAX? What does it stand for?

+ In Section 3.3, it is unclear to me why rho!=phi. Given that c_phi(z)=f(sigma_lambda(z))+r_rho(z), with lambda being a temperature parameter, why isn't rho renamed as phi? (the first term doesn't seem to have any parameters). In general, this section was a little bit unclear if you are not familiar with the REBAR method; consider adding more details.

+ Consider adding a brief review of the REBAR estimator in the Background section for those readers who are less familiar with this approach.

+ In the abstract, consider adding two of the main ideas that the estimator relies on: control variates and reparameterization gradients. This would probably be more clear than ""based on gradients of a learned function.""

+ In the first paragraph of Section 3, the sentence ""f is not differentiable or not computable"" may be misleading, because it is unclear what ""not computable"" means (one may think that it cannot be evaluated). Consider replacing with ""not analytically computable.""

+ In Section 3.3, it reads ""differentiable function of discrete random variables,"" which does not make sense.

+ Before Eq. 11, it reads ""where epsilon_t does not depend on theta"". I think it should be the distribution over epsilon_t what doesn't depend on theta.

+ In Section 6.1, it was unclear to me why t=.499 is a more challenging setting.

+ The header of Section 6.3.1 should be removed, as Section 6.3 is short.

+ In Section 6.3.1, there is a broken reference to a figure.

+ Please avoid contractions (doesn't, we'll, it's, etc.)

+ There were some other typos; please read carefully the paper and double-check the writing. In particular, I found some missing commas, some proper nouns that are not capitalized in Section 5, and others (e.g., ""an learned,"" ""gradient decent"").","[8, 7, 6]","[' Top 50% of accepted papers, clear accept', ' Good paper, accept', ' Marginally above acceptance threshold']","[4, 3, 2]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']","The review is largely positive. The reviewer states they ""enjoyed reading the paper"" and found the idea ""neat"", the paper ""clearly explained"", and the experiments ""convincing."" They also characterize their comments as ""minor."" All of this points towards a positive sentiment. The language used is polite and professional throughout, with no instances of rudeness or unprofessionalism.",75.0,90.0
Modular Continual Learning in a Unified Visual Environment,"['Kevin T. Feigelis', 'Blue Sheffer', 'Daniel L. K. Yamins']",Accept,2018,"[2, 2, 15]","[5, 6, 20]","[7, 4, 78]","[3, 2, 40]","[4, 1, 33]","[0, 1, 5]","Reading this paper feels like reading at least two closely-related papers compressed into one, with overflow into the appendix (e.g. one about the EMS module, one about the the recurrent voting, etc).

There were so many aspects/components, that I am not entirely confident I fully understood how they all work together, and in fact I am pretty confident there was at least some part of this that I definitely did not understand. Reading it 5-20 more times would most likely help.

For example, consider the opening example of Section 3. In principle, this kind of example is great, and more of these would be very useful in this paper. This particular one raises a few questions:
-Eq 5 makes it so that $(W \Psi)$ and $(a_x)$ need to be positive or negative together.  Why use ReLu's here at all? Why not just $sign( (W \Psi) a_x) $? Multiplying them will do the same thing, and is much simpler. I am probably missing something here, would like to know what it is... (Or, if the point of the artificial complexity is to give an example of the 3 basic principles, then perhaps point this out, or point out why the simpler version I just suggested would not scale up, etc)
-what exactly, in this example, does $\Psi$ correspond to? In prev discussion, $\Psi$ is always written with subscripts to denote state history (I believe), so this is an opportunity to explain what is different here. 
-Nitpick: why is a vector written as $W$? (or rather, what is the point of bold vs non-bold here?)
-a non-bold version of $Psi$, a few lines below, seems to correspond to the 4096 features of VGG's FC6, so I am still not sure what the bold version represents

-The defs/eqns at the beginning of section 3.1 (Sc, CReLu, etc) were slightly hard to follow and I wonder whether there were any typos, e.g. was CReS meant to refer directly to Sc, but used the notation ${ReLu}^2$ instead? 

Each of these on its own would be easier to overlook, but there is a compounding effect here for me, as a reader, such that by further on in the paper, I am rather confused.

I also wonder whether any of the elements described, have more ""standard"" interpretations/notations. For example, my slight confusion propagated further: after above point, I then did not have a clear intuition about $l_i$ in the EMS module. I get that symmetry has been built in, e.g. by the definitions of CReS and CReLu, etc, but I still don't see how it all works together, e.g. are late bottleneck architectures *exactly* the same as MLPs, but where inputs have simply been symmetrized, squared, etc? Nor do I have intuition about multiplicative symmetric interactions between visual features and actions, although I do get the sense that if I were to spend several hours implementing/writing out toy examples, it would clarify it significantly (in fact, I wouldn't be too surprised if it turns out to be fairly straightforward, as in my above comment indicating a seeming equivalence to simply multiplying two terms and taking the resulting sign). If the paper didn't need to be quite as dense, then I would suggest providing more elucidation for the reader, either with intuitions or examples or clearer relationships to more familiar formulations.

Later, I did find that some of the info I *needed* in order to understand the results (e.g. exactly what is meant by a ""symmetry ablation"", how was that implemented?) was in fact in the appendices (of which there are over 8 pages).

I do wonder how sensitive the performance of the overall system is to some of the details, like, e.g. the low-temp Boltzmann sampling rather than identity function, as described at the end of S2.

My confidence in this review is somewhere between 2 and 3.

The problem is an interesting one, the overall approach makes sense, it is clear the authors have done a very substantial  amount of work, and very diligently so (well-done!), some of the ideas are interesting and seem creative, but I am not sure I understand the glue of the details, and that might be very important here in order to assess it effectively.","[8, 8, 6]","[' Top 50% of accepted papers, clear accept', ' Top 50% of accepted papers, clear accept', ' Marginally above acceptance threshold']","[2, 3, 2]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']","The reviewer expresses mixed sentiment, acknowledging the paper's interesting problem, sensible approach, and the authors' substantial work. However, they also express confusion and a lack of understanding regarding the paper's many components and how they work together. They find the paper dense and difficult to follow, with several points requiring multiple readings or further clarification. While they don't outright criticize the work, their lack of understanding hinders a fully positive assessment. Therefore, the sentiment leans slightly negative. The language used is polite and professional throughout, with constructive criticism and a recognition of the authors' efforts.",-20.0,80.0
VoiceLoop: Voice Fitting and Synthesis via a Phonological Loop,"['Yaniv Taigman', 'Lior Wolf', 'Adam Polyak', 'Eliya Nachmani']",Accept,2018,"[10, 19, 4, 3]","[15, 24, 9, 8]","[42, 413, 41, 47]","[19, 213, 16, 14]","[21, 165, 23, 30]","[2, 35, 2, 3]","This paper present the application of the memory buffer concept to speech synthesis, and additionally learns a ""speaker vector"" that makes the system adaptive and work reasonably well on ""in-the-wild"" speech data. This is a relevant problem, and a novel solution, but synthesis is a wicked problem to evaluate, so I am not sure if ICLR is the best venue for this paper. I see two competing goals:

- If the focus is on showing that the presented approach outperforms other approaches under given conditions, a different task would be better (for example recognition, or some sort of trajectory reconstruction)
- If the focus is on showing that the system outperforms other synthesis systems, then a speech oriented venue might be best (and it is unfortunate that optimized hyper-parameters for the other systems are not available for a fair comparsion)
- If fair comparisons with the other appraoches cannot be made, my sense is that the multi-speaker (post-training fitting) option is really the most interesting and novel contribution here, which could be discussed in mroe detail

Still, the approach is creative and interesting and deserves to be presented. I have a few questions/ suggestions:

Introduction

- The link to Baddeley's ""phonological loop"" concept seems weak at best. There is nothing phonological about the features that this model stores and retrieves, and no evidence that the model behaves in a way consistent with ""phonologcial"" (or articulatory) assumptions or models - maybe best to avoid distracting the reader with this concept and strengthen the speaker adaptation aspect?
- The memory model is not an RNN, but it is a recurrently called structure (as the name ""phonological loop"" also implies) - so I would also not highlight this point much
- Why would the four properties of the proposed method (mid of p. 2, end of introduction: memory buffer, shared memory, shallow fully connected networks, and simple reader mechanism) lead to better robustness and improve performance on noisy and limited training data? Maybe the proposed approach works better for any speech synthesis task? Why specifically for ""in-the-wild"" data? The results in Table 2 show that the proposed system outperforms other systems on Blizzard 2013, but not Blizzard 2011 - does this support the previous argument?
- Why not also evaluate MCD scores? This should be a quick and automatic way to diagnose what the system is doing? Or is this not meaningful with the noisy training data?

Previous work

- Please introduce abbreviations the first time they are used (""CBHG"" for example)
- There is other work on using ""in-the-wild"" speech as well: Pallavi Baljekar and Alan W Black. Utterance Selection Techniques for TTS Systems using Found Speech, SSW 2016, Sunnyvale, USA Sept 2016

The architecture
- Please explain the ""GMM"" (Gaussian Mixture Model?) attention mechanism in a bit more detail, how does back-propagation work in this case?
- Why was this approach chosen? Does it promise to be robust or good for low data situations specifically?
- The fonts in Figure 2 are very small, please make them bigger, and the Figure may not print well in b/w. Why does the mean of the absolute weights go up for high buffer positions? Is there some ""leaking"" from even longer contexts?
- I don't understand ""However, human speech is not deterministic and one cannot expect [...] truth"". You are saying that the model cannot be excepted to reproduce the input exactly? Or does this apply only to the temporal distribution of the sequence (but not the spectral characteristics)? The previous sentence implies that it does. And how does teacher-forcing help in this case?
- what type of speed is ""x5""? Five times slower or faster than real-time?

Experiments
- Table 2: maybe mention how these results were computed, i.e. which systems use optimized hyper parameters, and which don't? How do these results support the interpretation of hte results in the introruction re in-the-wild data and found data?
- I am not sure how to read Figure 4. Maybe it would be easier to plot the different phone sequences against each other and show how the timings are off, i.e. plot the time of the center of panel one vs the time of the center of panel 2 for the corresponding phone, and show how this is different from a straight line. Or maybe plot phones as rectangles that get deformed from square shape as durations get learned?
- Figure 5: maybe provide spectrograms and add pitch contours to better show the effect of the dfifferent intonations? 
- Figure 4 uses a lot of space, could be reduced, if needed

Discussion
- I think the first claim is a bit to broad - nowhere is it shown that the method is inherently more robust to clapping and laughs, and variable prosody. The authors will know the relevant data-sets better than I do, maybe they can simply extend the discussion to show that this is what happens. 
- Efficiency: I think Wavenet has also gotten much faster and runs in less than real-time now - can you expand that discussion a bit, or maybe give estimates in times of FLOPS required, rather than anecdotal evidence for systems that may or may not be comparable?

Conclusion
- Now the advantage of the proposed model is with the number of parameters, rather than the computation required. Can you clarify? Are your models smaller than competing models?
","[5, 6, 8]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Top 50% of accepted papers, clear accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer provides constructive criticism, acknowledges the novelty of the work, and suggests improvements. While they express uncertainty about the ideal venue and have concerns, their tone is professional and suggestive, rather than dismissive. There are no personal attacks or disrespectful language.",50.0,70.0
Cascade Adversarial Machine Learning Regularized with a Unified Embedding,"['Taesik Na', 'Jong Hwan Ko', 'Saibal Mukhopadhyay']",Accept,2018,"[11, 4, 17]","[15, 9, 22]","[35, 69, 311]","[19, 36, 178]","[5, 13, 37]","[11, 20, 96]","The authors proposed to supplement adversarial training with an additional regularization that forces the embeddings of clean and adversarial inputs to be similar. The authors demonstrate on MNIST and CIFAR that the added regularization leads to more robustness to various kinds of attacks. The authors further propose to enhance the network with cascaded adversarial training, that is, learning against iteratively generated adversarial inputs, and showed improved performance against harder attacks. 

The idea proposed is fairly straight-forward. Despite being a simple approach, the experimental results are quite promising.  The analysis on the gradient correlation coefficient and label leaking phenomenon provide some interesting insights.  

As pointed out in section 4.2, increasing the regularization coefficient leads to degenerated embeddings. Have the authors consider distance metrics that are less sensitive to the magnitude of the embeddings, for example, normalizing the inputs before sending it to the bidirectional or pivot loss, or use cosine distance etc.?

Table 4 and 5 seem to suggest that cascaded adversarial learning have more negative impact on test set with one-step attacks than clean test set, which is a bit counter-intuitive. Do the authors have any insight on this? 

Comments:
1. The writing of the paper could be improved. For example, ""Transferability analysis"" in section 1 is barely understandable;
2. Arrow in Figure 3 are not quite readable;
3. The paper is over 11 pages. The authors might want to consider shrink it down the recommended length. ","[6, 5, 6]","[' Marginally above acceptance threshold', ' Marginally below acceptance threshold', ' Marginally above acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer finds the idea fairly straightforward but acknowledges the promising experimental results and interesting insights. They provide constructive criticism and suggestions for improvement without resorting to harsh language. Overall, the tone is positive and encouraging.",60.0,80.0
TRAINING GENERATIVE ADVERSARIAL NETWORKS VIA PRIMAL-DUAL SUBGRADIENT METHODS: A LAGRANGIAN PERSPECTIVE ON GAN,"['Xu Chen', 'Jiang Wang', 'Hao Ge']",Accept,2018,"[21, 1, 12]","[26, 1, 17]","[481, 2, 67]","[205, 1, 31]","[83, 1, 8]","[193, 0, 28]","This paper proposed a framework to connect the solving of GAN with finding the saddle point of a minimax problem.
As a result, the primal-dual subgradient methods can be directly introduced to calculate the saddle point.
Additionally, this idea not only fill the relatviely lacking of theoretical results for GAN or WGAN, but also provide a new perspective to modify the GAN-type models.
But this saddle point model reformulation  in section 2 is quite standard, with limited theoretical analysis in Theorem 1.
As follows, the resulting algorithm 1 is also standard primal-dual method for a saddle point problem.
Most important I think, the advantage of considering GAN-type model as a saddle point model is that first--order methods can be designed to solve it. But the numerical experiments part seems to be a bit weak, because the MINST or CIFAR-10 dataset is not large enough to test the extensibility for large-scale cases. ","[6, 7, 7]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Good paper, accept']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with a positive acknowledgment of the paper's contribution, highlighting its novelty and potential impact. However, it then transitions into a more critical analysis, pointing out limitations in the theoretical depth and empirical validation. The reviewer acknowledges the potential of the proposed approach but expresses reservations about its practical significance due to the limited experimental scope. Overall, the review presents a balanced perspective, acknowledging both strengths and weaknesses, which leans slightly more towards the critical side.",20.0,70.0
Emergent Communication through Negotiation,"['Kris Cao', 'Angeliki Lazaridou', 'Marc Lanctot', 'Joel Z Leibo', 'Karl Tuyls', 'Stephen Clark']",Accept,2018,"[3, 8, 12, 8, 17, 20]","[8, 12, 17, 13, 22, 25]","[19, 64, 105, 92, 242, 139]","[7, 32, 46, 29, 145, 94]","[11, 30, 48, 54, 53, 28]","[1, 2, 11, 9, 44, 17]","The authors describe a variant of the negotiation game in which agents of different type, selfish or prosocial, and with different preferences. The central feature is the consideration of a secondary communication (linguistic) channel for the purpose of cheap talk, i.e. talk whose semantics are not laid out a priori. 

The essential findings include that prosociality is a prerequisite for effective communication (i.e. formation of meaningful communication on the linguistic channel), and furthermore, that the secondary channel helps improve the negotiation outcomes.

The paper is well-structured and incrementally introduces the added features and includes staged evaluations for the individual additions, starting with the differentiation of agent characteristics, explored with combination of linguistic and proposal channel. Finally, agent societies are represented by injecting individuals' ID into the input representation.

The positive:
- The authors attack the challenging task of given agents a means to develop communication patterns without apriori knowledge.
- The paper presents the problem in a well-structured manner and sufficient clarity to retrace the essential contribution (minor points for improvement).
- The quality of the text is very high and error-free.
- The background and results are well-contextualised with relevant related work. 

The problematic:
- By the very nature of the employed learning mechanisms, the provided solution provides little insight into what the emerging communication is really about. In my view, the lack of interpretable semantics hardly warrants a reference to 'cheap talk'. As such the expectations set by the well-developed introduction and background sections are moderated over the course of the paper.
- The goal of providing agents with richer communicative ability without providing prior grounding is challenging, since agents need to learn about communication partners at runtime. But it appears as of the main contribution of the paper can be reduced to the decomposition of the learnable feature space into two communication channels. The implicit relationship of linguistic channel on proposal channel input based on the time information (Page 4, top) provides agents with extended inputs, thus enabling a more nuanced learning based on the relationship of proposal and linguistic channel. As such the well-defined semantics of the proposal channel effectively act as the grounding for the linguistic channel. This, then, could have been equally achieved by providing agents with a richer input structure mediated by a single channel. From this perspective, the solution offers limited surprises. The improvement of accuracy in the context of agent societies based on provided ID follows the same pattern of extending the input features.
- One of the motivating factors of using cheap talk is the exploitation of lying on the part of the agents. However, apart from this initial statement, this feature is not explicitly picked up. In combination with the previous point, the necessity/value of the additional communication channel is unclear.

Concrete suggestions for improvement:

- Providing exemplified communication traces would help the reader appreciate the complexity of the problem addressed by the paper.
- Figure 3 is really hard to read/interpret. The same applies to Figure 4 (although less critical in this case).
- Input parameters could have been made explicit in order to facilitate a more comprehensive understanding of technicalities (e.g. in appendix).
- Emergent communication is effectively unidirectional, with one agent as listener. Have you observed other outcomes in your evaluation?

In summary, the paper presents an interesting approach to combine unsupervised learning with multiple communication channels to improve learning of preferences in a well-established negotiation game. The problem is addressed systematically and well-presented, but can leave the reader with the impression that the secondary channel, apart from decomposing the model, does not provide conceptual benefit over introducing a richer feature space that can be exploited by the learning mechanisms. Combined with the lack of specific cheap talk features, the use of actual cheap talk is rather abstract. Those aspects warrant justification.","[6, 5, 7]","[' Marginally above acceptance threshold', ' Marginally below acceptance threshold', ' Good paper, accept']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is mostly positive, highlighting the paper's strengths like well-structured presentation, clarity, and contextualization. However, it raises significant concerns about the core contribution and interpretation of results. The reviewer finds the lack of interpretability in the communication a major drawback, questioning the use of 'cheap talk' and suggesting the benefit of the second channel is more about feature space expansion than novel communication. While the language is critical, it's professional and constructive, offering concrete suggestions for improvement.",40.0,70.0
Learning to Multi-Task by Active Sampling,"['Sahil Sharma*', 'Ashutosh Kumar Jha*', 'Parikshit S Hegde', 'Balaraman Ravindran']",Accept,2018,"[6, 1, 1, 21]","[11, 1, 6, 26]","[36, 1, 10, 253]","[12, 1, 7, 142]","[12, 0, 3, 86]","[12, 0, 0, 25]","
The authors show empirically that formulating multitask RL itself as an active learning and ultimately as an RL problem can be very fruitful.  They design and explore several approaches  to the active learning (or active sampling) problem, from a basic 
change to the distribution to UCB to feature-based neural-network based RL. The domain is video games.   All proposed approaches beat the uniform sampling baselines and the more sophisticated approaches do better in the scenarios with more tasks (one multitask  problem had 21 tasks).


Pros:

- very promising results with an interesting active learning approach to multitask RL

- a number of approaches developed for the basic idea

- a variety of experiments, on challenging multiple task problems (up to 21 tasks/games)

- paper is overall well written/clear

Cons:

- Comparison only to a very basic baseline (i.e. uniform sampling)
Couldn't comparisons be made, in some way, to other multitask work?



Additional  comments:

- The assumption of the availability of a target score goes against
the motivation that one need not learn individual networks ..  authors
say instead one can use 'published' scores, but that only assumes
someone else has done the work (and furthermore, published it!).

The authors do have a section on eliminating the need by doubling an
estimate for each task) which makes this work more acceptable (shown
for 6 tasks or MT1, compared to baseline uniform sampling).

Clearly there is more to be done here for a future direction (could be
mentioned in future work section).

- The averaging metrics (geometric, harmonic vs arithmetic, whether
  or not to clip max score achieved) are somewhat interesting, but in
  the main paper, I think they are only used in section 6 (seems like
  a waste of space). Consider moving some of the results, on showing
  drawbacks of arithmetic mean with no clipping (table 5 in appendix E), from the appendix to
  the main paper.


- The can be several benefits to multitask learning, in particular
  time and/or space savings in learning new tasks via learning more
  general features. Sections 7.2 and 7.3 on specificity/generality of
  features were interesting.



--> Can the authors show that a trained network (via their multitask
    approached) learns significantly faster on a brand new game
    (that's similar to games already trained on), compared to learning from
    scratch?

--> How does the performance improve/degrade (or the variance), on the
    same set of tasks, if the different multitask instances (MT_i)
    formed a supersets hierarchy, ie if MT_2 contained all the
    tasks/games in MT_1, could training on MT_2 help average
    performance on the games in MT_1 ? Could go either way since the network
   has to allocate resources to learn other games too.  But is there a pattern?



- 'Figure 7.2' in section 7.2 refers to Figure 5.


- Can you motivate/discuss better why not providing the identity of a
  game as an input is an advantage? Why not explore both
  possibilities? what are the pros/cons? (section 3)




","[7, 7, 5]","[' Good paper, accept', ' Good paper, accept', ' Marginally below acceptance threshold']","[3, 5, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']","The review is positive overall. The reviewer highlights the strengths of the paper, such as the promising results, the variety of approaches and experiments, and the clear writing. While the reviewer points out some areas for improvement, such as the need for stronger baselines and further exploration of certain aspects, these are presented as constructive suggestions rather than harsh criticisms. The language used is polite and professional throughout.",60.0,80.0
Variational Network Quantization,"['Jan Achterhold', 'Jan Mathias Koehler', 'Anke Schmeink', 'Tim Genewein']",Accept,2018,"[9, 17, -3]","[13, 21, 1]","[5, 38, 2]","[3, 24, 1]","[0, 0, 0]","[2, 14, 1]","This paper presents Variational Network Quantization; a variational Bayesian approach for quantising neural network weights to ternary values post-training in a principled way. This is achieved by a straightforward extension of the scale mixture of Gaussians perspective of the log-uniform prior proposed at [1]. The authors posit a mixture of delta peaks hyperprior over the locations of the Gaussian distribution, where each peak can be seen as the specific target value for quantisation (including zero to induce sparsity). They then further propose an approximation for the KL-divergence, necessary for the variational objective, from this multimodal prior to a factorized Gaussian posterior by appropriately combining the approximation given at [2] for each of the modes. At test-time, the variational posterior for each weight is replaced by the target quantisation value that is closest, w.r.t. the squared distance, to the mean of the Gaussian variational posterior. Encouraging experimental results are shown with performance comparable to the state-of-the-art for ternary weight neural networks.

This paper presented a straightforward extension of the work done at [1, 2] for ternary networks through a multimodal quantising prior. It is generally well-written, with extensive preliminaries and clear equations. The visualizations also serve as a nice way to convey the behaviour of the proposed approach. The idea is interesting and well executed so I propose for acceptance. I only have a couple of minor questions: 
- For the KL-divergence approximation you report a maximum difference of 1 nat per weight that seems a bit high; did you experiment with the `naive` Monte Carlo approximation of the bound (e.g. as done at Bayes By Backprop) during optimization? If yes, was there a big difference in performance?
- Was pre-training necessary to obtain the current results for MNIST? As far as I know, [1] and [2] did not need pre-training for the MNIST results (but did employ pre-training for CIFAR 10).
- How necessary was each one of the constraints during optimization (and what did they prevent)? 
- Did you ever observe posterior means that do not settle at one of the prior modes but rather stay in between? Or did you ever had issues of the variance growing large enough, so that q(w) captures multiple modes of the prior (maybe the constraints prevent this)? How sensitive is the quantisation scheme?

Other minor comments / typos:
(1) 7th line of section 2.1 page 2, ‘a unstructured data’ -> ‘unstructured data’
(2) 5th line on page 3, remove ‘compare Eq. (1)’ (or rephrase it appropriately).
(3) Section 2.2, ’Kullback-Leibler divergence between the true and the approximate posterior’; between implies symmetry (and the KL isn’t symmetric) so I suggest to change it to e.g. ‘from the true to the approximate posterior’ to avoid confusion. Same for the first line of Section 3.3.
(4) Footnote 2, the distribution of the noise depends on the random variable so I would suggest to change it to a general \epsilon \sim p(\epsilon).
(5) Equation 4 is confusing.

[1] Louizos, Ullrich & Welling, Bayesian Compression for Deep Learning.
[2] Molchanov, Ashukha & Vetrov, Variational Dropout Sparsifies Deep Neural Networks.","[7, 7, 7]","[' Good paper, accept', ' Good paper, accept', ' Good paper, accept']","[4, 3, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review starts with positive statements, highlighting the merits of the paper such as being 'well-written', 'interesting', and 'well-executed'. The reviewer recommends acceptance, a clear indicator of a positive sentiment. While the review does list specific questions and suggestions for improvement, these are presented constructively and framed as opportunities for enhancing the paper. The language throughout is formal, respectful, and encouraging, signifying a high degree of politeness.",80.0,90.0
Variational Continual Learning,"['Cuong V. Nguyen', 'Yingzhen Li', 'Thang D. Bui', 'Richard E. Turner']",Accept,2018,"[19, 8, 5, 23]","[24, 13, 9, 28]","[209, 56, 16, 222]","[106, 27, 9, 134]","[22, 20, 5, 49]","[81, 9, 2, 39]","The paper describes the problem of continual learning, the non-iid nature of most real-life data and point out to the catastrophic forgetting phenomena in deep learning. The work defends the point of view that Bayesian inference is the right approach to attack this problem and address difficulties in past implementations. 

The paper is well written, the problem is described neatly in conjunction with the past work, and the proposed algorithm is supported by experiments. The work is a useful addition to the community.

My main concern focus on the validity of the proposed model in harder tasks such as the Atari experiments in Kirkpatrick et. al. (2017) or the split CIFAR experiments in Zenke et. al. (2017). Even though the experiments carried out in the paper are important, they fall short of justifying a major step in the direction of the solution for the continual learning problem.","[6, 6, 6]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[2, 3, 4]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with a positive sentiment, acknowledging the paper's contribution and clarity. Phrases like ""well-written,"" ""neatly described,"" and ""useful addition"" indicate a favorable view. However, the reviewer expresses concern about the model's applicability to more complex tasks, suggesting the experiments are not comprehensive enough to justify the claims fully. This reservation pulls the sentiment slightly towards neutral. The language remains consistently professional and polite throughout.",60.0,80.0
Evaluating the Robustness of Neural Networks: An Extreme Value Theory Approach,"['Tsui-Wei Weng*', 'Huan Zhang*', 'Pin-Yu Chen', 'Jinfeng Yi', 'Dong Su', 'Yupeng Gao', 'Cho-Jui Hsieh', 'Luca Daniel']",Accept,2018,"[3, 3, 9, 7, 13, 5, 11, 18]","[8, 8, 14, 12, 18, 9, 16, 23]","[45, 102, 390, 125, 37, 13, 355, 136]","[16, 51, 150, 64, 12, 8, 171, 63]","[28, 50, 199, 55, 11, 4, 158, 39]","[1, 1, 41, 6, 14, 1, 26, 34]","In this work, the objective is to analyze the robustness of a neural network to any sort of attack.

This is measured by naturally linking the robustness of the network to the local Lipschitz properties of the network function. This approach is quite standard in learning theory, I am not aware of how original this point of view is within the deep learning community.

This is estimated by obtaining values of the norm of the gradient (also naturally linked to the Lipschitz properties of the function) by backpropagation. This is again a natural idea.","[7, 7, 7]","[' Good paper, accept', ' Good paper, accept', ' Good paper, accept']","[1, 3, 3]","["" The reviewer's evaluation is an educated guess"", ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review is lukewarm in its assessment. While it acknowledges the work's objective and the standard nature of the approach in learning theory, it questions the originality within the deep learning community. The repeated use of ""natural"" and ""naturally"" suggests the reviewer finds the methods somewhat obvious, which doesn't convey strong positivity. The lack of strong praise or criticism points towards neutrality.",-10.0,50.0
TD or not TD: Analyzing the Role of Temporal Differencing in Deep Reinforcement Learning,"['Artemij Amiranashvili', 'Alexey Dosovitskiy', 'Vladlen Koltun', 'Thomas Brox']",Accept,2018,"[1, 6, 19, 17]","[5, 10, 24, 22]","[11, 96, 295, 297]","[4, 41, 135, 155]","[7, 44, 103, 101]","[0, 11, 57, 41]","This paper revisits a subject that I have not seen revisited empirically since the 90s: the relative performance of TD and Monte-Carlo style methods under different values for the rollout length. Furthermore, the paper performs controlled experiments using the VizDoom environment to investigate the effect of a number of other environment characteristics, such as reward sparsity or perceptual complexity. The most interesting and surprising result is that finite-horizon Monte Carlo performs competitively in most tasks (with the exception of problems where terminal states play a big role (it does not do well at all on Pong!), and simple gridworld-type representations), and outperforms TD approaches in many of the more interesting settings. There is a really interesting experiment performed that suggests that this is the case due to finite-horizon MC having an easier time with learning perceptual representations. They also show, as a side result, that the reward decomposition in Dosvitskiy & Koltun (oral presentation at ICLR 2017) is not necessary for learning a good policy in VizDoom.

Overall, I find the paper important for furthering the understanding of fundamental RL algorithms. However, my main concern is regarding a confounding factor that may have influenced the results: Q_MC uses a multi-headed model, trained on different horizon lengths, whereas the other models seem to have a single prediction head. May this helped Q_MC have better perceptual capabilities?

A couple of other questions:
- I couldn't find any mention of eligibility traces - why?
- Why was the async RL framework used? It would be nice to have a discussion on whether this choice may have affected the results.","[7, 7, 7]","[' Good paper, accept', ' Good paper, accept', ' Good paper, accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer clearly states that they find the paper important and interesting. They highlight the significance and surprising nature of the findings. While they raise a valid concern, it is presented as a question rather than a harsh criticism. The reviewer's tone remains constructive throughout.",75.0,75.0
A Bayesian Perspective on Generalization and Stochastic Gradient Descent,['Samuel L. Smith and Quoc V. Le'],Accept,2018,"[3, 14]","[8, 19]","[30, 299]","[10, 143]","[20, 145]","[0, 11]","Summary:
This paper presents a very interesting perspective on why deep neural networks may generalize well, in spite of their high capacity (Zhang et al, 2017). It does so from the perspective of ""Bayesian model comparison"", where two models are compared based on their ""marginal likelihood"" (aka, their ""evidence"" --- the expected probability of the training data under the model, when parameters are drawn from the prior).  It first shows that a simple weakly regularized (linear) logistic regression model over 200 dimensional data can perfectly memorize a random training set with 200 points, while also generalizing well when the class labels are not random (eg, when a simple linear model explains the class labels); this provides a much simpler example of a model generalizing well in spite of high capacity, relative to the experiments presented by Zhang et al (2017). It shows that in this very simple setting, the ""evidence"" of a model correlates well with the test accuracy, and thus could explain this phenomena (evidence is low for model trained on random data, but high for model trained on real data).

The paper goes on to show that if the evidence is approximated using a second order Taylor expansion of the cost function around a minimia $w_0$, then the evidence is controlled by the cost at the minimum, and by the logarithm of the ratio of the curvature at the minimum compared to the regularization constant (eg, standard deviation of gaussian prior).  Thus, Bayesian evidence prefers minima that are both deep and broad.  This provides a way of comparing models in a way which is independent of the model parametrization (unfortunately, however, computing the evidence is intractable for large networks). The paper then discusses how SGD can be seen as an algorithmic way of finding minima with large ""evidence"" --- the ""noise"" in the gradient estimation helps the model avoid ""sharp"" minima, while the gradient helps the model find ""deep"" minima.  The paper shows that SGD can be understood using stochastic differential equations, where the noise scale is approximately aN/((1-m)B) (a = learning rate, N = size of training set, B = batch size, m = momentum).  It argues that because there should be an optimal noise scale (which maximizes test performance), the batch size should be taken proportional to the learning rate, as well as the training set size, and proportional to 1/(1-m).  These scaling rules are confirmed experimentally (DNN trained on MNIST).  Thus, this Bayesian perspective can also help explain the observation that models trained with smaller batch sizes (noisier gradient estimates) often generalize better than those with larger batch sizes (Kesker et al, 2016). These scaling rules provide guidance on how to increase the batch size, which is desirable for increasing the parralelism of SGD training.

Review:
Quality: The quality of the work is high.  Experiments and analysis are both presented clearly.

Clarity: The paper is relatively clear, though some of the connections between the different parts of the paper felt unclear to me:
1) It would be nice if the paper were to explain, from a theoretical perspective, why large evidence should correspond to better generalization, or provide an overview of the work which has shown this (eg, Rissanen, 1983).
2) Could margin-based generalization bounds explain the superior generalization performance of the linear model trained on random vs. non-random data?  It seems to me that the model trained on meaningful data should have a larger margin.
3) The connection between the work on Bayesian evidence, and the work on SGD, felt very informal. The link seems to be purely intuitive (SGD should converge to minima with high evidence, because its updates are noisy).  Can this be formalized?  There is a footnote on page 7 regarding Bayesian posterior sampling -- I think this should be brought into the body of the paper, and explained in more detail.
4) The paper does not give any background on stochastic differential equations, and why there should be an optimal noise scale 'g', which remains constant during the stochastic process, for converging to a minima with high evidene.  Are there any theoretical results which can be leveraged from the stochastic processes literature? For example, are there results which prove anything regarding the convergence of a stochastic process under different amounts of noise?
5) It was unclear to me why momentum was used in the MNIST experiments.  This seems to complicate the experimental setting.  Does the generalization gap not appear when no momentum is used?  Also, why is the same learning rate used for both small and large batch training for Figures 3 and 4?  If the learning rate were optimized together with batch size (eg, keeping aN/B constant), would the generalization gap still appear?  Figure 5a seems to suggest that it would not appear (peaks appear to all have the same test accuracy).
6) It was unclear to me whether the analysis of SGD as a stochastic differential equation with noise scale aN/((1-m)B) was a contribution of this paper.  It would be good if it were made clearer which part of the mathematical analysis in sections 2 and 5 are original.
7) Some small feedback: The notation $< x_i > = 0$ and $< x_i^2 > = 1$ is not explained.  Is each feature being normalized to be zero mean, unit variance, or is each training example being normalized?

Originality: The works seems to be relatively original combination of ideas from Bayesian evidence, to deep neural network research.  However, I am not familiar enough with the literature on Bayesian evidence, or the literature on sharp/broad minima, and their generalization properties, to be able to confidently say how original this work is.

Significance: I believe that this work is quite significant in two different ways:
1) ""Bayesian evidence"" provides a nice way of understanding why neural nets might generalize well, which could lead to further theoretical contributions.
2) The scaling rules described in section 5 could help practitioners use much larger batch sizes during training, by simultaneously increasing the learning rate, the training set size, and/or the momentum parameter.  This could help parallelize neural network training considerably.

Some things which could limit the significance of the work:
1) The paper does not provide a way of measuring the (approximate) evidence of a model.  It simply says it is prohibitively expensive to compute for large models.  Can the ""Gaussian approximation"" to the evidence (equation 10) be approximated efficiently for large neural networks?
2) The paper does not prove that SGD converges to models of high evidence, or formally relate the noise scale 'g' to the quality of the converged model, or relate the evidence of the model to its generalization performance.

Overall, I feel the strengths of the paper outweight its weaknesses.  I think that the paper would be made stronger and clearer if the questions I raised above are addressed prior to publication.","[7, 3, 7]","[' Good paper, accept', ' Clear rejection', ' Good paper, accept']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer provides a generally positive assessment of the paper, highlighting its quality, originality, and significance. They find the paper interesting and well-written, praising the clarity of the experiments and analysis. While they raise several questions and suggest improvements, these are primarily aimed at enhancing the paper's clarity and addressing potential limitations. The reviewer's tone is constructive and professional throughout, indicating a positive disposition towards the paper.",65.0,80.0
Fix your classifier: the marginal value of training the last weight layer,"['Elad Hoffer', 'Itay Hubara', 'Daniel Soudry']",Accept,2018,"[4, 5, 9]","[8, 9, 14]","[35, 26, 90]","[14, 11, 35]","[19, 14, 42]","[2, 1, 13]","This paper proposes replacing the weights of the final classifier layer in a CNN with a fixed projection matrix.  In particular a Hadamard matrix can be used, which can be represented implicitly.

I'd have liked to see some discussion of how to efficiently implement the Hadamard transform when the number of penultimate features does not match the number of classes, since the provided code does not do this.

How does this approach scale as the number of classes grows very large (as it would in language modeling, for example)?

An interesting experiment to do here would be to look this technique interacts with distillation, when used in the teacher or student network or both.   Does fixing the features make it more difficult to place dog than on boat when classifying a cat?  Do networks with fixed classifier weights make worse teachers for distillation?
","[6, 6, 6]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[3, 5, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is mostly neutral leaning slightly positive. The reviewer acknowledges the premise of the paper and asks relevant questions to further the research. While the reviewer points out a potential shortcoming (handling mismatched feature/class numbers), the language remains constructive and inquisitive rather than critical. There's no harsh language, personal attacks, or dismissive phrasing.",20.0,70.0
Compositional Obverter Communication Learning from Raw Visual Input,"['Edward Choi', 'Angeliki Lazaridou', 'Nando de Freitas']",Accept,2018,"[13, 8, 19]","[18, 12, 24]","[102, 64, 199]","[41, 32, 101]","[56, 30, 84]","[5, 2, 14]","This paper proposes to apply the obverter technique of Batali (1998) to a multi-agent communication game. The main novelty with respect to Batali's orginal work is that the agents in this paper have to communicate about images represented at the raw pixel level. The paper presents an extensive analysis of the patterns learnt by the agents, in particular in terms of how compositional they are.

I greatly enjoyed reading the paper: while the obverter idea is not new, it's nice to see it applied in the context of modern deep learning architectures, and the analysis of the results is very interesting.

The writeup is somewhat confusing, and in particular the reader has to constantly refer to the supplementary materials to make sense of the models and experiments. At least the model architecture could be discussed in more detail in the main text.

It's a pity that there is no direct quantitative comparison between the obverter technique and a RL architecture.

Some more detailed comments:

It would be good if a native speaker could edit the paper for language and style (I only annotated English problems in the abstract, since there were just too many of them).

Abstract:

distinguishing natures -> aspects

describe the complex environment -> describe complex environments

with the accuracy -> with accuracy

Introduction:

have shown great progress -> has...

The claim that language learning in humans is entirely based on communication needs hedging. See, e.g., cultures where children must acquire quite a bit of their language skills from passive listening to adult conversations (Schieffelin & Ochs 1983; Shore 1997).

Also, while it's certainly the case that humans do not learn from the neatly hand-crafted features favored in early language emergence studies, their input is most definitely not pixels (and, arguably, it is something more abstract than raw visual stimuli, see, e.g., the work on ""object biases"" in language acquisition).

Method:

The description of the experiment sometimes refers to the listener having to determine whether it is seeing the same image as the speaker, whereas the game consists in the listener telling whether it sees the same object as the speaker. This is important, since, crucially, the images can present the same object in different locations.

Section 2.2 is very confusing, since the obverter technique has not been introduced, yet, so I kept wondering why you were only describing the listener architecture. Perhaps, current section 2.3 should be moved before 2.2.

Also in 2.2: what is the ""meaning vector"" that BAtali's RNN hidden vector has to be close to?

It's confusing to refer to the binary 0 and 1 vectors as outputs of the sigmoid: they are rather the binary categories you want the agents to approximate with the sigmoid function.

The obverter technique should be described in more detail in the main text. In particular, with max message length 20 and a vocabulary of 5 symbols, you'll have an astronomical number of possible inputs to evaluate at each step: is this really what you're doing?

Experiments:

distinctness: better call it distinctiveness

Is training accuracy in Fig 4 averaged across both agents? And what were the values the Jaccard similarity was computed over?

It's nice that the agents discovered some non-strictly agglutinative morphology, consisting in removing symbols from the prefix. Also, it looks like they are developing some notion of ""inflectional class"" (Aronoff 1994) (the color groups), which is also intriguing. However, I did not understand rules such as: ""remove two as and add one a""... isn't that the same as: ""add one a""?

Discussion and future work:

I didn't follow the discussion about partial truth and using negations.

There is another ICLR submission that proposes a quantitative measure of compositionality you might find interesting: https://openreview.net/pdf?id=HJGv1Z-AW
","[9, 6, 3]","[' Top 15% of accepted papers, strong accept', ' Marginally above acceptance threshold', ' Clear rejection']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer explicitly states enjoying the paper and finding the analysis interesting, which points towards a positive sentiment. They do however mention several points of improvement, but these are provided in a constructive and helpful manner. Overall, the tone is formal, professional, and polite with no use of harsh language.",60.0,80.0
Temporally Efficient Deep Learning with Spikes,"[""Peter O'Connor"", 'Efstratios Gavves', 'Matthias Reisser', 'Max Welling']",Accept,2018,"[22, 9, 1, 19]","[23, 14, 6, 24]","[32, 137, 9, 390]","[20, 64, 3, 190]","[5, 62, 6, 166]","[7, 11, 0, 34]","This paper presents a novel method for spike based learning that aims at reducing the needed computation during learning and testing when classifying temporal redundant data. This approach extends the method presented on Arxiv on Sigma delta quantized networks (Peter O’Connor and Max Welling. Sigma delta quantized networks. arXiv preprint arXiv:1611.02024, 2016b.). Overall, the paper is interesting and promising; only a few works tackle the problem of learning with spikes showing the potential advantages of such form of computing. The paper, however, is not flawless. The authors demonstrate the method on just two datasets, and effectively they show results of training only for Feed-Forward Neural Nets (the authors claim that “the entire spiking network end-to-end works” referring to their pre-trained VGG19, but this paper presents only training for the three top layers). Furthermore, even if suitable datasets are not available, the authors could have chosen to train different architectures. The first dataset is the well-known benchmark MNIST also presented in a customized Temporal-MNIST. Although it is a common base-line, some choices are not clear: why using a FFNN instead that a CNN which performs better on this dataset; how data is presented in terms of temporal series – this applies to the Temporal MNIST too; why performances for Temporal MNIST – which should be a more suitable dataset — are worse than for the standard MNIST; what is the meaning of the right column of Figure 5 since it’s just a linear combination of the GOps results. For the second dataset, some points are not clear too: why the labels and the pictures seem not to match (in appendix E); why there are more training iterations with spikes w.r.t. the not-spiking case. Overall, the paper is mathematically sound, except for the “future updates” meaning which probably deserves a clearer explanation. Moreover, I don’t see why the learning rule equations (14-15) are described in the appendix, while they are referred constantly in the main text. The final impression is that the problem of the dynamical range of the hidden layer activations is not fully resolved by the empirical solution described in Appendix D: perhaps this problem affects CCNs more than FFN. 
Finally, there are some minor issues here and there (the authors show quite some lack of attention for just 7 pages):
-	Two times “get” in “we get get a decoding scheme” in the introduction;
-	Two times “update” in “our true update update as” in Sec. 2.6;
-	Pag3 correct the capital S in 2.3.1
-	Pag4 Figure 1 increase font size (also for Figure2); close bracket after Equation 3; N (number of spikes) is not defined
-	Pag5 “one-hot” or “onehot”; 
-	in the inline equation the sum goes from n=1 to S, while in eq.(8) it goes from n=1 to N;
-	Eq(10)(11)(12) and some lines have a typo (a \cdot) just before some of the ws;
-	Pag6 k_{beta} is not defined in the main text;
-	Pag7 there are two “so that” in 3.1; capital letter “It used 32x10^12..”; beside, here, why do not report the difference in computation w.r.t. not-spiking nets?
-	Pag7 in 3.2 “discussed in 1” is section 1?
-	Pag14 Appendix E, why the labels don’t match the pictures;
-	Pag14 Appendix F, explain better the architecture used for this experiment.","[6, 8, 7]","[' Marginally above acceptance threshold', ' Top 50% of accepted papers, clear accept', ' Good paper, accept']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review starts with positive remarks, highlighting the paper's novelty and promising approach. However, it quickly delves into numerous criticisms, pointing out limitations in experimental methodology, dataset choices, and clarity of explanations. The reviewer also lists a significant number of minor issues, indicating a lack of polish in the paper. While the reviewer acknowledges the paper's mathematical soundness, the overall tone and the extensive list of concerns suggest a critical stance. Therefore, the sentiment leans towards the negative side, though not extremely so. The language used is direct and critical, but it maintains a professional and constructive tone, avoiding personal attacks or disrespectful language.",-30.0,60.0
Countering Adversarial Images using Input Transformations,"['Chuan Guo', 'Mayank Rana', 'Moustapha Cisse', 'Laurens van der Maaten']",Accept,2018,"[5, 7, 9, 14]","[10, 7, 13, 19]","[78, 3, 35, 118]","[27, 2, 18, 57]","[42, 1, 15, 48]","[9, 0, 2, 13]","Summary: This works proposes strategies to make neural networks less sensitive to adversarial attacks. They consist into applying different transformations to the images, such as quantization, JPEG compression, total variation minimization and image quilting. Four adversarial attacks strategies are considered to attack a Resnet50 model for classification of Imagenet images.
Experiments are conducted in a black box setting (when the model to attack is unknown by the adversary) or white box setting (the model and defense strategy are known by the adversary).
60% of attacks are countered in this last most difficult setting.
The previous best approach for this task consists in ensemble training and is attack specific. It is therefore pretty robust to the attack it was trained on but is largely outperformed by the authors methods that manage to reduce the classifier error drop below 25%.  

Comments: The paper is well written, the proposed methods are well adapted to the task and lead to satisfying results.
 
The discussion remarks are particularly interesting: the non differentiability of the total variation and image quilting methods seems to be the key to their best performance in practice.
Minor: the bibliography should be uniformed.","[8, 4, 7]","[' Top 50% of accepted papers, clear accept', ' Ok but not good enough - rejection', ' Good paper, accept']","[4, 3, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review is positive about the paper. It states that ""The paper is well written, the proposed methods are well adapted to the task and lead to satisfying results."" and highlights the interesting aspects of the discussion. The only negative point is qualified as ""Minor:"" and refers to a formatting issue, which doesn't impact the overall positive tone. The language used is formal and polite, without any negative or rude phrasing.",85.0,90.0
Fraternal Dropout,"['Konrad Zolna', 'Devansh Arpit', 'Dendi Suhubdy', 'Yoshua Bengio']",Accept,2018,"[3, 8, 2, 31]","[7, 13, 2, 36]","[35, 45, 6, 975]","[14, 20, 2, 405]","[19, 25, 4, 454]","[2, 0, 0, 116]","The paper proposes “fraternal dropout”, which passes the same input twice through a model with different dropout masks. The L2 norm of the differences is then used as an additional regulariser. As the authors note, this implicitly minimises the variance of the model under the dropout mask.

The method is well presented and adequately placed within the related work. The text is well written and easy to follow.

I have only two concerns. The first is that the method is rather incremental and I am uncertain how it will stand the test of time and will be adopted.

The second is that of the experimental evaluation. They authors write that a full hyper parameter search was not conducted in the fear of having a more thorough evaluation than the base lines, erroneously reporting superior results.

To me, this is not an acceptable answer. IMHO, the evaluation should be thorough for both the base lines and the proposed method. If authors can get away with a sub standard evaluation because the competing method did, the field might converge to sub standard evaluations overall. This is clearly not in anyones interest. I am open to the author's comments on this, as I understand that spending weeks on tuning a competing method is also not unbiased and work that could be avoided if all software was published.
","[5, 5, 6]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Marginally above acceptance threshold']","[3, 4, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer provides a mixed opinion. They acknowledge the merits of the paper, such as clear presentation and good positioning within existing literature (positive cues). However, they express concerns about the method's novelty and the evaluation's thoroughness, suggesting a potential for bias (negative cues). The language, while direct, maintains a professional and respectful tone.",20.0,60.0
MGAN: Training Generative Adversarial Nets with Multiple Generators,"['Quan Hoang', 'Tu Dinh Nguyen', 'Trung Le', 'Dinh Phung']",Accept,2018,"[2, 6, 9, 17]","[4, 10, 14, 22]","[6, 61, 145, 393]","[4, 36, 86, 228]","[2, 18, 47, 105]","[0, 7, 12, 60]","Summary:

The paper proposes a mixture of  generators to train GANs. The generators used have tied weights except the first layer that maps the random codes is generator specific, hence no extra computational cost is added.


Quality/clarity:

The paper is well written and easy to follow.

clarity: The appendix states how the weight tying is done , not the main paper, which might confuse the reader, would be better to state this weight tying that keeps the first layer free in the main text.

Originality:

 Using multiple generators for GAN training has been proposed in many previous work that are cited in the paper, the difference in this paper is in weight tying between generators of the mixture, the first layer is kept free for each generator.

General review:

- when only the first layer is free between generators, I think it is not suitable to talk about multiple generators, but rather it is just a multimodal prior on the z, in this case z is a mixture of Gaussians with learned covariances (the weights of the first layer). This angle should be stressed in the paper, it is in fine, *one generator* with a multimodal learned prior on z!

- Taking the multimodal z further , can you try adding a mean to be learned, together with the covariances also? see if this also helps?  
 
- in the tied weight case, in the synthetic example, can you show what each ""generator"" of the mixture learn? are they really learning modes of the data? 

- the theory is for general untied generators, can you comment on the tied case? I don't think the theory is any more valid, for this case, because again your implementation is one generator with a multimodal z prior.  would be good to have some experiments and  see how much we loose for example in term of inception scores, between tied and untied weights of generators.
","[7, 5, 6]","[' Good paper, accept', ' Marginally below acceptance threshold', ' Marginally above acceptance threshold']","[5, 4, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review is generally positive, acknowledging the paper's clarity and the novelty of the weight-tying approach. However, the reviewer raises significant concerns and suggestions, essentially arguing that the proposed method is more akin to a single generator with a multimodal prior rather than multiple generators. The reviewer's tone is direct and critical, pushing the authors to reconsider their approach, but remains professional and focused on improving the paper. There are no personal attacks or disrespectful language.",40.0,60.0
Generative Models of Visually Grounded Imagination,"['Ramakrishna Vedantam', 'Ian Fischer', 'Jonathan Huang', 'Kevin Murphy']",Accept,2018,"[5, 13, 27, 24]","[10, 17, 32, 29]","[31, 45, 94, 164]","[14, 19, 53, 91]","[15, 23, 33, 57]","[2, 3, 8, 16]","The authors propose a generative method that can produce images along a hierarchy of specificity, i.e. both when all relevant attributes are specified, and when some are left undefined, creating a more abstract generation task. 

Pros:
+ The results demonstrating the method's ability to generate results for (1) abstract and (2) novel/unseen attribute descriptions, are generally convincing. Both quantitative and qualitative results are provided. 
+ The paper is fairly clear.

Cons:
- It is unclear how to judge diversity qualitatively, e.g. in Fig. 4(b).
- Fig. 5 could be more convincing; ""bushy eyebrows"" is a difficult attribute to judge, and in the abstract generation when that is the only attribute specified, it is not clear how good the results are.
","[7, 7, 7]","[' Good paper, accept', ' Good paper, accept', ' Good paper, accept']","[3, 4, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review acknowledges the merits of the paper, highlighting its convincing results and clarity. However, it also points out specific areas for improvement, suggesting that certain aspects lack clarity and could be more convincing. Overall, the tone is constructive and helpful, aiming to guide the authors towards improving their work. Therefore, the sentiment leans slightly towards the positive side, while the politeness remains neutral and professional.",60.0,0.0
Multi-View Data Generation Without View Supervision,"['Mickael Chen', 'Ludovic Denoyer', 'Thierry Artières']",Accept,2018,"[3, 16, 26]","[8, 20, 30]","[18, 147, 95]","[8, 88, 72]","[9, 40, 9]","[1, 19, 14]","This paper firstly proposes a GAN architecture that aim at decomposing the underlying distribution of a particular class into ""content"" and ""view"". The content can be seen as an intrinsic instantiation of the class that is independent of certain types of variation (eg viewpoint), and a view is the observation of the object under a particular variation. The authors additionally propose a second conditional GAN that learns to generate different views given a specific content. 

I find the idea of separating content and view interesting and I like the GMV and CGMV architectures. Not relying on manual attribute/class annotation for the views is also positive. The approach seems to work well for a relatively clean setup such as the chair dataset, but for the other datasets the separation is not so apparent. For example, in figure 5, what does each column represent in terms of view? It seems that it depends heavily on the content. That raises the question of how useful it is to have such a separation between content and views; for some datasets their diversity can be a bottleneck for this partition, making the interpretation of views difficult. 

A missing (supervised) reference that considers also the separation of content and views.
[A] Learning to generate chairs with convolutional neural networks, Alexey Dosovitskiy, Jost Tobias Springenberg, Thomas Brox, CVPR 15

Q:Figure 5, you mean ""all images in a column were generated with the same view vector""
Q: Why on Figure 7 you use different examples for CGAN?","[5, 7, 7]","[' Marginally below acceptance threshold', ' Good paper, accept', ' Good paper, accept']","[4, 5, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer provides a generally positive overview of the paper, highlighting the interesting aspects of the proposed method. While they point out limitations and raise questions, these are presented constructively and with the aim of improving the paper. The language used is formal and objective, typical of scientific peer reviews.",60.0,80.0
Multi-level Residual Networks from Dynamical Systems View,"['Bo Chang', 'Lili Meng', 'Eldad Haber', 'Frederick Tung', 'David Begert']",Accept,2018,"[13, 9, 21, 10, 2]","[18, 14, 26, 15, 2]","[78, 81, 100, 47, 4]","[44, 23, 22, 21, 2]","[14, 18, 43, 18, 2]","[20, 40, 35, 8, 0]","This paper interprets deep residual network as a dynamic system, and proposes a novel training algorithm to train it in a constructive way. On three image classification datasets, the proposed algorithm speeds up the training process without sacrificing accuracy. The paper is interesting and easy to follow. 

I have several comments:
1.	It would be interesting to see a comparison with Stochastic Depth, which is also able to speed up the training process, and gives better generalization performance. Moreover, is it possible to combine the proposed method with Stochastic Depth to obtain further improved efficiency?
2.	The mollifying networks [1] is related to the proposed method as it also starts with shorter networks, and ends with deeper models. It would be interesting to see a comparison or discussion. 
[1] C Gulcehre, Mollifying Networks, 2016
3.	Could you show the curves (on Figure 6 or another plot) for training a short ResNet (same depth as your starting model) and a deep ResNet (same depth as your final model) without using your approach?","[7, 7, 7]","[' Good paper, accept', ' Good paper, accept', ' Good paper, accept']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer explicitly states that they find the paper ""interesting and easy to follow."" This indicates a positive sentiment. They offer constructive suggestions for improvement rather than harsh criticisms, suggesting a polite tone. The suggestions themselves are framed as potential enhancements or points of interest, further reinforcing the positive and polite tone.",60.0,80.0
Deep Learning for Physical Processes: Incorporating Prior Scientific Knowledge,"['Emmanuel de Bezenac', 'Arthur Pajot', 'Patrick Gallinari']",Accept,2018,"[2, 2, 31]","[7, 6, 36]","[26, 7, 382]","[12, 3, 256]","[13, 3, 54]","[1, 1, 72]","The paper ‘Deep learning for Physical Process: incorporating prior physical knowledge’ proposes
to question the use of data-intensive strategies such as deep learning in solving physical 
inverse problems that are traditionally solved through assimilation strategies. They notably show
how physical priors on a given phenomenon can be incorporated in the learning process and propose 
an application on the problem of estimating sea surface temperature directly from a given 
collection of satellite images.

All in all the paper is very clear and interesting. The results obtained on the considered problem
are clearly of great interest, especially when compared to state-of-the-art assimilation strategies
such as the one of Béréziat. While the learning architecture is not original in itself, it is 
shown that a proper physical regularization greatly improves the performance. For these reasons I 
believe the paper has sufficient merits to be published at ICLR. That being said, I believe that 
some discussions could strengthen the paper:
 - Most classical variational assimilation schemes are stochastic in nature, notably by incorporating
uncertainties in the observation or physical evolution models. It is still unclear how those uncertainties 
can be integrated in the model;
 - Assimilation methods are usually independent of the type of data at hand. It is not clear how the model
learnt on one particular type of data transpose to other data sequences. Notably, the question of transfer
and generalization is of high relevance here. Does the learnt model performs well on other dataset (for instance,
acquired on a different region or at a distant time). I believe this type of issue has to be examinated 
for this type of approach to be widely use in inverse physical problems. 
","[7, 6, 7]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Good paper, accept']","[3, 2, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer clearly states that they find the paper interesting and believe it has merit for publication. They highlight the clarity and the interesting results. While they offer constructive criticism and suggestions for improvement, these are presented as opportunities to further strengthen an already strong paper. The language used is professional and respectful throughout.",80.0,90.0
Natural Language Inference over Interaction Space,"['Yichen Gong', 'Heng Luo', 'Jian Zhang']",Accept,2018,"[1, 25]","[6, 30]","[7, 221]","[2, 97]","[0, 8]","[5, 116]","Pros: 
The paper proposes a “Densely Interactive Inference Network (DIIN)” for NLI or NLI alike tasks. Although using tensors to capture high-order interaction and performing dimension reduction over that are both not novel, the paper explores them for NLI. The paper is written clearly and is very easy to follow. The ablation experiments in Table 5 give a good level of details to help observe different components' effectiveness.
Cons:
1) The differences of performances between the proposed model and the previous models are not very clear. With regard to MultiNLI, since the previous results (e.g., those in Table 2) did not use cross-sentence attention and had to represent a premise or a hypothesis as a *fixed-length* vector, is it fair to compare DIIN with them? Note that the proposed DIIN model does represent a premise or a hypothesis by variable lengths (see interaction layer in Figure 1), and tensors provide some sorts of attention between them. Can this (Table 2) really shows the advantage of the proposed models? However, when a variable-length representation is allowed (see Table 3 on SNLI), the advantage of the model is also not observed, with no improvement as a single model (compared with ESIM) and being almost same as previous models (e.g., model 18 in Table 3) in ensembling.
2) Method-wise, as discussed above, using tensors to capture high-order interaction and performing dimension reduction over that are both not novel.
3) The paper mentions the use of untied parameters for premise and hypothesis, but it doesn’t compare it with tied version in the experiment section. 
4) In Table 6, for CONDITIONAL tag, why the baseline models (lower total accuracies) have a 100% accuracy, but DIIN only has about a 60% accuracy?
","[6, 6, 5]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally below acceptance threshold']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review acknowledges the clarity and structure of the paper, highlighting its readability and the detailed ablation experiments. This suggests a somewhat positive sentiment. However, the reviewer raises significant concerns about the novelty of the proposed method and questions the validity of the results, particularly regarding the comparison with previous models. The use of phrases like ""not very clear"" and ""is it fair to compare"" indicates skepticism and potential flaws in the paper's approach. Overall, the sentiment leans towards the critical side due to these concerns. The language used is professional and avoids harsh or disrespectful language, maintaining a neutral to polite tone.",-10.0,50.0
Learning Sparse Neural Networks through L_0 Regularization,"['Christos Louizos', 'Max Welling', 'Diederik P. Kingma']",Accept,2018,['title not found'],['title not found'],[],[],[],[],"Learning sparse neural networks through L0 regularisation

Summary: 

The authors introduce a gradient-based approach to minimise an objective function with an L0 sparse penalty. The problem is relaxed onto a continuous optimisation by changing an expectation over discrete variables (representing whether a variable is present or not) to an expectation over continuous variables, inspired by earlier work from Maddison et al (ICLR 2017) where a similar transformation was used to learn over discrete variable prediction tasks with neural networks. Here the application is to learn sparse feedforward networks in standard classification tasks, although the framework described is quite general and could be used to impose L0 sparsity to any objective function in principal. The method provides equivalent accuracy and sparsity to published state-of-the-art results on these datasets but it is argue that learning sparsity during the training process will lead to significant speed-ups - this is demonstrated by comparing to a theoretical benchmark (standard training with dropout) rather than through empirical testing against other implementations. 

Pros:

The paper is well written and the derivation of the method is easy to follow with a good explanation of the underlying theory. 

Optimisation under L0 regularisation is a difficult and generally important topic and certainly has advantages over other sparse inference objective functions that impose shrinkage on non-sparse parameters. 

The work is put in context and related to some previous relaxation approaches to sparsity. 

The method allows for sparsity to be learned during training rather than after training (as in standard dropout approaches) and this allows the algorithm to obtain significant per-iteration speed-ups, which improves through training. 

Cons:

The method is applied to standard neural network architectures and performance in terms of accuracy and final achieved sparsity is comparable to the state-of-the-art methods. Therefore the main advance is in terms of learning speed to obtain this similar performance. However, the learning speed-up is presented against a theoretical FLOPs estimate per iteration for a similar network with dropout. It would be useful to know whether the number of iterations to achieve a particular performance is equivalent for all the different architectures considered, e.g. does the proposed sparse learning method converge at the same rate as the others? I felt a more thorough experimental section would have greatly improved the work, focussing on this learning speed aspect. 

It was unclear how much tuning of the lambda hyper-parameter, which tunes the sparsity, would be required in a practical application since tuning this parameter would increase computation time. It might be useful to provide a full Bayesian treatment so that the optimal sparsity can be chosen through hyper-parameter learning. 

Minor point: it wasn’t completely clear to me why the fact (3) is a variational approximation to a spike-and-slab is important (Appendix). I don’t see why the spike-and-slab is any more fundamental than the L0 norm prior in (2), it is just more convenient in Bayesian inference because it is an iid prior and potentially allows an informative prior over each parameter. In the context here this didn’t seem a particularly relevant addition to the paper. 

","[6, 6, 7]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Good paper, accept']","[3, 3, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review acknowledges the merits of the paper, such as its clarity, relevance of the topic, and potential advantages of the proposed method ('well-written', 'easy to follow', 'important topic', 'significant speed-ups'). However, it also raises concerns about the experimental validation and suggests several improvements ('theoretical benchmark', 'more thorough experimental section', 'unclear how much tuning', 'not completely clear'). Overall, the tone is constructive and suggests ways to improve the paper rather than outright rejecting it. ",50.0,80.0
Loss-aware Weight Quantization of Deep Networks,"['Lu Hou', 'James T. Kwok']",Accept,2018,"[4, 26]","[9, 31]","[70, 256]","[31, 136]","[22, 45]","[17, 75]","This paper proposes a new method to train DNNs with quantized weights, by including the quantization as a constraint in a proximal quasi-Newton algorithm, which simultaneously learns a scaling for the quantized values (possibly different for positive and negative weights). 

The paper is very clearly written, and the proposal is very well placed in the context of previous methods for the same purpose. The experiments are very clearly presented and solidly designed.

In fact, the paper is a somewhat simple extension of the method proposed by Hou, Yao, and Kwok (2017), which is where the novelty resides. Consequently, there is not a great degree of novelty in terms of the proposed method, and the results are only slightly better than those of previous methods.

Finally, in terms of analysis of the algorithm, the authors simply invoke a theorem from Hou, Yao, and Kwok (2017), which claims convergence of the proposed algorithm. However, what is shown in that paper is that the sequence of loss function values converges, which does not imply that the sequence of weight estimates also converges, because of the presence of a non-convex constraint ($b_j^t \in Q^{n_l}$). This may not be relevant for the practical results, but to be accurate, it can't be simply stated that the algorithm converges, without a more careful analysis.","[6, 8, 6]","[' Marginally above acceptance threshold', ' Top 50% of accepted papers, clear accept', ' Marginally above acceptance threshold']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with very positive framing, praising the clarity, experimental design, and contextualization of the work. However, it then transitions to point out a lack of significant novelty, describing the method as a ""somewhat simple extension."" The final paragraph raises a technical concern, questioning the accuracy of the convergence claim. While this is a valid criticism, it's presented mildly and acknowledges the practical irrelevance. Overall, the tone is balanced, leaning slightly towards the positive due to the initial praise.",20.0,70.0
"Don't Decay the Learning Rate, Increase the Batch Size","['Samuel L. Smith', 'Pieter-Jan Kindermans', 'Chris Ying', 'Quoc V. Le']",Accept,2018,"[3, 7, 2, 14]","[8, 11, 3, 19]","[30, 39, 9, 299]","[10, 17, 4, 143]","[20, 16, 5, 145]","[0, 6, 0, 11]","The paper analyzes the the effect of increasing the batch size in stochastic gradient descent as an alternative to reducing the learning rate, while keeping the number of training epochs constant. This has the advantage that the training process can be better parallelized, allowing for faster training if hundreds of GPUs are available for a short time. The theory part of the paper briefly reviews the relationship between learning rate, batch size, momentum coefficient, and the noise scale in stochastic gradient descent. In the experimental part, it is shown that the loss function and test accuracy depend only on the schedule of the decaying noise scale over training time, and are independent of whether this decaying noise schedule is achieved by a decaying learning rate or an increasing batch size. It is shown that simultaneously increasing the momentum parameter and the batch size also allows for fewer parameters, albeit at the price of some loss in performance.

COMMENTS:

The paper presents a simple observation that seems very relevant especially as computing resources are becoming increasingly available for rent on short time scales. The observation is explained well and substantiated by clear experimental evidence. The main issue I have is with the part about momentum. The paragraph below Eq. 7 provides a possible explanation for the performance drop when $m$ is increased. It is stated that at the beginning of the training, or after increasing the batch size, the magnitude of parameter updates is suppressed because $A$ has to accumulate gradient signals over a time scale $B/(N(1-m))$. The conclusion in the paper is that training at high momentum requires additional training epochs before $A$ reaches its equilibrium value. This effect is well known, but it can easily be remedied. For example, the update equations in Adam were specifically designed to correct for this effect. The mechanism is called ""bias-corrected moment estimate"" in the Adam paper, arXiv:1412.6980. The correction requires only two extra multiplications per model parameter and update step. Couldn't the same or a very similar trick be used to correctly rescale $A$ every time one increases the batch size? It would be great to see the equivalent of Figure 7 with correctly rescaled $A$.

Minor issues:
* The last paragraph of Section 5 refers to a figure 8, which appears to be missing.
* In Eqs. 4 & 5, the momentum parameter $m$ is not yet defined (it will be defined in Eqs. 6 & 7 below).
* It appears that a minus sign is missing in Eq. 7. The update steps describe gradient ascent.
* Figure 3 suggests that most of the time between the first and second change of the noise scale (approx. epochs 60 to 120) are spent on overfitting. This suggests that the number of updates in this segment was chosen unnecessarily large to begin with. It is therefore not surprising that reducing the number of updates does not deteriorate the test set accuracy.
* It would be interesting to see a version of figure 5 where the horizontal axis is the number of epochs. While reducing the number of updates allows for faster training if a large number of parallel hardware instances are available, the total cost of training is still governed by the number of training epochs.
* It appears like the beginning of the second paragraph in Section 5.2 describes figure 1. Is this correct?","[6, 7, 6]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Marginally above acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is positive overall. The reviewer finds the paper's observation relevant and well-supported by experiments. They provide constructive criticism and suggestions for improvement, particularly regarding the momentum parameter and its effect on training. The reviewer also points out minor issues like missing figures and equation errors, which are presented as areas for the authors to refine the paper. The language used is polite and professional throughout the review.",65.0,100.0
Neumann Optimizer: A Practical Optimization Algorithm for Deep Neural Networks,"['Shankar Krishnan', 'Ying Xiao', 'Rif. A. Saurous']",Accept,2018,"[24, 16, 3]","[29, 21, 8]","[79, 54, 43]","[41, 19, 18]","[11, 12, 24]","[27, 23, 1]","The paper proposes a new algorithm, where they claim to use Hessian implicitly and are using a motivation from power-series. In general, I like the paper.

To me, Algorithm 1 looks like some kind of proximal-point type algorithm. Algorithm 2 is more heuristic approach, with a couple of parameters to tune it.  Given the fact that there is convergence analysis or similar theoretical results, I would expect to have much more numerical experiments. E.g. there is no results of Algorithm 1. I know it serves as a motivation, but it would be nice to see how it works.

Otherwise, the paper is clearly written.
The topic is important, but I am a bit afraid of significance. One thing what I do not understand is, that why they did not compare with Adam? (they mention Adam algorithm soo many times, that it should be compared to).

I am also not sure, how sensitive the results are for different datasets? Algorithm 2 really needs so many parameters (not just learning rate). How \alpha, \beta, \gamma, \mu, \eta, K influence the speed? how sensitive is the algorithm for different choices of those parameters?


 ","[6, 6, 6]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[4, 3, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer starts with a positive statement (""In general, I like the paper""), indicating an overall positive sentiment. However, the review also raises several concerns and asks for significant improvements (more experiments, comparisons, analysis of parameters). Therefore, the sentiment is positive but closer to neutral than very positive. The language used is constructive and professional, suggesting a neutral to polite tone.",40.0,60.0
Learning Approximate Inference Networks for Structured Prediction,"['Lifu Tu', 'Kevin Gimpel']",Accept,2018,"[3, 11]","[8, 16]","[26, 161]","[11, 90]","[14, 66]","[1, 5]","The paper proposes training ``inference networks,'' which are neural network structured predictors. The setup is analogous to generative adversarial networks, where the role of the discriminator is played by a structured prediction energy network (SPEN) and the generator is played by an inference network.

The idea is interesting. It could be viewed as a type of adversarial training for large-margin structured predictors, where counterexamples, i.e., structures with high loss and low energy, cannot be found by direct optimization. However, it remains unclear why SPENs are the right choice for an energy function.

Experiments suggest that it can result in better structured predictors than training models directly via backpropagation gradient descent. However, the experimental results are not clearly presented. The clarity is poor enough that the paper might not be ready for publication.

Comments and questions:

1) It is unclear whether this paper is motivated by training SPENs or by training structured predictors. The setup focuses on using SPENs as an inference network, but this seems inessential. Experiments with simpler energy functions seem to be absent, though the experiments are unclear (see below).

2) The confusion over the motivation is confounded by the fact that the experiments are very unclear. Sometimes predictions are described as the output of SPENs (Tables 2, 3, 4, and 7), sometimes as inference networks (Table 5), and sometimes as a CRF (Tables 4 and 6). In 7.2.2 it says that a BiLSTM is used for the inference network in Twitter POS tagging, but Tables 4 and 6 indicate both CRFs and BiLSTMS? It is also unclear when a model, e.g., BiLSTM or CRF is the energy function (discriminator) or inference network (generator).

3) The third and fourth columns of Table 5 are identical. The presentation should be made consistent, either with dev/test or -retuning/+retuning as the top level headers.

4) It is also unclear how to compare Tables 4 and 5. The second to bottom row of Table 5 seems to correspond with the first row of Table 5, but other methods like slack rescaling have higher performance. What is the takeaway from these two tables supposed to be?

5) Part of the motivation for the work is said to be the increasing interest in inference networks: ""In these and related settings, gradient descent has started to be replaced by inference networks. Our results below provide more evidence for making this transition."" However, no other work on inference networks is directly cited.","[5, 9, 7]","[' Marginally below acceptance threshold', ' Top 15% of accepted papers, strong accept', ' Good paper, accept']","[3, 4, 5]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review acknowledges the paper's interesting idea but expresses significant concerns about clarity and experimental presentation. The reviewer finds the motivation unclear and criticizes the inconsistent descriptions of experiments and results. The repeated use of phrases like ""it is unclear"" and ""the clarity is poor"" indicates a negative sentiment. However, the language remains professional and avoids harsh or disrespectful tones, suggesting a neutral politeness level.",-30.0,0.0
When is a Convolutional Filter Easy to Learn?,"['Simon S. Du', 'Jason D. Lee', 'Yuandong Tian']",Accept,2018,"[3, 12, 13]","[8, 17, 18]","[178, 192, 150]","[74, 77, 69]","[100, 106, 73]","[4, 9, 8]","This paper studies the problem of learning a single convolutional filter using SGD. The main result is: if the ""patches"" of the convolution are sufficiently aligned with each other, then SGD with a random initialization can recover the ground-truth parameter of a convolutional filter (single filter, ReLU, average pooling). The convergence rate, and how ""sufficiently aligned"" depend on some quantities related to the underlying data distribution. A major strength of the result is that it can work for general continuous distributions and does not really rely on the input distribution being Gaussian; the main weakness is that some of the distribution dependent quantities are not very intuitive, and the alignment requirement might be very high.

Detailed comments:
1. It would be good to clarify what the angle requirement means on page 2. It says the angle between Z_i, Z_j is at most \rho, is this for any i,j? From the later part it seems that each Z_i should be \rho close to the average, which would imply pairwise closeness (with some constant factor).
2. The paper first proves result for a single neuron, which is a clean result. It would be interesting to see what are values of \gamma(\phi) and L(\phi) for some distributions (e.g. Gaussian, uniform in hypercube, etc.) to give more intuitions. 
3. The convergence rate depends on \gamma(\phi_0), from the initialization, \phi_0 is probably very close to \pi/2 (the closeness depend on dimension), which is  also likely to make \gamma(\phi_0) depend on dimension (this is especially true of Gaussian). 
4. More precisely, \gamma(\phi_0) needs to be at least 6L_{cross} for the result to work, and L_{cross} seems to be a problem dependent constant that is not related to the dimension of the data. Also \gamma(\phi_0) depends on \gamma_{avg}(\phi_0) and \rho, when \rho is reasonable (say a constant), \gamma(\phi_0) really needs to be a constant that is independent of dimension. On the other hand, in Theorem 3.4 we can see that the upperbound on \alpha (the quality of initialization) depends on the dimension. 
5. Even assuming \rho is a constant strictly smaller than \pi/2 seems a bit strong. It is certainly plausible that nearby patches are highly correlated, but what is required here is that all patches are close to the average. Given an image it is probably not too hard to find an almost all white patch and an almost all dark patch so that they cannot both be within a good angle to the average. 

Overall I feel the result is interesting but hard to interpret correctly. The details of the theorem do not really support the high level claims very strongly. The paper would be much better if it goes over several example distributions and show explicitly what are the guarantees. The reviewer tried to do that for Gaussian and as I mentioned above (esp. 4) the result does not seem very impressive, maybe there are other distributions where this result works better?

After reading the response, I feel the contribution for the single neuron case does not require too much assumptions and is itself a reasonable result. I am still not convinced by the convolution case (which is the main point of this paper), as even though it does not require Gaussian input (a major plus), it still seems very far from ""general distribution"". Overall this is a first step in an interesting direction, so even though it is currently a bit weak I think it is OK to be accepted. I hope the revised version will clearly discuss the limitations of the approach and potential future directions as the response did.","[6, 8, 9]","[' Marginally above acceptance threshold', ' Top 50% of accepted papers, clear accept', ' Top 15% of accepted papers, strong accept']","[3, 3, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer acknowledges the paper's interesting aspects and finds the single neuron case well-established. However, they express concerns about the convolution case, finding the assumptions strong and the overall result somewhat weak. Despite this, they recommend acceptance based on the paper's novelty and potential, appreciating the response's discussion of limitations and future work.",40.0,70.0
Self-ensembling for visual domain adaptation,"['Geoff French', 'Michal Mackiewicz', 'Mark Fisher']",Accept,2018,"[2, 14, 21]","[7, 19, 26]","[16, 28, 30]","[6, 15, 18]","[7, 8, 1]","[3, 5, 11]","This paper presents a domain adaptation algorithm based on the self-ensembling method proposed by [Tarvainen & Valpola, 2017]. The main idea is to enforce the agreement between the predictions of the teacher and the student classifiers on the target domain samples while training the student to perform well on the source domain. The teacher network is simply an exponential moving average of different versions of the student network over time.   

Pros:
+ The paper is well-written and easy to read
+ The proposed method is a natural extension of the mean teacher semi-supervised learning model by [Tarvainen & Valpola, 2017]
+ The model achieves state-of-the-art results on a range of visual domain adaptation benchmarks (including top performance in the VisDA17 challenge)

Cons:
- The model is tailored to the image domain as it makes heavy use of the data augmentation. That restricts its applicability quite significantly. I’m also very interested to know how the proposed method works when no augmentation is employed (for fair comparison with some of the entries in Table 1).
- I’m not particularly fond of the engineering tricks like confidence thresholding and the class balance loss. They seem to be essential for good performance and thus, in my opinion, reduce the value of the main idea.
- Related to the previous point, the final VisDA17 model seems to be engineered too heavily to work well on a particular dataset. I’m not sure if it provides many interesting insights for the scientific community at large.

In my opinion, it’s a borderline paper. While the best reported quantitative results are quite good, it seems that achieving those requires a significant engineering effort beyond just applying the self-ensembling idea. 

Notes:
* The paper somewhat breaks the anonymity of the authors by mentioning the “winning entry in the VISDA-2017”. Maybe it’s not a big issue but in my opinion it’s better to remove references to the competition entry.
* Page 2, 2.1, line 2, typo: “stanrdard” -> “standard”

Post-rebuttal revision:
After reading the authors' response to my review, I decided to increase the score by 2 points. I appreciate the improvements that were made to the paper but still feel that this work a bit too engineering-heavy, and the title does not fully reflect what's going on in the full pipeline.","[7, 7, 7]","[' Good paper, accept', ' Good paper, accept', ' Good paper, accept']","[4, 5, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer acknowledges the paper's strengths, such as clear writing, a novel extension of an existing method, and state-of-the-art results. However, they also point out significant drawbacks, mainly focusing on the heavy engineering required to achieve those results and limited applicability beyond image domains. While initially considering it borderline, the reviewer increased the score slightly after the authors' rebuttal, appreciating the improvements but maintaining some reservations about the engineering focus. The language is generally polite and professional, presenting criticism constructively.",20.0,70.0
Automatically Inferring Data Quality for Spatiotemporal Forecasting,"['Sungyong Seo', 'Arash Mohegh', 'George Ban-Weiss', 'Yan Liu']",Accept,2018,"[3, 1, 1, 17]","[7, 1, 1, 22]","[22, 1, 1, 219]","[13, 1, 1, 132]","[9, 0, 0, 57]","[0, 0, 0, 30]","Update:

I have read the rebuttal and the revised manuscript. Paper reads better and comparison to Auto-regression was added. This work presents a novel way of utilizing GCN and I believe it would be interesting to the community. In this regard, I have updated my rating.

On the downside, I still remain uncertain about the practical impact of this work. Results in Table 1 show that proposed method is capable of forecasting next hour temperature with about 0.45C mean absolute error. As no reference to any state of the art temperature forecasting method is given (i.e. what is the MAE of a weather app on a modern smartphone), I can not judge whether 0.45C is good or bad. Additionally, it would be interesting to see how well proposed method can deal with next day temperature forecasting.

---------------------------------------------
In this paper authors develop a notion of data quality as the function of local variation of the graph nodes. The concept of local variation only utilizes the signals of the neighboring vertices and GCN is used to take into account broader neighborhoods of the nodes. Data quality then used to weight the loss terms for training of the LSTM network to forecast temperatures at weather stations.

I liked the idea of using local variations of the graph signals as quality of the signal. It was new to me, but I am not very familiar with some of the related literature. I have one methodological and few experimental questions.

Methodology:
Why did you decide to use GCN to capture the higher order neighborhoods? GCN does so intuitively, but it is not clear what exactly is happening due to non-linearities. What if you use graph polynomial filter instead [1] (i.e. linear combination of powers of the adjacency)? It can more evidently capture the K-hop neighborhood of a vertex.

Experiments:
- Could you please formalize the forecasting problem more rigorously. It is not easy to follow what information is used for training and testing. I'm not quite certain what ""Temperature is used as a target measurement, i.e., output of LSTM, and others including Temperature are used as input signals."" means. I would expect that forecasting of temperature tomorrow is solely performed based on today's and past information about temperature and other measurements.
- What are the measurement units in Table 1?
- I would like to see comparison to some classical time series forecasting techniques, e.g. Gaussian Process regression and Auto-regressive models. Also some references and comparisons are needed to state-of-the-art weather forecasting techniques. These comparisons are crucial to see if the method is indeed practical.

Please consider proofreading the draft. There are occasional typos and excessively long wordings.

[1] Aliaksei Sandryhaila and José MF Moura. Discrete signal processing on graphs. IEEE transactions
on signal processing, 61(7):1644–1656, 2013.","[6, 6, 8]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Top 50% of accepted papers, clear accept']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with a positive note, acknowledging the novelty of the approach and improvement after revision. However, it expresses significant concerns about the practical impact and lack of comparison with state-of-the-art methods. The reviewer also suggests further experiments and points out clarity issues. Overall, the sentiment leans towards the positive side due to the novelty and acceptance after revision, but the numerous concerns hold it back from being overwhelmingly positive.",60.0,70.0
Learning Differentially Private Recurrent Language Models,"['H. Brendan McMahan', 'Daniel Ramage', 'Kunal Talwar', 'Li Zhang']",Accept,2018,"[16, 17, 18, 22]","[21, 21, 23, 27]","[98, 42, 182, 102]","[41, 23, 98, 52]","[52, 16, 67, 29]","[5, 3, 17, 21]","This paper extends the previous results on differentially private SGD to user-level differentially private recurrent language models. It experimentally shows that the proposed differentially private LSTM achieves comparable utility compared to the non-private model.

The idea of training differentially private neural network is interesting and very important to the machine learning + differential privacy community. This work makes a pretty significant contribution to such topic. It adapts techniques from some previous work to address the difficulties in training language model and providing user-level privacy. The experiment shows good privacy and utility.

The presentation of the paper can be improved a bit. For example, it might be better to have a preliminary section before Section2 introducing the original differentially private SGD algorithm with clipping, the original FedAvg and FedSGD, and moments accountant as well as privacy amplification; otherwise, it can be pretty difficult for readers who are not familiar with those concepts to fully understand the paper. Such introduction can also help readers understand the difficulty of adapting the original algorithms and appreciate the contributions of this work.
","[8, 7, 7]","[' Top 50% of accepted papers, clear accept', ' Good paper, accept', ' Good paper, accept']","[4, 2, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with positive statements, highlighting the significance of the paper's contribution and praising its experimental results. While it suggests improvements in presentation and provides specific advice, the tone remains constructive and encouraging. The reviewer acknowledges the complexity of the topic and aims to help the authors make the paper accessible to a wider audience. There are no negative remarks, only suggestions for improvement.",80.0,90.0
Auto-Encoding Sequential Monte Carlo,"['Tuan Anh Le', 'Maximilian Igl', 'Tom Rainforth', 'Tom Jin', 'Frank Wood']",Accept,2018,"[4, 2, 4, 1, 23]","[8, 6, 9, 1, 28]","[28, 29, 83, 1, 153]","[13, 15, 38, 1, 71]","[15, 13, 44, 0, 71]","[0, 1, 1, 0, 11]","[After author feedback]
I think the approach is interesting and warrants publication. However, I think some of the counter-intuitive claims on the proposal learning are overly strong, and not supported well enough by the experiments. In the paper the authors also need to describe the differences between their work and the concurrent work of Maddison et al. and Naesseth et al. 

[Original review]
The authors propose auto-encoding sequential Monte Carlo (SMC), extending the VAE framework to a new Monte Carlo objective based on SMC. The authors show that this can be interpreted as standard variational inference on an extended space, and that the true posterior can only be obtained if we can target the true posterior marginals at each step of the SMC procedure. The authors argue that using different number of particles for learning the proposal parameters versus the model parameters can be beneficial.

The approach is interesting and the paper is well-written, however, I have some comments and questions:

- It seems clear that the AESMC bound does not in general optimize for q(x|y) to be close to p(x|y), except in the IWAE special case. This seems to mean that we should not expect for q -> p when K increases?
- Figure 1 seems inconclusive and it is a bit difficult to ascertain the claim that is made. If I'm not mistaken K=1 is regular ELBO and not IWAE/AESMC? Have you estimated the probability for positive vs. negative gradient values for  K=10? To me it looks like the probability of it being larger than zero is something like 2/3. K>10 is difficult to see from this plot alone.
- Is there a typo in the bound given by eq. (17)? Seems like there are two identical terms. Also I'm not sure about the first equality in this equatiion, is I^2 = 0 or is there a typo?
- The discussion in section 4.1 and results in the experimental section 5.2 seem a bit counter-intuitive, especially learning the proposals for SMC using IS. Have you tried this for high-dimensional models as well? Because IS suffers from collapse even in the time dimension I would expect the optimal proposal parameters learnt from a IWAE-type objective will collapse to something close to the the standard ELBO. For example have you tried learning proposals for the LG-SSM in Section 5.1 using the IS objective as proposed in 4.1? Might this be a typo in 4.1? You still propose to learn the proposal parameters using SMC but with lower number of particles? I suspect this lower number of particles might be model-dependent.

Minor comments:
- Section 1, first paragraph, last sentence, ""that"" -> ""than""?
- Section 3.2, ""... using which..."" formulation in two places in the firsth and second paragraph was a bit confusing
- Page 7, second line, just ""IS""?
- Perhaps you can clarify the last sentence in the second paragraph of Section 5.1 about computational graph not influencing gradient updates?
- Section 5.2, stochastic variational inference Hoffman et al. (2013) uses natural gradients and exact variational solution for local latents so I don't think K=1 reduces to this?","[7, 3, 7]","[' Good paper, accept', ' Clear rejection', ' Good paper, accept']","[4, 2, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is fairly confident that the evaluation is correct']","The review is mostly positive. The reviewer finds the approach interesting and the paper well-written. They find merit in the paper and believe it warrants publication. However, they also raise several questions and concerns, indicating that they are not fully convinced by some of the claims and results. The reviewer also points out a few minor issues but these are presented constructively.",60.0,80.0
TRUNCATED HORIZON POLICY SEARCH: COMBINING REINFORCEMENT LEARNING & IMITATION LEARNING,"['Wen Sun', 'J. Andrew Bagnell', 'Byron Boots']",Accept,2018,"[6, 18, 14]","[11, 23, 19]","[76, 195, 221]","[35, 117, 112]","[39, 58, 98]","[2, 20, 11]","This work proposes to use the value function V^e of some expert policy \pi^e in order to speed up learning of an RL agent which should eventually do better than the expert. The emphasis is put on using k-steps (with k>1) Bellman updates using bootstrapping from V^e. 

It is claimed that the case k=1 does not allow the agent to outperform the expert policy, whereas k>1 does (Section 3.1, paragraph before Lemma 3.2).

I disagree with this claim. Indeed a policy gradient algorithm (similar to (10)) with a 1-step advantage c(s,a) + gamma V^e(s_{t+1}) - V^e(s_t) will converge (say in the tabular case, or in the case you consider of a rich enough policy space \Pi) to the greedy policy with respect to V^e, which is strictly better than V^e (if V^e is not optimal). So you don’t need to use k>1 to improve the expert policy. Now it’s true that this will not converge to the optimal policy (since you keep bootstrapping with V^e instead of the current value function), but neither the k-step advantage will. 

So I don’t see any fundamental difference between k=1 and k>1. The only difference being that the k-step bootstrapping will implement a k-step Bellman operator which contracts faster (as gamma^k) when k is large. But the best choice of k has to be discussed in light of a bias-variance discussion, which is missing here. So I find that the main motivation for this work is not well supported. 

Algorithmic suggestion:
Instead of bootstrapping with V^e, why not bootstrap with min(V^e, V), where V is your current approximation of the value function. In that way you would benefit from (1) fast initialization with V^e at the beginning of learning, (2) continual improvement once you’ve reached the performance of the expert. 

Other comments:
Requiring that we know the value function of the expert on the whole state space is a very strong assumption that we do not usually make in Imitation learning. Instead we assume we have trajectories from expert (from which we can compute value function along those trajectories only). Generalization of the value function to other states is a hard problem in RL and is the topic of important research.

The overall writing lacks rigor and the contribution is poor. Indeed the lower bound (Theorem 3.1) is not novel (btw, the constant hidden in the \Omega notation is 1/(1-gamma)). Theorems 3.2 and 3.3 are not novel either. Please read [Bertsekas and Tsitsiklis, 96] as an introduction to dynamic programming with approximation.

The writing could be improved, and there are many typos, such as:
- J is not defined (Equation (2))
- Why do you call A a disadvantage function whereas this quantity is usually called an advantage?
- You are considering a finite (ie, k) horizon setting, so the value function depend on time. For example the value functions defined in (11) depend on time. 
- All derivations in Section 4, before subsection 4.1 are very approximate and lack rigor.
- Last sentence of Proof of theorem 3.1. I don’t understand H -> 2H epsilon. H is fixed, right? Also your example does not seem to be a discounted problem.
","[3, 6, 7]","[' Clear rejection', ' Marginally above acceptance threshold', ' Good paper, accept']","[5, 4, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer disagrees with the core claim of the paper and finds the motivation not well-supported. They point out several flaws in the work, including lack of novelty, rigor, and clarity. The reviewer also suggests an alternative algorithmic approach. While they provide constructive criticism and suggestions, the overall tone is rather critical of the work.",-50.0,20.0
Understanding image motion with group representations ,"['Andrew Jaegle', 'Stephen Phillips', 'Daphne Ippolito', 'Kostas Daniilidis']",Accept,2018,"[5, 11, 3, 29]","[10, 13, 8, 34]","[40, 13, 38, 331]","[13, 6, 13, 190]","[24, 4, 24, 85]","[3, 3, 1, 56]","The authors propose to learn the rigid motion group (translation and rotation) from a latent representation of image sequences without the need for explicit labels.
Within their data driven approach they pose minimal assumptions on the model, requiring the group properties (associativity, invertibility, identity) to be fulfilled.
Their model comprises CNN elements to generate a latent representation in motion space and LSTM elements to compose these representations through time.
They experimentally demonstrate their method on sequences of MINST digits and the KITTI dataset.

Pros:
- interesting concept of combining algebraic structure with a data driven method
- clear idea development and well written
- transparent model with enough information for re-implementation
- honest pointers to scenarios where the method might not work well

Cons:
- the method is only intrinsically evaluated (Tables 2 and 3), but not compared with results from other motion estimation methods","[7, 5, 4]","[' Good paper, accept', ' Marginally below acceptance threshold', ' Ok but not good enough - rejection']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is positive overall. It highlights the strengths of the paper, such as the interesting concept, clear writing, transparency, and honesty about limitations. While it notes a 'Con' as the lack of comparison with other methods, this is a suggestion for improvement rather than a harsh criticism. The language is constructive and professional throughout.",60.0,80.0
Learning Latent Permutations with Gumbel-Sinkhorn Networks,"['Gonzalo Mena', 'David Belanger', 'Scott Linderman', 'Jasper Snoek']",Accept,2018,"[5, 20, 5, 13]","[7, 24, 10, 18]","[9, 51, 40, 80]","[5, 26, 26, 36]","[2, 22, 13, 34]","[2, 3, 1, 10]","The idea on which the paper is based - that the limit of the entropic regularisation over Birkhoff polytope is on the vertices = permutation matrices -, and the link with optimal transport, is very interesting. The core of the paper, Section 3, is interesting and represents a valuable contribution.

I am wondering whether the paper's approach and its Theorem 1 can be extended to other regularised versions of the optimal transport cost, such as this family (Tsallis) that generalises the entropic one:

https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14584/14420

Also, it would be good to keep in mind the actual proportion of errors that would make a random choice of a permutation matrix for your Jigsaws. When you look at your numbers, the expected proportion of parts wrong for a random assignment could be competitive with your results on the smallest puzzles (typically, 2x2). Perhaps you can put the *difference* between your result and the expected result of a random permutation; this will give a better understanding of what you gain from the non-informative baseline.
(also, it would be good to define ""Prop. wrong"" and ""Prop. any wrong"". I think I got it but it is better to be written down)

There should also be better metrics for bigger jigsaws -- for example, I would accept bigger errors if pieces that are close in the solution tend also to be put close in the err'ed solution.

Typos:

* Rewrite definition 2 in appendix. Some notations do not really make sense.","[6, 7, 8]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Top 50% of accepted papers, clear accept']","[2, 4, 4]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer explicitly states the core of the paper is ""interesting and represents a valuable contribution."" This suggests a positive sentiment. While the reviewer provides constructive criticism and suggestions for improvement, they are presented politely and with a goal of enhancing the paper. The reviewer acknowledges the value of the work but also encourages further exploration and strengthening of the findings.",60.0,80.0
A Framework for the Quantitative Evaluation of Disentangled Representations,"['Cian Eastwood', 'Christopher K. I. Williams']",Accept,2018,"[1, 28]","[5, 33]","[10, 140]","[5, 67]","[5, 32]","[0, 41]","The authors consider the metrics for evaluating disentangled representations. They define three criteria: Disentanglement, Informativeness, and Completeness. They  learning a linear mapping from the latent code to an idealized set of disentangled generative factors, and then define information-theoretic measures based on pseudo-distributions calculated from the relative magnitudes of weights. Experimental evaluation considers a dataset of 200k images of a teapot with varying pose and color.

I think that defining metrics for evaluating the degree of disentanglement in representations is  great problem to look at. Overall, the metrics approached by the authors are reasonable, though the way pseudo-distribution are define in terms of normalized weight magnitudes is seems a little ad hoc to me.  

A second limitation of the work is the reliance on a ""true"" set of disentangled factors. We generally want to learn learning disentangled representations in an unsupervised or semi-supervised manner, which means that we will in general not have access supervision data for the disentangled factors. Could the authors perhaps comment on how well these metrics would work in the semi-supervised case?

Overall, I would say this is somewhat borderline, but I could be convinced to argue for acceptance based on the other reviews and the author response. 

Minor Commments:

- Tables 1 and 2 would be easier to unpack if the authors were to list the names of the variables (i.e. azimuth instead of z_0) or at least list what each variable is in the caption. 

- It is not entirely clear to me how the proposed metrics, whose definitions all reference magnitudes of weights, generalize to the case of random forests. ","[6, 7, 6]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Marginally above acceptance threshold']","[4, 5, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The reviewer finds the problem interesting and the proposed metrics reasonable, which points towards a positive sentiment. However, they also have some reservations, hence it's not overly positive. The language is constructive and professional throughout, suggesting politeness. The reviewer even shows willingness to be convinced for acceptance.",50.0,75.0
A Neural Representation of Sketch Drawings,"['David Ha', 'Douglas Eck']",Accept,2018,"[3, 20]","[7, 24]","[57, 89]","[29, 59]","[25, 22]","[3, 8]","The paper presents both a novel large dataset of sketches and a new rnn architecture to generate new sketches.

+ new and large dataset
+ novel algorithm
+ well written
- no evaluation of dataset
- virtually no evaluation of algorithm
- no baselines or comparison

The paper is well written, and easy to follow. The presented algorithm sketch-rnn seems novel and significantly different from prior work.
In addition, the authors collected the largest sketch dataset, I know of. This is exciting as it could significantly push the state of the art in sketch understanding and generation. 

Unfortunately the evaluation falls short. If the authors were to push for their novel algorithm, I'd have expected them to compare to prior state of the art on standard metrics, ablate their algorithm to show that each component is needed, and show where their algorithm shines and where it falls short.
For ablation, the bare minimum includes: removing the forward and/or reverse encoder and seeing performance drop. Remove the variational component, and phrasing it simply as an auto-encoder. Table 1 is good, but not sufficient. Training loss alone likely does not capture the quality of a sketch.
A comparison the Graves 2013 is absolutely required, more comparisons are desired.
Finally, it would be nice to see where the algorithm falls short, and where there is room for improvement.

If the authors wish to push their dataset, it would help to first evaluate the quality of the dataset. For example, how well do humans classify these sketches? How diverse are the sketches? Are there any obvious modes? Does the discretization into strokes matter?
Additionally, the authors should present a few standard evaluation metrics they would like to compare algorithms on? Are there any good automated metrics, and how well do they correspond to human judgement?

In summary, I'm both excited about the dataset and new architecture, but at the same time the authors missed a huge opportunity by not establishing proper baselines, evaluating their algorithm, and pushing for a standardized evaluation protocol for their dataset. I recommend the authors to decide if they want to present a new algorithm, or a new dataset and focus on a proper evaluation.","[5, 8, 8]","[' Marginally below acceptance threshold', ' Top 50% of accepted papers, clear accept', ' Top 50% of accepted papers, clear accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer acknowledges the significance of the novel dataset and algorithm, describing them as ""exciting"" and ""significantly different."" However, they express strong concerns about the lack of thorough evaluation, stating it ""falls short"" and constitutes a ""missed opportunity."" The reviewer provides constructive criticism and detailed suggestions for improvement. Overall, the tone is critical due to the lack of evaluation, but the constructive feedback and suggestions indicate a willingness to see the work succeed.",40.0,70.0
Polar Transformer Networks,"['Carlos Esteves', 'Christine Allen-Blanchette', 'Xiaowei Zhou', 'Kostas Daniilidis']",Accept,2018,"[2, 4, 13, 29]","[7, 8, 18, 34]","[27, 11, 175, 331]","[12, 4, 79, 190]","[14, 6, 67, 85]","[1, 1, 29, 56]","This paper proposes a method to learn networks invariant to translation and equivariant to rotation and scale of arbitrary precision. The idea is to jointly train
- a network predicting a polar origin,
- a module transforming the image into a log-polar representation according to the predicted origin,
- a final classifier performing the desired classification task.
A (not too large) translation of the input image therefore does not change the log-polar representation.
Rotation and scale from the polar origin result in translation of the log-polar representation. As convolutions are translation equivariant, the final classifier becomes rotation and scale equivariant in terms of the input image. Rotation and scale can have arbitrary precision, which is novel to the best of my knowledge.

(+) In my opinion, this is a simple, attractive approach to rotation and scale equivariant CNNs.

(-) The evaluation, however, is quite limited. The approach is evaluated on:
 1) several variants of MNIST. The authors introduce a new variant (SIM2MNIST), which is created by applying random similitudes to the images from MNIST. This variant is of course very well suited to the proposed method, and a bit artificial.
 2) 3d voxel occupancy grids with a small resolution. The objects can be rotated around the z-axis, and the method is used to be equivariant to this rotation.

(-) Since the method starts by predicting the polar origin, wouldn't it be possible to also predict rotation and scale? Then the input image could be rectified to a canonical orientation and scale, without needing equivariance. My intuition is that this simpler approach would work better. It should at least be evaluated.

Despite these weaknesses, I think this paper should be interesting for researchers looking into equivariant CNNs.
","[7, 7, 8]","[' Good paper, accept', ' Good paper, accept', ' Top 50% of accepted papers, clear accept']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer finds the proposed method ""simple and attractive"" which points towards a positive sentiment. However, they also list several weaknesses, particularly regarding the limited evaluation and a potential simpler approach. Overall, the feedback is constructive and encouraging further development, thus leaning towards a slightly positive sentiment. The language used is polite and professional throughout the review, without any harsh criticism or disrespectful remarks.",60.0,80.0
Neural Speed Reading via Skim-RNN,"['Minjoon Seo', 'Sewon Min', 'Ali Farhadi', 'Hannaneh Hajishirzi']",Accept,2018,"[5, 2, 16, 12]","[10, 7, 21, 17]","[69, 51, 240, 237]","[26, 23, 117, 116]","[43, 28, 109, 118]","[0, 0, 14, 3]","Summary: The paper proposes a learnable skimming mechanism for RNN. The model decides whether to send the word to a larger heavy-weight RNN or a light-weight RNN. The heavy-weight and the light-weight RNN each controls a portion of the hidden state. The paper finds that with the proposed skimming method, they achieve a significant reduction in terms of FLOPS. Although it doesn’t contribute to much speedup on modern GPU hardware, there is a good speedup on CPU, and it is more power efficient.

Contribution:
- The paper proposes to use a small RNN to read unimportant text. Unlike (Yu et al., 2017), which skips the text, here the model decides between small and large RNN.

Pros:
- Models that dynamically decide the amount of computation make intuitive sense and are of general interests.
- The paper presents solid experimentation on various text classification and question answering datasets.
- The proposed method has shown reasonable reduction in FLOPS and CPU speedup with no significant accuracy degradation (increase in accuracy in some tasks).
- The paper is well written, and the presentation is good.

Cons:
- Each model component is not novel. The authors propose to use Gumbel softmax, but does compare other gradient estimators. It would be good to use REINFORCE to do a fair comparison with (Yu et al., 2017 ) to see the benefit of using small RNN.
- The authors report that training from scratch results in unstable skim rate, while Half pretrain seems to always work better than fully pretrained ones. This makes the success of training a bit adhoc, as one need to actively tune the number of pretraining steps.
- Although there is difference from (Yu et al., 2017), the contribution of this paper is still incremental.

Questions:
- Although it is out of the scope for this paper to achieve GPU level speedup, I am curious to know some numbers on GPU speedup.
- One recommended task would probably be text summarization, in which the attended text can contribute to the output of the summary.

Conclusion:
- Based on the comments above, I recommend Accept","[7, 8, 7]","[' Good paper, accept', ' Top 50% of accepted papers, clear accept', ' Good paper, accept']","[3, 3, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer provides both positive and negative points, but ultimately recommends acceptance. They acknowledge the paper's contributions (solid experimentation, FLOPS reduction, good writing) but also point out limitations (incremental contribution, training instability, lack of GPU speedup). The tone is constructive and professional throughout.",60.0,80.0
QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension,"['Adams Wei Yu', 'David Dohan', 'Minh-Thang Luong', 'Rui Zhao', 'Kai Chen', 'Mohammad Norouzi', 'Quoc V. Le']",Accept,2018,"[8, 21, 9, 27, 15, 10, 14]","[13, 26, 14, 32, 16, 15, 19]","[41, 29, 53, 140, 33, 136, 299]","[17, 13, 25, 69, 21, 59, 143]","[20, 16, 26, 27, 5, 72, 145]","[4, 0, 2, 44, 7, 5, 11]","This paper proposes two contributions: first, applying CNNs+self-attention modules instead of LSTMs, which could result in significant speedup and good RC performance; second, enhancing the RC model training with passage paraphrases generated by a neural paraphrasing model, which could improve the RC performance marginally.

Firstly, I suggest the authors rewrite the end of the introduction. The current version tends to mix everything together and makes the misleading claim. When I read the paper, I thought the speeding up mechanism could give both speed up and performance boost, and lead to the 82.2 F1. But it turns out that the above improvements are achieved with at least three different ideas: (1) the CNN+self-attention module; (2) the entire model architecture design; and (3) the data augmentation method. 

Secondly, none of the above three ideas are well evaluated in terms of both speedup and RC performance, and I will comment in details as follows:

(1) The CNN+self-attention was mainly borrowing the idea from (Vaswani et al., 2017a) from NMT to RC. The novelty is limited but it is a good idea to speed up the RC models. However, as the authors hoped to claim that this module could contribute to both speedup and RC performance, it will be necessary to show the RC performance of the same model architecture, but replacing the CNNs with LSTMs. Only if the proposed architecture still gives better results, the claims in the introduction can be considered correct.

(2) I feel that the model design is the main reason for the good overall RC performance. However, in the paper there is no motivation about why the architecture was designed like this. Moreover, the whole model architecture is only evaluated on the SQuAD dataset. As a result, it is not convincing that the system design has good generalization. If in (1) it is observed that using LSTMs in the model instead of CNNs could give on par or better results, it will be necessary to test the proposed model architecture on multiple datasets, as well as conducting more ablation tests about the model architecture itself.

(3) I like the idea of data augmentation with paraphrasing. Currently, the improvement is only marginal, but there seems many other things to play with. For example, training NMT models with larger parallel corpora; training NMT models with different language pairs with English as the pivot; and better strategies to select the generated passages for data augmentation.

I am looking forward to the test performance of this work on SQuAD.","[5, 6, 8]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Top 50% of accepted papers, clear accept']","[4, 3, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The reviewer acknowledges the contributions of the paper and finds the ideas interesting, particularly the data augmentation aspect. However, they express concerns about the evaluation and clarity regarding the individual contributions to speed and performance. The reviewer suggests additional experiments and clarifications. Overall, the tone is critical but constructive, suggesting improvements rather than outright rejection.",20.0,50.0
Deep Bayesian Bandits Showdown:  An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling,"['Carlos Riquelme', 'George Tucker', 'Jasper Snoek']",Accept,2018,"[5, 6, 13]","[10, 10, 18]","[38, 75, 80]","[14, 34, 36]","[22, 39, 34]","[2, 2, 10]","This paper presents the comparison of a list of algorithms for contextual bandit with Thompson sampling subroutine. The authors compared different methods for posterior estimation for Thompson sampling. Experimental comparisons on contextual bandit settings have been performed on a simple simulation and quite a few real datasets.

The main paper + appendix are clearly written and easy to understand. The main paper itself is very incomplete. The experimental results should be summarized and presented in the main context. There is a lack of novelty of this study. Simple comparisons of different posterior estimating methods do not provide insights or guidelines for contextual bandit problem. 

What's the new information provided by running such methods on different datasets? What are the newly observed advantages and disadvantages of them? What could be the fundamental reasons for the variety of behaviors on different datasets? No significant conclusions are made in this work.

Experimental results are not very convincing. There are lots of plots show linear cumulative regrets within the whole time horizon. Linear regrets represent either trivial methods or not long enough time horizon.
","[5, 7, 6]","[' Marginally below acceptance threshold', ' Good paper, accept', ' Marginally above acceptance threshold']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review acknowledges the paper's clarity and readability as positive aspects. However, it criticizes the lack of novelty, limited insights, and unconvincing experimental results. The reviewer raises several questions, indicating a lack of significant contributions and weaknesses in the study's execution. The tone, while direct, maintains a professional and constructive approach, suggesting areas for improvement rather than resorting to harsh language.",-30.0,60.0
The power of deeper networks for expressing natural functions,"['David Rolnick', 'Max Tegmark']",Accept,2018,"[8, 25]","[13, 30]","[63, 54]","[16, 8]","[35, 35]","[12, 11]","Summary and significance: The authors prove that for expressing simple multivariate monomials over n variables, networks of depth 1 require exp(n) many neurons, whereas networks of depth n can represent these monomials using only O(n) neurons. 
The paper provides a simple and clear explanation for the important problem of theoretically explaining the power of deep networks, and quantifying the improvement provided by depth.

+ves:
Explaining the power of depth in NNs is fundamental to an understanding of deep learning. The paper is very easy to follow. and the proofs are clearly written. The theorems provide exponential gaps for very simple polynomial functions.

-ves:
1. My main concern with the paper is the novelty of the contribution to the techniques. The results in the paper are more general than that of Lin et al., but the proofs are basically the same, and it's difficult to see the contribution of this paper in terms of the contributing fundamentally new ideas. 
2. The second concern is that the results apply only to non-linear activation functions with sufficiently many non-zero derivatives (same requirements as for the results of Lin et al.).
3. Finally, in prop 3.3, reducing from uniform approximations to Taylor approximations, the inequality |E(δx)| <= δ^(d+1) |N(x) - p(x)| does not follow from the definition of a Taylor approximation.

Despite these criticisms, I contend that the significance of the problem, and the clean and understandable results in the paper make it a decent paper for ICLR.","[6, 7, 6]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Marginally above acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer acknowledges the significance of the paper's findings and praises its clarity and understandability. While they raise valid concerns about novelty and limitations, they ultimately recommend the paper for acceptance, indicating a generally positive sentiment. The language used is professional and respectful throughout.",60.0,80.0
DORA The Explorer: Directed Outreaching Reinforcement Action-Selection,"['Lior Fox', 'Leshem Choshen', 'Yonatan Loewenstein']",Accept,2018,"[1, 1, 11]","[5, 6, 16]","[6, 62, 17]","[3, 25, 1]","[3, 36, 4]","[0, 1, 12]","This paper presents an exploration method for model-free RL that generalizes the counter-based exploration bonus methods and takes into account long term exploratory value of actions rather than a single step look-ahead. This generalization is achieved by relying on the convergence rate of SARSA updates on an auxiliary MDP.

The method presented in the paper trains a parallel ""E-value"" MDP, with initial value of 1 for all state-action pairs. It applies SARSA (on-policy) update rule to the E-value MDP, where the acting policy is selected on the original MDP. While the E-value MDP is training, the proposed method uses a 1/log transformation applied to E-values to get the corresponding exploration bonus term for the original MDP. This bonus term is shown to be equivalent counter-based methods for finite MDPs when the discount factor of the E-MDP is set to 0. The paper has minimal theoretical analysis of the proposed algorithm, essentially only showing convergence with infinite visiting. In that regard, the presented method seems like a useful heuristic with anecdotal empirical benefits.

What is crucially lacking from the paper is any reference to model-free Bayesian methods that have very similar intuition behind them: taking into account the long term exploratory benefits of actions (passed on through the Bayesian inference). A comparison would have been trivial to do (with a generic non-informative prior) for the finite MPD setting (section 3.4). Even for the function approximation case one could use Gaussian process methods as the Bayesian baseline. There are also several computationally tractable approximations of Bayesian RL that can be used as baseline for empirical analysis.

It would have also been nice to do some analysis on how the update rule in a function approximation case is affecting the bonus terms. Unlike the finite case, updates to the value of one E-value can change the value for another state-action pair and the convergence could be faster than (1-alpha)^n. Given the lack of any theory on this, an empirical analysis is certainly valuable. (Update: experiment added in the a later revision to study this effect)

Notes:
- The plots are horrible in a print. I had to zoom 400% into the PDF file to be able to read the plots. Please scale them at least by 200% and use a larger font for the legends.

- Add a minimal description of the initial setup for E-value neural network to section 4.1 (i.e. how the initializing is achieved to have a constant value for all state-action pairs as described in the appendix).

* Note: This review and rating has been partially revised with the updates to the paper after the initial comments. ","[6, 6, 7]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Good paper, accept']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review starts with a neutral tone, presenting a summary of the paper's contributions. However, the subsequent paragraphs highlight significant shortcomings, particularly the lack of comparison with Bayesian methods and limited theoretical analysis. The reviewer acknowledges some positive aspects, such as the generalization of counter-based methods and potential empirical benefits, but the overall tone leans towards the negative due to the identified gaps. The language used is professional and not rude, suggesting areas for improvement constructively. The mention of 'horrible plots' is a critique of the presentation, not the authors themselves. The concluding note about partial revision after updates implies the reviewer acknowledges the authors' efforts to address the feedback, indicating a somewhat collaborative approach.",-25.0,50.0
Learning a Generative Model for Validity in Complex Discrete Structures,"['Dave Janz', 'Jos van der Westhuizen', 'Brooks Paige', 'Matt Kusner', 'José Miguel Hernández-Lobato']",Accept,2018,"[3, 2, 6, 7, 13]","[7, 2, 11, 12, 18]","[13, 6, 53, 71, 163]","[5, 1, 25, 35, 82]","[8, 5, 27, 35, 70]","[0, 0, 1, 1, 11]","The authors use a recurrent neural network to build generative models of sequences in domains where the vast majority of sequences is invalid. The basic idea, outlined in Eq. 2, is moderately straightforward: at each step, use an approximation of the Q function for subsequences of the appropriate length to pick a valid extension. There are numerous details to get right. The writing is mostly clear, and the examples are moderately convincing. I wish the paper had more detailed arguments and discussions.

I question the appropriateness of Eq. 2 as a target. A correctly learned model will put positive weight on valid sequences, but it may be an arbitrarily slow way to generate diverse sequences, depending on the domain. For instance, imagine a domain of binary strings where the valid sequences are the all 1 sequence, or any sequence beginning with a 0. Half the generated sequences would be all 1's in this situation, right? And it's easy to construct further examples that are much worse than this?

The use of Bayesian active learning to generate the training set feels like an elegant idea. However, I wish there were more clarity about what was ad hoc and what wasn't. For instance, I think the use of  dropout to get q is suspect (see for instance https://arxiv.org/abs/1711.02989), and I'd prefer a little more detail on statements like ""The nonlinearity of g(·) means that our Monte
Carlo approximation is biased, but still consistent."" Do we have any way of quantifying the bias? Is the statement about K=16 being reasonable a statement about bias, variance, or both?

For Python strings: 
- Should we view the fact that high values of tau give a validity of 1.0 as indicative that the domain's constraints are fairly easy to learn?
- ""The use of a Boltzmann policy allows us to tune the temperature parameter to identify policies
which hit high levels of accuracy for any learned Q-function approximation."" This is only true to the extent the domain is sufficiently ""easy"" right? Is the argument that even in very hard domains, you might get this by just having an RNN which memorized a single valid sequence (assuming at least one could be found)?
- What's the best explanation for *why* the active model has much higher diversity? I understand that the active model is picking examples that tell us more about the uncertainty in w, but it's not obvious to me that means higher diversity. Do we think this is a universal property of domains?
- The highest temperature active model is exploring about half of valid sequences (modulo the non-tightness of the bound)? Have you tried gaining some insight by generating thousands of valid sequences manually and seeing which ones the model is rejecting?
- The coverage bound is used only for for Python expressions, right? Why not just randomly sample a few thousand positives and use that to get a better estimate of coverage? Since you can sample from the true positive set, it seems that your argument from the appendix about the validation set being ""too similar to the training set"" doesn't apply?
- It would be better to see a comparison to a strong non-NN baseline. For instance, I could easily make a PCFG over Python math expressions, and use rejection sampling to get rid of those that aren't exactly length 25, etc.?

I question how easy the Python strings example is. In particular, it might be that it's quite an easy example (compared to the SMILES) example. For SMILES, it seems like the Bayesian active learning technique is not by itself sufficient to create a good model? It is interesting that in the solubility domain the active model outperforms, but it would be nice to see more discussion / explanation.

Minor note: The incidence of valid strings in the Python expressions domain is (I believe) > 1/5000, although I guess 1 in 10,000 is still the right order of magnitude.

If I could score between ""marginal accept"" and ""accept"" I would. ","[7, 6, 7]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Good paper, accept']","[3, 4, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer provides constructive criticism, acknowledges the merits of the work, and suggests specific improvements. While they raise valid concerns, the overall tone is positive, suggesting that with some revisions, the paper would be acceptable. The language is formal and respectful, typical of academic peer reviews.",60.0,80.0
Progressive Reinforcement Learning with Distillation for Multi-Skilled Motion Control,"['Glen Berseth', 'Cheng Xie', 'Paul Cernek', 'Michiel Van de Panne']",Accept,2018,"[6, 15, 1, 29]","[11, 20, 1, 33]","[59, 69, 2, 125]","[25, 33, 1, 55]","[22, 6, 1, 24]","[12, 30, 0, 46]","This paper aims to learn a single policy that can perform a variety of tasks that were experienced sequentially. The approach is to learn a policy for task 1, then for each task k+1: copy distilled policy that can perform tasks 1-k, finetune to task k+1, and distill again with the additional task. The results show that this PLAID algorithm outperforms a network trained on all tasks simultaneously. 

Questions:
- When distilling the policies, do you start from a randomly initialized policy, or do you start from the expert policy network?
- What data do you use for the distillation? Section 4.1 states""We use a method similar to the DAGGER algorithm"", but what is your method. If you generate trajectories form the student network, and label them with the expert actions, does that mean all previous expert policies need to be kept in memory?
- I do not understand the purpose of ""input injection"" nor where it is used in the paper. 

Strengths:
- The method is simple but novel. The results support the method's utility.
- The testbed is nice; the tasks seem significantly different from each other. It seems that no reward shaping is used.
- Figure 3 is helpful for understanding the advantage of PLAID vs MultiTasker.

Weaknesses:
- Figure 2: the plots are too small.
- Distilling may hurt performance ( Figure 2.d)
- The method lacks details (see Questions above)
- No comparisons with prior work are provided. The paper cites many previous approaches to this but does not compare against any of them. 
- A second testbed (such as navigation or manipulation) would bring the paper up a notch. 

In conclusion, the paper's approach to multitask learning is a clever combination of prior work. The method is clear but not precisely described. The results are promising. I think that this is a good approach to the problem that could be used in real-world scenarios. With some filling out, this could be a great paper.","[7, 7, 5]","[' Good paper, accept', ' Good paper, accept', ' Marginally below acceptance threshold']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer provides both positive and negative points about the paper. They find the approach novel and the results promising. However, they also point out areas for improvement, such as lack of details, missing comparisons, and the need for a second testbed. The language is direct and professional, suggesting a constructive but critical review. Overall, the sentiment leans towards the positive side, as the reviewer sees the potential of the work.",60.0,70.0
Expressive power of recurrent neural networks,"['Valentin Khrulkov', 'Alexander Novikov', 'Ivan Oseledets']",Accept,2018,"[2, 5, 11]","[7, 9, 16]","[34, 32, 233]","[13, 10, 41]","[19, 17, 117]","[2, 5, 75]","In this paper, the expressive power of neural networks characterized by tensor train (TT) decomposition, a chain-type tensor decomposition, is investigated. Here, the expressive power refers to the rank of tensor decomposition, i.e., the number of latent components. The authors compare the complexity of TT-type networks with networks structured by CP decomposition, which corresponds to shallow networks. It is proved that the space of TT-type networks with rank O(r)  can be complex as the same as the space of CP-type networks with rank poly(r).

The paper is clearly written and easy to follow. 

The contribution is clear and it is distinguished from previous studies.

Though I enjoyed reading this paper, I have several concerns.

1. The authors compare the complexity of TT representation with CP representation (and HT representation). However, CP representation does not have universality (i.e., some tensors cannot be expressed by CP representation with finite rank, see [1]), this comparison may not make sense. It seems the comparison with Tucker-type representation makes much more sense because it has universality. 

2. Connecting RNN and TT representation is a bit confusing. Specifically, I found two gaps.
   (a) RNNs reuse the same parameter against all the input x_1 to x_d. This means that G_1 to G_d in Figure 1 are all the same. That's why RNNs can handle size-varying sequences. 
   (b) Standard RNNs do not use the multilinear units shown in Figure 3, but use a simple addition of an input and the output from the previous layer (i.e., h_t = f(Wx_t + Vh_{t-1}), where h_t is the t-th hidden unit, x_t is the t-th input, W and V are weights, and f is an activation function.) 
Due to the gaps, the analysis used in this paper seems not applicable to RNNs. If this is true, the story of this paper is somewhat misleading. Or, is your theory still applicable?

[1] Hackbusch, Wolfgang. Tensor spaces and numerical tensor calculus. Vol. 42. Springer Science & Business Media, 2012.","[6, 6, 6]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[5, 3, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer starts with positive remarks, praising the clarity and contribution of the paper. However, they raise several serious concerns about the methodology and applicability of the findings. The concerns suggest potential flaws in the paper's core arguments, which significantly impacts the overall impression. Therefore, the sentiment is moderately positive at the beginning but turns critical towards the end. The language remains polite throughout, employing constructive criticism and respectful phrasing.",40.0,80.0
Training wide residual networks for deployment using a single bit for each weight,['Mark D. McDonnell'],Accept,2018,[15],[19],[71],[36],[13],[22],"The authors propose to train neural networks with 1bit weights by storing and updating full precision weights in training, but using the reduced 1bit version of the network to compute predictions and gradients in training. They add a few tricks to keep the optimization numerically efficient. Since right now more and more neural networks are deployed to end users, the authors make an interesting contribution to a very relevant question.

The approach is precisely described although the text sometimes could be a bit clearer (for example, the text contains many important references to later sections).

The authors include a few other methods for comparision, but I think it would be very helpful to include also some methods that use a completely different approach to reduce the memory footprint. For example, weight pruning methods sometimes can give compression rates of around 100 while the 1bit methods by definition are limited to a compression rate of 32. Additionally, for practical applications, methods like weight pruning might be more promising since they reduce both the memory load and the computational load.

Side mark: the manuscript has quite a few typos.
","[6, 6, 6]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with positive statements, highlighting the relevance and interesting contribution. While it points out areas for improvement, it frames them constructively as suggestions rather than harsh criticisms. The language remains polite throughout, using phrases like ""it would be very helpful"" and ""the text sometimes could be a bit clearer."" The mention of typos is a neutral observation.",60.0,80.0
Learning Discrete Weights Using the Local Reparameterization Trick,"['Oran Shayer', 'Dan Levi', 'Ethan Fetaya']",Accept,2018,"[5, 13, 8]","[7, 18, 13]","[6, 32, 61]","[3, 19, 27]","[3, 8, 32]","[0, 5, 2]","This paper proposes training binary and ternary weight distribution networks through the local reparametrization trick and continuous optimization. The argument is that due to the central limit theorem (CLT) the distribution on the neuron pre-activations is approximately Gaussian, with a mean given by the inner product between the input and the mean of the weight distribution and a variance given by the inner product between the squared input and the variance of the weight distribution. As a result, the parameters of the underlying discrete distribution can be optimized via backpropagation by sampling the neuron pre-activations with the reparametrization trick. The authors further propose appropriate initialisation schemes and regularization techniques to either prevent the violation of the CLT or to prevent underfitting. The method is evaluated on multiple experiments.

This paper proposed a relatively simple idea for training networks with discrete weights that seems to work in practice. My main issue is that while the authors argue about novelty, the first application of CLT for sampling neuron pre-activations at neural networks with discrete r.v.s is performed at [1]. While [1] was only interested in faster convergence and not on optimization of the parameters of the underlying distribution, the extension was very straightforward. I would thus suggest that the authors update the paper accordingly. 

Other than that, I have some other comments:
- The L2 regularization on the distribution parameters for the ternary weights is a bit ad-hoc; why not penalise according to the entropy of the distribution which is exactly what you are trying to achieve? 
- For the binary setting you mentioned that you had to reduce the entropy thus added a “beta density regulariser”. Did you add R(p) or log R(p) to the objective function? Also, with alpha, beta = 2 the beta density is unimodal with a peak at p=0.5; essentially this will force the probabilities to be close to 0.5, i.e. exactly what you are trying to avoid. To force the probability near the endpoints you have to use alpha, beta < 1 which results into a “bowl” shaped Beta distribution. I thus wonder whether any gains you observed from this regulariser are just an artifact of optimization.  
- I think that a baseline (at least for the binary case) where you learn the weights with a continuous relaxation, such as the concrete distribution, and not via CLT would be helpful. Maybe for the network to properly converge the entropy for some of the weights needs to become small (hence break the CLT). 

[1] Wang & Manning, Fast Dropout Training.

Edit: After the authors rebuttal I have increased the rating of the paper: 
- I still believe that the connection to [1] is stronger than what the authors allude to; eg. the first two paragraphs of sec. 3.2 could easily be attributed to [1].
- The argument for the entropy was to include a term (- lambda * H(p)) in the objective function with H(p) being the entropy of the distribution p. The lambda term would then serve as an indicator to how much entropy is necessary.
- There indeed was a misunderstanding with the usage of the R(p) regularizer at the objective function (which is now resolved).
- The authors showed benefits compared to a continuous relaxation baseline.","[6, 7, 6]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Marginally above acceptance threshold']","[4, 3, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer acknowledges the paper presents a ""relatively simple idea that seems to work in practice,"" which indicates a somewhat positive sentiment. However, they also point out a significant flaw regarding the novelty claim, stating that a previous work already applied the central limit theorem (CLT) in a similar context. This significantly impacts the paper's contribution and explains the moderate sentiment score. Despite the criticism, the reviewer's language remains polite and professional throughout, engaging constructively with the authors' work and offering concrete suggestions for improvement.",40.0,70.0
Can Neural Networks Understand Logical Entailment?,"['Richard Evans', 'David Saxton', 'David Amos', 'Pushmeet Kohli', 'Edward Grefenstette']",Accept,2018,"[4, 35, 5, 16, 8]","[9, 36, 10, 21, 13]","[21, 20, 12, 322, 104]","[4, 5, 2, 180, 44]","[10, 7, 7, 102, 54]","[7, 8, 3, 40, 6]","This is a wonderful and a self-contained paper. In fact, it introduces a very important problem and it solves it. 

The major point of the paper is demonstrating that it is possible to model logical entailment in neural networks. Hence, a corpus and a NN model are introduced. The corpus is used to demonstrate that the model, named PossibleWorld, is nearly perfect for the task. A comparative analysis is done with respect to state of the art recurrent NN. So far, so good.

Yet, what is the take home message? In my opinion, the message is that generic NN should not be used for specific formal tasks whereas specific neural networks that model the task are desirable. This seems to be a trivial claim, but, since the PossibleWorld nearly completely solves the task, it is worth to be investigated. 

The point that the paper leaves unexplained is: what is in the PossibleWorld Network that captures what we need? The description of the network is in fact very criptic. No examples are given and a major effort is required to the reader. Can you provide examples and insights on why this is THE needed model?

Finally, the paper does not discuss a large body of research that has been done in the past by Plate. Plate has investigated how symbolic predicates can be described in distributed representations. This is strictly related to the problem this paper investigates. As discussed in ""Symbolic, Distributed and Distributional Representations for Natural Language Processing in the Era of Deep Learning: a Survey"", 2017, the link between symbolic and distributed representations has to be better investigated in order to propose innovative NN models. Your paper can be one of the first NN model that takes advantage of this strict link.","[4, 7, 7]","[' Ok but not good enough - rejection', ' Good paper, accept', ' Good paper, accept']","[4, 3, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review starts with very positive statements, like ""wonderful"" and ""solves it"". However, it then proceeds with constructive criticism and requests for clarification and further investigation. The tone remains respectful throughout, suggesting areas for improvement without resorting to negativity.",60.0,80.0
Scalable Private Learning with PATE,"['Nicolas Papernot', 'Shuang Song', 'Ilya Mironov', 'Ananth Raghunathan', 'Kunal Talwar', 'Ulfar Erlingsson']",Accept,2018,"[5, 6, 18, 10, 18, 23]","[10, 11, 23, 14, 23, 26]","[150, 39, 86, 29, 182, 58]","[56, 17, 45, 14, 98, 33]","[88, 22, 37, 14, 67, 17]","[6, 0, 4, 1, 17, 8]","This paper considers the problem of private learning and uses the PATE framework to achieve differential privacy. The dataset is partitioned and multiple learning algorithms produce so-called teacher classifiers. The labels produced by the teachers are aggregated in a differentially private manner and the aggregated labels are then used to train a student classifier, which forms the final output. The novelty of this work is a refined aggregation process, which is improved in three ways:
a) Gaussian instead of Laplace noise is used to achieve differential privacy.
b) Queries to the aggregator are ""filtered"" so that the limited privacy budget is only expended on queries where the teachers are confident and the student is uncertain or wrong.
c) A data-dependent privacy analysis is used to attain sharper bounds on the privacy loss with each query.

I think this is a nice modular framework form private learning, with significant refinements relative to previous work that make the algorithm more practical. On this basis, I think the paper should be accepted. However, I think some clarification is needed with regard to item c above:

Theorem 2 gives a data-dependent privacy guarantee. That is, if there is one label backed by a clear majority of teachers, then the privacy loss (as measured by Renyi divergence) is low. This data-dependent privacy guarantee is likely to be much tighter than the data-independent guarantee.
However, since the privacy guarantee now depends on the data, it is itself sensitive information. How is this issue resolved? If the final privacy guarantee is data-dependent, then this is very different to the way differential privacy is usually applied. This would resemble the ""privacy odometer"" setting of Rogers-Roth-Ullman-Vadhan [ https://arxiv.org/abs/1605.08294 ]. 
Another way to resolve this would be to have an output-dependent privacy guarantee. That is, the privacy guarantee would depend only on public information, rather than the private data. The widely-used ""sparse vector"" technique [ http://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf#page=59 ] does this.
In any case, this is an important issue that needs to be clarified, as it is not clear to me how this is resolved.

The algorithm in this work is similar to the so-called median mechanism [ https://www.cis.upenn.edu/~aaroth/Papers/onlineprivacy.pdf ] and private multiplicative weights [ http://mrtz.org/papers/HR10mult.pdf ]. These works also involve a ""student"" being trained using sensitive data with queries being answered in a differentially private manner. And, in particular, these works also filter out uninformative queries using the sparse vector technique. It would be helpful to add a comparison.
","[7, 6, 6]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[3, 1, 4]","[' The reviewer is fairly confident that the evaluation is correct', "" The reviewer's evaluation is an educated guess"", ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer explicitly recommends acceptance, which indicates positive sentiment. They praise the paper as a ""nice modular framework"" with ""significant refinements."" While they raise a significant question, they present it constructively and offer potential solutions, suggesting a collaborative approach rather than a dismissive one. The language is formal, respectful, and focused on improving the paper.",75.0,90.0
Emergent Complexity via Multi-Agent Competition,"['Trapit Bansal', 'Jakub Pachocki', 'Szymon Sidor', 'Ilya Sutskever', 'Igor Mordatch']",Accept,2018,"[5, 7, 4, 12, 12]","[9, 11, 8, 17, 17]","[31, 42, 14, 99, 109]","[19, 15, 4, 49, 48]","[12, 21, 9, 45, 56]","[0, 6, 1, 5, 5]","This paper demonstrates that a competitive multi-agent environment trained with self-play can produce behaviors that are far more complex than the environment itself and such environments come with a natural curriculum by introducing several multi-agent tasks with competing goals in a 3D world with simulated physics. It utilizes a decentralized training approach and use distributed implementation of PPO for very large scale multiagent training. This paper addresses the challenges in applying distributed PPO to train multiple competitive agents, including the problem of exploration with sparse reward by using full roll-outs and use the dense exploration reward which is gradually annealed to zero in favor of the sparse competition reward. It makes training more stable by selecting random old parameters for the opponent. 
 
Although the technical contributions seem to be not quite significant, this paper is well written and introduces a few new domains which are useful for studying problems in multiagent reinforcement learning. The paper also makes it clear regarding the connections and distinctions to many existing work. 

Minor issues:

E[Loss] in table 1 is undefined.

In the notation section, the observation model is missing, and the policy is restricted to be reactive.
 
Uniform (v, \deta v) -> Uniform (\deta v, v)
","[7, 3, 9]","[' Good paper, accept', ' Clear rejection', ' Top 15% of accepted papers, strong accept']","[4, 3, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review starts with positive statements, highlighting the paper's strengths and contributions. However, it then states that the technical contributions are 'not quite significant.' This, combined with the 'minor issues' section, suggests a sentiment leaning towards the neutral side of positive. The language used throughout is objective and professional, indicating a high level of politeness.",50.0,80.0
Non-Autoregressive Neural Machine Translation,"['Jiatao Gu', 'James Bradbury', 'Caiming Xiong', 'Victor O.K. Li', 'Richard Socher']",Accept,2018,"[15, 15, 13, 0, -3, 3, -3]","[20, 19, 18, 5, 1, 8, 1]","[51, 45, 69, 28, 5, 27, 5]","[25, 40, 38, 25, 5, 16, 5]","[5, 0, 14, 0, 0, 9, 0]","[21, 5, 17, 3, 0, 2, 0]","This work proposes non-autoregressive decoder for the encoder-decoder framework in which the decision of generating a word does not depends on the prior decision of generated words. The key idea is to model the fertility of each word so that copies for source words are fed as input to the encoder part, not the generated target words as inputs. To achieve the goal, authors investigated various techniques: For inference, sample fertility space for generating multiple possible translations. For training, apply knowledge distilation for better training followed by fine tuning by reinforce. Experiments for English/German and English/Romanian show comparable translation qualities with speedup by non-autoregressive decoding.

The motivation is clear and proposed methods are very sound. Experiments are carried out very carefully.

I have only minor concerns to this paper:

- The experiments are designed to achieve comparable BLEU with improved latency. I'd like to know whether any BLUE improvement might be possible under similar latency, for instance, by increasing the model size given that inference is already  fast enough.

- It'd also like to see other language pairs with distorted word alignment, e.g., Chinese/English, to further strengthen this work, though  it might have little impact given that attention already capture sort of alignment.

- What is the impact of the external word aligner quality? For instance, it would be possible to introduce a noise in the word alignment results or use smaller data to train a model for word aligner. 

- The positional attention is rather unclear and it would be better to revise it. Note that equation 4 is simply mentioning attention computation, not the proposed positional attention.","[7, 7, 6]","[' Good paper, accept', ' Good paper, accept', ' Marginally above acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer explicitly states the motivation is clear, the methods sound, and the experiments carefully carried out. They offer constructive criticism and suggestions for improvement rather than harsh critiques. This points to a positive sentiment overall, but with room for improvement in the paper. Therefore, the sentiment is closer to positive than neutral. The language used is polite and professional throughout, suggesting constructive feedback.",75.0,100.0
Generative networks as inverse problems with Scattering transforms,"['Tomás Angles', 'Stéphane Mallat']",Accept,2018,"[3, 30]","[5, 35]","[7, 117]","[3, 46]","[3, 39]","[1, 32]","The authors introduce scattering transforms as image generative models in the context of Generative Adversarial Networks and suggest why they could be seen as Gaussianization transforms with controlled information loss and invertibility.
Writing is suggestive and experimental results are interesting, so I clearly recommend acceptation. 

I would appreciate more intuition on some claims (e.g. relation between Lipschitz continuity and wavelets) but they refer to the appropriate reference to Mallat, so this is not a major problem for the interested reader.

However, related to the above non-intuitive claim, here is a question on a related Gaussianization transform missed by the authors that (I feel) fulfils the conditions defined in the paper but it is not obviously related to wavelets. Authors cite Chen & Gopinath (2000) and critizise that their approach suffers from the curse of dimensionality because of the ICA stage. However, other people [Laparra et al. Iterative Gaussianization from ICA to random rotations IEEE Trans.Neural Nets 2011] proved that the ICA stage is not required (but only marginal operations followed by even random rotations). That transform seems to be Lipschitz continuous as well -since it is smooth and derivable-. In fact it has been also used for image synthesis. However, it is not obviously related to wavelets... Any comment?

Another relation to previous literature: in the end, the proposed analysis (or Gaussianization) transform is basically a wavelet transform where the different scale filters are applied in a cascade (fig 1). This is similar to Gaussian Scale Mixture  models for texture analysis [Portilla & Simoncelli Int. J. Comp. Vis. 2000] in which after wavelet transform, local division is performed to obtain Gaussian variables, and these can be used to synthesize the learned textures. That is similar to Divisive Normalization models of visual neuroscience that perform similar normalization alfter wavelets to factorize the PDF (e.g. [Lyu&Simoncelli Radial Gaussianization Neur.Comput. 2009], or [Malo et al. Neur.Comput. 2010]).

Minor notation issues: authors use a notation for functions that seems confusing (to me) since it looks like linear products. For instance: GZ for G(Z) [1st page] and phiX for phi(X) [2nd page] Sx for S(x) [in page 5]... 
","[8, 6, 7]","[' Top 50% of accepted papers, clear accept', ' Marginally above acceptance threshold', ' Good paper, accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer clearly recommends acceptation, finding the writing suggestive and the experimental results interesting. This points to a positive sentiment. While the reviewer raises some questions and points for improvement, they are presented constructively and politely, suggesting areas for the authors to strengthen their work rather than offering harsh criticism. The reviewer acknowledges the authors' references and uses phrases like ""I would appreciate"" and ""Any comment?"" which lean towards a polite and collaborative tone.",75.0,75.0
SpectralNet: Spectral Clustering using Deep Neural Networks,"['Uri Shaham', 'Kelly Stanton', 'Henry Li', 'Ronen Basri', 'Boaz Nadler', 'Yuval Kluger']",Accept,2018,"[11, 2, 13, 31, 20, 18]","[15, 2, 17, 35, 25, 23]","[25, 6, 15, 159, 79, 60]","[6, 1, 7, 77, 28, 15]","[14, 2, 6, 34, 27, 30]","[5, 3, 2, 48, 24, 15]","PAPER SUMMARY

This paper aims to address two limitations of spectral clustering: its scalability to large datasets and its generalizability to new samples. The proposed solution is based on designing a neural network called SpectralNet that maps the input data to the eigenspace of the graph Laplacian and finds an orthogonal basis for this eigenspace. The network is trained by alternating between orthogonalization and gradient descent steps, where scalability is achieved by using a stochastic optimization scheme that instead of computing an eigendecomposition of the entire data (as in vanilla spectral clustering) uses a Cholesky decomposition of the mini batch to orthogonalize the output. The method can also handle out-of-sample data by applying the learned embedding function to new data. Experiments on the MNIST handwritten digit database and the Reuters document database demonstrate the effectiveness of the proposed SpectralNet.

COMMENTS

1) I find that the output layer (i.e. the orthogonalization layer) is not well-justified. In principle, different batches require different weights on the output layer. Although the authors observe empirically that orthogonalization weights are roughly shared across different batches, the paper lacks a convincing argument for why this can happen. Moreover, it is not clear why an output layer designed to orthogonalized batches from the training set would also orthogonalize batches from the test set?

2) One claimed contribution of this work is that it extends spectral clustering to large scale data. However, the paper could have commented more on what makes spectral clustering not scalable, and how the method in this paper addresses that. The authors did mention that spectral clustering requires computing eigenvectors for large matrices, which is prohibitive. However, this argument is not entirely true, as eigen-decomposition for large sparse matrices can be carried out efficiently by tools such as ARPACK. On the other hand, computing the nearest neighbor affinity or Gaussian affinity is N^2 complexity, which could be the bottleneck of computation for spectral clustering on large scale data. But this issue can be addressed using approximate nearest neighbors obtained, e.g., via hashing. Overall, the paper compares only to vanilla spectral clustering, which is not representative of the state of the art. The paper should do an analysis of the computational complexity of the proposed method and compare it to the computational complexity of both vanilla as well as scalable spectral clustering methods to demonstrate that the proposed approach is more scalable than the state of the art. 

3)  Continuing with the point above, an experimental comparison with prior work on large scale spectral clustering (see, e.g. [a] and the references therein) is missing. In particular, the result of spectral clustering on the Reuters database is not reported, but one could use other scalable versions of spectral clustering as a baseline.

4)  Another benefit of the proposed method is that it can handle out-of-sample data. However, the evaluation of such benefit in experiments is rather limited. In reporting the performance on out-of-sample data, there is no other baseline to compare with. One can at least compare with the following baseline: apply k-means to the training data in input space, and classify each test data to the nearest centroid.

5) The reason for using an autoencoder to extract features is unclear. In subspace clustering, it has been observed that features extracted from a scattering transform network [b] can significantly improve clustering performance, see e.g. [c] where all methods have >85% accuracy on MNIST. The methods in [c] are also tested on larger datasets.

[a] Choromanska, et. al., Fast Spectral Clustering via the Nystrom Method, International conference on algorithmic learning theory, 2013

[b] Bruna, Mallat, Invariant Scattering Convolution Networks, arXiv 2012

[c] You, et. al., Oracle Based Active Set Algorithm for Scalable Elastic Net Subspace Clustering, CVPR 2016","[4, 7, 6]","[' Ok but not good enough - rejection', ' Good paper, accept', ' Marginally above acceptance threshold']","[4, 5, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']","The review is critical of the paper, pointing out several limitations and suggesting areas for improvement. While it acknowledges the contributions, it focuses heavily on the shortcomings. The language used is formal and academic, but it directly addresses the weaknesses, leading to a less positive tone. ",-30.0,60.0
SGD Learns Over-parameterized Networks that Provably Generalize on Linearly Separable Data,"['Alon Brutzkus', 'Amir Globerson', 'Eran Malach', 'Shai Shalev-Shwartz']",Accept,2018,"[4, 18, 2, 17]","[8, 23, 7, 22]","[21, 169, 33, 178]","[11, 100, 14, 81]","[9, 61, 18, 67]","[1, 8, 1, 30]","Paper studies an interesting phenomenon of overparameterised models being able to learn well-generalising solutions. It focuses on a setting with three crucial simplifications:
- data is linearly separable
- model is 1-hidden layer feed forward network with homogenous activations
- **only input-hidden layer weights** are trained, while the hidden-output layer's weights are fixed to be (v, v, v, ..., v, -v, -v, -v, ..., -v) (in particular -- (1,1,...,1,-1,-1,...,-1))
While the last assumption does not limit the expressiveness of the model in any way, as homogenous activations have the property of f(ax)=af(x) (for positive a) and so for any unconstrained model in the second layer, we can ""propagate"" its weights back into first layer and obtain functionally equivalent network. However, learning dynamics of a model of form 
 z(x) = SUM( g(Wx+b) ) - SUM( g(Vx+c) ) + d
and ""standard"" neural model
 z(x) = Vg(Wx+b)+c
can be completely different.
Consequently, while the results are very interesting, claiming their applicability to the deep models is (at this point) far fetched. In particular, abstract suggests no simplifications are being made, which does not correspond to actual result in the paper. The results themselves are interesting, but due to the above restriction it is not clear whether it sheds any light on neural nets, or simply described a behaviour of very specific, non-standard shallow model.

I am happy to revisit my current rating given authors rephrase the paper so that the simplifications being made are clear both in abstract and in the text, and that (at least empirically) it does not affect learning in practice. In other words - all the experiments in the paper follow the assumption made, if authors claim is that the restriction introduced does not matter, but make proofs too technical - at least experimental section should show this. If the claims do not hold empirically without the assumptions made, then the assumptions are not realistic and cannot be used for explaining the behaviour of models we are interested in.

Pros:
- tackling a hard problem of overparametrised models, without introducing common unrealistic assumptions of activations independence
- very nice result of ""phase change"" dependend on the size of hidden layer in section 7

Cons:
- simplification with non-trainable second layer is currently not well studied in the paper; and while not affecting expressive power - it is something that can change learning dynamics completely

# After the update

Authors addressed my concerns by:
- making simplification assumption clearer in the text
- adding empirical evaluation without the assumption
- weakening the assumptions

I find these modifications satisfactory and rating has been updated accordingly. 
","[7, 7, 8]","[' Good paper, accept', ' Good paper, accept', ' Top 50% of accepted papers, clear accept']","[3, 3, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with a somewhat critical tone, pointing out limitations in the paper's claims and assumptions. However, it acknowledges the interesting aspects of the work. The reviewer is open to reconsidering their opinion if the authors address their concerns, which suggests a willingness to engage constructively. The language used is formal and direct, but not disrespectful. The updated part of the review confirms that the authors have addressed the concerns, leading to a positive shift in sentiment.",50.0,70.0
Depthwise Separable Convolutions for Neural Machine Translation,"['Lukasz Kaiser', 'Aidan N. Gomez', 'Francois Chollet']",Accept,2018,"[14, 2, 17]","[18, 6, 21]","[84, 29, 23]","[44, 9, 6]","[32, 18, 8]","[8, 2, 9]","This paper presents the SliceNet architecture, an sequence-to-sequence model based on super-dilated convolutions, which allow to reduce the computational cost of the model compared to standard convolution. The proposed model is then evaluated on machine translation and yields competitive performance compared to state-of-the-art approaches.

In terms of clarity, the paper is overall easy to follow, however I am a bit confused by Section 2 about what is related work and what is a novel contribution, although the section is called “Our Contribution”. For instance, it seems that the separable convolution presented in Section 2.1 were introduced by (Chollet, 2016) and are not part of the contribution of this paper. The authors should thus clarify the contributions of the paper.

In terms of significance, the SliceNet architecture is interesting and is a solid contribution for reducing computation cost of sequence-to-sequence models. The experiments on NMT are convincing and gives interesting insights, although I would like to see some pointers about why in Table 3 the Transformer approach (Vaswani et al. 2017) outperforms SliceNet.

I wonder if the proposed approach could be applied to other sequence-to-sequence tasks in NLP or even in speech recognition ? 

Minor comment: 
* The equations are not easy to follow, they should be numbered. The three equations just before Section 2.2 should also be adapted as they seem redundant with Table 1.
","[7, 7, 5]","[' Good paper, accept', ' Good paper, accept', ' Marginally below acceptance threshold']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with positive statements, highlighting the clarity and significance of the paper's contribution (SliceNet architecture). While it raises some questions and suggests improvements, these are presented constructively and aim to enhance the paper. The reviewer acknowledges the value of the work and encourages further exploration. The tone throughout is professional and respectful.",60.0,80.0
Measuring the Intrinsic Dimension of Objective Landscapes,"['Chunyuan Li', 'Heerad Farkhoor', 'Rosanne Liu', 'Jason Yosinski']",Accept,2018,"[8, 1, 9, 9]","[13, 1, 13, 14]","[167, 2, 36, 57]","[71, 1, 20, 26]","[82, 1, 16, 28]","[14, 0, 0, 3]","This paper proposes an empirical measure of the intrinsic dimensionality of a neural network problem. Taking the full dimensionality to be the total number of parameters of the network model, the authors assess intrinsic dimensionality by randomly projecting the network to a domain with fewer parameters (corresponding to a low-dimensional subspace within the original parameter), and then training the original network while restricting the projections of its parameters to lie within this subspace. Performance on this subspace is then evaluated relative to that over the full parameter space (the baseline). As an empirical standard, the authors focus on the subspace dimension that achieves a performance of 90% of the baseline. The authors then test out their measure of intrinsic dimensionality for fully-connected networks and convolutional networks, for several well-known datasets, and draw some interesting conclusions.

Pros:

* This paper continues the recent research trend towards a better characterization of neural networks and their performance. The authors show a good awareness of the recent literature, and to the best of my knowledge, their empirical characterization of the number of latent parameters is original. 

* The characterization of the number of latent variables is an important one, and their measure does perform in a way that one would intuitively expect. For example, as reported by the authors, when training a fully-connected network on the MNIST image dataset, shuffling pixels does not result in a change in their intrinsic dimensionality. For a convolutional network the observed 3-fold rise in intrinsic dimension is explained by the authors as due to the need to accomplish the classification task while respecting the structural constraints of the convnet.

* The proposed measures seem very practical - training on random projections uses far fewer parameters than in the original space (the baseline), and presumably the cost of determining the intrinsic dimensionality would presumably be only a fraction of the cost of this baseline training.

* Except for the occasional typo or grammatical error, the paper is well-written and organized. The issues are clearly identified, for the most part (but see below...).

Cons:

* In the main paper, the authors perform experiments and draw conclusions without taking into account the variability of performance across different random projections. Variance should be taken into account explicitly, in presenting experimental results and in the definition and analysis of the empirical intrinsic dimension itself. How often does a random projection lead to a high-quality solution, and how often does it not?

* The authors are careful to point out that training in restricted subspaces cannot lead to an optimal solution for the full parameter domain unless the subspace intersects the optimal solution region (which in general cannot be guaranteed). In their experiments (FC networks of varying depths and layer widths for the MNIST dataset), between projected and original solutions achieving 90% of baseline performance, they find an order of magnitude gap in the number of parameters needed. This calls into question the validity of random projection as an empirical means of categorizing the intrinsic dimensionality of a neural network.

* The authors then go on to propose that compression of the network be achieved by random projection to a subspace of dimensionality greater than or equal to the intrinsic dimension. However, I don't think that they make a convincing case for this approach. Again, variation is the difficulty: two different projective subspaces of the same dimensionality can lead to solutions that are extremely different in character or quality. How then can we be sure that our compressed network can be reconstituted into a solution of reasonable quality, even when its dimensionality greatly exceeds the intrinsic dimension?

* The authors argue for a relationship between intrinsic dimensionality and the minimum description length (MDL) of their solution, in that the intrinsic dimensionality should serve as an upper bound on the MDL. However they don't formally acknowledge that there is no standard relationship between the number of parameters and the actual number of bits needed to represent the model - it varies from setting to setting, with some parameters potentially requiring many more bits than others. And given this uncertain connection, and given the lack of consideration given to variation in the proposed measure of intrinsic dimensionality, it is hard to accept that ""there is some rigor behind"" their conclusion that LeNet is better than FC networks for classification on MNIST because its empirical intrinsic dimensionality score is lower.

* The experimental validation of their measure of intrinsic dimension could be made more extensive. In the main paper, they use three image datasets - MNIST, CIFAR-10 and ImageNet. In the supplemental information, they report intrinsic dimensions for reinforcement learning and other training tasks on four other sets.

Overall, I think that this characterization does have the potential to give insights into the performance of neural networks, provided that variation across projections is properly taken into account. For now, more work is needed.

====================================================================================================
Addendum:

The authors have revised their paper to take into account the effect of variation across projections, with results that greatly strengthen their results and provide a much better justification of their approach. I'm satisfied too with their explanations, and how they incorporated them into their revised version. I've adjusted my rating of the paper accordingly.

One point, however: the revisions seem somewhat rushed, due to the many typos and grammatical errors in the updated sections. I would like to encourage the authors to check their manuscript once more, very carefully, before finalizing the paper.
====================================================================================================","[7, 7, 6]","[' Good paper, accept', ' Good paper, accept', ' Marginally above acceptance threshold']","[3, 4, 2]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']","The review is initially quite critical, pointing out significant methodological concerns and limitations in the paper's approach. This justifies a sentiment score leaning towards the negative side. However, the addendum reveals a significant improvement in the paper after revisions, addressing the reviewer's main concerns. This suggests the sentiment ultimately shifted to a more positive one. The language used throughout the review is professional and respectful, even when pointing out flaws. There's no use of harsh language or personal attacks, indicating a polite and constructive tone.",50.0,80.0
Imitation Learning from Visual Data with Multiple Intentions,"['Aviv Tamar', 'Khashayar Rohanimanesh', 'Yinlam Chow', 'Chris Vigorito', 'Ben Goodrich', 'Michael Kahane', 'Derik Pridmore']",Accept,2018,"[8, 18, 8, 1, 10, 18, 1]","[13, 23, 13, 3, 13, 18, 1]","[108, 18, 71, 2, 14, 7, 1]","[47, 14, 34, 1, 7, 5, 1]","[57, 3, 32, 1, 6, 0, 0]","[4, 1, 5, 0, 1, 2, 0]","This paper focuses on imitation learning with intentions sampled 
from a multi-modal distribution. The papers encode the mode as a hidden 
variable in a stochastic neural network and suggest stepping around posterior 
inference over this hidden variable (which is generally required to 
do efficient maximum likelihood) with a biased importance 
sampling estimator. Lastly, they incorporate attention for large visual inputs. 

The unimodal claim for distribution without randomness is weak. The distribution 
could be replaced with a normalizing flow. The use of a latent variable 
in this setting makes intuitive sense, but I don't think multimodality motivates it.

Moreover, it really felt like the biased importance sampling approach should be 
compared to a formal inference scheme. I can see how it adds value over sampling 
from the prior, but it's unclear if it has value over a modern approximate inference 
scheme like a black box variational inference algorithm or stochastic gradient MCMC.

How important is using the pretrained weights from the deterministic RNN?

Finally, I'd also be curious about how much added value you get from having 
access to extra rollouts.
","[6, 6, 4]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Ok but not good enough - rejection']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review is a mix of positive and negative points. It starts by summarizing the paper's contributions, which is generally positive. However, it then raises several concerns about the paper's approach and results. The reviewer questions the necessity of certain choices and suggests alternative methods. The use of phrases like ""unclear"" and ""should be compared"" indicates a lack of strong enthusiasm. Overall, the tone is critical but professional and not disrespectful.",-20.0,60.0
Emergent Translation in Multi-Agent Communication,"['Jason Lee', 'Kyunghyun Cho', 'Jason Weston', 'Douwe Kiela']",Accept,2018,"[3, 9, 20, 6]","[6, 14, 25, 11]","[15, 396, 237, 158]","[6, 154, 126, 82]","[8, 215, 84, 72]","[1, 27, 27, 4]","Summary: The authors show that using visual modality as a pivot they can train a model to translate from L1 to L2. 

Please find my detailed comments/questions/suggestions below:

1) IMO, the paper could have been written much better. At the core, this is simply a model which uses images as a pivot for learning to translate between L1 and L2 by learning a common representation space for {L1, image} or {L2, image}. There are several works on such multimodal representation learning but the authors present their work in a way which makes it look very different from these works. IMO, this leads to unnecessary confusion and does more harm than good. For example, the abstract gives an impression that the authors have designed a game to collect data (and it took me a while to set this confusion aside).

2) Continuing on the above point, this is essentially about learning a common multimodal representation and then decode from this common representation. However, the authors do not cite enough work on such multimodal representation learning (for example, look at Spandana et. al.: Image Pivoting for Learning Multilingual Multimodal Representations, EMNLP 2017 for a good set of references)

3) This omission of related work also weakens the experimental section. At least for the word translation task many of these common representation learning frameworks could have been easily evaluated. For example, find the nearest german neighbour of the word ""dog"" in the common representation space. The authors instead compare with very simple baselines.

4) Even when comparing with simple baselines, the proposed model does not convincingly outperform them. In particular,  the P@5 and P@20 numbers are only slightly better. 

5) Some of the choices made in the Experimental setup seem questionable to me:
   - Why  use a NMT model without attention? That is not standard and does not make sense to use when a better baseline model (with attention) is available ?
   - It is mentioned that ""While their model unit-normalizes the output of every encoder, we found this to consistently hurt performance, so do not use normalization for fair comparison with our models."" I don't think this is a fair comparison. The authors can mention their results without normalization if that works well for them but it is not fair to drop normalization from the model of N&N if that gives better performance. Please mention the numbers with unit normalization to give a better picture. It does not make sense to weaken an existing baseline and then compare with it.

6) It would be good to mention the results of the NMT model in Table 1 itself instead of mentioning them separately in a paragraph. This again leads to poor readability and it is hard to read and compare the corresponding numbers from Table 1.  I am not sure why this cannot be accommodated in the Table itself.

7) In Figure 2, what exactly do you mean by ""Results are averaged over 30 translation scenarios"". Can you please elaborate ?","[5, 8, 7]","[' Marginally below acceptance threshold', ' Top 50% of accepted papers, clear accept', ' Good paper, accept']","[5, 5, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']","The review is highly critical of the paper, pointing out several significant weaknesses. The reviewer finds the writing confusing, the related work insufficient, the experimental setup questionable, and the results not convincing. The language used, while direct, maintains a professional and appropriately critical tone expected in a peer review setting. There are no personal attacks or disrespectful language. ",-50.0,50.0
Emergence of grid-like representations by training recurrent neural networks to perform spatial localization,"['Christopher J. Cueva', 'Xue-Xin Wei']",Accept,2018,"[2, 7]","[5, 11]","[6, 13]","[2, 7]","[4, 4]","[0, 2]","This paper aims at better understanding the functional role of grid cells found in the entorhinal cortex by training an RNN to perform a navigation task.

On the positive side: 

This is the first paper to my knowledge that has shown that grid cells arise as a product of a navigation task demand. I enjoyed reading the paper which is in general clearly written. I have a few, mostly cosmetic, complaints but this can easily be addressed in a revision.

On the negative side: 

The manuscript is not written in a way that is suitable for the target ICLR audience which will include, for the most part, readers that are not expert on the entorhinal cortex and/or spatial navigation. 

First, the contributions need to be more clearly spelled out. In particular, the authors tend to take shortcuts for some of their statements. For instance, in the introduction, it is stated that previous attractor network type of models (which are also recurrent networks) “[...] require hand-crafted and fined tuned connectivity patterns, and the evidence of such specific 2D connectivity patterns has been largely absent.” This statement is problematic for two reasons: 

(i) It is rather standard in the field of computational neuroscience to start from reasonable assumptions regarding patterns of neural connectivity then proceed to show that the resulting network behaves in a sensible way and reproduces neuroscience data. This is not to say that demonstrating that these patterns can arise as a byproduct is not important, on the contrary. These are just two complementary lines of work. In the same vein, it would be silly to dismiss the present work simply because it lacks spikes. 

(ii) the authors do not seem to address one of the main criticisms they make about previous work and in particular ""[a lack of evidence] of such specific 2D connectivity patterns"". My understanding is that one of the main assumptions made in previous work is that of a center-surround pattern of lateral connectivity. I would argue that there is a lot of evidence for local inhibitory connection in the cortex. Somewhat related to this point, it would be insightful to show the pattern of local connections learned in the RNN to see how it differs from the aforementioned pattern of connectivity.

Second, the navigation task used needs to be better justified. Why training a network to predict 2D spatial location from velocity inputs? Why is this a reasonable starting point to study the emergence of grid cells? It might be obvious to the authors but it will not be to the ICLR audience. Dead-reckoning (i.e., spatial localization from velocity inputs) is of critical ecological relevance for many animals. This needs to be spelled out and a reference needs to be added.  As a side note, I would have expected the authors to use actual behavioral data but instead, the network is trained using artificial trajectories based on ""modified Brownian motion”. This seems like an important assumption of the manuscript but the issue is brushed off and not discussed. Why is this a reasonable assumption to make? Is there any reference demonstrating that rodent locomotory behavior in a 2D arena is random?

Figure 4 seems kind of strange. I do not understand how the “representative units” are selected and where the “late” selectivity on the far right side in panel a arises if not from “early” units that would have to travel “far” from the left side… Apologies if I am missing something obvious.

I found the study of the effect of regularization to be potentially the most informative for neuroscience but it is only superficially treated. It would have been nice to see a more systematic treatment of the specifics of the regularization needed to get grid cells. ","[8, 9, 8]","[' Top 50% of accepted papers, clear accept', ' Top 15% of accepted papers, strong accept', ' Top 50% of accepted papers, clear accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer provides both positive and negative feedback, but the overall tone suggests that the paper needs significant revisions to be considered for publication. While the reviewer finds the research interesting and well-written, they have serious concerns about the clarity, justification of the methods, and depth of analysis. The criticism regarding the lack of clarity for a broader audience and the need for better justification of the chosen approach suggests a somewhat negative sentiment. However, the reviewer also offers concrete suggestions for improvement, indicating a willingness to see the paper revised and potentially published. ",-20.0,50.0
Memory Augmented Control Networks,"['Arbaaz Khan', 'Clark Zhang', 'Nikolay Atanasov', 'Konstantinos Karydis', 'Vijay Kumar', 'Daniel D. Lee']",Accept,2018,"[3, 2, 7, 7, 30, 23]","[7, 5, 12, 12, 35, 28]","[20, 15, 137, 76, 655, 200]","[5, 7, 53, 34, 369, 130]","[12, 7, 64, 30, 101, 45]","[3, 1, 20, 12, 185, 25]","The paper addresses the important problem of planning in partially observable environments with sparse rewards, and the empirical verification over several domains is convincing. My main concern is that the structure of these domains is very similar - essentially, a graph where only neighboring vertices are directly observable, and because of this, the proposed architecture might not be applicable to planning in general POMDPs (or, in their continuous counterparts, state-space models). The authors claim that what is remembered by the planner does not take the form of a map, but isn't the map estimate \hat{m} introduced at the end of Section 2.1 precisely such a map? From Section 2.4, it appears that these map estimates are essential in computing the low-level policies from which the final, high-level policy is computed. If the ability to maintain and use such local maps is essential for this method, its applicability is likely restricted to this specific geometric structure of domains and their observability. 

Some additional comments:

P. 2, Section 2.1: does H(s) contain 0s for non-observable and 1s for observable states? If yes, please state it.

P. 3: the concatenation of state and observation histories is missing from the definition of the transition function.

P. 3, Eq. 1: overloaded notation - if T is the transition function for the large MDP on histories, it should not be used for the transition function between states. Maybe the authors meant to use f() for that transition?

P. 3, Eq. 3: the sum is over i, but it is not clear what i indexes.

P.3, end of Section 2.1: when computing the map estimate \hat{m}, shouldn't the operator be min, that is, a state is assumed to be open (0), unless one or more observations show that it is blocked (-1)?

P.5: the description of the reward function is inconsistent - is it 0 at the goal state, or >0?

P. 11, above Fig. 9: typo, ""we observe that the in the robot world""
 
 ","[6, 4, 9]","[' Marginally above acceptance threshold', ' Ok but not good enough - rejection', ' Top 15% of accepted papers, strong accept']","[2, 5, 4]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer acknowledges the importance of the problem and finds the empirical results convincing, which points towards a positive sentiment. However, they raise significant concerns about the method's generalizability, suggesting potential limitations. The language used is professional and not rude, indicating a neutral politeness level. Overall, the sentiment leans slightly towards positive due to the acknowledgment of the paper's merits.",60.0,0.0
The High-Dimensional Geometry of Binary Neural Networks,"['Alexander G. Anderson', 'Cory P. Berg']",Accept,2018,"[3, 3]","[7, 3]","[10, 4]","[6, 2]","[4, 2]","[0, 0]","This paper investigates numerically and theoretically the reasons behind the empirical success of binarized neural networks. Specifically, they observe that:

(1) The angle between continuous vectors sampled from a spherical symmetric distribution and their binarized version is relatively small in high dimensions (proven to be about 37 degrees when the dimension goes to infinity), and this demonstrated empirically to be true for the binarized weight matrices of a convenet.

(2) Except the first layer, the dot product of weights*activations in each layer is highly correlated with the dot product of (binarized weights)*activations in each layer. There is also a strong correlation between (binarized weights)*activations and (binarized weights)*(binarized activations). This is claimed to entail that the continuous weights of the binarized neural net approximate the continuous weights of a non-binarized neural net trained in the same manner.

(3) To correct the issue with the first layer in (2) it is suggested to use a random rotation, or simply use continues weights in that layer.

The first observation is interesting, is explained clearly and convincingly, and is novel to the best of my knowledge.

The second observation is much less clear to me. Specifically,
a.	The author claim that “A sufficient condition for \delta u to be the same in both cases is L’(x = f(u)) ~ L’(x = g(u))”. However, I’m not sure if I see why this is true: in a binarized neural net, u also changes, since the previous layers are also binarized. 
b.	Related to the previous issue, it is not clear to me if in figure 3 and 5, did the authors binarize the activations of that specific layer or all the layers? If it is the first case, I would be interested to know the latter: It is possible that if all layers are binarized, then the differences between the binarized and non-binarized version become more amplified.
c.	For BNNs, where both the weights and activations are binarized, shouldn’t we compare weights*activations to (binarized weights)*(binarized activations)?
d.	To make sure, in figure 4, the permutation of the activations was randomized (independently) for each data sample? If not, then C is not proportional the identity matrix, as claimed in section 5.3.
e.	It is not completely clear to me that batch-normalization takes care of the scale constant (if so, then why did XNOR-NET needed an additional scale constant?), perhaps this should be further clarified. 

The third observation seems less useful to me. Though a random rotation may improve angle preservation in certain cases (as demonstrated in Figure 4), it may hurt classification performance (e.g., distinguishing between 6 and 9 in MNIST). Furthermore, since it uses non-binary operations, it is not clear if this rotation may have some benefits (in terms of resource efficiency) over simply keeping the input layer non-binarized.

To summarize, the first part is interesting and nice, the second part was not clear to me, and the last part does not seem very useful. 

%%% After Author's response %%%
a. My mistake. Perhaps it should be clarified in the text that u are the weights. I thought that g(u) is a forward propagation function, and therefore u is the neural input (i.e., pre-activation).

Following the author's response and revisions, I have raised my grade.
","[7, 4, 7]","[' Good paper, accept', ' Ok but not good enough - rejection', ' Good paper, accept']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer finds the first observation interesting and novel, contributing to a positive sentiment. However, they express significant concerns and lack of clarity regarding the second observation, lowering the overall sentiment. While the reviewer finds the third observation less useful, it doesn't seem to drastically impact the sentiment. The language used is polite and professional, focusing on constructive criticism and seeking clarification. The reviewer acknowledges their own misunderstanding and shows openness to the authors' response, indicating a respectful and professional exchange.",20.0,80.0
A Simple Neural Attentive Meta-Learner,"['Nikhil Mishra', 'Mostafa Rohaninejad', 'Xi Chen', 'Pieter Abbeel']",Accept,2018,"[3, 2, 4, 17]","[8, 2, 8, 22]","[12, 5, 47, 608]","[7, 3, 21, 291]","[5, 2, 24, 291]","[0, 0, 2, 26]","The paper proposes a general neural network structure that includes TC (temporal convolution) blocks and Attention blocks for meta-learning, specifically, for episodic task learning. Through intensive experiments on various settings including few-shot image classification on Omniglot and Mini-ImageNet, and four reinforcement learning applications, the authors show that the proposed structure can achieve highly comparable performance wrt the corresponding specially designed state-of-the-art methods. The experiment results seem solid and the proposed structure is with simple design and highly generalizable. The concern is that the contribution is quite incremental from the theoretical side though it involves large amount of experimental efforts, which could be impactful. Please see the major comment below.

One major comment:
- Despite that the work is more application oriented, the paper would have been stronger and more impactful if it includes more work on the theoretical side. 
Specifically, for two folds: 
(1) in general, some more work in investigating the task space would be nice. The paper assumes the tasks are “related” or “similar” and thus transferrable; also particularly in Section 2, the authors define that the tasks follow the same distribution. But what exactly should the distribution be like to be learnable and how to quantify such “related” or “similar” relationship across tasks? 
(2) in particular, for each of the experiments that the authors conduct, it would be nice to investigate some more on when the proposed TC + Attention network would work better and thus should be used by the community; some questions to answer include: when should we prefer the proposed combination of TC + attention blocks over the other methods? The result from the paper seems to answer with “in all cases” but then that always brings the issue of “overfitting” or parameter tuning issue. I believe the paper would have been much stronger if either of the two above are further investigated.

More detailed comments:
- On Page 1, “the optimal strategy for an arbitrary range of tasks” lacks definition of “range”; also, in the setting in this paper, these tasks should share “similarity” or follow the same “distribution” and thus such “arbitrariness” is actually constrained.

- On Page 2, the notation and formulation for the meta-learning could be more mathematically rigid; the distribution over tasks is not defined. It is understandable that the authors try to make the paradigm very generalizable; but the ambiguity or the abstraction over the “task distribution” is too large to be meaningful. One suggestion would be to split into two sections, one for supervised learning and one for reinforcement learning; but both share the same design paradigm, which is generalizable.

- For results in Table 1 and Table 2, how are the confidence intervals computed? Is it over multiple runs or within the same run? It would be nice to make clear; in addition, I personally prefer either reporting raw standard deviations or conduct hypothesis testing with specified tests. The confidence intervals may not be clear without elaboration; such is also concerning in the caption for Table 3 about claiming “not statistically-significantly different” because no significance test is reported. 

- At last, some more details in implementation would be nice (package availability, run time analysis); I suppose the package or the source code would be publicly available afterwards?","[6, 7, 6]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Marginally above acceptance threshold']","[3, 4, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review starts with a positive sentiment, acknowledging the solid experimental results, simple design, and generalizability of the proposed method. However, the reviewer raises a major concern about the theoretical contribution being incremental. While the reviewer acknowledges the practical value of the work, they suggest that a deeper theoretical analysis would significantly strengthen the paper. The language used throughout the review is polite and constructive, offering specific suggestions for improvement.",50.0,70.0
Semi-parametric topological memory for navigation,"['Nikolay Savinov', 'Alexey Dosovitskiy', 'Vladlen Koltun']",Accept,2018,"[5, 6, 19]","[9, 10, 24]","[24, 96, 295]","[10, 41, 135]","[14, 44, 103]","[0, 11, 57]","The paper introduces a graph based memory for navigation agents. The memory graph is constructed using nearest neighbor heuristics based on temporal adjacency and visual similarity. The agent uses Dijkstra's algorithm to plan a a path through the graph in order to solve the navigation task. 

There are several major problems with this paper. My overall impression is that the the proposed agent is a nearly hard-coded solution (which I think might be the correct approach to such problems), but a poorly implemented one. Specific points: 1-There are only 5 test mazes, and the proposed agent doesn't even solve all of them. 2-The way in which the maze is traversed in the exploration phase determines the accuracy of the graph that is constructed (i.e. traversing each location exactly once using a space-filling curve). 3-Of the two heuristics used in Equation 1 how many edges are actually constructed using the visual similarity heuristic? 4-How does the visual similarity heuristic handle visually similar map elements that correspond to distinct locations? 5- The success criteria of solving a maze is arbitrarily defined -- why exactly 2.4 min?   ","[3, 7, 7]","[' Clear rejection', ' Good paper, accept', ' Good paper, accept']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer expresses several strong negative opinions about the paper, using terms like ""major problems"", ""nearly hard-coded"" (presented negatively), ""poorly implemented"", and ""arbitrarily defined."" They also directly challenge the authors' decisions and results. This indicates a negative sentiment. However, the language, while direct and critical, maintains a professional tone without resorting to personal attacks or overly informal language. Therefore, the politeness score is leaning towards neutral.",-70.0,-20.0
Hierarchical Density Order Embeddings,"['Ben Athiwaratkun', 'Andrew Gordon Wilson']",Accept,2018,"[4, 9]","[9, 14]","[21, 163]","[8, 74]","[12, 87]","[1, 2]","The paper presents a study on the use of density embedding for modeling hierarchical semantic relations, and in particular on the hypernym one. The goal is to capture hypernyms of some synsets, even if their occurrence is scarce on the training data.
+++pros: 1) potentially a good idea, capable of filling an ontology of relations scarcely present in a given repository 2) solid theoretical background, even if no methodological novelty has been introduced (this is also a cons!)
---cons: 1) Badly presented: the writing of the paper fails in let the reader aware of what the paper actually serves

COMMENTS:
The introduction puzzled me:  the authors, once they stated the problem (the scarceness of the hypernyms' occurrences in the texts w.r.t. their hyponyms), proposed a solution which seems not to directly solve this problem. So I suggest the authors to better explain the connection between the told problem and their proposed solution, and how this can solve the problem.

This aspect is also present in the experiments section, since it is not possible to understand how much the problem (the scarceness of the hypernyms) is present in the HYPERLEX dataset.

How the 4000 hypernyms have been selected? Why a diagonal covariance has been estimated, and not a full covariance one? 

n Figure 4 middle, it is not clear whether the location and city concepts are intersecting the other synsets. It shouldn't be, but the authors should spend a little on this.

Apart from these comments, I found the paper interesting especially for the big amount fo comparisons carried out. 

As a final general comment, I would have appreciated a paper more self explanative, without referring to the paper [Vilnis & McCallum, 2014] which makes appear the paper a minor improvement of what it is actually. ","[6, 4, 8]","[' Marginally above acceptance threshold', ' Ok but not good enough - rejection', ' Top 50% of accepted papers, clear accept']","[4, 3, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The reviewer provides both positive and negative points about the paper. They acknowledge the good idea and solid background but criticize the presentation and clarity. The reviewer finds the paper interesting, especially the comparisons, indicating a somewhat positive stance. The criticism, though direct, is professional and not rude. Therefore, the sentiment is moderately positive, and the politeness is neutral to slightly positive.",50.0,20.0
MaskGAN: Better Text Generation via Filling in the _______,"['William Fedus', 'Ian Goodfellow', 'Andrew M. Dai']",Accept,2018,"[2, 10, 8]","[7, 12, 13]","[38, 107, 74]","[12, 48, 28]","[24, 55, 42]","[2, 4, 4]","Quality: The work focuses on a novel problem of generating text sample using GAN and a novel in-filling mechanism of words. Using GAN to generate samples in adversarial setup in texts has been limited due to the mode collapse and training instability issues. As a remedy to these problems an in-filling-task conditioning on the surrounding text has been proposed. But, the use of the rewards at every time step (RL mechanism) to employ the actor-critic training procedure could be challenging computationally challenging.

Clarity: The mechanism of generating the text samples using the proposed methodology has been described clearly. However the description of the reinforcement learning step could have been made a bit more clear.

Originality: The work indeed use a novel mechanism of in-filling via a conditioning approach to overcome the difficulties of GAN training in text settings. There has been some work using GAN to generate adversarial examples in textual context too to check the robustness of classifiers. How this current work compares with the existing such literature?

Significance: The research problem is indeed significant since the use of GAN in generating adversarial examples in image analysis has been more prevalent compared to text settings. Also, the proposed actor-critic training procedure via RL methodology is indeed significant from its application in natural language processing.

pros:
(a) Human evaluations applications to several datasets show the usefulness of MaskGen over the maximum likelihood trained model in generating more realistic text samples.
(b) Using a novel in-filling procedure to overcome the complexities in GAN training.
(c) generation of high quality samples even with higher perplexity on ground truth set.

cons:
(a) Use of rewards at every time step to the actor-critic training procure could be computationally expensive.
(b) How to overcome the situation where in-filling might introduce implausible text sequences with respect to the surrounding words?
(c) Depending on the Mask quality GAN can produce low quality samples. Any practical way of choosing the mask?","[7, 7, 7]","[' Good paper, accept', ' Good paper, accept', ' Good paper, accept']","[3, 5, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer provides a balanced perspective, outlining both the strengths (novelty, significance, positive results) and weaknesses (computational cost, potential for implausible sequences, mask quality dependence) of the work. They also pose relevant questions and suggest areas for improvement, indicating engagement with the research. The language is objective and professional throughout.",50.0,75.0
Boundary Seeking GANs,"['R Devon Hjelm', 'Athul Paul Jacob', 'Adam Trischler', 'Gerry Che', 'Kyunghyun Cho', 'Yoshua Bengio']",Accept,2018,"[5, 3, 3, 1, 9, 31]","[9, 7, 8, 1, 14, 36]","[71, 13, 82, 1, 396, 975]","[31, 6, 39, 1, 154, 405]","[37, 7, 43, 0, 215, 454]","[3, 0, 0, 0, 27, 116]","Thank you for the feedback, and I have read the revision.

I would say the revised version has more convincing experimental results (although I'm not sure about the NLP part). The authors have also addressed my concerns on variance reduction, although it's still mysterious to me that the density ratio estimation method seems to work very well even at the begining stage.

Also developing GAN approaches for discrete variables is an important and unsolved problem.

Considering all of the above, I would like to raise the rating to 7, but lower my confidence to 3 (as I'm not an expert for NLP which is the main task for discrete generative models).

==== original review ====

Thank you for an interesting read.

My understanding of the paper is that:

1. the paper proposes a density-ratio estimator via the f-gan approach;
2. the paper proposes a training criterion that matches the generator's distribution to a self-normalised importance sampling (SIS) estimation of the data distribution;
3. in order to reduce the variance of the REINFORCE gradient, the paper seeks out to do matching between conditionals instead.

There are a few things that I expect to see explanations, which are not included in the current version:

1. Can you justify your variance reduction technique either empirically or experimentally? Because your method requires sampling multiple x for a single given z, then in the same wall-clock time I should be able to obtain more samples for the vanilla version eq (8). How do they compare?

2. Why your density ratio estimation methods work in high dimensions, even when at the beginning p and q are so different?

3. It's better to include some quantitative metrics for the image and NLP experiments rather than just showing the readers images and sentences!

4. Over-optimising generators is like solving a max-min problem instead. You showed your method is more robust in this case, can you explain it from the objective you use, e.g. the convex/concavity of your approach in general?

Typo: eq (3) should be min max I believe?

BTW I'm not an expert of NLP so I won't say anything about the quality of the NLP experiment.","[7, 4, 7]","[' Good paper, accept', ' Ok but not good enough - rejection', ' Good paper, accept']","[3, 3, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer provides positive feedback, ultimately raising the rating to 7. They acknowledge improvements in the revised version and find the approach interesting. While they have some remaining questions, their overall tone is constructive and encouraging. The use of phrases like ""interesting read,"" ""thank you for the feedback,"" and ""considering all of the above"" indicates a respectful and positive attitude.",70.0,80.0
Active Learning for Convolutional Neural Networks: A Core-Set Approach,"['Ozan Sener', 'Silvio Savarese']",Accept,2018,"[7, 18]","[12, 23]","[46, 364]","[22, 194]","[19, 143]","[5, 27]","After reading rebuttals from the authors: The authors have addressed all of my concerns. THe additional experiments are a good addition.

************************
The authors provide an algorithm-agnostic active learning algorithm for multi-class classification. The core technique is to construct a coreset of points whose labels inform the labels of other points.  The coreset construction requires one to construct a set of  points which can cover the entire dataset. While this is NP-hard problem in general, the greedy algorithm is 2-approximate. The authors use a variant of the greedy algorithm along with bisection search to solve a series of feasibility problems to obtain a good cover of the dataset each time.  This cover tells us which points are to be queried. The reason why choosing the cover is a good idea is because under suitable Lipschitz continuity assumption the generalization error can be controlled via an appropriate value of the covering radius in the data space.  The authors use the coreset construction with a CNN to demonstrate an active learning algorithm for multi-class classification. 
The experimental results are convincing enough to show that it outperforms other active learning algorithms. However, I have a few major and minor comments.

Major comments:

1. The proof of Lemma 1 is incomplete. We need the Lipschitz constant of the loss function. The loss function is a function of the CNN function and the true label. The proof of lemma 1 only establishes the Lipschitz constant of the CNN function. Some more extra work is needed to derive the lipschitz constant of the loss function from the CNN function. 

2. The statement of Prop 1 seems a bit confusing to me. the hypothsis says that the loss on the coreset = 0. But the equation in proposition 1 also includes the loss on the coreset. Why is this term included. Is this term not equal to 0?

3. Some important works are missing.  Especially works related to pool based active learning, and landmark results on labell complexity of agnostic active learning.
UPAL: Unbiased Pool based active learning by Ganti & Gray. http://proceedings.mlr.press/v22/ganti12/ganti12.pdf
Efficient active learning of half-spaces by Gonen et al. http://www.jmlr.org/papers/volume14/gonen13a/gonen13a.pdf
A bound on the label complexity of agnostic active learning. http://www.machinelearning.org/proceedings/icml2007/papers/375.pdf

4.  The authors use L_2 loss as their objective function. This is a bit of a weird choice given that they are dealing with multi-class classification and the output layer is a sigmoid layer, making it a natural fit to work with something like a cross-entropy loss function. I guess the theoretical results do not extend to cross-entropy loss, but the authors do not mention these points anywhere in the paper. For example, the ladder network, which is one of the networks used by the authors is a network that uses cross-entropy for training.

Minor-comment: 
1. The feasibility program in (6) is an MILP. However, the way it is written it does not look like an MILP. It would have been great had the authors mentioned that u_j \in {0,1}. 

2. The authors write on page 4, ""Moreover, zero training error can be enforced by converting average loss into maximal loss"". It is not clear to me what the authors mean here. For example, can I replace the average error in proposition 1, by maximal loss? Why can I do that? Why would that result in zero training error?

On the whole this is interesting work and the results are very nice. But, the proof for Lemma 1 seems incomplete to me, and some choices (such as choice of loss function) are unjustified. Also, important references in active learning literature are missing.","[7, 7, 7]","[' Good paper, accept', ' Good paper, accept', ' Good paper, accept']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with a positive note acknowledging the authors' effort in addressing the reviewer's concerns. The reviewer then summarizes the paper's contributions and finds the experimental results convincing. However, the review also points out several major and minor issues, including an incomplete proof, confusing statements, missing citations, and questionable choices in the methodology. While acknowledging the work's potential, the reviewer emphasizes the need to address these issues. Therefore, the sentiment is leaning towards the positive side, but not overly positive due to the raised concerns. The language used is formal, respectful, and typical of academic peer reviews.",60.0,80.0
A Deep Reinforced Model for Abstractive Summarization,"['Romain Paulus', 'Caiming Xiong', 'Richard Socher']",Accept,2018,"[5, 10, 12]","[8, 15, 17]","[8, 383, 229]","[4, 165, 111]","[3, 208, 111]","[1, 10, 7]","This is a very clearly written paper, and a pleasure to read.

It combines some mechanisms known from previous work for summarization (intra-temporal attention; pointing mechanism with a switch) with novel architecture design components (intra-decoder attention), as well as a new training objective drawn from work from reinforcement learning, which directly optimizes ROUGE-L. The model is trained by a policy gradient algorithm. 

While the new mechanisms are simple variants of what is taken from existing work, the entire combination is well tested in the experiments. ROUGE results are reported for the full hybrid RL+ML model, as well as various versions that drop each of the new components (RL training; intra-attention). The best method finally outperforms the lea-3d baseline for summarization. What makes this paper more compelling is that they compared against a recent extractive method (Durret et al., 2016), and the fact that they also performed human readability and relevance assessments to demonstrate that their ML+RL model doesn't merely over-optimize on ROUGE. It was a nice result that only optimizing ROUGE directly leads to lower human evaluation scores, despite the fact that that model achieves the best ROUGE-1 and ROUGE-L performance on CNN/Daily Mail.

Some minor points that I wonder about:
 - The heuristic against repeating trigrams seems quite crude. Is there a more sophisticated method that can avoid redundancy without this heuristic?
 - What about a reward based on a general language model, rather than one that relies on L_{ml} in Equation (14)? If the LM part really is to model grammaticality and coherence, a general LM might be suitable as well.
 - Why does ROUGE-L seem to work better than ROUGE-1 or ROUGE-2 as the reward? Do you have any insights are speculations regarding this?","[7, 8, 6]","[' Good paper, accept', ' Top 50% of accepted papers, clear accept', ' Marginally above acceptance threshold']","[5, 3, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with positive framing, highlighting the clarity and novelty of the work. It acknowledges the strengths, such as the well-tested combination of mechanisms and the comprehensive evaluation including human assessments. While it raises some questions and suggestions, these are presented constructively as points for further exploration rather than criticisms. Overall, the tone is positive, acknowledging the paper's contributions and potential.",75.0,90.0
Semantic Interpolation in Implicit Models,"['Yannic Kilcher', 'Aurelien Lucchi', 'Thomas Hofmann']",Accept,2018,"[3, 10, 26]","[8, 15, 31]","[20, 110, 205]","[6, 51, 112]","[13, 52, 69]","[1, 7, 24]","The paper concerns distributions used for the code space in implicit models, e.g. VAEs and GANs. The authors analyze the relation between the latent space dimension and the normal distribution which is commonly used for the latent distribution. The well-known fact that probability mass concentrates in a shell of hyperspheres as the dimensionality grows is used to argue for the normal distribution being sub-optimal when interpolating between points in the latent space with straight lines. To correct this, the authors propose to use a Gamma-distribution for the norm of the latent space (and uniform angle distribution). This results in more mass closer to the origin, and the authors show both that the midpoint distribution is natural in terms of the KL divergence to the data points, and experimentally that the method gives visually appealing interpolations.

While the contribution of using a standard family of distributions in a standard implicit model setup is limited, the paper does make interesting observations, analyses and an attempt to correct the interpolation issue. The paper is clearly written and presents the theory and experimental results nicely. I find that the paper can be accepted but the incremental nature of the contribution prevents a higher score.","[6, 7, 5]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Marginally below acceptance threshold']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer acknowledges the limited contribution but finds the observations, analysis, and attempt to correct the interpolation issue interesting. The reviewer also mentions the clear writing and nice presentation. The statement about accepting the paper with a limited score suggests overall positive sentiment, but not overly enthusiastic. The language used is polite and professional throughout.",50.0,80.0
Active Neural Localization,"['Devendra Singh Chaplot', 'Emilio Parisotto', 'Ruslan Salakhutdinov']",Accept,2018,"[2, 12, 19]","[3, 16, 24]","[2, 16, 195]","[2, 11, 120]","[0, 2, 30]","[0, 3, 45]","I have evaluated this paper for NIPS 2017 and gave it an ""accept"" rating at the time, but the paper was ultimately not accepted. This resubmission has been massively improved and definitely deserves to be published at ICLR.

This paper formulates the problem localisation on a known map using a belief network as an RL problem. The goal of the agent is to minimise the number of steps to localise itself (the agent needs to move around to accumulate evidence about its position), which corresponds to reducing the entropy of the joint distribution over a discretized grid over theta (4 orientations), x and y. The model is evaluated on a grid world, on textured 3D mazes with simplified motion (Doom environment) and on a photorealistic environment using the Unreal engine. Optimisation is done through A3C RL. Transfer from the crude simulated Doom environment to the photorealistic Unreal environment is achieved.

The belief network consists of an observation model, a motion prediction model that allows for translations along x or y and 90deg rotation, and an observation correction model that either perceives the depth in front of the agent (a bold and ambiguous choice) and matches it to the 2D map, or perceives the image in front of the agent. The map is part of the observation.

The algorithm outperforms Bayes filters for localisation in 2D and 3D and the idea of applying RL to minimise the entropy of position estimation is brilliant. Minor note: I am surprised that the cognitive map reference (Gupta et al, 2017) was dropped, as it seemed relevant.","[8, 7, 6]","[' Top 50% of accepted papers, clear accept', ' Good paper, accept', ' Marginally above acceptance threshold']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with a positive note, highlighting the improvement since the last submission and advocating for acceptance. The reviewer finds the paper's core idea ""brilliant"" and praises the algorithm's performance. While there's a minor suggestion, it's presented constructively. Overall, the tone is encouraging and appreciative.",85.0,90.0
Critical Percolation as a Framework to Analyze the Training of Deep Networks,"['Zohar Ringel', 'Rodrigo Andrade de Bem']",Accept,2018,"[2, 16]","[5, 19]","[9, 15]","[2, 12]","[7, 2]","[0, 1]","This paper thoroughly analyzes an algorithmic task (determining if two points in a maze are connected, which requires BFS to solve) by constructing an explicit ConvNet solution and analytically deriving properties of the loss surface around this analytical solution. They show that their analytical solution implements a form of BFS algorithm, characterize the probability of introducing ""bugs"" in the algorithm as the weights move away from the optimal solution, and how this influences the error surface for different depths. This analysis is conducted by drawing on results from the field of critical percolation in physics.

Overall, I think this is a good paper and its core contribution is definitely valuable: it provides a novel analysis of an algorithmic task which sheds light on how and when the network fails to learn the algorithm, and in particular the role which initialization plays. The analysis is very thorough and the methods described may find use in analyzing other tasks. In particular, this could be a first step towards better understanding the optimization landscape of memory-augmented neural networks (Memory Networks, Neural Turing Machines, etc) which try to learn reasoning tasks or algorithms. It is well-known that these are sensitive to initialization and often require running the optimizer with multiple random seeds and picking the best one. This work actually explains the role of initialization for learning BFS and how certain types of initialization lead to poor solutions. I am curious if a similar analysis could be applied to methods evaluated on the bAbI question-answering tasks (which can be represented as graphs, like the maze task) and possibly yield better initialization or optimization schemes that would remove the need for multiple random seeds.  

With that being said, there is some work that needs to be done to make the paper clearer. In particular, many parts are quite technical and may not be accessible to a broader machine learning audience. It would be good if the authors spent more time developing intuition (through visualization for example) and move some of the more technical proofs to the appendix. Specifically:
- I think Figure 3 in the appendix should be moved to the main text, to help understand the behavior of the analytical solution. 
- Top of page 5, when you describe the checkerboard BFS: please include a visualization somewhere, it could be in the Appendix.
- Section 6: there is lots of math here, but the main results don't obviously stand out. I would suggest highlighting equations 2 and 4 in some way (for example, proposition/lemma + proof), so that the casual reader can quickly see what the main results are. Interested readers can then work through the math if they want to. Also, some plots/visualizations of the loss surface given in Equations 4 and 5 would be very helpful. 

Also, although I found their work to be interesting after finishing the paper, I was initially confused by how the authors frame their work and where the paper was heading. They claim their contribution is in the analysis of loss surfaces (true) and neural nets applied to graph-structured inputs. This second part was confusing - although the maze can be viewed as a graph, many other works apply ConvNets to maze environments [1, 2, 3], and their work has little relation to other work on graph CNNs. Here the assumptions of locality and stationarity underlying CNNs are sensible and I don't think the first paragraph in Section 3 justifying the use of the CNN on the maze environment is necessary. However, I think it would make much more sense to mention how their work relates to other neural network architectures which learn algorithms (such as the Neural Turing Machine and variants) or reasoning tasks more generally (for example, memory-augmented networks applied to the bAbI tasks). 

There are lots of small typos, please fix them. Here are a few:
- ""For L=16, batch size of 20, ..."": not a complete sentence. 
- Right before 6.1.1: ""when the these such"" -> ""when such""
- Top of page 8: ""it also have a"" -> ""it also has a"", ""when encountering larger dataset"" -> ""...datasets""
-  First sentence of 6.2: ""we turn to the discuss a second"" -> ""we turn to the discussion of a second""
- etc. 

Quality: High
Clarity: medium-low
Originality: high
Significance: medium-high

References:
[1] https://arxiv.org/pdf/1602.02867.pdf
[2] https://arxiv.org/pdf/1612.08810.pdf
[3] https://arxiv.org/pdf/1707.03497.pdf","[7, 7, 6]","[' Good paper, accept', ' Good paper, accept', ' Marginally above acceptance threshold']","[3, 3, 1]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', "" The reviewer's evaluation is an educated guess""]","The reviewer clearly states that this is a ""good paper"" and finds the core contribution valuable. They are impressed by the thorough analysis and see potential for the methods to be applied in other areas of research. The reviewer also expresses curiosity and interest in further applications of the research. While they point out areas for improvement, these are largely focused on clarity and accessibility for a broader audience. The reviewer acknowledges the technical complexity but suggests ways to make the findings clearer, indicating a positive view of the work's potential. ",75.0,75.0
Improving Search Through A3C Reinforcement Learning Based Conversational Agent,"['Milan Aggarwal', 'Aarushi Arora', 'Shagun Sodhani', 'Balaji Krishnamurthy']",Reject,2018,"[2, 2, 4, 31]","[7, 2, 9, 36]","[22, 2, 42, 97]","[8, 1, 12, 45]","[14, 1, 27, 51]","[0, 0, 3, 1]","The paper describes reinforcement learning techniques for digital asset search.  The RL techniques consist of A3C and DQN.  This is an application paper since the techniques described already exist.  Unfortunately, there is a lack of detail throughout the paper and therefore it is not possible for someone to reproduce the results if desired.  Since there is no corpus of message response pairs to train the model, the paper trains a simulator from logs to emulate user behaviours.  Unfortunately, there is no description of the algorithm used to obtain the simulator.  The paper explains that the simulator is obtained from log data, but this is not sufficient.  The RL problem is described at a very high level in the sense that abstract states and actions are listed, but there is no explanation about how those abstract states are recognized from the raw text and there is no explanation about how the actions are turned into text.  There seems to be some confusion in the notion of state.  After describing the abstract states, it is explained that actions are selected based on a history of states.  This suggests that the abstract states are really abstract observations.   In fact, this becomes obvious when the paper introduces the RNN where a hidden belief is computed by combining the observations.  The rewards are also described at a hiogh level, but it is not clear how exactly they are computed.  The digital search application is interesting, however a detailed description with comprehensive experiments are needed for the publication of an application paper.","[3, 5, 2]","[' Clear rejection', ' Marginally below acceptance threshold', ' Strong rejection']","[5, 4, 5]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review is critical of the paper, pointing out a lack of detail that makes it difficult to reproduce the results. The reviewer finds the application interesting but ultimately deems the paper insufficient for publication in its current form. The language used, while direct, is professional and within the bounds of academic discourse.",-50.0,50.0
Seq2SQL: Generating Structured Queries From Natural Language Using Reinforcement Learning ,"['Victor Zhong', 'Caiming Xiong', 'Richard Socher']",Reject,2018,"[4, 10, 12]","[8, 15, 17]","[37, 383, 229]","[18, 165, 111]","[19, 208, 111]","[0, 10, 7]","The authors have addressed the problem of translating natural language queries to SQL queries. They proposed a deep neural network based solution which combines the attention based neural semantic parser and pointer networks. They also released a new dataset WikiSQL for the problem. The proposed method outperforms the existing semantic parsing baselines on WikiSQL dataset.

Pros:
1. The idea of using pointer networks for reducing search space of generated queries is interesting. Also, using extrinsic evaluation of generated queries handles the possibility of paraphrasing SQL queries.
2. A new dataset for the problem.
3. The experiments report a significant boost in the performance compared to the baseline. The ablation study is helpful for understanding the contribution of different component of the proposed method.

Cons:
1. It would have been better to see performance of the proposed method in other datasets (wherever possible). This is my main concern about the paper.
2. Extrinsic evaluation can slow down the overall training. Comparison of running times would have been helpful.
3. More details about training procedure (specifically for the RL part) would have been better.","[5, 4, 5]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review is positive overall. The reviewer highlights the strengths of the paper, such as the novelty of the approach, the new dataset, and the strong experimental results. While the reviewer mentions some concerns, these are presented as constructive suggestions for improvement rather than strong criticisms. The language used is polite and professional throughout.",65.0,80.0
Learning to select examples for program synthesis,"['Yewen Pu', 'Zachery Miranda', 'Armando Solar-Lezama', 'Leslie Pack Kaelbling']",Reject,2018,"[8, 2, 14, 33]","[13, 2, 19, 38]","[32, 4, 157, 298]","[15, 2, 81, 168]","[15, 2, 49, 91]","[2, 0, 27, 39]","The paper proposes a method for identifying representative examples for program
synthesis to increase the scalability of existing constraint programming
solutions. The authors present their approach and evaluate it empirically.

The proposed approach is interesting, but I feel that the experimental section
does not serve to show its merits for several reasons. First, it does not
demonstrate increased scalability. Only 1024 examples are considered, which is
by no means large. Even then, the authors approach selects the highest number of
examples (figure 4). CEGIS both selects fewer examples and has a shorter median
time for complete synthesis. Intuitively, the authors' method should scale
better, but they fail to show this -- a missed opportunity to make the paper
much more compelling. This is especially true as a more challenging benchmark
could be created very easily by simply scaling up the image.

Second, there is no analysis of the representativeness of the found sets of
constraints. Given that the results are very close to other approaches, it
remains unclear whether they are simply due to random variations, or whether the
proposed approach actually achieves a non-random improvement.

In addition to my concerns about the experimental evaluation, I have concerns
about the general approach. It is unclear to me that machine learning is the
best approach for modeling and solving this problem. In particular, the
selection probability of any particular example could be estimated through a
heuristic, for example by simply counting the number of neighbouring examples
that have a different color, weighted by whether they are in the set of examples
already, to assess its ""borderness"", with high values being more important to
achieve a good program. The border pixels are probably sufficient to learn the
program perfectly, and in fact this may be exactly what the neural net is
learning. The above heuristic is obviously specific to the domain, but similar
heuristics could be easily constructed for other domains. I feel that this is
something the authors should at least compare to in the empirical evaluation.

Another concern is that the authors' approach assumes that all parameters have
the same effect. Even for the example the authors give in section 2, it is
unclear that this would be true.

The text says that rand+cegis selects 70% of examples of the proposed approach,
but figure 4 seems to suggest that the numbers are very close -- is this initial
examples only?

Overall the paper appears rushed -- the acknowledgements section is left over
from the template and there is a reference to figure ""blah"". There are typos and
grammatical mistakes throughout the paper. The reference to ""Model counting"" is
incomplete.

In summary, I feel that the paper cannot be accepted in its current form.","[4, 5, 5]","[' Ok but not good enough - rejection', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer expresses interest in the proposed approach but raises significant concerns about the experimental section and the general approach. They find the experiments unconvincing due to small scale and lack of analysis of representativeness. The reviewer also suggests alternative approaches and points out specific issues like unclear statements and typos, indicating a rushed paper.",-20.0,50.0
Adversarial Learning for Semi-Supervised Semantic Segmentation,"['Wei-Chih Hung', 'Yi-Hsuan Tsai', 'Yan-Ting Liou', 'Yen-Yu Lin', 'Ming-Hsuan Yang']",Reject,2018,"[30, 11, 7, 11]","[35, 16, 12, 16]","[203, 205, 78, 156]","[79, 112, 43, 96]","[18, 6, 2, 5]","[106, 87, 33, 55]","This paper describes techniques for training semantic segmentation networks. There are two key ideas:

- Attach a pixel-level GAN loss to the output semantic segmentation map. That is, add a discriminator network that decides whether each pixel in the label map belongs to a real label map or not. Of course, this loss alone is unaware of the input image and would drive the network to produce plausible label maps that have no relation to the input image. An additional cross-entropy loss (the standard semantic segmentation loss) is used to tie the network to the input and the ground-truth label map, when available.

- Additional unlabeled data is utilized by using a trained semantic segmentation network to produce a label map with associated confidences; high-confidence pixels are used as ground-truth labels and are fed back to the network as training data.

The paper is fine and the work is competently done, but the experimental results never quite come together. The technical development isn’t surprising and doesn’t have much to teach researchers working in the area. Given that the technical novelty is rather light and the experimental benefits are not quite there, I cannot recommend the paper for publication in a first-tier conference.

Some more detailed comments:

1. The GAN and the semi-supervised training scheme appear to be largely independent. The GAN can be applied without any unlabeled data, for example. The paper generally appears to present two largely independent ideas. This is fine, except they don’t convincingly pan out in experiments.

2. The biggest issue is that the experimental results do not convincingly indicate that the presented ideas are useful.
2a. In the “Full” condition, the presented approach does not come close to the performance of the DeepLab baseline, even though the DeepLab network is used in the presented approach. Perhaps the authors have taken out some components of the DeepLab scheme for these experiments, such as multi-scale processing, but the question then is “Why?”. These components are not illegal, they are not cheating, they are not overly complex and are widely used. If the authors cannot demonstrate an improvement with these components, their ideas are unlikely to be adopted in state-of-the-art semantic systems, which do use these components and are doing fine.
2b. In the 1/8, 1/4, and 1/2 conditions, the performance of the baselines is not quoted. This is wrong. Since the authors are evaluating on the validation sets, there is no reason not to train the baselines on the same amount of labeled data (1/8, 1/4, 1/2) and report the results. The training scripts are widely available and such training of baselines for controlled experiments is commonly done in the literature. The reviewer is left to suspect, with no evidence given to the contrary, that the presented approach does not outperform the DeepLab baseline even in the reduced-data conditions.

A somewhat unflattering view of the work would be that this is another example of throwing a GAN at everything to see if it sticks. In this case, the experiments do not indicate that it did.","[5, 5, 5]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with neutral language but clearly states that the paper's experimental results are not convincing and the technical novelty is limited. The reviewer finds the paper ""fine"" and ""competently done"" but ultimately suggests rejection. The detailed comments highlight the lack of strong results and methodological issues. The final sentence, comparing the work to ""throwing a GAN at everything,"" is critical, suggesting a lack of impactful contribution. Therefore, the sentiment leans towards the negative side, although the language remains largely polite and professional.",-30.0,60.0
Ground-Truth Adversarial Examples,"['Nicholas Carlini', 'Guy Katz', 'Clark Barrett', 'David L. Dill']",Reject,2018,"[7, 8, 23, 33]","[12, 13, 28, 37]","[110, 97, 203, 174]","[44, 52, 121, 135]","[66, 38, 56, 7]","[0, 7, 26, 32]","The paper describes a method for generating so called ground truth adversarial examples: adversaries that have minimal (L1 or L_inf) distance to the training example used to generate them. The technique uses the recently developed reluplex, which can be used to verify certian properties of deep neural networks that use ReLU activations. The authors show how the L1 distance can be formulated using a ReLU and therefore extend the reluplex also work with L1 distances. The experiments on MNIST suggest that the C&W attack produces close to optimal adversarial examples, although it is not clear if these findings would transfer to larger more complex networks. The evaluation also suggests that training with iterative adversarial examples does not overfit and does indeed harden the network to attacks in many cases.

In general, this is a nice idea, but it seems like the inherent computational cost will limit the applicability of this approach to small networks and datasets for the time being. Incidentally, it would have been useful if the authors provided indicative information on the computational cost (e.g. in the form of time on a standard GPU) for generating these ground truths and carrying out experiments.

The experiments are quite small scale, which I expect is due to the computational cost of generating the adversarial examples. It is difficult to say how far the findings can be generalized from MNIST to more realistic situations. Tests on another dataset would have been welcomed.

Also, while interesting, are adversarial examples that have minimal L_p distance from training examples really that useful in practice? Of course, it's nice that we can find these, but it could be argued that L_p norms are not a good way of judging the similarity of an adversarial example to a true example. I think it would be more useful to investigate attacks that are perceptually insignificant, or attacks that operate in the physical world, as these are more likely to be a concern for real world systems. 

In summary, while I think the paper is interesting, I suspect that the applicability of this technique is possibly limited at present, and I'm unsure how much we can really read into the findings of the paper when the experiments are based on MNIST alone.
","[6, 4, 5]","[' Marginally above acceptance threshold', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer finds the paper interesting and presents a nice idea (positive cues). However, they also express concerns about the computational cost, limited applicability, and the reliance on MNIST. They suggest further research directions and question the practical usefulness of the current approach. Overall, this suggests a somewhat positive but cautious sentiment. The language used is polite and constructive throughout the review, without resorting to harsh or negative phrasing.",50.0,80.0
Generation and Consolidation of Recollections for Efficient Deep Lifelong Learning,"['Matt Riemer', 'Michele Franceschini', 'and Tim Klinger']",Reject,2018,"[4, 14, 15]","[9, 15, 20]","[46, 36, 38]","[20, 25, 22]","[25, 3, 15]","[1, 8, 1]","The paper proposes an architecture for efficient deep lifelong learning. The key idea is to use recollection generator (autoencoder) to remember the previously processed data in a compact representation. Then when training a reasoning model, recollections generated from the recollection generator are used with real-world examples as input data. Using the recollection, it can avoid forgetting previous data. In the experiments, it has been shown that the proposed approach is efficient for transfer knowledge with small data compared to random sampling approach.

It is an interesting idea to remember previous examples using the compact representation from autoencoder and use it for transfer learning. However, I think the paper would be improved if the following points are clarified.

1. It seems that reconstructed data from autoencoder does not contain target values. It is not clear to me how the reasoning model can use the reconstructed data (recollections) for supervised learning tasks. 

2. It seems that the proposed framework can be better presented as a method for data compression for deep learning. Ideally, for lifelong learning, the reasoning model should not forget previously learned kwnoledge embeded in their weights. 
However, under the current architecture, it seems that the reasoning model does not have such mechanisms.

3. For lifelong learning, it would be interesting to test if the same reasoning model can deal with increasing number of tasks from different datasets using the recollection mechanisms.

 



","[5, 5, 5]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[2, 3, 3]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer finds the core idea interesting and acknowledges the efficiency of the proposed approach. However, they also raise several concerns and suggest areas of improvement, indicating a mixed sentiment leaning towards positive. The language used is constructive and professional, suggesting politeness.",50.0,75.0
Trace norm regularization and faster inference for embedded speech recognition RNNs,"['Markus Kliegl', 'Siddharth Goyal', 'Kexin Zhao', 'Kavya Srinet', 'Mohammad Shoeybi']",Reject,2018,"[2, 11, 17, 2, 13]","[1, 16, 22, 7, 18]","[4, 26, 63, 15, 46]","[1, 13, 21, 4, 16]","[3, 10, 5, 11, 27]","[0, 3, 37, 0, 3]","Paper is well written and clearly explained. The paper is a experimental paper as it has more content on the experimentation and less content on problem definition and formulation. The experimental section is strong and it has evaluated across different datasets and various scenarios. However, I feel the contribution of the paper toward the topic is incremental and not significant enough to be accepted in this venue. It only considers a slight modification into the loss function by adding a trace norm regularization.","[5, 4, 5]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[3, 3, 5]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review starts with positive statements about the paper's clarity and experimental section. However, it delivers a negative sentiment by stating the contribution is ""incremental"" and ""not significant enough."" The language used is factual and professional throughout.",-20.0,50.0
Comparison of Paragram and GloVe Results for Similarity Benchmarks,"['Jakub Dutkiewicz', 'Czesław Jędrzejek']",Reject,2018,"[6, 21]","[8, 23]","[17, 41]","[15, 37]","[1, 1]","[1, 3]","This paper proposes a ranking-based similarity metric for distributional semantic models. The main idea is to learn ""baseline"" word embeddings, retrofitting those and applying localized centering, to then calculate similarity using a measure called ""Ranking-based Exponential Similarity Measure"" (RESM), which is based on the recently proposed APSyn measure.

I think the work has several important issues:

1. The work is very light on references. There is a lot of previous work on evaluating similarity in word embeddings (e.g. Hill et al, a lot of the papers in RepEval workshops, etc.); specialization for similarity of word embeddings (e.g. Kiela et al., Mrksic et al., and many others); multi-sense embeddings (e.g. from Navigli's group); and the hubness problem (e.g. Dinu et al.). For the localized centering approach, Hara et al.'s introduced that method. None of this work is cited, which I find inexcusable. 

2. The evaluation is limited, in that the standard evaluations (e.g. SimLex would be a good one to add, as well as many others, please refer to the literature) are not used and there is no comparison to previous work. The results are also presented in a confusing way, with the current state of the art results separate from the main results of the paper. It is unclear what exactly helps, in which case, and why. 

3. There are technical issues with what is presented, with some seemingly factual errors. For example, ""In this case we could apply the inversion, however it is much more convinient [sic] to take the negative of distance. Number 1 in the equation stands for the normalizing, hence the similarity is defined as follows"" - the 1 does not stand for normalizing, that is the way to invert the cosine distance (put differently, cosine distance is 1-cosine similarity, which is a metric in Euclidean space due to the properties of the dot product). Another example, ""are obtained using the GloVe vector, not using PPMI"" - there are close relationships between what GloVe learns and PPMI, which the authors seem unaware of (see e.g. the GloVe paper and Omer Levy's work). 

4. Then there is the additional question, why should we care? The paper does not really motivate why it is important to score well on these tests: these kinds of tests are often used as ways to measure the quality of word embeddings, but in this case the main contribution is the similarity metric used *on top* of the word embeddings. In other words, what is supposed to be the take-away, and why should we care?

As such, I do not recommend it for acceptance - it needs significant work before it can be accepted at a conference.

Minor points:
- Typo in Eq 10
- Typo on page 6 (/cite instead of \cite)","[4, 2, 3]","[' Ok but not good enough - rejection', ' Strong rejection', ' Clear rejection']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer outlines several major issues with the paper, including lack of references, limited evaluation, technical errors, and lack of clear motivation. They explicitly recommend against acceptance, indicating a negative sentiment. While the reviewer points out the flaws directly, the language used is framed within academic discourse and maintains a professional tone.",-50.0,50.0
Optimal transport maps for distribution preserving operations on latent spaces of Generative Models,"['Eirikur Agustsson', 'Alexander Sage', 'Radu Timofte', 'Luc Van Gool']",Reject,2018,"[5, 2, 10, 35]","[10, 3, 15, 40]","[56, 4, 468, 1396]","[29, 2, 250, 801]","[25, 2, 194, 420]","[2, 0, 24, 175]","This paper is concerned with the mismatch between the input distribution used for training and interpolated input. It extends the discussion on this phenomenon and the correction method proposed by White (2016), and proposes an optimal transport-based approach, which essentially makes use of the trick of change of variables. The discussion of the phenomenon is interesting, and the proposed method seems well motivated and useful. There are a number of errors or inconsistencies in the paper, and the experiments results, compared to those given by SLERP, see rather weak. My big concern about the paper is that it seems to be written in a rush and needs a lot of improvement before being published. Below please see more detailed comments.

- In Introduction, the authors claim that ""This is problematic, since the generator G was trained on a fixed prior and expects to see inputs with statistics consistent with that distribution."" Here the learned generative network might still apply even if the input distribution changes (e.g., see the covariate shift setting); should one claim that the support of the test input distribution may not be contained in the support of the input distribution for training? Is there any previous result supporting this? 
- Moreover, I am wondering whether Sections 2.2 and 2.3 can be simplified or improved--the underlying idea seems intuitive, but some of the statements seem somewhat confusing. For instance, what does equation (6) mean?
- Note that a parenthesis is missing in line 3 below (4). In (6), the dot should follow the equation.
- Line 1 of page 7: here it would be nice to make it clear what p_{y|x} means. How did you obtain values of f(x) from this conditional distribution?
- Theorem 2: here does one assume that F_Y is invertible? (Maybe this is not necessary according to the definition of F_Y^{[-1]}...)
- Line 4 above Section 4.2: the sentence is not complete.
- Section 4.2: It seems that Figure 3 appears in the main text earlier than Figure 2. Please pay attention to the organization.
- Line 3, page 10: ""slightly different, however...""
- Line 3 below Figure 2: I failed to see ""a slight loss in detain for the SLERP version."" Perhaps the authors could elaborate on it?
- The paragraph above Figure 3 is not complete.","[6, 6, 4]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Ok but not good enough - rejection']","[3, 4, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer finds the paper's topic interesting and well-motivated. However, they also point out numerous errors, inconsistencies, and a need for overall improvement. The reviewer's concern about the rushed writing and the weakly supported experimental results point towards a more negative sentiment. The language used, while direct and critical, maintains a professional and polite tone. The numerous detailed recommendations exemplify this politeness by offering constructive criticism.",-20.0,60.0
Tandem Blocks in Deep Convolutional Neural Networks,"['Chris Hettinger', 'Tanner Christensen', 'Jeff Humpherys', 'Tyler J Jarvis']",Reject,2018,"[2, 2, 13, 15]","[2, 2, 16, 19]","[3, 4, 22, 7]","[0, 2, 9, 0]","[3, 2, 3, 5]","[0, 0, 10, 2]","This paper performs an analysis of shortcut connections in ResNet-like architectures. The authors hypothesize that the success of shortcut connections comes from the combination of linear and non-linear features at each layer and propose to substitute the identity shortcuts with a convolutional one (without non-linearity). This alternative is referred to as tandem block. Experiments are performed on a variety of image classification tasks such as CIFAR-10, CIFAR-100, SVHN and Fashion MNIST.

The paper is well structured and easy to follow. The main contribution of the paper is the comparison between identity skip connections and skip connections with one convolutional layer.

My main concerns are related to the contribution of the paper and experimental pipeline followed to perform the comparison. First, the idea of having convolutional shortcuts was already explored in the ResNet paper (see https://arxiv.org/pdf/1603.05027.pdf). Second, given Figures 3-4-5-6, it would seem that the authors are monitoring the performance on the test set during training. Moreover, results on Table 2 are reported as the ones with “the highest test accuracy achieved with each tandem block”. Could the authors give more details on how the hyperparameters of the architectures/optimization were chosen and provide more information on how the best results were achieved?

In section 3.5, the authors mention that batchnorm was not useful in their experiments, and was more sensitive to the learning rate value. Do the authors have any explanation/intuition for this behavior?

In section 4, authors claim that their results are competitive with the best published results for a similar number of parameters. It would be beneficial to add the mentioned best performing models in Table 2 to back this statement. Moreover, it seems that in some cases such as SVHN the differences between all the proposed blocks are too minor to draw any strong conclusions. Could those differences be due to, for example, luck in picking the initialization seed? How many times was each experiment run? If more than once, what was the std?

The experiments were performed on relatively shallow networks (8 to 26 layers). I wonder how the conclusions drawn scale to much deeper networks (of 100 layers for example) and on larger datasets such as ImageNet.

Figures 3-5 are not referenced nor discussed in the text.

Following the design of the tandem blocks proposed in the paper, I wonder why the tandem block B3x3(2,w) was not included.

Finally, it might be interesting to initialize the convolutions in the shortcut connections with the identity, and check what they have leant at the end of the training.

Some typos that the authors might want to fix:

- backpropegation -> backpropagation (Introduction, paragraph 3)
- dropout is a kind of regularization as well (Introduction, second to last paragraph)
- nad -> and (Sect 3.1. paragraph 1)
","[5, 4, 7]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Good paper, accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is cautiously positive. The reviewer acknowledges the paper's clarity and structure but raises significant concerns about the novelty of the contribution and the experimental methodology. The reviewer points out prior work exploring similar concepts and questions the rigor of the experimental setup and result interpretation. The reviewer provides constructive criticism and suggestions for improvement, indicating a willingness to see the paper strengthened rather than outright rejected. The language is polite and professional throughout, employing a neutral tone and focusing on specific issues within the research.",30.0,80.0
Entropy-SGD optimizes the prior of a PAC-Bayes bound: Data-dependent PAC-Bayes priors via differential privacy,"['Gintare Karolina Dziugaite', 'Daniel M. Roy']",Reject,2018,"[4, 29]","[9, 34]","[51, 108]","[21, 54]","[30, 49]","[0, 5]","1) I would like to ask for the clarification regarding the generalization guarantees. The original Entropy-SGD paper shows improved generalization over SGD using uniform stability, however the analysis of the authors rely on an unrealistic assumption regarding the eigenvalues of the Hessian (they are assumed to be away from zero, which is not true at least at local minima of interest). What is the enabling technique in this submission that avoids taking this assumption? (to clarify: the analysis is all-together different in both papers, however this aspect of the analysis is not fully clear to me).
2) It is unclear to me what are the unrealistic assumptions made in the paper. Please, list them all in one place in the paper and discuss in details.
","[6, 6, 6]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[3, 3, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review is rather critical of the submitted work, pointing out potential flaws in the paper's argumentation and demanding clarification. The language used, however, is framed as a request for clarification and avoids harsh or accusatory tones, suggesting a professional and polite approach.",-50.0,70.0
Graph2Seq: Scalable Learning Dynamics for Graphs,"['Shaileshh Bojja Venkatakrishnan', 'Mohammad Alizadeh', 'Pramod Viswanath']",Reject,2018,"[5, 9, 23]","[10, 14, 28]","[37, 159, 262]","[14, 92, 104]","[17, 53, 99]","[6, 14, 59]","This paper proposes a novel way of embedding graph structure into a sequence that can have an unbounded length. 

There has been a significant amount of prior work (e.g. d graph convolutional neural networks) for signals supported on a specific graph. This paper on the contrary tries to encode the topology of a graph using a dynamical system created by the graph and randomization. 

The main theorem is that the created dynamical system can be used to reverse engineer the graph topology for any digraph. 
As far as I understood, the authors are doing essentially reverse directed graphical model learning. In classical learning of directed graphical models (or causal DAGs) one wants to learn the structure of a graph from observed data created by this graph inducing conditional independencies on data. This procedure is creating a dynamical system that (following very closely previous work) estimates conditional directed information for every pair of vertices u,v and can find if an edge is present from the observed trajectory. 
The recovery algorithm is essentially previous work (but the application to graph recovery is new).

The authors state:
``Estimating conditional directed information efficiently from samples is itself an active area of research Quinn et al. (2011), but simple plug-in estimators with a standard kernel density estimator will be consistent.''

One thing that is missing here is that the number of samples needed could be exponential in the degrees of the graph. Therefore, it is not clear at all that high-dimensional densities or directed information can be estimated from a number of samples that is polynomial in the dimension (e.g. graph degree).

This is related to the second limitation, that there is no sample complexity bounds presented only an asymptotic statement. 

One remark is that there are many ways to represent a finite graph with a sequence that can be decoded back to the graph (and of course if there is no bound on the graph size, there will be no bound on the size of the sequence). For example, one could take the adjacency matrix and sequentially write down one row after the other (perhaps using a special symbol to indicate 'next row'). Many other simple methods can be obtained also, with a size of sequence being polynomial (in fact linear) in the size of the graph. I understand that such trivial representations might not work well with RNNs but they would satisfy stronger versions of Theorem 1 with optimal size. 
On the contrary it was not clear how the proposed sequence will scale in the graph size. 


Another remark is that it seems that GCNN and this paper solve different problems. 
GCNNs want to represent graph-supported signals (on a fixed graph) while this paper tries to represent the topology of a graph, which seems different. 


The experimental evaluation was somewhat limited and that is the biggest problem from a practical standpoint. It is not clear why one would want to use these sequences for solving MVC. There are several graph classification tasks that try to use the graph structure (as well as possibly other features) see eg the bioinformatics 
and other applications. Literature includes for example:
Graph Kernels by S.V.N. Vishwanathan et al. 
Deep graph kernels (Yanardag & Vishwanathan and graph invariant kernels (Orsini et al.),
which use counts of small substructures as features. 

The are many benchmarks of graph classification tasks where the proposed representation could be useful but significantly more validation work would be needed to make that case. 

","[4, 4, 4]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is critical of the paper's novelty, methodology, and experimental validation. While acknowledging the theoretical contribution, the reviewer raises concerns about sample complexity, lack of comparison with simpler methods, and limited experimental evaluation. The reviewer suggests alternative approaches and highlights the need for more comprehensive validation. The tone is direct and critical, but maintains a professional and academic demeanor.",-20.0,60.0
Residual Gated Graph ConvNets,"['Xavier Bresson', 'Thomas Laurent']",Reject,2018,"[17, 13]","[22, 18]","[97, 39]","[36, 10]","[35, 23]","[26, 6]","The authors revised the paper according to all reviewers suggestions, I am satisfied with the current version.

Summary: this works proposes to employ recurrent gated convnets to solve graph node labeling problems on arbitrary graphs. It build upon several previous works, successively introducing convolutional networks, gated edges convnets on graphs, and LSTMs on trees. The authors extend the tree LSTMs formulation to perform graph labeling on arbitrary graphs, merge convnets with residual connections and edge gating mechanisms. They apply the 2 proposed models to 3 baselines also based on graph neural networks on two problems: sub-graph matching (expressing the problem of sub-graph matching as a node classification problem), and semi supervised clustering.  

Main comments:
It would strengthen the paper to also compare all these network learning based approaches to variational ones. For instance, to a spectral clustering method for the semi supervised clustering, or
solving the combinatorial Dirichlet problem as in Grady: random walks for image segmentation, 2006.

The abstract and the conclusion should be revised, they are very vague.
- The abstract should be self contained and should not contain citations.
- The authors should clarify which problem they are dealing with.
- instead of the ""numerical result show the performance of the new model"", give some numerical results here, otherwise, this sentence is useless.
- we propose ... as propose -> unclear: what do you propose?
 

Minor comments:
- You should make sentences when using references with the author names format. Example: ... graph theory, Chung (1997) -> graph theory by Chung (1997)
- As Eq 2 -> As the minimization of Eq 2 (same with eq 4)
- Don't start sentences with And, or But

","[7, 3, 6]","[' Good paper, accept', ' Clear rejection', ' Marginally above acceptance threshold']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer starts by stating satisfaction with the revised version of the paper, which clearly indicates a positive sentiment. While they provide constructive criticism and suggestions for improvement, the overall tone remains respectful and encouraging. The suggestions focus on enhancing clarity, adding comparisons, and improving presentation, rather than pointing out fundamental flaws. ",75.0,80.0
Learning to Optimize Neural Nets,"['Ke Li', 'Jitendra Malik']",Reject,2018,"[4, 36]","[8, 41]","[32, 446]","[11, 242]","[20, 154]","[1, 50]","[Main comments]

* I would advice the authors to explain in more details in the intro
what's new compared to Li & Malik (2016) and Andrychowicz et al. (2016).
It took me until section 3.5 to figure it out.

* If I understand correctly, the only new part compared to Li & Malik (2016) is
section 3.5, where block-diagonal structure is imposed on the learned matrices.
Is that correct?

* In the experiments, why not comparing with Li & Malik (2016)? (i.e., without
  block-diagonal structure)

* Please clarify whether the objective value shown in the plots is wrt the training
  set or the test set. Reporting the training objective value makes little
sense to me, unless the time taken to train on MNIST is taken into account in
the comparison. 

* Please clarify what are the hyper-parameters of your meta-training algorithm
  and how you chose them.

I will adjust my score based on the answer to these questions.

[Other comments]

* ""Given this state of affairs, perhaps it is time for us to start practicing
  what we preach and learn how to learn""

This is in my opinion too casual for a scientific publication...

* ""aim to learn what parameter values of the base-level learner are useful
  across a family of related tasks""

If this is essentially multi-task learning, why not calling it so?  ""Learning
what to learn"" does not mean anything.  I understand that the authors wanted to
have ""what"", ""which"" and ""how"" sections but this is not clear at all.

What is a ""base-level learner""? I think it would be useful to define it more
precisely early on.

* I don't see the difference between what is described in Section 2.2
  (""learning which model to learn"") and usual machine learning (searching for
the best hypothesis in a hypothesis class).

* Typo: p captures the how -> p captures how

* The L-BFGS results reported in all Figures looked suspicious to me.  How do you
  explain that it converges to a an objective value that is so much worse?
Moreover, the fact that there are huge oscillations makes me think that the
authors are measuring the function value during the line search rather than
that at the end of each iteration.
","[5, 6, 6]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[3, 4, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review is quite critical of the paper, pointing out several areas that need significant clarification and improvement. The reviewer questions the novelty of the work, criticizes the clarity of the writing, and raises concerns about the experimental methodology and results. While the reviewer offers suggestions for improvement, the overall tone suggests significant concerns about the paper's current state. The language used is direct and critical, but not overtly rude. The reviewer points out specific issues with the writing style, such as being too casual, but the criticism is constructive and aims to improve the paper.",-50.0,50.0
Multi-Advisor Reinforcement Learning,"['Romain Laroche', 'Mehdi Fatemi', 'Joshua Romoff', 'Harm van Seijen']",Reject,2018,"[1, 14, 21, 12]","[6, 19, 26, 17]","[22, 198, 135, 114]","[9, 112, 79, 45]","[10, 49, 33, 62]","[3, 37, 23, 7]","This paper presents MAd-RL, a method for decomposition of a single-agent RL problem into a simple sub-problems, and aggregating them back together. Specifically, the authors propose a novel local planner - emphatic, and analyze the newly proposed local planner along of two existing ones - egocentric and agnostic. The MAd-RL, and theoretical analysis, is evaluated on the Pac-Boy task, and compared to DQN and Q-learning with function approximation.

Pros:
1. The paper is well written, and well-motivated.
2. The authors did an extraordinary job in building the intuition for the theoretical work, and giving appropriate examples where needed.
3. The theoretical analysis of the paper is extremely interesting. The observation that a linearly weighted reward, implies linearly weighted Q function, analysis of different policies, and local minima that result is the strongest and the most interesting points of this paper.

Cons:
1. The paper is too long. 14 pages total - 4 extra pages (in appendix) over the 8 page limit, and 1 extra page of references. That is 50% overrun in the context, and 100% overrun in the references. The most interesting parts and the most of the contributions are in the Appendix, which makes it hard to assess the contributions of the paper. There are two options: 
  1.1 If the paper is to be considered as a whole, the excessive overrun gives this paper unfair advantage over other ICLR papers. The flavor and scope and quality of the problems that can be tackled with 50% more space is substantially different from what can be addressed within the set limit. If the extra space is necessary, perhaps this paper is better suited for another publication? 
  1.2 If the paper is assessed only based on the main part without Appendix, then the only novelty is emphatic planner, and the theoretical claims with no proofs. The results are interesting, but are lacking implementation details. Overall, a substandard paper.
2. Experiments are disjoint from the method’s section. For example:
  2.1 Section 5.1 is completely unrelated with the material presented in Section 4.
  2.2 The noise evaluation in Section 5.3 is nice, but not related with the Section 4. This is problematic because, it is not clear if the focus of the paper is on evaluating MAd-RL and performance on the Ms.PacMan task, or experimentally demonstrating claims in Section 4.

Recommendations:
1. Shorten the paper to be within (or close to the recommended length) including Appendix.
2. Focus paper on the analysis of the advisors, and Section 5. on demonstrating the claims.
3. Be more explicit about the contributions.
4. How does the negative reward influence the behavior the agent? The agent receives negative reward when near ghosts.
5. Move the short (or all) proofs from Appendix into the main text.
6. Move implementation details of the experiments (in particular the short ones) into the main text.
7. Use the standard terminology (greedy and random policies vs. egoistic and agnostic) where possible. The new terms for well-established make the paper needlessly more complex. 
8. Focus the literature review on the most relevant work, and contrast the proposed work with existing peer reviewed methods.
9. Revise the literature to emphasize more recent peer reviewed references. Only three references are recent (less than 5 years), peer reviewed references, while there are 12 historic references. Try to reduce dependencies on non-peer reviewed references (~10 of them).
10. Make a pass through the paper, and decouple it from the van Seijen et al., 2017a
11. Minor: Some claims need references:
  11.1 Page 5: “egocentric sub-optimality  does not come from the actions that are equally good, nor from the determinism of the policy, since adding randomness…” - Wouldn’t adding epsilon-greediness get the agent unstuck?
  11.2 Page 1. “It is shown on the navigation task ….” - This seems to be shown later in the results, but in the intro it is not clear if some other work, or this one shows it.  
12. Minor:
  12.1 Mix genders when talking about people. Don’t assume all people that make “complex and important problems”, or who are “consulted for advice”, are male.
  12.2 Typo: Page 5: a_0 sine die
  12.3 Page 7 - omit results that are not shown
  12.4 Make Figures larger - it is difficult, if not impossible to see
  12.5 What is the difference between Pac-Boy and Ms. Pacman task? And why not use Ms. Packman?
 
","[4, 4, 4]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review is mostly positive, praising the paper's clarity, motivation, and theoretical analysis. However, the reviewer raises significant concerns about the paper's length, organization, and focus. The reviewer finds the appendix to contain crucial contributions but criticizes its excessive size, putting the paper over the allowed page limit. The reviewer also points out disconnects between different sections and suggests improvements to the experimental validation and literature review. While the criticism is direct and substantial, the language remains professional and constructive throughout, suggesting specific actions for improvement. Therefore, the sentiment leans towards the positive side, but the numerous and significant revision requests indicate a not entirely smooth acceptance.",60.0,70.0
Joint autoencoders: a flexible meta-learning framework,"['Baruch Epstein', 'Ron Meir', 'Tomer Michaeli']",Reject,2018,"[2, 31, 12]","[6, 35, 17]","[5, 106, 95]","[2, 46, 43]","[3, 18, 38]","[0, 42, 14]","The work proposed a generic framework for end-to-end transfer learning / domain adaptation with deep neural networks. The idea is to learn a joint autoencoders, containing private branch with task/domain-specific weights, as well as common branch consisting of shared weights used across tasks/domains, as well as task/domain-specific weights.  Supervised losses are added after the encoders to utilize labeled samples from different tasks. Experiments on the MNIST and CIFAR datasets showed improvements over baseline models. Its performance is comparable to / worse than several existing deep domain adaptation works on the MNIST, USPS and SVHN digit datasets.

The structure of the paper is good, and easy to read.  The idea is fairly straight-forward. It reads as an extension of ""frustratingly easy domain adaptation"" to DNN (please cite this work). Different from most existing work on DNN for multi-task/transfer learning, which focuses on weight sharing in bottom layers, the work emphasizes the importance of weight sharing in deeper layers. The overall novelty of the work is limited though. 

The authors brought up two strategies on learning the shared and private weights at the end of section 3.2. However, no follow-up comparison between the two are provided. It seems like most of the results are coming from the end-to-end learning. 

Experimental results:
section 4.1: Figure 2 is flawed. The colors do not correspond to the sub-tasks. For example, there are digits 1, 4 in color magenta, which is supposed to be the shared branch of digits of 5~9. Vice versa. 
From reducing the capacity of JAE to be the same as the baseline, most of the improvement is gone. It is not clear how much of the improvement will remain if the baseline model gets to see all the samples instead of just those from each sub-task. 

section 4.2.1: The authors demonstrate the influence of shared layer depth in table 2. While it does seem to matter for tasks of dissimilar inputs, have the authors compare having a completely shared branch or sharing more than just a single layer?

The authors suggested in section 4.1 CIFAR experiment that the proposed method provides more performance boost when the two tasks are more similar, which seems to be contradicting to the results shown in Figure 3, where its performance is worse when transferring between USPS and MNIST, which are more similar tasks vs between SVHN and MNIST. Do the authors have any insight?","[4, 5, 5]","[' Ok but not good enough - rejection', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review is mostly neutral, tending slightly towards the positive. The reviewer acknowledges the merits of the paper, such as its clear structure and the potential of the proposed idea. However, they also point out several limitations, including limited novelty, flaws in the experiments, and contradictory results. The language used is polite and professional throughout, offering constructive criticism and suggestions for improvement.",10.0,80.0
Flexible Prior Distributions for Deep Generative Models,"['Yannic Kilcher', 'Aurelien Lucchi', 'Thomas Hofmann']",Reject,2018,"[3, 10, 26]","[8, 15, 31]","[20, 110, 205]","[6, 51, 112]","[13, 52, 69]","[1, 7, 24]","Summary:

The paper proposes to learn new priors for latent codes z  for GAN training.  for this the paper shows that there is a mismatch between the gaussian prior and an estimated of the latent codes of real data by reversal of the generator . To fix this the paper proposes to learn a second GAN to learn the prior distributions of ""real latent code"" of the first GAN. The first GAN then uses the second GAN as prior to generate the z codes. 
 
Quality/clarity:

The paper is well written and easy to follow.

Originality:

pros:
-The paper while simple sheds some light on important problem with the prior distribution used in GAN.
- the second GAN solution trained on reverse codes from real data is interesting 
- In general the topic is interesting, the solution presented is simple but needs more study

cons:

- It related to adversarial learned inference and BiGAN, in term of learning the mapping  z ->x, x->z and seeking the agreement. 
- The solution presented is not end to end (learning a prior generator on learned models have been done in many previous works on encoder/decoder)

General Review:

More experimentation with the latent codes will be interesting:

- Have you looked at the decay of the singular values of the latent codes obtained from reversing the generator? Is this data low rank? how does this change depending on the dimensionality of the latent codes? Maybe adding plots to the paper can help.

- the prior agreement score is interesting but assuming gaussian prior also for the learned latent codes from real data is maybe not adequate.  Maybe computing the entropy of the codes using a nearest neighbor estimate of the entropy  can help understanding the entropy difference wrt to the isotropic gaussian prior?

- Have you tried to multiply the isotropic normal noise with the learned singular values and generate images from  this new prior  and compute inceptions scores etc? Maybe also rotating the codes with the singular vector matrix V or \Sigma^{0.5} V?

- What architecture did you use for the prior generator GAN?

- Have you thought of an end to end way to learn the prior generator GAN? 

****** I read the authors reply. Thank you for your answers and for the SVD plots this is  helpful.  *****

","[6, 6, 5]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally below acceptance threshold']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer provides constructive criticism, acknowledges the paper's strengths (well-written, addresses an important problem, interesting solution), and offers specific suggestions for improvement. They also express interest in the research (""More experimentation with the latent codes will be interesting..."") and thank the authors for their response. This suggests a positive but not overly enthusiastic sentiment, as the reviewer sees potential in the work but also points out areas for further development. The language is polite and professional throughout, with no signs of harshness or disrespect.",60.0,80.0
Prediction Under Uncertainty with Error Encoding Networks,"['Mikael Henaff', 'Junbo Zhao', 'Yann Lecun']",Reject,2018,"[8, 4, 31]","[12, 5, 36]","[24, 12, 315]","[11, 5, 162]","[12, 7, 113]","[1, 0, 40]","Summary: 

I like the general idea of learning ""output stochastic"" noise models in the paper, but the idea is not fully explored (in terms of reasonable variations and their comparative performance).  I don't fully understand the rationale for the experiments: I cannot speak to the reasons for the GAN's failure (GANs are not easy to train and this seems to be reflected in the results); the newly proposed model seems to improve with samples simply because the evaluation seems to reward the best sample.  I.e., with enough throws, I can always hit the bullseye with a dart even when blindfolded.

Comments:

The model proposes to learn a conditional stochastic deep model by training an output noise model on the input x_i and the residual y_i - g(x_i).  The trained residual function can be used to predict a residual z_i for x_i.  Then for out-of-sample prediction for x*, the paper appears to propose sampling a z uniformly from the training data {z_i}_i (it is not clear from the description on page 3 that this uniformly sampled z* = z_i depends on the actual x* -- as far as I can tell it does not).  The paper does suggest learning a p(z|x) but does not provide implementation details nor experiment with this approach.

I like the idea of learning an ""output stochastic"" model -- it is much simpler to train than an ""input stochastic"" model that is more standard in the literature (VAE, GAN) and there are many cases where I think it could be quite reasonable.  However, I don't think the authors explore the idea well enough -- they simply appear to propose a non-parametric way of learning the stochastic model (sampling from the training data z_i's) and do not compare to reasonable alternative approaches.  To start, why not plot the empirical histogram of p(z|x) (for some fixed x's) to get a sense of how well-behaved it is as a distribution.  Second, why not simply propose learning exponential family models where the parameters of these models are (deep nets) conditioned on the input?  One could even start with a simple Gaussian and linear parameterization of the mean and variance in terms of x.  If the contribution of the paper is the ""output stochastic"" noise model, I think it is worth experimenting with the design options one has with such a model.

The experiments range over 4 video datasets.  PSNR is evaluated on predicted frames -- PSNR does not appear to be explicitly defined but I am taking it to be the metric defined in the 2nd paragraph from the bottom on page 7.  The new model ""EEN"" is compared to a deterministic model and conditional GAN.  The GAN never seems to perform well -- the authors claim mode collapse, but I wonder if the GAN was simply hard to train in the first place and this is the key reason?  Unsurprisingly (since the EEN noise does not seem to be conditioned on the input), the baseline deterministic model performs quite well.  If I understand what is being evaluated correctly (i.e., best random guess) then I am not surprised the EEN can perform better with enough random samples.  Have we learned anything?
","[5, 4, 5]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[2, 4, 3]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer expresses interest in the paper's core idea but finds significant shortcomings in its execution and experimental validation. They find the exploration of the idea insufficient and question the rationale and interpretation of the experiments. While the reviewer doesn't resort to harsh language, their disappointment and skepticism are apparent throughout the review. ",-20.0,60.0
Learning Deep ResNet Blocks Sequentially using Boosting Theory,"['Furong Huang', 'Jordan T. Ash', 'John Langford', 'Robert E. Schapire']",Reject,2018,"[9, 8, 21, 32]","[14, 13, 26, 36]","[105, 23, 222, 202]","[36, 10, 112, 121]","[60, 13, 83, 38]","[9, 0, 27, 43]","Summary:
This paper considers a learning method for the ResNet using the boosting framework. More precisely, the authors view the structure of the ResNet as a (weighted) sum of base networks (weak hypotheses) and apply the boosting framework. The merit of this approach is to decompose the learning of complex networks to that of small to large networks in a moderate way and it uses less computational costs. The experimental results are good. The authors also show training and generalization error bounds for the proposed approach.

Comments: 
The idea of the paper is natural and interesting. Experimental results are somewhat impressive. However, I am afraid that theoretical results in the paper contain several mistakes and does not hold. The details are below.

I think the proof of Theorem 4.2 is wrong. More precisely, there are several possibly wrong arguments as follows:
- In the proof, \alpha_t+1 is chosen so as to minimize an upper bound of Z_t, while the actual algorithm is chosen to minimize Z_t. The minimizer of Z_t and that of an upper bound are different in general. So, the obtained upper bound does not hold for the training error of the actual algorithm. 
- It is not a mistake, but, there is no explanation why the equality between (27) and (28) holds. Please add an explanation. Indeed, equation (21) matters. 

Also, the statement of Theorem 4.2 looks somewhat cheating: The statement seems to say that it holds for any iteration T and the training error decays exponentially w.r.t. T. However, the parameter T is determined by the parameter gamma, so it is some particular iteration, which might be small and the bound could be large. 

The generalization error bound Corollary 4.3 seems to be wrong, too. More precisely, Lemma 2 of Cortes et al. is OK, but the application of Lemma 2 is not. In particular, the proof does not take into account of the function \sigma. In other words, the proof considers the Rademacher complexity R_m(\calF_t), of the class \calF_t, but, acutually, I think it should consider R_m(\sigma(\calF_t)), where the class \sigma(\calF_t) consists of the composition of functions \sigma and f_t in \calF_t. Talagrand’s lemma (see, e.g., Mohri et al.’ s book: Foundation of Machine Learning) can be used to analyze the complexity of the composite class. But, the resulting bound would depend on the Lipschizness of \sigma in an exponential way. 

The explanation of the generalization ability is not sufficient. While the latter weak hypotheses are complex enough and would have large edges, the complexity of the function class of weak hypotheses grows exponentially w.r.t. the iteration T, which should be mentioned. 

As a summary, the paper contains nice ideas and experimental results are promising, but has non-negligible mistakes in theoretical parts which degrade the contribution of the paper.

Minor Comments:
-In Algorithm 1, \gamma_t is not defined when a while-loop starts. So, the condition of the while-loop cannot be checked.

 
","[4, 5, 5]","[' Ok but not good enough - rejection', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer acknowledges the paper's interesting idea and impressive experimental results. However, they express serious concerns about the theoretical analysis, pointing out potential errors and omissions that they believe invalidate the theoretical results. The reviewer uses phrases like ""I am afraid that"", ""somewhat cheating"", ""seems to be wrong"", indicating a negative sentiment stemming from the perceived flaws. However, they maintain a professional and respectful tone throughout, focusing on the technical aspects and providing constructive criticism. Therefore, the sentiment is rather negative due to the significant criticism of the theoretical work, but the politeness remains high.",-20.0,80.0
Multimodal Sentiment Analysis To Explore the Structure of Emotions,"['Anthony Hu', 'Seth Flaxman']",Reject,2018,"[1, 10]","[5, 15]","[13, 40]","[5, 13]","[8, 20]","[0, 7]","The paper presents a multi-modal CNN model for sentiment analysis that combines images and text.  The model is trained on a new dataset collected from Tumblr.

Positive aspects:
+ Emphasis in model interpretability and its connection to psychological findings in emotions
+ The idea of using Tumblr data seems interesting, allowing to work with a large set of emotion categories, instead of considering just the binary task positive vs. negative. 

Weaknesses:
- A deeper analysis of previous work on the combination of image and text for sentiment analysis (both datasets and methods) and its relation with the presented work is necessary. 
- The proposed method is not compared with other methods that combine text and image for sentiment analysis.
-  The study is limited to just one dataset.

The paper presents interesting ideas and findings in an important challenging area. The main novelties of the paper are: (1) the use of Tumblr data, (2) the proposed CNN architecture, combining images and text (using word embedding. 

I missed a ""related work section"", where authors clearly mention previous works on similar datasets. Some related works are mentioned in the paper, but those are spread in different sections. It's hard to get a clear overview of the previous research: datasets, methods and contextualization of the proposed approach in relation with previous work. I think authors should cite Sentibanks. Also, at some point authors should compare their proposal with previous work. 

More comments:

- Some figures could be more complete: to see more examples in Fig 1, 2, 3 would help to understand better the dataset and the challenges. 
- In table 4, for example, it would be nice to see the performance on the different emotion categories.
- It would be interesting to see qualitative visual results on recognitions.

I like this work, but I think authors should improve the aspects I mention for its publication.
","[5, 6, 4]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Ok but not good enough - rejection']","[5, 5, 5]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The reviewer finds the paper interesting and highlights its positive aspects, especially the novelty of the dataset and the focus on interpretability. However, they also point out significant weaknesses, mainly the lack of comparison with existing work and the need for a more in-depth analysis. The reviewer's suggestion for improvement and the statement ""I like this work, but..."" indicate a positive but critical stance. ",50.0,70.0
Generating Adversarial Examples with Adversarial Networks,"['Chaowei Xiao', 'Bo Li', 'Jun-Yan Zhu', 'Warren He', 'Mingyan Liu', 'Dawn Song']",Reject,2018,"[1, 17]","[6, 22]","[6, 118]","[3, 57]","[3, 43]","[0, 18]","This paper describes AdvGAN, a conditional GAN plus adversarial loss. AdvGAN is able to generate adversarial samples by running a forward pass on generator. The authors evaluate AdvGAN on semi-white box and black box setting.

AdvGAN is a simple and neat solution to for generating adversary samples. The author also reports state-of-art results.

Comment:

1. For MNIST samples, we can easily find the generated sample is a mixture of two digitals. Eg, for digital 7 there is a light gray 3 overlap. I am wondering this method is trying to mixture several samples into one to generate adversary samples. For real color samples, it is harder to figure out the mixture.
2. Based on mixture assumption, I suggest the author add one more comparison to other method, which is relative change from original image, to see whether AdvGAN is the most efficient model to generate the adversary sample (makes minimal change to original image).



","[6, 7, 4]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Ok but not good enough - rejection']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with a factual summary of the paper's content. It then praises the paper for its simplicity, neat solution, and state-of-the-art results, indicating a positive sentiment. While the review raises a couple of comments/questions, it frames them constructively as suggestions for improvement rather than criticisms. The language used is formal, objective, and respectful throughout.",75.0,90.0
Discriminative k-shot learning using probabilistic models,"['Matthias Bauer', 'Mateo Rojas-Carulla', 'Jakub Bartłomiej Świątkowski', 'Bernhard Schölkopf', 'Richard E. Turner']",Reject,2018,"[3, 2, 2, 24, 12]","[8, 6, 1, 29, 17]","[16, 11, 1, 777, 147]","[7, 3, 0, 380, 67]","[9, 6, 1, 286, 70]","[0, 2, 0, 111, 10]","This paper presents a procedure to efficiently do K-shot learning in a classification setting by creating informative priors from information learned from a large, fully labeled dataset.  Image features are learned using a standard convolutional neural network---the last layer form image features, while the last set of weights are taken to be image ""concepts"".  The method treats these weights as data, and uses these data to construct an informative prior over weights for new features.

- Sentence two: would be nice to include a citation from developmental psychology.

- Probabilistic modeling section: treating the trained weights like ""data"" is a good way to convey intuition about your method.  It might be good to clarify some specifics earlier on in the ""Probabilistic Modeling"" paragraph, e.g. how many ""observations"" are associated with this matrix. 
 
- In the second phase, concept transfer, is the only information from the supervised weights the mean and estimated covariance?  For instance, if there are 80 classes and 256 features from the supervised phase, the weight ""data"" model is 80 conditionally IID vectors of length 256 ~ Normal(\mu, \Sigma).  The posterior MAP for \mu and \Sigma are then used as a prior for weights in the K-shot task.  How many parameters are estimated for 

  * gauss iso: mu = 256-length vector, \sigma = scalar variance value of weights
  * log reg: mu = 256-length zero vector, \sigma = scalar variance value of weights
  * log reg cross val: mu = 256-length zero vector, \sigma = cross validated value

If the above is correct, the information boosts K-shot accuracy is completely contained in the 256-length posterior mean vector and the scalar weight variance value?

- Is any uncertainty about \mu_MAP or \Sigma_MAP propagated through to uncertainty in the K-shot weights?  If not, would this influence the choice of covariance structure for \Sigma_MAP? How sensitive are inferences to the choice of Normal inverse-Wishart hyper parameters? 

- What do you believe is the source of the mis-calibration in the ""predictied probability vs. proportion of times correct"" plot in Figure 2?  

Technical: The method appears to be technically correct.

Clarity: The paper is pretty clearly written, however some specific details of the method are difficult to understand.

Novel: I am not familiar with K-shot learning tasks to assess the novelty of this approach. 

Impact: While the reported results seem impressive and encouraging, I believe this a relatively incremental approach. ","[5, 5, 5]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[3, 3, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer provides constructive criticism and asks relevant questions to improve the paper. They acknowledge the technical correctness and clarity of the paper, but also point out areas for improvement and express some reservations about the novelty and impact. Overall, the tone is engaged and helpful, suggesting a positive outlook on the paper's potential.",60.0,80.0
GraphVAE: Towards Generation of Small Graphs Using Variational Autoencoders,"['Martin Simonovsky', 'Nikos Komodakis']",Reject,2018,"[3, 15]","[5, 19]","[13, 130]","[6, 71]","[6, 28]","[1, 31]","The authors propose a variational auto encoder architecture to generate graphs.  

Pros:
- the formulation of the problem as the modeling of a probabilistic graph is of interest 
- some of the main issues with graph generation are acknowledged (e.g. the problem of invariance to node permutation) and a solution is proposed (the binary assignment  matrix)
- notions for measuring the quality of the output graphs are of interest: here the authors propose some ways to use domain knowledge to check simple properties of molecular graphs 

Cons: 
- the work is quite preliminary
- many crucial elements  in graph generation are not dealt with: 
 a) the adjacency matrix and the label tensors are not independent of each other, the notion of a graph is in itself a way to represent the 'relational links' between the various components
 b) the boundaries between a feasible and an infeasible graph are sharp: one edge or one label can be sufficient for acting the transition independently of the graph size, this makes it a difficult task for a continuous model. The authors acknowledge this but do not offer ways to tackle the issue
 c) conditioning on the label histogram should make the problem easy: one is giving away the number of nodes and the label identities after all; however even in this setup the approach fails more often than not 
 d) the graph matching procedure proposed is a rough patch for a much deeper problem
- the evaluation should include a measure of the capacity of the architecture to :
 a) reconstruct perfectly the input
 b) denoise perturbations over node labels and additional/missing  edges  ","[7, 7, 5]","[' Good paper, accept', ' Good paper, accept', ' Marginally below acceptance threshold']","[4, 2, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is fairly confident that the evaluation is correct']","The review acknowledges some positive aspects of the work, such as the problem formulation and the proposed solutions for specific issues. However, it heavily criticizes the work for being preliminary, not addressing crucial elements in graph generation, and having a flawed evaluation. The language, while direct and critical, maintains a professional and analytical tone without resorting to personal attacks or disrespectful language.",-30.0,60.0
Time Limits in Reinforcement Learning,"['Fabio Pardo', 'Arash Tavakoli', 'Vitaly Levdik', 'Petar Kormushev']",Reject,2018,"[2, 3, 2, 10]","[5, 8, 4, 15]","[11, 32, 6, 99]","[4, 13, 2, 50]","[7, 17, 4, 24]","[0, 2, 0, 25]","Summary: This paper explores how to handle two practical issues in reinforcement learning. The first is including time remaining in the state, for domains where episodes are cut-off before a terminal state is reached in the usual way. The second idea is to allow bootstrapping at episode boundaries, but cutting off episodes to facilitate exploration. The ideas are illustrated through several well-worked micro-world experiments.

Overall the paper is well written and polished. They slowly worked through a simple set of ideas trying to convey a better understanding to the reader, with a focus on performance of RL in practice.

My main issue with the paper is that these two topics are actually not new and are well covered by the existing RL formalisms. That is not to say that an empirical exploration of the practical implications is not of value, but that the paper would be much stronger if it was better positioned in the literature that exists.

The first idea of the paper is to include time-remaining in the state. This is of course always possible in the MDP formalism. If it was not done, as in your examples, the state would not be Markov and thus it would not be an MDP at all. In addition, the technical term for this is finite horizon MDPs (in many cases the horizon is taken to be a constant, H). It is not surprising that algorithms that take this into account do better, as your examples and experiments illustrate. The paper should make this connection to the literature more clear and discuss what is missing in our existing understanding of this case, to motivate your work. See Dynamic Programming and Optimal Control and references too it.

The second idea is that episodes may terminate due to time out, but we should include the discounted value of the time-out termination state in the return. I could not tell from the text but I assume, the next transition to the start state is fully discounted to zero, otherwise the value function would link the values of S_T and the next state, which I assume you do not want. The impact of this choice is S_T is no longer a termination state, and there is a direct fully discounted transition to the start states. This is in my view is how implementations of episodic tasks with a timeout should be done and is implemented this way is classic RL frameworks (e.g., RL glue). If we treat the value of S_T as zero or consider gamma on the transition into the time-out state as zero, then in cost to goal problems the agent will learn that these states are good and will seek them out leading to suboptimal behavior. The literature might not be totally clear about this, but it is very well discussed in a recent ICML paper: White 2017 [1]

Another way to pose and think about this problem is using the off-policy learning setting---perhaps best described in the Horde paper [2]. In this setting the behavior policy can have terminations and episodes in the classic sense (perhaps due to time outs). However, the agent's continuation function (gamma : S -> [0,1]) can specify weightings on states representing complex terminations (or not), completely independent of the behavior policy or actual state transition dynamics of the underlying MDP. To clearly establish your contributions, the authors must do a better job of relating their work to [1] and [2].

[1] White. Unifying task specification in reinforcement learning. Martha White. International Conference on Machine Learning (ICML), 2017.

[2] Sutton, R. S., Modayil, J., Delp, M., Degris, T., Pilarski, P. M., White, A., & Precup, D. (2011). Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction. In The 10th International Conference on Autonomous Agents and Multiagent Systems: 2, 761--768. 

Small comments that did not impact paper scoring:
1) eq 1 we usually don't use the superscript \gamma
2) eq2, usually we talk about truncated n-step returns include the value of the last state to correct the return. You should mention this
3) Last paragraph of page 2 should not be in the intro
4) in section 2.2 why is the behavior policy random instead of epsilon greedy?
5) It would be useful to discuss the average reward setting and how it relates to your work.
6) Fig 5. What does good performance look like in this domain. I have no reference point to understand these graphs
7) page 9, second par outlines alternative approaches but they are not presented as such. Confusing ","[5, 4, 4]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is generally positive in its sentiment, acknowledging the paper's clarity and focus on practical RL. However, it raises significant concerns about the novelty of the contributions, arguing that the explored concepts are already well-established within existing RL formalisms. The reviewer suggests that the paper would be strengthened by more clearly positioning its empirical explorations within the context of existing literature and explicitly addressing what new insights are provided. The language used is polite and professional, offering constructive criticism and specific recommendations for improvement.",40.0,80.0
Learning Priors for Adversarial Autoencoders,"['Hui-Po Wang', 'Wei-Jan Ko', 'Wen-Hsiao Peng']",Reject,2018,"[1, 1, 17]","[6, 5, 22]","[13, 5, 108]","[6, 3, 67]","[7, 2, 21]","[0, 0, 20]","This paper propose a simple extension of the adversarial auto-encoders for (conditional) image generation. The general idea is that instead of using Gaussian prior, the propose algorithm uses a ""code generator"" network  to warp the gaussian distribution, such that the internal prior of the latent encoding space is more expressive and complicated. 

Pros:
- The proposed idea is simple and easy to implement
- The results show improvement in terms of visual quality

Cons:
- I agree that the proposed prior should better capture the data distribution. However, incorporating a generic prior over the latent space plays a vital role as regularisation, this helps avoid model collapse. Adding a complicated code generation network brings too much flexibility for the prior part. This makes the prior and posterior learnable, which makes it easier to fool the regularisation discriminator (think about the latent code and prior code collapsed to two different points). As a result, this weakens the regularisation over the latent encoder space.  
- The above mentioned could be verified through qualitative results. As shown in Fig. 5. I believe this is a result due to the fact that the adversarial loss in the regularisation phase does not a significant influence there. 
- I have some doubts over why AAE works so poorly when the latent dimension is 2000. How to make sure it's not a problem of implementation or the model wasn't trapped into a bad local optima / saddle points. Could you justify this?
- Contributions; this paper propose an improvement over a existing model. However, neither the idea/insights it brought can be applied onto other generative models, nor the improvement bring a significant improvement over the-state-of-the-arts. I am wondering what the community will learn from this paper, or what the author would like to claim as significant contributions. ","[5, 6, 6]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[3, 4, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer acknowledges the simplicity and improved visual quality of the proposed approach, indicating some positive aspects. However, they raise significant concerns about the core idea, questioning its impact on regularization and potential for model collapse. The reviewer also points out limitations in the experimental validation and questions the overall contribution of the work. The tone, while critical, maintains a professional and constructive approach, suggesting specific areas for improvement and further investigation. Therefore, the sentiment leans towards the negative side due to the concerns raised, but the politeness remains neutral.",-30.0,0.0
Lifelong Learning by Adjusting Priors,"['Ron Amit', 'Ron Meir']",Reject,2018,"[7, 31]","[11, 35]","[9, 106]","[5, 46]","[4, 18]","[0, 42]","I personally warmly welcome any theoretically grounded methods to perform deep learning. I read the paper with interest, but I have two concerns about the main theoretical result (Theorem 1, lifelong learning PAC-Bayes bound).
* Firstly, the bound is valid for a [0,1]-valued loss, which does not comply with the losses used in the experiments (Euclidean distance and cross-entropy). This is not a big issue, as I accept that the authors are mainly interested in the learning strategy promoted by the bound. However, this should clearly appear in the theorem statement.
* Secondly, and more importantly, I doubt that the uaw of the meta-posterior as a distribution over priors for each task is valid. In Proposition 1 (the classical single-task PAC-Bayes bound), the bound is valid with probability 1-delta for one specific choice of prior P, and this choice must be independent of the learning sample S. However, it appears that the bound should be valid uniformly for all P in order to be used in Theorem 1 proof (see Equation 18). From a learning point of view, it seems counterintuitive that the prior used in the KL term to learn from a task relies on the training samples (i.e., the same training samples are used to learn the meta-posterior over priors, and the task specific posterior).  

A note about the experiments:
I am slightly disappointed that the authors compared their algorithm solely with methods learning from fewer tasks. I would like to see the results obtained by another method using five tasks. A simple idea would be to learn a network independently for each of the five tasks, and consider as a meta-prior an isotropic Gaussian distribution centered on the mean of the five learned weight vectors.

Typos and minor comments:
- Equation 1: \ell is never explicitly defined.
- Equation 4: Please explicitly define m in this context (size of the learning sample drawn from tau).
- Page 4, before Equation 5: A dot is missing between Q and ""This"".
- Page 7, line 3: Missing parentheses around equation number 12.
- Section 5.1.1, line 5: ""The hypothesis class is a the set of...""
- Equation 17: Q_1, ... Q_n are irrelevant.

=== UPDATE ===
I increased my score after author's rebuttal. See my other post.","[6, 6, 6]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer finds the paper interesting and welcomes the theoretical approach. However, they raise serious concerns about the main theorem's validity, leading to a somewhat negative sentiment. Despite the criticism, the language remains polite and constructive throughout, suggesting areas for improvement and further experiments. The update mentioning an increased score after the author's rebuttal further confirms a positive shift in sentiment based on the addressed concerns.",-10.0,80.0
Variance Regularized Counterfactual Risk Minimization via Variational Divergence Minimization,['Hang Wu'],Reject,2018,"[14, 16]","[19, 21]","[91, 204]","[46, 156]","[5, 8]","[40, 40]","This paper studies off-policy learning in the bandit setting. It develops a new learning objective where the empirical risk is regularized by the squared Chi-2 divergence between the new and old policy. This objective is motivated by a bound on the empirical risk, where this divergence appears. The authors propose to solve this objective by using generative adversarial networks for variational divergence minimization (f-GAN). The algorithm is then evaluated on settings derived from supervised learning tasks and compared to other algorithms.

I find the paper well written and clear. I like that the proposed method is both supported by theory and empirical results. 

Minor point: I do not really agree with the discussion on the impact of the stochasticity of the logging policy in section 5.6. Based on Figure 5 a and b, it seems that the learned policy is performing equally well no matter how stochastic the logging policy is. So I find it a bit misleading to suggest that the learned policy are not being improved when the logging policy is more deterministic. Rather, the gap reduces between the two policies because the logging policy gets better. In order to better showcase this mechanism, perhaps you could try using a logging policy that does not favor the best action.

quality and clarity:
++ code made available
+ well written and clear
- The proof of theorem 2 is not in the paper nor appendix (the authors say it is similar to another work).


originality
+ good extension of the work by Swaminathan & Joachims (2015a): derivation of an alternative objective and use of a deep networks
. This paper leverages a set of diverse results

significance
- The proposed method can only be applied if propensity scores were recorded when the data was generated.
- no test on a real setting
++ The proposed method is supported both by theoretical insights and empirical experiments.
+ empirical improvement with respect to previous methods


details/typos:

3.1, p3: R^(h) has an indexed parenthesis
5.2; and we more details
5.3: so that results more comparable","[7, 5, 4]","[' Good paper, accept', ' Marginally below acceptance threshold', ' Ok but not good enough - rejection']","[3, 5, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer explicitly states positive aspects like ""well-written"", ""clear"", and ""supported by theory and empirical results."" They also commend the originality and significance of the work. While they have a minor disagreement on a specific point, their tone is constructive and suggestive rather than dismissive. This suggests an overall positive sentiment leaning towards neutral due to the constructive criticism. The language used is polite and professional throughout the review.",60.0,100.0
A Self-Organizing Memory Network,"['Callie Federer', 'Joel Zylberberg']",Reject,2018,"[19, 15, 30]","[24, 20, 34]","[223, 121, 121]","[111, 62, 76]","[11, 20, 0]","[101, 39, 45]","A neural network model consisting of recurrently connected neurons and one or more readouts is introduced which aims to retain some output over time. A plasticity rule for this goal is derived. Experiments show the robustness of the network with respect to noisy weight updates, number of non-plastic connections, and sparse connectivity. Multiple consecutive runs increase the performance; furthermore, remembering multiple stimuli is possible. Finally, ideas for the biological implementation of the rule are suggested.

While the presentation is generally comprehensible a number of errors and deficits exist (see below). In general, this paper addresses a question that seems only relevant from a neuroscience perspective. Therefore, I wonder whether it is relevant in terms of the scope of this conference. I also think that the model is rather speculative. The authors argue that the resulting learning rule is biologically plausible. But even if this is the case, it does not imply that it is implemented in neuronal circuits in the brain. As far as I can see, there exists no experimental evidence for this rule. 

The paper shows the superiority of the proposed model over the approach of Druckmann & Chkolvskii (2012), however, it lacks in-depth analysis of the network behavior. Specifically, it is not clear how the information is stored. Do neurons show time-varying responses as in Druckmann & Chkolvskii (2012) or do all neuron stabilize within the first 50 ms (as in Fig. 2A, it is not detailed how the neurons shown there have been selected)? Do the weights change continuously within the delay period or do they also converge rapidly? This question is particularily important when considering multiple consecutive trials (cf. Fig. 5) as it seem that a specific but constant network architecture can retain the desired stimulus without further plasticity. Weight histograms should be presented for the different cases and network states. Also, since autapses are allowed, an analysis of their role should be performed. This information is vital to compare the model to findings from neuroscience and judge the biologic realism.

The target used is \hat{s}(t) / \hat{s}(t = 0), this is dubbed ""fraction of stimulus retained"". In most plots, the values for this measure are <= 1, but in Fig. 3A, the value (for the FEVER network) is > 1. Thus, the name is arguably not well-chosen: how can a fraction of remembrance be greater than one? Also, in a realistic environment, it is not clear that the neuronal activities decay to zero (resulting in \hat{s}(t) also approaching zero). A squared distances measure should therefore be considered.

It is not clear from the paper when and how often weight updates are performed. Therefore, the biologic plausability cannot be assessed, since the learning rule might lead to much more rapid changes of weights than the known learning rules in biological neural networks. Since the goal seems to be biologic realism, generally, spiking neurons should be used for the model. This is important as spiking neural networks are much more fragile than artificial ones in terms of stability.

Further remarks:

- In Sec. 3.2.1, noise is added to weight updates. The absolute values of alpha are hard to interpret since it is not clear in what range the weights, activities, and weight updates typically lie.

- In Sec. 3.2.2 it is shown that 10% plastic synapses is enough for reasonable performance. In this case, it should be investigated whether the full network is essential for the memory task at all (especially since later, it is argued that 100 neurons can store up to 100 stimuli).

- For biologic realism, just assuming that the readout value at t = 0 is the target seems a bit too simple. How does this output arise in the first place? At least, an argument for this choice should be presented.


Remarks on writing:

- Fig. 1A is too small to read.

- The caption of Fig. 4C is missing.

- In Fig. 7AB, q_i and q_j are swapped. Also, it is unclear in the figure to which connection the ds and qs belong.

- In 3.6.1, Fig. 7 is referenced, but in the figure the terminology of Eq. 5 is used, which is only introduced in Sec. 3.6.2. This is confusing.

- The beginning of Sec. 3.6 claims that all information is local except d\hat{s}_k / dt, but this is not the case as d_i is not local (which is explained later).

- The details of the ""stimulus presentation"" (i.e. it is not performed explicitly) should be emphasised in 2.1. Also, the description of the target \hat{s} is much clearer in 3.4 than in 2.1 (where it should primarily be explained).

- The title of the citation Cowan (2010) is missing.

- In Fig. 2A, the formulas are too small too read in a printed version.

- In Sec. 3.6.1 some sums are given over k, but k is also the index of a neuron in Fig. 7A (which is referenced there), this can be ambiguous and could be changed. ","[4, 3, 4]","[' Ok but not good enough - rejection', ' Clear rejection', ' Ok but not good enough - rejection']","[4, 2, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is largely critical of the paper, pointing out a lack of depth in the analysis, questioning its relevance to the conference, and finding the model speculative. While the reviewer acknowledges the comprehensibility of the presentation, the numerous concerns and the questioning of the paper's fundamental direction point towards a negative sentiment. However, the critique is presented in a professional, scientific manner, focusing on content and methodology rather than resorting to personal attacks or dismissive language. Therefore, the politeness remains within a neutral to moderately positive range.",-40.0,40.0
Quadrature-based features for kernel approximation,"['Marina Munkhoeva', 'Yermek Kapushev', 'Evgeny Burnaev', 'Ivan Oseledets']",Reject,2018,"[1, 4, 6, 11]","[6, 8, 11, 16]","[15, 13, 282, 233]","[5, 4, 108, 41]","[8, 6, 143, 117]","[2, 3, 31, 75]","The paper proposes to improve the kernel approximation of random features by using quadratures, in particular, stochastic spherical-radial rules. The quadrature rules have smaller variance given the same number of random features, and experiments show its reconstruction error and classification accuracies are better than existing algorithms.

It is an interesting paper, but it seems the authors are not aware of some existing works [1, 2] on quadrature for random features. Given these previous works, the contribution and novelty of the paper is limited.

[1] Francis Bach. On the Equivalence between Kernel Quadrature Rules and Random Feature Expansions. JMLR, 2017.
[2] Tri Dao, Christopher De Sa, Christopher Ré. Gaussian Quadrature for Kernel Features. NIPS 2017","[4, 6, 7]","[' Ok but not good enough - rejection', ' Marginally above acceptance threshold', ' Good paper, accept']","[3, 4, 5]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review starts with a positive statement acknowledging the paper's interesting proposal and positive results. However, it quickly points out a lack of awareness regarding existing literature, which diminishes the perceived novelty and contribution. The language remains factual and professional throughout. Therefore, the sentiment leans slightly towards the negative side due to the limited novelty, and the politeness remains neutral.",-20.0,0.0
DDRprog: A CLEVR Differentiable Dynamic Reasoning Programmer,"['Joseph Suarez', 'Justin Johnson', 'L. Fei-Fei']",Reject,2018,"[2, 4, 16]","[7, 9, 21]","[10, 74, 406]","[4, 28, 226]","[6, 43, 147]","[0, 3, 33]","
Summary: This paper leverages an explicit program format and proposes a stack based RNN to solve question answering. The paper shows state-of-the art performance on the CLEVR dataset.

Clarity:
- The description of the model is vague: I have to looking into appendix on what are the Cell and Controller function.
- The authors should also improve the intro and related work section. Currently there is a sudden jump between deep learning and the problem of interest. Need to expand the related work section to go over more literature on structured RNN.

Pros:
- The model is fairly easy to understand and it achieves state-of-the-art performance on CLEVR.
- The model fuses text and image features in a single model.

Cons:
- This paper doesn’t mention recursive NN (Socher et al., 2011) and Tree RNN (Tai et al., 2015). I think they have fairly similar structure, at least conceptually, the stack RNN can be thought as a tree parser. And since the push/pop operations are static (based on the inputs), it’s no more different than encoding the question structure in the tree edges.
- The IEP (Cells) module (Johnson et al., 2017) seems to do all the heavy-lifting in my opinion. That’s why the proposed method only uses 9M parameters. The task isn’t very challenging to learn because all the stack operations are already given. Table 1 should note clearly which methods use problem specific parsing information to train and which use raw text. Based on my understanding of FiLM at least, they use raw words instead of groundtruth parse trees. So it’s not very surprising that the proposed method can outperform FiLM (by a little bit).
- I don’t fully agree with the title - the stack operations are not differentiable. So whatever network that outputs the stack operation cannot be jointly learned with gradients. This is based on the if-else statements I see in Algorithm 1.

Conclusion:
- Since the novelty is limited and it requires explicit program supervision, and the performance is only on par with the state-of-the-art (FiLM), I am not convinced that this paper brings enough contribution to be accepted. Weak reject.

References:
- Socher, R., Lin, C., Ng, A.Y., Manning, C.D. Parsing Natural Scenes and Natural Language with Recursive Neural Networks. The 28th International Conference on Machine Learning (ICML 2011).
- Tai, K.S., Socher, R., Manning C.D. Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks. The 53rd Annual Meeting of the Association for Computational Linguistics (ACL 2015).","[5, 5, 6]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Marginally above acceptance threshold']","[2, 2, 2]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']","The reviewer acknowledges the paper's strengths, such as the model's simplicity and good performance. However, they also point out significant limitations, including a lack of novelty, dependence on explicit program supervision, and only marginal improvement over existing methods. The reviewer's conclusion of ""weak reject"" and the detailed critique contribute to a somewhat negative sentiment. The language used is professional and avoids harsh or disrespectful phrasing, indicating a neutral to polite tone.",-25.0,50.0
Reward Design in Cooperative Multi-agent Reinforcement Learning for Packet Routing,"['Hangyu Mao', 'Zhibo Gong', 'Zhen Xiao']",Reject,2018,"[3, 2, 20]","[8, 4, 25]","[32, 10, 90]","[16, 4, 52]","[13, 5, 11]","[3, 1, 27]","The paper provides an empirical study of different  reward schemes for cooperative multi-agent reinforcement learning.  A number of alternative reward schemes are proposed, partly based on existing literature. These reward schemes are evaluated empirically in a packet routing problem.  

The approach taken by this paper is very ad-hoc. It is not clear to me that this papers offers any general insights or methodologies for reward design for MARL. The only conclusions that can be drawn from this paper is which reward performs best on these specific problem instances(and even this is hard to conclude from the paper).  

In general, it seems strange to propose the packet routing problems as benchmark environments for reward design. From the descriptions in the paper these environments seem relatively complex and make it difficult to study the actual learning dynamics. The results shown provide global performance but do not allow to study specific properties.

The paper is also quite hard to read.  It is littered with non-intuitive abbreviations. The methods and experiments are poorly explained. It claims to study rewards for multi-agent reinforcement learning, but never properly details the learning setting that is considered or how this affects the choice of rewards.  Experiments are mostly described in terms of method A outperforms method B. No effort is made to investigate the cause of these results or to design experiments that would offer deeper insights. The graphs are not properly labelled, poorly described in general and are almost impossible to interpret. The main results are presented simply as a large table of raw performance numbers. This paper does not seem to offer any major fundamental or applied contributions. 
","[2, 5, 5]","[' Strong rejection', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[4, 2, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is fairly confident that the evaluation is correct']","The review is highly critical of the paper's methodology, clarity, and contribution. It uses strong negative language such as ""ad-hoc"", ""not clear"", ""strange"", ""poorly explained"", ""impossible to interpret"", and ""does not seem to offer any major contributions."" This indicates a negative sentiment. While the reviewer provides direct and critical feedback, they maintain a professional tone without resorting to personal attacks.",-75.0,20.0
Softmax Q-Distribution Estimation for Structured Prediction: A Theoretical Interpretation for RAML,"['Xuezhe Ma', 'Pengcheng Yin', 'Jingzhou Liu', 'Graham Neubig', 'Eduard Hovy']",Reject,2018,"[9, 6, 7, 10, 34]","[14, 11, 12, 15, 39]","[79, 61, 20, 641, 517]","[37, 25, 8, 350, 350]","[42, 26, 9, 245, 123]","[0, 10, 3, 46, 44]","The authors claim three contributions in this paper. (1) They introduce the framework of softmax Q-distribution estimation, through which they are able to interpret the role the payoff distribution plays in RAML. Specifically, the softmax Q-distribution serves as a smooth approximation to the Bayes decision boundary. The RAML approximately estimates the softmax Q-distribution, and thus approximates the Bayes decision rule. (2) Algorithmically, they further propose softmax Q-distribution maximum likelihood (SQDML) which improves RAML by achieving the exact Bayes decision boundary asymptotically. (3) Through one experiment using synthetic data on multi-class classiﬁcation and one using real data on image captioning, they show that SQDML is consistently as good or better than RAML on the task-speciﬁc metrics that is desired to optimize. 

I found the first contribution is sound, and it reasonably explains why RAML achieves better performance when measured by a specific metric. Given a reward function, one can define the Bayes decision rule. The softmax Q-distribution (Eqn. 12) is defined to be the softmax approximation of the deterministic Bayes rule. The authors show that the RAML can be explained by moving the expectation out of the nonlinear function and replacing it with empirical expectation (Eqn. 17). Of course, the moving-out is biased but the replacing is unbiased. 

The second contribution is partially valid, although I doubt how much improvement one can get from SQDML. The authors define the empirical Q-distribution by replacing the expectation in Eqn. 12 with empirical expectation (Eqn. 15). In fact, this step can result in biased estimation because the replacement is inside the nonlinear function. When x is repeated sufficiently in the data, this bias is small and improvement can be observed, like in the synthetic data example. However, when x is not repeated frequently, both RAML and SQDML are biased. Experiment in section 4.1.2 do not validate significant improvement, either.

The numerical results are relatively weak. The synthetic experiment verifies the reward-maximizing property of RAML and SQDML. However, from Figure 2, we can see that the result is quite sensitive to the temperature \tau. Is there any guidelines to choose \tau? For experiments in Section 4.2, all of them are to show the effectiveness of RAML, which are not very relevant to this paper. These experiment results show very small improvement compared to the ML baselines (see Table 2,3 and 5).  These results are also lower than the state of the art performance. 

A few questions:
(1). The author may want to check whether (8) can be called a Bayes decision rule. This is a direct result from definition of conditional probability. No Bayesian elements, like prior or likelihood appears here.
(2). In the implementation of SQDML, one can sample from (15) without exactly computing the summation in the denominator. Compared with the n-gram replacement used in the paper, which one is better?
(3). The authors may want to write Eqn. 17 in the same conditional form of Eqn. 12 and Eqn. 14. This will make the comparison much more clear.
(4). What is Theorem 2 trying to convey? Although \tau goes to 0, there is still a gap between Q and Q'. This seems to suggest that for small \tau, Q' is not a good approximation of Q. Are the assumptions in Theorem 2 reasonable? There are several typos in the proof of Theorem 2. 
(5). In section 4.2.2, the authors write ""the rewards we directly optimized in training (token-level accuracy for NER and UAS for dependency parsing) are more stable w.r.t. τ than the evaluation metrics (F1 in NER), illustrating that in practice, choosing a training reward that correlates well with the evaluation metric is important"". Could you explain it in more details?

","[6, 5, 5]","[' Marginally above acceptance threshold', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[3, 4, 2]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']","The review acknowledges the soundness of the first contribution and finds the second contribution partially valid. However, the reviewer raises concerns about the significance of the improvements, the weakness of the numerical results, and the sensitivity to a parameter. The reviewer also poses several questions and points out areas for improvement. The language used is formal, objective, and provides constructive criticism without resorting to personal attacks or disrespectful language.",20.0,80.0
Relevance of Unsupervised Metrics in Task-Oriented Dialogue for Evaluating Natural Language Generation,"['Shikhar Sharma', 'Layla El Asri', 'Hannes Schulz', 'Jeremie Zumer']",Reject,2018,"[6, 7, 13, 6]","[9, 11, 17, 6]","[19, 31, 59, 7]","[8, 18, 35, 3]","[10, 13, 13, 4]","[1, 0, 11, 0]","1) This paper conducts an empirical study of different unsupervised metrics' correlations in task-oriented dialogue generation. This paper can be considered as an extension of Liu, et al, 2016 while the later one did an empirical study in non-task-oriented dialogue generation.  

2)My questions are as follows:
i) The author should give the more detailed definition of what is non-task-oriented and task-oriented dialogue system. The third paragraph in the introduction should include one use case about non-task-oriented dialogue system, such as chatbots.
ii) I do not think DSTC2 is good dataset here in the experiments. Maybe the dataset is too simple with limited options or the training/testing are very similar to each other, even the random could achieve very good performance in table 1 and 2. For example, the random solution is only 0.005 (out of 1) worse then d-scLSTM, and it also has a close performance compared with other metrics. Even the random could achieve 0.8 (out of 1) in BLEU, this is a very high performance.
iii) About the scatter plot Figure 3, the authors should include more points with a bad metric score (similar to Figure 1 in Liu 2016). 
iv) About the correlations in figure b, especially for BLEU and METEOR, I do not think they have good correlations with human's judgments. 
v) BLEU usually correlates with human better when 4 or more references are provided. I suggest the authors include some dataset with 4 or more references instead of just 2 references.
","[5, 4, 5]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review starts with a relatively neutral observation of the paper's topic and relation to prior work. However, the following list of questions and suggestions reveals concerns about the methodology and results.  The reviewer points out potential flaws in the dataset choice and questions the validity of some findings. While the reviewer offers constructive criticism and suggestions for improvement, the overall tone suggests a somewhat negative sentiment due to the perceived flaws. The language remains polite and professional throughout, focusing on the scientific content and avoiding personal remarks.",-30.0,70.0
Latent Topic Conversational Models,"['Tsung-Hsien Wen', 'Minh-Thang Luong']",Reject,2018,"[9, 9]","[13, 14]","[66, 53]","[34, 25]","[26, 26]","[6, 2]","This paper proposed the combination of topic model and seq2seq conversational model.
The idea of this combination is not surprising but the attendee of ICLR might be interested in the empirical results if the model clearly outperforms the existing method in the experimental results.
However, I'm not sure that the empirical evaluation shows the really impressive results.
In particular, the difference between LV-S2S and LTCM seem to be trivial.
There are many configurations in the LSTM-based model.
Can you say that there is no configuration of LV-S2S that outperforms your model?
Moreover, the details of human evaluation are not clear, e.g., the number of users and the meaning of each rating.

","[4, 6, 5]","[' Ok but not good enough - rejection', ' Marginally above acceptance threshold', ' Marginally below acceptance threshold']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer finds the paper's idea unsurprising but acknowledges potential interest in strong empirical results. However, they express doubt about the presented results, finding the performance difference between the proposed model and a baseline (LV-S2S) to be ""trivial."" The reviewer also raises concerns about the experimental methodology and lack of detail in the human evaluation. Overall, this suggests skepticism and a leaning towards the negative side. The language, while direct, remains professional and polite throughout.",-20.0,60.0
Understanding GANs: the LQG Setting,"['Soheil Feizi', 'Changho Suh', 'Fei Xia', 'David Tse']",Reject,2018,"[11, 17, 27, 26]","[16, 22, 32, 31]","[155, 140, 355, 333]","[61, 71, 222, 112]","[83, 41, 50, 123]","[11, 28, 83, 98]","*Paper summary*

The paper considers GANs from a theoretical point of view. The authors approach GANs from the 2-Wasserstein point of view and provide several insights for a very specific setting. In my point of view, the main novel contribution of the paper is to notice the following fact:

(*) It is well known that the 2-Wasserstein distance W2(PY,QY) between multivariate Gaussian PY and its empirical version QY scales as $n^{-2/d}$, i.e. converges very slow as the dimensionality of the space $d$ increases. In other words, QY is not such a good way to estimate PY in this setting. A somewhat better way is use a Gaussian distribution PZ with covariance matrix S computed as a sample covariance of QY. In this case W2(PY, PZ) scales as $\sqrt{d/n}$.

The paper introduces this observation in a very strange way within the context of GANs. Moreover, I think the final conclusion of the paper (Eq. 19) has a mistake, which makes it hard to see why (*) has any relation to GANs at all.

There are several other results presented in the paper regarding relation between PCA and the 2-Wasserstein minimization for Gaussian distributions (Lemma 1 & Theorem 1). This is indeed an interesting point, however the proof is almost trivial and I am not sure if this provides any significant contribution for the future research.

Overall, I think the paper contains several novel ideas, but its structure requires a *significant* rework and in the current form it is not ready for being published. 

*Detailed comments*

In the first part of the paper (Section 2) the authors propose to use the optimal transport distance Wc(PY, g(PX)) between the data distribution PY (or its empirical version QY) and the model as the objective for GAN optimization. This idea is not novel: WGAN [1] proposed (and successfully implemented) to minimize the particular case of W1 distance by going through the dual form, [2] proposed to approach any Wc using auto-encoder reformulation of the primal (and also shoed that [5] is doing exactly W2 minimization), and [3] proposed the same using Sinkhorn algorithm. So this point does not seem to be novel.

The rest of the paper only considers 2-Wasserstein distance with Gaussian PY and Gaussian g(PX) (which I will abbreviate with R), which looks like an extremely limited scenario (and certainly has almost no connection to the applications of GANs).

Section 3 first establishes a relation between PCA and minimizing 2-Wasserstein distance for Gaussian distributions (Lemma 1, Theorem 1). Then the authors show that if R minimizes W2(PY, R) and QR minimizes W2(QY, QR) then the excess loss W2(PY, QR) - W2(PY, R) approaches zero at the rate $n^{-2/d}$ (both for linear and unconstrained generators). This result basically provides an upper bound showing that GANs need exponentially many samples to minimize W2 distance. I don't find these results novel, as they already appeared in [4] with a matching lower bound for the case of Gaussians (Theorem B.1 in Appendix can be modified easily to show this). As the authors note in the conclusion of Section 3, these results have little to do with GANs, as GANs are known to learn quite quickly (which contradicts the theory of Section 3).

Finally, in Section 4 the authors approach the same W2 problem from its dual form and notice that for the LQG model the optimal discriminator is quadratic. Based on this they reformulate the W2 minimization for LQG as the constrained optimization with respect to p.d. matrix A (Eq 16). The same conclusion does not work unfortunately for W2(QY, R), which is the real training objective of GANs. Theorem 3 shows that nevertheless, if we still constrain discriminator in the dual form of W2(QY, R) to be quadratic, the resulting soliton QR* performs the empirical PCA of Pn. 

This leads to the final conclusion of the paper, which I think contains a mistake. In Eq 19 the first equation, according to the definitions of the authors, reads
\[
W2(PY, QR) = W2(PY, PZ),   (**)
\]
where QR is trained to minimize min_R W2(QY, R) and PZ is as defined in (*) in the beginning of these notes. 
However, PZ is not the solution of min_R W2(QY, R) as the authors notice in the 2nd paragraph of page 8. Thus (**) is not true (at least, it is not proved in the current version of the text). PZ is a solution of min_R W2(QY, R) *where the discriminator is constrained to be quadratic*. This mismatch is especially strange, given the authors emphasize in the introduction that they provide bounds on divergences which are the same as used during the training (see 2nd paragraph on page 2) --- here the bound is on W2, but the empirical GAN actually does a regularized training (with constrained discriminator).

Finally, I don't think the experiments provide any convincing insights, because the authors use W1-minimization to illustrate properties of the W2. Essentially the authors say ""we don't have a way to perform W2 minimization, so we rather do the W1 minimization and assume that these two are kind of similar"".

* Other comments *
(1) Discussion in Section 2.1 seems to never play a role in the paper.
(2) Page 4: in p-Wasserstein distance, ||.|| does not need to be a Euclidean metric. It can be any metric.
(3) Lemma 2 seems to repeat the result from (Canas and Rosasco, 2012) as later cited by authors on page 7?
(4) It is not obvious how does Theorem 2 translate to the excess loss? 
(5) Section 4. I am wondering how exactly the authors are going to compute the conjugate of the discriminator, given the discriminator most likely is a deep neural network?


[1] Arjovsky et al., Wasserstein GAN, 2017
[2] Bousquet et al, From optimal transport to generative modeling: the VEGAN cookbook, 2017
[3] Genevay et al., Learning Generative Models with Sinkhorn Divergences, 2017
[4] Arora et al, Generalization and equilibrium in GANs, 2017
[5] Makhazani et al., Adversarial Autoencoders, 2015","[4, 4, 5]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer raises serious concerns about the paper's novelty, correctness (potential mistake in Equation 19), and relevance to GANs. They find some ideas interesting but ultimately judge the paper not ready for publication in its current form. The tone is critical and points out flaws directly, but remains within the bounds of academic discourse. There's no personal attack or disrespectful language.",-50.0,50.0
Interpretable and Pedagogical Examples,"['Smitha Milli', 'Pieter Abbeel', 'Igor Mordatch']",Reject,2018,"[3, 17, 12]","[8, 22, 17]","[26, 608, 109]","[13, 291, 48]","[13, 291, 56]","[0, 26, 5]","This paper looks at a specific aspect of the learning-to-teach problem, where the learner is assumed to have a teacher that selects training examples for the student according to a strategy. The teacher's strategy should also be  learned from data.  In this case the authors look at finding interpretable teaching strategies.  The authors define the ""good"" strategies as similar to intuitive strategies (based on human intuition about the structure of the domain) or strategies that are effective for teaching humans.  
The suggested method follow an iterative process in which the student and teacher are interchangeably used. At each iteration the teacher generates  examples based on the students current concept. 

I found it very difficult to follow the claims in the paper. Why is it assumed that human intuition is necessarily good?  The experiments do not answer these questions, but are designed to show that the suggested approach follows human intuition. There are not enough details to get a good grasp of the suggested method and the different choices for it,  and similarly the experiments are not described in a very convincing way. Specifically - the domains picked seem very contrived,  there actual results are not reported, the size of the data seems minimal so it's not clear what is actually learned.
How would you analyze the teaching strategy in realistic cases, where there is no simple intuitive strategy? This would be more convincing.","[4, 8, 8]","[' Ok but not good enough - rejection', ' Top 50% of accepted papers, clear accept', ' Top 50% of accepted papers, clear accept']","[3, 4, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer highlights several concerns, including a lack of clarity regarding the claims, methodology, and experimental setup. They question the assumption of human intuition being ""good"" and find the chosen domains contrived. The lack of detailed results and the use of minimal data further contribute to the negative sentiment. While the reviewer doesn't employ overtly rude language, the critique is quite direct and pointed, suggesting a level of dissatisfaction with the paper.",-50.0,20.0
A Boo(n) for Evaluating Architecture Performance,"['Ondrej Bajgar', 'Rudolf Kadlec', 'and Jan Kleindienst']",Reject,2018,"[3, 12, 23]","[8, 15, 23]","[12, 29, 48]","[4, 19, 38]","[7, 9, 7]","[1, 1, 3]","This manuscript raises an important issue regarding the current lack of standardization regarding methods for evaluating and reporting algorithm performance in deep learning research.  While I believe that raising this issue is important and that the method proposed is a step in the right direction, I have a number of concerns which I will list below.  One risk is that if the proposed solution is not adequate or widely agreeable then we may find a proliferation of solutions from which different groups might pick and choose as it suits their results!

The method of choosing the best model under 'internal' cross-validation to take through to 'external' cross-validation against a second hold-out set should be regarded as one possible stochastic solution to the optimisation problem of hyper-parameter selection.  The authors are right to emphasize that this should be considered part of the cost of the technique, but I would not suggest that one specify a 'benchmark' number of trials (n=5) for comparison.  Rather I would suggest that this is a decision that needs to be explored and understood by the researchers presenting the method in order to understand the cost/benefit ratio for their algorithm provided by attempting to refine their guess of the optimal hyperparameters.  This would then allow for other methods not based on internal cross-validation to be compared on a level footing.

I think that the fundamental issue of stochasticity of concern for repeatability and generalisability of these performance evaluation exercises is not in the stochastic optimisation search but in the use of a single hold-out sample.  Would it not be wise to insist on a mean performance (a mean Boo_n or other) over multiple random partitions of the entire dataset into training and hold-out?  I wonder if in theory both the effect of increasing n and the mean hold-out performance could be learnt efficiently with a clever experimental design. 

Finally, I am concerned with the issue of how to compute the suggested Boo_n score.  Use of a parameteric Gaussian approximation is a strong assumption, while bootstrap methods for order statistics can be rather noisy.  It would be interesting to see a comparison of the results from the parametric and non-parameteric Boo_n versions applied to the test problems.  ","[6, 4, 4]","[' Marginally above acceptance threshold', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer acknowledges the importance of the manuscript's topic and finds the proposed method a step in the right direction. However, they also express several concerns and suggest alternative approaches. The language is constructive and professional, suggesting areas for improvement rather than outright dismissing the work.",50.0,70.0
Recurrent Relational Networks for complex relational reasoning,"['Rasmus Berg Palm', 'Ulrich Paquet', 'Ole Winther']",Reject,2018,"[2, 16, 26]","[6, 20, 31]","[27, 40, 140]","[12, 23, 53]","[14, 12, 46]","[1, 5, 41]","This paper introduces recurrent relational networks: a deep neural network for structured prediction (or relational reasoning). The authors use it to achieve state-of-the-art performance on Soduku puzzles and the BaBi task (a text based QA dataset designed as a set of to toy prerequisite tasks for reasoning).

Overall I think that by itself the algorithm suggested in the paper is not enough to be presented in ICLR, and on the other hand the authors didn't show it has a big impact (could do so by adding more tasks - as they suggest in the discussion). This is why I think the paper is marginally below the acceptance threshold but could be convinced otherwise.

C an the authors give experimental evidences for their claim: ""As such, the network could use a small part of the hidden state for retaining a current best guess, which might remain constant over several steps, and other parts of the hidden state for running a non-greedy..."" - 

Pros
- The idea of the paper is clearly presented, the algorithm is easy to follow.
- The motivation to do better relational reasoning is clear and the network suggested in this paper succeeds to achieve it in the challenging tasks.

Cons
- The recurrent relational networks is basically a complex learned message passing algorithm. As the authors themselves state there are several works from recent years which also tackle this (one missing reference is Deeply Learning the Messages in Message Passing Inference of Lin et al from NIPS 2016). It would been interesting to compare results to these algorithms.
- For the Sudoku the proposed architecture of the network seems a bit to complex, for example why do a 16 embedding is needed for representing a digit between 0-9? Some other choices (batch size of 252) seem very specific.","[5, 3, 5]","[' Marginally below acceptance threshold', ' Clear rejection', ' Marginally below acceptance threshold']","[3, 5, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer finds the algorithm interesting and well-presented ('The idea of the paper is clearly presented, the algorithm is easy to follow.', 'The motivation to do better relational reasoning is clear and the network suggested in this paper succeeds to achieve it in the challenging tasks.'). However, they have concerns about the significance ('by itself the algorithm suggested in the paper is not enough to be presented in ICLR') and suggest additional experiments are needed to demonstrate impact. The reviewer also provides constructive criticism and suggestions for improvement. Overall, the tone is critical but professional and aims to help improve the paper.",-10.0,50.0
Balanced and Deterministic Weight-sharing Helps Network Performance,"['Oscar Chang', 'Hod Lipson']",Reject,2018,"[38, 23]","[43, 28]","[23, 197]","[13, 110]","[6, 36]","[4, 51]","The manuscript advocates to study the weight sharing in a more systematic way by proposing ArbNets which defines the weight sharing function as a hash function. In this framework, any existing neural network architectures, including CNN and RNN, could be incorporated into ArbNets.

The manuscript is not well written. There are multiple grammar errors and typos. Content-wise, it is already well known that CNN and RNN can be expressed as general MLP with weight sharing. The introduction of ArbNets does not bring much value or insight to this area. So it seems that most content before experimental section is common sense.

In the experimental section, it is interesting to see how different hash function with different level of entropy can affect the performance of neural nets. However, this single observation cannot enrich the whole manuscript. Two questions:
(1) What is the definition of sparsity here, and how is it controlled?
(2) There seems to be a step change in Figure 3. All the results are either between 10 to 20, or near 50. And the blue line goes up and down. Is this expected?","[4, 4, 4]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with a relatively neutral summary of the manuscript's contributions. However, it quickly becomes negative by stating that the manuscript is ""not well written,"" the content is ""common sense,"" and the introduction of ArbNets does not bring much value. The reviewer acknowledges one interesting observation in the experimental section but ultimately finds it insufficient to enrich the entire manuscript. The use of phrases like ""common sense"" and the dismissive tone regarding the contributions contribute to the negative sentiment. While the language is critical, it avoids overtly rude or disrespectful language, staying within the bounds of professional academic criticism.",-40.0,20.0
Classification and Disease Localization in Histopathology Using Only Global Labels: A Weakly-Supervised Approach,"['Pierre Courtiol', 'Eric W. Tramel', 'Marc Sanselme', 'Gilles Wainrib']",Reject,2018,"[1, 9, 1, 9]","[3, 14, 1, 9]","[2, 40, 2, 17]","[0, 15, 1, 3]","[2, 20, 1, 6]","[0, 5, 0, 8]","The authors approach the task of labeling histology images with just a single global label, with promising results on two different data sets. This is of high relevance given the difficulty in obtaining expert annotated data. At the same time the key elements of the presented approach remain identical to those in a previous study, the main novelty is to replace the final step of the previous architecture (that averages across a vector) with a multiplayer perceptron.  As such I feel that this would be interesting to present if there is interest in the overall application (and results of the 2016 CVPR paper), but not necessarily as a novel contribution to MIL and histology image classification.

Comments to the authors:

* The intro starts from a very high clinical level. A introduction that points out specifics of the technical aspects of this application, the remaining technical challenges, and the contribution of this work might be appreciated by some of your readers.
* There is preprocessing that includes feature extraction, and part of the algorithm that includes the same feature extraction. This is somewhat confusing to me and maybe you want to review the structure of the sections.  You are telling us you are using the first layer (P=1) of the ResNet50 in the method description, and you mention that you are using the pre-final layer in the preprocessing section. I assume you are using the latter, or is P=1 identical to the prefinal layer in your notation?  Tell us. Moreover, not having read Durand 2016, I would appreciate a few more technical details or formal description here and there.  Can you detail about the ranking method in Durand 2016, for example?
* Would it make sense to discuss Durand 2016 in the base line methods section? 
* To some degree this paper evaluates WELDON (Durand 2016) on new data, and compares it against and an extended WELDON algorithm called CHOWDER that features the final MLP step. Results in table 1 suggest that this leads to some 2-5% performance increase which is a nice result.  I would assume that experimental conditions (training data, preprocessing, optimization, size of ensemble) are kept constant in between those two comparisons? Or is there anything of relevance that also changed (like size of the ensemble, size of training data) because the WELDON results are essentially previously generated results? Please comment in case there are differences. ","[5, 6, 5]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Marginally below acceptance threshold']","[3, 3, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer acknowledges the relevance of the paper's application and the promising results. However, they also point out that the novelty is limited to replacing one component of a previous method. The reviewer's tone is neutral and professional, posing questions for clarification and suggesting improvements without resorting to harsh language.",20.0,80.0
AMPNet: Asynchronous Model-Parallel Training for Dynamic Neural Networks,"['Alexander L. Gaunt', 'Matthew A. Johnson', 'Alan Lawrence', 'Maik Riechert', 'Daniel Tarlow', 'Ryota Tomioka', 'Dimitrios Vytiniotis', 'Sam Webster']",Reject,2018,"[3, 30, 3, 13, 13, 14, 2]","[7, 35, 6, 18, 18, 18, 6]","[26, 31, 8, 83, 76, 64, 4]","[13, 16, 3, 46, 38, 40, 2]","[13, 4, 2, 35, 24, 12, 2]","[0, 11, 3, 2, 14, 12, 0]","This paper presents AMPNet, that addresses parallel training for dynamic networks. This is accomplished by building a static graph like IR that can serve as a target for compilation for high-level libraries such as tensor flow. In the IR each node of the computation graph is a parallel worker, and synchronization occurs when a sufficient number of gradients have been accumulated. The IR uses constructs such as concat, split, broadcast,.. allowing dynamic, instance dependent control flow decisions. The primary improvement in training performance is from reducing synchronization costs.

Comments for the author:

The paper proposes a solution to an important problem of model parallel training especially over dynamic batching that is increasingly important as we see more complex models where batching is not straightforward. The proposed solution can be effective. However, this is not really evident from the evaluation. Furthermore, the paper can be a little dense read for the ICLR audience. I have the following additional concerns:

1) The paper stresses new hardware throughout the paper. The paper also alludes to “simulator"" of a 1 TFLOPs FPGA in the conclusion. However, your entire evaluation is over CPU. The said simulator is a bunch of sleep() calls (unless some details are skipped). I would encourage the authors to remove these references since these new devices have very different hardware behavior. For example, on a real constrained device, you may not enjoy a large L2 cache which you are benefitting from by doing an entire evaluation over CPUs. Likewise, the vector instruction processing behavior is also very different since these devices have limited power budgets and may not be able to support AVX style instructions. Unless an actual simulator like GEM5 is used, a correct representation of what hardware environment is being used is necessary before making claims that this is ideal for emerging hardware.

2) To continue on the hardware front and the evaluation, I feel for this paper to be accepted or appreciated, a simulated hardware is not necessary. Personally, I found the evaluation with simulated sleep functions more confusing than helpful. An appropriate evaluation for this paper can be just benefits over CPU or GPUs, For example, you have a 7 TFLOPS device (e.g. a GPU or a CPU). Existing algorithms extract X TFLOPs of processing power and using your IR/system one gets Y effective TFLOPs and Y>X. This is all that is required. Currently, looking at your evaluation riddled with hypothetical hardware, it is unclear to me if this is helpful for existing hardware. For example, in Table 1, are Tensorflow numbers only provided over the 1 TFLOPs device (they correspond to the 1 TFLOPs column for all workloads except for MNIST)?  Do you use the parallelism at all in your Tensorflow baseline?  Please clarify.

3) How do you compare for dynamic batching with dynamic IR platforms like pytorch? Furthermore, more details about how dynamic batching is happening in benchmarks mentioned in Table 1 will be nice to have. Finally, an emphasis on the novel contributions of the paper will also be appreciated.

4) Finally, the evaluation appears to be sensitive to the two hyper-parameters introduced. Are they dataset specific? I feel tuning them would be rather cumbersome for every model given how sensitive they are (Figure 5).
","[4, 6, 6]","[' Ok but not good enough - rejection', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[5, 4, 5]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The reviewer acknowledges the importance of the problem tackled and the potential of the proposed solution (""The paper proposes a solution to an important problem... The proposed solution can be effective.""). However, they express several concerns, particularly regarding the evaluation methodology and clarity. The tone is critical, questioning the validity of the simulated hardware approach and pointing out potential flaws in the evaluation. However, the language remains professional and avoids personal attacks, focusing on technical aspects.",20.0,60.0
Avoiding degradation in deep feed-forward networks by phasing out skip-connections,"['Ricardo Pio Monti', 'Sina Tootoonian', 'Robin Cao']",Reject,2018,"[5, 13, 1]","[10, 17, 1]","[27, 4, 1]","[11, 3, 1]","[10, 0, 0]","[6, 1, 0]","UPDATED COMMENT
I've improved my score to 6 to reflect the authors' revisions to the paper and their response to my and R2's comments. I still think the work is somewhat incremental, but they have done a good job of exploring the idea (which is nice).

ORIGINAL REVIEW BELOW

The paper introduces an architecture that linearly interpolates between ResNets and vanilla deep nets (without skip connections). The skip connections are penalized by Lagrange multipliers that are gradually phased out during training. The resulting architecture outperforms vanilla deep nets and sometimes approaches the performance of ResNets.

It’s a nice, simple idea. However, I don’t think it’s sufficient for acceptance. Unfortunately, this seems to be a simple idea that doesn't work as well as the simpler idea (ResNets) that inspired it. Moreover, the experiments are weak in two senses: (i) there are lots of obvious open questions that should have been explored and closed, see below, and (ii) the results just aren’t that good. 

Comments:

1. Why force the Lag. multipliers to 1 at the end of training? It seems easy enough to treat the alphas as just more parameters to optimize with gradient descent. I would expect the resulting architecture to perform at least as well as variable action nets. If not, I’d be curious as to why.

2.Similarly, it’s not obvious that initializing the multipliers at 0.5 is the best choice. The “looks linear” initialization proposed in “The shattered gradients problem” (Balduzzi et al) implies that alpha=0 may work better. Did the authors try any values besides 0.5? 

3. The final paragraph of the paper discusses extending the approach to architectures with skip-connections. Firstly, it’s not clear to me what this would add, since the method is already interpolating in some sense between vanilla and resnets. Secondly, why not just do it? 

","[6, 6, 6]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer finds the core idea ""nice"" and ""simple"" but ultimately too incremental and not well-supported by the experiments. They explicitly state the work ""doesn't work as well as the simpler idea"", pushing the sentiment towards the negative.  The numerous ""obvious open questions"" further reinforce this. The updated score of 6/10, while improved, still confirms this isn't highly positive. 

Politeness remains neutral. The reviewer is direct and critical, but within the bounds of academic discourse. They don't resort to personal attacks or dismissive language.",20.0,0.0
Neighbor-encoder,"['Chin-Chia Michael Yeh', 'Yan Zhu', 'Evangelos E. Papalexakis', 'Abdullah Mueen', 'Eamonn Keogh']",Reject,2018,"[0, 25, 4, 16]","[5, 30, 9, 21]","[19, 45, 27, 39]","[11, 35, 20, 18]","[6, 6, 6, 6]","[2, 4, 1, 15]","This paper presents a variant of auto-encoder that relaxes the decoder targets to be neighbors of a data point. Different from original auto-encoder, where data point x and the decoder output \hat{x} are forced to be close, the neighbor-encoder encourage the decoder output to be similar to the neighbors of the input data point. By considering the neighbor information, the decoder targets would have smaller intra-class distances, thus larger inter-class distances, which helps to learn better separated latent representation of data in terms of data clusters. The authors conduct experiments on several real but relative small-scale data sets, and demonstrate the improvements of learned latent representations by using neighbors as targets. 

The method of neighbor prediction is a simple and small modification of the original auto-encoder, but seems to provide a way to augment the targets such that intra-class distance of decoder targets can be tightened. Improvements in the conducted experiments seem significant compared to the most basic auto-encoder.

Major issues:

There are some unaddressed theoretical questions. The optimal solution to predict the set of neighbor points in mean-squared metric is to predict the average of those points, which is not well justified as the averaged image can easily fall off the data manifold. This may lead to a more blurry reconstruction when k increases, despite the intra-class targets are tight. It can also in turn harm the latent representation when euclidean neighbors are not actually similar (e.g. images in cifar10/imagenet that are not as simple as 10 digits). This seems to be a defect of the neighbor-encoder method and is not discussed in the paper.

The data sets used in the experiments  are relatively small and simple, larger-scale experiments should be conducted. The fluctuations in Figure 9 and 10 suggest the significant variances in the results. Also, more complicated data/images can decrease the actual similarities of euclidean neighbors, thus affecting the results.

The baselines are weak. Only the most basic auto-encoder is compared, no additional variants or other data augmentation techniques are compared. It is possible other variants improve the basic auto-encoder in similar ways. 

Some results are not very well explained. It seems the performance increases monotonically as the number of neighbors increases (Figure 5, 9, 10). Will this continue or when will the performance decrease? I would expect it to decrease as the far away neighbors will be dissimilar. The authors can either attach the nearest neighbors figures or their statistics, and provide explanations on when and why the performance decrease is expected.

Some notations are confusing and need to be improved. For example, X and Y are actually the same set of images, the separation is a bit confusing; y_i \in y in last paragraph of page 4 is incorrect, should use something like y_i in N(y).","[4, 6, 5]","[' Ok but not good enough - rejection', ' Marginally above acceptance threshold', ' Marginally below acceptance threshold']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review starts with a neutral summary of the paper's contributions. While it acknowledges the simplicity and potential of the proposed method, it raises several major issues, questioning the theoretical grounding and experimental setup. The reviewer points out potential flaws and demands more rigorous experimentation and analysis. The tone is critical and points out shortcomings directly, but it maintains a professional and constructive approach by suggesting specific improvements and further investigations. Therefore, the sentiment leans towards the negative side due to the significant concerns raised, while the politeness remains relatively neutral.",-20.0,20.0
Synthesizing Robust Adversarial Examples,"['Anish Athalye', 'Logan Engstrom', 'Andrew Ilyas', 'Kevin Kwok']",Reject,2018,"[2, 2, 5, 3]","[6, 7, 10, 3]","[16, 37, 48, 3]","[6, 16, 21, 1]","[8, 21, 27, 2]","[2, 0, 0, 0]","Summary: This work proposes a way to create 3D objects to fool the classification of their pictures from different view points by a neural network.
Rather than optimizing the log-likelihood of a single example, the optimization if performed over a the expectation of a set of transformations of sample images. Using an inception v3 net, they create adversarial attacks on a subset of the imagenet validation set transformed by translations, lightening conditions, rotations, and scalings among others, and observe a drop of the classifier accuracy performance from 70% to less than 1%. They also create two 3D printed objects which most pictures taken from random viewpoints are fooling the network in its class prediction.
 

Main comments:
- The idea of building 3D adversarial objects is novel so the study is interesting. However, the paper is incomplete, with a very low number of references, only 2 conference papers if we assume the list is up to date. 
See for instance Cisse et al. Houdini: fooling Deep Structured Prediction Models, NIPS 2017 for a recent list of related work in this research area.
- The presentation of the results is not very clear. See specific comments below.
- It would be nice to include insights to improve neural nets to become less sensitive to these attacks.


Minor comments:
Fig1 : a bug with color seems to have been fixed
Model section: be consistent with the notations. Bold everywhere or nowhere
Results: The tables are difficult to read and should be clarified:
What does the l2 metric stands for ? 
How about min, max ?
Accuracy -> classification accuracy
Models -> 3D models
Describe each metric (Adversarial, Miss-classified, Correct)
","[5, 8, 6]","[' Marginally below acceptance threshold', ' Top 50% of accepted papers, clear accept', ' Marginally above acceptance threshold']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer acknowledges the novelty of the work and finds the study interesting. However, they point out several shortcomings, such as incompleteness, lack of references, and unclear presentation. The language used is generally neutral, with constructive criticism and suggestions for improvement.",40.0,60.0
On Characterizing the Capacity of Neural Networks Using Algebraic Topology,"['William H. Guss', 'Ruslan Salakhutdinov']",Reject,2018,"[3, 16]","[6, 21]","[17, 419]","[4, 207]","[13, 201]","[0, 11]","Paper Summary:

This paper looks at empirically measuring neural network architecture expressivity by examining performance on a variety of complex datasets, measuring dataset complexity with algebraic topology. The paper first introduces the notion of topological equivalence for datasets -- a desirable measure to use as it is invariant to superficial differences such as rotation, translation and curvature. The definition of homology from algebraic topology can then be used as a robust measure of the ""complexity"" of a dataset. This notion of difficulty focuses roughly on determining the number of holes of dimension n (for varying n) there are in the dataset, with more holes roughly leading to a more complex connectivity pattern to learn. They provide a demonstration of this on two synthetic toy datasets in Figure 1, training two (very small -- 12 and 26 neuron) single hidden layer networks on these two datasets, where the smaller of the two networks is unable to learn the data distribution of the second dataset. These synthetic datasets have a well defined data distribution, and for an empirical sample of N points, a (standard) method of determining connectivity by growing epsilon balls around each datapoint in section 2.3.

The authors give a theoretical result on the importance of homology: if a binary classifier has support homology not equal to the homology of the underlying dataset, then there is at least one point that is misclassified by the classifier. Experiments are then performed with single hidden layer networks on synthetic datasets, and a phase transition is observed: if h_phase is the number of hidden units where the phase transition happens, and h' < h < h_phase, h' has higher error and takes longer to converge than h. Finally, the authors touch on computing homology of real datasets, albeit with a low dimensional projection (e.g. down to 3 dimensions for CIFAR-10).

Main Comments

The motivation to consider algebraic topology and dataset difficulty is interesting, but I think this method is ultimately ill suited and unable to be adapted to more complex and interesting settings. In particular, the majority of experiments and justification of this method comes from use on a low dimensional manifold with either known data distribution, or with a densely sampled manifold. (The authors look at using CIFAR-10, but project this down to 3 dimensions -- as current methods for persistent homology cannot scale -- which somewhat invalidates the goal of testing this out on real data.) This is an important and serious drawback because it seems unlikely that the method described in Figure 3 of determining the connectivity patterns of a dataset are likely to yield insightful results in a high dimensional space with very few datapoints (in comparison to 2^{dimension}), where distance between datapoints is unlikely to have any nice class related correspondence.

Furthermore, while part of the motivation of this paper is to use dataset complexity measured with topology to help select architectures, experiments demonstrating that this might be useful are very rudimentary. All experiments only look at single hidden layers, and the toy task in Figure 1 and in section 3.2.1 and Figure 5 use extremely small networks (hidden size 12-26). It's hard to be convinced that these results necessarily generalize even to other larger hidden layer models. On real datasets, exploring architectures does not seem to be done at all (Section 4).


Minor Comments
Some kind of typo in Thm 1? (for all f repeated twice)
Small typos (missing spaces) in related work and conclusion
How is h_phase determined? Empirically? (Or is there a construction?)

Review Summary:

This paper is not ready to be accepted.","[3, 4, 4]","[' Clear rejection', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[5, 5, 5]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review is mostly negative. While it finds the initial motivation interesting, it ultimately deems the method unsuitable and lacking support for complex scenarios. The reviewer criticizes the reliance on low-dimensional manifolds and the lack of convincing evidence for generalization to larger models or real-world datasets. The tone, however, remains professional and polite throughout, focusing on constructive criticism and suggesting improvements.",-40.0,70.0
STRUCTURED ALIGNMENT NETWORKS,"['Yang Liu', 'Matt Gardner']",Reject,2018,"[4, 10, 20]","[9, 15, 25]","[67, 131, 303]","[29, 68, 158]","[35, 58, 93]","[3, 5, 52]","This paper proposes a model of ""structured alignments"" between sentences as a means of comparing two sentences by matching their latent structures. Overall, this paper seems a straightforward application of the model first proposed by Kim et al. 2017 with latent tree attention.

In section 3.1, the formula for p(c|x) looks wrong: c_{ijk} are indicator variables. but where are the scores for each span? I think it should be c_{ijk} * \delta_{ijk} under the summations instead.

In the same section, the expression for \alpha_{ij} seems to assume that \delta_{ijk} = \dlta_{ij} regardless of k. I.e. there are no production rule scores (transitions). This seems rather limiting, can you comment on that?

In the answer selection and NLI experiments, the proposed model does not beat the SOTA, and is only marginally better than unstructured decomposable attention. This is rather disappointing.

The plots in Fig 2 with the marginals on CKY charts are not very enlightening. How do this marginals help solving the NLI task?

Minor comments:
- Sec. 3: ""Language is inherently tree structured"" -- this is debatable...
- page 8: (laf, 2008): bad formatted reference","[5, 5, 6]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Marginally above acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with a neutral statement, indicating the paper's main idea. However, it then raises several concerns and questions, suggesting potential flaws in the methodology and expressing disappointment with the results. While the reviewer points out specific issues and asks for clarification, the tone remains largely neutral and professional, without resorting to personal attacks or overly negative language.",-20.0,60.0
Stabilizing GAN Training with Multiple Random Projections,"['Behnam Neyshabur', 'Srinadh Bhojanapalli', 'Ayan Chakrabarti']",Reject,2018,"[6, 6, 12]","[10, 11, 17]","[88, 61, 76]","[39, 24, 35]","[46, 35, 33]","[3, 2, 8]","
- Paper summary

The paper proposes a GAN training method for improving the training stability. The key idea is to let a GAN generator competes with multiple GAN discriminators where each discriminator takes a random low-dimensional projection of an input image for differentiate whether the input image is a real or generated one. Visual generation results from the proposed method with comparison to those generated by the DCGAN were used as the main experimental validation for the merit of the proposed method. Due to poor experimental validation and inconclusive results, the reviewer does not recommend the acceptance of the paper.

- Inconclusive results

The paper fails to compare the proposed method with the GMAN framework [a], which was the first work proposing utilizing multiple discriminators for more stable GAN training. Without comparing to the GMAN work, we do not know whether the benefit is from using multiple discriminators proposed in the GMAN work or from using the random low dimensional projections proposed in this paper. If it is former, then the proposed method has no merits at all.

In addition, the generator loss curve shown in Figure 2 is not making much sense. The generator loss curve will be meaningful if each discriminator update is optimal. However, this is not the case in the proposed method. There is little to conclude from Figure 2.

[a] Durugkar et al. ""Generative multi-adversarial networks."" ICLR 2017

- Poor experimental validation

The paper fails to utilize more established performance metrics such as the inception loss or human evaluation score to evaluate its benefit. It does not compare to other approaches for stabilizing GAN training such as WGAN or LSGAN. The main results shown in the paper are generating 64x64 human face images, which is not impressive.","[3, 8, 5]","[' Clear rejection', ' Top 50% of accepted papers, clear accept', ' Marginally below acceptance threshold']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is highly critical of the paper, pointing out significant flaws in the experimental validation and results. The reviewer explicitly recommends against acceptance. The language used, while direct, maintains a professional and objective tone, avoiding personal attacks or overly negative phrasing.",-75.0,50.0
Long-term Forecasting using Tensor-Train RNNs,"['Rose Yu', 'Stephan Zheng', 'Anima Anandkumar', 'Yisong Yue']",Reject,2018,"[8, 3, 13, 12]","[13, 8, 18, 17]","[88, 39, 419, 230]","[35, 10, 154, 101]","[50, 27, 223, 110]","[3, 2, 42, 19]","The paper proposes Tensor-Train RNN and Tensor-Train LSTM (TT-RNN/TLSTM), a RNN/LSTM architecture whose hidden unit at time t h_t is computed from the tensor-vector product between a tensor of weights and a concatenation of hidden units from the previous L time steps. The motivation is to incorporate previous hidden states and high-order correlations among them to better predict long-term temporal dependencies for seq2seq problems. To address the issue of the number of parameters growing exponentially in the rank of the tensor, the model uses a low rank decomposition called the ‘tensor-train decomposition’ to make the number of parameters linear in the rank. Some theoretical analysis on the number of hidden units required for a given estimation error, and experimental results have been provided for synthetic and real sequential data.

First of all, the presentation of the method in section 2.1 is confusing and there seem to be various ambiguities in the notation that harms understanding of the method. The tensor-vector product in equation (6) appears problematic. The notation that I think is standard is as follows: given a tensor W \in R^{n_1 \times … \times n_P} and vectors v_p \in R^{n_p}, the tensor-vector product W \times_{p=1}^P v_p = vec(W) \otimes_{p=1}^P v_p = \sum{i_1,...,i_P} \prod_{p=1}^P v_{p,i_p}. So I’m guessing you want to get rid of the \otimes signs (the kronecker products) in (6) or you want to remove the summation and write W \times_{p=1}^P s_{t-1}. Also \alpha that appears in (6) is never defined. Is it another index? This is confusing because you say W is P-dimensional but have P+1 indices for it including alpha (W_{\alpha i_1 … i_p}). Moreover the dimensionality of W^{hx} x_t in (6) is R^H judging from the notation in page 2, but isn’t the tensor-vector product a scalar? Also am I correct in thinking that s_{t-1} should be [1, h_{t-1}^T, …, h_{t-L}^T], i.e. a vector of length LH+1 rather than a matrix? The notation from page 2 implies that you are using column vectors, so the definition of s_{t-1} makes it appear as an (L+1) by H matrix, which could make the reader interpret s_{t-1;i_1} in (6) as vectors instead of scalars (this is reinforced by the kronecker product between these s_{t-1;i_p}). I had to work this out from the number of parameters (HL+1)^P in section 2.2. The diagram of s_{t-1} in Figure 3 is also confusing, because it isn’t obvious that the unlabelled grey bars are copies of s_{t-1}. Also I notice that the name ‘Tensor Train RNN/LSTM’ has been used in Yang et al, 2017. You probably want to avoid using the same name since the models are different. It would be nice if you could explain in a bit more detail about how they are different in the related work section.

Assuming I have understood the method correctly, the idea of using tensor products to incorporate higher order interactions between the hidden states at different times appears sensible. From the theoretical analysis, you claim that 1) smoother f is easier to approximate, and 2) polynomial interactions are more efficient than linear ones. The first point seems fairly self-explanatory and doesn’t seem to require a proof. The second point isn’t so convincing because you have two additive terms on the right hand side of the inequality in Theorem 3.1 (btw I’m guessing you want the inequality to be the other way round): the first term is independent of p, and the second decreases exponentially with p. Your second point would only hold if this first term is reasonably small, but this doesn’t seem obvious to me.

Regarding the experiments, I’m sceptical as to whether a grid search over hyperparameters for TLSTM vs grid search over the same hyperparameters for (M)LSTM provides a fair comparison. You probably want to compare the models given the same number of parameters, since given the same state size, TLSTM will have many more parameters than (M)LSTM. A plot of x-axis: # parameters, y-axis: average RMSE at convergence would be informative. Moreover for figure 8, you probably want to control the time taken for training instead of just comparing validation loss at the same number of steps. I imagine the best performing TLSTM model will have many more parameters and hence take much longer to train than the best performing LSTM model. 
Moreover, it seems as though the increased prediction accuracy from LSTM is marginal considering you have 3 more hyperparameters to tune (L,S,P - what was the value of P used for the experiments?) and that tuning them is important to prevent overfitting.

I’m also curious as to how TLSTM compares to hierarchical RNN approaches for modelling long-term dependencies. It will be interesting to compare against models like Stacked LSTM (Graves, 2013), Grid LSTM (Kalchbrenner, 2015) and HM LSTM (Chung, 2017). These models have mostly been evaluated on text, but I don’t see any reason they can’t be extended to sequential forecasting on time series data. Also regularisation techniques such as batch-norm for LSTMs (Cooijmans et al, 2016) and layer-norm (Ba et al, 2016) seem to help a lot for increasing prediction accuracy. Did you investigate these techniques to control overfitting?

Other minor comments on presentation:
For figure 6, the legends are inconsistent with the caption. Also you might want to overlay predictions on top of the ground truth for better comparison and also to save space.

Overall, I think there are vast scopes for improvement in presentation and comparisons with other methods, and hence find the paper not yet ready for publication.
","[4, 5, 6]","[' Ok but not good enough - rejection', ' Marginally below acceptance threshold', ' Marginally above acceptance threshold']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is critical of the paper, pointing out ambiguities, questioning the theoretical analysis, and suggesting stronger comparisons with other methods. While the reviewer acknowledges the potential of the proposed method, the overall tone suggests significant revisions are needed. The language, while direct, maintains a professional and constructive tone. There are no personal attacks or disrespectful remarks.",-30.0,60.0
BLOCK-DIAGONAL HESSIAN-FREE OPTIMIZATION FOR TRAINING NEURAL NETWORKS,"['Huishuai Zhang', 'Caiming Xiong', 'James Bradbury', 'Richard Socher']",Reject,2018,"[5, 10, 4, 12]","[10, 15, 6, 17]","[68, 383, 18, 229]","[26, 165, 9, 111]","[35, 208, 9, 111]","[7, 10, 0, 7]","In this paper, authors discuss the use of block-diagonal hessian when computing the updates. The block-diagonal hessian makes it easier to solve the ""newton"" directions, as the CG can be run only on smaller blocks (and hence less CG iterations are needed).

The paper is nicely written and all was clear to me. In general, I agree that having larger batch-size is the way to go, for very large datasets and a pure SGD type of methods are having problems to efficiently utilize large clusters.

The only negative thing I find in the paper is the lack of more numerical results. Indeed, the paper is clearly not a theoretical paper, is proposing a new algorithm, hence there should be evidence that it works. For example, I would like to see how the choice of hyper-parameters influences the speed of the algorithm. Was ""CG"" cost included in the ""x""-axis? i.e. if we put ""passes"" over the data as x-axis, then 1 update \approx 30 CG + some more == 32 batch evaluation.
So please try to make the ""x""-axis more fair.

 ","[6, 6, 4]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Ok but not good enough - rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer provides a generally positive sentiment, stating the paper is ""nicely written"" and agreeing with the core ideas. However, they express a desire for more numerical results to validate the claims, which slightly lowers the positivity. The language remains polite throughout, with constructive criticism and suggestions for improvement.",60.0,80.0
Semi-Supervised Learning via New Deep Network Inversion,"['Balestriero R.', 'Roger V.', 'Glotin H.', 'Baraniuk R.']",Reject,2018,"[4, 4, 21, 30]","[9, 8, 26, 35]","[87, 6, 148, 495]","[23, 1, 115, 230]","[59, 2, 17, 149]","[5, 3, 16, 116]",This paper proposed a new optimization framework for semi-supervised learning based on derived inversion scheme for deep neural networks. The numerical experiments show a significant improvement in accuracy of the approach.,"[7, 4, 5]","[' Good paper, accept', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[2, 5, 4]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is short and does not explicitly state positive or negative sentiment. It simply summarizes the paper and states the results are positive (""a significant improvement"").  Therefore, the sentiment is positive, but not overtly so. The language is neutral and professional throughout.",50.0,50.0
Phase Conductor on Multi-layered Attentions for Machine Comprehension,"['Rui Liu', 'Wei Wei', 'Weiguang Mao', 'Maria Chikina']",Reject,2018,"[19, 26, 6, 2]","[24, 31, 10, 6]","[365, 604, 6, 6]","[190, 311, 0, 0]","[43, 48, 3, 1]","[132, 245, 3, 5]","Summary: The paper introduces ""Phase Conductor"", which consists of two phases, context-question attention phase and context-context (self) attention phase. Each phase has multiple layers of attention, for which the paper uses a novel way to fuse the layers, and context-question attention uses different question embedding for getting the attention weight and getting the attention vector. The paper shows that the model achieves state of the art on SQuAD among published papers, and also quantitatively and visually demonstrates that having multiple layers of attention is helpful for context-context attention, while it is not so helpful for context-question attention.


Note: While I will mostly try to ignore recently archived, non-published papers when evaluating this paper, I would like to mention that the paper's ensemble model currently stands 11th on SQuAD leaderboard.


Pros:
- The model achieves SOTA on SQuAD among published papers.
- The sequential fusing (GRU-like) of the multiple layers of attention is interesting and novel. Visual analysis of the attention map is convincing.
- The paper is overall well-written and clear.

Cons:
- Using different embedding for computing attention weights and getting attended vector is not entirely novel but rather an expected practice for many memory-based models, and should cite relevant papers. For instance, Memory Networks [1] uses different embedding for key (computing attention weight) and value (computing attended vector).
- While ablations for number of attention layers (1 or 2) were visually convincing, numerically there is a very small difference even for selfAtt. For instance, in Table 4, having two layers of selfAtt (with two layers of question-passage) only increases max F1 by 0.34, where the standard deviation is 0.31 for the one layer. While this may be statistically significant, it is a very small gain nonetheless.
- Given the above two cons, the main contribution of the paper is 1.1% improvement over previous state of the art. I think this is a valuable engineering contribution, but I feel that it is not well-suited / sufficient for ICLR audience. 


Questions:
- page 7 first para: why have you not tried GloVe 300D, if you think it is a critical factor?


Errors:
- page 2 last para: ""gives an concrete"" -> ""gives a concrete""
- page 2 last para: ""matching"" -> ""matched""
Figure 1: I think ""passage embedding h"" and ""question embedding v"" boxes should be switched.
- page 7 3.3 first para: ""evidence fully"" -> ""evidence to be fully""


[1] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory Networks. ICLR 2015.","[5, 8, 5]","[' Marginally below acceptance threshold', ' Top 50% of accepted papers, clear accept', ' Marginally below acceptance threshold']","[5, 3, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer acknowledges the paper's achievement of SOTA on SQuAD and finds the sequential fusing of attention layers interesting. However, they also point out that some aspects of the work are not entirely novel and that the improvement over previous SOTA is relatively small. The reviewer's tone is critical but professional and suggests the work might be more suited for a different venue. Overall, the sentiment leans slightly towards the positive side due to the acknowledgment of the paper's merits.",20.0,70.0
Data-driven Feature Sampling for Deep Hyperspectral Classification and Segmentation,"['William M. Severa', 'Jerilyn A. Timlin', 'Suraj Kholwadwala', 'Conrad D. James', 'James B. Aimone']",Reject,2018,"[9, 15, 2, 19, 12]","[14, 14, 1, 21, 17]","[36, 3, 1, 33, 65]","[20, 2, 0, 16, 35]","[12, 1, 1, 9, 19]","[4, 0, 0, 8, 11]","Authors propose a greedy scheme to select a subset of (highly correlated) spectral features in a classification task. The selection criterion used is the average magnitude with which this feature contributes to the activation of a next-layer perceptron. Once validation accuracy drops too much, the pruned network is retrained, etc. 

Pro: 
- Method works well on a single data set and solves the problem
- Paper is clearly written 
- Good use of standard tricks 

Con: 
- Little novelty

This paper could be a good fit for an applied conference such as the International Symposium on Biomedical Imaging. 
","[3, 6, 4]","[' Clear rejection', ' Marginally above acceptance threshold', ' Ok but not good enough - rejection']","[5, 5, 5]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review is lukewarm-positive. It acknowledges the merits of the work (clear writing, good methodology, positive results) but expresses reservations about its novelty. The recommendation to submit to a good applied conference instead of a top-tier venue further suggests that the reviewer sees the value of the work but doesn't consider it a groundbreaking contribution. The language is polite and professional throughout.",50.0,75.0
Meta-Learning Transferable Active Learning Policies by Deep Reinforcement Learning,"['Kunkun Pang', 'Mingzhi Dong', 'Timothy Hospedales']",Reject,2018,"[2, 7, 13, 12]","[6, 12, 18, 17]","[5, 36, 96, 305]","[3, 14, 46, 139]","[2, 12, 23, 131]","[0, 10, 27, 35]","The approach solves an important problem as getting labelled data is hard. The focus is on the key aspect, which is generalisation across heteregeneous data. The novel idea is the dataset embedding so that their RL policy can be trained to work across diverse datasets.

Pros: 
1. The approach performs well against all the baselines, and also achieves good cross-task generalisation in the tasks they evaluated on. 
2. In particular, they alsoevaluated on test datasets with fairly different statistics from the training datasets, which isnt very common in most meta-learning papers today, so it’s encouraging that the method works in that regime.

Cons: 
1. The embedding strategy, especially the representative and discriminative histograms, is complicated. It is unclear if the strategy is general enough to work on harder problems / larger datasets, or with higher dimensional data like images. More evidence in the paper for why it would work on harder problems would be great. 
2. The policy network would have to output a probability for each datapoint in the dataset U, which could be fairly large, thus the method is computationally much more expensive than random sampling. A section devoted to showing what practical problems could be potentially solved by this method would be useful.
3. It is unclear to me if the results in table 3 and 4 are achieved by retraining from scratch with an RBF SVM, or by freezing the policy network trained on a linear SVM and directly evaluating it with a RBF SVM base learner.

Significance/Conclusion: The idea of meta-learning or learning to learn is fairly common now. While they do show good performance, it’s unclear if the specific embedding strategy suggested in this paper will generalise to harder tasks. 

Comments: There’s lots of typos, please proof read to improve the paper.

Revision: I thank the authors for the updates and addressing some of my concerns. I agree the computational budget makes sense for cross data transfer, however the embedding strategy and lack of larger experiments makes it unclear if it'll generalise to harder tasks. I update my review to 6. ","[6, 7, 6]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Marginally above acceptance threshold']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer acknowledges the importance of the problem and the novelty of the approach, highlighting the positive aspects of the paper. However, they also raise valid concerns about the complexity and generalizability of the proposed method. The reviewer acknowledges the authors' efforts in addressing some concerns but maintains reservations about the embedding strategy and lack of larger experiments. The overall tone is constructive and helpful, with specific suggestions for improvement. The final score of 6 (out of 10 implied) suggests a leaning towards the positive side, but with room for improvement.",50.0,70.0
Learning to Compute Word Embeddings On the Fly,"['Dzmitry Bahdanau', 'Tom Bosc', 'Stanisław Jastrzębski', 'Edward Grefenstette', 'Pascal Vincent', 'Yoshua Bengio']",Reject,2018,"[5, 3, 6, 8, 29, 31]","[10, 7, 10, 13, 34, 36]","[55, 9, 53, 104, 128, 975]","[22, 4, 16, 44, 63, 405]","[33, 4, 29, 54, 52, 454]","[0, 1, 8, 6, 13, 116]","This paper describes a method for computing representations for out-of-vocabulary words, e.g. based on their spelling or dictionary definitions. The main difference from previous approaches is that the model is that the embeddings are trained end-to-end for a specific task, rather than trying to produce generically useful embeddings. The method leads to better performance than using no external resources, but not as high performance as using Glove embeddings. The paper is clearly written, and has useful ablation experiments. However, I have a couple of questions/concerns:
- Most of the gains seem to come from using the spelling of the word. As the authors note, this kind of character level modelling has been used in many previous works. 
- I would be slightly surprised if no previous work has used external resources for training word representations using an end-task loss, but I don’t know the area well enough to make specific suggestions 
- I’m a little skeptical about how often this method would really be useful in practice. It seems to assume that you don’t have much unlabelled text (or you’d use Glove), but you probably need a large labelled dataset to learn how to read dictionary definitions well. All the experiments use large tasks - it would be helpful to have an experiment showing an improvement over character-level modelling on a smaller task.
- The results on SQUAD seem pretty weak - 52-64%, compared to the SOTA of 81. It seems like the proposed method is quite generic, so why not apply it to a stronger baseline?
","[5, 7, 5]","[' Marginally below acceptance threshold', ' Good paper, accept', ' Marginally below acceptance threshold']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer acknowledges the paper's clarity, useful experiments, and the novelty of the end-to-end training approach. However, they raise several concerns, questioning the significance of the gains (mainly from spelling), the lack of comparison with potential similar works, and the practicality of the method. The weak performance on SQUAD and the suggestion to test on smaller tasks further suggest a slightly negative sentiment. Overall, the tone is polite and professional, presenting concerns and suggestions constructively.",-20.0,80.0
Efficient Exploration through Bayesian   Deep Q-Networks,"['Kamyar Azizzadenesheli', 'Emma Brunskill', 'Animashree Anandkumar']",Reject,2018,"[3, 18, 13]","[8, 23, 18]","[90, 194, 419]","[26, 120, 154]","[56, 67, 223]","[8, 7, 42]","(Last minute reviewer brought in as a replacement).

This paper proposed ""Bayesian Deep Q-Network"" as an approach for exploration via Thompson sampling in deep RL.
This algorithm maintains a Bayesian posterior over the last layer of the neural network and uses that as an approximate measure of uncertainty.
The agent then samples from this posterior for an approximate Thompson sampling.
Experimental results show that this outperforms an epsilon-greedy baseline.

There are several things to like about this paper:
- The problem of efficient exploration with deep RL is important and under-served by practical algorithms. This seems like a good algorithm in many ways.
- The paper is mostly clear and well written.
- The experimental results are impressive in their outperformance.

However, there are also some issues, many of which have already been raised:
- The poor performance of the DDQN baseline is concerning and does not seem to match the behavior of prior work (see Pong for example).
- There are some loose and misleading descriptions of the algorithm computing ""the posterior"" when actually this is very much an approximation method... that's OK to have approximations but it shouldn't be hidden away.
- The connection to RLSVI is definitely understated, since with a linear architecture this is precisely RLSVI. The sentiment that extending TS to larger spaces hasn't been fully explored is definitely valid... but this line of work should certainly be mentioned in the 4th paragraph. RLSVI is provably-efficient with a state-of-the-art regret bound for tabular learning - you would probably strengthen the case for this algorithm as an extension of RLSVI by building on this connection... otherwise it's a bit adhoc to justify this approximation method.
- This paper spends a lot of time re-deriving Bayesian linear regression in a really standard way... and without much discussion of how/why this method is an approximation (it is) especially when used with deep nets.

Overall, I like this paper and the approach of extending TS-style algorithms to Deep RL by just taking the final layer of the neural network.
However, it also feels like there are some issues with the baselines + being a bit more clear about the approximations / position relative to other algorithms for approximate TS would be a better approach.
For example, in linear networks this is the same as RLSVI, bootstrapped DQN is one way to extend this idea to deep nets, but this is another one and it is much better because XYZ. (this discussion could perhaps replace the rather mundane discussion of BLR, for example).

In it's current state I'd say marginally above, but wouldn't be surprised if these changes turned it into an even better paper quite quickly.


===============================================================

Revising my review following the rebuttal period and also the (ongoing) revisions to the paper.

I've been disappointed by the authors have incorporated the feedback/reviews - I expected something a little more clear / honest. Given the ongoing review decisions/issues I'm putting my review slightly below accept.

## Relation to literature on ""randomized value functions""
It's really wrong to present BDQN as is if it's the first attempt at large-scale approximations to Thompson sampling (and then slip in a citation to RLSVI as a BDQN-like algorithm). This algorithm is a form of RLSVI (2014) where you only consider uncertainty over the last (linear) layer - I think you should present it like this. Similarly *some* of the results for Bootstrapped DQN (2016) on Atari are presented without bootstrapping (pure ensemble) but this is very far from an essential part of the algorithm! If you say something like ""they did not estimate a true posterior"" then you should quantify this and (presumably) justify the implication that taking a gaussian approximation to the final layer is a *true* posterior. In a similar vein, you should be clear about the connections to Lipton et al 2016 as another method for approximate Bayesian posteriors in DQN.

## Quality/science of experiments
The experimental results have been updated, and the performance of the baseline now seems much more reasonable. However, the procedure for ""selecting arbitrary number of frames"" to report performance seems really unnecessary... it would be clear that BDQN is outperforming DDQN... you should run them all for the same number of frames and then either compare (final score, cumulative score, #frames to human) or something else more fair/scientific. This type of stuff smells like overfitting!","[5, 5, 6]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Marginally above acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with positive remarks, highlighting the importance and clarity of the paper. However, it then delves into serious concerns about misleading descriptions, understated connections to prior work (RLSVI), and unnecessary derivations. The reviewer acknowledges improvements in the rebuttal but expresses disappointment with the authors' response to feedback. The final paragraph criticizes the experimental methodology, hinting at potential overfitting. The language, while direct, maintains a professional and constructive tone.",20.0,60.0
Revisiting The Master-Slave Architecture In Multi-Agent Deep Reinforcement Learning,"['Xiangyu Kong', 'Fangchen Liu', 'Bo Xin', 'Yizhou Wang']",Reject,2018,"[13, 7, 2, 19]","[18, 10, 7, 24]","[77, 30, 23, 233]","[21, 17, 9, 129]","[6, 8, 12, 68]","[50, 5, 2, 36]","The paper proposes a neural network architecture for centralized and decentralized settings in multi-agent reinforcement learning (MARL) which is trainable with policy gradients.
Authors experiment with the proposed architecture on a set of synthetic toy tasks and a few Starcraft combat levels, where they find their approach to perform better than baselines.

Overall, I had a very confusing feeling when reading the paper. First, authors do not formulate what exactly is the problem statement for MARL. Is it an MDP or poMDP? How do different agents perceive their time, is it synchronized or not? Do they (partially) share the incentive or may have completely arbitrary rewards?
What is exactly the communication protocol?

I find this question especially important for MARL, because the assumption on synchronous and noise-free communication, including gradients is too strong to be useful in many practical tasks.

Second, even though the proposed architecture proved to perform empirically better that the considered baselines, the extent to which it advances RL research is unclear to me.
Currently, it looks 

Based on that, I can’t recommend acceptance of the paper.

To make the paper stronger and justify importance of the proposed architecture, I suggest authors to consider relaxing assumptions on the communication protocol to allow delayed and/or noisy communication (including gradients).
It would be also interesting to see if the network somehow learns an implicit global state representation used for planning and how is the developed plan changed when new information from one of the slave agents arrives.","[5, 5, 4]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Ok but not good enough - rejection']","[4, 5, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer highlights several significant concerns about the paper, such as a lack of clear problem formulation and strong assumptions about communication. While acknowledging the empirical performance, the reviewer questions the overall contribution and recommends against acceptance. The language used is critical but professional and avoids personal attacks.",-50.0,50.0
Bayesian Hypernetworks,"['David Krueger', 'Chin-Wei Huang', 'Riashat Islam', 'Ryan Turner', 'Alexandre Lacoste', 'Aaron Courville']",Reject,2018,"[9, 13, -2, 11, 11, 6, 7, 3, 30]","[14, 18, 2, 16, 16, 10, 11, 8, 35]","[50, 206, 3, 180, 158, 41, 6, 5, 688]","[33, 86, 2, 98, 89, 19, 4, 2, 398]","[13, 94, 1, 51, 48, 10, 2, 2, 138]","[4, 26, 0, 31, 21, 12, 0, 1, 152]","This paper presents Bayesian Hypernetworks; variational Bayesian neural networks where the variational posterior over the weights is governed by a hyper network that implements a normalizing flow (NF) such as RealNVP and IAF. As directly outputting the weight matrix with a hyper network is computationally expensive the authors instead propose to utilize weight normalisation on the weights and then use the hyper network to output scalar scaling variables for each hidden unit, similarly to what was done at [1]. The main difference with this prior work is that [1] consider these NF scaling variables as auxiliary random variables to a mean field Gaussian distribution over the weights whereas this paper attempts to posit a distribution directly on the weights via the NF. This avoids the nested variational approximation and auxiliary models of [1], which can potentially yield a tighter bound. The proposed method is evaluated on extensive experiments.

This paper seems like a plausible idea with extensive experiments but the similarity with [1] make it an incremental contribution and, furthermore, it seems that it has a technical issue with what is explained at Section 3.3. More specifically, if you generate the parameters \theta according to Eq. 7 and posit a prior over \theta then you will have a problematic variational bound as there will be a KL divergence, KL(q(\theta) || p(\theta)), with distributions of different support (since q(\theta) is defined only along the directions spanned by u), which is infinite. For the KL to be valid you will need to posit a prior distribution over `g`, p(g), and then consider KL(q(g) || p(g)), with q(g) being given by the NF. From the experiment paragraph at page 5 though I deduct that you instead employ “an isotropic standard normal prior over the weights”, i.e. \theta, thus I believe that you indeed have a problematic bound. How do you actually compute logq(\theta) when you employ the parametrisation discussed at 3.3? Did you use that parametrisation in every experiment?

Other than that, I believe that it would be interesting to experiment with a `full` hyper network, i.e. generating directly the entire parameter vector \theta, e.g. at the toy regression experiment where the dimensionality is small. This would then better illustrate the tradeoffs you make when you reduce the flexibility of the hyper-network to just outputting the row scaling variables and the effect this has at the posterior approximation.
 
Typos:
(1) Page 3, 3.1.1 log(\theta) -> logp(\theta).
(2) Eq. 6, it needs to be |det \frac{\partial h(\epsilon)}{\partial \epsilon}|^{-1} or |det \frac{\partial h^{-1}(\theta)}{\partial \theta}| for a valid change of variables formula.

[1] Louizos & Welling, Multiplicative Normalizing Flows for Variational Bayesian Neural Networks.","[6, 6, 6]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer acknowledges the paper presents a plausible idea with extensive experiments but finds it to be an incremental contribution. The reviewer also points out a technical issue with the variational bound, which significantly impacts the paper's contribution. The language used is formal and professional, without resorting to harsh or impolite language.",20.0,80.0
Adversary A3C for Robust Reinforcement Learning,"['Zhaoyuan Gu', 'Zhenzhong Jia', 'Howie Choset']",Reject,2018,"[0, 8, 26]","[4, 13, 31]","[6, 48, 366]","[1, 32, 227]","[5, 8, 62]","[0, 8, 77]","The authors propose an extension of adversarial reinforcement learning to A3C. The proposed technique is of modest contribution and the experimental results do not provide sufficient validation of the approach.  

The authors propose extending A3C to produce more robust policies by training a zero-sum game with two agents: a protagonist and an antagonist. The protagonist is attempting to achieve the given task while the antagonist's goal is for the task to fail. 

The contribution of this work, AR-A3C, is extending adversarial reinforcement learning, namely robust RL (RRL) and robust adversarial RL (RARL), to A3C. In the context of this prior work the novelty is extending the family of adversarial RL methods. However, the proposed method is still within the same family methods as demonstrated by RARL.

The authors state that AR-A3C requires half as many rollouts as compared to RARL. However, no empirical comparison between the two methods is performed. The paper only performs analysis against the A3C and no other adversarial baseline and on only one environment: cartpole.  While they show transfer to the real world cartpole with this technique, there is not sufficient analysis to satisfactorily demonstrate the benefits of the proposed technique. 

The paper reads well. There are a few notational issues in the paper that should be addressed. The authors mislabel the value function V as the  action value, or Q function. The action value function is action dependent where the value function is not.  As a much more minor issue, the authors introduce y as the discount factor, which deviates from the standard notation of \gamma without any obvious reason to do so.

Double blind was likely compromised with the youtube video, which was linked to a real name account instead of an anonymous account.

Overall, the proposed technique is of modest contribution and the experimental results do not provide sufficient validation of the approach.    ","[4, 4, 4]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review acknowledges the work as an ""extension"" but clearly states it is of ""modest contribution"" and lacks sufficient validation. The reviewer points out several limitations and questions the novelty. All this points to a slightly negative sentiment. The language used is formal and critical but maintains a professional and respectful tone, suggesting a neutral politeness level.",-25.0,0.0
ANALYSIS ON GRADIENT PROPAGATION IN BATCH NORMALIZED RESIDUAL NETWORKS,"['Abhishek Panigrahi', 'Yueru Chen', 'C.-C. Jay Kuo']",Reject,2018,"[2, 3, 32]","[7, 8, 37]","[17, 40, 1008]","[7, 16, 568]","[10, 18, 154]","[0, 6, 286]","This manuscript is fairly well-written, and discusses how the batch normalization step helps to stabilize the scale of the gradients.  Intriguingly, the analysis suggests that using a shallower but wider resnet should provide competitive performance, which is supported by empirical evidence.  This work should help elucidate the structure in the learning, and help to support efforts to improve both learning algorithms and the architecture.

Pros:
Clean, simple analysis
Empirical support suggests that theory captures reasonable effects behind learning

Cons:
The reasonableness of the assumptions used in the analysis needs a more careful analysis.  In particular, the assumption that all weights are independent is valid only at the first random iteration.  Therefore, the utility of this theory during initialization seems reasonable, but during learning the theory seems quite tenuous.  I would encourage the authors to discuss their assumptions, and talk about how the math would change as a result of relaxing the assumptions.
The empirical support does provide evidence that the theory is reasonable.  However, it is limited to a single dataset.  It would be nice to see that the effect happens more generally. Second, it is clear that shallow+wide networks may be better than deep+narrow networks, but it's not clear about how the width is evaluated and supported. I would encourage the authors to do more extensive experiments and evaluate the architecture further.

Revision:
Upon examining the comments of the other reviews, I have agreed with several of their points and it is necessary to increase the explanation of the mathematical points.  I encourage the authors to address these comments and revise their work.","[4, 1, 4]","[' Ok but not good enough - rejection', ' Trivial or wrong', ' Ok but not good enough - rejection']","[4, 5, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review starts with positive remarks, highlighting the well-written nature of the manuscript and its intriguing findings. While it points out limitations and calls for further investigation, it does so constructively and encourages improvement. The reviewer acknowledges the other reviewers' comments, indicating a collaborative approach.",60.0,80.0
Lifelong Generative Modeling,"['Jason Ramapuram', 'Magda Gregorova', 'Alexandros Kalousis']",Reject,2018,"[2, 4, 24]","[7, 8, 28]","[25, 18, 104]","[9, 5, 55]","[15, 11, 35]","[1, 2, 14]","The paper proposed a teacher-student framework and a modified objective function to adapt VAE training to streaming data setting. The qualitative experimental result shows that the learned model can generate reasonable-looking samples. I'm not sure about what conclusion to make from the numerical result, as the test negative ELBO actually increased after decreasing initially. Why did it increase?

The modified objective function is a little ad-hoc, and it's unclear how to relate the overall objective function to Bayesian posterior inference (what exactly is the posterior that the encoder tries to approximate?). There is a term in the objective function that is synthetic data specific. Does that imply that the objective function is different depending on if the data is synthetic or real? What is the motivation/justification of choosing KL(Q_student||Q_teacher) as regularisation instead of the other way around? Would that make a difference in the goodness of the learned model? If not, wouldn't KL(Q_teacher||Q_student) result reduction in the variance of gradients and therefore a better choice?

Details on the minimum number of real samples per interval for the model to be able to learn is also missing. Also, how many synthetic samples per real samples are needed? How is the update with respect to synthetic sample scheduled? Given infinite amount of streaming data with a fixed number of classes/underlying distributions and interval length, and sample the class of each interval (uniformly) randomly, will the model/algorithm converge? Is there a minimum number of real examples that the student learner needs to see before it can be turned into a teacher?

Other question: How is the number of latent category J of the latent discrete distribution chosen?

Quality: The numerical experiment doesn't really compare to any other streaming benchmark and is a little unsatisfying. Without a streaming benchmark or a realistic motivating example in which the proposed scheme makes a significant difference, it's difficult to judge the contribution of this work.
Clarity: The manuscript is reasonably well-written. (minor: Paragraph 2, section 5, 'in principle' instead of 'in principal')
Originality: Average. The student-teacher framework by itself isn't novel. The modifications to the objective function appears to be novel as far as I am aware, but it doesn't require much special insights.
Significance: Below average. I think it will be very helpful if the authors can include a realistic motivating example where lifelong unsupervised learning is critical, and demonstrate that the proposed scheme makes a difference in the example.


","[4, 9, 4]","[' Ok but not good enough - rejection', ' Top 15% of accepted papers, strong accept', ' Ok but not good enough - rejection']","[2, 5, 5]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The reviewer raises several questions and expresses uncertainty about the results, particularly the increase in negative ELBO. They find the modified objective function somewhat ad-hoc and unclear in its relation to Bayesian inference. While acknowledging the clarity of writing and some originality, they criticize the lack of comparison with benchmarks and a compelling real-world application. This suggests a lukewarm response leaning towards the critical side.",-20.0,50.0
Clustering with Deep Learning: Taxonomy and New Methods,"['Elie Aljalbout', 'Vladimir Golkov', 'Yawar Siddiqui', 'Daniel Cremers']",Reject,2018,"[1, 4, 1, 19]","[5, 9, 6, 24]","[18, 20, 12, 602]","[5, 6, 4, 327]","[11, 13, 8, 186]","[2, 1, 0, 89]","In this paper the authors give a nice review of clustering methods with deep learning and a systematic taxonomy for existing methods. Finally, the authors propose a new method by using one unexplored combination of taxonomy features.

The paper is well-written and easy to follow. The proposed combination is straightforward, but lack of novelty. From table 1, it seems that the only differences between the proposed method and DEPICK is whether the method uses balanced assignment and pretraining. I am not convinced that these changes will lead to a significant difference. The performance of the proposed method and DEPICK are also similar in table 1. 

In addition, the experiments section is not comprehensive enough as well. the author only tested on two datasets. More datasets should be tested for evaluation. In addition, It seems that nearly all the experiments results from comparison methods are borrowed from the original publications. The authors should finish the experiments on comparison methods and fill the entries in Table 1.

In summary, the proposed method is lack of novelty compare to existing methods. The survey part is nice, however extensive experiments should be conducted by running existing methods on different datasets and analyzing the pros and cons of the methods and their application scenarios. Therefore, I think the paper cannot be accepted at this stage.
","[3, 2, 3]","[' Clear rejection', ' Strong rejection', ' Clear rejection']","[5, 5, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with a positive sentiment, praising the paper's review of existing methods and clarity. However, it quickly transitions into a negative sentiment, criticizing the lack of novelty in the proposed method and the limited experimental validation. The reviewer finds the proposed method to be only a minor variation of an existing method and questions its significance. The lack of comprehensive experiments further weakens the paper. The final verdict is that the paper is not acceptable in its current stage, indicating an overall negative sentiment. 

While the reviewer expresses concerns and criticisms, the language used is professional and avoids harsh or disrespectful tones. The reviewer provides constructive feedback, suggesting improvements like more comprehensive experiments and further analysis. ",-30.0,60.0
Exploring the Space of Black-box Attacks on Deep Neural Networks,"['Arjun Nitin Bhagoji', 'Warren He', 'Bo Li', 'Dawn Song']",Reject,2018,"[2, 7, 10, 20]","[7, 11, 15, 25]","[37, 27, 300, 440]","[16, 16, 124, 254]","[20, 9, 148, 154]","[1, 2, 28, 32]","This paper generates adversarial examples using the fast gradient sign (FGS) and iterated fast gradient sign (IFGS) methods, but replacing the gradient computation with finite differences or another gradient approximation method. Since finite differences is expensive in high dimensions, the authors propose using directional derivatives based on random feature groupings or PCA. 

This paper would be much stronger if it surveyed a wider variety of gradient-free optimization methods. Notably, there's two important black-box optimization baselines that were not included: simultaneous perturbation stochastic approximation ( https://en.wikipedia.org/wiki/Simultaneous_perturbation_stochastic_approximation), which avoids computing the gradient explicitly, and evolutionary strategies ( https://blog.openai.com/evolution-strategies/ ), a similar method that uses several random directions to estimate a better descent direction.

The gradient approximation methods proposed in this paper may or may not be better than SPSA or ES. Without a direct comparison, it's hard to know.  Thus, the main contribution of this paper is in demonstrating that gradient approximation methods are sufficient for generating good adversarial attacks and applying those attacks to Clarifai models. That's interesting and useful to know, but is still a relatively small contribution, making this paper borderline. I lean towards rejection, since the paper proposes new methods without comparing to or even mentioning well-known alternatives.

REVISION: Thank you for your response! The additional material does strengthen the paper. There is now some discussion of how Chen et al. differs, and an explicit comparison to SPSA and PSO. I think there are some interesting results here, including attacks on Clarifai. However, the additional evaluations are not thorough. This is understandable (given the limited time frame), but unfortunate. SPSA is only evaluated on MNIST, and while the paper claims its distortion is greater, this is never shown explicitly (or was too difficult for me to find, even when searching through the revised paper). Chen et al. is only compared in terms of time, not on success rate, distortion, or number of queries. These timing results aren't necessarily comparable, since the experiments were done under different conditions. Overall, the new experiments and discussion are a step towards a thorough analysis of zero-order attacks, but they're not there yet. I've increased my rating from 4 to 5, but this is still below the bar for me.","[5, 6, 7]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Good paper, accept']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer acknowledges some merits of the paper, finding the results ""interesting and useful."" However, they express concerns about the lack of comparison with established methods, ultimately leaning towards rejection. While the reviewer acknowledges improvements in the revised version, they still find the analysis insufficiently thorough, leading to a slightly improved but still negative sentiment.",-30.0,70.0
Deep Generative Dual Memory Network for Continual Learning,"['Nitin Kamra', 'Umang Gupta', 'Yan Liu']",Reject,2018,"[4, 23, 17]","[9, 28, 22]","[16, 31, 219]","[7, 13, 132]","[7, 14, 57]","[2, 4, 30]","This paper introduces a neural network architecture for continual learning. The model is inspired by current knowledge about long term memory consolidation mechanisms in humans. As a consequence, it uses:
-	One temporary memory storage (inspired by hippocampus) and a long term memory
-	A notion of memory replay, implemented by generative models (VAE), in order to simultaneously train the network on different tasks and avoid catastrophic forgetting of previously learnt tasks.
Overall, although the result are not very surprising, the approach is well justified and extensively tested. It provides some insights on the challenges and benefits of replay based memory consolidation.

Comments:
	
1-	The results are somewhat unsurprising: as we are able to learn generative models of each tasks, we can use them to train on all tasks at the same time, a beat algorithms that do not use this replay approach. 
2-	It is unclear whether the approach provides a benefit for a particular application: as the task information has to be available, training separate task-specific architectures or using classical multitask learning approaches would not suffer from catastrophic forgetting and perform better (I assume). 
3-	So the main benefit of the approach seems to point towards the direction of what possibly happens in real brains. It is interesting to see how authors address practical issues of training based on replay and it show two differences with real brains: 1/ what we know about episodic memory consolidation (the system modeled in this paper) is closer to unsupervised learning, as a consequence information such as task ID and dictionary for balancing samples would not be available, 2/ the cortex (long term memory) already learns during wakefulness, while in the proposed algorithm this procedure is restricted to replay-based learning during sleep.
4-	Due to these differences, I my view, this work avoids addressing directly the most critical and difficult issues of catastrophic forgetting, which relates more to finding optimal plasticity rules for the network in an unsupervised setting
5-	The writing could have been more concise and the authors could make an effort to stay closer to the recommended number of pages.
","[7, 6, 5]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Marginally below acceptance threshold']","[2, 4, 4]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer acknowledges the merits of the paper, such as the well-justified approach and extensive testing. However, they also point out limitations, including unsurprising results and lack of clear practical benefits. The reviewer's main concern is the limited relevance to real-world catastrophic forgetting due to reliance on supervised information. While critical, the comments are presented in a neutral and professional manner.",20.0,50.0
Dynamic Evaluation of Neural Sequence Models,"['Ben Krause', 'Emmanuel Kahembwe', 'Iain Murray', 'Steve Renals']",Reject,2018,"[13, 2, 15, 32]","[16, 5, 19, 37]","[16, 8, 70, 301]","[5, 1, 36, 209]","[10, 6, 29, 44]","[1, 1, 5, 48]","The authors provide an improved implementation of the idea of dynamic evaluation, where the update of the parameters used in the last time step proposed in (Mikolov et al. 2010) is replaced with a back-propagation through the last few time steps, and uses  RMSprop rather than vanilla SGD. The method is applied to word level and character level language modeling where it yields some gains in perplexity. The algorithm also appears able to perform domain adaptation, in a setting where a character-level language model trained mostly on English manages to quickly adapt to a Spanish test set. 

While the general idea is not novel, the implementation choices matter, and the authors provide one which appears to work well with recently proposed models. The character level experiments on the multiplicative LSTM make the most convincing point, providing a significant improvement over already good results on medium size data sets. Figure 2 also makes a strong case for the method's suitability for applications where domain adaptation is important.

The paper's weakest part is the word level language modeling section. Given the small size of the data sets considered, the results provided are of limited use, especially since the development set is used to fit the RMSprop hyper-parameters. How sensitive are the final results to this choice? Comparing dynamic evaluation to neural cache models is a good idea, given how both depend en medium-term history: (Grave et al. 2017) provide results on the larger text8 and wiki103, it would be useful to see results for dynamic evaluation at least on the former.

An indication of the actual additional evaluation time for word-level, char-level and sparse char-level dynamic evaluation would also be welcome.

Pros:
- Good new implementation of an existing idea
- Significant perplexity gains on character level language modeling
- Good at domain adaptation

Cons:
- Memory requirements of the method
- Word-level language modeling experiments need to be run on larger data sets

(Edit: the authors did respond satisfactorily to the original concern about the size of the word-level data set)","[7, 7, 3]","[' Good paper, accept', ' Good paper, accept', ' Clear rejection']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer provides both positive and negative points about the paper. They acknowledge the good implementation and results in character-level modeling and domain adaptation. However, they also point out weaknesses in the word-level experiments and data set size. The tone is critical but professional and suggests concrete improvements. Overall, it leans towards the positive side, but with reservations.",60.0,70.0
ResBinNet: Residual Binary Neural Network,"['Mohammad Ghasemzadeh', 'Mohammad Samragh', 'Farinaz Koushanfar']",Reject,2018,"[2, 2, 19]","[2, 6, 24]","[6, 33, 295]","[4, 13, 162]","[2, 13, 72]","[0, 7, 61]","This paper proposes ResBinNet, with residual binarization, and temperature adjustment. It is a reconfigurable binarization method for neural networks. It improves the convergence rate during training. 

I appreciate a lot that the authors were able to validate their idea by building a prototype of an actual hardware accelerator.

I am wondering what are the values of \gamma’s in the residual binarization after learning? What is its advantage over having only one \gamma, and then the rest are just 1/2*\gamma, 1/4* \gamma, … , etc.? The latter is an important baseline for residual binarization because that corresponds to the widely used fixed point format for real numbers. If you can show some results that residual encoding is better than having {\gamma, 1/2*\gamma, 1/4* \gamma, …, } (which contains only one \gamma), it would validate the need of using this relatively complex binarization scheme. Otherwise, we can just use the l-bit fixed point multiplications, which is off-the-shelf and already highly optimized in many hardwares. 

For the temperature adjustment, modifying the tanh() scale has already had a long history, for example, http://yann.lecun.com/exdb/publis/pdf/lecun-89.pdf page 7, which is exactly the same form as in this paper. Adjusting the slope during training has also been explored in some straight-through estimator approaches, such as https://arxiv.org/pdf/1609.01704.pdf. In addition, having this residual binarization and adjustable tanh(), is already adding extra computations for training. Could you provide some data for comparing the computations before and after adding residual binarization and temperature adjustment? 

The authors claimed that ResBinNet converges faster during training, and in table 2 it shows that ResBinNet just needs 1/10 of the training epochs needed by BinaryNet. However, I don’t find it very fair. Given that the accuracy RBN gets is much lower than Binary Net, the readers might suspect that maybe the other two models already reach ResBinNet’s accuracy at an earlier training epochs (like epoch 50), and just take all the remaining epochs to reach a higher accuracy. On the other hand, this comparison is not fair for ResBinNet as well. The model size was much larger in BinaryNet than in ResBinNet. So it makes sense to train a BinaryNet or FINN, in the same size, and then compare the training curves. Lastly, in CIFAR-10 1-level case, it didn’t outperform FINN, which has the same size. Given these experiments, I can’t draw any convincing conclusion.

Apart from that, There is an error in Figure 7 (b), where the baseline has an accuracy of 80.1% but its corresponding bar is lower than RBN1, which has an accuracy of 76%. ","[4, 4, 4]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer acknowledges the authors' work and contribution ('I appreciate a lot that the authors were able to validate their idea by building a prototype of an actual hardware accelerator.') but raises several significant concerns and questions regarding the methodology and results. The reviewer also points out an error in the figures. The language is professional and not aggressive, but the overall tone suggests that the reviewer is not fully convinced by the paper.",20.0,60.0
FAST READING COMPREHENSION WITH CONVNETS,"['Felix Wu', 'Ni Lao', 'John Blitzer', 'Guandao Yang', 'Kilian Weinberger']",Reject,2018,"[25, 16, 16, 2, 15]","[30, 21, 17, 7, 20]","[54, 56, 35, 23, 199]","[22, 27, 28, 12, 106]","[26, 23, 6, 11, 82]","[6, 6, 1, 0, 11]","This paper proposes a convnet-based neural network architecture for reading comprehension and demonstrates reasonably good performance on SQuAD and TriviaQA with a great speed-up.

The proposed architecture combines a few recent DL techniques: residual networks, dilated convolutions and gated linear units.

I understand the motivation that ConvNet has a great advantage of easing parallelization and thus is worth exploring. However, I think the proposed architecture in this paper is less motivated. Why is GLU chosen? Why is dilation used? According to Table 4, dilation is really not worth that much and GLU seems to be significantly better than ReLU, but why?

The architecture search (Table 3 and Figure 4) seems to quite arbitrary. I  would like to see more careful architecture search and ablation studies. Also, why is Conv DrQA significantly worse than DrQA while Conv BiDAF can be comparable to BiDAF?

I would like to see more explanations of Figure 4. How important is # of layers and residual connections?

Minor:
- It’d be helpful to add the formulation of gated linear units and residual layers. 
- It is necessary to put Table 5 in the main paper instead of Appendix. These are still the main results of the paper.","[5, 4, 7]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Good paper, accept']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer acknowledges the paper's merits (good performance, speed) but raises several concerns about the architectural choices, suggesting they seem arbitrary and lack sufficient justification. The reviewer desires more in-depth analysis and explanations. While the reviewer uses direct language and points out shortcomings, the tone remains professional and constructive, suggesting improvements rather than outright criticism.",20.0,60.0
Learning to Generate Filters for Convolutional Neural Networks,"['Wei Shen', 'Rujie Liu']",Reject,2018,"[24, 18]","[29, 23]","[170, 69]","[81, 46]","[25, 16]","[64, 7]","This paper explores learning dynamic filters for CNNs. The filters are generated by using the features of an autoencoder on the input image, and linearly combining a set of base filters for each layer. This addresses an interesting problem which has been looked at a lot before, but with some small new parts. There is a lot of prior work in this area that should be cited in the area of dynamic filters and steerable filters. There are also parallels to ladder networks that should be highlighted. 

The results indicate improvement over baselines, however baselines are not strong baselines. 
A key question is what happens when this method is combined with VGG11 which the authors train as a baseline? 
What is the effect of the reconstruction loss? Can it be removed? There should be some ablation study here.
Figure 5 is unclear what is being displayed, there are no labels.

Overall I would advise the authors to address these questions and suggest this as a paper suitable for a workshop submission.
","[4, 4, 5]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review acknowledges the paper's exploration of an interesting problem and its novelty but suggests it's a small advancement. The reviewer finds the baselines weak and requires further experiments and clarifications. Recommending it as a workshop submission indicates a moderate assessment, not a strong rejection. The language is direct and critical but maintains academic professionalism.",20.0,50.0
Label Embedding Network: Learning Label Representation for Soft Training of Deep Networks,"['Xu Sun', 'Bingzhen Wei', 'Xuancheng Ren', 'Shuming Ma']",Reject,2018,"[13, 2, 2, 9]","[18, 4, 6, 14]","[278, 16, 91, 111]","[130, 5, 34, 46]","[132, 9, 50, 62]","[16, 2, 7, 3]","This paper proposes a label embedding network method that learns label embeddings during the training process of deep networks. 
Pros: Good empirical results.
Cons:  There is not much technical contribution. The proposed approach is neither well motivated, nor well presented/justified.  The presentation of the paper needs to be improved. 

1. Part of the motivation on page 1 does not make sense. In particular, for paragraph 3, if the classification task is just to separate A from B, then (1,0) separation should be better than (0.8, 0.2). 

2. Label embedding learning has been investigated in many previous works. The authors however ignored all the existing works on this topic, but enforce label embedding vectors as similarities between labels in Section 2.1 without clear motivation and justification. This assumption is not very natural — though label embeddings can capture semantic information and label correlations, it is unnecessary that label embedding matrix should be m xm and each entry should represent the similarity between a pair of labels.  The paper needs to provide a clear rationale/justification for the assumptions made, while clarifying the difference (and reason) from the literature works. 

3. The proposed model is not well explained.  
(1) By using the objective in eq.(14), how to learn the embeddings E? 
(2) The authors state “In back propagation, the gradient from z2 is kept from propagating to h”.  This makes the learning process quite arbitrary under the objective in eq.(14). 
(3) The label embeddings are not directly used for the classification (H(y, z’_1)), but rather as auxiliary part of the objective.  How to decide the test labels?
","[4, 3, 4]","[' Ok but not good enough - rejection', ' Clear rejection', ' Ok but not good enough - rejection']","[3, 4, 5]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The reviewer highlights both positive and negative aspects of the paper. While praising the empirical results, they express concerns about the technical contribution, motivation, presentation, and justification of assumptions. The language, while direct and critical, maintains a professional and academic tone.",-20.0,60.0
DeepArchitect: Automatically Designing and Training Deep Architectures,"['Renato Negrinho', 'Geoff Gordon']",Reject,2018,"[5, 26]","[8, 30]","[11, 181]","[5, 115]","[6, 49]","[0, 17]","The author present a language for expressing hyperparameters (HP) of a network. This language allows to define a tree structure search space to cover the case where some HP variable exists only if some previous HP variable took some specific value. Using this tool, they explore the depth of the network, when to apply batch-normalization, when to apply dropout and some optimization variables. They compare the search performance of random search, monte carlo tree search and a basic implementation of a Sequential Model Based Search. 

The novelty in this paper is below what is expected for a publication at ICLR. I recommend rejection.","[4, 4, 5]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[5, 5, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']","The review starts with a neutral summary of the paper's content. However, it quickly concludes that the ""novelty in this paper is below what is expected"" and recommends rejection. This indicates a negative sentiment. The language used is formal and typical for academic peer reviews, suggesting a neutral politeness level.",-50.0,0.0
Enhance Word Representation for Out-of-Vocabulary on Ubuntu Dialogue Corpus,"['JIANXIONG DONG', 'Jim Huang']",Reject,2018,"[8, 20]","[8, 23]","[2, 14]","[1, 11]","[1, 2]","[0, 1]","The main contributions in this paper are:
1) New variants of a recent LSTM-based model (""ESIM"") are applied to the task of response-selection in dialogue modeling -- ESIM was originally introduced and evaluated for natural language inference. In this new setting, the ESIM model (vanilla and extended) outperform previous models when trained and evaluated on two distinct conversational datasets.

2) A fairly trivial method is proposed to extend the coverage of pre-trained word embeddings to deal with the OOV problem that arises when applying them to these conversational datasets.
The method itself is to combine d1-dimensional word embeddings that were pretrained on a large unannotated corpus (vocabulary S) with distinct d2-dimensional word embeddings that are trained on the task-specific training data (vocabulary T). The enhanced (d1+d2)-dimensional representation for a word is constructed by concatenating its vectors from the two embeddings, setting either the d1- or d2-dimensional subvector to zeros when the word is absent from either S or T, respectively. This method is incorporated as an extension into ESIM and evaluated on the two conversation datasets.

The main results can be characterized as showing that this vocabulary extension method leads to performance gains on two datasets, on top of an ESIM-model extended with character-based word embeddings, which itself outperforms the vanilla ESIM model.

These empirical results are potentially meaningful and could justify reporting, but the paper's organization is very confusing, and too many details are too unclear, leading to low confidence in reproducibility. 

There is basic novelty in applying the base model to a new task, and the analysis of the role of the special conversational boundary tokens is interesting and can help to inform future modeling choices. The embedding-enhancing method has low originality but is effective on this particular combination of model architecture, task and datasets. I am left wondering how well it might generalize to other models or tasks, since the problem it addresses shows up in many other places too...

Overall, the presentation switches back and forth between the Douban corpus and the Ubuntu corpus, and between word2vec and Glove embeddings, and this makes it very challenging to understand the details fully.

S3.1 - Word representation layer: This paragraph should probably mention that the character-composed embeddings are newly introduced here, and were not part of the original formulation of ESIM. That statement is currently hidden in the figure caption.

Algorithm 1:
- What set does P denote, and what is the set-theoretic relation between P and T?
- Under one possible interpretation, there may be items in P that are in neither T nor S, yet the algorithm does not define embeddings for those items even though its output is described as ""a dictionary with word embeddings ... for P"". This does not seem consistent? I think the sentence in S4.2 about initializing remaining OOV words as zeros is relevant and wonder if it should form part of the algorithm description?

S4.1 - What do the authors mean by the statement that response candidates for the Douban corpus were ""collected by Lucene retrieval model""?

S4.2 - Paragraph two is very unclear. In particular, I don't understand the role of the Glove vectors here when Algorithm 1 is used, since the authors refer to word2vec vectors later in this paragraph and also in the Algorithm description.

S4.3 - It's insufficiently clear what the model definitions are for the Douban corpus. Is there still a character-based LSTM involved, or does FastText make it unnecessary?

S4.3 - ""It can be seen from table 3 that the original ESIM did not perform well without character embedding."" This is a curious way to describe the result, when, in fact, the ESIM model in table 3 already outperforms all the previous models listed.

S4.4 - gensim package -- for the benefit of readers unfamiliar with gensim, the text should ideally state explicitly that it is used to create the *word2vec* embeddings, instead of the ambiguous ""word embeddings"".

","[5, 6, 3]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Clear rejection']","[4, 3, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The reviewer acknowledges the potential significance of the findings, particularly the application of the base model to a new task and the analysis of conversational boundary tokens. However, they express concerns about the paper's clarity, organization, and reproducibility. The reviewer finds the embedding-enhancing method to have low originality and questions its generalizability. The numerous unclear details and confusing presentation contribute to a less positive sentiment. The language used is generally polite and professional, focusing on constructive criticism and suggestions for improvement.",20.0,60.0
Sequential Coordination of Deep Models for Learning Visual Arithmetic,"['Eric Crawford', 'Guillaume Rabusseau', 'Joelle Pineau']",Reject,2018,"[6, 5, 19]","[8, 10, 24]","[10, 60, 308]","[4, 23, 151]","[4, 33, 119]","[2, 4, 38]","Summary: This work is a variant of previous work (Zaremba et al. 2016) that enables the use of (noisy) operators that invoke pre-trained neural networks and is trained with Actor-Critic. In this regard it lacks a bit of originality. The quality of the experimental evaluation is not great. The clarity of the paper could be improved upon but is otherwise fine. The existence of previous work (Zaremba et al. 2016) renders this work (including its contributions) not very significant. Relations to prior work are missing. But let's wait for the rebuttal phase. 

Pros 
-It is confirmed that noisy operators (in the form of neural networks) can be used on the visual arithmetic task

Cons
-Not very novel
-Experimental evaluation is wanting

The focus of this paper is on integrating perception and reasoning in a single system. This is done by specifying an interface that consists of a set of discrete operations (some of which involve perception) and memory slots. A parameterized policy that can make use of these these operations is trained via Actor-Critic to solve some reasoning tasks (arithmetics in this case). 

The proposed system is a variant of previous work (Zaremba et al. 2016) on the concept of interfaces, and similarly learns a policy that utilizes such an interface to perform reasoning tasks, such as arithmetics. In fact, the only innovation proposed in this paper is to incorporate some actions that invoke a pre-trained neural network to “read” the symbol from an image, as opposed to parsing the symbol directly. However, there is no reason to expect that this would not function in previous work (Zaremba et al. 2016), even when the network is suboptimal (in which case the operator becomes noisy and the policy should adapt accordingly). Another notable difference is that the proposed system is trained with Actor-Critic as opposed to Q-learning, but this is not further elaborated on by the authors. 

The proposed system is evaluated on a visual arithmetics task. The input consists of a 2x2 grid of extended MNIST characters. Each location in the grid then corresponds to the 28 x 28 pixel representation of the digit. Actions include shifting the “fovea” to a different entry of the grid, invoking the digit NN or the operator NN which parse the current grid entry, and some symbolic operations that operate on the memory. The fact that the input is divided into a 2x2 grid severely limits the novelty of this approach compared to previous work (Zaremba et al. 2016). Instead it would have been interesting to randomly spawn digits and operators in a 56 x 56 image and maintain 4 coordinates that specify a variable-sized grid that glimpses a part of the image. This would make the task severely more difficult, given fixed pre-trained networks. The addition of the salience network is unclear to me in the context of MNIST digits, since any pixel that is greater than 0 is salient? I presume that the LSTM uses this operator to evaluate whether the current entry contains a digit or an operator. If so, wouldn’t simply returning the glimpse be enough?

In the experiments the proposed system is compared to three CNNs on two different visual arithmetic tasks, one that includes operators as part of the input and one that incorporates operators only in the tasks description. In all cases the proposed method requires fewer samples to achieve the final performance, although given enough samples all of the CNNs will solve the tasks. This is not surprising as this comparison is rather unfair. The proposed system incorporates pre-trained modules, whose training samples are not taken into account. On the other hand the CNNs are trained from scratch and do not start with the capability to recognize digits or operators. Combined with the observation that all CNNs are able to solve the task eventually, there is little insight in the method's performance that can be gained from this comparison. 

Although the visual arithmetics on a 2x2 grid is a toy task it would at least be nice to evaluate some of the policies that are learned by the LSTM (as done by Zaremba) to see if some intuition can be recovered from there. Proper evaluation on a more complex environment (or at least on that does not assume discrete grids) is much desired. When increasing the complexity (even if by just increasing the grid size) it would be good to compare to a recurrent method (Pyramid-LSTM, Pixel-RNN) as opposed to a standard CNN as it lacks memory capabilities and is clearly at a disadvantage compared to the LSTM.

Some detailed comments are:

The introduction invokes evidence from neuroscience to argue that the brain is composed of (discrete) modules, without reviewing any of the counter evidence (there may be a lot, given how bold this claim is).

From the introduction it is unclear why the visual arithmetic task is important.

Several statements including the first sentence lack citations.

The contribution section is not giving any credit to Zaremba et al. (2016) whereas this work is at best a variant of that approach.

In the experiment section the role of the saliency detector is unclear.

Experiment details are lacking and should be included.

The related work section could be more focused on the actual contribution being made.

It strikes me as odd that in the discussion the authors propose to make the entire system differentiable, since this goes against the motivation for this work.


Relation to prior work:

p 1: The authors write: ""We also borrow the notion of an interface as proposed in Zaremba et al. (2016). An interface is a designed, task-specific machine that mediates the learning agent’s interaction with the external world, providing the agent with a representation (observation and action spaces) which is intended to be more conducive to learning than the raw representations. In this work we formalize an interface as a separate POMDP I with its own state, observation and action spaces."" 

This interface terminology for POMDPs was actually introduced in:

J.  Schmidhuber. Reinforcement learning in Markovian and non-Markovian environments. In D. S. Lippman, J. E. Moody, and D. S. Touretzky, editors, Advances in Neural Information Processing Systems 3, NIPS'3, pages 500-506. San Mateo, CA: Morgan Kaufmann, 1991.

p 4: authors write: ""For the policy πθ, we employ a Long Short-Term Memory (LSTM)"" 

Do the authors use the (cited) original LSTM of 1997, or do they also use the forget gates (recurrent units with gates) that most people are using now, often called the vanilla LSTM, by Gers et al (2000)?

p 4: authors write: ""One obvious point of comparison to the current work is recent research on deep neural networks designed to learn to carry out algorithms on sequences of discrete symbols. Some of these frameworks, including the Differen-tiable Forth Interpreter (Riedel and Rocktäschel, 2016) and TerpreT (Gaunt et al., 2016b), achieve this by explicitly generating code, while others, including the Neural Turing Machine (NTM; Graves et al., 2014), Neural Random-Access Machine (NRAM; Kurach et al., 2015), Neural Programmer (NP; Neelakan- tan et al., 2015), Neural Programmer-Interpreter (NPI; Reed and De Freitas, 2015) and work in Zaremba et al. (2016) on learning algorithms using reinforcement learning, avoid gen- erating code and generally consist of a controller network that learns to perform actions in a (sometimes differentiable) external computational medium in order to carry out an algorithm.""

Here the original work should be mentioned, on differentiable neural stack machines: 

G.Z. Sun and H.H. Chen and  C.L. Giles and Y.C. Lee and D. Chen. Connectionist Pushdown Automata that Learn Context-Free Grammars. IJCNN-90, Lawrence Erlbaum, Hillsdale, N.J., p 577, 1990.

Mozer, Michael C and Das, Sreerupa. A connectionist symbol manipulator that discovers the structure of context-free languages. Advances in Neural Information Processing Systems (NIPS), p 863-863, 1993.


","[4, 3, 2]","[' Ok but not good enough - rejection', ' Clear rejection', ' Strong rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is highly critical of the paper, pointing out a lack of novelty and concerns with the experimental evaluation. The reviewer uses phrases like ""lacks a bit of originality,"" ""quality of the experimental evaluation is not great,"" ""not very significant,"" and ""renders this work (...) not very significant,"" which all point towards a negative sentiment. While the reviewer acknowledges some positive aspects and provides constructive feedback, the overall tone and the numerous criticisms contribute to the negative sentiment. The language used is formal and academic, but direct in its criticism. However, it avoids harsh or disrespectful language, staying within the bounds of professional academic discourse.",-60.0,50.0
A Neural-Symbolic Approach to Natural Language Tasks,"['Qiuyuan Huang', 'Paul Smolensky', 'Xiaodong He', 'Li Deng', 'Dapeng Wu']",Reject,2018,"[8, 36, 20, 28, 20]","[13, 40, 25, 30, 25]","[58, 64, 316, 329, 458]","[29, 28, 189, 188, 125]","[18, 24, 106, 36, 45]","[11, 12, 21, 105, 288]","The paper claims that ""Deep Learning (DL) has not been able to explicitly represent and enforce grammatical structures"", which is false, see ""Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks"", ""Ask Me Anything: Dynamic Memory Networks for Natural Language Processing"", ""DRAGNN: A Transition-based Framework for Dynamically Connected Neural Networks"" or ""Deep Compositional Question Answering with Neural Module Networks"".

The Introduction triple challenge is confusing, not clear what are the challenges this paper tries to address.

""The representation learned in a crucial layer of the TPGN can be interpreted as encoding grammatical roles"" Doesn't refer to any specific kind of layer, or what it make it special.

The idea of using outer product as a layer has already been explored in ""Multimodal compact bilinear pooling for visual question answering and visual grounding""

The following paragraph in page 2 is not clear, very confusing:
The work reported here .... their categories


In page 3 authors claim that the ""vectors are linearly independent"" but didn't specify how they enforce that.

Figure 3 contradicts Figure 1, not clear what are the inputs for module S.

The experiments reported in Table1 are useless, there a tons of previous work with much better results, see 
https://competitions.codalab.org/competitions/3221#results

Even the numbers reported for Vinyals et al. (2015) are much higher in the leaderboard. 

There is no comparison with other models that use attention or analysis of the impact of the increased number of parameters of the method proposed. 

The experiments about POS tagger and Phrase Classifier are reported on 5000 from the COCO test set, which is useful for comparisons. Should report numbers on PennTreeBank or other common POS dataset.

The text is missing a lot of references, for example:
 - page 2 GSC
 - page 2 The first approach takes the detected by a CNN ....
 - page 3 previous work where TPRs are hand-crafted","[4, 4, 5]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is highly critical of the paper, pointing out several flaws and weaknesses. The reviewer lists multiple instances where the paper's claims are inaccurate or lack clarity. They also question the methodology and results, deeming the experiments insufficient and the comparisons unfair. The tone is direct and critical, bordering on harsh in some parts, indicating a negative sentiment. However, the language remains professional and avoids personal attacks, suggesting a level of politeness within the academic critique.",-75.0,20.0
Large-scale Cloze Test Dataset Designed by Teachers,"['Qizhe Xie', 'Guokun Lai', 'Zihang Dai', 'Eduard Hovy']",Reject,2018,"[4, 5, 3, 34]","[9, 7, 7, 39]","[26, 18, 51, 517]","[10, 8, 25, 350]","[13, 10, 26, 123]","[3, 0, 0, 44]","1) this paper introduces a new cloze dataset, ""CLOTH"", which is designed by teachers. The authors claim that this cloze dataset is a more challenging dataset since CLOTH requires a deeper language understanding and wider attention span. I think this dataset is useful for demonstrating the robustness of current RC models. However, I still have the following questions which lead me to reject this paper.

2) I have the questions as follows:
i) The major flaw of this paper is about the baselines in experiments. I don't think the language model is a robust baseline for this paper.  When a wider span is used for selecting answers, the attention-based model should be a reasonable baseline instead of pure LM. 
ii) the author also should provide the error rates for each kind of questions (grammar questions or long-term reasoning). 
iii) the author claim that this CLOTH dataset requires wider span for getting the correct answer, however, there are only 22.4 of the entire data need long-term reasoning. More importantly, there are 26.5% questions are about grammar. These problems can be easily solved by LM. 
iv) I would not consider 16% percent of accuracy is a ""significant margin"" between human and pure LM-based methods. LM-based methods should not be considered as RC model.
v) what kind accuracy is improved if you use 1-billion corpus trained LM? Are these improvements mostly in grammar? I did not see why larger training corpus for LM could help a lot about reasoning since reasoning is only related to question document.
","[4, 7, 4]","[' Ok but not good enough - rejection', ' Good paper, accept', ' Ok but not good enough - rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer acknowledges the usefulness of the dataset but raises several significant concerns, leading to a rejection recommendation. The tone is critical, focusing on flaws in the baseline choices, data composition, and interpretation of results. The reviewer questions the validity of using language models as baselines and points out inconsistencies in the authors' claims about the dataset's difficulty.",-60.0,20.0
Principled Hybrids of Generative and Discriminative Domain Adaptation,"['Han Zhao', 'Zhenyao Zhu', 'Junjie Hu', 'Adam Coates', 'Geoff Gordon']",Reject,2018,"[6, 6, 7, 15, 26]","[11, 7, 12, 16, 30]","[111, 20, 63, 42, 181]","[54, 9, 31, 29, 115]","[51, 11, 30, 10, 49]","[6, 0, 2, 3, 17]","The authors propose a probabilistic framework for semi-supervised learning and domain adaptation. By varying the prior distribution, the framework can incorporate both generative and discriminative modeling.  The authors emphasize on one particular form of constraint on the prior distribution, that is weight (parameter) sharing, and come up with a concrete model named Dauto for domain adaptation. A domain confusion loss is added to learn domain-invariant feature representations. The authors compared Dauto with several baseline methods on several datasets and showed improvement. 

The paper is well-organized and easy to follow. The probabilistic framework itself is quite straight-forward. The paper will be more interesting if the authors are able to extend the discussion on different forms of prior instead of the simple parameter sharing scheme. 

The proposed DAuto is essentially DANN+autoencoder.  The minimax loss employed in DANN and DAuto is known to be prone to degenerated gradient for the generator. It would be interesting to see if the additional auto-encoder part help address the issue. 

The experiments miss some of the more recent baseline in domain adaptation, such as Adversarial Discriminative Domain Adaptation (Tzeng, Eric, et al. 2017). 

It could be more meaningful to organize the pairs in table by target domain instead of source, for example, grouping 9->9, 8->9, 7->9 and 3->9 in the same block. DAuto does seem to offer more boost in domain pairs that are less similar. ","[5, 5, 6]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Marginally above acceptance threshold']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review starts with positive statements, highlighting the merits of the paper such as clarity, organization, and the promising nature of the framework. However, it also presents constructive criticisms and suggestions for improvement, indicating that the paper could be stronger. The reviewer suggests exploring different prior distributions, addressing potential limitations of the DAuto model, including more recent baselines, and improving the presentation of results. The language used is professional and polite throughout, employing constructive criticism rather than harsh language.",50.0,80.0
Towards Binary-Valued Gates for Robust LSTM Training ,"['Zhuohan Li', 'Di He', 'Fei Tian', 'Wei Chen', 'Tao Qin', 'Liwei Wang', 'Tie-Yan Liu']",Reject,2018,"[8, 18, 10, 10, 17, 14, 18]","[13, 23, 14, 15, 22, 19, 23]","[29, 230, 69, 122, 485, 217, 565]","[10, 107, 37, 51, 240, 89, 280]","[12, 74, 18, 61, 163, 101, 220]","[7, 49, 14, 10, 82, 27, 65]","This paper aims to push the LSTM gates to be binary. To achieve this, the paper proposes to employ the recent Gumbel-Softmax trick to obtain end-to-end trainable categorical distribution (taking 0 or 1 value). The resulted G2-LSTM is applied for language model and machine translation in the experiments. 

The novelty of this paper is limited. Just directly apply the Gumbel-Softmax trick. 

The motivation is not explained clearly and convincingly. Why need to pursue binary gates? According to the paper, it may give better generalization performance. But there is no theoretical or experimental evidence provided by this paper to support this argument. 

The results of the new G2-LSTM are not significantly better than baselines in the experiments.","[4, 6, 6]","[' Ok but not good enough - rejection', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review is primarily negative. The reviewer points out limitations in novelty and motivation, stating they are not convincingly explained. The reviewer also undermines the paper's claim of better generalization and concludes by highlighting the insignificant improvement over baselines. While the language is critical, it avoids harsh or disrespectful wording, maintaining a professional tone. ",-50.0,50.0
GENERATIVE LOW-SHOT NETWORK EXPANSION,"['Adi Hayat', 'Mark Kliger', 'Shachar Fleishman', 'Daniel Cohen-Or']",Reject,2018,"[1, 20, 24, 30]","[1, 22, 25, 35]","[2, 23, 23, 486]","[1, 9, 10, 115]","[1, 6, 4, 120]","[0, 8, 9, 251]","The paper proposes a method for adapting a pre-trained network, trained on a fixed number of
classes, to incorporate novel classes for doing classification, especially when the novel classes
only have a few training examples available. They propose to do a `hard' distillation, i.e. they
introduce new nodes and parameters to the network to add the new classes, but only fine-tune the new
networks without modifying the original parameters. This ensures that, in the new expanded and
fine-tuned network, the class confusions will only be between the old and new classes and not
between the old classes, thus avoiding catastrophic forgetting. In addition they use GMMs trained on
the old classes during the fine-tuning process, thus avoiding saving all the original training data.
They show experiments on public benchmarks with three different scenarios, i.e.  base and novel
classes from different domains, base and novel classes from the same domain and novel classes have
similarities among themselves, and base and novel classes from the same domain and each novel class
has similarities with at least one of the base class.                        
                                                                             
- The paper is generally well written and it is clear what is being done     
- The idea is simple and novel; to the best of my knowledge it has not been tested before
- The method is compared with Nearest Class Means (NCM) and Prototype-kNN with soft distillation
  (iCARL; where all weights are fine-tuned). The proposed method performs better in low-shot
  settings and comparably when large number of training examples of the novel classes are available
- My main criticism will be the limited dataset size on which the method is validated. The ILSVRC12
  subset contains 5 base and 5 novel classes and the UT-Zappos50K subset also has 10 classes. The
  idea is simple and novel, which is good, but the validation is limited and far from any realistic
  use. Having only O(10) classes is not convincing, especially when the datasets used do have large
  number of classes. I agree that this will not allow or will takes some involved manual effort to
  curate subsets for the settings proposed, but it is necessary for being convincing.","[4, 6, 4]","[' Ok but not good enough - rejection', ' Marginally above acceptance threshold', ' Ok but not good enough - rejection']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer provides a positive sentiment by stating the paper is well-written, the idea is simple, novel, and performs well in experiments. However, the reviewer has a major concern regarding the limited dataset size used for validation, which impacts the convincingness of the results. Overall, the sentiment is positive but tempered by the significant limitation. Therefore, the score is moderately positive.",60.0,70.0
Spectral Graph Wavelets for Structural Role Similarity in Networks,"['Claire Donnat', 'Marinka Zitnik', 'David Hallac', 'Jure Leskovec']",Reject,2018,"[3, 7, 4, 16]","[8, 12, 7, 21]","[14, 100, 22, 460]","[5, 21, 9, 232]","[9, 40, 10, 188]","[0, 39, 3, 40]","The paper derived a way to compare nodes in graph based on wavelet analysis of graph laplacian. The method is correct but it is not clear whether the method can match the performance of state-of-the-art methods such as graph convolution neural network of Duvenaud  et al. and Structure2Vec of Dai et al. in large scale datasets.  
1. Convolutional Networks on Graphs for Learning Molecular Fingerprints. D Duvenaud et al., NIPS 2015. 
2. Discriminative embeddings of latent variable models for structured data. Dai et al. ICML 2016.

","[5, 3, 5]","[' Marginally below acceptance threshold', ' Clear rejection', ' Marginally below acceptance threshold']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review acknowledges the correctness of the method ('The method is correct') which indicates a positive attitude. However, it expresses uncertainty about the method's performance compared to other methods, which lowers the positivity. Overall, the sentiment is mildly positive. The language used is neutral and professional, without any negative or overly positive phrasing.",30.0,50.0
Censoring Representations with Multiple-Adversaries over Random Subspaces,"['Yusuke Iwasawa', 'Kotaro Nakayama', 'Yutaka Matsuo']",Reject,2018,"[8, 14, 21]","[13, 18, 26]","[56, 44, 252]","[29, 32, 156]","[16, 6, 52]","[11, 6, 44]","The below review addresses the first revision of the paper. The revised version does address my concerns. The fact that the paper does not come with substantial theoretical contributions/justification still stands out.

---

The authors present a variant of the adversarial feature learning (AFL) approach by Edwards & Storkey. AFL aims to find a data representation that allows to construct a predictive model for target variable Y, and at the same time prevents to build a predictor for sensitive variable S. The key idea is to solve a minimax problem where the log-likelihood of a model predicting Y is maximized, and the log-likelihood of an adversarial model predicting S is minimized. The authors suggest the use of multiple adversarial models, which can be interpreted as using an ensemble model instead of a single model.

The way the log-likelihoods of the multiple adversarial models are aggregated does not yield a probability distribution as stated in Eq. 2. While there is no requirement to have a distribution here - a simple loss term is sufficient - the scale of this term differs compared to calibrated log-likelihoods coming from a single adversary. Hence, lambda in Eq. 3 may need to be chosen differently depending on the adversarial model. Without tuning lambda for each method, the empirical experiments seem unfair. This may also explain why, for example, the baseline method with one adversary effectively fails for Opp-L. A better comparison would be to plot the performance of the predictor of S against the performance of Y for varying lambdas. The area under this curve allows much better to compare the various methods.

There are little theoretical contributions. Basically, instead of a single adversarial model - e.g., a single-layer NN or a multi-layer NN - the authors propose to train multiple adversarial models on different views of the data. An alternative interpretation is to use an ensemble learner where each learner is trained on a different (overlapping) feature set. Though, there is no theoretical justification why ensemble learning is expected to better trade-off model capacity and robustness against an adversary. Tuning the architecture of the single multi-layer NN adversary might be as good?

In short, in the current experiments, the trade-off of the predictive performance and the effectiveness of obtaining anonymized representations effectively differs between the compared methods. This renders the comparison unfair. Given that there is also no theoretical argument why an ensemble approach is expected to perform better, I recommend to reject the paper.","[6, 6, 5]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally below acceptance threshold']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with a neutral statement acknowledging the authors' effort in addressing previous concerns. However, it expresses disappointment with the lack of theoretical depth, which is a significant drawback. The reviewer then delves into specific technical issues, questioning the fairness of the experimental setup and the justification for the proposed approach. The overall tone is critical, pointing out flaws and suggesting alternative explanations for the observed results. The recommendation to reject the paper clearly indicates a negative sentiment. While the reviewer provides constructive criticism, the language remains polite and professional throughout.",-40.0,60.0
Auxiliary Guided Autoregressive Variational Autoencoders,"['Thomas Lucas', 'Jakob Verbeek']",Reject,2018,"[3, 18]","[7, 23]","[21, 136]","[10, 71]","[10, 41]","[1, 24]","The authors present Auxiliary Guided Autoregressive Variational autoEncoders (AGAVE), a hybrid approach that combines the strengths of variational autoencoders (global statistics) and autorregressive models (local statistics) for improved image modeling. This is done by controlling the capacity of the autorregressive component within an auxiliary loss function.

The proposed approach is a straightforward combination of VAE and PixelCNN that although empirically better than PixelCNN, and presumably VAE, does not outperform PixelCNN++. Provided that the authors use PixelCNN++ in their approach, quantitively speaking, it is difficult to defend the value of adding a VAE component to the model. The authors do not describe how \lambda was selected, which is critical for performance, provided the results in Figure 4. That being said, the contribution from the VAE is likely to be negligible given the performance of PixelCNN++ alone.

- The KL divergence in (3) does more than simply preventing the approximation q() from becoming a point mass distribution.","[5, 5, 7]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Good paper, accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review acknowledges the proposed approach as a straightforward combination of existing methods (VAE and PixelCNN) and notes its empirical improvement over PixelCNN. However, it also points out that the approach doesn't outperform PixelCNN++, a more advanced model. The reviewer questions the value of the VAE component given the performance of PixelCNN++ and the lack of clarity regarding the selection of the \lambda parameter. The language is direct and critical but within the bounds of professional academic discourse. ",-10.0,60.0
Coupled Ensembles of Neural Networks,"['Anuvabh Dutt', 'Denis Pellerin', 'Georges Quénot']",Reject,2018,"[2, 25, 33]","[6, 27, 38]","[10, 84, 149]","[6, 60, 122]","[2, 3, 5]","[2, 21, 22]","This paper presents a deep network architecture which processes data using multiple parallel branches and combines the posterior from these branches to compute the final scores; the network is trained in end-to-end, thus training the parallel branches jointly. Existing literature with branching architecture either employ a 2 stage training approach, training branches independently and then training the fusion network, or the branching is restricted to local regions (set of contiguous layers). In effect, this paper extends the existing literature suggesting end-to-end branching. While the technical novelty, as described in the paper, is relatively limited, the thorough experimentation together with detailed comparisons between intuitive ways to combine the output of the parallel branches is certainly valuable to the research community.

+ Paper is well written and easy to follow.
+ Proposed branching architecture clearly outperforms the baseline network (same number of parameters with a single branch) and thus offer yet another interesting choice while creating the network architecture for a problem
+ Detailed experiments to study and analyze the effect of various parameters including the number of branches as well as various architectures to combine the output of the parallel branches.
+ [Ease of implementation] Suggested architecture can be easily implemented using existing deep learning frameworks.

- Although joint end-to-end training of branches certainly brings value compared to independent training, but the increased resource requirements may limits the applicability to large benchmarks such as ImageNet. While authors suggests a way to circumvent such limitations by training branches on separate GPUs but this would still impose limits on the number of branches as well as its ease of implementation.
- Adding an overview figure of the architecture in the main paper (instead of supplementary) would be helpful.
- Branched architecture serve as a regularization by distributing the gradients across different branches; however this also suggests that early layers on the network across branches would be independent. It would helpful if authors would consider an alternate archiecture where early layers may be shared across branches, suggesting a delayed branching, with fusion at the final layer.
- One of the benefits of architectures such as DenseNet is their usefulness as a feature extractor (output of lower layers) which generalizes even to domain other that the dataset; the branched architecture could potentially diminish this benefit.

Minor edits: Page 1. 'significantly match and improve' => 'either match or improve'

Additional notes:
- It would interesting to compare this approach with a conditional training pipeline that sequentially adds branches, keeping the previous branches fixed. This may offer as a trade-off between benefits of joint training of branches vs being able to train deep models with several branches.
","[6, 6, 6]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is overall positive. The reviewer acknowledges the limited novelty but highlights the thoroughness and value of the experiments. The reviewer also provides constructive criticism and suggestions for improvement, indicating a desire to see the work strengthened. 

- Sentiment Score: The numerous positive points, use of positive language ('thorough', 'valuable', 'interesting'), outweigh the constructive criticism, making the overall sentiment positive. 
- Politeness Score: The language is constructive, professional, and respectful throughout. The suggestions are framed as potential improvements rather than flaws.",65.0,80.0
Heterogeneous Bitwidth Binarization in Convolutional Neural Networks,"['Josh Fromm', 'Matthai Philipose', 'Shwetak Patel']",Reject,2018,"[6, 16, 23]","[11, 21, 26]","[20, 206, 81]","[11, 136, 58]","[6, 36, 6]","[3, 34, 17]","The paper tries to maintain the accuracy of 2bits network, while uses possibly less than 2bits weights.

1.  The paper misses some more recent reference, e.g. [a,b]. The author should also have a discussion on them.

2. Indeed, AlexNet is a good seedbed to test binary methods. However, it is more interesting and important to test on more advanced networks. So, I wish to see a section on testing with Resnet and GoogleNet.

Indeed, the authors have commented: ""AlexNet with batch-normalization (AlexNet-BN) is the standard model ... acceptance that improvements made to accuracy transfer well to more modern architectures."" So, please show that.

3. The paper wants to find a good trade-off on speed and accuracy. The authors have plotted such trade-off on space v.s. accuracy in Figure 3(b), then how about speed v.s. accuracy?

My concern is that one-bit system is already complicated to implement. Indeed, the authors have discussed their implementation in Section 3.3, so, how their method works in practice? One example is Section 4 in [Courbariaux et al. 2016].

4. Is trade-off between 1 to 2 bits really important? 

Compared with 2bits or ternary network, the proposed method at most achieving (1.4/2) compression ratio and (2/1.4) speedup (based on their Table 1). Is such improvement really important?

Reference:
[a]. Trained Ternary Quantization. ICLR 2017
[b]. Extremely low bit neural network: Squeeze the last bit out with ADMM. arvix 2017","[5, 6, 4]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Ok but not good enough - rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer raises valid concerns and suggestions for improvement, indicating a desire to see the paper strengthened. While not overly negative, the tone is critical, particularly in questioning the significance of the work. The language remains professional and polite throughout.",-10.0,60.0
Deep Learning is Robust to Massive Label Noise,"['David Rolnick', 'Andreas Veit', 'Serge Belongie', 'Nir Shavit']",Reject,2018,"[8, 6, 25, 33]","[13, 10, 30, 38]","[63, 47, 314, 176]","[16, 20, 189, 107]","[35, 25, 99, 26]","[12, 2, 26, 43]","The paper makes a bold claim, that deep neural networks are robust to arbitrary level of noise. It also implies that this would be true for any type of noise, and support this later claim using experiments on CIFAR and MNIST with three noise types: (1) uniform label noise (2) non-uniform but image-independent label noise, which is named ""structured noise"", and (3) Samples from out-of-dataset classes. The experiments show robustness to these types of noise. 

Review: 
The claim made by the paper is overly general, and in my own experience incorrect when considering real-world-noise. This is supported by the literature on ""data cleaning"" (partially by the authors), a procedure which is widely acknowledged as critical for good object recognition.  While it is true that some image-independent label noise can be alleviated in some datasets, incorrect labels in real world datasets can substantially harm classification accuracy.

It would be interesting to understand the source of the difference between the results in this paper and the more common results (where label noise damages recognition quality). The paper did not get a chance to test these differences, and I can only raise a few hypotheses. First, real-world noise depends on the image and classes in a more structured way. For instance, raters may confuse one bird species from a similar one, when the bird is photographed from a particular angle. This could be tested experimentally, for example by adding incorrect labels for close species using the CUB data for fine-grained bird species recognition.  Another possible reason is that classes in MNIST and CIFAR10 are already very distinctive, so are more robust to noise. Once again, it would be interesting for the paper to study why they achieve robustness to noise while the effect does not hold in general. 

Without such an analysis, I feel the paper should not be accepted to ICLR because the way it states its claim may mislead readers. 

Other specific comments: 
-- Section 3.4 the experimental setup, should clearly state details of the optimization, architecture and hyper parameter search. For example, for Conv4, how many channels at each layer? how was the net initialized? which hyper parameters were tuned and with which values? were hyper parameters tuned on a separate validation set? How was the train/val/test split done, etc. These details are useful for judging technical correctness.
-- Section 4, importance of large datasets. The recent paper by Chen et al (2017) would be relevant here.
-- Figure 8 failed to show for me. 
-- Figure 9,10, need to specify which noise model was used.





","[5, 4, 5]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[4, 5, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The reviewer raises serious concerns about the paper's claims and methodology, ultimately recommending rejection. This points to a negative sentiment. However, the reviewer does offer constructive criticism and suggestions for improvement, indicating a level of politeness in their delivery.",-50.0,50.0
Sparse Attentive Backtracking: Long-Range Credit Assignment in Recurrent Networks,"['Nan Rosemary Ke', 'Anirudh Goyal', 'Olexa Bilaniuk', 'Jonathan Binas', 'Laurent Charlin', 'Chris Pal', 'Yoshua Bengio']",Reject,2018,"[8, 5, 5, 7, 13, 22, 31]","[13, 10, 10, 11, 18, 27, 36]","[61, 101, 21, 29, 79, 230, 975]","[22, 40, 10, 12, 35, 98, 405]","[38, 60, 10, 16, 41, 109, 454]","[1, 1, 1, 1, 3, 23, 116]","re. Introduction, page 2: Briefly explain here how SAB is different from regular Attention?

Good paper. There's not that much discussion of the proposed SAB compared to regular Attention, perhaps that could be expanded. Also, I suggest summarizing the experimental findings in the Conclusion.","[8, 5, 5]","[' Top 50% of accepted papers, clear accept', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer opens with ""Good paper"", which clearly indicates a positive sentiment. They offer constructive criticism and suggestions, but the overall tone is encouraging and helpful rather than negative or dismissive. The language used is polite and professional throughout.",70.0,80.0
Parametrizing filters of a CNN with a GAN,"['Yannic Kilcher', 'Gary Becigneul', 'Thomas Hofmann']",Reject,2018,"[3, 2, 26]","[8, 5, 31]","[20, 24, 205]","[6, 11, 112]","[13, 13, 69]","[1, 0, 24]","Recent work on incorporating prior knowledge about invariances into neural networks suggests that the feature dimension in a stack of feature maps has some kind of group or manifold structure, similar to how the spatial axes form a plane. This paper proposes a method to uncover this structure from the filters of a trained ConvNet. The method uses an InfoGAN to learn the distribution of filters. By varying the latent variables of the GAN, one can traverse the manifold of filters. The effect of moving over the manifold can be visualized by optimizing an input image to produce the same activation profile when using a perturbed synthesized filter as when using an unperturbed synthesized filter.

The idea of empirically studying the manifold / topological / group structure in the space of filters is interesting. A priori, using a GAN to model a relatively small number of filters seems problematic due to overfitting, but the authors show that their InfoGAN approach seems to work well.

My main concerns are:

Controls
To generate the visualizations, two coordinates in the latent space are varied, and for each variation, a figure is produced. To figure out if the GAN is adding anything, it would be nice to see what would happen if you varied individual coordinates in the filter space (""x-space"" of the GAN), or varied the magnitude of filters or filter planes. Since the visualizations are as much a function of the previous layers as they are a function of the filters in layer l which are modelled by the GAN, I would expect to see similar plots for these baselines.

Lack of new Insights
The visualizations produced in this paper are interesting to look at, but it is not clear what they tell us, other than ""something non-trivial is going on in these networks"". In fact, it is not even clear that the transformations being visualized are indeed non-linear in pixel space (note that even a 2D diffeomorphism, which is a non-linear map on R^2, is a linear operator on the space of *functions* on R^2, i.e. on the space of images). In any case, no attempt is made to analyze the results, or provide new insights into the computations performed by a trained ConvNet.

Interpretation
This is a minor point, but I would not say (as the paper does) that the method captures the invariances learned by the model, but rather that it aims to show the variability captured by the model. A ReLU net is only invariant to changes that are mapped to zero by the ReLU, or that end up in the kernel of one of the linear layers. The presented method does not consider this and hence does not analyze invariances.

Minor issues:
- In the last equation on page 2, the right-hand side is missing a ""min max"".","[4, 4, 2]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Strong rejection']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is mostly positive in its tone, acknowledging the interesting aspects of the paper's idea and the effectiveness of the proposed method. However, it also raises significant concerns about the lack of new insights and the need for further analysis and controls. The reviewer maintains a professional and constructive tone throughout, suggesting improvements rather than simply criticizing. Therefore, the sentiment leans slightly towards the positive side, while the politeness remains highly positive.",60.0,80.0
"Dissecting Adam: The Sign, Magnitude and Variance of Stochastic Gradients","['Lukas Balles', 'Philipp Hennig']",Reject,2018,"[3, 9]","[8, 14]","[17, 165]","[6, 66]","[11, 84]","[0, 15]","Stochastic Sign Descent (SSD) and Stochastic Variance Adapted Gradient (SVAG) are inspired by ADAM and studied in this paper, together with momentum terms. 

Analysis showed that SSD should work better than usual SGD when the Hessian of training loss is highly diagonal dominant.  It is intrigued to observe that for MNIST and CIFAR10, SSD with momentum champions with better efficiency than ADAM, SGD and SVAG, while on the other hand, in CIFAR100, momentum-SVAG and SGD beat SSD and ADAM. Does it suggest the Hessians associated with MNIST and CIFAR10 training loss more diagonally dominant? 

There are other adaptive step-sizes such as Barzilai-Borwein (BB) Step Sizes introduced to machine learning by Tan et al. NIPS 2016. Is there any connections between variance adaptation here and BB step size? ","[6, 4, 4]","[' Marginally above acceptance threshold', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is fairly neutral. It raises some interesting questions and observations about the paper's findings but doesn't express outright praise or criticism. The language is academic and polite, suggesting areas for further investigation rather than flaws in the research.",10.0,70.0
Unbiased scalable softmax optimization,"['Francois Fagan', 'Garud Iyengar']",Reject,2018,"[6, 21]","[10, 26]","[11, 74]","[4, 23]","[6, 18]","[1, 33]","The paper presents interesting algorithms for minimizing softmax with many classes. The objective function is a multi-class classification problem (using softmax loss) and with linear model. The main idea is to rewrite the obj as double-sum using the dual formulation and then apply SGD to solve it. At each iteration, SGD samples a subset of training samples and labels. The main contribution of this paper is: 1) proposing a U-max trick to improve the numerical stability and 2) proposing an implicit SGD approach. It seems the implicit SGD approach is better in the experimental comparisons. 

I found the paper quite interesting, but meanwhile I have the following comments and questions: 

- As pointed out by the authors, the idea of this formulation and doubly SGD is not new. (Raman et al, 2016) has used a similar trick to derive the double-sum formulation and solved it by doubly SGD. The authors claim that  the algorithm in (Raman et al) has an O(NKD) cost for updating u at the end of each epoch. However, since each epoch requires at least O(NKD) time anyway (sometimes larger, as in Proposition 2), is another O(NKD) a significant bottleneck? Also, since the formulation is similar to (Raman et al., 2016), a comparison is needed. 

- I'm confused by Proposition 1 and 2. In appendix E.1, the formulation of the update is derived, but why we need Newton to get log(1/epsilon) time complexity? I think most first order methods instead of Newton will have linear converge (log(1/epsilon) time)? Also, I guess we are assuming the obj is strongly convex?

- The step size is selected in one dataset and used for all others. This might lead to divergence of other algorithms, since usually step size depends on data. As we can see, OVE, NCE and IS diverges on Wiki-small, which may be fixed if the step size is chosen for each data (in practice we can choose using subsamples for each data). 

- All the comparisons are based on ""epochs"", but the competing algorithms are quite different and can have very different running time for each epoch. For example, implicit SGD has another iterative solver for each update. Therefore, the timing comparison is needed in this paper to justify that implicit SGD is faster. 

- The claim that ""implicit SGD never overshoots the optimum"" needs more supports. Is it proved in some previous papers? 

- The presentation can be improved. I think it will be helpful to state the algorithms explicitly in the main paper.","[5, 5, 5]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer finds the paper interesting, which points towards a positive sentiment. However, they also raise several questions and concerns, suggesting the paper is not without its flaws. Therefore, the sentiment is moderately positive. The language used is polite and professional throughout, employing constructive criticism and respectful phrasing.",60.0,80.0
Image Segmentation by Iterative Inference from Conditional Score Estimation,"['Adriana Romero', 'Michal Drozdzal', 'Akram Erraqabi', 'Simon Jégou', 'Yoshua Bengio']",Reject,2018,"[8, 9, 3, 3, 31]","[12, 14, 7, 7, 36]","[44, 58, 12, 8, 975]","[20, 21, 6, 2, 405]","[21, 30, 6, 6, 454]","[3, 7, 0, 0, 116]","The paper proposes an image segmentation method which iteratively refines the semantic segmentation mask obtained from a deep net. To this end the authors investigate a denoising auto-encoder (DAE). Its purpose is to provide a semantic segmentation which improves upon its input in terms of the log-likelihood.

More specifically, the authors `propose to condition the autoencoder with an additional input’ (page 1). To this end they use features obtained from the deep net. Instead of training the DAE with ground truth y, the authors found usage of the deep net prediction to yield better results.

The proposed approach is evaluated on the CamVid dataset.

Summary:
——
I think the paper discusses a very interesting topic and presents an elegant approach. A few points are missing which would provide significantly more value to a reader. Specifically, an evaluation on the classical Pascal VOC dataset, details regarding the training protocol of the baseline (which are omitted right now), an assessment regarding stability of the proposed approach (not discussed right now), and a clear focus of the paper on segmentation or conditioning. See comments below for details and other points.

Comments:
——
1. When training the DAE, a combination of squared loss and categorical cross-entropy loss is used. What’s the effect of the squared error loss and would the categorical cross-entropy on its own be sufficient? This question remains open when reading the submission.

2. The proposed approach is evaluated on the CamVid dataset which is used less compared to the standard and larger Pascal VOC dataset. I conjecture that the proposed approach wouldn’t work too well on Pascal VOC. On Pascal VOC, images are distinctly different from each other whereas subsequent frames are similar in CamVid, i.e., the road is always located at the bottom center of the image. The proposed architecture is able to take advantage of this dataset bias, but would fail to do so on Pascal VOC, which has a much more intricate bias. It would be great if the authors could check this hypothesis and report quantitative results similar to Tab. 1 and Fig. 4 for Pascal VOC.

3. The authors mention a grid-search for the stepsize and the number of iterations. What values were selected in the end on the CamVid and hopefully the Pascal VOC dataset?

4. Was the dense CRF applied out of the box, or were its parameters adjusted for good performance on the CamVid validation dataset? While parameters such as the number of iterations and epsilon are tuned for the proposed approach on the CamVid validation set, the submission doesn’t specify whether a similar procedure was performed for the CRF baseline.

5. Fig. 4 seems to indicate that the proposed approach doesn’t converge. Hence an appropriate stepsize and a reasonable number of iterations need to be chosen on a validation set. Choosing those parameters guarantees that the method performs well on average, but individual results could potentially be entirely wrong, particularly if large step sizes are chosen. I suspect this effect to be more pronounced on the Pascal VOC dataset (hence my conjecture in point 2). To further investigate this property, as a reader, I’d be curious to get to know the standard deviation/variance of the accuracy in addition to the mean IoU. Again, it would be great if the authors could check this hypothesis and report those results.

6. I find the experimental section to be slightly disconnected from the initial description. Specifically, the paper `proposes to condition the autoencoder with an additional input’ (page 1). No experiments are conducted to validate this proposal. Hence the main focus of the paper (image segmentation or DAE conditioning) remains vague. If the authors choose to focus on image segmentation, a comparison to state-of-the-art should be provided on classical datasets such as Pascal VOC, if DAE conditioning is the focus, some experiments in this direction should be included in addition to the Pascal VOC results.

Minor comment:
——
- I find it surprising that the authors choose not to cite some related work on combining deep nets with structured prediction.","[5, 4, 4]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer provides constructive criticism, acknowledges the paper's strengths (interesting topic, elegant approach), and offers specific suggestions for improvement. While they express some concerns and conjectures, the language remains professional and focused on enhancing the paper. The reviewer's request for additional experiments and comparisons indicates a desire to see the work reach its full potential, suggesting a positive outlook overall.",60.0,80.0
DEEP DENSITY NETWORKS AND UNCERTAINTY IN RECOMMENDER SYSTEMS,"['Yoel Zeldes', 'Stavros Theodorakis', 'Efrat Solodnik', 'Aviv Rotman', 'Gil Chamiel', 'Dan Friedman']",Reject,2018,"[6, 10, 2, 2, 11, 29]","[10, 14, 1, 1, 15, 34]","[4, 14, 1, 1, 4, 15]","[1, 8, 0, 0, 2, 6]","[3, 2, 1, 1, 2, 8]","[0, 4, 0, 0, 0, 1]","The paper adresses a very interesting question about the handling of the dynamics of a recommender systems at scale (here for linking to some articles).
The defended idea is to use the context to fit a mixture of Gaussian with a NN and to assume that the noise could be additively split into two terms. One depend only on the number of observations of the given context and the average reward in this situation and the second term begin the noise. This is equivalent to separate a local estimation error from the noise. 

The idea is interesting but maybe not pushed far enough in the paper:
*At fixed context x, assuming that the error is a function of the average reward u and of the number of displays r of the context could be a constant could be a little bit more supported (this is a variance explanation that could be tested statistically, or the shape of this 2D function f(u,r) could be plot to exhibit its regularity). 
* None of the experiments is done on public data which lead to an impossible to reproduce paper
* The proposed baselines are not really the state of the art (Factorization Machines, GBDT features,...) and the used loss is MSE which is strange in the context of CTR prediction (logistic loss would be a more natural choice)
* I'm not confident with the proposed surrogate metrics. In the paper, the work of Lihong Li &al on offline evaluation on contextual bandits is mentioned and considered as infeasible here because of the renewal of the set of recommendation. Actually this work can be adapted to handle theses situations (possibly requiring to bootstrap if the set is actually regenerating too fast). Also note that Yahoo Research R6a - R6b  datasets where used in ICML'12 Exploration and Exploitation 3 challenge where about pushing some news in a given context and could be reused to support the proposed approach. An other option would be to use some counterfactual estimates (See Leon Bottou &all and Thorsten Joachims &all)
* If the claim is about a better exploration,  I'd like to have an idea of the influence of the tuning parameters and possibly a discussion/comparison over alternatives strategies (including an epsilon-n greedy algorithm)

Besides theses core concerns, the papers suffers of some imprecisions on the notations which should be clarified. 
* As an example using O(1000) and O(1M) in the figure one. Everyone understands what is meant but O notation are made to eliminate constant terms and O(1) = O(1000).
* For eqn (1) it would be better to refer to and ""optimistic strategy"" rather to UCB because the name is already taken by an algorithm which is not this one. Moreover the given strategy would achieve a linear regret if used as described in the paper which is not desirable for bandits algorithms (smallest counter example with two arms following a Bernouilli with different parameters if the best arms generates two zero in a row at the beginning, it is now stuck with a zero mean and zero variance estimate). This is why bandits bounds include a term which increase with the total number of plays. I agree that in practice this effect can be mitigated at that the strategy can be correct in the contextual case (but then I'd like to the dependancies on x to be clear) 
* The papers never mentions whats is a scalar, a vector or a matrix. This creates confusion: as an example eqn (3) can have several different meaning depending if the values are scalars, scalars depending on x or having a diagonal \sigma matrix
* In the paragraph above (2) I unsure of what is a ""binomial noise error distribution"" for epsilon, but a few lines later epsilon becomes a gaussian why not just mention that you assume the presence of a gaussian noise on the parameters of a Bernouilli distribution ? 

 ","[4, 4, 3]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Clear rejection']","[5, 3, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer provides a mixed opinion, acknowledging the interesting nature of the paper's core idea but expressing several significant concerns. They find the experimental setup lacking due to the absence of public data and the choice of baselines. The reviewer also points out methodological issues and suggests alternative approaches. While the reviewer maintains a professional tone, the extensive criticism suggests a leaning towards the negative side. The language avoids harsh or disrespectful phrasing, indicating politeness.",-20.0,60.0
DNN Feature Map Compression using Learned Representation over GF(2),"['Denis A. Gudovskiy', 'Alec Hodgkinson', 'Luca Rigazio']",Reject,2018,"[6, 1, 21]","[11, 3, 25]","[19, 6, 35]","[7, 2, 23]","[10, 4, 8]","[2, 0, 4]","In order to compress DNN intermediate feature maps the authors covert fixed-point activations into vectors over the smallest finite field, the Galois field of two elements (GF(2)) and use nonlinear dimentionality reduction layers.

The paper reads well and the methods and experiments are generally described in sufficient detail.

My main concern with this paper and approach is the performance achieved. According to Table 1 and Table 2 there is a small accuracy benefit from using the proposed approach over the ""quantized"" SqueezeNet baseline. If I am weighing in the need to alter the network for the proposed approach in comparison with the ""quantized"" setting then, from practical point of view, I would prefer the later ""quantized"" approach.
","[5, 7, 4]","[' Marginally below acceptance threshold', ' Good paper, accept', ' Ok but not good enough - rejection']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer finds the paper well-written and the methodology sound. However, they express a significant concern about the practical performance improvement, leaning towards a simpler ""quantized"" approach over the proposed method due to its limited benefit. This suggests a somewhat negative sentiment, although the tone remains professional and polite.",-20.0,80.0
Recurrent Neural Networks with Top-k Gains for Session-based Recommendations,"['Balázs Hidasi', 'Alexandros Karatzoglou']",Reject,2018,"[8, 16]","[12, 21]","[28, 90]","[18, 61]","[6, 25]","[4, 4]","This paper presents a few modifications on top of some earlier work (GRU4Rec, Hidasi et al. 2016) for session-based recommendation using RNN. The first one is to include additional negative samples based on popularity raised to some power between 0 and 1. The second one is to mitigate the vanishing gradient problem for pairwise ranking loss, especially with the increased number of negative samples from the first modification. The basic idea is to weight all the negative examples by their “relevance”, since for the irrelevant negatives the gradients are vanishingly small. Experimentally these modifications prove to be effective compared with the original GRU4Rec paper. 

The writing could have been more clear, especially in terms of notations and definitions. I found myself sometimes having to infer the missing bits. For example, in Eq (4) and (5), and many that follow, the index i and j are not defined (I can infer it from the later part), as well as N_s (which I take it as the number of negative examples). This is just one example, but I hope the authors could carefully check the paper and make sure all the notations/terminologies are properly defined or referred with a citation when first introduced (e.g., pointwise, pairwise, and listwise loss functions). I consider myself very familiar with the RecSys literature, and yet sometimes I cannot follow the paper very well, not to mention the general ICLR audience. 

Regarding the two main modifications, I found the negative sampling rather trivial (and I am surprised in Hidasi et al. (2016) the negatives are only from the same batch, which seems a huge computational compromise) with many existing work on related topic: Steck (Item popularity and recommendation accuracy, 2011) used the same “popularity to the power between 0 and 1” strategy (they weighted the positive by the inverse popularity to the power). More closely, the negative sampling distribution in word2vec is in fact a unigram raised to the power of 0.75, which is the same as the proposed strategy here. As for the gradient vanishing problem for pairwise ranking loss, it has been previously observed in Rendle & Freudenthaler (Improving Pairwise Learning for Item Recommendation from Implicit Feedback, 2014) for BPR and they proposed an adaptive negative sampling strategy (trying to sample more relevant negatives while still keeping the computational cost low), which is closely related to the ranking-max loss function proposed in this paper. Overall, I don’t think this paper adds much on top of the previous work, and I think a more RecSys-oriented venue might benefit more from the insights presented in this paper.   

I also have some high-level comments regarding using RNN for session-based recommendation (this was also my initial reaction after reading Hidasi et al. 2016). As mentioned in this paper, when applying RNN on RecSys datasets with longer time-span (which means there can be more temporal dynamics in users’ preference and item popularity), the results are not striking (e.g., Wu et al. 2017) with the proposed methods barely outperforming standard matrix factorization methods. It is puzzling that how RNN can work better for session-based case where a user’s preference can hardly change within such a short period of time. I wonder how a simple matrix factorization approach would work for session-based recommendation (which is an important baseline that is missing): regarding the claim that MF is not suited for session-based because of the absence of the concept of a user, each session can simply be considered as a pseudo-user and approaches like asymmetric matrix factorization (Paterek 2007, Improving regularized singular value decomposition for collaborative filtering) can even eliminate the need for learning user factors. ItemKNN is a pretty weak baseline and I wonder if a scalable version of the SLIM (Ning & Karypis 2011, SLIM: Sparse Linear Methods for Top-N Recommender Systems) would give better results. Finally, my general experience with BPR-type of pairwise ranking loss is that it is good at optimizing AUC, but not very well-suited for head-heavy metrics (MRR, Recall, etc.) I wonder how the propose loss would perform comparing with more competitive baselines. 

Regarding the page limit, given currently the paper is quite long (12 pages excluding references), I suggest the authors cutting down some space. For example, the part about fixing the cross entropy is not very relevant and can totally be put in the appendix. 

Minor comment:

1. Section 3.3.1, “Part of the reasons lies in the rare occurrence…”, should r_j >> r_i be the other way around?
","[4, 8, 6]","[' Ok but not good enough - rejection', ' Top 50% of accepted papers, clear accept', ' Marginally above acceptance threshold']","[5, 4, 5]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review is critical of the paper's novelty and significance. The reviewer finds the modifications somewhat trivial and points out prior work that already addressed similar issues. While the reviewer acknowledges the experimental effectiveness, they express doubt about the overall impact and suggest a more specialized venue might be appropriate. The language, while direct and critical, maintains a professional and respectful tone. There's no personal attack or disrespectful phrasing.",-20.0,60.0
Graph Classification with 2D Convolutional Neural Networks,"['Antoine J.-P. Tixier', 'Giannis Nikolentzos', 'Polykarpos Meladianos', 'Michalis Vazirgiannis']",Reject,2018,"[3, 4, 4, 26]","[8, 9, 5, 31]","[30, 55, 19, 316]","[14, 28, 14, 181]","[16, 21, 5, 77]","[0, 6, 0, 58]","The paper introduces a method for learning graph representations (i.e., vector representations for graphs). An existing node embedding method is used to learn vector representations for the nodes. The node embeddings are then projected into a 2-dimensional space by PCA. The 2-dimensional space is binned using an imposed grid structure. The value for a bin is the (normalized) number of nodes falling into the corresponding region. 

The idea is simple and easily explained in a few minutes. That is an advantage. Also, the experimental results look quite promising. It seems that the methods outperforms existing methods for learning graph representations. 

The problem with the approach is that it is very ad-hoc. There are several (existing) ideas of how to combine node representations into a representation for the entire graph. For instance, averaging the node embeddings is something that has shown promising results in previous work. Since the methods is so ad-hoc (node2vec -> PCA -> discretized density map -> CNN architecure) and since a theoretical understanding of why the approach works is missing, it is especially important to compare your method more thoroughly to simpler methods. Again, pooling operations (average, max, etc.) on the learned node2vec embeddings are examples of simpler alternatives. 

The experimental results are also not explained thoroughly enough. For instance, since two runs of node2vec will give you highly varying embeddings (depending on the initialization), you will have to run node2vec several times to reduce the variance of your resulting discretized density maps. How many times did you run node2vec on each graph? 

","[4, 7, 3]","[' Ok but not good enough - rejection', ' Good paper, accept', ' Clear rejection']","[3, 3, 5]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review starts with positive remarks, highlighting the simplicity and promising results. However, it then raises concerns about the ad-hoc nature of the method and the lack of theoretical grounding. The reviewer suggests stronger baselines and questions the thoroughness of the experimental setup. Overall, the sentiment leans towards the positive side due to the initial praise and the constructive suggestions for improvement, but the criticism balances it out.",60.0,70.0
Optimizing the Latent Space of Generative Networks,"['Piotr Bojanowski', 'Armand Joulin', 'David Lopez-Paz', 'Arthur Szlam']",Reject,2018,"[6, 9, 8, 10]","[11, 14, 13, 15]","[75, 130, 60, 138]","[28, 59, 28, 54]","[43, 67, 27, 74]","[4, 4, 5, 10]","In this paper, the authors propose a new architecture for generative neural networks. Rather than the typical adversarial training procedure used to train a generator and a discriminator, the authors train a generator only. To ensure that noise vectors get mapped to images from the target distribution, the generator is trained to map noise vectors to the set of training images as closely as possible. Both the parameters of the generator and the noise vectors themselves are optimized during training. 

Overall, I think this paper is useful. The images generated by the model are not (qualitatively and in my opinion) as high quality as extremely recent work on GANs, but do appear to be better than those produced by DCGANs. More importantly than the images produced, however, is the novel training procedure. For all of their positive attributes, the adversarial training procedure for GANs is well known to be fairly difficult to deal with. As a result, the insight that if a mapping from noise vectors to training images is learned directly, other noise images still result in natural images is interesting.

However, I do have a few questions for the authors, mostly centered around the choice of noise vectors.

In the paper, you mention that you ""initialize the z by either sampling them from a Gaussian distribution or by taking the whitened PCA of the raw image pixels."" What does this mean? Do you sample them from a Gaussian on some tasks, and use PCA on others? Is it fair to assume from this that the initialization of z during training matters? If so, why?

After training, you mention that you fit a full Gaussian to the noise vectors learned during training and sample from this to generate new images. I would be interested in seeing some study of the noise vectors learned during training. Are they multimodal, or is a unimodal distribution indeed sufficient? Does a Gaussian do a good job (in terms of likelihood) of fitting the noise vectors, or would some other model (even something like kernel density estimation) allow for higher probability noise vectors (and therefore potentially higher quality images) to be drawn? Does the choice of distribution even matter, or do you think uniform random vectors from the space would produce acceptable images?","[6, 4, 6]","[' Marginally above acceptance threshold', ' Ok but not good enough - rejection', ' Marginally above acceptance threshold']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer finds the paper useful and interesting, highlighting the novelty of the training procedure. They find the results to be promising, even if not state-of-the-art. The reviewer poses several questions, but these are framed as avenues for further exploration and improvement rather than criticisms.  Therefore, the sentiment is positive. The language used is polite and professional throughout, with no signs of harsh criticism or negativity.",60.0,80.0
Automatic Goal Generation for Reinforcement Learning Agents,"['David Held', 'Xinyang Geng', 'Carlos Florensa', 'Pieter Abbeel']",Reject,2018,"[2, 15, 3, 17]","[5, 20, 8, 22]","[18, 116, 20, 608]","[8, 49, 6, 291]","[9, 60, 13, 291]","[1, 7, 1, 26]","This paper proposed a method for automatic curriculum generation that allow an agent to learn to reach multiple goals in an environment with considerable sample efficiency. They use a generator network to propose tasks for the agent accomplish. The generator network is trained with GAN.  In addition, the proposed method is also shown to be able to solve tasks with sparse rewards without the need manually modify reward functions. They compare the Goal GAN method with four baselines, including Uniform sampling, Asymmetric Self-play, SAGG-RIAC, and Rejection sampling. The proposed method is tested on two environments: Free Ant and Maze Ant. The empirical study shows that the proposed method is able to improve policies’ training efficiency comparing to these baselines. The technical contributions seem sound, however I find it is slightly difficult to fully digest the whole paper without getting the insight from each individual piece and there are some important details missing, as I will elaborate more below.

1. it is unclear to me why the proposed method is able to solve tasks with sparse rewards? Is it because of the horizons of the problems considered are not long enough? The author should provide more insight for this contribution.

2. It is unclear to me how R_min and R_max as hyperparameters are obtained and how their settings affect the performance.

3. Another concern I have is regarding the generalizability of the proposed method. One of the assumption is “A policy trained on a sufficient number of goals in some area of the goal-space will learn to interpolate to other goals within that area”. This seems to mean that the area is convex. It might be better if some quantitative analysis can be provided to illustrate geometry of goal space (given complex motor coordination) that is feasible for the proposed method.

4. It is difficult to understand the plots in Figure 4 without more details. Do you assume for every episode, the agent starts from the same state? 

5. For the plots in Figure 2, is there any explanation for the large variance for Goal GAN? Given that the state space is continuous, 10 runs seems not enough.

6. According to the experimental details, three rollouts are performed to estimate the empirical return. It there any justification why three rollouts are enough?

7. Minor comments
Achieve tasks -> achieve goals or accomplish/solve tasks
A variation of to -> variation of 
Allows a policy to quickly learn to reach …-> allow an agent to be quickly learn a policy to reach…
…the difficulty of the generated goals -> … the difficulty of reaching
","[6, 4, 8]","[' Marginally above acceptance threshold', ' Ok but not good enough - rejection', ' Top 50% of accepted papers, clear accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer provides constructive criticism, acknowledges the technical contributions, and offers specific recommendations for improvement. They point out areas that need clarification and suggest additional experiments or analyses. The language is formal and objective, focusing on the scientific content and clarity of the paper.",50.0,75.0
Training RNNs as Fast as CNNs,"['Tao Lei', 'Yu Zhang', 'Yoav Artzi']",Reject,2018,"[9, 6, 8]","[14, 11, 13]","[179, 174, 85]","[68, 90, 42]","[15, 81, 39]","[96, 3, 4]","The authors propose to drop the recurrent state-to-gates connections from RNNs to speed up the model. The recurrent connections however are core to an RNN. Without them, the RNN defaults simply to a CNN with gated incremental pooling. This results in a somewhat unfortunate naming (simple *recurrent* unit), but most importantly makes a comparison with autoregressive sequence CNNs [ Bytenet (Kalchbrenner et al 2016), Conv Seq2Seq (Dauphin et al, 2017) ] crucial in order to show that gated incremental pooling is beneficial over a simple CNN architecture baseline. 

In essence, the paper shows that autoregressive CNNs with gated incremental pooling perform comparably to RNNs on a number of tasks while being faster to compute. Since it is already extensively known that autoregressive CNNs and attentional models can achieve this, the *CNN* part of the paper cannot be counted as a novel contribution. What is left is the gated incremental pooling operation; but to show that this operation is beneficial when added to autoregressive CNNs, a thorough comparison with an autoregressive CNN baseline is necessary.

Pros:
- Fairly well presented
- Wide range of experiments, despite underwhelming absolute results

Cons:
- Quasi-RNNs are almost identical and already have results on small-scale tasks.
- Slightly unfortunate naming that does not account for autoregressive CNNs
- Lack of comparison with autoregressive CNN baselines, which signals a major conceptual error in the paper.
- I would suggest to focus on a small set of tasks and show that the model achieves very good or SOTA performance on them, instead of focussing on many tasks with just relative improvements over the RNN baseline.

I recommend showing exhaustively and experimentally that gated incremental pooling can be helpful for autoregressive CNNs on sequence tasks (MT, LM and ASR). I will adjust my score accordingly if the experiments are presented.

","[4, 8, 7]","[' Ok but not good enough - rejection', ' Top 50% of accepted papers, clear accept', ' Good paper, accept']","[5, 5, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is quite critical of the paper's core idea, stating that the proposed model is basically a known concept (autoregressive CNN) with a different name. While acknowledging some positives like clear presentation and a wide range of experiments, the reviewer heavily emphasizes the lack of comparison with a proper baseline, which they deem a ""major conceptual error."" The language, while direct and critical, maintains a professional and analytical tone without resorting to personal attacks or disrespectful language.",-40.0,60.0
Neural Networks with Block Diagonal Inner Product Layers,"['Amy Nesky', 'Quentin Stout']",Reject,2018,"[1, 39]","[4, 43]","[4, 84]","[2, 38]","[1, 8]","[1, 38]","This is a mostly experimental paper which evaluates the capabilities of neural networks with weight matrices that are block diagonal. The authors describe two methods to obtain this structure: (1) enforced during training, (2) enforced through regularization and pruning. As a second contribution, the authors show experimentally that the random matrix theory can provide a good model of the spectral behavior of the weight matrix when it is large. However, the authors only conjecture as to the potential of this method without describing clear ways of approaching this subject, which somewhat lessens the strength of their argument.

Quality: this paper is of good quality
Clarity: this paper is clear, but would benefit from better figures and from tables to report the numerical results instead of inserting them into plain text.
Originality: this paper introduces block diagonal matrices to structure the weights of a neural network. The idea of structured matrices in this context is not new, but the diagonal block structure appears to be.  
Significance: This paper is somewhat significant.

PROS 
- A new approach to analyzing the behavior of weight matrices during learning
- A new structure for weight matrices that provides good performance while reducing matrix storage requirements and speeding up forward and backward passes.

CONS
- Some of the figures are hard to read (in particular Fig 1 & 2 left) and would benefit from a better layout.
- It would be valuable to see experiments on bigger datasets than only MNIST and CIFAR-10. 
- I understand that the main advantage of this method is the speedup; however, providing the final accuracy as a function of the nonzero entries for slower methods (e.g. the sparse pruning showed in Fig 1. a) would provide a more complete picture.

Main questions:
- Could you briefly comment on the training time in section 4.1? 
- Could you elaborate on the last sentence of section 4.1?
- You state: ""singular values of an IP layer behave according to the MP distribution even after 1000s of training iterations."" Is this a known fact, or something that you observed empirically? In practice, how large must the weight matrix be to observe this behavior?

Nitpicks:
- I believe the term ""fully connected"" is more standard than ""inner product"" and would add clarity to the paper, but I may be mistaken. ","[5, 6, 4]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Ok but not good enough - rejection']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review acknowledges the paper's contributions ('new approach', 'new structure') and calls it 'good quality' and 'clear'. It also lists more pros than cons. While it does point out areas for improvement, the language remains constructive and polite throughout, suggesting a positive overall sentiment. Therefore, the sentiment score leans positive, and the politeness score is also high.",65.0,80.0
AUTOMATA GUIDED HIERARCHICAL REINFORCEMENT LEARNING FOR ZERO-SHOT SKILL COMPOSITION,"['Xiao Li', 'Yao Ma', 'Calin Belta']",Reject,2018,"[3, 11, 18]","[5, 16, 23]","[11, 96, 341]","[3, 37, 174]","[6, 18, 103]","[2, 41, 64]","This paper proposes to join temporal logic with hierarchical reinforcement learning to simplify skill composition.  The combination of temporal logic formulas with reinforcement learning was developed previously in the literature, and the main contribution of this paper is for fast skill composition.  The system uses logic formulas in truncated linear temporal logic (TLTL), which lacks an Always operator and where the LTL formula (A until B) also means that B must eventually hold true. The temporal truncation also requires the use of a specialized MDP formulation with an explicit and fixed time horizon T.  The exact relationship between the logical formulas and the stochastic trajectories of the MDP is not described in detail here, but relies on a robustness metric, rho.  The main contributions of the paper are to provide a method that converts a TLTL formula that specifies a task into a reward function for a new augmented MDP (that can be used by a conventional RL algorithm to yield a policy), and a method for quickly combining two such formulas (and their policies) into a new policy.  The proposed method is evaluated on a small Markov chain and a simulated Baxter robot.

The main problem with this paper is that the connections between the TLTL formulas and the conventional RL objectives are not made sufficiently clear.  The robustness term rho is essential, but it is not defined.  I was also confused by the notation $D_\phi^q$, which was described but not defined.  The method for quickly combining known skills (the zero-shot skill composition in the title) is switching between the two policies based on rho.  The fact that there may be many policies which satisfy a particular reward function (or TLTL formula) is ignored.  This means that skill composition that is proposed in this paper might be quite far from the best policy that could be learned directly from a single conjunctive TLTL formula. It is unclear how this approach manages tradeoffs between objectives that are specified as a conjunction of TLTL goals. is it better to have a small probability of fulfilling all goals, or to prefer a high probability of fulfilling half the goals?  In short the learning objectives of the proposed composition algorithm are unclear after translation from TLTL formulas to rewards.
","[4, 3, 5]","[' Ok but not good enough - rejection', ' Clear rejection', ' Marginally below acceptance threshold']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review acknowledges the paper's goal and contribution but raises several concerns about clarity, methodology, and potential limitations. The reviewer points out the lack of clarity regarding key concepts like the robustness term 'rho' and the notation '$D_\phi^q$'. They also criticize the paper for not adequately addressing the potential for suboptimal policy composition and the handling of trade-offs between multiple objectives. The language used, while critical, maintains a professional and academic tone, avoiding personal attacks or disrespectful remarks.",-25.0,50.0
Real-valued (Medical) Time Series Generation with Recurrent Conditional GANs,"['Stephanie Hyland', 'Cristóbal Esteban', 'Gunnar Rätsch']",Reject,2018,"[4, 4, 22]","[5, 9, 27]","[11, 28, 179]","[5, 10, 82]","[5, 18, 41]","[1, 0, 56]","In this paper, the authors propose a recurrent GAN architecture that generates continuous domain sequences. To accomplish this, they use a generator LSTM that takes in a sequence of random noise as well as a sequence of conditonal information and outputs a sequence. The discriminator LSTM takes a sequence (and conditional information) as input and classifies each element of the sequence as real or synthetic -- the entire sequence is then classified by vote. The authors evaluate on several synthetic tasks, as well as an ICU timeseries data task.

Overall, I thought the paper was clearly written and extremely easy to follow. To the best of my knowledge, the method proposed by the authors is novel, and differs from traditional sentence generation (as an example) models because it is intended to produce continuous domain outputs. Furthermore, the story of generating medical training data for public release is an interesting use case for a model like this, particularly since training on synthetic data appears to achieve not competitive but quite reasonable accuracy, even when the model is trained in a differentially private fashion.

My most important piece of feedback is that I think it would be useful to include a few examples of the eICU time series data, both real and synthetic. This might give a better sense of: (1) how difficult the task is, (2) how much variation there is in the real data from patient to patient, and (3) how much variation we see in the synthetic time series. Are the synthetic time series clearly multimodal, or do they display some of the mode collapse behavior occasionally seen in GANs?

I would additionally like to see a few examples of the time series data at both the 5 minute granularity and the 15 minute granularity. You claim that downsampling the data to 15 minute time steps still captures the relevant dynamics of the data -- is it obvious from the data that variations in the measured variables are not significant over a 5 minute interval? As it stands, this is somewhat an unknown, and should be easy enough to demonstrate.","[6, 4, 5]","[' Marginally above acceptance threshold', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer provides positive feedback, praising the paper's clarity, novelty, and interesting use case. They find the results promising, especially the performance of the model trained with differential privacy. While they request additional information and examples to further assess the task's difficulty, data variation, and potential mode collapse, these are constructive suggestions rather than harsh criticisms. The language used is polite and professional throughout.",75.0,90.0
Discrete Autoencoders for Sequence Models,"['Lukasz Kaiser', 'Samy Bengio']",Reject,2018,"[14, 32]","[18, 37]","[84, 261]","[44, 155]","[32, 60]","[8, 46]","This is an interesting paper focusing on building discrete reprentations of sequence by autoencoder. 
However, the experiments are too weak to demonstrate the effectiveness of using discrete representations.
The design of the experiments on language model is problematic.
There are a few interesting points about discretizing the represenations by saturating sigmoid and gumbel-softmax, but the lack of comparisons to benchmarks is a critical defect of this paper. 


Generally, continuous vector representations are more powerful than discrete ones, but discreteness corresponds to some inductive biases that might help the learning of deep neural networks, which is the appealing part of discrete representations, especially the stochastic discrete representations. 
However, I didn't see the intuitions behind the model that would result in its superiority to the continuous counterpart. 
The proposal of DSAE might help evaluate the usage of the 'autoencoding function' c(s), but it is certainly not enough to convince people. 
How is the performance if c(s) is replaced with the representations achieved from autoencoder, variational autoencoder or simply the sentence vectors produced by language model?
The qualitative evaluation on 'Deciperhing the Latent Code' is not enough either. 
In addition, the language model part doesn't sound correct, because the model cheated on seeing the further before predicting the words autoregressively.
One suggestion is to change the framework to variational auto-encoder, otherwise anything related to perplexity is not correct in this case.

Overall, this paper is more suitable for the workshop track. It also needs a lot of more studies on related work.","[4, 6, 5]","[' Ok but not good enough - rejection', ' Marginally above acceptance threshold', ' Marginally below acceptance threshold']","[4, 1, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', "" The reviewer's evaluation is an educated guess"", ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review starts with a mildly positive note by calling the paper 'interesting' but quickly transitions into a series of negative points. The reviewer finds the experiments weak, the design problematic, and the lack of comparisons a critical flaw. While acknowledging the appeal of discrete representations, the reviewer doesn't see the model's superiority and criticizes the evaluation methods. The suggestion to relegate the paper to a workshop track further underscores the negative sentiment. The language, while direct and critical, maintains a professional and academic tone.",-60.0,40.0
Improving Conditional Sequence Generative Adversarial Networks by Stepwise Evaluation,"['Yi-Lin Tuan', 'Hung-yi Lee']",Reject,2018,"[1, 10]","[5, 15]","[29, 383]","[11, 166]","[17, 195]","[1, 22]","Quality: The paper proposes a direct improvement over SeqGAN by Yu. et. al. (2017). My assessment is partially determined by comparing this paper to Yu et. al (2017). In my opinion, this paper is lacking in quality in comparison to Yu et. al (2017). In particular, Yu et. al. (2017) provides detailed derivation of the policy gradient accompanied by a pseudo-code (algorithm) on how one can implement SeqGAN. On the contrary, this paper does not provide such details. Perhaps, all of the details of SeqGAN follows immediately, but the paper should not assume that all readers will be familiar with SeqGAN. 

Clarity: 

1. The paper provides a review of related methods on conditional sequence generation in Section 3. However, it is very brief and as a non-expert in this field, I needed to refer to the original papers anyways. Perhaps, the review of the related methods can go to the Appendix and this space can be better utilized to expand on the original contributions made by the paper. 
2. MCMC (Markov chain Monte Carlo) is mentioned in 4.1 but it is not explained. 
3. Figure 1 is not sufficiently explained; neither in text nor in the figure caption. It would help greatly to describe the details of the network architecture shown in this figure.

Originality: The paper proposes a generalization of SeqGAN; however, in my opinion, the methodological contribution appears to be only incremental on SeqGAN. 

Significance: The paper's significance may be evaluated in terms of its impact on applications as it proposes an improvement over the previous work of SeqGAN. However, the extent to which the evaluation is carried out is somewhat unsatisfactory with only one real application. Also, the applications considered in the experiments are primarily on dialogue generation. My initial impression is that the methodology lacks generality and may perhaps cater better to domain specific publication venues. 
","[4, 5, 5]","[' Ok but not good enough - rejection', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[3, 4, 2]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']","The reviewer expresses multiple concerns about the paper, stating its lack of clarity, originality, and significant contribution compared to previous work. They find the paper lacking details and generalizability. While the reviewer doesn't use harsh language, the overall tone is critical and leans towards the negative side.",-50.0,50.0
Maintaining cooperation in complex social dilemmas using deep reinforcement learning,"['Alexander Peysakhovich', 'Adam Lerer']",Reject,2018,"[10, 5]","[14, 9]","[55, 49]","[26, 21]","[27, 23]","[2, 5]","This paper addresses multiagent learning problems in which there is a social dilemma: settings where there are no 'cooperative polices' that form an equilibrium. The paper proposes a way of dealing with these problems via amTFT, a variation of the well-known tit-for-that strategy, and presents some empirical results.

My main problem with this paper is clarity and I am afraid that not everything might be technically correct. Let me just list my main concerns in the below.

The definition of social dilemma, is unclear:
""A social dilemma is a game where there are no cooperative policies which form equilibria. In other
words, if one player commits to play a cooperative policy at every state, there is a way for their
partner to exploit them and earn higher rewards at their expense.""
does this mean to say ""there are no cooperative *Markov* policies"" ? It seems to me that the paper precisely intents to show that by resorting to history-dependent policies (such as both using amTFT), there is a cooperative equilibrium. 

I don't understand:
""Note that in a social dilemma there may be policies which achieve the payoffs of cooperative policies because they cooperate on the trajectory of play and prevent exploitation by threatening non-cooperation on states which are never reached by the trajectory. If such policies exist, we call the social dilemma solvable.""
is this now talking about non-Markov policies? If not, there seems to be a contradiction?

The work focuses on TFT-like policies, motivated by 
""if one can commit to them, create incentives for a partner to behave cooperatively""
however it seems that, as made clear below definition 4, we can only create such incentives for sufficiently powerful agents, that remember and learn from their failures to cooperate in the past?

Why is the method called ""approximate Markov""? As soon as one introduces history dependence, the Markov property stops to hold?

On page 4, I have problems following the text due to inconsistent use of notation: subscripts and superscripts seem random, it is not clear which symbols denote strategy profiles (rather than individual strategies), there seems mix-ups between 'i' and '1' / '2', there is sudden use of \hat{}, and other undefined symbols (Q_CC?).

For all practical purposes, it seems that the made assumptions imply uniqueness of the cooperative joint strategy. I fully appreciate that the coordination question is difficult and important, so if the proposed method is not compatible with dealing with that important question, that strikes me as a large drawback.

I have problems understanding how it is possible to guarantee ""If they start in a D phase, they eventually return to a C phase."" without making more assumptions on the domain. The clear example being the typical 'heaven or hell' type of problems: what if after one defect, we are trapped in the 'hell' state where no cooperation is even possible? 

""If policies converge with this training then πˆ is a Markov equilibrium (up to function approximation)."" There are two problems here:
1) A problem is that very typically things will not converge... E.g., 
Wunder, Michael, Michael L. Littman, and Monica Babes. ""Classes of multiagent q-learning dynamics with epsilon-greedy exploration."" Proceedings of the 27th International Conference on Machine Learning (ICML-10). 2010.
2) ""Up to function approximation"" could be arbitrary large?


Another significant problem seems to be with this statement:
""while in the cooperative reward schedule the standard RL convergence guarantees apply. The latter is because cooperative training is equivalent to one super-agent controlling both players and trying to optimize for a single scalar reward."" The training of individual learners is quite different from ""joint action learners"" [Claus & Boutilier 98], and this in turn is different from a 'super-agent' which would also control the exploration. In absence of the super-agent, I believe that the only guarantee is that one will, in the limit, converge to a Nash equilibrum, which might be arbitrary far from the optimal joint policy. And this only holds for the tabular case. See the discussion in 
A concise introduction to multiagent systems and distributed artificial intelligence. N Vlassis. Synthesis Lectures on Artificial Intelligence and Machine Learning 1 (1), 1-71

Also, the approach used in the experiments ""Cooperative (self play with both agents receiving sum of rewards) training for both games"", would be insufficient for many settings where a cooperative joint policy would be asymmetric.

The entire approach hinges on using rollouts (the commented lines in Algo. 1). However, it is completely not clear to me how this works. The one paragraph is insufficient to get across these crucial parts of the proposed approach.

It is not clear why the tables in Figure 1 are not symmetric; this strikes me as extremely problematic. It is not clear what the colors encode either.

It also seems that ""grim"" is better against all, except against amTFT, why should we not use that? In general, the explanation of this closely related paper by De Cote & Littman (which was published at UAI'08), is insufficient. It is not quite clear to me what the proposed approach offers over the previous method.









","[4, 3, 4]","[' Ok but not good enough - rejection', ' Clear rejection', ' Ok but not good enough - rejection']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review expresses significant concerns about the paper's clarity, technical correctness, and novelty. The reviewer lists multiple issues, including unclear definitions, potential contradictions, and concerns about the experimental setup and results. The language used is critical and points out flaws directly, but it maintains a professional and academic tone. ",-50.0,50.0
Learning to Encode Text as Human-Readable Summaries using Generative Adversarial Networks,"['Yau-Shian Wang', 'Hung-Yi Lee']",Reject,2018,"[2, 10]","[7, 15]","[18, 383]","[6, 166]","[12, 195]","[0, 22]","This paper proposes a model for generating long text strings given shorter text strings, and for inferring suitable short text strings given longer strings. Intuitively, the inference step acts as a sort of abstractive summarization. The general gist of this paper is to take the idea from ""Language as a Latent Variable"" by Miao et al., and then change it from a VAE to an adversarial autoencoder. The authors should cite ""Adversarial Autoencoders"" by Makzhani et al. (ICLR 2016).

The experiment details are a bit murky, and seem to involve many ad-hoc decisions regarding preprocessing and dataset management. The vocabulary is surprisingly small. The reconstruction cost is not precisely explained, though I assume it's a teacher-forced conditional log-likelihood (conditioned on the ""summary"" sequence). The description of baselines for REINFORCE is a bit strange -- e.g., annealing a constant in the baseline may affect variance of the gradient estimator, but the estimator is still unbiased and shouldn't significantly impact exploration. Similar issues are present in the ""Self-critical..."" paper by Rennie et al. though, so this point isn't a big deal.

The results look decent, but I would be more impressed if the authors could show some benefit relative to the supervised model, e.g. in a reasonable semisupervised setting. Overall, the paper covers an interesting topic but could use extra editing to clarify details of the model and training procedure, and could use some redesign of the experiments to minimize the number of arbitrary (or arbitrary-seeming) decisions.","[5, 6, 4]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Ok but not good enough - rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with a neutral summary of the paper's contributions. While it points out some positive aspects (decent results, interesting topic), it also highlights several shortcomings like murky experiment details, ad-hoc decisions, and a lack of comparison with supervised models. The reviewer suggests improvements but doesn't outright reject the paper. Overall, the tone is critical but constructive, leaning slightly more towards the negative side.",-20.0,60.0
Learnability of Learned Neural Networks,"['Rahul Anand Sharma', 'Navin Goyal', 'Monojit Choudhury', 'Praneeth Netrapalli']",Reject,2018,"[14, 6, 34, 3, 14]","[19, 11, 39, 6, 18]","[53, 25, 332, 4, 16]","[20, 8, 150, 1, 6]","[5, 4, 8, 1, 2]","[28, 13, 174, 2, 8]","Summary:
This paper presents very nice experiments comparing the complexity of various different neural networks using the notion of ""learnability"" --- the learnability of a model (N1) is defined as the ""expected agreement"" between the output of N1, and the output of another model N2 which has been trained to match N1 (on a dataset of size n).  The paper suggests that the learnability of a model is a good measure of how simple the function learned by that model is --- furthermore, it shows that this notion of learnability correlates well (across extensive experiments) with the test accuracy of the model.

The paper presents a number of interesting results:
1) Larger networks are typically more learnable than smaller ones (typically we think of larger networks as being MORE complicated than smaller networks -- this result suggests that in an important sense, large networks are simpler).
2) Networks trained with random data are significantly less learnable than networks trained on real data.
3) Networks trained on small mini-batches (larger variance SGD updates) are more learnable than those trained on large minibatches.

These results are in line with several of the observations made by Zhang et al (2017), which showed that neural networks are able to both (a) fit random data, and (b) generalize well; these results at first seem to run counter to the ideas from statistical learning theory that models with high capacity (VC dimension, radamacher complexity, etc.) have much weaker generalization guarantees than lower capacity models.  These results suggest that models that have high capacity (by one definition) are also capable of being simple (by another definition).  These results nicely complement the work which studies the ""sharpness/curvature"" of the local minima found by neural networks, which argue that the minima which generalize better are those with lower curvature.

Review:
Quality:  I found this to be high quality work. The paper presents many results across a variety of network architectures.  One area for improvement is presenting results on larger datasets (currently all experiments are on CIFAR-10), and/or on non-convolutional architectures.  Additionally, a discussion of why learnabiblity might imply low generalization error would have been interesting (the more formal, the better), though it is unclear how difficult this would be.

Clarity:  The paper is written clearly.  A small point: Step 2 in section 3.1 should specify that argmax of N1(D2) is used to generate labels for the training of the second network.  Also, what dataset D_i is used for tables 3-6? Please specify.

Originality: The specific questions tackled in this paper are original (learnability on random vs. real data, large vs. small networks, and large vs. small mini-batch training).  But it is unclear to me exactly how original this use of ""learnability"" is in evaluating how simple a model is.  It seems to me that this particular use of ""learnability"" is original, even though PAC learnability was defined a while ago.

Significance:  I find the results in this paper to be quite significant, and to provide a new way of understanding why deep neural networks generalize.  I believe it is important to find new ways of formally defining the ""simplicity/capacity"" of a model, such that ""simpler"" models can be proven to have smaller generalization gap (between train and test error) relative to more ""complicated"" models. It is clear that VC dimension and radamacher complexity alone are not enough to explain the generalization performance of neural networks, and that neural networks with high capacity by these definitions are likely ""simple"" by other definitions (as we have seen in this paper).  This paper makes an important contribution to this conversation, and could perhaps provide a starting point for theoreticians to better explain why deep networks generalize well.

Pros
- nice experiments, with very interesting results.
- Helps explain one way in which large networks are in fact ""simple""

Cons
- The paper does not attempt to relate the notion of learnability to that of generalization performance.  All it says is that these two metrics appear to be well correlated.","[7, 6, 4]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Ok but not good enough - rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer explicitly states that they found the work to be ""high quality"" and the results ""quite significant."" They believe the paper makes an ""important contribution"" to understanding deep learning. The reviewer also uses positive language throughout, such as ""nice experiments,"" ""interesting results,"" and ""helps explain.""  There are no overtly negative statements or harsh criticisms. Therefore, the sentiment is highly positive, and the language is polite and constructive.",85.0,90.0
One-shot and few-shot learning of word embeddings,"['Andrew Kyle Lampinen', 'James Lloyd McClelland']",Reject,2018,"[2, 34]","[7, 38]","[36, 74]","[12, 35]","[23, 15]","[1, 24]","I am highly sympathetic to the goals of this paper, and the authors do a good job of contrasting human learning with current deep learning systems, arguing that the lack of a mechanism for few-shot learning in such systems is a barrier to applying them in realistic scenarios. However, the main evaluation only considers four words - ""bonuses"", ""explained"", ""marketers"", ""strategist"" - with no explanation of how these words were chosen. Can I really draw any meaningful conclusions from such an experimental setup? Even the authors acknowledge, in footnote 1, that, for one of the tests, getting lower perplexity in three out of the four casess ""may just be chance variation, of course"". I wonder why we can't arrive at a similar conclusion for the other results in the paper. At the very least I need convincing that this is a reasonable experimental paradigm.

I don't understand the first method for initializing the word embeddings. How can we use the ""current"" embedding for a word if it's never been seen before? What does ""current"" mean in this context?

I also didn't understand the Latin square setup. Training on ten different permutations of the ten sentences suggests that all ten sentences are being used, so I don't see how this can lead to a few-shot or one-shot scenario.

","[4, 4, 3]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Clear rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with positive aspects by acknowledging the paper's goals and the authors' efforts in contrasting human and deep learning. However, it quickly transitions into a highly critical tone, questioning the validity of the core experiment and expressing strong doubts about the results. The reviewer finds the experimental setup with only four words unconvincing and even suggests the results might be due to chance. While the language avoids direct personal attacks, the repeated use of strong phrasing like ""Can I really draw any meaningful conclusions...?"" or ""I wonder why we can't arrive at a similar conclusion..."" indicates a significant level of skepticism bordering on negativity. The reviewer also points out a lack of understanding regarding the methodology, further contributing to the overall critical sentiment. 

While the reviewer expresses concerns and doubts, the language remains professional and avoids informal or disrespectful language. The critique focuses on the methodology and results, not on the authors themselves. The reviewer also asks for clarification, suggesting a willingness to engage constructively.",-50.0,50.0
Unseen Class Discovery in Open-world Classification,"['Lei Shu', 'Hu Xu', 'Bing Liu']",Reject,2018,"[6, 8, 31]","[11, 13, 36]","[53, 67, 350]","[24, 29, 235]","[29, 38, 67]","[0, 0, 48]","This paper concerns open-world classification.  The open-world related tasks have been defined in many previous works. This paper had made a good survey. 
The only special point of the open-word classification task defined in this paper is to employ the constraints from the similarity/difference expected for examples from the same class or from different classes.  Unfortunately, this paper is lack of novelty. 

Firstly, the problem context and setting is kinda synthesized. I cannot quite imagine in what kind of applications we can get “a set of pairs of intra-class (same class) examples, and the negative training data consists of a set of pairs of inter-class”.

Secondly, this model is just a direct combination of the recent powerful algorithms such as DOC and other simple traditional models. I do not really see enough novelty here.

Thirdly, the experiments are only on the MNIST and EMNIST; still not quite sure any real-world problems/datasets can be used to validate this approach.
I also cannot see the promising performance. The clustering results of rejected
examples are still far from the ground truth, and comparing the result with
a total unsupervised K-means is a kind of unreasonable.
","[5, 4, 5]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with a mildly positive note, acknowledging the good survey on open-world classification. However, it quickly turns negative, using terms like ""Unfortunately, this paper is lack of novelty,"" ""kinda synthesized,"" ""I do not really see enough novelty here,"" ""still not quite sure,"" ""I also cannot see,"" and ""a kind of unreasonable."" These phrases indicate a negative sentiment towards the paper. The language used is quite direct and critical, suggesting a lack of politeness. ",-60.0,-30.0
Deep Asymmetric Multi-task Feature Learning,"['Hae Beom Lee', 'Eunho Yang', 'Sung Ju Hwang']",Reject,2018,"[2, 13, 9]","[6, 18, 14]","[26, 122, 207]","[11, 68, 95]","[15, 49, 108]","[0, 5, 4]","Summary: The paper proposes a multi-task feature learning framework with a focus on avoiding negative transfer. The objective has two kinds of terms to minimise: (1) The reweighed per-task loss, and (2) Regularisation. The new contribution is an asymmetric reconstruction error in the regularisation term, and one parameter matrix in the regulariser influences the reweighing of the pre-task loss. 

Strength: 
The method has some contribution in dealing with negative transfer. The experimental results are positive.
Weakness:
Several issues in terms of concept, methodology, experiments and analysis.

Details:
1. Overall conceptual issues.
1.1. Unclear motivation re prior work. The proposed approach is motivated by the claim that GO-MTL style models assumes symmetric transfer where bad tasks can hurt good tasks. This assertion seems flawed. The point of grouping/overlap in “GO”-MTL is that a “noisy”, “hard”, or “unrelated"" task can just take its own latent predictor that is disjoint from the pool of predictors shared by the good/related tasks. 
Correspondingly, Fig 2 seems over-contrived. A good GO-MTL solution would assign the noisy task $w_3$ its own latent basis, and let the two good tasks share the other two latent bases. 

1.2  Very unclear intuition of the algorithm. In the AMTFL, task asymmetry is driven by the per-task loss. The paper claims this is because transfer must go from easy=>hard to avoid negative transfer. But this logic relies on several questionable assumptions surrounding conflating the distinct issues of difficulty and relatedness: (i) There could be several easy tasks that are totally un-related. One could construct synthetic examples with data that are trivially separable (easy) but require unrelated or orthogonal classifiers. (ii) A task could appear to be “easy"" just by severe overfitting, and therefore still be detrimental to transfer despite low loss. (iii) A task could be very ""difficult"" in the sense of high loss, but it could still be perfectly learned in the sense of finding the ideal ""ground-truth” classifier, but for a dataset that is highly non-separable in the provided feature-space. Such a perfectly learned classifier may still be useful to transfer despite high loss. (iv) Analogous to point (i), there could be several “difficult” tasks that are indeed related and should share knowledge. (Since difficult/high loss != badly learned as mentioned before). Overall there are lots of holes in the intuitive justification of the algorithm.

2. Somewhat incremental method. 
3.1 It’s a combination of AMTL (Lee 2016) and vanilla auto encoder. 

3. Methodology issues: 
3.1 Most of the explanation (Sec 3-3.1) is given re: Matrix B in Eq.(4) (AMTL method’s objective function). However the final proposed model uses matrix A in Eq.(6) for the same purpose of measuring the amount of outgoing transfers from task $t$ to all other tasks. However in the reconstruction loss, they work in very different ways: matrix B is for the reconstruction of model parameters, while matrix A is for the reconstruction of latent features. This is a big change of paradigm without adequate explanation. Why is it still a valid approach?
3.2 Matrix B in the original paper of AMTL (Eq.(1) of Lee et al., 2016) has a constraint $B \geq 0$, should matrix A have the same constraint? If not, why?
3.3 Question Re: the |W-WB| type assumption for task relatedness. A bad task could learn an all-zero vector of outgoing related ness $b^0_t$ so it doesn’t directly influence other tasks in feed-forward sense. But hat about during training? Does training one task’s weights endup influencing other tasks’s weights via backprop? If a bad task is defined in terms of incoming relatedness from good tasks, then tuning the bad task with backprop will eventually also update the good tasks? (presumably detrimentally).

4. Experimental Results not very strong.
4.1 Tab 1: Neural Network NN and MT-NN beat the conventional shallow MTL approaches decisively for AWA and MNIST.  The difference between MT-NN and AMTFL is not significant. The performance boost is more likely due to using NNs rather than the proposed MTL module. For School, there is not significant difference between the methods. For ImageNet-Room AMTL and AMTFL have overlapping errors. Also, a variant of AMTL (AMTL-imbalance) was reported in Lee’2016, but not here where the number is $40\pm1.71$. 
4.2 Tab 2: The “real” experiments are missing state of the art competitors. Besides a deep GO-MTL alternative, which should be a minimum,  there are lots of deep MTL state of the art: Misra CVPR’16 , Yang ICLR’17, Long arXiv/NIPS’17 Multilinear Relationship Nets,  Ruder arXiv’17 Sluice Nets, etc.

5. Analysis
5.1 The proposed method revolves around the notion of “noisy”/“unrelated”/“difficult” tasks. Although the paper conflates them, it may still be a useful algorithm in practice. But it in this case it should devise much better analysis to provide insight and convince us that this is not a fatal oversimplification: What is the discovered relatedness matrix in some benchmarks? Does the discovered relatedness reflect expert knowledge where this is available? Is there a statistically significant correlation between relatedness and task difficulty in practice? Or between relatedness and degree of benefit from transfer, etc? But this is hard to do cleanly as even if the results show a correlation between difficulty and relatedness, it may just be because that’s how relatedness is defined in the proposed algorithm.
","[3, 5, 6]","[' Clear rejection', ' Marginally below acceptance threshold', ' Marginally above acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is critical of the paper, pointing out several weaknesses in the concept, methodology, experiments, and analysis. While acknowledging some strengths like the contribution to negative transfer and positive experimental results, the reviewer raises significant concerns. The language used, while direct and critical, maintains a professional and academic tone.",-50.0,50.0
Learning to Infer Graphics Programs from Hand-Drawn Images,"['Kevin Ellis', 'Daniel Ritchie', 'Armando Solar-Lezama', 'Joshua B. Tenenbaum']",Reject,2018,"[5, 10, 14, 25]","[10, 15, 19, 30]","[39, 65, 157, 610]","[20, 22, 81, 353]","[14, 28, 49, 226]","[5, 15, 27, 31]","I think the idea of inferring programmatic descriptions of handwritten diagrams is really cool, and that the combination of SMC-based inference with constraint-based synthesis is nice. I also think the application is clearly useful – one could imagine that this type of technology would eventually become part of drawing / note-taking applications.

That said, based on the current state of the manuscript, I find it difficult to recommend acceptance. I understand that the ICLR does not strictly have a page limit, but I think submitting a manuscript of over 11 pages is taking things a bit too far. The manuscript would greatly benefit from a thorough editing pass and some judicious reconsideration of space allocated to figures. Moreover, despite its relative verbosity, or perhaps because of it, I found it surprisingly difficult to extract simple implementation details from the text (for example I had to dig up the size of the synthetic training corpus from the 44-page appendix). 

Presentation issues aside, I think this is great work. There is a lot here, and I am sympathetic to the challenges of explaining everything clearly in a single (short) paper. That said, I do think that the authors need to take another stab at this to get the manuscript to a point where it can be impactful. 

Minor Comments 

- I don't understand what the ""hypothesis"" is in the trace hypothesis. Breaking down the problem into an AIR-style sequential detection task and a program induction is certainly a reasonable thing to do. However, the word ""hypothesis"" is generally used to refer to a testable explanation of a phenomenon, which is not really applicable here. 

- How is the edit distance defined? In particular, are we treating the drawing commands as a set or a sequence when we calculate ""the number of drawing commands by which two trace sets differ""?

- I took me a while to understand that the authors first consider the case of SMC for synthetic images with a pixel-based likelihood, and then move on to SMC with and edit-distance based surrogate likelihood for hand-drawn pictures. The text seems to suggest that only 100 of such hand drawn images were actually used, is that correct?
 
- What does the (+) operator do in Figure 3?

- I am not sure that ""correcting errors made by the neural network"" is the most accurate way to describe a reranking of the top-k samples returned by the SMC sweep.

- Table 3 is very nice, but does not need to be a full page. 

- I would recommend that the authors consolidate wrap-around figures into full-width figures. 
","[4, 4, 6]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Marginally above acceptance threshold']","[4, 2, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer explicitly states ""I think this is great work"", which clearly indicates positive sentiment. However, they also state  ""based on the current state of the manuscript, I find it difficult to recommend acceptance."" indicating it's not perfect. Thus, a moderately positive score is warranted. The reviewer's critique is direct and detailed, but always professional and respectful in tone, justifying a high politeness score.",60.0,80.0
Continuous-Time Flows for Efficient Inference and Density Estimation,"['Changyou Chen', 'Chunyuan Li', 'Liqun Chen', 'Wenlin Wang', 'Yunchen Pu', 'Lawrence Carin']",Reject,2018,"[11, 8, 15, 16, 7, 22]","[16, 13, 20, 20, 10, 27]","[176, 167, 48, 64, 43, 602]","[87, 71, 16, 33, 26, 306]","[79, 82, 2, 24, 17, 173]","[10, 14, 30, 7, 0, 123]","The authors try to use continuous time generalizations of normalizing flows for improving upon VAE-like models or for standard density estimation problems.

Clarity: the text is mathematically very sloppy / hand-wavy.

1. I do not understand proposition (1). I do not think that the proof is correct (e.g. the generator L needs to be applied to a function -- the notation L(x) does not make too much sense): indeed, in the case when the volatility is zero (or very small), this proposition would imply that any vector field induces a volume preserving transformation, which is indeed false.

2. I do not really see how the sequence of minimization Eq(5) helps in practice. The Wasserstein term is difficult to hand.

3. in Equation (6), I do not really understand what $\log(\bar{\rho})$ is if $\bar{\rho}$ is an empirical distribution. One really needs $\bar{\rho}$ to be a probability density to make sense of that.","[3, 6, 6]","[' Clear rejection', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is critical of the paper, pointing out flaws in the mathematical clarity and validity of the propositions. The reviewer finds the text ""mathematically very sloppy / hand-wavy"" and raises several concerns about the proofs and equations. The use of ""I do not understand"" indicates a lack of clarity in the paper, which contributes to the negative sentiment. However, the reviewer does not use harsh language or personal attacks, keeping the tone relatively neutral.",-50.0,20.0
LatentPoison -- Adversarial Attacks On The Latent Space,"['Antonia Creswell', 'Biswa Sengupta', 'Anil A. Bharath']",Reject,2018,"[3, 21, 14]","[7, 26, 18]","[31, 110, 44]","[6, 48, 6]","[21, 32, 22]","[4, 30, 16]","This paper misses the point of what VAEs (or GANs, in general) are used for. The idea of using VAEs is not to encode and decode images (or in general any input), but to recover the generating process that created those images so we have an unlimited source of samples. The use of these techniques for compressing is still unclear and their quality today is too low. So the attack that the authors are proposing does not make sense and my take is that we should see significant changes before they can make sense. 

But let’s assume that at some point they can be used as the authors propose. In which one person encodes an image, send the latent variable to a friend, but a foe intercepts it on the way and tampers with it so the receiver recovers the wrong image without knowing. Now if the sender believes the sample can be tampered with, if the sender codes z with his private key would not make the attack useless? I think this will make the first attack useless. 

The other two attacks require that the foe is inserted in the middle of the training of the VAE. This is even less doable, because the encoder and decoder are not train remotely. They are train of the same machine or cluster in a controlled manner by the person that would use the system. Once it is train it will give away the decoder and keep the encoder for sending information.

","[3, 5, 4]","[' Clear rejection', ' Marginally below acceptance threshold', ' Ok but not good enough - rejection']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with a very strong negative statement, ""This paper misses the point..."". The reviewer then continues to negate the premise of the paper and proposes changes that would require a complete rewrite.  While the reviewer provides the reasoning for their opinion, the language used is quite direct and critical. There's a lack of constructive feedback aimed at improvement, focusing instead on fundamental flaws. Therefore, the sentiment is highly negative, and the politeness is leaning towards the lower end of the spectrum, although not outright rude.",-80.0,-30.0
Hierarchical Adversarially Learned Inference,"['Mohamed Ishmael Belghazi', 'Sai Rajeswar', 'Olivier Mastropietro', 'Negar Rostamzadeh', 'Jovana Mitrovic', 'Aaron Courville']",Reject,2018,"[4, 5, 3, 7, 3, 18]","[7, 9, 7, 12, 8, 23]","[10, 24, 9, 51, 18, 309]","[4, 11, 2, 22, 6, 135]","[6, 12, 7, 29, 12, 160]","[0, 1, 0, 0, 0, 14]","******
Please note the adjusted review score after revisions and clarifications of the authors. 
The paper was improved significantly but still lacks novelty. For context, multi-layer VAEs also were not published unmodified as follow-up papers since the objective is identical. Also, I would suggest the authors study the modified prior with marginal statistics and other means to understand not just 'that' their model performs better with the extra degree of freedom but also 'how' exactly it does it. The only evaluation is sampling from z1 and z2 for reconstruction which shows that some structure is learned in z2 and the attribute classification task. However, more statistical understanding of the distributions of the extra layers/capacity of the model would be interesting.
******

The authors propose a hierarchical GAN setup, called HALI, where they can learn multiple sets of latent variables.
They utilize this in a deep generative model for image generation and manage to generate good-looking images, faithful reconstructions and good inpainting results.

At the heart of the technique lies the stacking of GANS and the authors claim to be proposing a novel model here.
First, Emily Denton et. al proposed a stacked version of GANs in ""Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks"", which goes uncited here and should be discussed as it was the first work stacking GANs, even if it did so with layer-wise pretraining.
Furthermore, the differences to another very similar work to that of the authors (StackGan by Huan et al) are unclear and not well motivated.
And third, the authors fail to cite 'Adversarial Message Passing' by Karaletsos 2016, which has first introduced joint training of generative models with structure by hierarchical GANs and generalizes the theory to a particular form of inference for structured models with GANs in the loop. 
This cannot be called concurrent work as it has been around for a year and has been seen and discussed at length in the community, but the authors fail to acknowledge that their basic idea of a joint generative model and inference procedure is subsumed there. In addition, the authors also do not offer any novel technical insights compared to that paper and actually fall short in positioning their paper in the broader context of approximate inference for generative models.

Given these failings, this paper has very little novelty and does not perform accurate attribution of credit to the community.
Also, the authors propose particular one-off models and do not generalize this technique to an inference principle that could be reusable.

As to its merits, the authors manage to get a particularly simple instance of a 'deep gan' working for image generation and show the empirical benefits in terms of image generation tasks. 
In addition, they test their method on a semi-supervised task and show good performance, but with a lack of details.

In conclusion, this paper needs to flesh out its contributions on the empirical side and position its exact contributions accordingly and improve the attribution.","[5, 5, 7]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Good paper, accept']","[5, 5, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']","The review is highly critical of the paper, pointing out significant flaws in novelty and attribution. While acknowledging some merits in the empirical results, the reviewer emphasizes the lack of originality and the failure to properly acknowledge prior work. The language, while direct and critical, maintains a professional tone.",-70.0,60.0
Data-efficient Deep Reinforcement Learning for Dexterous Manipulation,"['Ivo Popov', 'Nicolas Heess', 'Timothy P. Lillicrap', 'Roland Hafner', 'Gabriel Barth-Maron', 'Matej Vecerik', 'Thomas Lampe', 'Tom Erez', 'Yuval Tassa', 'Martin Riedmiller']",Reject,2018,"[8, 10, 11, 16, 5, 3, 6, 12, 14, 26]","[7, 15, 16, 21, 9, 3, 10, 17, 18, 31]","[2, 200, 127, 39, 17, 7, 30, 51, 39, 168]","[1, 81, 47, 15, 5, 2, 14, 23, 22, 100]","[1, 111, 70, 19, 10, 5, 16, 24, 14, 47]","[0, 8, 10, 5, 2, 0, 0, 4, 3, 21]","The authors propose to learn to pick up a block and put it on another block using DDPG. A few tricks are described, which I believe already appear in prior work. The discussion of results presented in prior work also has a number of issues. The claim of ""data efficient"" learning is not really accurate, since even with demonstrations, the method requires substantially more experience than prior methods. Overall, it's hard to discern a clear contribution, either experimentally or conceptually, and the excessive claims in the paper are very off-putting. This would perhaps make a reasonable robotics paper if it had a real-world evaluation and if the claims were scoped more realistically, but as-is, I don't think this work is ready for publication.

More detailed comments:

The two main contributions -- parallel training and asynchrony -- already appear in the Gu et al. paper. In fact, that paper demonstrates learning entirely in the real world, and substantially more efficiently than described in this paper. The authors don't discuss this at all, except a passing mention of Gu et al.

The title is not appropriate for this paper. The method is data-efficient compared to what? The results don't look very data efficient: the reported result is something on the order of 160 robot-hours, and 16 robot-hours with demonstration. That's actually dramatically less efficient than prior methods.

""our results on data efficiency hint that it may soon be feasible to train successful stacking policies by collecting interactions on real robots"": Prior work already shows successful stacking policies on real robots, as well as successful pick-and-place policies and a variety of other skills. The funny thing is that many of these papers are actually cited by the authors, but they simply pretend that those works don't exist when discussing the results.

""We assess the feasibility of performing analogous experiments on real robotics hardware"": I assume this is a typo, but the paper does not actually contain any real robotics hardware experiments.

""To our knowledge our results provide the first demonstration of end-to-end learning for a complex manipulation problem involving multiple freely moving objects"": This was demonstrated by Finn et al. in ""Deep Spatial Autoencoders for Visuomotor Learning,"" with training times that are a tiny fraction of those reported in this paper, and using raw images and real hardware.

""both rely on access to a well defined and fully observed state space"": This is not true of the Finn et al. paper mentioned above.","[2, 4, 3]","[' Strong rejection', ' Ok but not good enough - rejection', ' Clear rejection']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is highly critical of the paper, pointing out major flaws in the claims and comparisons to prior work. The reviewer finds the claims of novelty and data efficiency to be inaccurate and unsupported. The tone, while direct and critical, maintains a professional demeanor without resorting to personal attacks.",-75.0,20.0
Regularization for Deep Learning: A Taxonomy,"['Jan Kukačka', 'Vladimir Golkov', 'Daniel Cremers']",Reject,2018,"[2, 4, 19]","[5, 9, 24]","[7, 20, 602]","[2, 6, 327]","[4, 13, 186]","[1, 1, 89]","The main aim of ICLR conference, at least as it is written on its website, is to provide new results on theories, methods and algorithms, supporting further breakthroughs in AI and DL.

In this respect the authors of the paper claim that their “systematic approach enables the discovery of new, improved regularization methods by combining the best properties of the existing ones.”

However, the authors did not provide any discoveries concerning new approaches to regularisation supporting this claim. Thus, the main contribution of the paper is that the authors made a review and performed classification of available regularisation methods. So, the paper is in fact a survey paper, which is more appropriate for full-scale journals. The work, developed by the authors, is really big. However, I am not sure it will bring a lot of benefits for readers except those who need review for some reports, introductions in PhD thesis, etc.

Although the authors mentioned some approaches to combine different regularisations, they did not performed any experiments supporting their ideas.

Thus, I think that
- the paper is well written in general,
- it can be improved (by taking into account several important comments from the Reviewer 2) and served as a review paper in some appropriate journal,
- the paper is not suited for ICLR proceedings due to reasons, mentioned above.","[4, 5, 4]","[' Ok but not good enough - rejection', ' Marginally below acceptance threshold', ' Ok but not good enough - rejection']","[5, 5, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer acknowledges the effort put into the paper (""The work, developed by the authors, is really big."") but ultimately finds the paper's contribution to be more of a review than novel research suitable for ICLR (""the paper is in fact a survey paper, which is more appropriate for full-scale journals.""). They suggest the paper would be more suited for a journal after revisions. This suggests an overall negative sentiment. The language used is quite direct and critical of the paper's suitability for the conference, but it avoids personal attacks and maintains a professional tone, suggesting a slightly negative politeness score.",-50.0,-10.0
Realtime query completion via deep language models,"['Po-Wei Wang', 'J. Zico Kolter', 'Vijai Mohan', 'Inderjit S. Dhillon']",Reject,2018,"[10, 3, 16, 24, 16]","[14, 8, 17, 29, 21]","[20, 102, 16, 277, 234]","[12, 51, 9, 160, 118]","[5, 50, 7, 71, 108]","[3, 1, 0, 46, 8]","The authors describe a method for performing query completion with error correction using a neural network that can achieve real-time performance. The method described uses a character-level LSTM, and modifies the beam search procedure with a an edit distance-based probability to handle cases where the prefix may contain errors. Details are also given on how the authors are able to achieve realtime completion.

Overall, it’s nice a nice study of the query completion application. The paper is well explained, and it’s also nice that the runtime is shown for each of the algorithm blocks. Could imagine this work giving nice guidelines for others who also want to run query completion using neural networks. The final dataset is also a good size (36M search queries).

My major concerns are perhaps the fit of the paper for ICLR as well as the thoroughness of the final experiments. Much of the paper provides background on LSTMs and edit distance, which granted, are helpful for explaining the ideas. But much of the realtime completion section is also standard practice, e.g. maintaining previous hidden states and grouping together the different gates. So the paper feels directed to an audience with less background in neural net LMs.

Secondly, the experiments could have more thorough/stronger baselines. I don’t really see why we would try stochastic search. And expected to see more analysis of how performance was impacted as the number of errors increased, even if errors were introduced artificially, and expected analysis of how different systems scale with varying amounts of data. The fact that 256 hidden dimension worked best while 512 overfit was also surprising, as character language models on datasets such as Penn Treebank with only 1 million words use hidden states far larger than that for 2 layers. More regularization required?","[5, 6, 4]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Ok but not good enough - rejection']","[3, 3, 5]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The reviewer starts with positive remarks, highlighting the study's merits and practical value. However, they express concerns about the paper's suitability for ICLR and the depth of the experiments. They find the background information excessive for the target audience and suggest more robust baselines and analysis in the experiments. The tone is generally constructive and not aggressive.",40.0,70.0
Inducing Grammars with and for Neural Machine Translation,"['Ke Tran', 'Yonatan Bisk']",Reject,2018,"[9, 5]","[14, 7]","[105, 16]","[50, 8]","[53, 8]","[2, 0]","This paper induces latent dependency syntax in the source side for NMT. Experiments are made in En-De and En-Ru.

The idea of imposing a non-projective dependency tree structure was proposed previously by Liu and Lapata (2017) and the structured attention model by Kim and Rush (2017). In light of this, I see very little novelty in this paper. The only novelty seems to be the gate that controls the amount of syntax needed for generating each target word. Seems thin for a ICLR paper.

Caption of Fig 1: ""subject/object"" are syntactic functions, not semantic roles.

I don't see how the German verb ""orders"" inflects with gender... Can you post the gold German sentence?

Sec 2 is poorly explained. What is z_t? Do you mean u_t instead? This is confusing.
 
Expressions (12) to (15) are essentially the same as in Liu and Lapata (2017), not original contributions of this paper.

Why is hard attention (sec 3.3) necessary? It's not differentiable and requires sampling for training. This basically spoils the main advantage of structured attention mechanisms as proposed by Kim and Rush (2017).

Experimentally, the gains are quite small compared to flat attention, which is disappiointing.

In table 3, it would be very helpful to display the English source.

Table 4 is confusing. The DA numbers (rightmost three columns) are for the 2016 or 2017 dataset?

Comparison with predicted parses by Spacy are by no means ""gold"" parses...

Minor comments:
- Sec 1: ""... optimization techniques like Adam, Attention, ..."" -> Attention is not an optimization technique, but part of a model
- Sec 1: ""abilities not its representation"" -> comma before ""not""
","[5, 6, 3]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Clear rejection']","[4, 5, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review is highly critical of the paper, pointing out a lack of novelty and questioning several methodological choices. The reviewer finds the contributions thin and the experimental results disappointing. While the language is direct and critical, it avoids personal attacks and maintains a professional tone.",-70.0,40.0
Byte-Level Recursive Convolutional Auto-Encoder for Text,"['Xiang Zhang', 'Yann LeCun']",Reject,2018,"[5, 31]","[6, 36]","[10, 315]","[4, 162]","[6, 113]","[0, 40]","The authors propose autoencoding text using a byte-level encoding and a convolutional network with shared filters such that the encoder and decoder should exhibit recursive structure. They show that the model can handle various languages and run various experiments testing the ability of the autoencoder to reconstruct the text with varying lengths, perturbations, depths, etc.

The writing is fairly clear, though many of the charts and tables are hard to decipher without labels (and in Figure 8, training errors are not visible -- maybe they overlap completely?).

Main concern would be the lack of experiments showing that the network learns meaningful representations in the hidden layer. E.g. through semi-supervised learning experiments or experiments on learning semantic relatedness of sentences. Obvious citations such as https://arxiv.org/pdf/1511.06349.pdf and https://arxiv.org/pdf/1503.00075.pdf are missing, along with associated baselines. Although the experiment with randomly permuting the samples is nice, would hesitate to draw any conclusions without results on downstream tasks and a clearer survey of the literature.","[5, 5, 7]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Good paper, accept']","[3, 5, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with a neutral summary of the paper's contributions. While it acknowledges the clarity of writing, it then raises a ""main concern"" regarding the lack of experiments validating the meaningfulness of the learned representations. The reviewer suggests specific experiments and relevant literature, indicating a desire for stronger evidence. The tone is suggestive rather than demanding, using phrases like ""would be"" and ""would hesitate."" Overall, the feedback is critical but constructive, aiming to improve the paper.",40.0,70.0
Acquiring Target Stacking Skills by Goal-Parameterized Deep Reinforcement Learning,"['Wenbin Li', 'Jeannette Bohg', 'Mario Fritz']",Reject,2018,"[7, 10, 17]","[8, 15, 22]","[14, 157, 256]","[8, 59, 128]","[6, 72, 114]","[0, 26, 14]","The authors use a variant of deep RL to solve a  simplified 2d physical stacking task. To accommodate different goal stacking states the authors extend the state representation of DQN. The input to the network is the current state of the environment as represented by the 2d projection of the objects in the simulated grid world and a representation of the goal state in the same projection space. The reward function in its basic form rewards only the correctly finished model. A number of heuristics are used to augment this reward function so as to provide shaping rewards along the way and speed up learning. The learnt policy is evaluated on the successful assembly of the target stack and on a distance measure between the stack specified as goal and the actual stack. 

Currently, I don’t understand from the manuscript, how DQN is actually trained. Are all different tasks used on a single network? If so, is it surprising that the network performs worse than when augmenting the state representation with the goal? Or are separate DQNs trained for multiple tasks?

The definition of value function at the bottom of page 4 uses the definition for continual tasks but in the current setting the tasks are naturally episodic. This should be reflected by the definition.

It would be good if the authors could comment on any classic research in RL augmenting the state representation with the goal state and any recent related developments, e.g. multi-task RL or the likes of Dosovitskiy & Koltun “Learning to act by predicting the future”.

It would be helpful do obtain more information about the navigation task, especially a plot of sorts would be helpful. Currently, it is particularly difficult to judge exactly what the authors did. 

How physically “rich” is this environment compared to some of the cited work, e.g. Yildirim et al. or Battaglia et al:?

Overall it feels as if this is an interesting project but that it is not yet ready for publication. ","[5, 5, 4]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Ok but not good enough - rejection']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer acknowledges the project as interesting but suggests it's not ready for publication yet. This, along with constructive criticism and requests for clarification, indicates a neutral rather than negative sentiment. The language is polite and professional throughout, asking clarifying questions and providing suggestions in a constructive manner.",0.0,50.0
ShakeDrop regularization,"['Yoshihiro Yamada', 'Masakazu Iwamura', 'Koichi Kise']",Reject,2018,"[26, 19, 31]","[29, 24, 36]","[10, 103, 220]","[4, 74, 179]","[4, 11, 11]","[2, 18, 30]","The paper proposes a new form of regularization that is an extension of ""Shake-Shake"" regularization (Gastaldi, 2017). The original ""shake-shake"" proposes using two residual paths adding to the same output (so x + F_1(x) + F_2(x)), and during training, considering different randomly selected convex combinations of the two paths (while using an equally weighted combination at test time). However, this paper contends that this requires additional memory, and attempt to achieve similar regularization with a single path. To do so, they train a network with a single residual path, where the residual is included without attenuation in some cases with some fixed probability, and  attenuated randomly (or even inverted) in others. The paper contends that this achieves superior performance than choosing simply a random attenuation for every sample (although, this can be seen as choosing an attenuation under a distribution with some fixed probability mass at 1). Experiments show improved generalization on CIFAR-10 and CIFAR-100.

I don't think the paper contains sufficiently novel elements to be accepted as a conference track paper at ICLR. While it is interesting that this works well (especially the ""negative"" weight on the residual), the proposed method is fundamentally a combination of prior work: dropout and ""shake-shake"" regularization. Moreover, the evaluation is somewhat limited---essentially, I feel there isn't conclusive proof that ""shake-drop"" is a generically useful regularization technique. For one, the method is evaluated only on small toy-datasets: CIFAR-10 and CIFAR-100. I think at the very least, evaluation on Imagenet is necessary. The proposed regularization is applied only to the ""PyramidNet"" architecture---which begs the question of whether the proposed regularization is useful only for this specific network architecture. It would have been more useful to see results with and without ""shake-drop"" on different architectures (the point being to show a consistent improvement with this regularization, rather than achieving 'state of the art' on CIFAR-10). Moreover, it would be interesting to see if the hyperparameter comparison shown in Tables 1 and 2 remained consistent across architectures.","[4, 4, 5]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[4, 3, 2]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']","The reviewer acknowledges the work's basis on prior research and finds the results interesting, particularly the use of negative weights. However, they express concerns about novelty and limited evaluation, suggesting the work is not a significant leap forward. The language is direct and critical but maintains a professional tone without resorting to personal attacks or disrespectful language.",-20.0,60.0
INTERPRETATION OF NEURAL NETWORK IS FRAGILE,"['Amirata Ghorbani', 'Abubakar Abid', 'James Zou']",Reject,2018,"[2, 4, 9]","[7, 9, 14]","[36, 30, 208]","[14, 10, 78]","[16, 16, 109]","[6, 4, 21]","The authors study cases where interpretation of deep learning predictions is extremely fragile. They systematically characterize the fragility of several widely-used feature-importance interpretation methods. In general, questioning the reliability of the visualization techniques is interesting. Regarding the technical details, the reviewer has the following comments: 

- What's the limitation of this attack method?

- How reliable are the interpretations? 

- The authors use spearman's rank order correlation and Top-k intersection as metrics for interpretation similarity. 

- Understanding whether influence functions provide meaningful explanations is very important and challenging problem in medical imaging applications. The authors showed that across the test images, they were able to perturb the ordering of the training image influences. I am wondering how this will be used and evaluated in medical imaging setting. 
","[6, 4, 5]","[' Marginally above acceptance threshold', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[2, 4, 5]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review starts with a positive statement acknowledging the relevance of the paper's topic and its general contribution. However, the following questions, while formulated in a seemingly neutral tone, point out potential weaknesses and areas for improvement. Therefore, the sentiment is slightly leaning towards the positive side. The language used is entirely professional and polite, without any negative or disrespectful phrasing.",20.0,100.0
A Semantic Loss Function for Deep Learning with Symbolic Knowledge,"['Jingyi Xu', 'Zilu Zhang', 'Tal Friedman', 'Yitao Liang', 'Guy Van den Broeck']",Reject,2018,"[15, 10, 2, 4, 10]","[20, 14, 6, 9, 15]","[50, 9, 13, 37, 186]","[23, 4, 7, 15, 101]","[15, 1, 5, 20, 70]","[12, 4, 1, 2, 15]","The authors propose a new loss function that is directed to take into account Boolean constraints involving the variables of a classification problem. This is a nice idea, and certainly relevant. The authors clearly describe their problem, and overall the paper is well presented. The contributions are a loss function derived from a set of axioms, and experiments indicating that this loss function captures some valuable elements of the input. This is a valid contribution, and the paper certainly has some significant strengths.

Concerning the loss function, I find the whole derivation a bit distracting and unnecessary. Here we have some axioms, that are not simple when taken together, and that collectively imply a loss function that makes intuitive sense by itself. Well, why not just open the paper with Definition 1, and try to justify this definition on the basis of its properties. The discussion of axioms is just something that will create debate over questionable assumptions. Also it is frustrating to see some axioms in the main text, and some axioms in the appendix (why this division?). 

After presenting the loss function, the authors consider some applications. They are nicely presented; overall the gains are promising but not that great when compared to the state of the art --- they suggest that the proposed semantic loss makes sense. However I find that the proposal is still in search of a ""killer app"". Overall, I find that the whole proposal seems a bit premature and in need of more work on applications (the work on axiomatics is fine as long as it has something to add).

Concerning the text, a few questions/suggestions:
- Before Lemma 3, ""this allows..."" is the ""this including the other axioms in the appendix?
- In Section 4, line 3: I suppose that the constraint is just creating a problem with a class containing several labels, not really a multi-label classification problem (?).
- The beginning of Section 4.1 is not very clear. By reading it, I feel that the best way to handle the unlabeled data would be to add a direct penalty term forcing the unlabeled points to receive a label. Is this fair?
- Page 6: ""a mor methodological""... should it be ""a more methodical""?
- There are problems with capitalization in the references. Also some references miss page numbers and some do not even indicate what they are (journal papers, conference papers, arxiv, etc).
","[5, 4, 7]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Good paper, accept']","[3, 4, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer acknowledges the paper's strengths, calling the idea ""nice"" and the problem ""relevant."" They find the contributions valid and see significant strengths. However, they also express reservations, finding the axiomatic derivation distracting and suggesting the proposal seems ""a bit premature"" and in need of stronger applications. The reviewer offers constructive criticism and detailed suggestions for improvement. Overall, the tone is critical but professional and aims to be helpful. Therefore, the sentiment leans slightly positive, and the politeness is high.",60.0,80.0
Generating Differentially Private Datasets Using GANs,"['Aleksei Triastcyn', 'Boi Faltings']",Reject,2018,"[7, 34]","[11, 39]","[15, 382]","[5, 262]","[8, 61]","[2, 59]","The paper proposes a technique for differentially privately generating synthetic data using GAN, and experimentally showed that their method achieves both high utility and good privacy.
The idea of building a differentially private GAN and generating differentially private synthetic data is very interesting. However, my main concern is the privacy aspect of the technique, as it is not explained clearly enough in the paper. There is also room for improvement in the presentation and clarity of the paper.

More details:
- About the differential privacy aspect:
  The author didn't provide detailed privacy analysis of the Gaussian noise layer, and I don't find the values of the sensitivity (C = 1) provided in the answer to a public comment easy to see. Also, the paper mentioned that the batch size is 32 and the author mentioned in the comment that the std of the Gaussian noise is 0.7, and the number of epoch is 50 or 150. I think these values would lead to epsilon much larger than 8 (as in Table 1). However, in Section 5.2, it is said that ""Privacy bounds were evaluated using the moments accountant and the privacy amplification theorem (Abadi et al., 2016), and therefore, are data-dependent and are tighter than using normal composition theorems."" I don't see clearly why privacy amplification is needed here, and why using moments accountant and privacy amplification can lead to data-dependent privacy loss.
  In general, I don't find the privacy analysis of this paper clear and detailed enough to convince me about the correctness of the privacy results. However, I am very happy to change my opinion if there are convincing details in the rebuttal.

- About the presentation:
  As a paper proposing a differentially private algorithm, detailed and formal analysis of the privacy guarantees is essential to convince the readers. For example, I think it would be much better if there is a formal theorem showing the sensitivity of the Gaussian noise layer. And it would be better to restate (in Appendix 7.4) not only the definition of moments accountant, but the composition and tail bound, as well as the moments accountant for the Gaussian mechanism, since they are all used in the privacy analysis of this paper.
","[5, 6, 4]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Ok but not good enough - rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer finds the idea interesting but has serious concerns about the privacy aspect, stating it's not clearly explained and the analysis isn't convincing. This points towards a more negative sentiment. However, they are open to changing their opinion based on the rebuttal, indicating it's not entirely negative. The language used is professional and not rude, suggesting a neutral to polite tone.",-50.0,50.0
Global Convergence of Policy Gradient Methods for Linearized  Control Problems,"['Maryam Fazel', 'Rong Ge', 'Sham M. Kakade', 'Mehran Mesbahi']",Reject,2018,"[18, 11, 20, 22]","[23, 16, 25, 27]","[121, 155, 297, 187]","[63, 63, 144, 111]","[38, 78, 126, 30]","[20, 14, 27, 46]","The paper studies the global convergence for policy gradient methods for linear control problems. 
(1) The topic of this paper seems to have minimal connection with ICRL. It might be more appropriate for this paper to be reviewed at a control/optimization conference, so that all the technical analysis can be evaluated carefully. 

(2) I am not convinced if the main results are novel. The convergence of policy gradient does not rely on the convexity of the loss function, which is known in the community of control and dynamic programming. The convergence of policy gradient is related to the convergence of actor-critic, which is essentially a form of policy iteration. I am not sure if it is a good idea to examine the convergence purely from an optimization perspective.

(3) The main results of this paper seem technical sound. However, the results seem a bit limited because it does not apply to neural-network function approximator. It does not apply to the more general control problem rather than quadratic cost function, which is quite restricted. I might have missed something here. I strongly suggest that these results be submitted to a more suitable venue.

","[5, 5, 6]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Marginally above acceptance threshold']","[3, 3, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is mostly negative. While it acknowledges the technical soundness of the paper, it questions its relevance to the conference (ICRL) and novelty. The reviewer suggests a more suitable venue (control/optimization conference) multiple times. The language used is polite and provides constructive criticism, suggesting improvements and further exploration of the topic.",-30.0,70.0
Siamese Survival Analysis with Competing Risks,"['Anton Nemchenko', 'Kartik Ahuja', 'Mihaela Van Der Schaar']",Reject,2018,"[1, 1, 19]","[1, 5, 24]","[3, 16, 829]","[2, 6, 344]","[1, 8, 230]","[0, 2, 255]","The paper entitled 'Siamese Survival Analysis' reports an application of a deep learning to three cases of competing risk survival analysis. The author follow the reasoning that '... these ideas were not explored in the context of survival analysis', thereby disregarding the significant published literature based on the Concordance Index (CI). 

Besides this deficit, the paper does not present a proper statistical setup (e.g. 'Is censoring assumed to be at random? ...) , and numerical results are only referring to some standard implementations, thereby again neglecting the state-of-the-art solution. That being said, this particular use of deep learning in this context might be novel.","[4, 4, 4]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with a mildly positive statement acknowledging the novelty of the paper's application ('might be novel'). However, it quickly transitions into a series of negative critiques. The reviewer points out a significant flaw - neglecting existing literature on the Concordance Index.  Furthermore, the reviewer criticizes the paper for lacking a proper statistical setup and failing to compare the proposed method with state-of-the-art solutions. The language, while critical, maintains a professional and academic tone. There are no personal attacks or inflammatory language.",-30.0,60.0
An inference-based policy gradient method for learning options,"['Matthew J. A. Smith', 'Herke van Hoof', 'Joelle Pineau']",Reject,2018,"[2, 7, 19]","[6, 12, 24]","[3, 79, 308]","[3, 41, 151]","[0, 30, 119]","[0, 8, 38]","This paper proposes what is essentially an off-policy method for learning options in complex continuous problems.  The idea is to use policy gradient style algorithms to update a suite of options using relatively 

On the positive side, I like the core idea of this paper.  The idea of updating multiple options at once is a good one.  I think the authors should definitely continue to investigate this line of work.  I also appreciated that the authors took the time to try and visualize what was learned.  The paper is generally well-written and easy to read.

On the negative side: ultimately, the algorithm doesn't seem to work all that well.  Empirically, the method doesn't seem to perform substantially better than other algorithms, although there seems to be some slight advantage.  A clearly missing comparison would be something like TRPO or DDPG.

Figure 1 was helpful in understanding marginalization and the forward algorithm.  Thanks.

Was there really only 4 options that were learned?  How would this scale to more?
","[4, 3, 3]","[' Ok but not good enough - rejection', ' Clear rejection', ' Clear rejection']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review starts with positive remarks, highlighting the core idea and presentation. However, it transitions to a negative sentiment, expressing concerns about the algorithm's effectiveness and lack of strong empirical support. While it acknowledges a ""slight advantage,"" the criticism regarding performance compared to other algorithms significantly impacts the overall sentiment. The language remains polite throughout, employing constructive criticism and suggestions rather than harsh language.",-10.0,60.0
Generative Adversarial Networks using Adaptive Convolution,"['Nhat M. Nguyen', 'Nilanjan Ray']",Reject,2018,"[1, 18]","[5, 23]","[12, 141]","[5, 74]","[7, 33]","[0, 34]","The paper proposes to use Adaptive Convolution (Niklaus 2017) in the context of GANs. A simple paper with: idea, motivation, experiments

Idea:
It proposes a block called AdaConvBlock that replaces a regular Convolution with two steps:
step 1: regress convolution weights per pixel location conditioned on the input
step 2: do the convolution using these regressed weights
Since local convolutions are generally expensive ops, it provides a few modifications to the size and shape of convolutions to make it efficient (like using depthwise)

Motivation:
- AdaConvBlock gives more local context per kernel weight, so that it can generate locally flexible objects / pixels in images

Motivation is hand-wavy, the claim would need good experiments.

Experiments:
- Experiments are very limited, only overfit to inception score.
- The experiments are not constructed to support the motivation / claim, but just to show that model performance improves.

Inception score experiments as the only experiments of a paper are woefully inadequate. The inception score is computed using a pre-trained imagenet model. It is not hard to overfit to.
The experiments need to support the motivation / claim better.
Ideally the experiments need to show:
- inception score improvements
- actual samples showing that this local context helped produced better local regions / shapes
- some kind of human evaluation supporting claims

The paper's novelty is also quite limited.","[4, 4, 4]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[4, 5, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review is critical of the paper, pointing out significant weaknesses in the motivation, experiments, and novelty. While the language is direct and critical, it is within the bounds of professional academic discourse and avoids personal attacks or disrespectful language.",-50.0,50.0
Tensor Contraction & Regression Networks,"['Jean Kossaifi', 'Zack Chase Lipton', 'Aran Khanna', 'Tommaso Furlanello', 'Anima Anandkumar']",Reject,2018,"[4, -3, 3, 4, 16]","[9, 1, 7, 9, 21]","[36, 1, 18, 25, 542]","[10, 1, 7, 4, 158]","[1, 0, 1, 1, 23]","[25, 0, 10, 20, 361]","In this paper, new layer architectures of neural networks using a low-rank representation of tensors are proposed. The main idea is assuming Tucker-type low-rank assumption for both a weight and an input. The performance is evaluated with toy data and Imagenet.

[Clarity]
The paper is well written and easy to follow.

[Originality]
I mainly concern about the originality. Applying low-rank tensor decomposition in a network architecture has a lot of past studies and I feel this paper fails to clarify what is really distinguished from the other studies. For example, I found at least two papers [1,2] that are relevant. ([2] appears at the reference but it is not referred to.) How is the proposed method different from them?

Also, the ""end-to-end"" feature is repeatedly emphasized in the paper, but I don't understand its benefit. 

[1] Tai, Cheng, et al. ""Convolutional neural networks with low-rank regularization."" arXiv preprint arXiv:1511.06067 (2015).
[2] Lebedev, Vadim, et al. ""Speeding-up convolutional neural networks using fine-tuned cp-decomposition."" arXiv preprint arXiv:1412.6553 (2014).

[Significance]
In the experiments, the proposed method is compared with the vanilla model (i.e., the model having no low-rank structure) but with no other baseline using different compression techniques such as Novikov et al., 2015. So I cannot judge whether this method is better in terms of compression-accuracy tradeoff.


Pros:
- The proposed model (layer architecture) is simple and easy to implement

Cons:
- The novelty is low
- No competitive baseline in experiments
","[4, 6, 4]","[' Ok but not good enough - rejection', ' Marginally above acceptance threshold', ' Ok but not good enough - rejection']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review is critical about the paper's originality and significance, highlighting the lack of comparison with relevant past studies and competitive baselines. While acknowledging the clarity and simplicity of the proposed model, the reviewer's concerns point towards major shortcomings. The language, however, remains professional and constructive throughout.",-30.0,70.0
Distributed non-parametric deep and wide networks,"['Biswa Sengupta', 'Yu Qian']",Reject,2018,"[14, 16]","[18, 21]","[44, 119]","[6, 56]","[22, 12]","[16, 51]","- The paper is fairly written and it is clear what is being done
- There is not much novelty in the paper; it combines known techniques and is a systems paper, so I 
  would judge the contributions mainly in terms of the empirical results and messsage conveyed (see
  third point)
- The paper builds on a  previous paper (ICCV Workshops, https://arxiv.org/pdf/1707.06923.pdf),
  however, there is non-trivial overlap between the two papers, e.g. Fig. 1 seems to be almost the
  same figure from that paper, Sec 2.1 from the previous paper is largely copied     
- The message from the empirical validation is also not novel, in the ICCVW paper it was shown that
  the combination of different modalities etc. using a multiple kernel learning framework improved
  results (73.3 on HMDB51), while in the current paper the same message comes across with another
  kind of (known) method for combining different classifiers and modality (without iDT their best
  results are 73.6 for CNN+GP-PoE) ","[3, 3, 3]","[' Clear rejection', ' Clear rejection', ' Clear rejection']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review acknowledges the paper's clarity and fairness but raises concerns about novelty and overlap with a previous work. The reviewer finds the contributions to be incremental, especially since the empirical results echo the findings of the previous paper. The language used, while direct, maintains a professional and respectful tone. There are no personal attacks or inflammatory remarks.",-20.0,50.0
An Out-of-the-box Full-network Embedding for Convolutional Neural Networks,"['Dario Garcia-Gasulla', 'Armand Vilalta', 'Ferran Parés', 'Jonatan Moreno', 'Eduard Ayguadé', 'Jesús Labarta', 'Ulises Cortés', 'Toyotaro Suzumura']",Reject,2018,"[9, 2, 2, 30, 36, 33, 19]","[13, 6, 6, 34, 40, 37, 24]","[64, 19, 19, 419, 369, 175, 136]","[28, 8, 7, 290, 279, 89, 84]","[23, 8, 8, 26, 18, 17, 36]","[13, 3, 4, 103, 72, 69, 16]","The paper addresses the scenario when using a pretrained deep network as learnt feature representation for another (small) task where retraining is not an option or not desired. In this situation it proposes to use all layers of the network to extract feature from, instead of only one layer. 
Then it proposes to standardize different dimensions of the features based on their response on the original task. Finally, it discretize each dimension into {-1, 0, 1} to compress the final concatenated feature representation. 
Doing this, it shows improvements over using a single layer for 9 target image classification datasets including object, scene, texture, material, and animals.

The reviewer does not find the paper suitable for publication at ICLR due to the following reasons:
- The paper is incremental with limited novelty.
- the results are not encouraging
- the pipeline of standardization, discretization is relatively costly, the final feature vector still large. 
- combining different layers, as the only contribution of the paper, has been done in the literature before,  for instance:
“The Treasure beneath Convolutional Layers: Cross-convolutional-layer Pooling
for Image Classification” CVPR 2016
","[4, 4, 3]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Clear rejection']","[5, 5, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer starts with a neutral summary of the paper's content. However, the tone shifts negatively when stating the paper is ""not suitable for publication."" The reviewer lists several negative points, including ""limited novelty"" and ""not encouraging results."" The language used is direct and critical, typical of academic peer review, but doesn't resort to personal attacks or disrespectful phrasing.",-60.0,20.0
Bounding and Counting Linear Regions of Deep Neural Networks,"['Thiago Serra', 'Christian Tjandraatmadja', 'Srikumar Ramalingam']",Reject,2018,"[7, 11, 16]","[12, 15, 21]","[32, 20, 104]","[13, 7, 63]","[14, 7, 31]","[5, 6, 10]","This paper investigates the complexity of neural networks with piecewise linear activations by studying the number of linear regions of the representable functions. It builds on previous works Montufar et al. (2014) and Raghu et al. (2017) and presents improved bounds on the maximum number of linear regions. It also evaluates the number of regions of small networks during training. 

The improved upper bound given in Theorem 1 appeared in SampTA 2017 - Mathematics of deep learning ``Notes on the number of linear regions of deep neural networks'' by Montufar. 

The improved lower bound given in Theorem 6 is very modest but neat. Theorem 5 follows easily from this. 

The improved upper bound for maxout networks follows a similar intuition but appears to be novel. 

The paper also discusses the exact computation of the number of linear regions in small trained networks. It presents experiments during training and with varying network sizes. These give an interesting picture, consistent with the theoretical bounds, and showing the behaviour during training. 

Here it would be interesting to run more experiments to see how the number of regions might relate to the quality of the trained hypotheses. 



","[6, 4, 6]","[' Marginally above acceptance threshold', ' Ok but not good enough - rejection', ' Marginally above acceptance threshold']","[5, 5, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']","The review acknowledges the significance of the paper's findings by using phrases like ""improved bounds,"" ""modest but neat,"" ""appears to be novel,"" and ""interesting picture."" The reviewer also provides constructive suggestions for further research, indicating a positive outlook on the paper's potential. However, the reviewer points out that one of the main results was already presented elsewhere, which slightly dampens the enthusiasm. Overall, the tone is neutral and professional, leaning towards the positive side.",60.0,80.0
ElimiNet: A Model for Eliminating Options for Reading Comprehension with Multiple Choice Questions,"['Soham Parikh', 'Ananya Sai', 'Preksha Nema', 'Mitesh M Khapra']",Reject,2018,"[1, 1, 2, 10]","[6, 6, 7, 15]","[6, 12, 27, 144]","[2, 3, 13, 71]","[4, 7, 14, 64]","[0, 2, 0, 9]","This paper proposes a new reading comprehension model for multi-choice questions and the main motivation is that some options should be eliminated first to infer better passage/question representations.

It is a well-written paper, however, I am not very convinced by its motivation, the proposed model and the experimental results. 

First of all, the improvement is rather limited. It is only 0.4 improvement overall on the RACE dataset; although it outperforms GAR on 7 out of 13 categories; but why is it worse on the other 6 categories? I don’t see any convincing explanations here.

Secondly, in terms of the development of reading comprehension models, I don’t see why we need to care about eliminating the irrelevant options. It is hard to generalize to any other RC/QA tasks. If the point is that the options can add useful information to induce better representations for passage/question, there should be some simple baselines in the middle that this paper should compare to. The two baselines SAR and GAR both only induce a representation from paragraph/question, and finally compare to the representation of each option. Maybe a simple baseline is to merge the question and all the options and see if a better document representation can be defined. 

Some visualizations/motivational examples could be also useful to understand how some options are eliminated and how the document representation has been changed based on that.
","[4, 5, 5]","[' Ok but not good enough - rejection', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[4, 3, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer starts with a positive note acknowledging the paper is well-written. However, they express skepticism about the core ideas, model, and results, leading to a rather negative overall sentiment. The criticism, while direct, maintains a professional and analytical tone without resorting to harsh language.",-30.0,60.0
Learning to search with MCTSnets,"['Arthur Guez', 'Theophane Weber', 'Ioannis Antonoglou', 'Karen Simonyan', 'Oriol Vinyals', 'Daan Wierstra', 'Remi Munos', 'David Silver']",Reject,2018,"[11, 11, 6, 11, 12, 15, 23, 19]","[15, 16, 10, 15, 17, 16, 28, 23]","[48, 70, 26, 102, 209, 70, 266, 174]","[21, 26, 10, 44, 101, 35, 143, 88]","[22, 38, 12, 51, 98, 27, 96, 58]","[5, 6, 4, 7, 10, 8, 27, 28]","The authors introduce an approach for adding learning to search capability to Monte Carlo tree search. The proposed method incorporates simulation-based search inside a neural network by expanding, evaluating and backing-up a vector-embedding. The key is to represent the internal state of the search by a memory vector at each node. The computation of the network proceeds just like a simulation of MCTS, but using a simulation policy based on the memory vector to initialize the memory vector at the leaf. The proposed method allows each component of MCTS to be rich and learnable, and allows the joint training of the evaluation network, backup network, and simulation policy in optimizing the MCTS network. The paper is thorough and well-explained. My only complaint is the evaluation is only done on one domain, Sokoban. More evaluations on diverse domains are called for.  ","[7, 4, 5]","[' Good paper, accept', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer explicitly states ""The paper is thorough and well-explained."", which indicates a positive sentiment. The only suggestion for improvement is to include evaluations on more domains. This is a common suggestion in a review and doesn't detract from the generally positive tone. Therefore, the sentiment is scored as moderately positive. The language used is polite and constructive throughout the review.",75.0,100.0
Sensor Transformation Attention Networks,"['Stefan Braun', 'Daniel Neil', 'Enea Ceolini', 'Jithendar Anumula', 'Shih-Chii Liu']",Reject,2018,"[3, 5, 3, 2, 30]","[4, 7, 8, 4, 34]","[9, 32, 25, 12, 188]","[7, 20, 16, 8, 112]","[2, 11, 5, 3, 33]","[0, 1, 4, 1, 43]","Summary: 

The authors consider the use of attention for sensor, or channel, selection. The idea is tested on several speech recognition datasets, including TIDIGITS and CHiME3, where the attention is over audio channels, and GRID, where the attention is over video channels. Results on TIDIGITS and GRID show a clear benefit of attention (called STAN here) over concatenation of features. The results on CHiME3 show gain over the CHiME3 baseline in channel-corrupted data.

Review:

The paper reads well, but as a standard application of attention lacks novelty. The authors mention that related work is generalized but fail to differentiate their work relative to even the cited references (Kim & Lane, 2016; Hori et al., 2017). Furthermore, while their approach is sold as a general sensor fusion technique, most of their experimentation is on microphone arrays with attention directly over magnitude-based input features, which cannot utilize the most important feature for signal separation using microphone arrays---signal phase. Their results on CHiME3 are terrible: the baseline CHiME3 system is very weak, and their system is only slightly better! The winning system has a WER of only 5.8%(vs. 33.4% for the baseline system), while more than half of the submissions to the challenge were able to cut the WER of the baseline system in half or better! http://spandh.dcs.shef.ac.uk/chime_challenge/chime2015/results.html. Their results wrt channel corruption on CHiME3, on the other hand, are reasonable, because the model matches the problem being addressed…

Overall Assessment: 

In summary, the paper lacks novelty wrt technique, and as an “application-of-attention” paper fails to be even close to competitive with the state-of-the-art approaches on the problems being addressed. As such, I recommend that the paper be rejected.


Additional comments: 

-	The experiments in general lack sufficient detail: Were the attention masks trained supervised or unsupervised? Were the baselines with concatenated features optimized independently? Why is there no multi-channel baseline for the GRID results? 
-	Issue with noise bursts plot (Input 1+2 attention does not sum to 1)
-	A concatenation based model can handle a variable #inputs: it just needs to be trained/normalized properly during test (i.e. like dropout)…
","[3, 7, 4]","[' Clear rejection', ' Good paper, accept', ' Ok but not good enough - rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is extremely negative about the paper. The reviewer states the paper lacks novelty and has terrible results on one of the datasets. They recommend rejection. While the reviewer uses harsh language like ""terrible"" and ""fails"", they back up their claims with evidence and context, suggesting a critical but not necessarily impolite tone.",-80.0,-20.0
Autoregressive Convolutional Neural Networks for Asynchronous Time Series,"['Mikolaj Binkowski', 'Gautier Marti', 'Philippe Donnat']",Reject,2018,"[2, 4, 4]","[6, 7, 4]","[18, 19, 16]","[9, 10, 8]","[9, 8, 7]","[0, 1, 1]","The author proposed:
1. A data augmentation technique for asynchronous time series data.
2. A convolutional 'Significance' weighting neural network that assigns normalised weights to the outputs of a fully-connected autoregressive 'Offset' neural network, such that the output is a weighted average of the 'Offset' neural net.
3. An 'auxiliary' loss function.

The experiments showed that:
1. The proposed method beat VAR/CNN/ResNet/LSTM 2 synthetic asynchronous data sets, 1 real electricity meter data set and 1 real financial bid/ask data set. It's not immediately clear how hyper-parameters for the benchmark models were chosen.
2. The author observed from the experiments that the depth of the offset network has negligible effect, and concluded that the 'Significance' network has crucial impact. (I don't see how this conclusion can be made.)
3. The proposed auxiliary loss is not useful.
4. The proposed architecture is more robust to noise in the synthetic data set compared to the benchmarks, and together with LSTM, are least prone to overfitting.

Pros
- Proposed a useful way of augmenting asynchronous multivariate time series for fitting autoregressive models
- The convolutional Significance/weighting networks appears to reduce test errors (not entirely clear)

Cons
- The novelties aren't very well-justified. The 'Significance' network was described as critical to the performance, but there is no experimental result to show the sensitivity of the model's performance with respect to the architecture of the 'Significance' network. At the very least, I'd like to see what happens if the weighting was forced to be uniform while keeping the 'Offset' network and loss unchanged.
- It's entirely unclear how the train and test data was split. This may be quite important in the case of the financial data set.
- It's also unclear if model training was done on a rolling basis, which is common for time series forecasting.
- The auxiliary loss function does not appear to be very helpful, but was described as a key component in the paper.

Quality: The quality of the paper was okay. More details of the experiments should be included in the main text to help interpret the significance of the experimental results. The experiment also did not really probe the significance of the 'Significance' network even though it's claimed to be important.
Clarity: Above average. 
Originality: Mediocre. Nothing really shines. Weighted average-type architecture has been proposed many times in neural networks (e.g., attention mechanisms). 
Significance: Low. It's unclear how useful the architecture really is.","[5, 4, 5]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[3, 5, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review acknowledges some positive aspects of the work, such as the data augmentation technique and potential of the weighting network. However, it also raises several significant concerns regarding the justification of the novelties, lack of experimental details, and limited significance of the findings. The reviewer questions the claims about the 'Significance' network and points out missing information about data splitting and training procedures. Overall, the tone is critical but professional, focusing on areas for improvement. Therefore, the sentiment leans slightly towards the negative side due to the unresolved issues and limited impact. The language remains polite throughout, maintaining a professional tone.",-20.0,60.0
CrescendoNet: A Simple Deep Convolutional Neural Network with Ensemble Behavior,"['Xiang Zhang', 'Nishant Vishwamitra', 'Hongxin Hu', 'Feng Luo']",Reject,2018,"[37, 3, 12, 17]","[42, 7, 17, 21]","[543, 14, 165, 65]","[261, 11, 105, 33]","[71, 2, 17, 8]","[211, 1, 43, 24]","This paper proposes a new convolutional network architecture, which is tested on three image classification tasks.

Pros:
The network is very clean and easy to implement, and the results are OK.

Cons:
The idea is rather incremental compared to FractalNet. The results seem to be worse than existing networks, e.g., DenseNet (Note that SVHN is no longer a good benchmark dataset for evaluating state-of-the-art CNNs). Not much insights were given.

One additional question: Skip connections have been shown to be very useful in ConvNets. Why not adopt it in CrescendoNet? What's the point of designing a network without skip connections?
","[5, 4, 4]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[4, 5, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review acknowledges some positives like the network's simplicity and ease of implementation. However, it mainly points out shortcomings - being incremental, potentially yielding worse results than existing networks, and lacking insightful explanations. The question about skip connections also highlights a potential flaw in the paper's approach. Overall, the tone is critical, suggesting areas for improvement rather than outright praise.",-30.0,50.0
Learning to Mix n-Step Returns: Generalizing Lambda-Returns for Deep Reinforcement Learning,"['Sahil Sharma', 'Girish Raguvir J *', 'Srivatsan Ramesh *', 'Balaraman Ravindran']",Reject,2018,"[6, 2, 2, 21]","[11, 4, 6, 26]","[36, 5, 2, 253]","[12, 1, 1, 142]","[12, 3, 1, 86]","[12, 1, 0, 25]","The authors present confidence-based autodidactic returns, a Deep learning RL method to adjust the weights of an eligibility vector in TD(lambda)-like value estimation to favour more stable estimates of the state. The key to being able to learn these confidence values is to not allow the error of the confidence estimates propagate back though the deep learning architecture.

However, the method by which these confidence estimates are refined could be better described. The authors describe these confidences variously as: ""some notion of confidence that the agent has in the value function estimate"" and ""weighing the returns based on a notion of confidence has been explored earlier (White & White, 2016; Thomas et al., 2015)"". But the exact method is difficult to piece together from what is written. I believe that the confidence estimates are considered to be part of the critic and the w vector to be part of the theta_c parameters. This would then be captured by the critic gradient for the CAR method that appears towards the end of page 5. If so, this should be stated explicitly.

There is another theoretical point that could be clearer. The variation in an autodidactic update of a value function (Equation (4)) depends on a few things, the in variation future value function estimates themselves being just one factor. Another two sources of variation are: the uncertainty over how likely each path is to be taken, and the uncertainty in immediate rewards accumulated as part of some n-step return. In my opinion, the quality of the paper would be much improved by a brief discussion of this, and some reflection on what aspects of these variation contribute to the confidence vectors and what isn't captured.

Nonetheless, I believe that the paper represents an interesting and worthy submission to the conference. I would strongly urge the authors to improve the method description in the camera read version though. A few additional comments are as follows:

  • The plot in Figure 3 is the leading collection of results to demonstrate the dominance of the authors' adaptive weight approach (CAR) over the A3C (TD(0) estimates) and LRA3C (truncated TD(lambda) estimates) approaches. However, the way the results are presented/plotted, namely the linear plot of the (shifted) relative performance of CAR (and LRA3C) versus A3C, visually inflates the importance of tasks on which CAR (and LRA3C) perform better than A3C, and diminishes the importance of those tasks on which A3C performs better. It would be better kept as a relative value and plotted on a log-scale so that positive and negative improvements can be viewed on an equal setting.
  • On page 3, when Gt is first mentioned, Gt should really be described first, before the reader is told what it is often replaced with.
  • On page 3, where delta_t is defined (the j step return TD error, I think the middle term should be $gamma^j V(S_{t+j})$
  • On page 4 and 5, when describing the gradient for the actor and critic, it would be better if these were given their own terminology, but if not, then use of the word respectively in each case would help.","[6, 5, 5]","[' Marginally above acceptance threshold', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer provides constructive criticism and identifies areas for improvement without resorting to negative language. They acknowledge the paper's potential ('interesting and worthy submission') and recommend acceptance with revisions ('strongly urge the authors to improve'). This suggests an overall positive sentiment, but not overly enthusiastic. The language is formal, respectful, and focuses on the scientific content.",60.0,80.0
An Ensemble of Retrieval-Based and Generation-Based Human-Computer Conversation Systems.,"['Yiping Song', 'Rui Yan', 'Cheng-Te Li', 'Jian-Yun Nie', 'Ming Zhang', 'Dongyan Zhao']",Reject,2018,"[4, 12, 31, 17, 11, 7]","[9, 17, 36, 22, 16, 12]","[36, 158, 291, 217, 334, 196]","[21, 104, 190, 146, 202, 123]","[11, 19, 43, 51, 96, 64]","[4, 35, 58, 20, 36, 9]","The approach involves multiple steps.
On a high level the query is first used to retrieve k best matching response candidates. Then a concatenation of the query and the candidates are fed into a generative model to generate an additional artificial candidate.
In a final step, the k+1 candidates are re-ranked to report the final response.
Each of these steps involves careful engineering and for each there are some minor novel components.
Yet, not all of the steps are presented in complete technical detail.
Also, training corpora and human labeling of the test data do not seem to be publicly available.
Consequently, it would be hard to exactly reproduce the results of the paper.
Experimental validation also is relatively thin.
While the paper report both BLEU metrics and Fleiss kappa from a small-scale human test, the results are based on a single split of a single corpus into training, validation and test data.
While the results for the ensemble are reported to be higher than for the various components for almost all metrics, measures of spread/variance would allow the reader to better judge the degree and significance of improvement.

Minor:
The paper should be read by a native speaker, as it involves a number of minor grammar issues and typos.
","[6, 5, 5]","[' Marginally above acceptance threshold', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[3, 3, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review acknowledges the novelty of the approach (""Each of these steps involves careful engineering and for each there are some minor novel components."") but raises concerns about reproducibility and limited experimental validation. The language is critical but professional and not disrespectful. Therefore, the sentiment is somewhat negative but not overly so.",-30.0,60.0
Loss Functions for Multiset Prediction,"['Sean Welleck', 'Zixin Yao', 'Yu Gai', 'Jialin Mao', 'Zheng Zhang', 'Kyunghyun Cho']",Reject,2018,"[2, 2, 2, 2, 25, 9]","[7, 2, 7, 7, 30, 14]","[55, 2, 11, 15, 137, 396]","[25, 1, 4, 8, 86, 154]","[30, 1, 7, 6, 38, 215]","[0, 0, 0, 1, 13, 27]","Summary: 
The paper considers the prediction problem where labels are given as multisets. The authors give a definition of a loss function for multisets and show experimental results. The results show that the proposed methods optimizing the loss function perform better than other alternatives.

Comments: 
The problem of predicting multisets looks challenging and interesting. The experimental results look nice. On the other hand, I have several concerns about writing and technical discussions. 

First of all, the goal of the problem is not exactly stated. After I read the experimental section, I realized that the goal is to optimize the exact match score (EM) or F1 measure w.r.t. the ground truth multisets. This goal should be explicitly stated in the paper. Now then, the approach of the paper is to design surrogate loss functions to optimize these criteria. 

The technical discussions for defining the proposed loss function seems not reliable for the reasons below. Therefore, I do not understand the rationale of the definition of the proposed loss function.:  
- An exact definition of the term multiset is not given. If I understand it correctly, a multiset is a “set” of instances allowing duplicated ones. 
- There is no definition of Prec or Rec (which look like Precision and Recall) in Remark 1. The definitions appear in Appendix, which might not be well-defined. For example, let y, Y be mutisets , y=[a, a, a] and Y = [a, b]. Then, by definition, Prec(y,Y)=3/3 =1. Is this what you meant? (Maybe, the ill-definedness  comes from the lack of definition of inclusion in a mutiset.) 
- I cannot follow the proof of Remark 1 since it does not seem to take account of the randomness by the distribution \pi^*. 
- I do not understand the definition of the oracle policy exactly. It seems to me that, the oracle policy knows the correct label (multi-set) \calY for each instance x and use it to construct \calY_t. But, this implicit assumption is not explicitly mentioned. 
- In (1), (2) and Definition 3, what defines \calY_t? If \calY_t is determined by some “optimal” oracle, you cannot define the loss function in Def. 3 since it is not known a priori. Or, if the learner determines \calY_t, I don’t understand why the oracle policy is optimal since it depends on the learner’s choices. 

Also, I expect an investigation of theoretical properties of the proposed loss function, e.g., relationship to EM or F1 or other loss functions. Without understanding the theoretical properties and the rationale, I cannot judge the goodness of the experimental results (look good though). In other words, I cannot judge the paper in a qualitative perspective, not in a quantitative view. 

As a summary, I think the technical contribution of the paper is marginal because of the lack of reliable mathematical discussion or investigation.
","[4, 5, 7]","[' Ok but not good enough - rejection', ' Marginally below acceptance threshold', ' Good paper, accept']","[3, 4, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer finds the problem interesting and the experimental results promising (positive aspects). However, they express several concerns about the clarity, technical soundness, and theoretical grounding of the proposed approach. The lack of clear definitions, questionable proofs, and missing theoretical analysis contribute to a negative perception. The reviewer's tone is critical but professional, pointing out specific weaknesses without resorting to personal attacks.",-20.0,60.0
Multi-Task Learning by Deep Collaboration and Application in Facial Landmark Detection,"['Ludovic Trottier', 'Philippe Giguère', 'Brahim Chaib-draa']",Reject,2018,"[5, 14, 31]","[7, 19, 36]","[13, 99, 187]","[8, 58, 125]","[4, 28, 16]","[1, 13, 46]","The collaborative block that authors propose is a generalized module that can be inserted in deep architectures for better multi-task learning. The problem is relevant as we are pushing deep networks to learn representation for multiple tasks. The proposed method while simple is novel. The few places where the paper needs improvement are:

1. The authors should test their collaborative block on multiple tasks where the tasks are less related. Ex: Scene and object classification. The current datasets where the model is evaluated is limited to Faces which is a constrained setting. It would be great if Authors provide more experiments beyond Faces to test the universality of the proposed approach.
2. The Face datasets are rather small. I wonder if the accuracy improvements hold on larger datasets and if authors can comment on any large scale experiments they have done using the proposed architecture. 

In it's current form I would say the experiment section and large scale experiments are two places where the paper falls short.  ","[6, 5, 6]","[' Marginally above acceptance threshold', ' Marginally below acceptance threshold', ' Marginally above acceptance threshold']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer finds the paper's topic relevant and the proposed method novel. However, they point out shortcomings in the experimental setup and suggest additional experiments. The language is constructive and not dismissive. Therefore, the sentiment is positive, leaning towards neutral due to the constructive criticism. The language used is polite and professional.",50.0,75.0
Reinforcement and Imitation Learning for Diverse Visuomotor Skills,"['Yuke Zhu', 'Ziyu Wang', 'Josh Merel', 'Andrei Rusu', 'Tom Erez', 'Serkan Cabi', 'Saran Tunyasuvunakool', 'János Kramár', 'Raia Hadsell', 'Nando de Freitas', 'Nicolas Heess']",Reject,2018,"[7, 7, 7, 9, 14, 2, 1, 9, 16, 19, 10]","[12, 10, 11, 13, 18, 7, 6, 14, 21, 24, 15]","[146, 53, 49, 28, 39, 19, 20, 21, 101, 199, 200]","[66, 20, 19, 13, 22, 7, 5, 10, 47, 101, 81]","[74, 29, 25, 14, 14, 12, 11, 11, 44, 84, 111]","[6, 4, 5, 1, 3, 0, 4, 0, 10, 14, 8]","This paper claims to present a ""general deep reinforcement learning"" method that addresses the issues of real-world robotics: data constraints, safety, and lack of state information, and exploration by using demonstrations. However, this paper actually addresses these problems by training in a simulator, and only transferring 2 of the 6 tasks to the real world. The real world results are  lackluster. However, the simulated results are nice.

The method in the paper is as follows: the environment reward is augmented by a reward function learned from human demonstrations using GAIL on full state (except for the arm). Then, an actor-critic method is used where the critic gets full state information, while the actor needs to learn from an image. However, the actor's convolutional layers are additionally trained to detect the object positions. 

Strengths:
+ The simulated tasks are novel and difficult (sorting, clearing a table)
+ Resetting to demonstration states is a nice way to provide curriculum

Limitations:
+ The results make me worries that the simulation environments have been hyper-tailored to the method, as the real environments looks very similar, and should transfer. 
+ Each part of the method is not particularly novel. Combining IRL and RL has been done before (as the authors point out in the related work), side-training perception module to predict full state has been done before (""End-to-end visuomotor learning""), diversity of training conditions has been done before (Domain randomization).
+ Requiring hand-specified clusters of states for both selecting starting states and defining a reward functions requires domain knowledge. Why can't they be clustered using a clustering method?
+ Because the method needs simulation to learn a policy, it is limited to tasks that can be simulated somewhat accurately (e.g. ones with simple dynamics). As shown by the poor transfer of the stacking task, block stacking with foam blocks is not a such task.


Questions:
+ How many demonstrations do you use per task?
+ What are the ""relative"" positions included in the ""object-centric"" state input? 

Misleading parts of the paper:
+ The introduction of the paper primes the reader to expect a method that can work on a real system. However, this method only gets 64% accuracy on a simple block lifting task, 35% on a stacking task.
+ ""Appendix C. ""We define functions on the underlying physical state to determine the stage of a state…The definition of stages also gives rise to a convenient way of specifying the reward functions without hand-engineering a shaping reward. ""-> You are literally hand engineering a shaping reward. The main text misleadingly refers to ""sparse reward"", which usually refers to a single reward upon task completion.

In conclusion, I find that the work lacks significance because the results are dependent on a list of hacks that are only possible in simulation.","[4, 6, 4]","[' Ok but not good enough - rejection', ' Marginally above acceptance threshold', ' Ok but not good enough - rejection']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is critical of the paper, pointing out limitations and misleading aspects. While it acknowledges the strengths of the simulated results, it focuses heavily on the shortcomings in real-world application and the lack of significant novelty. The reviewer uses strong language like ""lackluster"" and ""hacks"" to express their disappointment. However, the critique is presented in a professional and analytical manner, focusing on specific aspects of the paper.",-30.0,60.0
Composable Planning with Attributes,"['Amy Zhang', 'Adam Lerer', 'Sainbayar Sukhbaatar', 'Rob Fergus', 'Arthur Szlam']",Reject,2018,"[7, 8, 10, 10, 16]","[12, 13, 14, 15, 21]","[54, 43, 55, 138, 131]","[17, 17, 26, 54, 72]","[33, 26, 27, 74, 49]","[4, 0, 2, 10, 10]","- This paper proposes a framework where the agent has access to a set of user defined attributes parametrizing features of interest. The agent learns a policy for transitioning between similar sets of attributes and given a test task, it can repurpose its attributes to reactively plan a policy to achieve the task. A grid world and tele-kinetically operated block stacking task is used to demonstrate the idea

- This framework is exactly the same as semi-MDPs (Precup, Sutton) and its several generalizations to function approximators as cited in the paper. The authors claim that the novelty is in using the framework for test generalization. 

- So the main burden lies on experiments. I do not believe that the experiments alone demonstrate anything substantially new about semi-MDPs even within the deep RL setup. There is a lot of new vocabulary (e.g. sets of attributes) that is introduced, but it dosen't really add a new dimension to the setup. But I do believe in the general setup and I think its an important research direction. However the demonstrations are not strong enough yet and need further development. For instance automatically discovering attributes is the next big open question and authors allude to it.

- I want to encourage the authors to scale up their stacking setup in the most realistic way possible to develop this idea further. I am sure this will greatly improve the paper and open new directions of researchers. 

","[4, 7, 5]","[' Ok but not good enough - rejection', ' Good paper, accept', ' Marginally below acceptance threshold']","[5, 3, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer acknowledges the importance of the research direction and finds the general setup interesting. However, they consider the current contribution and experiments insufficient to demonstrate novelty. While encouraging the authors to expand on their work, the criticism is significant.",20.0,60.0
Attention-based Graph Neural Network for Semi-supervised Learning,"['Kiran K. Thekumparampil', 'Sewoong Oh', 'Chong Wang', 'Li-Jia Li']",Reject,2018,"[5, 15, 11, 12]","[9, 20, 16, 16]","[24, 80, 196, 66]","[8, 41, 80, 38]","[14, 30, 89, 22]","[2, 9, 27, 6]","The paper proposes graph-based neural network in which weights from neighboring nodes are adaptively determined. The paper shows importance of propagation layer while showing the non-linear layer does not have significant effect. Further the proposed method also provides class relation based on the edge-wise relevance.

The paper is easy to follow and the idea would be reasonable. 

Importance of the propagation layer than the non-linear layer is interesting, and I think it is worth showing.

Variance of results of AGNN is comparable or even smaller than GLN. This is a bit surprising because AGNN would be more complicated computation than GLN. Is there any good explanation of this low variance of AGNN?

Interpretation of Figure 2 is not clear. All colored nodes except for the thick circle are labeled node? I couldn't judge those predictions are appropriate or not.","[6, 6, 7]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Good paper, accept']","[2, 3, 4]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer finds the paper easy to follow and the idea reasonable, indicating a positive sentiment. They are interested in the findings and suggest further exploration. While they raise a point of surprise and seek clarification, the tone remains constructive and inquisitive rather than dismissive. The language used is formal and respectful throughout.",60.0,80.0
State Space LSTM Models with Particle MCMC Inference,"['Xun Zheng', 'Manzil Zaheer', 'Amr Ahmed', 'Yuan Wang', 'Eric P. Xing', 'Alex Smola']",Reject,2018,"[8, 8, 18, 23, 18, 23]","[11, 13, 22, 28, 23, 28]","[27, 143, 120, 605, 625, 341]","[16, 69, 82, 278, 339, 216]","[10, 73, 21, 48, 218, 83]","[1, 1, 17, 279, 68, 42]","This article presents an approach for learning and inference in nonlinear state-space models (SSM) based on LSTMs. Learning is done using a stochastic EM where Particle PMCM is used to sample state trajectories.

The model is presented assuming that SSMs are linear. This is not necessarily the case since nonlinear SSMs have been used for a long time (see for example Ljung, 1999, ""System Identification, Theory for the User""). The presented model is a nonlinear SSM with a particular structure that uses LSTMs.

The model described in the paper is Markovian: if one defines the variable sz_t = {s_t, z_t} there exists a Markov chain for the latent state sz:

sz_t -> sz_{t+1} -> sz_{t+2} -> ...

Marginalizing the latent variables s_t leads to a structure that, in general, is not Markovian. The authors claim that this marginalization ""allows the SSL to have non-Markovian state transition"". The word ""allows"" may mislead the reader in thinking that the model has gained some appealing property whereas the model is still essentially Markovian as evidenced by the Markov chain in sz. Any general algorithm for inference in nonlinear Markovian models could be used for inference of sz.

The algorithm used for inference and learning is stochastic EM with PMCMC but the authors do not cite important prior work such as: Lindsten (2013) ""An efficient stochastic approximation EM algorithm using conditional particle filters""


Pros:

The model is sound.

The overall structure of the paper is good.


Cons:

The authors formulate the problem in such a way that they are forced to use an algorithm for non-Markovian models when they could have conserved the Markovian structure by choosing the appropriate parameterization.

The presentation of state-space models, filtering and smoothing shows some lack of familiarity with the literature. The control theory literature has dealt with nonlinear SSMs for decades and there is recent work in the machine learning community on nonlinear SSMs, e.g. Gaussian Process SSMs. 

I would advise against the use of non-English expressions unless they are used precisely:

   - sine qua non: LSTMs are not literally an indispensable model for sequence modeling nowadays. If the use of Latin was unavoidable, ""de facto standard"" would have been slightly more accurate.

   - bona fide: I am not sure what the authors wanted to say.

   - naívely: the correct spelling would be naïvely or naively.","[3, 5, 7]","[' Clear rejection', ' Marginally below acceptance threshold', ' Good paper, accept']","[5, 5, 5]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review is critical of the paper's approach and presentation, pointing out flaws in the authors' understanding of SSMs and their claim about non-Markovian properties. The reviewer also criticizes the authors' grasp of basic concepts like filtering and smoothing. However, the reviewer acknowledges the soundness of the model and the good structure of the paper. The language used, while direct and critical, maintains a professional and academic tone. The use of ""non-English"" expressions is pointed out as unnecessary rather than offensive. Therefore, the sentiment leans towards the negative side due to the critical points raised, but not overly negative due to the positive points mentioned. The politeness remains relatively neutral.",-30.0,20.0
Theoretical properties of the global optimizer of two-layer Neural Network,"['Digvijay Boob', 'Guanghui Lan']",Reject,2018,"[2, 13]","[7, 18]","[17, 92]","[4, 9]","[10, 39]","[3, 44]","I only got access to the paper after the review deadline; and did not have a chance to read it until now. Hence the lateness and brevity.

The paper tackles an important theoretical question; and it offers results that are complementary to existing results (e.g., Soudry et al). However, the paper does not properly relate their results, assumptions in the context of the existing literature. Much explanation is needed in the author reply in order to clear these questions.

The work should not be evaluated from a practical perspective as it is of a theoretical nature.

I agree with most of the criticism raised by other reviewers. However, I also believe the authors managed to clear essentially of the criticism in they reply. The paper lacks in clarity as currently written. 

The results are interesting, but more explanation is needed for the main message to be conveyed more clearly. I suggest 7, but the paper has a potential to become 8 in my eyes in a future resubmission.
","[7, 7, 4]","[' Good paper, accept', ' Good paper, accept', ' Ok but not good enough - rejection']","[4, 5, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The reviewer acknowledges the paper's value in addressing an important theoretical question and providing complementary results to existing research. While they find the results interesting, they express concerns about the paper's clarity and the need for more explanation to convey the main message effectively. The reviewer also mentions agreeing with criticisms from other reviewers but acknowledges that the authors have addressed most of them. They see the potential for the paper to be even better in a future resubmission. Overall, this indicates a slightly positive sentiment. The language used is polite and professional throughout the review.",60.0,80.0
Overcoming the vanishing gradient problem in plain recurrent networks,"['Yuhuang Hu', 'Adrian Huber', 'Shih-Chii Liu']",Reject,2018,"[3, 1, 2, 30]","[7, 3, 4, 34]","[23, 7, 12, 188]","[12, 3, 8, 112]","[10, 1, 3, 33]","[1, 3, 1, 43]","Summary: 
The authors present a simple variation of vanilla recurrent neural networks, which use ReLU hiddens and a fixed identity matrix that is added to the hidden-to-hidden weight matrix. This identity connection acts as a “surrogate memory” component, preserving hidden activations over time steps. 
The experiments demonstrate that this architecture reliably solves the addition task for up to 400 input frames. It also achieves a very good performance on sequential and permuted MNIST and achieves SOTA performance on bAbI.
The authors observe that the proposed recurrent identity network (RIN) is relatively robust to hyperparameter choices. After Le et al. (2015), the paper presents another convincing case for the application of ReLUs in RNNs.

Review: 
I very much like the paper. The motivation and architecture is presented very clearly and I am happy to also see explorations of simpler recurrent architectures in parallel to research of gated architectures!
I have a few comments and questions:
1) Clarification: In Section 2.2, do you really mean bit-wise multiplication or element-wise? If bit-wise, can you elaborate why? I might have missed something.
2) Why does the learning curve of the IRNN stop around epoch 270 in Figure 2c? Also some curves in the appendix stop abruptly without visible explosions. Were these experiments run until completion? If so, would it be possible to plot the complete curves?
3) I think for a fair comparison with LSTMs and IRNNs a limited hyperparameter search should be performed separately on all three architectures at least for the addition task. Optimal hyperparameters are usually model-specific. Admittedly, the authors mention that they do not intend to make claims about superior performance to LSTMs, however the competitive performance of small RINs is mentioned a couple of times in the manuscript.
Le et al. (2015) for instance perform a coarse grid search for each model.
4) I wouldn't say that ResNets are Gated Neural Networks, as the branches are just summed up. There is no (multiplicative) gating as in Highway Networks.
5) I think what enables the training of very deep networks or LSTMs on long sequences is the presence of a (close-to-)identity component in forward/backward propagation, not the gating. The use of ReLU activations in IRNNs (with identity initialization of the hidden-to-hidden weights) and RINs (effectively initialized with identity plus some noise) makes the recurrence more linear than with squashing activation functions.
6) Regarding the absence of gating in RINs: What is your intuition on how the model would perform in tasks for which conditional forgetting is useful. Consider for example a task with long sequences, outputs at every time step and hidden activations not necessarily being encouraged to estimate last step hidden activations. Would RINs readily learn to reset parts of the hidden state?
7) Henaff et al. (2016) might be related, as they are also looking into the addition task with long sequences.

Overall, the presented idea is novel to the best of my knowledge and the manuscript is well-written. I would recommend it for acceptance, but would like to see the above points addressed (especially 1-3 and some comments on 4-6). After a revision I would consider to increase the score.

References:
Henaff, Mikael, Arthur Szlam, and Yann LeCun. ""Recurrent orthogonal networks and long-memory tasks."" In International Conference on Machine Learning, pp. 2034-2042. 2016.
Le, Quoc V., Navdeep Jaitly, and Geoffrey E. Hinton. ""A simple way to initialize recurrent networks of rectified linear units."" arXiv preprint arXiv:1504.00941 (2015).","[7, 2, 4]","[' Good paper, accept', ' Strong rejection', ' Ok but not good enough - rejection']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The reviewer explicitly states they ""very much like the paper"" and find the idea novel and the manuscript well-written. They recommend acceptance upon addressing their comments, which are constructive and suggest further exploration rather than major flaws. This points to a positive sentiment with polite language. The score is slightly below 100 as it's not perfect praise and there's room for improvement according to the reviewer.",85.0,90.0
Discrete Sequential Prediction of Continuous Actions for Deep RL,"['Luke Metz', 'Julian Ibarz', 'Navdeep Jaitly', 'James Davidson']",Reject,2018,"[3, 8, 12, 43]","[8, 13, 16, 45]","[48, 45, 78, 33]","[19, 17, 31, 14]","[29, 24, 38, 14]","[0, 4, 9, 5]","Originality
--------------
When the action space is N-dimensional, computing argmax could be problematic. The paper proposes to address the problem by creating N MDPs with 1-D actions. 

Clarity
---------
1) Explicitly writing down DDPG will be helpful
2) The number of actions in each of the domains will also be useful

Quality
----------
1) The paper reports experimental results on order of actions as well as binning, and the results confirm with what one would expect from intuition. 
2) It will be important to talk about the case when the action dimension N is very large, what happens in that case? Does the proposed method would work in such a scenario? A discussion is needed.
3) Given that the ordering of actions does not matter, what is the real take away of looking at them as 'sequence' (which has not temporal structure because action order could be arbitrary)?


Significance
----------------
While the proposed method seems a reasonable approach to handle the argmax problem, it still requires training multiple networks for Q^i (i=1,..N) for Q^L, which is a limitation. Further, since the actions could be arbitrary, it is unclear where 'sequence' approach helps. These limit the understand and hence significance.
","[5, 7, 4]","[' Marginally below acceptance threshold', ' Good paper, accept', ' Ok but not good enough - rejection']","[1, 5, 5]","["" The reviewer's evaluation is an educated guess"", ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review is lukewarm in its assessment of the paper. While it acknowledges the reasonableness of the proposed method, it raises several concerns, particularly regarding scalability, clarity, and significance. The use of phrases like ""it is unclear"" and ""These limit the understanding"" suggests a somewhat negative sentiment, but the tone remains professional and constructive throughout. The reviewer offers concrete suggestions for improvement, indicating a willingness to engage with the authors' work.",-10.0,70.0
Convolutional Normalizing Flows,"['Guoqing Zheng', 'Yiming Yang', 'Jaime Carbonell']",Reject,2018,"[17, 15, 18, 31]","[22, 20, 23, 36]","[161, 93, 160, 210]","[68, 57, 95, 77]","[8, 12, 58, 72]","[85, 24, 7, 61]","In this paper, the authors propose a type of Normalizing Flows (Rezende and Mohamed, 2015) for Variational Autoencoders (Kingma and Welling, 2014; Rezende et al., 2014) they call Convolutional Normalizing Flows.
More particularly, it aims at extending on the Planar Flow scheme proposed in Rezende and Mohamed (2015). The authors notice an improvement through their method over Normalizing Flows, IWAE with diagonal gaussian approximation, and standard Variational Autoencoders. 
As noted by AnonReviewer3, several baselines are missing. But the authors partly address that issue in the comment section for the MNIST dataset.
The requirement of h being bijective seems wrong. For example, if h was a rectifier nonlinearity in the zero-derivative regime, the Jacobian determinant of the ConvFlow would be 1. 
More importantly, the main issue is that this paper might need to highlight the fundamental difference between their proposed method and Inverse Autoregressive Flow (Kingma et al., 2016). The proposed connectivity pattern proposed for the convolution in order to make the Jacobian determinant computation is exactly the same as Inverse Autoregressive Flow and the authors seems to be aware of the order dependence of their architecture which is every similar to autoregressive models. This presentation of the paper can be misleading concerning the true innovation in the model trained. Proposing ConvFlow as a type of Inverse Autoregressive Flow would be more accurate and would allow to highlight better the innovation of the work.
Since this work does not offer additional significant insight over Inverse Autoregressive Flow, its value should be on demonstrating the efficiency of the proposed method. MNIST and Omniglot seems insufficient for that purpose given currently published work.
In the current state, I can't recommend the paper for acceptance. 


Danilo Jimenez Rezende, Shakir Mohamed: Variational Inference with Normalizing Flows. ICML 2015
Danilo Jimenez Rezende, Shakir Mohamed, Daan Wierstra: Stochastic Back-propagation and Variational Inference in Deep Latent Gaussian Models. ICML 2014
Diederik P. Kingma, Max Welling: Auto-Encoding Variational Bayes. ICLR 2014
Diederik P. Kingma, Tim Salimans, Rafal Józefowicz, Xi Chen, Ilya Sutskever, Max Welling: Improving Variational Autoencoders with Inverse Autoregressive Flow. NIPS 2016","[3, 3, 5]","[' Clear rejection', ' Clear rejection', ' Marginally below acceptance threshold']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer acknowledges some improvements of the paper's method but highlights a major flaw: the proposed method seems to be a reinvention of an existing method (Inverse Autoregressive Flow) without proper acknowledgment. The reviewer suggests the paper's value lies in demonstrating efficiency, but finds the current evidence insufficient. The language is critical but professional and not disrespectful.",-50.0,50.0
Forward Modeling for Partial Observation Strategy Games - A StarCraft Defogger,"['Gabriel Synnaeve', 'Zeming Lin', 'Jonas Gehring', 'Vasil Khalidov', 'Nicolas Carion', 'Nicolas Usunier']",Reject,2018,"[9, 3, 6, 1, 1, 11, 3, 14]","[14, 7, 11, 1, 5, 16, 7, 19]","[132, 22, 23, 3, 12, 23, 12, 125]","[59, 11, 15, 1, 3, 14, 4, 72]","[67, 9, 7, 2, 7, 7, 8, 44]","[6, 2, 1, 0, 2, 2, 0, 9]","# Summary
This paper introduces a new prediction problem where the model should predict the hidden opponent's state as well as the agent's state. This paper presents a neural network architecture which takes the map information and several other features and reconstructs the unit occupancy and count information in the map. The result shows that the proposed method performs better than several hand-designed baselines on two downstream prediction tasks in Starcraft.

[Pros]
- Interesting problem

[Cons]
- The proposed method is not much novel.
- The evaluation is a bit limited to two specific downstream prediction tasks.

# Novelty and Significance
- The problem considered in this paper is interesting.
- The proposed method is not much novel. 
- Overall, this paper is too specific to Starcraft domain + particular downstream prediction tasks. It would be much stronger to show the benefit of defogging objective on the actual gameplay rather than prediction tasks. Alternatively, it could be also interesting to consider an RL problem where the agent should reveal the hidden state of the opponent as much/quickly as possible.

# Quality
- The experimental result is not much comprehensive. The proposed method is expected to perform better than hand-designed methods on downstream prediction tasks. It would be better to show an in-depth analysis of the learned model or show more results on different tasks (possibly RL tasks rather than prediction tasks).

# Clarity
- I did not fully understand the learning objective. Does the model try to reconstruct the state of the current time-step or the future? The learning objective is not clearly defined. In Section 4.1, the target x and y have time steps from t1 to t2. What is the range of t1 and t2? If the proposed model is doing future prediction, it would be important to show and discuss long-term prediction results.","[5, 5, 4]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Ok but not good enough - rejection']","[3, 4, 1]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', "" The reviewer's evaluation is an educated guess""]","The reviewer finds the problem interesting but criticizes the novelty of the proposed method and the limited evaluation. They suggest exploring alternative applications like reinforcement learning to strengthen the paper. The language used is polite, employing constructive criticism and suggestions rather than harsh language.",20.0,80.0
Parametric Manifold Learning Via Sparse Multidimensional Scaling,"['Gautam Pai', 'Ronen Talmon', 'Ron Kimmel']",Reject,2018,"[4, 10, 26]","[9, 15, 31]","[20, 100, 268]","[8, 33, 116]","[9, 26, 58]","[3, 41, 94]","The authors argue that the spectral dimensionality reduction techniques are too slow, due to the complexity of computing the eigenvalue decomposition, and that they are not suitable for out-of-sample extension. They also note the limitation of neural networks, which require huge amounts of data to properly learn the data structure. The authors therefore propose to first sub-sample the data and afterwards to learn an MDS-like cost function directly with a neural network, resulting in a parametric framework.

The paper should be checked for grammatical errors, such as e.g. consistent use of (no) hyphen in low-dimensional (or low dimensional).

The abbreviations should be written out on the first use, e.g. MLP, MDS, LLE, etc.

In the introduction the authors claim that the complexity of parametric techniques does not depend on the number of data points, or that moving to parametric techniques would reduce memory and computational complexities. This is in general not true. Even if the number of parameters is small, learning them might require complex computations on the whole data set. On the other hand, even if the number of parameters is equal to the number of data points, the computations could be trivial, thus resulting in a complexity of O(N).

In section 2.1, the authors claim ""Spectral techniques are non-parametric in nature""; this is wrong again. E.g. PCA can be formulated as MDS (thus spectral), but can be seen as a parametric mapping which can be used to project new words.

In section 2.2, it says ""observation that the double centering..."". Can you provide a citation for this?

In section 3, the authors propose they technique, which should be faster and require less data than the previous methods, but to support their claim, they do not perform an analysis of computational complexity. It is not quite clear from the text what the resulting complexity would be. With N as number of data points and M as number of landmarks, from the description on page 4 it seems the complexity would be O(N + M^2), but the steps 1 and 2 on page 5 suggest it would be O(N^2 + M^2). Unfortunately, it is also not clear what the complexity of previous techniques, e.g DrLim, is.

Figure 3, contrary to text, does not provide a visualisation to the sampling mechanism.

In the experiments section, can you provide a citation for ADAM and explain how the parameters were selected? Also, it is not meaningful to measure the quality of a visualisation via the MDS fit. There are more useful approaches to this task, such as the quality framework [*].

In figure 4a, x-axis should be ""number of landmarks"".

It is not clear why the equation 6 holds. Citation?
It is also not clear how exactly the equation 7 is evaluated. It says ""By varying the number of layers and the number of nodes..."", but the nodes and layer are not a part of the equation.

The notation for equation 8 is not explained.

Figure 6a shows visualisations by different techniques and is evaluated ""by looking at it"". Again, use [*].

[*] Lee, John Aldo ; Verleysen, Michel. Scale-independent quality criteria for dimensionality reduction. In: Pattern Recognition Letters, Vol. 31, no. 14, p. 2248-2257 (2010). doi:10.1016/j.patrec.2010.04.013.
","[4, 5, 3]","[' Ok but not good enough - rejection', ' Marginally below acceptance threshold', ' Clear rejection']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is quite critical of the paper, pointing out several flaws in the arguments, methodology, and results. The reviewer questions the authors' claims and finds inconsistencies and unsupported statements. While the reviewer does provide constructive criticism and suggestions for improvement, the overall tone suggests a significant need for revisions. The language is formal and academic, but direct in pointing out the shortcomings. There's no personal attack or disrespectful language used.",-50.0,50.0
Learning Independent Causal Mechanisms,"['Giambattista Parascandolo', 'Mateo Rojas Carulla', 'Niki Kilbertus', 'Bernhard Schoelkopf']",Reject,2018,"[11, -1, 1, 20, 24]","[16, 4, 6, 25, 29]","[11, 5, 28, 252, 121]","[6, 1, 9, 153, 73]","[4, 3, 17, 69, 33]","[1, 1, 2, 30, 15]","This paper describes a setting in which a system learns collections of inverse-mapping functions that transform altered inputs to their unaltered ""canonical"" counterparts, while only needing unassociated and separate sets of examples of each at training time.  Each inverse map is an ""expert"" E akin to a MoE expert, but instead of using a feed-forward gating on the input, an expert is selected (for training or inference) based on the value of a distribution-modeling function c applied to the output of all experts:  The expert with maximum value c(E(x)) is selected.  When c is an adversarially trained discriminator network, the experts learn to model the different transformations that map altered images back to unaltered ones.  This is demonstrated using MNIST with a small set of synthetic translations and noise.

The fact that these different inverse maps arise under these conditions is interesting --- and Figure 5 is quite convincing in showing how each expert generalizes.  However, I think the experimental conditions are very limited:  Only one collection of transformations is studied, and on MNIST digits only.  In particular, I found the fact that only one of ten transformations can be applied at a time (as opposed to a series of multiple transforms) to be restrictive.  This is touched on in the conclusion, but to me it seems fundamental, as any real-world new example will undergo significantly more complex processes with many different variables all applied at once.

Another direction I think would be interesting, is how few examples are needed in the canonical distribution?  For example, in MNIST, could the canonical distribution P be limited to just one example per digit (or just one example per mode / style of digit, e.g. ""2"" with loop, and without loop)?  The different handwriters of the digits, and sampling and scanning process, may themselves constitute in-the-wild transformations that might be inverted to single (or few) canonical examples --- Is this possible with this mechanism?

Overall, it is nice to see the different inverse maps arise naturally in this setting.  But I find the single setting limiting, and think the investigation could be pushed further into less restricted settings, a couple of which I mention above.



Other comments:

- c is first described to be any distribution model, e.g. the autoencoder described on p.5.  But it seems that using such a fixed, predefined c like the autoencoder may lead to collapse:  What is preventing an expert from learning a single constant mode that has high c value?  The adversarially trained c doesn't suffer from this, because presumably the discriminator will be able to learn the difference between a single constant mode output and the distribution P.  But if this is the case, it seems a critical part of the system, not a simple implementation choice as the text seems to say.

- The single-net baseline is good, but I'd like to get a clearer picture of its results.  p.8 says this didn't manage to ""learn more than one inverse mechanism"" --- Does that mean it learns to invert a single mechanism (that is always translates up, for example, when presented an image)?  Or that it learned some mix of transforms that didn't seem to generalize as well?  Or does it have some other behavior?  Also, I'm not entirely clear on how it was trained wrt c --- is argmax(c(E(x)) always just the single expert?  Is c also trained adversarially?  And if so, is the approximate identity initialization used?
","[5, 6, 5]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Marginally below acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer finds the paper's concept interesting and acknowledges the convincing results demonstrated in Figure 5. However, they express concerns about the limited experimental conditions, particularly the focus on a single transformation type and the use of MNIST only. They suggest exploring more complex and realistic scenarios. The reviewer also raises specific questions and requests clarifications, indicating a need for further investigation and details. While the reviewer offers constructive criticism and suggestions for improvement, their overall tone remains positive and encouraging. The language used is polite and professional throughout the review.",60.0,80.0
Weighted Transformer Network for Machine Translation,"['Karim Ahmed', 'Nitish Shirish Keskar', 'Richard Socher']",Reject,2018,"[3, 4, 12]","[7, 8, 17]","[12, 46, 229]","[6, 16, 111]","[5, 27, 111]","[1, 3, 7]","TL;DR of paper: they modify the Transformer architecture of Vaswani et al. (2017) to used branched attention with learned weights instead of concatenated attention, and achieve improved results on machine translation.

Using branches instead of a single path has become a hot architecture choice recently, and this paper applies the branching concept to multi-head attention. Weirdly, they propose using two different sets of weights for each branch: (a) kappa, which premultiplies the head before fully connected layers, and (b) alpha, which are the weights of the sum of the heads after the fully connected layers. Both weights have simplex constraints. A couple of questions about this:

* What is the performance of only using kappa? Only alpha? Neither? What happens if I train only of them?
* What happens if you remove the simplex constraints (i.e., don't have to sum to one, or can be negative)?
* Why learn a global set of weights for the branch combiners? What happens if the weights are predicted for each input example? This is the MoE experiment, but where k = M (i.e., no discrete choices made).
* Are the FFN layer parameters shared across the different heads?
* At the top of page 4, it is said ""all bounds are respected during each training step by projection"". What does this mean? Is projected gradient descent used, or is a softmax used? If the former, why not use a softmax?
* In Figure 3, it looks like the kappa and alpha values are still changing significantly before they are frozen. What happens if you let them train longer? On the same note, the claim is that Transformer takes longer to train. What is the performance of Transformer if using the same number of steps as the weighted Transformer?
* What are the Transformer variants A, B, and C?

While the results are an improvement over the baseline Transformer, my main concern with this paper is that the improved results are because of extensive hyperparameter tuning. Design choices like having a separate learning rate schedule for the alpha and kappa parameters, and needing to freeze them at the end of training stoke this concern. I'm happy to change my score if the authors can provide empirical evidence for each design choice","[6, 9, 4]","[' Marginally above acceptance threshold', ' Top 15% of accepted papers, strong accept', ' Ok but not good enough - rejection']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer raises several valid questions and expresses concerns about the paper, indicating a critical but not dismissive stance. They acknowledge the results are an improvement but are skeptical about the reasons behind the improvement. The language is direct and inquisitive, typical of academic peer review, but not disrespectful. ",10.0,50.0
Integrating Episodic Memory into a Reinforcement Learning Agent Using Reservoir Sampling,"['Kenny J. Young', 'Shuo Yang', 'Richard S. Sutton']",Reject,2018,"[3, 36, 14]","[7, 41, 19]","[18, 168, 226]","[6, 83, 94]","[12, 63, 39]","[0, 22, 93]","The paper proposes a modified approach to RL, where an additional ""episodic memory"" is kept by the agent. What this means is that the agent has a reservoir of n ""states"" in which states encountered in the past can be stored. There are then of course two main questions to address (i) which states should be stored and how (ii) how to make use of the episodic memory when deciding what action to take. 

For the latter question, the authors propose using a ""query network"" that based on the current state, pulls out one state from the memory according to certain probability distribution. This network has many tunable parameters, but the main point is that the policy then can condition on this state drawn from the memory. Intuitively, one can see why this may be advantageous as one gets some information from the past. (As an aside, the authors of course acknowledge that recurrent neural networks have been used for this purpose with varying degrees of success.)

The first question, had a quite an interesting and cute answer. There is a (non-negative) importance weight associated with each state and a collection of states has weight that is simply the product of the weights. The authors claim (with some degree of mathematical backing) that sampling a memory of n states where the distribution over the subsets of past states of size n is proportional to the product of the weights is desired. And they give a cute online algorithm for this purpose. However, the weights themselves are given by a network and so weights may change (even for states that have been observed in the past). There is no easy way to fix this and for the purpose of sampling the paper simply treats the weights as immutable. 

There is also a toy example created to show that this approach works well compared to the RNN based approaches.

Positives:

- An interesting new idea that has potential to be useful in RL
- An elegant algorithm to solve at least part of the problem properly (the rest of course relies on standard SGD methods to train the various networks)

Negatives:
- The math is fudged around quite a bit with approximations that are not always justified
- While overall the writing is clear, in some places I feel it could be improved. I had a very hard time understanding the set-up of the problem in Figure 2. [In general, I also recommend against using figure captions to describe the setup.]
- The experiments only demonstrate the superiority of this method on an example chosen artificially to work well with this approach.","[4, 4, 4]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[3, 4, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review is mostly positive. The reviewer finds the idea interesting and elegant, highlighting the potential of the approach. While there are criticisms regarding mathematical rigor, clarity in figure description, and limited experimental validation, these are presented constructively. The reviewer acknowledges the paper's strengths and presents their concerns as areas for improvement, suggesting an overall positive disposition towards the work.",60.0,70.0
Variational Bi-LSTMs,"['Samira Shabanian', 'Devansh Arpit', 'Adam Trischler', 'Yoshua Bengio']",Reject,2018,"[4, 8, 3, 31]","[9, 13, 8, 36]","[11, 45, 82, 975]","[3, 20, 39, 405]","[8, 25, 43, 454]","[0, 0, 0, 116]","This paper proposes a particular form of variational RNN that uses a forward likelihood and a backwards posterior.  Additional regularization terms are also added to encourage the model to encode longer term dependencies in its latent distributions.

My first concern with this paper is that the derivation in Eq. 1 does not seem to be correct.  There is a p(z_1:T) term that should appear in the integrand.

It is not clear to me why h_t should depend on \tilde{b}_t.  All paths from input to output through \tilde{b}_t also pass through z_t so I don't see how this could be adding information.  It may add capacity to the decoder in the form of extra weights, but the same could be achieved by making z_t larger. Why not treat \tilde{b}_t symmetrically to \tilde{h}_t, and use it only as a regularizer?  

In the no reconstruction loss experiments do you still sample \tilde{b}_t in the generative part?  Baselines where the \tilde{b}_t -> h_t edge is removed would be very nice.

It seems the Blizzard results in Figure 2 are missing no reconstruction loss + full backprop.

I don't understand the description of the ""Skip Gradient"" trick.  Exactly which gradients are you skipping at random?

Do you have any intuition for why it is sometimes necessary to set beta=0?
","[7, 6, 4]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Ok but not good enough - rejection']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with a somewhat negative point, mentioning a concern about a potentially incorrect derivation. However, the overall tone is inquisitive, suggesting areas of improvement and further investigation rather than outright dismissal. The reviewer asks for clarifications, proposes alternative approaches, and points out missing information, indicating a constructive approach to improving the paper. The language is formal and professional throughout.",-10.0,80.0
Understanding Deep Learning Generalization by Maximum Entropy,"['Guanhua Zheng', 'Jitao Sang', 'Changsheng Xu']",Reject,2018,"[2, 11, 20]","[4, 16, 25]","[4, 121, 571]","[0, 52, 307]","[4, 31, 55]","[0, 38, 209]","Summary:

This paper presents a derivation which links a DNN to recursive application of
maximum entropy model fitting. The mathematical notation is unclear, and in
one cases the lemmas are circular (i.e. two lemmas each assume the other is
correct for their proof). Additionally the main theorem requires complete
independence, but the second theorem provides pairwise independence, and the
two are not the same.

Major comments:

- The second condition of the maximum entropy equivalence theorem requires
  that all T are conditionally independent of Y. This statement is unclear, as
it could mean pairwise independence, or it could mean jointly independent
(i.e. for all pairs of non-overlapping subsets A & B of T I(T_A;T_B|Y) = 0).
This is the same as saying the mapping X->T is making each dimension of T
orthogonal, as otherwise it would introduce correlations. The proof of the
theorem assumes that pairwise independence induces joint independence and this
is not correct.

- Section 4.1 makes an analogy to EM, but gradient descent is not like this
  process as all the parameters are updated at once, and only optimised by a
single (noisy) step. The optimisation with respect to a single layer is
conditional on all the other layers remaining fixed, but the gradient
information is stale (as it knows about the previous step of the parameters in
the layer above). This means that gradient descent does all 1..L steps in
parallel, and this is different to the definition given.

- The proofs in Appendix C which are used for the statement I(T_i;T_j) >=
  I(T_i;T_j|Y) are incomplete, and in generate this statement is not true, so
requires proof.

- Lemma 1 appears to assume Lemma 2, and Lemma 2 appears to assume Lemma 1.
  Either these lemmas are circular or the derivations of both of them are
unclear.

- In Lemma 3 what is the minimum taken over for the left hand side? Elsewhere
  the minimum is taken over T, but T does not appear on the left hand side.
Explicit minimums help the reader to follow the logic, and implicit ones
should only be used when it is obvious what the minimum is over.

- In Lemma 5, what does ""T is only related to X"" mean? The proof states that
  Y -> T -> X forms a Markov chain, but this implies that T is a function of
Y, not X.

Minor comments:

- I assume that the E_{P(X,Y)} notation is the expectation of that probability
  distribution, but this notation is uncommon, and should be replaced with a
more explicit one.

- Markov is usually romanized with a ""k"" not a ""c"".

- The paper is missing numerous prepositions and articles, and contains
  multiple spelling mistakes & typos.","[2, 3, 6]","[' Strong rejection', ' Clear rejection', ' Marginally above acceptance threshold']","[3, 3, 2]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']","The review is highly critical of the paper, pointing out significant flaws in the mathematical derivations and clarity. The reviewer lists several major concerns, including circular lemmas, incorrect assumptions about independence, and unclear analogies. The tone, while direct and critical, maintains a professional and objective stance, focusing on the paper's content rather than resorting to personal attacks.",-60.0,50.0
PACT: Parameterized Clipping Activation for Quantized Neural Networks,"['Jungwook Choi', 'Zhuo Wang', 'Swagath Venkataramani', 'Pierce I-Jen Chuang', 'Vijayalakshmi Srinivasan', 'Kailash Gopalakrishnan']",Reject,2018,"[13, 15, 7, 10, 22, 11]","[18, 20, 11, 15, 26, 15]","[95, 205, 83, 39, 68, 47]","[53, 99, 59, 23, 50, 29]","[17, 8, 11, 10, 4, 12]","[25, 98, 13, 6, 14, 6]","This paper presents a new idea to use PACT to quantize networks, and showed improved compression and comparable accuracy to the original network. The idea is interesting and novel that PACT has not been applied to compressing networks in the past. The results from this paper is also promising that it showed convincing compression results. 

The experiments in this paper is also solid and has done extensive experiments on state of the art datasets and networks. Results look promising too.

Overall the paper is a descent one, but with limited novelty. I am a weak reject","[5, 5, 5]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer finds the paper interesting with promising results, indicating a positive sentiment. However, they mention ""limited novelty"" and ultimately give a ""weak reject"", suggesting a slightly less positive sentiment overall. The language used is polite and professional throughout, without any harsh or disrespectful phrasing.",40.0,80.0
Lifelong Word Embedding via Meta-Learning,"['Hu Xu', 'Bing Liu', 'Lei Shu', 'Philip S. Yu']",Reject,2018,"[8, 31, 6, 39]","[13, 36, 11, 44]","[67, 350, 53, 1879]","[29, 235, 24, 991]","[38, 67, 29, 389]","[0, 48, 0, 499]","This paper presents a lifelong learning method for learning word embeddings.  Given a new domain of interest, the method leverages previously seen domains in order to hopefully generate better embeddings compared to ones computed over just the new domain, or standard pre-trained embeddings.

The general problem space here -- how to leverage embeddings across several domains in order to improve performance in a given domain -- is important and relevant to ICLR.  However, this submission needs to be improved in terms of clarity and its experiments.

In terms of clarity, the paper has a large number of typos (I list a few at the end of this review) and more significantly, at several points in the paper is hard to tell what exactly was done and why.  When presenting algorithms, starting with an English description of the high-level goal and steps of the algorithm would be helpful.  What are the inputs and outputs of the meta-learner, and how will it be used to obtain embeddings for the new domain?  The paper states the purpose of the meta learning is ""to learn a general word context similarity from the first m domains"", but I was never sure what this meant.  Further, some of the paper's pseudocode includes unexplained steps like ""invert by domain index"" and ""scanco-occurrence"".  

In terms of the experiments, the paper is missing some important baselines that would help us understand how well the approach works.  First, besides the GloVe common crawl embeddings used here, there are several other embedding sets (including the other GloVe embeddings released along with the ones used here, and the Google News word2vec embeddings) that should be considered.  Also, the paper never considers concatenations of large pre-trained embedding sets with each other and/or with the new domain corpus -- such concatenations often give a big boost to accuracy, see :
""Think Globally, Embed Locally—Locally Linear Meta-embedding of Words"", Bollegala et al., 2017
https://arxiv.org/pdf/1709.06671.pdf

That paper is not peer reviewed to my knowledge so it is not necessary to compare against the new methods introduced there, but their baselines of concatenation of pre-trained embedding sets should be compared against in the submission.

Beyond trying other embeddings, the paper should also compare against simpler combination approaches, including simpler variants of its own approach.  What if we just selected the one past domain that was most similar to the new domain, by some measure?  And how does the performance of the technique depend on the setting of m?  Investigating some of these questions would help us understand how well the approach works and in which settings.

Minor:

Second paragraph, GloVec should be GloVe

""given many domains with uncertain noise for the new domain"" -- not clear what ""uncertain noise"" means, perhaps ""uncertain relevance"" would be more clear

The text refers to a Figure 3 which does not exist, probably means Figure 2.  I didn't understand the need for both figures, Figure 1 is almost contained within Figure 2

When m is introduced, it would help to say that m < n and justify why dividing the n domains into two chunks (of m and n-m domains) is necessary.

""from the first m domain corpus"" -> ""from the first m domains""?

""may not helpful"" -> ""may not be helpful""

""vocabularie"" -> ""vocabulary""

""system first retrieval"" -> ""system first retrieves""

COMMENTS ON REVISIONS: I appreciate the authors including the new experiments against concatenation baselines.  The concatenation does fairly comparably to LL in Tables 3&4.  LL wins by a bit more in Table 2.  Given these somewhat close/inconsistent wins, it would help the paper to include an explanation of why and under what conditions the LL approach will outperform concatenation.","[4, 3, 5]","[' Ok but not good enough - rejection', ' Clear rejection', ' Marginally below acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review acknowledges the relevance of the paper's topic and appreciates the authors' efforts in addressing it. However, it points out significant areas for improvement, particularly regarding clarity, missing baselines, and typos. The reviewer's tone is constructive and provides specific suggestions for improvement. The inclusion of ""I appreciate the authors including the new experiments..."" in the comments on revisions suggests a positive shift in sentiment after the authors addressed some concerns. Overall, the sentiment leans towards the neutral side, as the reviewer sees potential in the paper but requires substantial revisions.",-10.0,80.0
UNSUPERVISED SENTENCE EMBEDDING USING DOCUMENT STRUCTURE-BASED CONTEXT,"['Taesung Lee', 'Youngja Park']",Reject,2018,"[8, 18]","[12, 22]","[27, 59]","[17, 38]","[5, 7]","[5, 14]","This paper extends the idea of forming an unsupervised representation of sentences used in the SkipThought approach by using a broader set of evidence for forming the representation of a sentence. Rather than simply encoding the preceding sentence and then generating the next sentence, the model suggests that a whole bunch of related ""sentences"" could be encoded, including document title, section title, footnotes, hyperlinked sentences. This is a valid good idea and indeed improves results. The other main new and potentially useful idea is a new idea for handling OOVs in this context where they are represented by positional placeholder variables. This also seems helpful. The paper is able to show markedly better results on paraphrase detection that skipthought and some interesting and perhaps good results on domain-specific coreference resolution.

On the negative side, the model of the paper isn't very excitingly different. It's a fairly straightforward extension of the earlier SkipThought model to a situation where you have multiple generators of related text. There isn't a clear evaluation that shows the utility of the added OOV Handler, since the results with and without that handling aren't comparable. The OOV Handler is also related to positional encoding ideas that have been used in NMT but aren't reference. And the coreference experiment isn't that clearly described nor necessarily that meaningful. Finally, the finding of dependencies between sentences for the multiple generators is done in a rule-based fashion, which is okay and works, but not super neural and exciting.

Other comments:
 - p.3. Another related sentence you could possibly use is first sentence of paragraph related to all other sentences? (Works if people write paragraphs with a ""topic sentence"" at the beginning.
 - p.5. Notation seemed a bit non-standard. I thought most people use \sigma for a sigmoid (makes sense, right?), whereas you use it for a softmax and use calligraphic S for a sigmoid....
 - p.5. Section 5 suggests the standard way to do OOVs is to average all word vectors. That's one well-know way, but hardly the only way. A trained UNK encoding and use of things like character-level encoders is also quite common.
 - p.6. The basic idea of the OOV encoder seems a good one. In domain specific contexts, you want to be able to refer to and re-use words that appear in related sentences, since they are likely to appear again and you want to be able to generate them. A weakness of this section however is that it makes no reference to related work whatsoever. It seems like there's quite a bit of related work. The idea of using a positional encoding so that you can generate rare words by position has previously been used in NMT, e.g. Luong et al. (Google brain) (ACL 2015). More generally, a now quite common way to handle this problem is to use ""pointing"" or ""copying"", which appears in a number of papers. (e.g., Vinyals et al. 2015) and might also have been used here and might be expected to work too. 
 - p.7. Why such an old Wikipedia dump? Most people use a more recent one!
 - p.7. The paraphrase results seem good and prove the idea works. It's a shame they don't let you see the usefulness of the OOV model.
 - p.8. For various reasons, the coreference results seem less useful than they could have been, but they do show some value for the technique in the area of domain-specific coreference.

","[5, 5, 7]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Good paper, accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with positive remarks, highlighting the good ideas and improved results. However, the second paragraph introduces a more critical perspective, pointing out limitations and suggesting the novelty is limited. The reviewer acknowledges the work's value but also expresses some reservations. The tone remains professional and polite throughout, offering constructive criticism and suggestions for improvement.",20.0,70.0
CyCADA: Cycle-Consistent Adversarial Domain Adaptation,"['Judy Hoffman', 'Eric Tzeng', 'Taesung Park', 'Jun-Yan Zhu', 'Phillip Isola', 'Kate Saenko', 'Alyosha Efros', 'Trevor Darrell']",Reject,2018,"[8, 6, 16, 7, 8, 15, 25, 32]","[13, 8, 21, 12, 13, 20, 30, 37]","[112, 30, 156, 121, 126, 321, 249, 653]","[51, 14, 57, 47, 59, 158, 128, 376]","[57, 15, 12, 59, 62, 147, 82, 230]","[4, 1, 87, 15, 5, 16, 39, 47]","This paper proposes  a natural extension of the CycleGAN approach. This is achieved by leveraging the feature and semantic losses to achieve a more realistic image reconstruction. The experiments show that including these additional losses is critical for improving the models performance.  The paper is very well written and technical details are well described and motivated. It would be good to identify the cases where the model fails and comment on those. For instance, what if the source data cannot be well reconstructed from adapted target data? What are the bounds of the domain discrepancy in this case? ","[9, 5, 5]","[' Top 15% of accepted papers, strong accept', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[5, 5, 5]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review starts with positive statements, highlighting the value of the paper's proposal and clarity. While it points out an area for improvement, it frames it as a suggestion rather than a harsh criticism. The language used is constructive and professional throughout.",60.0,80.0
Learning Graph Convolution Filters from Data Manifold,"['Guokun Lai', 'Hanxiao Liu', 'Yiming Yang']",Reject,2018,"[5, 4, 35]","[7, 9, 40]","[18, 74, 305]","[8, 35, 173]","[10, 33, 88]","[0, 6, 44]","The paper presents an extension of the Xception network of (Chollet et al. 2016) 2D grids to generic graphs. The Xception network decouples the spatial correlations from depth channels correlations by having separate weights for each depth channel. The weights within a depth channel is shared thus maintaining the stationary requirement. The proposed filter relaxes this requirement by forming the weights as the output of a two-layer perception. 

The paper includes a detailed comparison of the existing formulations from the traditional label propagation scheme to more recent more graph convolutions (Kipf & Welling, 2016 ) and geometric convolutions  (Monti et al. 2016). 

The paper provides quantitative evaluations under three different settings i) image classification, ii) Time series forcasting iii) Document classification. The proposed method out-performs all other graph convolutions on all the tasks (except image classification) though having comparable or less number of parameters. For image classification, the performance of proposed method is below its predecessor Xception network. 

Pros:
i) Detailed review of the existing work and comparison with the proposed work.
ii) The three experiments performed showed variety in terms of underlying graph structure hence provides a thorough evaluation of different methods under different settings.
iii) Superior performance with fewer number of parameters compared to other methods. 
Cons:
i) The architecture of the 2 layer MLP used to learn weights for a particular depth channel is not provided.
ii) The performance difference between Xception and proposed method for image classification experiments using CIFAR is incoherent with the intuitions provided Sec 3.1 as the proposed method have more parameters and is a generalized version of DSC.","[6, 5, 4]","[' Marginally above acceptance threshold', ' Marginally below acceptance threshold', ' Ok but not good enough - rejection']","[4, 3, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review provides a balanced assessment of the paper, outlining both its strengths (detailed review of existing work, thorough evaluation, superior performance) and weaknesses (missing architecture details, incoherent performance in one setting). The language is neutral, sticking to objective observations and constructive criticism.",50.0,50.0
Compact Neural Networks based on the Multiscale Entanglement Renormalization Ansatz,"['Andrew Hallam', 'Edward Grant', 'Vid Stojevic', 'Simone Severini', 'Andrew G. Green']",Reject,2018,"[2, 29, 2, 16, 2]","[2, 33, 3, 19, 2]","[2, 75, 3, 74, 2]","[1, 54, 1, 14, 1]","[1, 3, 2, 23, 1]","[0, 18, 0, 37, 0]","The authors study compressing feed forward layers using low rank tensor decompositions. For instance a feed forward layer of 4096 x 4096 would first be reshaped into a rank-12 tensor with each index having dimension 2, and then a tensor decomposition would be applied to reduce the number of parameters. 

Previous work used tensor trains which decompose the tensor as a chain. Here the authors explore a tree like decomposition. The authors only describe their model using pictures and do not provide any rigorous description of how their decomposition works.

The results are mediocre. While the author's approach does seem to reduce the feed forward net parameters by 30% compared to the tensor train decomposition for similar accuracy, the total number of parameters for both MERA (authors' approach) and Tensor Train is similar since in this regime the CNN parameters dominate (and the authors' approach does not work to compress those).

","[4, 5, 5]","[' Ok but not good enough - rejection', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with a neutral factual description of the work. However, the second paragraph criticizes the authors for not providing a rigorous description of their model. The third paragraph states the results are ""mediocre"" and that the author's approach ""does not work"" for compressing CNN parameters. The combination of these points suggests a negative sentiment. The language used is relatively neutral and professional, but phrases like ""mediocre"" and ""does not work"" carry a somewhat negative connotation. Overall, the tone is critical but not overtly rude.",-30.0,0.0
A Self-Training Method for Semi-Supervised GANs,"['Alan Do-Omri', 'Dalei Wu', 'Xiaohua Liu']",Reject,2018,"[2, 16, 18]","[7, 21, 23]","[8, 111, 113]","[3, 57, 67]","[5, 3, 6]","[0, 51, 40]","The paper presents to combine self-learning and GAN. The basic idea is to first use GAN to generate data, and then infer the pseudo label, and finally use the pseudo labeled data to enhance the learning process. Experiments are conducted on one image data set. The paper contains several deficiencies.

1.	The experiment is weak. Firstly, only one data set is employed for evaluation, which is hard to justify the applicability of the proposed approach. Secondly, the compared methods are too few and do not include many state-of-the-art SSL methods like graph-based approaches. Thirdly, in these cases, the results in table 1 contain evident redundancy. Fourthly, the performance improvement over compared method is not significant and the result is based on 3 splits of data set, which is obviously not convincing and involves large variance. 
2.	The paper claims that ‘when paired with deep, semi-supervised learning has had a few success’. I do not agree with such a claim. There are many success SSL deep learning studies on embedding. They are not included in the discussions. 
3.	The layout of the paper could be improved. For example, there are too many empty spaces in the paper. 
4.	Overall technically the proposed approach is a bit straightforward and does not bring too much novelty.
5.	The format of references is not consistent. For example, some conference has short name, while some does not have. ","[4, 3, 3]","[' Ok but not good enough - rejection', ' Clear rejection', ' Clear rejection']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review starts with a neutral statement, outlining the paper's topic. However, it quickly transitions into a predominantly negative assessment using terms like ""several deficiencies,"" ""weak,"" ""not convincing,"" ""straightforward,"" and ""not too much novelty."" The reviewer lists several specific concerns, indicating a critical stance. While the critique is direct, it avoids harsh or disrespectful language, maintaining a professional tone.",-50.0,50.0
XGAN: Unsupervised Image-to-Image Translation for many-to-many Mappings,"['Amelie Royer', 'Konstantinos Bousmalis', 'Stephan Gouws', 'Fred Bertsch', 'Inbar Mosseri', 'Forrester Cole', 'Kevin Murphy']",Reject,2018,"[4, 15, 9, 2, 6, 13, 11]","[9, 19, 11, 6, 11, 17, 16]","[14, 36, 16, 9, 24, 49, 31]","[8, 17, 9, 3, 10, 23, 15]","[6, 14, 7, 6, 12, 16, 14]","[0, 5, 0, 0, 2, 10, 2]","This paper proposed an X-shaped GAN for the so called semantic style transfer task, in which the goal is to transfer the style of an image from one domain to another without altering the semantic content of the image. Here, a domain is collectively defined by the images of the same style, e.g., cartoon faces. 

The cost function used to train the network consists of five terms of which four are pretty standard: a reconstruction loss, two regular GAN-type losses, and an imitation loss. The fifth term, called the semantic consistency loss, is one of the main contributions of this paper. This loss ensures that the translated images should be encoded into about the same location as the embedding of the original image, albeit by different encoders. 

Strengths:
1. The new CartoonSet dataset is carefully designed and compiled. It could facilitate the future research on style transfer. 
2. The paper is very well written. I enjoyed reading the paper. The text is concise and also clear enough and the figures are illustrative.
3. The semantic consistency loss is reasonable, but I do not think this is significantly novel. 

Weaknesses:
1. Although “the key aim of XGAN is to learn a joint meaningful and semantically consistent embedding”, the experiments are actually devoted to the qualitative style transfer only. A possible experiment design for evaluating “the key aim of XGAN” may be the facial attribute prediction. The CartoonSet contains attribute labels but the authors may need collect such labels for the VGG-face set.
2. Only one baseline is considered in the style transfer experiments. Both CycleGAN and UNIT are very competitive methods and would be better be included in the comparison. 
3. The “many-to-many” is ambiguous. Style transfer in general is not a one-to-one or many-to-one mapping. It is not necessary to stress the many-to-many property of the proposed new task, i.e., semantic style transfer. 

The CartoonSet dataset and the new task, which is called semantic style transfer between two domains, are nice contributions of this paper. In terms of technical contributions, it is not significant to have the X-shaped GAN or the straightforward semantic consistency loss. The experiments are somehow mismatched with the claimed aim of the paper. ","[4, 4, 3]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Clear rejection']","[4, 3, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The reviewer acknowledges the strengths of the paper, such as the new dataset, the writing quality, and the reasonable semantic consistency loss. However, they also point out weaknesses, including the lack of quantitative evaluation for the claimed aim, limited baselines, and ambiguous use of ""many-to-many."" Overall, the review leans towards the positive side due to the valuable contributions but expresses reservations about the technical novelty and experimental validation. ",60.0,80.0
The Cramer Distance as a Solution to Biased Wasserstein Gradients,"['Marc G. Bellemare', 'Ivo Danihelka', 'Will Dabney', 'Shakir Mohamed', 'Balaji Lakshminarayanan', 'Stephan Hoyer', 'Remi Munos']",Reject,2018,"[12, 10, 2, 13, 10, 2, 23]","[17, 14, 7, 17, 15, 6, 28]","[102, 28, 67, 66, 90, 16, 266]","[45, 12, 30, 27, 28, 2, 143]","[51, 14, 36, 33, 56, 12, 96]","[6, 2, 1, 6, 6, 2, 27]","The manuscript proposes to use the Cramer distance as a measure between distributions (acting as a loss) when optimizing
an objective function using stochastic gradient descent (SGD). Cramer distance is a Bregman divergence and is a member of the Lp family of divergences.  Here a ""distance"" means a symmetric divergence measure that satisfies the relaxed triangle inequality. The motivation for using the Cramer distance is that it has unbiased sample gradients while still enjoying some other properties such as scale sensitivity and sum invariant. The authors also proof that for the Bernoulli distribution, there is a lower bound independent of the sample size for the deviation between the gradient of the Cramer distance, and the expectation of the estimated gradient of the Cramer distance. Then, the multivariate case of the Cramer distance, called the energy distance, is also briefly presented. The paper closes with some experiments on ordinal regression using neural networks and training GANs using the Cramer distance. 

In general, the manuscript is well written and the ideas are smoothly presented. While the manuscript gives some interesting insights, I find that the contribution could have been explained in a more broader sense, with a stronger compelling message.

Some remarks and questions:

1.	The KL divergence considered here is sum invariant but not scale sensitive, and has unbiased sample gradients. The 
	authors are considering here the standard (asymmetric) KL divergence (sec. 2.1). Is it the case that losing scale
	sensitivity make the KL divergence insensitive to the geometry of the outcomes? or is it due to the fact the KL 
	divergence is not symmetric? or ?

2.	The main argument for the paper is that the simple sample-based estimate for the gradient using the Wasserstein 
	metric is a biased estimate for the true gradient of the Wasserstein distance, and hence it is not favored with
	SGD-type algorithms. Are there any other estimators in the literature for the gradient of the Wasserstein distance?
	Was this issue overlooked in the literature?

3.	I am not sure if a biased estimate for the gradient will lead to a ``wrong minimum'' in an energy space that has 
	infinitely many local minima.  Of course one should use an unbiased estimate for the gradient whenever this is possible.
	However, even when this is possible, there is no guarantee that this will consistently lead to deeper and ``better''
	minima, and there is no guarantee as well that these deep local minima reflect meaningful results.

4.	To what extent can one generalize theorem 1 to other probability distributions (continuous and discrete) and to the 
	multivariate cases as well?

5.	I also don't think that the example given in sec. 4.2 and depicted in Fig. 1 is the best and simplest way to illustrate
	the benefit of Cramer distance over Wasserstein. Similarly, the experiments for the multivariate case using GANs and
	Neural Networks do not really deliver tangible, concrete and conclusive results. Partly, these results are very  
       qualitative, which can be understood within the context of GANs. However, the authors could have used other       
       models/algorithms where they can obtain concrete quantitative results (for this type of contribution). In addition, 
	such sophisticated models (with various hyper-parameters) can mask the true benefit for the Cramer distance, and can 
	also mask the extent of how good/poor is the sample estimate for the Wasserstein gradient.","[5, 4, 7]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Good paper, accept']","[3, 5, 2]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']","The reviewer provides constructive criticism, acknowledges the manuscript's strengths (well-written, interesting insights), but also points out weaknesses (limited contribution, lack of strong conclusions in experiments). The tone is professional and typical of academic peer review, using direct language but not resorting to personal attacks or disrespectful phrasing.",40.0,70.0
Rethinking generalization requires revisiting old ideas: statistical mechanics approaches and complex learning behavior,"['Charles H. Martin', 'Michael W. Mahoney']",Reject,2018,"[2, 16]","[6, 21]","[11, 364]","[3, 128]","[7, 178]","[1, 58]","I find myself having a very hard time making a review of this paper,  because I mostly agree with the intro and discussion, and certainly agree that the ""typical"" versus ""worse case"" analysis is certainly an important point.  The authors are making a strong case for the use of these models to understand overfitting and generalization in deep leaning.

The problem is however that, except from advocating the use of these ""spin glass"" models studied back in the days by Seung, Sompolinksy, Opper and others, there are little new results presented in the paper. The arguments using the Very Simple Deep Learning (VSDL) are essentially a review of old known results --which I agree should maybe be revisited-- and the motivation to their application to deep learning stems from the reasoning  that, since this is the behavior observed in all these model, well then deep learning should behave just the same as well. This might very well be, but this is precisly the point: is it ? 

After reading the paper,  I agree with many points and enjoyed reading the discussion. I found interesting ideas discussed and many papers reviewed, and ended up discovering interesting papers on arxiv as a concequence.

This is all nice, interesting, and well written, but at the end of the day, the paper is not doing too much beyond being a nice review of all ideas. While this has indeed some values, and might trigger a renewal of interested for these approaches, I will let the comity decide if this is the material they want in ICLR.

A minor comment: The generalization result of [9,11] obtained with heuristic tools (the replica method of statistical mechanics) and plotted in Fig.1 (a) has been proven recently with rigorous mathematical methods in arxiv:1708.03395 

Another remark:  if deep learning is indeed well described by these models, then again so are many other simpler problems, such as compressed sensing, matrix and tensor factorization, error corrections, etc etc... with similar phase diagram as in fig. 1.  For instance gaussian mixtures are discussed in http://iopscience.iop.org/article/10.1088/0305-4470/27/6/016/and  SVM (which the authors argue should behave quite differently) methods have been treated by statistical mechanics tools in https://arxiv.org/pdf/cond-mat/9811421.pdf with similar phase diagrams. I am a bit confused what would be so special about deep learning then?
","[7, 6, 3]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Clear rejection']","[4, 5, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer expresses a mixed sentiment. They appreciate the paper's topic, clarity, and the review of relevant literature. However, they find the paper lacking in novel contributions beyond being a well-written summary of existing work. The reviewer acknowledges the value of such a review but questions its suitability for ICLR. The tone remains polite throughout, offering constructive criticism and suggestions rather than harsh judgments.",20.0,80.0
Nearest Neighbour Radial Basis Function Solvers for Deep Neural Networks,"['Benjamin J. Meyer', 'Ben Harwood', 'Tom Drummond']",Reject,2018,"[2, 3, 20]","[7, 6, 25]","[9, 14, 200]","[4, 7, 121]","[4, 6, 55]","[1, 1, 24]","(Summary)
This paper proposes weighted RBF distance based loss function where embeddings for cluster centroids and data are learned and used for class probabilities (eqn 3). The authors experiment on CUB200-2011, Cars106, Oxford 102 Flowers datasets.

(Pros)
The citations and related works cover fairly comprehensive and up-to-date literatures on deep metric learning.

(Cons)
The proposed method is unlikely to scale with respect to the number of classes. ""..our approach is also free to create multiple clusters for each class.."" This makes it unfair to deep metric learning baselines in figures 2 and 3 because DMP baselines has memory footprint constant in the number of classes. In contrast, the proposed method have linear memory footprint in the number of classes. Furthermore, the authors ommit how many centroids are used in each experiments.

(Assessment)
Marginally below acceptance threshold. The method is unlikely to scale and the important details on how many centroids the authors used in each experiments is omitted.","[5, 4, 3]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Clear rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is mostly critical of the paper, pointing out a significant limitation in scalability. While it acknowledges the positive aspect of comprehensive literature coverage, the overall tone and the use of phrases like ""unlikely to scale"" and ""makes it unfair"" suggest a negative sentiment. However, the language remains professional and avoids harsh or disrespectful terms, indicating a neutral politeness level.",-30.0,0.0
Prototype Matching Networks for Large-Scale Multi-label  Genomic Sequence Classification,"['Jack Lanchantin', 'Arshdeep Sekhon', 'Ritambhara Singh', 'Yanjun Qi']",Reject,2018,"[3, 2, 3, 18]","[8, 7, 7, 23]","[27, 26, 34, 124]","[10, 11, 10, 65]","[15, 14, 13, 47]","[2, 1, 11, 12]","The authors of this manuscript proposed a model called PMN based on previous works for the classification of transcription factor binding. Overall, this manuscript is not well written. Clarification is needed in the method and data sections. The model itself is an incremental work, but the application is novel. My specific concerns are given below.

1. It is unclear how the prototype of a TF is learned. Detailed explanation is necessary. 

2. Why did the authors only allow a TF to have only one prototype? A TF can have multiple distinct motifs.

3. Why peaks with p-value>=1 were defined as positive? Were negative classes considered in the computational experiments?

4. What's the relationship between the LSTM component in the proposed method and sparse coding?

5. The manuscript contains lots of low-end issues, such as:
5.1. Inconsistency in the format when referring to equations (eq. equation, Equation, attention LSTM, attentionLSTM, t and T etc);
5.2. Some ""0""s are missing in Table 3;
5.3. L2 should be L_2 norm; 
5.4. euclidean -> Euclidean; pvalue-> p-value;
5.5. Some author name and year citations in the manuscript should be put in brackets;
5.6. The ENCODE paper should be cited properly, (""Consortium et al., 2012"" is weird!) ;
5.7. The references should be carefully reformatted, for example, some words in the references should be in uppercase (e.g. DNA, JASPER, CNN etc.), some items are duplicated, ...

Comments for the revised manuscript: I decide to keep my decision as it is. My major and minor concerns are not fully well addressed in the revised paper. ","[5, 5, 5]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[5, 4, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer finds the paper to contain novel application but is not satisfied with the clarity and presentation. They list several major and minor concerns, indicating a negative sentiment. However, the language, while direct, maintains a professional and critical tone without resorting to personal attacks or overly harsh wording.",-30.0,60.0
When and where do feed-forward neural networks learn localist representations?,"['Ella M. Gale', 'Nicolas Martin', 'Jeffrey Bowers']",Reject,2018,"[8, 23, 10]","[13, 27, 15]","[51, 37, 32]","[14, 31, 12]","[29, 1, 12]","[8, 5, 8]","Quality and Clarity
The neural networks and neural codes are studied  in a concise way, most of the paper is clear. The section on data design, p3, could use some additional clarification wrt to how the data input is encoded (right now, it is hard to understand exactly what happens). 

Originality
I am not aware of other studies on this topic, the proposed approach seems original. 

Significance
The biggest problem I have is with the significance: I don't see at all how finding somewhat localized responses in the hidden layer of an MLP with just one hidden layer has any bearing on deeper networks structured as CNNs: compared to MLPs, neurons in CNNs have much smaller receptive fields, and are known to be sensitive to selective and distinct  features.  

Overall the results seem rather trivial without greater implications for modern deep neural networks: ie, of course adding dropout improves the degree of localist coding (sec 3.4). Similarly, for a larger network, you will find fewer localist codes (though this is hard to judge, as an exact definition of selectivity is missing). 

Minor issues: the ""selectivity"" p3 is not properly defined.  On p3, a figure is undefined. 
Typo: p2: ""could as be"". 
Many of the references are ugly : p3,  ""in kerasChollet (2015)"", this needs fixing. ","[3, 3, 5]","[' Clear rejection', ' Clear rejection', ' Marginally below acceptance threshold']","[5, 3, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with a positive sentiment, highlighting clarity and originality. However, it quickly shifts to a critical tone, questioning the significance and generalizability of the findings. The reviewer finds the results 'trivial' and lacking implications for modern networks. The use of 'of course' and the overall questioning of the study's impact indicate a negative sentiment. The language remains professional and polite throughout, offering constructive criticism and specific suggestions for improvement.",-30.0,70.0
Pixel Deconvolutional Networks,"['Hongyang Gao', 'Hao Yuan', 'Zhengyang Wang', 'Shuiwang Ji']",Reject,2018,"[2, 17, 7, 12]","[6, 22, 12, 17]","[37, 155, 58, 224]","[14, 76, 15, 85]","[18, 21, 21, 76]","[5, 58, 22, 63]","This paper is well written and easy to follow. The authors propose pixel deconvolutional layers for convolutional neural networks. The motivation of the proposed method, PixelDCL, is to remove the checkerboard effect of deconvolutoinal layers. 
The method consists of adding direct dependencies among the intermediate feature maps generated by the deconv layer. PixelDCL is applied sequentially, therefore it is slower than the original deconvolutional layer. The authors evaluate the model in two different problems: semantic segmentation (on PASCAL VOC and MSCOCO datasets) and in image generation VAE (with the CelebA dataset). 

The authors justify the proposed method as a way to alleviate the checkerboard effect (while introducing more complexity to the model and making it slower). In the experimental section, however, they do not compare with other approaches to do so For example, the upsampling+conv approach, which has been shown to remove the checkerboard effect while being more efficient than the proposed method (as it does not require any sequential computation). Moreover, the PixelDCL does not seem to bring substantial improvements on DeepLab (a state-of-the-art semantic segmentation algorithm). More comments and further exploration on this results should be done. Why no performance boost? Is it because of the residual connection? Or other component of DeepLab? Is the proposed layer really useful once a powerful model is used?

I also think the experiments on VAE are not conclusive. The authors simply show set of generated images. First, it is difficult to see the different of the image generated using deconv and PixelDCL. Second, a set of 20 qualitative images does not (and cannot) validate any research idea.","[5, 6, 5]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Marginally below acceptance threshold']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with a positive sentiment, acknowledging the paper's clarity and the authors' proposal. However, it transitions into a more critical tone, pointing out the lack of comparison with existing methods and unconvincing results. The reviewer raises valid concerns about the method's effectiveness and questions its practical value. The language remains professional and focused on the research's merit, suggesting areas for improvement without resorting to personal attacks.",20.0,60.0
Unbiasing Truncated Backpropagation Through Time,"['Corentin Tallec', 'Yann Ollivier']",Reject,2018,"[2, 16]","[6, 20]","[25, 47]","[10, 15]","[15, 28]","[0, 4]","This is an interesting paper.

It is well known that TBPTT is biased because of a fixed truncation length. The authors propose to make it  unbiased by sampling different truncation lengths and hence changing  the optimization procedure which corresponds to adding noise in the gradient estimates which leads to  unbiased gradients. 

Pros:

- Its a well written and easy to follow paper.
- If I understand correctly, they are changing the optimization procedure so that the proposed approach is able to find a local minima, which was not possible by using truncated backpropagation through time.  
- Its interesting to see in there PTB results that they get better validation score as compared to truncated BPTT.

Cons: 

- Though the approach is interesting, the results are quite preliminary. And given the fact there results are worse than the LSTM baseline (1.40 v/s 1.38). The authors note that it might be because of they are applying without sub-sequence shuffling. 

- I'm not convinced of the approach yet. The authors could do some large scale experiments on datasets like Text8 or speech modelling. 


Few points

- If I'm correct that the proposed approach indeed changes the optimization procedure, than I'd like to know what the authors think about exposure bias issue. Its a well known[1, 2] that we can't sample from RNN's for more number of steps, than what we used for trained (difference b/w teacher forcing and free running RNN). I'd like to know how does there method perform in such a regime (where you sample for more number of steps than you have trained for)

- Another thing, I'd like to see is the results of this model as compared to truncated backpropagation when you increase the sequence length. For example, Lets say you are doing language modelling on PTB, how the result varies when you change the length of the input sequence. I'd like to see a graph where on X axis is the length of the input sequence and on the Y axis is the bpc score (for PTB) and how does it compare to truncated backpropagation through time. 

-  PTB dataset has still not very long term dependencies, so I'm curious what the authors think about using there method for something like speech modelling or some large scale experiments.

- I'd expect the proposed approach to be more computationally expensive as compared to Truncated Back-propagation through time. I dont think the authors mentioned this somewhere in the paper. How much time does a single update takes as compared to Truncated Back-propagation through time ?

- Does the proposed approach help in flow of gradients?  

- In practice for training RNN's people use gradient clipping which also makes the gradient biased. Can the proposed method be used for training longer sequences?  

[1] Scheduled Sampling For Sequence Prediction with RNN's https://arxiv.org/abs/1506.03099
[2] Professor Forcing  https://arxiv.org/abs/1610.09038


Overall, Its an interesting paper which requires some more analysis to be published in this conference. I'd be very happy to increase my score if the authors can provide me results what I have asked for. ","[5, 6, 5]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Marginally below acceptance threshold']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer finds the paper interesting and acknowledges its merits (well-written, addresses a known issue, interesting results on PTB). However, they also express concerns about the preliminary nature of the results, particularly given the performance compared to the LSTM baseline. The reviewer's request for further experiments and analysis, along with their willingness to increase the score based on the results, suggests a slightly positive but cautious stance.",20.0,80.0
Post-training for Deep Learning,"['Thomas Moreau', 'Julien Audiffren']",Reject,2018,"[-1, 9]","[1, 14]","[1, 61]","[1, 24]","[0, 24]","[0, 13]","This paper demonstrate that by freezing all the penultimate layers at the end of regular training improves generalization. However, the results do not convince this reviewer to switch to using 'post-training'.

Learning features and then use a classifier such as a softmax or SVM is not new and were actually widely used 10 years ago. However, freezing the layers and continue to train the last layer is of a minor novelty. The results of the paper show a generalization gain in terms of better test time performance, however, it seems like the gain could be due to the \lambda term which is added for post-training but not added for the baseline. c.f. Eq 3 and Eq 4.
Therefore, it's unclear whether the gain in generalization is due to an additional \lambda term or from the post-training training itself.

A way to improve the paper and be more convincing would be to obtain the state-of-the-art results with post-training that's not possible otherwise.

Other notes, 

Remark 1: While it is true that dropout would change the feature function, to say that dropout 'should not be' applied, it would be good to support that statement with some experiments.

For table 1, please use decimal points instead of commas.
","[4, 3, 5]","[' Ok but not good enough - rejection', ' Clear rejection', ' Marginally below acceptance threshold']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer acknowledges the novelty of the paper's approach but expresses skepticism regarding the results. They point out a potential flaw in the experimental setup (the use of different lambda terms) that could explain the observed improvements. The reviewer suggests additional experiments to strengthen the paper's claims. The language used is critical but professional and not overtly negative. Therefore, the sentiment is somewhat negative, but not extremely so, and the politeness remains neutral.",-30.0,0.0
Contextual Explanation Networks,"['Maruan Al-Shedivat', 'Avinava Dubey', 'Eric P. Xing']",Reject,2018,"[5, 10, 18]","[9, 15, 23]","[42, 49, 625]","[19, 27, 339]","[20, 19, 218]","[3, 3, 68]","The article ""Contextual Explanation Networks"" introduces the class of models which learn the intermediate explanations in order to make final predictions. The contexts can be learned by, in principle, any model including neural networks, while the final predictions are supposed to be made by some simple models like linear ones. The probabilistic model allows for the simultaneous training of explanation and prediction parts as opposed to some recent post-hoc methods.

The experimental part of the paper considers variety of experiments, including classification on MNIST, CIFAR-10, IMDB and also some experiments on survival analysis. I should note, that the quality of the algorithm is in general similar to other methods considered (as expected). However, while in some cases the CEN algorithm is slightly better, in other cases it appears to sufficiently loose, see for example left part of Figure 3(b) for MNIST data set. It would be interesting to know the explanation. Also, it would be interesting to have more examples of qualitative analysis to see, that the learned explanations are really useful. I am a bit worried, that while we have interpretability with respect to intermediate features, these features theirselves might be very hard to interpret.

To sum up, I think that the general idea looks very natural and the results are quite supportive. However, I don't feel myself confident enough in this area of research to make strong conclusion on the quality of the paper.","[6, 6, 6]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[2, 5, 3]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer provides a generally positive overview of the research paper, highlighting its interesting approach and supportive results. However, they also express some reservations, noting that the algorithm's performance is not consistently superior to other methods and calling for further qualitative analysis to confirm the usefulness of the learned explanations. The reviewer's use of phrases like ""very natural"" and ""quite supportive"" suggests a positive inclination, while expressions like ""a bit worried"" and ""don't feel myself confident enough"" indicate some remaining concerns. The language used is constructive and professional throughout the review.",60.0,80.0
Jointly Learning Sentence Embeddings and Syntax with Unsupervised Tree-LSTMs,"['Jean Maillard', 'Stephen Clark', 'Dani Yogatama']",Reject,2018,"[4, 20, 10]","[9, 25, 14]","[22, 139, 74]","[10, 94, 33]","[9, 28, 34]","[3, 17, 7]","This paper proposes to jointly learning a semantic objective and inducing a binary tree structure for word composition, which is similar to (Yogatama et al, 2017). Differently from (Yogatama et al, 2017), this paper doesn’t use reinforcement learning to induce a hard structure, but adopts a chart parser manner and basically learns all the possible binary parse trees in a soft way. 

Overall, I think it is really an interesting direction and the proposed method sounds reasonable. However, I am concerned about the following points:  

- The improvements are really limited on both the SNLI and the Reverse Dictionary tasks. (Yogatama et al, 2017) demonstrate results on 5 tasks and I think it’d be helpful to present results on a diverse set of tasks and see if conclusions can generally hold. Also, it would be much better to have a direct comparison to (Yogatama et al, 2017), including the performance and also the induced tree structures.

- The computational complexity of this model shouldn’t be neglected. If I understand it correctly, the model needs to compute O(N^3) LSTM compositions. This should be at least discussed in the paper. And I am not also sure how hard this model is being converged in all experiments (compared to LSTM or supervised tree-LSTM).

- I am wondering about the effects of the temperature parameter t. Is that important for training?

Minor:
- What is the difference between LSTM and left-branching LSTM?
- I am not sure if the attention overt chart is a highlight of the paper or not. If so, better move that part to the models section instead of mention it briefly in the experiments section. Also, if any visualization (over the chart) can be provided, that’d be helpful to understand what is going on. 
","[5, 6, 4]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Ok but not good enough - rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer finds the direction of the paper interesting and the method reasonable, which points towards a positive sentiment. However, they also express several concerns and ask for major improvements. Therefore, the sentiment is not overly positive. The language used is constructive and professional, suggesting a polite tone.",50.0,75.0
Stochastic Training of Graph Convolutional Networks,"['Jianfei Chen', 'Jun Zhu']",Reject,2018,"[6, 14, 15]","[11, 19, 20]","[40, 468, 343]","[16, 204, 185]","[19, 211, 122]","[5, 53, 36]","The paper proposes a method to speed up the training of graph convolutional networks, which are quite slow for large graphs. The key insight is to improve the estimates of the average neighbor activations (via neighbor sampling) so that we can either sample less neighbors or have higher accuracy for the same number of sampled neighbors. The idea is quite simple: estimate the current average neighbor activations as a delta over the minibatch running average. I was hoping the method would also include importance sampling, but it doesn’t. The assumption that activations in a graph convolution are independent Gaussians is quite odd (and unproven). 

Quality: Statistically, the paper seems sound. There are some odd assumptions (independent Gaussian activations in a graph convolution embedding?!?) but otherwise the proposed methodology is rather straightforward. 

Clarity: It is well written and the reader is able to follow most of the details. I wish the authors had spent more time discussing the independent Gaussian assumption, rather than just arguing that a graph convolution (where units are not interacting through a simple grid like in a CNN) is equivalent to the setting of Wang and Manning (I don’t see the equivalence). Wang and Manning are looking at MLPs, not even CNNs, which clearly have more independent activations than a CNN or a graph convolution. 

Significance: Not very significant. The problem of computing better averages for a specific problem (neighbor embedding average) seems a bit too narrow. The solution is straightforward, while some of the approximations make some odd simplifying assumptions (independent activations in a convolution, infinitesimal learning rates). 

Theorem 2 is not too useful, unfortunately: Showing that the estimated gradient is asymptotically unbiased with learning rates approaching zero over Lipchitz functions does not seem like an useful statement. Learning rates will never be close enough to zero (specially for large batch sizes). And if the running activation average converges to the true value, the training is probably over. The method should show it helps when the values are oscillating in the early stages of the training, not when the training is done near the local optimum.


","[4, 3, 7]","[' Ok but not good enough - rejection', ' Clear rejection', ' Good paper, accept']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer acknowledges the paper addresses a relevant problem and is technically sound. However, they express concerns about the significance of the contribution, the narrowness of the problem addressed, and the strength of some assumptions. They find the central idea ""quite simple"" and question the validity and relevance of a key assumption. The reviewer also criticizes the practical usefulness of a presented theorem. The language is direct and critical, but not disrespectful.",20.0,60.0
Characterizing Sparse Connectivity Patterns in Neural Networks,"['Sourya Dey', 'Kuan-Wen Huang', 'Peter A. Beerel', 'Keith M. Chugg']",Reject,2018,"[3, 4, 28, 23]","[7, 5, 33, 27]","[21, 11, 205, 88]","[7, 5, 118, 44]","[11, 4, 45, 14]","[3, 2, 42, 30]","The paper seems to claims that
1) certain ConvNet architectures, particularly AlexNet and VGG, have too many parameters,
2) the sensible solution is leave the trunk of the ConvNet unchanged, and to randomly sparsify the top-most weight matrices.
I have two problems with these claims:
1) Modern ConvNet architectures (Inception, ResNeXt, SqueezeNet, BottleNeck-DenseNets and ShuffleNets) don't have large fully connected layers.
2) The authors reject the technique of 'Deep compression' as being impractical. I suspect it is actually much easier to use in practice as you don't have to a-priori know the correct level of sparsity for every level of the network.

p3. What does 'normalized' mean? Batch-norm?
p3. Are you using an L2 weight penalty? If not, your fully-connected baseline may be unnecessarily overfitting the training data.
p3. Table 1. Where do the choice of CL Junction densities come from? Did you do a grid search to find the optimal level of sparsity at each level?
p7-8. I had trouble following the left/right & front/back notation.
p8. Figure 7. How did you decide which data points to include in the plots?","[4, 4, 5]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[3, 3, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer expresses several concerns about the paper's claims, methodology, and clarity. They find flaws in the premise regarding ConvNet architectures and sparsity solutions. The reviewer also raises pointed questions about the technical details, suggesting potential weaknesses in the experimental setup and result interpretation. This suggests an overall negative sentiment. However, the language remains professional and avoids personal attacks, indicating a neutral politeness level.",-60.0,0.0
Learning Document Embeddings With CNNs,"['Shunan Zhao', 'Chundi Lui', 'Maksims Volkovs']",Reject,2018,"[2, 5, 12]","[3, 10, 17]","[4, 12, 43]","[2, 7, 27]","[2, 1, 14]","[0, 4, 2]","This paper uses CNNs to build document embeddings.  The main advantage over other methods is that CNNs are very fast.

First and foremost I think this: ""The code with the full model architecture will be released … and we thus omit going into further details here.""  is not acceptable.  Releasing code is commendable, but it is not a substitute for actually explaining what you have done.  This is especially true when the main contribution of the work is a network architecture.  If you're going to propose a specific architecture I expect you to actually tell me what it is.

I'm a bit confused by section 3.1 on language modelling.  I think the claim that it is showing ""a direct connection to language modelling"" and that ""we explore this relationship in detail"" are both very much overstated.  I think it would be more accurate to say this paper takes some tricks that people have used for language modelling and applies them to learning document embeddings.

This paper proposed both a model and a training objective, and I would have liked to see some attempt to disentangle their effect.  If there is indeed a direct connection between embedding models and language models then I would have also expected to see some feedback effect from document embedding to language modeling.  Does the embedding objective proposed here also lead to better language models?

Overall I do not see a substantial contribution from this paper. The main claims seem to be that CNNs are fast, and can be used for NLP, neither of which are new.
","[4, 6, 2]","[' Ok but not good enough - rejection', ' Marginally above acceptance threshold', ' Strong rejection']","[3, 4, 5]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review is quite negative. The reviewer criticizes the authors for not explaining their model architecture, overstating their claims, and not exploring the relationship between embedding models and language models thoroughly. The reviewer also states that the paper lacks a substantial contribution. While the language is direct and critical, it is within the bounds of professional academic feedback and doesn't resort to personal attacks or disrespectful language.",-60.0,40.0
Fixing Weight Decay Regularization in Adam,"['Ilya Loshchilov', 'Frank Hutter']",Reject,2018,"[9, 17]","[10, 22]","[41, 252]","[23, 103]","[16, 120]","[2, 29]","At the heart of the paper, there is a single idea: to decouple the weight decay from the number of steps taken by the optimization process (the paragraph at the end of page 2 is the key to the paper). This is an important and largely overlooked area of implementation and most off-the-shelf optimization algorithms, unfortunately, miss this point, too. I think that the proposed implementation should be taken seriously, especially in conjunction with the discussion that has been carried out with the work of Wilson et al., 2017 (https://arxiv.org/abs/1705.08292).

The introduction does a decent job explaining why it is necessary to pay attention to the norm of the weights as the training progresses within its scope. However, I would like to add a couple more points to the discussion: 
- ""Optimal weight decay is a function (among other things) of the total number of epochs / batch passes."" in principle, it is a function of weight updates. Clearly, it depends on the way the decay process is scheduled. However, there is a bad habit in DL where time is scaled by the number of epochs rather than the number of weight updates which sometimes lead to misleading plots (for instance, when comparing two algorithms with different batch sizes).
- Another ICLR 2018 submission has an interesting take on the norm of the weights and the algorithm (https://openreview.net/forum?id=HkmaTz-0W&noteId=HkmaTz-0W). Figure 3 shows the histograms of SGD/ADAM with and without WD (the *un-fixed* version), and it clearly shows how the landscape appear misleadingly different when one doesn't pay attention to the weight distribution in visualizations. 
- In figure 2, it appears that the training process has three phases, an initial decay, a steady progress, and a final decay that is more pronounced in AdamW. This final decay also correlates with the better test error of the proposed method. This third part also seems to correspond to the difference between Adam and AdamW through the way they branch out after following similar curves. One wonders what causes this branching and whether the key the desired effects are observed at the bottom of the landscape.
- The paper concludes with ""Advani & Saxe (2017) analytically showed that in the limited data regime of deep networks the presence of eigenvalues that are zero forms a frozen subspace in which no learning occurs and thus smaller (e.g., zero) initial weight norms should be used to achieve best generalization results."" Related to this there is another ICLR 2018 submission (https://openreview.net/forum?id=rJrTwxbCb), figure 1 shows that the eigenvalues of the Hessian of the loss have zero forms at the bottom of the landscape, not at the beginning. Back to the previous point, maybe that discussion should focus on the second and third phases of the training, not the beginning. 
- Finally, it would also be interesting to discuss the relation of the behavior of the weights at the last parts of the training and its connection to pruning. 

I'm aware that one can easily go beyond the scope of the paper by adding more material. Therefore, it is not completely reasonable to expect all such possible discussions to take place at once. The paper as it stands is reasonably self-contained and to the point. Just a minor last point that is irrelevant to the content of the work: The slash punctuation mark that is used to indicate 'or' should be used without spaces as in 'epochs/batch'.

Edit: Thanks very much for the updates and refinements. I stand by my original score and would like to indicate my support for this style of empirical work in scientific conferences.","[7, 8, 4]","[' Good paper, accept', ' Top 50% of accepted papers, clear accept', ' Ok but not good enough - rejection']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer acknowledges the importance of the paper's core idea and suggests it should be taken seriously. They provide constructive criticism and suggestions for improvement, indicating a positive attitude towards the paper. The language used is formal, respectful, and suggestive, not demanding, indicating politeness.",75.0,80.0
Transfer Learning to Learn with Multitask Neural Model Search,"['Catherine Wong', 'Andrea Gesmundo']",Reject,2018,"[3, 15, 5, 1, 33, 19]","[8, 20, 10, 2, 38, 23]","[39, 116, 70, 5, 481, 101]","[15, 58, 31, 3, 318, 44]","[21, 8, 27, 2, 76, 5]","[3, 50, 12, 0, 87, 52]","In this paper authors are summarizing their work on building a framework for automated neural network (NN) construction across multiple tasks simultaneously. 

They present initial results on the performance of their framework called Multitask Neural Model Search (MNMS) controller. The idea behind building such a framework is motivated by the successes of recently proposed reinforcement based approaches for finding the best NN architecture across the space of all possible architectures. Authors cite the Neural Architecture Search (NAS) framework as an example of such a framework that yields better results compared to NN architectures configured by humans. 

Overall I think that the idea is interesting and the work presented in this paper is very promising. Given the depth of the empirical analysis presented the work still feels that it’s in its early stages. In its current state and format the major issue with this work is the lack of more in-depth performance analysis which would help the reader draw more solid conclusions about the generalization of the approach.

Authors use two text classification tasks from the NLP domain to showcase the benefits of their proposed architecture. It would be good if they could expand and analyze how well does their framework generalizes across other non-binary tasks, tasks in other domains and different NNs. This is especially the case for the transfer learning task. 

In the NAS overview section, readers would benefit more if authors spend more time in outlining the RL detail used in the original NAS framework instead of Figure 1 which looks like a space filler. 

Across the two NLP tasks authors show that MNMS models trained simultaneously give better performance than hand tuned architectures. In addition, on the transfer learning evaluation approach they showcase the benefit of using the proposed framework in terms of the initially retrieved architecture and the number of iterations required to obtain the best performing one. 
For better clarity figures 3 and 5 should be made bigger. 
What is LSS in figure 4?","[4, 5, 7]","[' Ok but not good enough - rejection', ' Marginally below acceptance threshold', ' Good paper, accept']","[4, 2, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer finds the paper interesting and promising, highlighting its positive aspects. They provide constructive criticism, suggesting improvements like a deeper performance analysis and expansion to other tasks/domains. The reviewer also points out minor issues like figure clarity and a need for better explanation in certain sections. The language used is formal, respectful, and suggestive, not demanding, indicating politeness.",65.0,75.0
PrivyNet: A Flexible Framework for Privacy-Preserving Deep Neural Network Training,"['Meng Li', 'Liangzhen Lai', 'Naveen Suda', 'Vikas Chandra', 'David Z. Pan']",Reject,2018,"[4, 7, 6, 17, 22]","[9, 11, 10, 22, 27]","[73, 36, 25, 121, 421]","[37, 16, 10, 71, 268]","[24, 15, 11, 39, 46]","[12, 5, 4, 11, 107]","1. This is an interesting paper - introduces useful concepts such as the formulation of the utility and privacy loss functions with respect to the learning paradigm
2. From the initial part of the paper, it seems that the proposed PrivyNet is supposed to be a meta-learning framework to split a DNN in order to improve privacy while maintaining a certain accuracy level
3. However, the main issue is that the meta-learning mechanism is a bit ad-hoc and empirical - therefore not sure how seamless and user-friendly it will be in general, it seems it needs empirical studies for every new application - this basically involves generation of a pareto front and then choose pareto-optimal points based on the user's requirements, but it is unclear how a privy net construction based on some data set considered from the internet has the ability to transfer and help in maintaining privacy in another type of data set, e.g., social media pictures","[6, 5, 3]","[' Marginally above acceptance threshold', ' Marginally below acceptance threshold', ' Clear rejection']","[5, 3, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review starts with positive remarks, highlighting interesting aspects and potential of the paper. However, it then raises a significant concern about the ad-hoc and empirical nature of the core methodology, questioning its generalizability and user-friendliness. This concern significantly impacts the overall impression, making it more critical. The language used is professional and avoids harsh or disrespectful tones.",40.0,80.0
Distribution Regression Network,"['Connie Kou', 'Hwee Kuan Lee', 'Teck Khim Ng']",Reject,2018,"[1, 15, 21, 5, -2]","[5, 20, 26, 10, 2]","[20, 189, 263, 21, 6]","[11, 121, 109, 2, 2]","[8, 40, 39, 4, 2]","[1, 28, 115, 15, 2]","The paper considers distribution to distribution regression with MLPs.  The authors use an energy function based approach.  They test on a few problems, showing similar performance to other distribution to distribution alternatives, but requiring fewer parameters.

This seems to be a nice treatment of distribution to distribution regression with neural networks. The approach is methodological similar to using expected likelihood kernels.  While similar performance is achieved with fewer parameters, it would be more enlightening to consider accuracy vs runtime instead of accuracy vs parameters.  That’s what we really care about.  In a sense, because this problem has been considered several times in slightly different model classes, there really ought to be a pretty strong empirical investigation.  In the discussion, it says 
“For future work, a possible study is to investigate what classes of problems DRN can solve.”  It feels like in the present work there should have been an investigation about what classes of problems the DRN can solve.  Its practical utility is questionable.  It’s not clear how much value there is adding yet another distribution to distribution regression approach, this time with neural networks, without some pretty strong motivation (which seems to be lacking), as well as experiments.  In the introduction, it would also improve the paper to outline clear points of methodological novelty.  
","[5, 7, 7]","[' Marginally below acceptance threshold', ' Good paper, accept', ' Good paper, accept']","[4, 4, 2]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']","The review starts with a neutral summary of the paper's content. While it acknowledges the work as ""nice"" and methodologically sound, the reviewer quickly transitions into a series of concerns and suggestions for improvement. The reviewer questions the practical utility and novelty of the work, pushing for stronger empirical evidence and clearer motivation. Phrases like ""It’s not clear how much value there is"" and ""It feels like in the present work there should have been..."" indicate a lack of enthusiasm and a desire for more substantial contributions. Overall, the sentiment leans towards the critical side, suggesting areas for significant revision rather than outright rejection.",-20.0,60.0
Neural Compositional Denotational Semantics for Question Answering,"['Nitish Gupta', 'Mike Lewis']",Reject,2018,"[8, 12]","[13, 17]","[42, 116]","[20, 50]","[21, 61]","[1, 5]","This paper proposes for training a question answering model from answers only and a KB by learning latent trees that capture the syntax and learn the semantic of words, including referential terms like ""red"" and also compositional operators like ""not"".

I think this model is elegant, beautiful and timely. The authors do a good job of explaining it clearly. I like the modules of composition that seem to make a very intuitive sense for the ""algebra"" that is required and the parsing algorithm is clean. 

However, I think that the evaluation is lacking, and in some sense the model exposes the weakness of the dataset that it uses for evaluation.

I have 2.5 major issues with the paper and a few minor comments: 

Parsing:

* The authors don't really say what is the base case for \Psi that scores tokens (unless I missed it and if indeed it is missing it really needs to be added) and only provide the recursive case. From that I understand that the only features that they use are whether a certain word makes sense in a certain position of the rule application in the context of the question. While these features are based on Durrett et al.'s neural syntactic parser it seems like a pretty weak signal to learn from. This makes me wonder, how does the parser learn whether one parse is better than the other? Only based on this signal? It makes me suspicious that the distribution of language is not very ambiguous and that as long as you can construct a tree in some context you can do it in almost any other context. This is probably due to the fact that the CLEVR dataset was generated mostly using templates and is not really natural utterances produced by people. Of course many people have published on CLEVR although of its language limitations, but I was a bit surprised that only these features are enough to solve the problem completely, and this makes me curious as to how hard is it to reverse-engineer the way that the language was generated with a context-free mechanism that is similar to how the data was produced.

* Related to that is that the decision for a score of a certain type t for a span (i,j) is the sum for all possible rule applications, rather than a max, which again means that there is no competition between different parse trees that result with the same type of a single span. Can the authors say something about what the parser learns? Does it learn to extract from the noise clear parse trees? What is the distribution of rules in those sums? is there some rule that is more preferred than others usually? It seems like there is loss of information in the sum and it is unclear what is the effect of that in the paper.

Evaluation:

* Related to that is indeed the fact that they use CLEVR only. There  is now the Cornell NLVR dataset that is more challenging from a language perspective and it would be great to have an evaluation there as well. Also the authors only compare to 3 baselines where 2 don't even see the entire KB, so the only ""real"" baseline is relation net. The authors indeed state that it is state-of-the-art on clevr. 

* It is worth noting that relation net is reported to get 95.5 accuracy while the authors have 89.4. They use a subset so this might be the reason, but I am not sure how they compared to relation net exactly. Did they re-tune parameters once you have the new dataset? This could make a difference in the final accuracy and cause an unfair advantage.

* I would really appreciate more analysis on the trees that one gets. Are sub-trees interpretable? Can one trace the process of composition? This could have been really nice if one could do that. The authors have a figure of a purported tree, but where does this tree come from? From the mode? Form the authors?

Scalability:
* How much of a problem would it be to scale this? Will this work in larger domains? It seems they compute an attention score over every entity and also over a matrix that is squared in the number of entities. So it seems if the number of entities is large that could be very problematic. Once one moves to larger KBs it might become hard to maintain full differentiability which is one of the main selling points of the paper. 

Minor comments:
* I think the phrase ""attention"" is a bit confusing - I thought of a distribution over entities at first. 
* The feature function is not super clearly written I think - perhaps clarify in text a bit more what it does.
* I did not get how the denotation that is based on a specific rule applycation t_1 + t_2 --> t works. Is it by looking at the grounding that is the result of that rule application?
* Authors say that the neural enquirer and neural symbolic machines produce flat programs - that is not really true, the programs are just a linearized form of a tree, so there is nothing very flat about it in my opinion.

Overall, I really enjoyed reading the paper, but I was left wondering whether the fact that it works so well mostly attests to the way the data was generated and am still wondering how easy it would be to make this work in for more natural language or when the KB is large.


","[7, 4, 5]","[' Good paper, accept', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer expresses positive sentiment towards the paper, praising its elegance, clarity, and timeliness. They particularly appreciate the intuitive composition modules and parsing algorithm. However, they also raise significant concerns about the evaluation, particularly the reliance on the CLEVR dataset, which they believe might be too simplistic and easily reverse-engineered. The reviewer suggests using a more challenging dataset like the Cornell NLVR dataset for a more robust evaluation. They also point out the lack of comparison with a wider range of baselines and the need for more in-depth analysis of the generated trees. While acknowledging the paper's strengths, the reviewer's concerns about the evaluation and scalability limit the overall positivity of the review. The language used is polite and professional throughout, with constructive criticism offered in a respectful manner.",60.0,80.0
Deep Continuous Clustering,"['Sohil Atul Shah', 'Vladlen Koltun']",Reject,2018,"[14, 1, 1]","[19, 6, 5]","[144, 15, 4]","[57, 6, 3]","[60, 7, 1]","[27, 2, 0]","The authors proposed a new clustering algorithm named deep continuous clustering (DCC) that integrates autoencoder into continuous clustering. As a variant of  continuous clustering (RCC), DCC formed a global continuous objective for joint nonlinear dimensionality reduction and clustering. The objective can be directly optimized using SGD like method. Extensive experiments on image and document datasets show the effectiveness of DCC. However, part of experiments are not comprehensive enough. 

The idea of integrating autoencoder with continuous clustering is novel, and the optimization part is quite different. The trick used in the paper (sampling edges but not samples) looks interesting and seems to be effective. 

In the following, there are some detailed comments:
1. The paper is well written and easy to follow, except the definition of Geman-McClure function is missing. It is difficult to follow Eq. (6) and (7).
2. Compare DCC to RCC, the pros and cons are obvious. DCC does improve the performance of clustering with the cost of losing robustness. DCC is more sensitive to the hyper-parameters, especially embedding dimensionality d. With a wrong d DCC performs worse than RCC on MNIST and similar on Reuters. Since clustering is one unsupervised learning task. The author should consider heuristics to determine the hyper-parameters. This will increase the usability of the proposed method.
3. However, the comparison to the DL based partners are not comprehensive enough, especially JULE and DEPICT on image clustering. Firstly, the authors only reported AMI and ACC, but not NMI that is reported in JULE. For a fair comparison, NMI results should be included. Secondly, the reported results do not agree with the one in original publication. For example, JULE reported ACC of 0.964 and 0.684 on MNIST and YTF. However, in the appendix the numbers are 0.800 and 0.342 respectively. Compared to the reported number in JULE paper, DCC is not significantly better.

In general, the paper is interesting and proposed method seems to be promising. I would vote for accept if my concerns can be addressed.

The author's respond address part of my concerns, so I have adjusted my rating.","[7, 6, 3]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Clear rejection']","[4, 3, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The reviewer finds the idea novel and interesting, using terms like ""interesting"", ""effective"", and ""promising"". They also state they would vote for acceptance if concerns are addressed. This indicates positive sentiment. However, the reviewer also lists several concerns and requests for improvement, which moderates the positivity. Therefore, the sentiment is somewhat positive but not overly enthusiastic. The language used is professional and polite throughout, with constructive criticism and suggestions for improvement rather than harsh language.",60.0,80.0
Ensemble Methods as a Defense to Adversarial Perturbations Against Deep Neural Networks,"['Thilo Strauss', 'Markus Hanselmann', 'Andrej Junginger', 'Holger Ulmer']",Reject,2018,"[4, 2, 2, 16]","[9, 4, 6, 18]","[13, 5, 4, 19]","[3, 0, 1, 13]","[6, 4, 3, 5]","[4, 1, 0, 1]","This paper describes the use of ensemble methods to improve the robustness of neural networks to adversarial examples. Adversarial examples are images that have been slightly modified (e.g. by adding some small perturbation) so that the neural network will predict a wrong class label.

Ensemble methods have been used by the machine learning community since long time ago to provide more robust and accurate predictions.

In this paper the authors explore their use to increase the robustness of neural networks to adversarial examples.

Different ensembles of 10 neural networks are considered. These include techniques such as bagging or injecting noise in the 
training data. 

The results obtained show that ensemble methods can sometimes significantly improve the robustness against adversarial examples. However,
the performance of the ensemble is also highly deteriorated by these examples, although not as much as the one of a single neural network.

The paper is clearly written.

I think that this is an interesting paper for the deep learning community showing the benefits of ensemble methods against adversarial
examples. My main concern with this paper is the lack of comparison with alternate techniques to increase the robustness against adversarial examples. The authors should have compared with the methods described in:

(Goodfellow et al., 2014; Papernot et al., 2016c), 
(Papernot et al., 2016d) 
(Gu & Rigazio, 2014)

Furthermore, the ensemble approach has the main disadvantage of increasing the prediction time by a lot. For example, with 10 elements in the ensemble, predictions are 10 times more expensive.
------------------------------
I have read the updated version of the paper. I think the authors have done a good job comparing with related techniques. Therefore, I have slightly increased my score.
","[7, 5, 4]","[' Good paper, accept', ' Marginally below acceptance threshold', ' Ok but not good enough - rejection']","[3, 3, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer finds the paper interesting and well-written, highlighting its value to the deep learning community. They increase their score after the authors address their main concern, indicating a positive view of the work. However, they point out a drawback regarding prediction time, suggesting the sentiment isn't overly positive. Therefore, the sentiment is scored as moderately positive. The language used is constructive and professional throughout, suggesting a polite tone.",70.0,80.0
Log-DenseNet: How to Sparsify a DenseNet,"['Hanzhang Hu', 'Debadeepta Dey', 'Allie Del Giorno', 'Martial Hebert', 'J. Andrew Bagnell']",Reject,2018,"[6, 10, 3, 36, 18]","[10, 15, 7, 41, 23]","[22, 66, 4, 409, 195]","[5, 31, 1, 282, 117]","[5, 31, 3, 64, 58]","[12, 4, 0, 63, 20]","The paper proposes a nice idea of sparsification of skip connections in DenseNets. The authors decide to use a principle for sparsification that would minimize the distance among layers during the backpropagation. 

The presentation of the paper could be improved. The paper presents an elegant and simple idea in a dense and complex way making the paper difficult to follow. E. g., Fig 1 d is discussed in Appendix and not in the main body of the paper, thus, it could be moved to Appendix section.

Table 1 and 3 presents the results only for LogDenseNet V1, would it be possible to add results for V2 that have different MBD. Also, the budget for the skip connections is defined as log(i) in Table 1 and Table 2 has the budget of log(i/2), would it be possible to add the total number of skip connections to the tables? It would be interesting to compare the total number of skip connections in Jegou et. al. to LogDenseNet V1 in Table 3.

Other issues:
- Table 3, has an accuracy of nan. What does it mean? Not available or not a number? 
- L is used as the depth, however, in table 1 it appears as short for Log-DenseNetV1. Would it be possible to use another letter here?
- “…, we make x_i also take the input from x_{i/4}, x_{i/8}, x_{i/16}…”. Shouldn’t x_{1/2} be used too?
- I’m not sure I understand the reasons behind blurred image in Fig 2 at ½. It is mentioned that “it and its feature are at low resolution”. Could the authors comment on that?
- Abstract: “… Log-DenseNets are easier than DenseNet to implement and to scale.” It is not clear why would LogDenseNets be easier to implement. ","[5, 6, 6]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer starts by praising the idea presented in the paper, describing it as ""nice"" and ""elegant."" This suggests a positive sentiment. However, the reviewer also provides constructive criticism, pointing out areas for improvement in presentation, clarity, and experimental details. The language used is polite and professional throughout, focusing on suggestions and requests rather than harsh criticism. Therefore, the sentiment is positive but measured, not overly enthusiastic.",60.0,80.0
Incremental Learning through Deep Adaptation,"['Amir Rosenfeld', 'John K. Tsotsos']",Reject,2018,"[12, 43]","[14, 48]","[24, 230]","[9, 119]","[13, 51]","[2, 60]","----------------- Summary -----------------
The paper tackles the problem of task-incremental learning using deep networks. It devises an architecture and a training procedure aiming for some desirable properties; a) it does not require retraining using previous tasks’ data, b) the number of network parameters grows only sublinearly c) it preserves the output of the previous tasks intact.

----------------- Overall -----------------
The paper tackles an important problem, aims for important characteristics, and does extensive and various experiments. While the broadness of the experiments are encouraging, the main task which is to propose an effective task-incremental learning procedure is not conclusively tested, mainly due to the lack of thorough ablation studies (for instance when convolutional layers are fixed) and the architecture seems to change from one baseline (method) to another.

----------------- Details -----------------
- in the abstract it says: ""Existing approaches either learn sub-optimal solutions, require joint training, or incur a substantial increment in the number of parameters for each added task, typically as many as the original network.""
The linear-combination constraint in the proposed approach is a strong one and can learn a sub-optimal solution for the newly introduced tasks.

- Page 3: R^C → R^{C_o}

- The notation is (probably unnecessarily) too complicated, perhaps it’s better to formulate it without being faithful to the actual implementation but for higher clarity and ease of understanding. For instance, one could start from denoting feature maps and applying the controller/transform matrix W on that, circumventing the clutter of convolutional kernels.

- What is the DAN architecture? 

- In table 1 a better comparison is when using same architecture (instead of VGG) to train it from scratch or fine-tune from ImageNet (the first two rows)

- What is the architecture used for random-weights baseline?

- An experiment is needed where no controller is attached but just the additional fully-connected layers to see the isolated improvements gained by the linear transform of convolutional layers.

- Multiple Base Networks: The assumption in incremental learning is that one does not have access to all tasks/datasets at once, otherwise one would train them jointly which would save parameters, training time and performance. So, finding the best base network using the validation set is not relevant.

- The same concern as above applies to the transferability and dataset decider experiments
","[5, 6, 4]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Ok but not good enough - rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is quite critical of the paper's core proposition, pointing out flaws in the methodology and suggesting significant improvements. While it acknowledges the paper's positive aspects (important problem, extensive experiments), the overall tone and the numerous concerns raised indicate a negative sentiment. However, the language remains professional and polite throughout, focusing on constructive criticism and suggestions for improvement rather than direct attacks on the authors or their work.",-30.0,70.0
Understanding Grounded Language Learning Agents,"['Felix Hill', 'Karl Moritz Hermann', 'Phil Blunsom', 'Stephen Clark']",Reject,2018,"[7, 7, 15, 20]","[12, 9, 20, 25]","[84, 44, 178, 139]","[35, 22, 91, 94]","[43, 20, 70, 28]","[6, 2, 17, 17]","This paper presents an analysis of the properties of agents who learn grounded language through reinforcement learning in a simple environment that combines verbal instruction with visual information. The analyses are motivated by results from cognitive and developmental psychology, exploring questions such as whether agents develop biases for shape/color, the difficulty of learning negation, the impact of curriculum format, and how representations at different levels of abstraction are acquired. I think this is a nice example of a detailed analysis of the representations acquired by a reinforcement learning agent. The extent to which it provides us with insight into human cognition depends on the degree to which we believe the structure of the agent and the task have a correspondence to the human case, which is ultimately probably quite limited. Nonetheless the paper takes on an ambitious goal of relating questions in machine learning in cognitive science and does a reasonably good job of analyzing the results.

Comments:

1. The results on word learning biases are not particularly surprising given previous work in this area, much of which has used similar neural network models. Linda Smith and Eliana Colunga have published a series of papers that explore these questions in detail:

http://www.iub.edu/~cogdev/labwork/kinds.pdf
http://www.iub.edu/~cogdev/labwork/Ontology2003.pdf

2. In figure 2 and the associated analyses, why were 20 shape terms used rather than 8 to parallel the other cases? It seems like there is a strong basic color bias. This seems like one of the most novel findings in the paper and is worth highlighting.

This figure and the corresponding analysis could be made more systematic by mapping out the degree of shape versus color bias as a function of the number of shape and color terms in a 2D plot. The resulting plot would show the degree of bias towards color.

3. The section on curriculum learning does not mention relevant work on “starting small”  and the “less is more"" hypothesis in language development by Jeff Elman and Elissa Newport:

https://pdfs.semanticscholar.org/371b/240bebcaa68921aa87db4cd3a5d4e2a3a36b.pdf
http://www.sciencedirect.com/science/article/pii/0388000188900101

4. The section on learning speeds could include more information on the actual patterns that are found with human learners, for example the color words are typically acquired later. I found these human results hard to reconcile with the results from the models. I also found it hard to understand why colors were hard to learn given the bias towards colors shown earlier in the paper.

5. The section on layerwise attention claims to give a “computational level” explanation, but this is a misleading term to use — it is not a computational level explanation in the sense introduced by David Marr which is the standard use of this term in cognitive science. The explanation of layerwise attention could be clearer.

Minor:

“analagous” -> “analogous”

The paper runs longer than eight pages, and it is not obvious that the extra space is warranted.
","[7, 4, 5]","[' Good paper, accept', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[4, 5, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer provides constructive criticism and suggestions for improvement, indicating a positive attitude towards the paper. They find the research interesting and valuable, praising the detailed analysis and the ambition to connect machine learning with cognitive science. While they point out limitations and areas for improvement, they do so in a respectful and helpful manner. The reviewer doesn't use harsh language and maintains a professional tone throughout.",60.0,80.0
Massively Parallel Hyperparameter Tuning,"['Lisha Li', 'Kevin Jamieson', 'Afshin Rostamizadeh', 'Katya Gonina', 'Moritz Hardt', 'Benjamin Recht', 'Ameet Talwalkar']",Reject,2018,"[1, 10, 12, 10, -1, 13, 17, 11]","[6, 15, 17, 12, 1, 18, 22, 16]","[14, 99, 70, 15, 1, 130, 212, 126]","[6, 49, 37, 10, 1, 61, 90, 54]","[8, 47, 30, 3, 0, 62, 91, 61]","[0, 3, 3, 2, 0, 7, 31, 11]","This paper adapts the sequential halving algorithm that underpins Hyperband to run across multiple workers in a compute cluster. This represents a very practical scenario where a user of this algorithm would like to trade off computational efficiency for a reduction in wall time. The paper's empirical results confirm that indeed significant reductions in wall time come with modest increases in overall computation, it's a practical improvement.

The paper is crisply written, the extension is a natural one, the experiment protocols and choice of baselines are appropriate.

The left panel of figure 3 is blurry, compared with the right one.","[6, 5, 5]","[' Marginally above acceptance threshold', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[3, 5, 5]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review is positive about the paper's contribution, stating its practical value and clear presentation. It finds the extension natural and the experiments well-designed. The only issue pointed out is a minor presentation detail (blurry figure), which doesn't detract from the overall positive tone. Therefore, the sentiment is assessed as positive and the language as polite.",85.0,90.0
UCB EXPLORATION VIA Q-ENSEMBLES,"['Richard Y. Chen', 'Szymon Sidor', 'Pieter Abbeel', 'John Schulman']",Reject,2018,"[13, 4, 17, 8]","[13, 8, 22, 13]","[9, 14, 608, 66]","[3, 4, 291, 27]","[6, 9, 291, 37]","[0, 1, 26, 2]","The authors propose a new exploration algorithm for Deep RL. They maintain an ensemble of Q-values (based on different initialisations) to model uncertainty over Q. The ensemble is then used to derive a confidence interval at each step, which is used to select actions UCB-style.

There is some attempt at a Bayesian interpretation for the Bellman update. But to me it feels a bit like shoehorning the probabilistic interpretation into an already existing update - I’m not sure this is justified and necessary here. Moreover, the UCB strategy is generally not considered a Bayesian strategy, so I wasn’t convinced by the link to Bayesian RL in this paper.

I liked the actual proposed method otherwise, and the experimental results on Atari seem good (but see also latest SOTA Atari results, for example the Rainbow paper). Some questions about the results:
-How does it perform compared to epsilon-greedy added on top of Alg1, or is there evidence that this produces any meaningful exploration versus noise? 
-How does the distribution of Q values look like during different phases of learning?
-Was epsilon-greedy used in addition to UCB exploration? Question for both Alg 1 and Alg 2.
-What’s different between Alg 1 and bootstrapped DQN (other than the action selection)?

Minor things:
-Missing propto in Eq 7?
-Maybe mention that the leftarrows are not hard updates. Maybe you already do somewhere…
-it looks more a Bellman residual update as written in (11).
","[7, 5, 6]","[' Good paper, accept', ' Marginally below acceptance threshold', ' Marginally above acceptance threshold']","[4, 3, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review starts with a neutral summary of the paper's contributions. While the reviewer expresses doubts about the Bayesian interpretation, they clearly state they ""liked the actual proposed method"" and find the experimental results promising. The reviewer also poses several relevant questions, indicating engagement with the work. The tone remains professional and constructive throughout, focusing on specific areas for improvement.",50.0,75.0
Learning to diagnose from scratch by exploiting dependencies among labels,"['Li Yao', 'Eric Poblenz', 'Dmitry Dagunts', 'Ben Covington', 'Devon Bernard', 'Kevin Lyman']",Reject,2018,"[15, 4, 2, 2, 2, 17]","[20, 4, 1, 3, 1, 18]","[130, 3, 1, 5, 1, 6]","[64, 0, 0, 0, 0, 0]","[15, 3, 1, 5, 1, 5]","[51, 0, 0, 0, 0, 1]","This paper presents an impressive set of results on predicting lung pathologies from chest x-ray images. 
Authors present two architectures: one based on denseNet, and one based on denseNet + LSTM on output dimensions (i.e. similar to NADE model), and compare it to state of the art on the chest x-ray classification. Experiments are clearly described and results are significantly better compared to state of the art.

The only issue with this paper is, that their proposed method, in practice is not tractable for inference on estimating probability of a single output, a task which would be critical in medical domain. Considering that their paper is titled as a work to use ""dependencies"" among labels, not being able to evaluate their network's, and lack of interpretable evaluation results on this model in the experiment section is a major limitation. 

On the other hand, there are many alternative models where one could simply use multi-task learning and shared parameter, to predict multiple outcomes extremely efficiently. To be able to claim that this paper improved the prediction by better modeling of 'dependencies' among labels, I would need to see how the (much simpler) multi-task setting works as well. 

That said, the paper has several positive aspects in all areas:

Originality - the paper presents first combination of DenseNets with LSTM-based output factorization,
Writing clarity - the paper is very well written and clear.
Quality - (apart from the missing multi-task baseline), the results are significantly better than state of the art, and experiments are well done,
Significance - Apart from the issue of intractable inference which is arguably a large limitation of this work, the application in medical field is significant. 

","[6, 6, 6]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[4, 3, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review starts with very positive remarks, highlighting the impressive results, clear descriptions, and significant improvements over the state-of-the-art. However, it then raises a major concern about the method's intractability for single output probability estimation, which is crucial for medical applications. The reviewer suggests a simpler multi-task learning approach as a baseline comparison to validate the claim of improved dependency modeling. Despite the criticism, the review acknowledges the paper's originality, clarity, quality, and significance. Overall, the sentiment leans towards the positive side due to the strong initial praise and recognition of the paper's merits, but the concern regarding inference limits the positivity. The language used is polite and professional throughout, offering constructive criticism and suggestions for improvement.",60.0,80.0
Learning Sparse Structured Ensembles with SG-MCMC and Network Pruning,"['Yichi Zhang', 'Zhijian Ou']",Reject,2018,"[10, 18]","[15, 23]","[174, 101]","[71, 52]","[52, 42]","[51, 7]","The authors propose a procedure to generate an ensemble of sparse structured models. To do this, the authors propose to (1) sample models using SG-MCMC with group sparse prior, (2) prune hidden units with small weights, (3) and retrain weights by optimizing each pruned model. The ensemble is applied to MNIST classification and language modelling on PTB dataset. 

I have two major concerns on the paper. First, the proposed procedure is quite empirically designed. So, it is difficult to understand why it works well in some problems. Particularly. the justification on the retraining phase is weak. It seems more like to use SG-MCMC to *initialize* models which will then be *optimized* to find MAP with the sparse-model constraints. The second problem is about the baselines in the MNIST experiments. The FNN-300-100 model without dropout, batch-norm, etc. seems unreasonably weak baseline. So, the results on Table 1 on this small network is not much informative practically. Lastly, I also found a significant effort is also desired to improve the writing. 

The following reference also needs to be discussed in the context of using SG-MCMC in RNN.
- ""Scalable Bayesian Learning of Recurrent Neural Networks for Language Modeling"", Zhe Gan*, Chunyuan Li*, Changyou Chen, Yunchen Pu, Qinliang Su, Lawrence Carin","[4, 6, 6]","[' Ok but not good enough - rejection', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[4, 3, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The reviewer raises major concerns, questioning the methodology and choice of baselines. They use phrases like ""difficult to understand,"" ""justification is weak,"" ""seems more like,"" and ""unreasonably weak baseline,"" which indicate a negative sentiment. However, the language remains professional and offers constructive criticism with specific recommendations. Therefore, the politeness score leans towards neutral-positive.",-30.0,50.0
Block-Sparse Recurrent Neural Networks,"['Sharan Narang', 'Eric Undersander', 'Gregory Diamos']",Reject,2018,"[4, 2, 11]","[9, 7, 15]","[33, 11, 49]","[8, 4, 26]","[23, 6, 20]","[2, 1, 3]","Thanks to the authors for their response.

Though the paper presents an interesting approach, but it relies heavily on heuristics (such as those mentioned in the initial review) without a thorough investigation of scenarios in which this might not work. Also, it might be helpful to investigate if there ways to better group the variables for group  lasso regularization. The paper therefore needs further improvements towards following a more principled approach.

=====================================
This paper presents methods for inducing sparsity in terms of blocks of weights in neural networks which aims to combine benefits of sparsity and faster access based on computing architectures. This is achieved by pruning blocks of weights and group lasso regularization. It is demonstrated empirically that model size can be reduced by upto 10 times with some loss in prediction accuracy.

Though the paper presents some interesting evaluations on the impact of block based sparsity in RNNs, some of the shortcomings of the paper seem to be :

- The approach taken consists of several heuristics rather than following a more principled approach such as taking the maximum of the weights in a block to represent that block and stop pruning till 40% training has been achieved. Also, the algorithm for computing the pruning threshold is based on a new set of hyper-parameters. It is not clear under what conditions the above settings will (not) work.

 - For the group lasso method, since there are many ways to group the variable, it is not clear how the variables are grouped. Is there a reasoning behind a particular grouping of the variables. Individually, group lasso does not seem to work, and gives much worse results. The reasons for worse performance could be investigated. It is possible that important weights are in different groups, and group sparsity is forcing some of them to be zero, and hence leading to worse results. It would be insightful to explain the kind of solver used for group lasso regularization, and if that works for large-scale problems.

 - The results for various kinds of sparsity are unclear in the sense that it is not clear how to set the block size a-priori for having minimum reduction in accuracy and still significant sparsity without having to repeat the process for various choices.

Overall, the paper does not seem to present novel ideas, and is mainly focused on evaluating the impact of block-based sparsity instead of weight pruning by Han etal. As mentioned in Section 2, regularization has been used earlier to achieve sparsity in deep networks. In this view the significance over existing work is relatively narrow, and no explicit comparison with existing methods is provided. It is possible that an existing method leads to pruning method such as by Han etal. leads to 8x decrease in model size while retaining the accuracy, while the proposed method leads to 10x decrease while also decreasing the accuracy by 10%. Scenarios like these need to be evaluated to understand the impact of the method proposed in this paper.","[5, 7, 5]","[' Marginally below acceptance threshold', ' Good paper, accept', ' Marginally below acceptance threshold']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review acknowledges the interesting approach but expresses concerns about the heavy reliance on heuristics and lack of principled approach. The reviewer suggests further improvements and investigations. The language, while direct, maintains a professional and constructive tone.",20.0,60.0
Topology Adaptive Graph Convolutional  Networks,"['Jian Du', 'Shanghang Zhang', 'Guanhang Wu', 'José M. F. Moura', 'Soummya Kar']",Reject,2018,"[9, 7, 3, 37, 13]","[9, 12, 7, 42, 18]","[21, 108, 17, 502, 319]","[7, 45, 10, 260, 130]","[9, 54, 7, 99, 106]","[5, 9, 0, 143, 83]","The authors propose a new CNN approach to graph classification that generalizes previous work. Instead of considering the direct neighborhood of a vertex in the convolution step, a filter based on outgoing walks of increasing length is proposed. This incorporates information from more distant vertices in one propagation step.

The proposed idea is not exceptional original, but the paper has several strong points:

* The relation to previous work is made explicit and it is show that several previous approaches are generalized by the proposed one.
* The paper is clearly written and well illustrated by figures and examples. The paper is easy to follow although it is on an adequate technical level.
* The relation between the vertex and spectrum domain is well elaborated and nice (although neither important for understanding nor implementing the approach).
* The experimental evaluation appears to be sound. A moderate improvement compared to other approaches is observed for all data sets.

In summary, I think the paper can be accepted for ICLR.
----------- EDIT -----------
After reading the publications mentioned by the other reviewers as well as the following related contributions

* Network of Graph Convolutional Networks Trained on Random Walks (under review for ICLR 2018)
* Graph Convolution: A High-Order and Adaptive Approach, Zhenpeng Zhou, Xiaocheng Li (arXiv:1706.09916)

I agree that the relation to previous work is not adequately outlined. Therefore I have modified my rating accordingly.","[6, 5, 4]","[' Marginally above acceptance threshold', ' Marginally below acceptance threshold', ' Ok but not good enough - rejection']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is net positive, highlighting several strong points like clarity, relation to previous work (initially), and sound experimental evaluation. However, the 'EDIT' section adds a critical note, indicating a significant concern about the paper's originality. This lowers the overall positivity but doesn't make it negative. The language remains polite throughout, using constructive criticism.",60.0,80.0
Normalized Direction-preserving Adam,"['Zijun Zhang', 'Lin Ma', 'Zongpeng Li', 'Chuan Wu']",Reject,2018,"[12, 22, 16, 17]","[17, 27, 21, 22]","[64, 260, 268, 212]","[11, 119, 136, 113]","[13, 32, 25, 29]","[40, 109, 107, 70]","The paper extended the Adam optimization algorithm to preserve the update direction. Instead of using the un-centered variance of individual weights, the proposed method adapts the learning rate for the incoming weights to a hidden unit jointly using the L2 norm of the gradient vector. The authors empirically demonstrated the method works well on CIFAR-10/100 tasks.

Comments:

- I found the paper very hard to follow. The authors could improve the clarity of the paper greatly by listing their contribution clearly for readers to digest. The authors also combined the proposed method with a few existing deep learning tricks in the paper. All those tricks that, ie. section 3.3 and 4, should go into the background section.

- Overall, the only contribution of the paper seems to be the ad-hoc modification to Adam in Eq. (9). Why is this a reasonable modification? Do we expect this modification to fail in any circumstances? The experiments on CIFAR dataset and one CNN architecture do not provide enough evidence to show the proposed method work well in general.

","[4, 5, 5]","[' Ok but not good enough - rejection', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review starts with a neutral factual summary of the paper's contributions. However, the comments are mostly negative, pointing out a lack of clarity and questioning the significance and justification of the proposed method. The reviewer also criticizes the limited experimental validation. While the language is critical, it is framed constructively with suggestions for improvement rather than outright dismissal. ",-40.0,60.0
Depth separation and weight-width trade-offs for sigmoidal neural networks,"['Amit Deshpande', 'Navin Goyal', 'Sushrut Karmalkar']",Reject,2018,"[17, 16, 2]","[22, 21, 6]","[52, 92, 31]","[27, 41, 16]","[21, 37, 15]","[4, 14, 0]","This paper proves a new separation results from 3-layer neural networks to 2-layer neural networks. The core of the analysis is a proof that any 2-layer neural networks can be well approximated by a polynomial function with reasonably low degrees. Then the authors constructs a highly non-smooth function can be represented by a 3-layer network, but impossible to approximate by any polynomial-degree polynomial function.

Similar results about polynomial approximation can be found in [1] (Theorem 4). To me, the result proved in [1] is spiritually very similar to propositions 3-4. The authors need to justify the difference.

The main strength of the new separation result is that it holds for a larger class of input distributions. Comparing to Daniely (2017) which requires the input distribution to be spherically uniform, the new result only needs the distribution to be lower bounded by 1/poly(d) in a small ball of radius 1/poly(d). Conceptually I don't think this is a much weaker condition. For a ""truly"" non-uniform distribution, one should allow its density function to be very close to zero at certain regions of the ball. Nevertheless, the result is a step forward from Daniely (2017) and the paper is well written.

I am still in doubt of the practical value of such kind of separation results. The paper proves the separation by constructing a very specific function that cannot be approximated by 2-layer networks. This function has a super large Lipschitz constant, which we don't expect to see in practice. Consider the function f(x)=cos(Nx). When N is chosen large enough, the function f can not be well approximated by any 2-layer network with polynomial size. Does it imply that the family of cosine functions is rich enough so that it is a better family to learn than 2-layer neural networks? I guess the answer would be negative. In addition, the paper doesn't show that any 2-layer network can be well approximated by a 3-layer network, which is a missing piece in justifying the richness of 3-layer nets.

Finally, the constructed ""hard"" function has order d^5 Lipschitz constant, but Theorem 7 assumes that the 2-layer networks' weight must be bounded by O(d^2). This assumption is crucial to the proof but not well justified (especially considering the d^5 factor in the function definition).

[1] On the Computational Efficiency of Training Neural Networks, Livni et al., NIPS'14","[5, 6, 3]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Clear rejection']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The reviewer acknowledges the paper's contribution (""The result is a step forward..."") and finds it well-written. However, they express doubts about the practical implications of the findings and point out some potential weaknesses in the assumptions and comparisons made. Overall, the tone is critical but professional and suggests areas for improvement rather than outright rejection.",40.0,60.0
A Classification-Based Perspective on GAN Distributions,"['Shibani Santurkar', 'Ludwig Schmidt', 'Aleksander Madry']",Reject,2018,"[5, 5, 14]","[10, 10, 19]","[41, 104, 127]","[17, 50, 55]","[24, 50, 64]","[0, 4, 8]","This paper propose to evaluate the distributions learned by GAN using classification-based methods. As two examples, the authors evaluates the mode collapse effect and measure the diversity for GAN distributions. The proposed approaches are experimental but does not require human inspection. The main idea is to fit a classifier on the training data and also learn a GAN model using the training data. Then generate simulated data using GAN and use the classifier to predict the labels of the simulated data. The distribution of predicted labels  and the labels of the true data can be easily compared.

Detailed comments:

1. The proposed method is purely experimental. It  would be better to gain some theoretical insights of this methodology. Moreover, in terms of experiments, it would be nice to consider more examples except for mode collapse and diversity, since these problems are well-known for GAN.

2. Since mode collapse is a well-known phenomenon, the novelty of this paper is not sufficient.

3. There are other measures for the quality of GAN. For example, the inception scores and mode scores (Salimans et al. 2016, Che et al. 2017). It would be nice to compare the method here with other related work.

References:
1. Improved Techniques for Training GANs https://arxiv.org/abs/1606.03498

2. Mode Regularized Generative Adversarial Networks https://arxiv.org/abs/1612.02136

","[3, 6, 5]","[' Clear rejection', ' Marginally above acceptance threshold', ' Marginally below acceptance threshold']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review is lukewarm in its assessment of the paper. While it acknowledges the practicality of the proposed experimental approach, it points out significant limitations regarding novelty and theoretical depth. The reviewer suggests several improvements, indicating a desire to see the paper strengthened rather than rejected outright.",20.0,70.0
Influence-Directed Explanations for Deep Convolutional Networks,"['Anupam Datta', 'Matt Fredrikson', 'Klas Leino', 'Linyi Li', 'Shayak Sen']",Reject,2018,"[1, 5, 18, 11, 12]","[6, 8, 22, 16, 17]","[24, 20, 140, 89, 64]","[11, 9, 82, 47, 25]","[12, 11, 49, 35, 23]","[1, 0, 9, 7, 16]","SUMMARY 
========
This paper proposes to measure the ""influence"" of single neurons w.r.t. to a quantity of interest represented by another neuron, typically w.r.t. to an output neuron for a class of interest, by simply taking the gradient of the corresponding output neuron w.r.t to the considered neuron. This gradient is used as is, given a single input instance, or else, gradients are averaged over several input instances. 
In the latter case the averaging is described by an ad-hoc distribution of interest P which is introduced in the definition of the influence measure, however in the present work only two types of averages are practically used: either the average is performed over all instances belonging to one class, or over all input instances.

In other words, standard gradient backpropagation values (or average of them) are used as a proxy to quantify the importance of neurons (these neurons being within hidden layers or at the input layer), and are intended to better explain the classification, or sometimes even misclassification, performed by the network.

The proposed importance measure is theoretically justified by stating a few properties (called axioms) an importance measure should generally verify, and then showing the proposed measure fullfills these requirements.

Empirically the proposed measure is used to inspect the classification of a few input instances, to extract ""class-expert"" neurons, and to find a preprocessing bug in one model. The only comparison to a related work method is done qualitatively on one image visualization, where the proposed method is compared to Integrated Gradients [Sundararajan et al. 2017].

WEAKNESSES
==========
The similarity and differences between the proposed method and related work is not made clear. For example, in the case of a single input instance, and when the quantity of interest is one output neuron corresponding to one class, the proposed measure is identical to the image-specific class saliency of [Simonyan et al. 2014].
The difference to Integrated Gradients [Sundararajan et al. 2017] at the end of Section 1.1 is also not clearly formulated: why is the constraint on distribution marginality weaker here ?
An important class of explanation methods, namely decomposition-based methods (e.g. LRP, Excitation Backprop, Deep Taylor Decomposition), are not mentioned. Recent work (Montavon et al., Digital Signal Processing, 2017), discusses the advantages of decomposition-based methods over gradient-based approaches. Thus, the authors should clearly state the advantages/disadvantes of the proposed gradient-based method over decomposition-based techniques.

Concerning the theoretical justification:
It is not clear how Axiom 2 ensures that the proposed measure only depends on points within the input data manifold. This is indeed an important issue, since otherwise the gradients in equation (1) might be averaged completely outside the data manifold and thus the influence measure be unrelated to the data and problem the neural network was trained on. Also the notation used in Axiom 5 is very confusing. Moreover it seems this axiom is even not used in the proof of Theorem 2.

Concerning the experiments:
The experimental setup, especially in Section 3.3.1, is not well defined: on which layer of the network is the mask applied? What is the ""quantity of interest"": shouldn't it be an output neuron value rather than h|i (as stated at the begin of the fourth paragraph of Section 3.3.1)?
The proposed method should to be quantitatively compared with other explanation techniques (e.g. by iteratively perturbing most relevant pixels and tracking the performance drop, see Samek et al., IEEE TNNLS, 2017).
The last example of explaining the bug is not very convincing, since the observation that class 2 distinctive features are very small in the image space, and thus might have been erased through gaussian blur, is not directly related to the influence measure and could have been made aso independently from it.

CONCLUSION
==========
Overall this work does not introduce any new importance measure for neurons, it merely formalizes the use of standard backpropagation gradients as influence measure.
Using gradients as importance measure was already done in previous work (e.g. [Simonyan et al. 2014]). Though taking the average of gradients over several input instances is new, this information might not be of great help for practical applications.
Recent work also showed that raw gradients are less informative than decomposition-based quantities to explain the classification decisions made by a neural network.","[4, 5, 4]","[' Ok but not good enough - rejection', ' Marginally below acceptance threshold', ' Ok but not good enough - rejection']","[5, 3, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review is critical of the paper, pointing out a lack of novelty and comparison to related work, unclear explanations, and unconvincing experiments. It does not use overtly negative language but highlights significant weaknesses. Therefore, the sentiment is negative, but the politeness remains neutral.",-40.0,0.0
Federated Learning: Strategies for Improving Communication Efficiency,"['Jakub Konečný', 'H. Brendan McMahan', 'Felix X. Yu', 'Ananda Theertha Suresh', 'Dave Bacon', 'Peter Richtárik']",Reject,2018,"[6, 16, 8, 9, 9, 14]","[10, 21, 12, 14, 14, 16]","[41, 98, 73, 296, 124, 19]","[7, 41, 36, 82, 59, 3]","[27, 52, 34, 168, 59, 7]","[7, 5, 3, 46, 6, 9]","This paper proposes several client-server neural network gradient update strategies aimed at reducing uplink usage while maintaining prediction performance.  The main approaches fall into two categories: structured, where low-rank/sparse updates are learned, and sketched, where full updates are either sub-sampled or compressed before being sent to the central server.  Experiments are based on the federated averaging algorithm.  The work is valuable, but has room for improvement.

The paper is mainly an empirical comparison of several approaches, rather than from theoretically motivated algorithms.  This is not a criticism, however, it is difficult to see the reason for including the structured low-rank experiments in the paper (itAs a reader, I found it difficult to understand the actual procedures used.  For example, what is the difference between the random mask update and the subsampling update (why are there no random mask experiments after figure 1, even though they performed very well)?  How is the structured update ""learned""?  It would be very helpful to include algorithms.

It seems like a good strategy is to subsample, perform Hadamard rotation, then quantise.    For quantization, it appears that the HD rotation is essential for CIFAR, but less important for the reddit data.  It would be interesting to understand when HD works and why,  and perhaps make the paper more focused on this winning strategy, rather than including the low-rank algo.  

If convenient, could the authors comment on a similarly motivated paper under review at iclr 2018:
VARIANCE-BASED GRADIENT COMPRESSION FOR EFFICIENT DISTRIBUTED DEEP LEARNING

pros:

- good use of intuition to guide algorithm choices
- good compression with little loss of accuracy on best strategy
- good problem for FA algorithm / well motivated
- 

cons:

- some experiment choices do not appear well motivated / inclusion is not best choice
- explanations of algos / lack of 'algorithms' adds to confusion

a useful reference:

Strom, Nikko. ""Scalable distributed dnn training using commodity gpu cloud computing."" Sixteenth Annual Conference of the International Speech Communication Association. 2015.

","[5, 5, 7]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Good paper, accept']","[3, 5, 5]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The reviewer states that the ""work is valuable"", which indicates a positive sentiment. However, they also mention that the paper ""has room for improvement"" and list several concerns, suggesting that it's not perfect. Therefore, the sentiment is likely to be positive but not overly enthusiastic. The language used is constructive and professional throughout the review. While the reviewer points out areas for improvement, they do so in a polite and respectful manner, using phrases like ""it would be helpful"" and ""it seems like"".",60.0,80.0
Learning Non-Metric Visual Similarity for Image Retrieval,"['Noa Garcia', 'George Vogiatzis']",Reject,2018,"[2, 16]","[7, 21]","[48, 58]","[21, 38]","[22, 8]","[5, 12]","This paper presents a simple image retrieval method. Paper claims it is a deep learning method, however it is not an end-to-end network. The main issue of the paper is lack of technical contributions.

Paper assumes that image retrieval task can be reformulated at a supervised similarity learning task. That is fine, however image retrieval is traditionally an unsupervised task. 

Even after using supervised method and deep learning technique, still this method is not able to obtain better results than hand crafted methods. Why is that? See - paper from CVPR2012 -  Arandjelović, Relja, and Andrew Zisserman. ""Three things everyone should know to improve object retrieval."" Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on. IEEE, 2012.

Paper make use of external signal to obtain y_{i,j}. It is not clear to me how does this generalize to large datasets?

If features are L2 normalized, why you need to normalize the features again in equation 5?

In equation 5, why not simply use a max margin deep similarity metric learning method with slack variables to generalizability?

The performance of entire network really rely on the accuracy of y_{i,j} and it is not clear the obtained performance is simply due to this supervision.

Paper does not argue well why we need this supervision.

Technically, there is nothing new here.
","[3, 7, 4]","[' Clear rejection', ' Good paper, accept', ' Ok but not good enough - rejection']","[5, 5, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is highly critical of the paper, pointing out major flaws and lack of novelty. The language used is quite direct and critical, bordering on harsh in some instances (e.g., ""Technically, there is nothing new here.""). While it doesn't resort to personal attacks, it lacks the constructive tone often seen in polite academic discourse.",-70.0,-20.0
Anomaly Detection with Generative Adversarial Networks,"['Lucas Deecke', 'Robert Vandermeulen', 'Lukas Ruff', 'Stephan Mandt', 'Marius Kloft']",Reject,2018,"[-3, -3, -3, -3, -3, 3]","[1, 1, 1, 1, 1, 7]","[2, 1, 3, 1, 1, 13]","[2, 1, 2, 1, 1, 6]","[0, 0, 1, 0, 0, 2]","[0, 0, 0, 0, 0, 5]","In the paper, the authors proposed using GAN for anomaly detection.
In the method, we first train generator g_\theta from a dataset consisting of only healthy data points.
For evaluating whether the data point x is anomalous or not, we search for a latent representation z such that x \approx g_\theta(z).
If such a representation z could be found, x is deemed to be healthy, and anomalous otherwise.
For searching z, the authors proposed a gradient-descent based method that iteratively update z.
Moreover, the authors proposed updating the parameter \theta of the generator g_\theta.
The authors claimed that this parameter update is one of the novelty of their method, making it different from the method of Schlegl et al. (2017).
In the experiments, the authors showed that the proposed method attained the best AUC on MNIST and CIFAR-10.

In my first reading of the paper, I felt that the baselines in the experiments are too primitive.
Specifically, for KDE and OC-SVM, a naive PCA is used to reduce the data dimension.
Nowadays, there are several publicly available CNNs that are trained on large image datasets such as ImageNet.
Then, one can use such CNNs as feature extractor, that will give better low dimensional expression of the data than the naive PCA.
I believe that the performances of KDE and OC-SVM can be improved by using such feature extractors.

Additionally, I found that some well-known anomaly detection methods are excluded from the comparison.
In Emmott et al. (2013), which the authors referred as a related work, it was reported that Isolation Forest and Ensemble of GMMs performed well on several datasets (better than KDE and OC-SVM).
It would be essential to add these methods as baselines to be compared with the proposed method.

Overall, I think the experimental results are far from satisfactory.


### Response to Revision ###
It is interesting to see that the features extracted from AlexNet are not helpful for anomaly detection.
It would be interesting to see whether features extracted from middle layers are helpful or they are still useless.
I greatly appreciate the authors for their extensive experiments as a response to my comments.
However, I have decided to keep my score unchanged, as the additional experiments have shown that the performance of the proposed method is not significantly better than the other methods.
In particular, in MNIST, GMM performed better.","[4, 6, 4]","[' Ok but not good enough - rejection', ' Marginally above acceptance threshold', ' Ok but not good enough - rejection']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer finds the experimental setup flawed and the results not convincing. They point out missing baselines and question the choice of feature extraction. While they appreciate the authors' efforts in addressing their initial concerns, they remain unconvinced, leading to a somewhat negative sentiment. The language used is polite and professional, focusing on constructive criticism.",-20.0,70.0
MINE: Mutual Information Neural Estimation,"['Mohamed Ishmael Belghazi', 'Sai Rajeswar', 'Aristide Baratin', 'Devon Hjelm', 'Aaron Courville']",Reject,2018,"[4, 5, 1, 5, 18]","[7, 9, 5, 9, 23]","[10, 24, 14, 71, 309]","[4, 11, 3, 31, 135]","[6, 12, 10, 37, 160]","[0, 1, 1, 3, 14]","This paper presents a new method for estimation of mutual information (MI) based on the Donsker-Varhan (DV) representation of KL-divergence. This representation requires the calculation of a supremum over a set of functions and a lower bound is achieved when a neural network is used for the maximisation of it. Computing the DV representation also requires evaluating expectations wrt to the distributions of interest, the proposed method uses Monte-Carlo estimates based on the empirical distributions.

The experiments evaluating the quality of the OMIE estimator for mutual information should be more thorough to make a point that OMIE beats competing estimators. The bivariate Gaussian case presented in Figure 1 is not a very relevant test case as estimating MI is especially difficult in higher dimensions. It would also be interesting to know the number of samples used as the ratio nbr dimensions/samples matters for estimation quality. The caption for Figure 2 mentions “bivariate Gaussians of dimension 50”, do the author mean two Gaussians of dimension 50 each?

The results of the proposed method on the swiss-roll dataset look good, however the authors only provide a comparison to a classic GAN where it seems more natural to compare with the other works on mode-dropping for GAN cited in the related works section. A comparison with InfoGAN and Dai et al. would be especially relevant to evaluate the effectiveness of OMIE. 

On the application of OMIE to the Information Bottleneck (IB) problem:
How was the optimization of the objective exactly performed? How are gradients calculated? Is the reparametrisation trick used? More details should be provided on the results presented in table 3. Are the results obtained on the test set? What was the value of beta and to which values of I(X,Z) and I(Z,Y) does it correspond? Was the misclassification rate averaged over multiple runs?

The generalization to f-divergences is interesting but seems rather straightforward. 

The second line of equation (20) does not make sense to me, it is not equivalent to the first line.

The methods proposed in Alemi et al. and Chalk et al. differ also in the way the bounds are estimated, not only in the choice of the marginal distribution.

The authors mention that strong consistency and convergence properties (page 3) are proven in the appendix, however I could not find them.","[5, 3, 5]","[' Marginally below acceptance threshold', ' Clear rejection', ' Marginally below acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is mostly critical, pointing out several areas where the paper needs significant improvement in terms of experimental thoroughness, clarity, and comparison with existing methods. While it acknowledges some positive aspects (e.g., ""The results of the proposed method on the swiss-roll dataset look good"", ""The generalization to f-divergences is interesting""), these are outweighed by the criticisms. The language is direct and professional, but not overly positive.",-30.0,50.0
End-to-End Abnormality Detection in Medical Imaging,"['Dufan Wu', 'Kyungsang Kim', 'Bin Dong', 'Quanzheng Li']",Reject,2018,"[6, 7, 13, 15]","[10, 12, 18, 20]","[20, 36, 76, 156]","[4, 13, 22, 66]","[11, 12, 32, 53]","[5, 11, 22, 37]","This paper proposes to jointly model computed tomography reconstruction and lesion detection in the lung, training the mapping from raw sinogram to detection outputs in an end-to-end manner. In practice, such a mapping is computed separately, without regard to the task for wich the data is to be used. Because such a mapping loses information, optimizing such a mapping jointly with the task should preserve more information that is relevant to the task. Thus, using raw medical image data should be useful for lesion detection in CT as well as most other medical image analysis tasks.


Style considerations:

The work is adequately motivated and the writing is generally clear. However, some phrases are awkward and unclear and there are occasional minor grammar errors. It would be useful to ask a native English speaker to polish these up, if possible. Also, there are numerous typos that could nonetheless be easily remedied with some final proofreading. Generally, the work is well articulated with sound structure but needs polish.

A few other minor style points to address:
- ""g"" is used throughout the paper for two different networks and also to define gradients - if would be more clear if you would choose other letters.
- S3.3, p. 7 : reusing term ""iteration""; clarify
- fig 10: label the columns in the figure, not in the description
- fig 11: label the columns in the figure with iterations
- fig 8 not referenced in text


Questions:

1. Before fine-tuning, were the reconstruction and detection networks trained end-to-end (with both L2 loss and cross-entropy loss) or were they trained separately and then joined during fine-tuning?
(If it is the former and not the latter, please make that more clear in the text. I expect that it was indeed the former; in case that it was not, I would expect fully end-to-end training in the revision.)

2. Please confirm: during the fine-tuning phase of training, did you use only the cross-entropy loss and not the L2 loss?

3a. From equation 3 to equation 4 (on an iteration of reconstruction), the network g() was dropped. It appears to replace the diagonal of a Hessian (of R) which is probably a conditioning term. Have you tried training a g() network? Please discuss the ramifications of removing this term.

3b. Have you tracked the condition number of the Jacobian of f() across iterations? This should be like tracking the condition number of the Hessian of R(x).

4. Please discuss: is it better to replace operations on R() with neural networks rather than to replace R()? Why?

5. On page 5, you write ""masks for lung regions were pre-calculated"". Were these masks manual segmentations or created with an automated method?

6. Why was detection only targetted on ""non-small nodules""? Have you tried detecting small nodules?

7. On page 11, you state: ""The tissues in lung had much better contrast in the end-to-end network compared to that in the two-step network"". I don't see evidence to support that claim. Could you demonstrate that?

8. On page 12, relating to figure 11, you state:

""Whereas both methods kept similar structural component, the end-to-end method had more focus on the edges and tissues inside lung compared to the two-step method. As observed in figure 11(b), the structures of the lung tissue were much more clearer in the end-to-end networks. This observation indicated that sharper edge and structures were of more importance for the detection network than the noise level in the reconstructed images, which is in accordance with human perceptions when radiologists perform the same task.""

However, while these claims appear intuitive and such results may be expected, they are not backed up by figure 11. Looking at the feature map samples in this figure, I could not identify whether they came from different populations. I do not see the evidence for ""more focus on the edges and tissues inside lung"" for the end-to-end method in fig 11. It is also not obvious whether indeed ""the structures of the lung tissue were much more clearer"" for the end-to-end method, in fig 11. Can you clarify the evidence in support of these claims? 


Other points to address:

1. Please report statistical significance for your results (eg. in fig 5b, in the text, etc.). Also, please include confidence intervals in table 2.

2. Although cross-entropy values, detection metrics were not (except for the ROC curve with false positives and false negatives). Please compute: accuracy, precision, and recall to more clearly evaluate detection performance.

3a. ""Abnormality detection"" implies the detection of anything that is unusual in the data. The method you present targets a very specific abnormality (lesions). I would suggest changing ""abnormality detection"" to ""lesion detection"".

3b. The title should also be updated accordingly. Considering also that the presented work is on a single task (lesion detection) and a single medical imaging modality (CT), the current title appears overly broad. I would suggest changing it from ""End-to-End Abnormality Detection in Medical Imaging"" -- possibly to something like ""End-to-End Computed Tomography for Lesion Detection"".


Conclusion:

The motivation of this work is valid and deserves attention. The implementation details for modeling reconstruction are also valuable. It is interesting to see improvement in lesion detection when training end-to-end from raw sinogram data.  However, while lung lesion detection is the only task on which the utility of this method is evaluated, detection improvement appears modest. This work would benefit from additional experimental results or improved analysis and discussion.","[5, 6, 4]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Ok but not good enough - rejection']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review acknowledges the value of the paper's motivation and implementation details, indicating a positive sentiment. The reviewer finds the results interesting and acknowledges the improvement in lesion detection. However, they also point out limitations and suggest improvements, particularly regarding the modest improvement in detection and the need for additional experimental results or analysis. The language used is formal, professional, and respectful, suggesting a neutral to positive politeness level.",60.0,70.0
Transformation Autoregressive Networks,"['Junier Oliva', 'Avinava Dubey', 'Barnabás Póczos', 'Eric P. Xing', 'Jeff Schneider']",Reject,2018,"[6, 10, 8, 17, 16, 18, 26]","[11, 15, 13, 22, 21, 23, 31]","[65, 49, 143, 239, 419, 625, 177]","[29, 27, 69, 120, 207, 339, 118]","[35, 19, 73, 105, 201, 218, 52]","[1, 3, 1, 14, 11, 68, 7]","This paper offers an extension to density estimation networks that makes them better able to learn dependencies between covariates of a distribution.

This work does not seem particularly original as applying transformations to input is done in most AR estimators.

Unfortunately, it's not clear if the work is better than the state-of-the-art. Most results in the paper are comparisons of toy conditional models. The paper does not compare to work for example from Papamakarios et al. on the same datasets. The one Table that lists other work showed LAM and RAM to be comparable. Many of the experiments are on synthetic results, and the paper would have benefited from concentrating on more real-world datasets.","[5, 5, 8]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Top 50% of accepted papers, clear accept']","[2, 3, 4]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with a neutral statement, acknowledging the paper's aim. However, it quickly turns critical, questioning the originality and impact of the work. The reviewer points out flaws like lack of comparison with relevant state-of-the-art and over-reliance on synthetic data. The language is professional and avoids harsh or personal criticism, focusing on the scientific shortcomings. Therefore, the sentiment leans negative, but the politeness remains neutral to slightly positive.",-40.0,40.0
Unsupervised Hierarchical Video Prediction,"['Nevan Wichers', 'Dumitru Erhan', 'Honglak Lee']",Reject,2018,"[7, 3, 0, 22]","[11, 8, 4, 27]","[3, 20, 14, 294]","[3, 11, 8, 171]","[0, 0, 6, 106]","[0, 9, 0, 17]","The paper presents a method for predicting future video frames. The method is based on Villegas et al. (2017), with the main difference being that no ground truth pose is needed to train the network.

The novelty of the method is limited. It seems that there is very little innovation in terms of network architecture compared to Villegas et al. The difference is mainly on how the network is trained. But it is straightforward to train the architecture of Villegas et al. without pose -- just use any standard choice of loss that compares the predicted frame versus the ground truth frame. I don't see what is non-trivial or difficult about not using pose ground truth in training.

Overall I think the contribution is not significant enough. 
","[4, 4, 4]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is negative about the paper, pointing out a lack of significant contribution and novelty. The reviewer does not see the method proposed by the authors as particularly innovative or difficult to achieve through more straightforward means. However, the language used, while direct, is within the bounds of academic criticism and does not resort to personal attacks or disrespectful language.",-60.0,50.0
Sparse-Complementary Convolution for Efficient Model Utilization on CNNs,"['Chun-Fu (Richard) Chen', 'Jinwook Oh', 'Quanfu Fan', 'Marco Pistoia', 'Gwo Giun (Chris) Lee']",Reject,2018,"[8, 10, 14, 20]","[12, 15, 19, 25]","[73, 39, 77, 83]","[40, 26, 50, 53]","[23, 0, 21, 16]","[10, 13, 6, 14]","This paper introduces a new design of kernels in convolutional neural networks. The idea is to have sparse but complementary kernels with predefined patterns, which altogether cover the same receptive field as dense kernels. Because of the sparsity of such kernels, deeper or wider networks can be designed at the same computational cost as networks with dense kernels.

Strengths:
- The complementary kernels come at no loss compare to standard ones
- The resulting wider networks can achieve better accuracies than the original ones

Weaknesses:
- The proposed patterns are clear for 3x3 kernels, but no solution is proposed for other dimensions
- The improvement over the baseline is not very impressive
- There is no comparison against other strategies, such as 1xk and kx1 kernels (e.g., Ioannou et al. 2016)

Detailed comments:
- The separation into + and x patterns is quite clear for 3x3 kernels. However, two such patterns would not be sufficient for 5x5 or 7x7 kernels. This idea would have more impact if it generalized to arbitrary kernel dimensions.

- The improvement over the original models are of the order of less than 1 percent. I understand that such improvements are not easy to achieve, but one could wonder if they are not due to the randomness of initialization/mini-batches. It would be more meaningful to report average accuracies and standard deviations over several runs of each experiment.

- Section 4.4 briefly discusses the comparison with using 3x1 and 1x3 kernels, mentioning that an empirical comparison is beyond the scope of this paper. To me, this comparison is a must. In fact, the discussion in this section is not very clear to me, as it mentions additional experiments that I could not find (maybe I misunderstood the authors). What I would like to see is the results of a model based on the method of Ioannou et al, 2016 with the same number of FLOPS.

- In Section 2, the authors review ideas of so-called random kernel sparsity. Note that the work of Wen et al., 2016, and that of Alvarez & Salzmann, NIPS 2016, do not really impose random sparsity, but rather aim to cancel out entire kernels, thus reducing the size of the model and not requiring implementation overhead. They also do not require pre-training and re-training, but just a single training procedure. Note also that these methods often tend not to decrease accuracy, but rather even increase it (by a similar magnitude to that in this paper), for a more compact model.

- In the context of random sparsity, it would be worth citing the work of Collins & Kohli, 2014, Memory Bounded Deep Convolutional Networks.

- I am not entirely convinced by the discussion of the grouped sparsity method in Section 3.1. In fact, the order of the channels is arbitrary, since the kernels are learnt. Therefore, it seems to me that they could achieve the same result. Maybe the authors can clarify this?

- Is there a particular reason why the central points appears in both complementary kernels (+ and x)?

- Why did the authors change the training procedure of ResNets slightly compared to the original paper, i.e., 50k training images instead of 45k training + 5k validation? Did the baseline (original model) reported here also use 50k? What would the results be with 45k?

- Fig. 5 is not entirely clear to me. What was the width of each layer? The original one or the modified one?

- It would be interesting to report the accuracy of a standard ResNet with 1.325*width as a comparison, as well as the runtime of such a model.

- In Table 4, I find it surprising that there is an actual speedup for the model with larger width. I would have expected the same runtime. How do the authors explain this? 
","[5, 6, 5]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Marginally below acceptance threshold']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review acknowledges the strengths of the paper, such as the proposed kernels being on par with standard ones and wider networks showing improved accuracy. However, it points out significant weaknesses, including limited generalizability of the proposed patterns, marginal performance improvement, and lack of comparison with relevant techniques. The reviewer also raises several questions and requests clarifications, indicating a need for substantial improvements to the paper. The language used is professional and not overtly negative, but the numerous concerns suggest a critical stance.",20.0,60.0
An empirical study on evaluation metrics of generative adversarial networks,"['Gao Huang', 'Yang Yuan', 'Qiantong Xu', 'Chuan Guo', 'Yu Sun', 'Felix Wu', 'Kilian Weinberger']",Reject,2018,"[2, 7, 16, 5, 4, 25, 15]","[7, 12, 21, 10, 4, 30, 20]","[43, 207, 104, 78, 10, 54, 199]","[20, 71, 47, 27, 5, 22, 106]","[23, 85, 34, 42, 4, 26, 82]","[0, 51, 23, 9, 1, 6, 11]","This paper introduces a comparison between several approaches for evaluating GANs. The authors consider the setting of a pre-trained image models as generic representations of generated and real images to be compared. They compare the evaluation methods based on five criteria termed disciminability, mode collapsing and mode dropping, sample efficiency,computation efficiency, and robustness to transformation. This paper has some interesting insights and a few ideas of how to validate an evaluation method. The topic is an important one and a very difficult one. However, the work has some problems in rigor and justification and the conclusions are overstated in my view.

Pros
-Several interesting ideas for evaluating evaluation metrics are proposed
-The authors tackle a very challenging subject

Cons
-It is not clear why GANs are the only generative model considered
-Unprecedented visual quality as compared to other generative models has brought the GAN to prominence and yet this is not really a big factor in this paper.
-The evaluations rely on using a pre-trained imagenet model as a representation. The authors point out that different architectures yield similar results for their analysis, however it is not clear how the biases of the learned representations affect the results. The use of learned representations needs more rigorous justification
-The evaluation for discriminative metric, increased score when mix of real and unreal increases, is interesting but it is not convincing as the sole evaluation for “discriminativeness” and seems like something that can be gamed. 
- The authors implicitly contradict the argument of Theis et al against monolithic evaluation metrics for generative models, but this is not strongly supported.

Several references I suggest:
https://arxiv.org/abs/1706.08500 (FID score)
https://arxiv.org/abs/1511.04581 (MMD as evaluation)
","[5, 5, 7, 8]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Good paper, accept', ' Top 50% of accepted papers, clear accept']","[3, 5, 4, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review starts with positive remarks, acknowledging the relevance of the topic and the interesting insights presented. However, it then delves into several significant concerns regarding the paper's rigor, justification, and potentially overstated conclusions. The reviewer also points out specific areas where the paper's arguments are weak or potentially flawed. The tone, while critical, remains professional and constructive throughout, suggesting specific improvements and additional references. Overall, the sentiment leans towards the negative side due to the considerable concerns raised.",-20.0,70.0
Discovering Order in Unordered Datasets: Generative Markov Networks,"['Yao-Hung Hubert Tsai', 'Han Zhao', 'Nebojsa Jojic', 'Ruslan Salakhutdinov']",Reject,2018,"[5, 6, 16, 23]","[9, 11, 21, 28]","[68, 111, 419, 155]","[31, 54, 207, 102]","[33, 51, 201, 33]","[4, 6, 11, 20]","
The authors deal with the problem of implicit ordering in a dataset and the challenge of recovering it, i.e. when given a random dataset with no explicit ordering in the samples, the model is able to recover an ordering. They propose to learn a distance-metric-free model that assumes a Markov chain as the generative mechanism of the data and learns not only the transition matrix but also the optimal ordering of the observations.


> Abstract
“Aiming to find such orders, we introduce a novel Generative Markov Network (GMN) which we use to extract the order of data instances automatically. ”
I am not sure what automatically refers here to. Do the authors mean that the GMN model does not explicitly assume any ordering in the observed dataset? This needs to be better stated here. 
“Aiming to find such orders, we introduce a novel Generative Markov Network (GMN) which we use to extract the order of data instances automatically; given an unordered dataset, it outputs the best -most possible- ordering.”

Most of the models assume an explicit ordering in the dataset and use it as an integral modelling assumption. Contrary to that they propose a model where no ordering assumption is made explicitly, but the model itself will recover it if any.

> Introduction
The introduction is fairly well structured and the example of the joint locations in different days helps the reader.  

In the last paragraph of page 1, “we argue that … a temporal model can generate it.”, the authors present very good examples where ordered observations (ballerina poses, video frames) can be shuffled and then the proposed model can recover a temporal ordering out of them. What I would like to think also here is about an example where the recovered ordering will also be useful as such. An example where the recovered ordering will increase the importance of the inferred solution would be more interesting..



2. Related work
This whole section is not clear how it relates to the proposed model GMN. Rewriting is strongly suggested. 
The authors mention Deep Generative models and One-shot learning methods as related work but the way this section is constructed makes it hard for the reader to see the relation. It is important that first the authors discuss the characteristics of GMN that makes it similar to Deep generative models and the one-shot learning models. They should briefly explain the characteristics of DGN and one-shot learning so that the readers see the relationship. 
Also, the authors never mention that the architecture they propose is deep.
 
Regarding the last paragraph of page 2, “Our approach can be categorised … can be computed efficiently.”:
Not sure why the authors assume that the samples can be sampled from an unmixed chain. An unmixed chain can also result in observing data that do not exhibit the real underlying relationships. Also the authors mention couple of characteristics of the GMN but without really explaining them.  What are the explicit and implicit models [1] … this needs more details. 

[1] P. J. Diggle and R. J. Gratton. Monte Carlo methods of inference for implicit statistical models. Journal of the Royal Statistical Society. Series B (Methodological), pages 193–227, 1984. 

“Second, prior approaches were proposed based on the notion of denoising models. In other words, their goal was generating high-quality images; on the other hand, we aim at discovering orders in datasets.” —>this bit is confusing. Do the authors mean that prior approaches were considering the observed ordering as part of the model assumptions and were just focusing on the denoising? 

3. Generative Markov models
First, I would like to draw the attention of the authors on the terminology they use. The states here are not the latent states usually referred in the literature of Markov chains. The states here are observed and should not be confused with the emissions also usually stated in the corresponding literature. There are as many states as the number of observations and not differentiation is made for ties. All these are based on my understanding of the model.

In  the Equation just before equation (1),  on the left hand side, shouldn’t \pi be after the `;’. It’s an average over the possible \pi.  We cannot  consider the average over \pi when we also want to find the optimal \pi.  The sum doesn’t need to be there. Shouldn’t it just be  max_{\theta, \pi} log P({s_i}^{n}_{i=1}; \pi, \theta) ?
Equation (1), same. The summation over the possible \pi is confusing. It’s an optimisation problem…

page 4, section 3.1: The discussion about the use of Neural Net for the construction of the transition matrix needs expansion. It is unclear how the matrix is constructed. Please add more details. E.g. use of soft-max non-linear transformation so that the output of the Neural Net can be interpreted as the probabilities of jumping to one of the possible states. In this fashion, we map the input (current state) and transform it to the probability gf occupying states at the next time step.

Why this needs expansion: The construction of the transition matrix is the one that actually plays the role of the distance metric in the related models. More specifically, the choice of the non-linear function that outputs the transition probability is crucial; e.g. a smooth function will output comparable transition probabilities to similar inputs (i.e. similar states). 

section 3.2: 
My concern about averaging over \pi applies on the equations here too. 

“However, without further assumption on the structure of the transitional operator..”—> I think the choice of the nonlinear function in the output node of the NN is actually related to the transition matrix and defines the probabilities. It is a confusing statement to make and authors need to discuss more about it. After all, what is the driving force of the inference? This is a problem/task where the observations are considered in a number of different permutations. As such, the ordering is not fixed and the main driving force regarding the best choice of ordering should come from the architecture of the transition matrix; what kind of transitions does the Neural Net architecture favour? Distance free metric but still assumptions are made that favour specific transitions over others. 

“At first, Alg. 1 enumerates all the possible states appearing in the first time step. For each of the following steps, it finds the next state by maximizing the transition probability at the current step, i.e., a local search to find the next state. ” —>  local search in the sense that the algorithm chooses as the next state the state with the biggest transition probability (to it) as defined in the Neural Net (transition operator) output? This is a deterministic step, right? 

4.1 DISCOVERING ORDERS IN DATASETS 
Nice description of the datasets. In the <MSR_SenseCam> the choice of one of the classes needs to be supported.  Why? What do the authors expect to happen if a number of instances from different classes are chosen? 

4.1.1 IMPLICIT ORDERS IN DATASETS 
The explanation of the inferred orderings for the GMN and Nearest Neighbour model is not clear. In figure 2, what forces the GMN to make distinguishable transitions as opposed to the Nearest neighbour approach that prefers to get stuck to similar states? Is it the transition matrix architecture as defined by the neural network? 

>> Figure 10: why use of X here? Why not keep being consistent by using s?

*** DO the authors test the model performance on a ordered dataset (after shuffling it…) ?  Is the model able of recovering the order? **
","[4, 4, 4]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer provides constructive criticism, suggestions for improvement, and points out areas of confusion. They highlight positive aspects like the well-structured introduction and helpful examples. However, they also express concerns about clarity, terminology, and the need for further explanation in several sections. The tone is direct and professional, suggesting a desire to improve the paper rather than simply criticize it.",50.0,70.0
Efficiently applying attention to sequential data with the Recurrent Discounted Attention unit,"['Brendan Maginnis', 'Pierre Richemond']",Reject,2018,"[2, 2]","[3, 7]","[4, 18]","[0, 3]","[3, 15]","[1, 0]","Summary:
This paper proposes an extension to the RWA model by introducing the discount gates to computed discounted averages instead of the undiscounted attention. The problem with the RWA is that the averaging mechanism can be numerically unstable due to the accumulation operations when computing d_t.

Pros:
- Addresses an issue of RWAs.

Cons:
-The paper addresses a problem with an issue with RWAs. But it is not clear to me why would that be an important contribution.
-The writing needs more work.
-The experiments are lacking and the results are not good enough.

General Comments:

This paper addresses an issue regarding to RWA which is not really widely adopted and well-known architecture, because it seems to have some have some issues that this paper is trying to address. I would still like to have a better justification on why should we care about RWA and fixing that model. 

The writing of this paper seriously needs more work.  The Lemma 1 doesn't make sense to me, I think it has a typo in it, it should have been (-1)^t c instead of -1^t c.

The experiments are only on toyish and small scale tasks. According to the results the model doesn't really do better than a simple LSTM or GRU.","[3, 4, 6]","[' Clear rejection', ' Ok but not good enough - rejection', ' Marginally above acceptance threshold']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer raises several concerns about the paper, questioning the significance of the contribution, criticizing the writing clarity, and finding the experimental validation insufficient. While the reviewer acknowledges the paper attempts to address an existing issue with RWAs, the overall tone is rather negative due to the perceived lack of impact and rigor.",-50.0,20.0
Unleashing the Potential of CNNs for Interpretable Few-Shot Learning,"['Boyang Deng', 'Qing Liu', 'Siyuan Qiao', 'Alan Yuille']",Reject,2018,"[2, 2, 10, 36]","[7, 6, 14, 41]","[24, 22, 49, 702]","[7, 10, 21, 337]","[15, 11, 26, 263]","[2, 1, 2, 102]","The paper adds few operations after the pipeline for obtaining visual concepts from CNN as proposed by Wang et al. (2015). This latter paper showed how to extract from a CNN some clustered representations of the features of the internal layers of the network, working on a large training dataset. The clustered representations are the visual concepts. This paper shows that these representations can be used as exemplars by test images, in the same vein as bag of words used word exemplars to create the bag of words of unseen images.

 A simple nearest neighborhood and a likelihood model is built to assign a picture to an object class.

The results a are convincing, even if they are not state of the art in all the trials. 
The paper is very easy to follows, and the results are explained in a very simple way.


Few comments:
The authors in the abstract should revise their claims, too strong with respect to a literature field which has done many advancements on the cnn interpretation (see all the literature of Andrea Vedaldi) and the literature on zero shot learning, transfer learning, domain adaptation and fine tuning in general.","[7, 5, 4]","[' Good paper, accept', ' Marginally below acceptance threshold', ' Ok but not good enough - rejection']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The reviewer finds the paper's concept simple and not particularly novel, pointing out that it builds upon existing work with minor additions. While they acknowledge the results are convincing and the paper is well-written, they don't see it as a significant contribution to the field. The suggestion to revise claims in the abstract further indicates that the reviewer perceives the authors' tone as somewhat overstated.",20.0,50.0
Stochastic Hyperparameter Optimization through Hypernetworks,"['Jonathan Lorraine', 'David Duvenaud']",Reject,2018,"[1, 9]","[5, 13]","[16, 103]","[6, 52]","[10, 49]","[0, 2]","*Summary*

The paper proposes to use hyper-networks [Ha et al. 2016] for the tuning of hyper-parameters, along the lines of [Brock et al. 2017]. The core idea is to have a side neural network sufficiently expressive to learn the (large-scale, matrix-valued) mapping from a given configuration of hyper-parameters to the weights of the model we wish to tune.
The paper gives a theoretical justification of its approach, and then describes several variants of its core algorithm which mix the training of the hyper-networks together with the optimization of the hyper-parameters themselves. Finally, experiments based on MNIST illustrate the properties of the proposed approach.

While the core idea may appear as appealing, the paper suffers from several flaws (as further detailed afterwards):
-Insufficient related work
-Correctness/rigor of Theorem 2.1
-Clarity of the paper (e.g., Sec. 2.4)
-Experiments look somewhat artificial
-How scalable is the proposed approach in the perspective of tuning models way larger/more complex than those treated in the experiments?

*Detailed comments*

-""...and training the model to completion."" and ""This is wasteful, since it trains the model from scratch each time..."" (and similar statement in Sec. 2.1): Those statements are quite debatable. There are lines of work, e.g., in Bayesian optimization, to model early stopping/learning curves (e.g., Domhan2014, Klein2017 and references therein) and where training procedures are explicitly resumed (e.g., Swersky2014, Li2016). The paper should reformulate its statements in the light of this literature.

-""Uncertainty could conceivably be incorporated into the hypernet..."". This seems indeed an important point, but it does not appear as clear how to proceed (e.g., uncertainty on w_phi(lambda) which later needs to propagated to L_val); could the authors perhaps further elaborate?

-I am concerned about the rigor/correctness of Theorem 2.1; for instance, how is the continuity of the best-response exploited? Also, throughout the paper, the argmin is defined as if it was a singleton while in practice it is rather a set-valued mapping (except if there is a unique minimizer for L_train(., lambda), which is unlikely to be the case given the nature of the considered neural-net model). In the same vein, Jensen's inequality states that Expectation[g(X)] >= g(Expectation[X]) for some convex function g and random variable X; how does it precisely translate into the paper's setting (convexity, which function g, etc.)? 

-Specify in Alg. 1 that ""hyperopt"" refers to a generic hyper-parameter procedure.

-More details should be provided to better understand Sec. 2.4. At the moment, it is difficult to figure out (and potentially reproduce) the model which is proposed.

-The training procedure in Sec. 4.2 seems quite ad hoc; how sensitive was the overall performance with respect to the optimization strategy? For instance, in 4.2 and 4.3, different optimization parameters are chosen.

-typo: ""weight decay is applied the..."" --> ""weight decay is applied to the...""

-""a standard Bayesian optimization implementation from sklearn"": Could more details be provided? (there does not seem to be implementation there http://scikit-learn.org/stable/model_selection.html to the best of my knowledge)

-The experimental set up looks a bit far-fetched and unrealistic: first scalar, than diagonal and finally matrix-weighted regularization schemes. While the first two may be used in practice, the third scheme is not used in practice to the best of my knowledge.

-typo: ""fit a hypernet same dataset."" --> ""fit a hypernet on the same dataset.""

-(Franceschi2017) could be added to the related work section.

*References*

(Domhan2014) Domhan, T.; Springenberg, T. & Hutter, F. Extrapolating learning curves of deep neural networks ICML 2014 AutoML Workshop, 2014

(Franceschi2017) Franceschi, L.; Donini, M.; Frasconi, P. & Pontil, M. Forward and Reverse Gradient-Based Hyperparameter Optimization preprint arXiv:1703.01785, 2017

(Klein2017) Klein, A.; Falkner, S.; Springenberg, J. T. & Hutter, F. Learning curve prediction with Bayesian neural networks International Conference on Learning Representations (ICLR), 2017, 17

(Li2016) Li, L.; Jamieson, K.; DeSalvo, G.; Rostamizadeh, A. & Talwalkar, A. Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization preprint arXiv:1603.06560, 2016

(Swersky2014) Swersky, K.; Snoek, J. & Adams, R. P. Freeze-Thaw Bayesian Optimization preprint arXiv:1406.3896, 2014

*********
Update post rebuttal
*********

I acknowledge the fact that I read the rebuttal of the authors, whom I thank for their detailed answers.

My minor concerns have been clarified. Regarding the correctness of the proof, I am still unsure about the applicability of Jensen inequality; provided it is true, then it is important to see that the results seem to hold only for particular hyperparameters, namely regularization parameters (as explained in the new updated proof). This limitation should be exposed transparently upfront in the paper/abstract. 
Together with the new experiments and comparisons, I have therefore updated my rating from 5 to 6.
","[6, 6, 6]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[4, 1, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', "" The reviewer's evaluation is an educated guess"", ' The reviewer is fairly confident that the evaluation is correct']","The review starts with ""While the core idea may appear as appealing, the paper suffers from several flaws"", which sets a rather negative tone. The reviewer then lists a series of concerns, some of them quite serious (""correctness/rigor"", ""clarity of the paper"", ""experiments look somewhat artificial"").  The general tone is rather negative, but it is still professional and not aggressive. The reviewer acknowledges the authors' rebuttal and slightly upgrades their rating, which indicates they are open to discussion and willing to reconsider their judgment. Therefore, the sentiment is closer to negative, but not extremely negative, and the politeness is rather good.",-40.0,60.0
Orthogonal Recurrent Neural Networks with Scaled Cayley Transform,"['Kyle Helfrich', 'Devin Willmott', 'Qiang Ye']",Reject,2018,"[2, 8, 23]","[6, 12, 28]","[8, 12, 34]","[3, 3, 3]","[4, 6, 5]","[1, 3, 26]","The paper is clearly written, with a good coverage of previous relevant literature. 
The contribution itself is slightly incremental, as several different parameterization of orthogonal or almost-orthogonal weight matrices for RNN have been introduced.
Therefore, the paper must show that this new method performs better in some way compared with previous methods. They show that the proposed method is competitive on several datasets and a clear winner on one task: MSE on TIMIT.

Pros:
1. New, relatively simple method for learning orthogonal weight matrices for RNN

2. Clearly written

3. Quite good results on several relevant tasks.

Cons:
1. Technical novelty is somewhat limited

2. Experiments do not evaluate run time, memory use, computational complexity, or stability. Therefore it is more difficult to make comparisons: perhaps restricted-capacity uRNN is 10 times faster than the proposed method?","[5, 6, 7]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Good paper, accept']","[4, 3, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review acknowledges the merits of the paper, such as clear writing and good results ('clearly written', 'good coverage', 'quite good results'). However, it also points out the incremental nature of the contribution and the lack of certain experiments. The language is balanced, not overly enthusiastic nor overly negative, suggesting a slightly positive sentiment overall. The tone is professional and courteous throughout.",20.0,80.0
Clipping Free Attacks Against Neural Networks,['Boussad ADDAD'],Reject,2018,"[11, 24, 22]","[11, 24, 26]","[9, 7, 34]","[3, 2, 15]","[1, 1, 1]","[5, 4, 18]","This paper presents a reparametrization of the perturbation applied to features in adversarial examples based attacks. It tests this attack variation on against Inception-family classifiers on ImageNet. It shows some experimental robustness to JPEG encoding defense.

Specifically about the method: Instead of perturbating a feature x_i by delta_i, as in other attacks, with delta_i in range [-Delta_i, Delta_i], they propose to perturbate x_i^*, which is recentered in the domain of x_i through a heuristic ((x_i ± Delta_i + domain boundary that would be clipped)/2), and have a similar heuristic for computing a Delta_i^*. Instead of perturbating x_i^* directly by delta_i, they compute the perturbed x by x_i^* + Delta_i^* * g(r_i), so they follow the gradient of loss to misclassify w.r.t. r (instead of delta). 

+/-:
+ The presentation of the method is clear.
+ ImageNet is a good dataset to benchmark on.
- (!) The (ensemble) white-box attack is effective but the results are not compared to anything else, e.g. it could be compared to (vanilla) FGSM nor C&W.
- The other attack demonstrated is actually a grey-box attack, as 4 out of the 5 classifiers are known, they are attacking the 5th, but in particular all the 5 classifiers are Inception-family models.
- The experimental section is a bit sloppy at times (e.g. enumerating more than what is actually done, starting at 3.1.1.).
- The results on their JPEG approximation scheme seem too explorative (early in their development) to be properly compared.

I think that the paper need some more work, in particular to make more convincing experiments that the benefit lies in CIA (baselines comparison), and that it really is robust across these defenses shown in the paper.","[4, 5, 3]","[' Ok but not good enough - rejection', ' Marginally below acceptance threshold', ' Clear rejection']","[3, 2, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer provides a balanced perspective with both positive and negative points. They acknowledge the clear presentation and relevant dataset choice but raise concerns about missing baselines, attack clarity, experimental rigor, and the maturity of the JPEG defense evaluation. The overall tone is critical but professional, suggesting areas for improvement rather than outright rejection.",20.0,60.0
Novelty Detection with GAN,"['Mark Kliger', 'Shachar Fleishman']",Reject,2018,"[23, 13, 17]","[28, 18, 22]","[218, 72, 318]","[113, 32, 146]","[19, 13, 27]","[86, 27, 145]","
The paper proposes a GAN for novelty detection (predicting novel versus nominal data), using a mixture generator with feature matching loss.  The key difference between this paper and previous is the different definition of mixture generator.  Here the authors enforce p_other to have some significant mass in the tails of p_data (Def 1), forcing the 'other' data to be on or around the true data, creating a tight boundary around the nominal data.

The paper is well written, derives cleanly from previous work, and has solid experiments.  The experiments are weak 1) in the sense that they are not compared against simple baselines like p(x) (from, say, just thresholding a vae, or using a more powerful p(x) model -- there are lots out there), 2) other than KNNs, only compared with class-prediction based novelty detection (entropy, thresholds), and 3) in my view perform consistently, but not significantly better, than simply using the entropy of the class predictions.  How would entropy improve if it was a small ensemble instead of a single classifier?

The authors may be interested in [1], a principled approach for learning a well-calibrated uncertainty estimate on predictions.    Considering how well entropy works, I would be surprised in the model in [1] does not perform even better.

pros:
- good application of GAN models
- good writing and clarity
- solid experiments and explanations

cons:
 - results weak relative to naive baseline (entropy)
 - weak comparisons
 - lack of comparison to density models 


[1] Louizos, Christos, and Max Welling. ""Multiplicative Normalizing Flows for Variational Bayesian Neural Networks."" arXiv preprint arXiv:1703.01961 (2017).","[6, 4, 5]","[' Marginally above acceptance threshold', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer acknowledges the merits of the paper, such as good writing, clear derivation, solid experiments, and a good application of GAN models. However, they also express concerns about the weakness of the results compared to a simple baseline (entropy), weak comparisons overall, and the lack of comparison to density models. The tone is critical but professional and polite, suggesting areas of improvement without resorting to harsh language.",40.0,70.0
The (Un)reliability of saliency methods,"['Pieter-Jan Kindermans', 'Sara Hooker', 'Julius Adebayo', 'Kristof T. Schütt', 'Maximilian Alber', 'Sven Dähne', 'Dumitru Erhan', 'Been Kim']",Reject,2018,"[7, 2, 5, 3, 7, 9, 13, 9]","[11, 7, 9, 8, 11, 10, 17, 14]","[39, 30, 17, 13, 22, 26, 57, 74]","[17, 5, 6, 3, 6, 9, 26, 29]","[16, 23, 9, 9, 14, 4, 26, 41]","[6, 2, 2, 1, 2, 13, 5, 4]","The authors explore how different methods of visualizing network decisions (saliency methods) react to mean shifts of the input data by comparing them on two networks that are build to compensate for this mean shift. With the emergence of more and more saliency methods, the authors contribute an interesting idea to a very important and relevant discussion.

However, I'm missing a more general and principled discussion. The question that the authors address is how different saliency methods react to transformations of the input data. Since the authors make sure that their two models compensate for these transformation, the difference in saliency can be only due to underlying assumptions about the input data made by the saliency methods and therefore the discussion boils down to which invariance properties are justified for which kind of input -- it is not by chance that the attribution methods that work are exactly those that extract statistics from the input data and therefore compensate for the input transformation: IG with black reference point and Pattern Attribution.
The mean shift explored by the authors assumes that there is no special point in the input space (especially that zero is not a special point).
However, since images usally are considered bounded by 0 and 1 (or 255), there are in fact two special points (as a side note, in Figure 2 left column, the two inputs look very different which might be due to the fact that it is not at all obvious how to visualize ""image"" input that does not adhere to the common image input structure).
Would the authors argue that scaling the input with a positive factor should also lead to invariant saliency methods?
What about scaling with a negative factor?
I would argue that if the input has a certain structure, then it should be allowed for the saliency method to make use of this structure.

Minor points:

Understanding the two models in section 3 is a bit hard since the main point (both networks share the weights and biases except for the bias of the first layer) is only said in 2.1
","[4, 4, 5]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer acknowledges the relevance of the paper's topic and contribution ('interesting idea,' 'very important and relevant discussion'). However, they find the core argument lacking in depth and principle, wishing for a more generalized discussion about invariance properties of saliency methods. The reviewer poses several pointed questions to highlight this desire for a more thorough exploration. While the reviewer offers constructive criticism and suggestions, the overall tone leans towards a need for significant improvement. The language remains polite throughout, maintaining a professional and respectful dialogue.",20.0,70.0
Three factors influencing minima in SGD,"['Stanisław Jastrzębski', 'Zac Kenton', 'Devansh Arpit', 'Nicolas Ballas', 'Asja Fischer', 'Amos Storkey', 'Yoshua Bengio']",Reject,2018,"[6, 2, 8, 7, 9, 31, 23]","[10, 6, 13, 12, 14, 36, 28]","[53, 14, 45, 75, 79, 975, 135]","[16, 4, 20, 39, 30, 405, 62]","[29, 10, 25, 35, 36, 454, 54]","[8, 0, 0, 1, 13, 116, 19]","The paper investigates how the learning rate and mini-batch size in SGD impacts the optima that the SGD algorithm finds.
Empirically, the authors argue that it was observed that larger learning rates converge to minima which are more wide,
and that smaller learning rates more often lead to convergence to minima which are narrower, i.e. where the Hessian has large Eigenvalues. In this paper, the authors derive an analytical theory that aims at explaining this phenomenon.

Point of departure is an analytical theory proposed by Mandt et al., where SGD is analyzed in a continuous-time stochastic
formalism. In more detail, a stochastic differential equation is derived which mimicks the behavior of SGD. The advantage of
this theory is that under specific assumptions, analytic stationary distributions can be derived. While Mandt et al. focused
on the vicinity of a local optima, the authors of the present paper assumed white diagonal gradient noise, which allows to
derive an analytic, *global* stationary distribution (this is similar as in Langevin dynamics).

Then, the authors focus again on individual local optima and ""integrate out"" the stationary distribution around a local optimum, using again a Gaussian assumption. As a result, the authors obtain un-normalized probabilities of getting trapped in a given local optimum. This un-normalized probability depends on the strength of the value of the loss function in the vicinity of the optimum, the gradient noise, and the width of the optima. In the end, these un-normalized probabilities are taken as
probabilities that the SGD algorithm will be trapped around the given optimum in finite time.


Overall assessment:
I find the analytical results of the paper very original and interesting. The experimental part has some weaknesses. The paper could be drastically improved when focusing on the experimental part.

Detailed comments:

Regarding the analytical part, I think this is all very nice and original. However, I have some comments/requests:

1. Since the authors focus around Gaussian regions around the local minima, perhaps the diagonal white noise assumption could be weakened. This is again the multivariate Ornstein-Uhlenbeck setup examined in Mandt et al., and probably possesses an analytical solution for the un-normalized probabilities (even if the noise is multivariate Gaussian). Would the authors to consider generalizing the proof for the camera-ready version perhaps?

2. It would be nice to sketch the proof of theorem 2 in the main paper, rather than to just refer to the appendix. In my opinion, the theorem results from a beautiful and instructive calculation that should provide the reader with some intuition.

3. Would the authors comment on the underlying theoretical assumptions a bit more? In particular, the stationary distribution predicted by the Ornstein-Uhlenbeck formalism is never reached in practice. When using SGD in practice, one is in the initial mode-seeking phase. So, why is it a reasonable assumption to still use results obtained from the stationary (equilibrated) distribution which is never reached?


Regarding the experiments: here I see a few problems. First, the writing style drops in quality. Second, figures 2 and 3 are cryptic. Why do the authors focus on two manually selected optima? In which sense is this statistically significant? How often were the experiments repeated? The figures are furthermore hard to read. I would recommend overhauling the entire experiments section.

Details:

- Typo in Figure 2: ”with different with different”.
- “the endpoint of SGD with a learning rate schedule η → η/a, for some a > 0, and a constant batch size S, should be the same
  as the endpoint of SGD with a constant learning rate and a batch size schedule S → aS.” This is clearly wrong as there are many local minima, and running teh algorithm twice results in different local optima.  Maybe add something that this only true on average, like “the characteristics of these minima ... should be the same”.","[6, 5, 3]","[' Marginally above acceptance threshold', ' Marginally below acceptance threshold', ' Clear rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with a neutral summary of the paper. It then praises the analytical aspects, using terms like ""very original and interesting"" and ""beautiful and instructive calculation."" This suggests a positive sentiment. However, the reviewer identifies weaknesses in the experimental part, criticizing the clarity, methodology, and even stating that the writing quality drops. The tone remains professional and provides constructive feedback throughout, indicating politeness. Overall, the sentiment leans positive due to the strong endorsement of the analytical core, but the experimental concerns temper it.",60.0,70.0
Stabilizing Gradients for Deep Neural Networks via Efficient SVD Parameterization,"['Jiong Zhang', 'Qi Lei', 'Inderjit S. Dhillon']",Reject,2018,"[20, 8, 24]","[25, 13, 29]","[109, 92, 277]","[50, 40, 160]","[18, 35, 71]","[41, 17, 46]","This paper suggests a reparametrization of the transition matrix. The proposed reparametrization which is based on Singular Value Decomposition can be used for both recurrent and feedforward networks.

The paper is well-written and authors explain related work adequately. The paper is a follow up on Unitary RNNs which suggest a reparametrization that forces the transition matrix to be unitary. The problem of vanishing and exploding gradient in deep network is very challenging and any work that shed lights on this problem can have a significant impact. 

I have two comments on the experiment section:

- Choice of experiments. Authors have chosen UCR datasets and MNIST for the experiments while other experiments are more common. For example, the adding problem, the copying problem and the permuted MNIST problem and language modeling are the common experiments in the context of RNNs. For feedforward settings, classification on CIFAR10 and CIFAR100 is often reported.

- Stopping condition. The plots suggest that the optimization has stopped earlier for some models. Is this because of some stopping condition or because of gradient explosion? Is there a way to avoid this?

- Quality of figures. Figures are very hard to read because of small font. Also, the captions need to describe more details about the figures.","[5, 5, 7]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Good paper, accept']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer provides positive feedback in the first two paragraphs, praising the paper's clarity, writing, and relevance. They acknowledge the importance of addressing the vanishing and exploding gradient problem. While they raise valid points about the experiments, these are constructive suggestions for improvement rather than harsh criticisms. The tone throughout is professional and polite.",60.0,80.0
IVE-GAN: Invariant Encoding Generative Adversarial Networks,"['Robin Winter', 'Djork-Arnè Clevert']",Reject,2018,"[18, 13]","[22, 18]","[9, 27]","[2, 7]","[4, 11]","[3, 9]","The paper proposes a modified GAN objective, summarized in Eq.(3). It consists of two parts:
(A) Classic GAN term: \E_{x ~ P_{data} } \log D'(x) + \E_{z ~ P_{Z}, z' ~ P_{Z'}  } \log D'( G(z',E(x))   )
(B) Invariant Encoding term: \E_{x ~ P_{data} }  [ \log D(T(x),x) + \E_{z' ~ P_{Z'}  } \log D( G(z',E(x)), x   ) ]

Term (A) is standard, except the latent space of original GAN is decomposed into (1) the feature, which should be invariant between x and T(x), and (2) the noise, which is for the diversity of generated x.

Term (B) is the proposed invariant-encoding scheme. It is essentially a conditional GAN, where the the generated sample G(z',E(x)) is conditioned on input sample x, which guarantees that the generated sample is T(x) of x. 
In fact, this could be theoretically justified. Suggestion: the authors might want to follow the proofs of Proposition 1 or 2 in [*] to show similar conclusion, making the paper stronger.

[*] ALICE: Towards Understanding Adversarial Learning for Joint Distribution Matching, NIPS 2017

The definition of feature-invariant is E(T(x))=E(x)=E(G(z',E(x))), while the objective of the paper achieves T(x)=G(z',E(x)). Applying E() to both side yields the invariant features.  It might be better to make this point clear.

Overall Comments:

Originality: the proposed IVE-GAN algorithm is quite novel.
Quality: The paper could be stronger, if the theoretical justification has been provided. 
Clarity: Overall clear, while important details are missing. Please see some points in Detailed Comments.
Significance: The idea is interesting, it would be better if the quantitative evidence has been provided to demonstrate the use of the learned invariant feature. For example, some classification task to demonstrate the learned rotation-invariant feature shows higher accuracy.

Detailed Comments:

-- In Figure 1, please explains that ""||"" is the concatenation operator for better illustration.

-- When summarizing the contributions of the paper, it is mentioned that ""our GANs ... without mode collapsing issues"". This is a strong point to claim. While precisely identifying the ""mode collapsing issue"" itself is difficult, the authors only show that samples in all modes are generated on the toy datasets. Please consider to rephrase. 

-- In Section 2, y is first used to indicate true/false of x in Eq.(1), then y is used to indicate the associated information (e.g., class label) of x in Eq.(2). Please consider to avoid overloading notations.

-- In Eq.(3), the expectation \E_{z ~ P_Z} in the 3rd term is NOT clear, as z is not involved in the evaluation. I guess it may be implemented as z=E(x), where x ~ P_{data}. From the supplement tables, It seems that the novel sample G(z', E(x)) is implemented as G evaluated on the concatenation of noise sample z' ~ P_{Z'} and encoded feature z=E(x). 
I am wondering how to generate novel samples? Related to this,  Please clarify how to implement: ""To generate novel samples, we can draw samples z ~ P_Z as latent space"".

-- Section 5, ""include a encoding unit"" ---> ""an""

-- In Supplement, please revise G(z'E(x)) to G(z', E(x)) in every table.
","[5, 5, 4]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Ok but not good enough - rejection']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review provides constructive criticism, suggesting improvements like theoretical justification and clarification of certain points. While it acknowledges the novelty and interesting aspects of the work, it doesn't express overwhelmingly positive sentiment. The language is formal, professional, and polite throughout, suggesting areas for improvement without resorting to negativity.",50.0,80.0
An information-theoretic analysis of deep latent-variable models,"['Alex Alemi', 'Ben Poole', 'Ian Fischer', 'Josh Dillon', 'Rif A. Saurus', 'Kevin Murphy']",Reject,2018,"[4, 8, 13, 12, 3, 24]","[8, 13, 17, 17, 8, 29]","[46, 61, 45, 36, 43, 164]","[15, 24, 19, 12, 18, 91]","[30, 36, 23, 21, 24, 57]","[1, 1, 3, 3, 1, 16]","Summary:

This paper optimizes the beta-VAE objective and analyzes the resulting models in terms of the two components of the VAE loss: the reconstruction error (which the authors refer to as distortion, “D”) and the KL divergence term (which the authors refer to as rate, “R”). Various VAEs using either PixelCNN++ or a simpler model for the encoder, decoder, or marginal distribution of a VAE are trained on MNIST (with some additional results on OMNIGLOT) and analyzed in terms of samples, reconstructions, and their rate-distortion trade-off.

Review:

I find it difficult to point my finger to novel conceptual or theoretical insights in this paper. The idea of maximizing information for unsupervised learning of representations has of course been explored a lot (e.g., Bell & Sejnowski, 1995). Deeper connections between variational inference and rate-distortion have been made before (e.g., Balle et al., 2017; Theis et al., 2017), while this paper merely seems to rename the reconstruction and KL terms of the ELBO. Variational lower and upper bounds on mutual information have been used before as well (e.g., Barber & Agakov, 2003; Alemi et al., 2017), although they are introduced like new results in this paper. The derived “sandwich equation” only seems to be used to show that H - D - R <= 0, which also follows directly from Gibbs’ inequality (since the left-hand side is a negative KL divergence). The main contributions therefore seem to be the proposed analysis of models in the R-D plane, and the empirical contribution of analyzing beta-VAEs.

Based on the R-D plots, the authors identify a potential problem of generative models, namely that none of the trained models appear to get close to the “auto-encoding limit” where the distortion is close zero. Wouldn’t this gap easily be closed by a model with identity encoder, identity decoder, and PixelCNN++ for the marginal distribution? Given that autoregressive models generally perform better than VAEs in terms of log-likelihood, the model’s performance would probably be closer to the true entropy than the ELBO plotted in Figure 3a). What about increasing the capacity of the used in this paper? This makes me wonder what exactly the R-D plot can teach us about building better generative models.

The toy example in Figure 2 is interesting. What does it tell us about how to build our generative models? Should we be using powerful decoders but a lower beta?

The authors write: “we are able to learn many models that can achieve similar generative performance but make vastly different trade-offs in terms of the usage of the latent variable”. Yet in Figure 3b) it appears that changing the rate of a model can influence the generative performance (ELBO) quite a bit?","[5, 7, 5]","[' Marginally below acceptance threshold', ' Good paper, accept', ' Marginally below acceptance threshold']","[4, 5, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review is critical of the paper's novelty and questions the significance of its contributions. The reviewer finds the paper's insights to be already explored in prior work and raises concerns about the methodology and conclusions drawn. The tone, while critical, maintains a professional and analytical approach without resorting to personal attacks or disrespectful language.",-50.0,50.0
Character Level Based Detection of DGA Domain Names,"['Bin Yu', 'Jie Pan', 'Jiaming Hu', 'Anderson Nascimento', 'Martine De Cock']",Reject,2018,"[29, 27, 3, 18, 20]","[34, 32, 7, 23, 25]","[330, 121, 19, 121, 189]","[141, 46, 10, 48, 109]","[13, 7, 3, 45, 29]","[176, 68, 6, 28, 51]","
SUMMARY

This paper addresses the cybersecurity problem of domain generation algorithm (DGA)  detection. A class of malware uses algorithms to automatically generate artificial domain names for various purposes, e.g. to generate large numbers of rendezvous points. DGA detection concerns the (automatic) distinction of actual and artificially generated domain names. In this paper, a basic problem formulation and general solution approach is investigated, namely that of treating the detection as a text classification task and to let domain names arrive to the classifier as strings of characters. A set of five deep learning architectures (both CNNs and RNNs) are compared empirical on the text classification task. A domain name data set with two million instances is used for the experiments. The main conclusion is that the different architectures are almost equally accurate and that this prompts a preference of simpler architectures over more complex architectures, since training time and the likelihood for overfitting can potentially be reduced.

COMMENTS

The introduction is well-written, clear, and concise. It describes the studied real-world problem and clarifies the relevance and challenge involved in solving the problem. The introduction provides a clear overview of deep learning architectures that have already been proposed for solving the problem as well as some architectures that could potentially be used. One suggestion for the introduction is that the authors take some of the description of the domain problem and put it into a separate background section to reduce the text the reader has to consume before arriving at the research problem and proposed solution.

The methods section (Section 2) provides a clear description of each of the five architectures along with brief code listings and details about whether any changes or parameter choices were made for the experiment. In the beginning of the section, it is not clarified why, if a 75 character string is encoded as a 128 byte ASCII sequence, the content has to be stored in a 75 x 128 matrix instead of a vector of size 128. This is clarified later but should perhaps be discussed earlier to allow readers from outside the subarea to grasp the approach.

Section 3 describes the experiment settings, the results, and discusses the learned representations and the possible implications of using either the deep architectures or the “baseline” Random Forest classifier. Perhaps, the authors could elaborate a little bit more on why Random Forests were trained on a completely different set of features than the deep architectures? The data is stated to be randomly divided into training (80%), validation (10%), and testing (10%). How many times is this procedure repeated? (That is, how many experimental runs were averaged or was the experiment run once?).

In summary, this is an interesting and well-written paper on a timely topic. The main conclusion is intuitive. Perhaps the conclusion is even regarded as obvious by some but, in my opinion, the result is important since it was obtained from new, rather extensive experiments on a large data set and through the comparison of several existing (earlier proposed) architectures. Since the main conclusion is that simple models should be prioritised over complex ones (due to that their accuracy is very similar), it would have been interesting to get some brief comments on a simplicity comparison of the candidates at the conclusion.

MINOR COMMENTS

Abstract: “Little studies” -> “Few studies”

Table 1: “approach” -> “approaches”

Figure 1: Use the same y-axis scale for all subplots (if possible) to simplify comparison. Also, try to move Figure 1 so that it appears closer to its inline reference in the text.

Section 3: “based their on popularity” -> “based on their popularity”

","[7, 5, 4]","[' Good paper, accept', ' Marginally below acceptance threshold', ' Ok but not good enough - rejection']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is positive overall. The reviewer highlights the strengths of the paper, such as the well-written introduction, clear methodology, and interesting results. While the reviewer offers constructive criticism and suggestions for improvement, they are presented politely and with the aim of enhancing the paper. The reviewer acknowledges the value of the research, even if the main conclusion might seem intuitive, emphasizing its contribution to the field.",65.0,90.0
Exploring Asymmetric Encoder-Decoder Structure for Context-based Sentence Representation Learning,"['Shuai Tang', 'Hailin Jin', 'Chen Fang', 'Zhaowen Wang', 'Virginia R. de Sa']",Reject,2018,"[7, 19, 11, 12, 27]","[12, 24, 16, 17, 31]","[51, 162, 140, 132, 62]","[14, 95, 65, 64, 35]","[19, 46, 34, 48, 11]","[18, 21, 41, 20, 16]","Update:

I'm going to change my review to a 6 to acknowledge the substantial improvements you've made—I no longer fear that there are major errors in the paper, but this paper is still solidly borderline, and I'm not completely convinced that any new claim is true. The evidence presented for the main claim—that you can get by without an autoregressive decoder when pretraining encoders—is somewhat persuasive, but isn't as unequivocal as I'd hope, and even if the claim is true, it is arguably too limited a claim for an ICLR main conference paper. As R1 says, a *ACL short paper would be more appropriate.  The writing is also still unclear in places.

----

This paper presents a new RNN encoder–CNN decoder hybrid design for use in pretraining reusable sentence encoders on Kiros's SkipThought objective. The task is interesting and important, and the results are generally good: The new model outperforms SkipThought, and all other prior models for training sentence encoders on unlabeled data. However, some of the design choices seem a bit odd, and I have a large number of minor concerns about the paper. I'd like to see the authors' replies and the other reviews before I can confidently endorse this paper as correct.


Non-autoregressive decoding with a CNN strikes me as a somewhat ill-posed problem, even for in this case where you don't actually use the decoder in the final application of your model. At each position, you're training your model to predict a distribution over all words that could appear at the beginning/tenth position/twentieth position in sentences on some topic. I'd appreciate some more discussion of why this should or shouldn't hurt performance. I'd be less concerned about this if the results supporting the use of the CNN decoder were a bit more conclusive: while they are better on average across your smaller experiments, your largest experiment (2400D) shows them roughly tied.

Your paper opens with the line ""Context information plays an important role in human language understanding."" This sounds like it's making an empirical claim that your paper doesn't support, but it's so vague that it's hard to tell exactly what that claim is. Please clarify this or remove it.

This sentence is quite inaccurate: ""The idea of learning from the context information was first successfully applied to vector representation learning for words in Mikolov et al. (2013b) and learning from the occurrence of words also succeeded in Pennington et al. (2014)."" Turney and Pantel 2010 ( https://www.jair.org/media/2934/live-2934-4846-jair.pdf ) offer a survey of the substantial prior work that existed at that time.

The ""Neighborhood Hypothesis"" is given quite a lot of importance, given that it's a fairly small empirical effect without any corresponding theory. The fact that it's emphasized so heavily makes me suspect that I can guess the author of the paper. I'd tone down that part of your framing.

I would appreciate some more analysis of which of the non-central tricks that you describe in section 3 help. For example, max pooling seems reasonable, but you report yourself that mean pooling generally works much better in prior work. Without an explicit experiment, it's not clear why you'd add a mean pooling component.

It seems misleading to claim that your CNN is modeled after AdaSent, as that model uses a number of layers that varies with the length of the sentence (and differs from yours in a few other less-important ways). Please correct or clarify.

The use of “†” in table to denote models that predict the next sentence in a sequence doesn't make sense. It should apply to all of your models if I understand correctly. Please clarify.

You could do a better job at table placement and formatting. Table 3 is in the wrong section, for example.

You write that: ""Our proposed RNN-CNN model gets similar result on SNLI as Skip-thought, but with much less training time."" This seems to be based on a comparison between your model run on your hardware and their model run on their (presumably older) hardware, and possibly also with their older version of CuDNN. If that's right, you should tone down this claim or offer some more evidence.","[6, 7, 3]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Clear rejection']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The reviewer acknowledges improvements in the paper and finds the results 'somewhat persuasive' but still has reservations. They find the main claim 'arguably too limited' and point out areas of unclear writing. The language is critical but professional, suggesting a moderate rather than extreme sentiment.",40.0,60.0
Gaussian Process Neurons,"['Sebastian Urban', 'Patrick van der Smagt']",Reject,2018,"[21, 24]","[24, 29]","[7, 189]","[4, 88]","[0, 12]","[3, 89]","The paper addresses the problem of learning the form of the activation functions in neural networks.  The authors propose to place Gaussian process (GP) priors on the functional form of each activation function (each associated with a hidden layer and unit) in the neural net. This  somehow allows to non-parametrically infer from the data the ""shape"" of the activation functions needed for a specific problem.  The paper then proposes an inference framework (to approximately marginalize out all GP functions)  based on sparse GP methods that use inducing points and variational inference.  The inducing point approximation used here is very efficient since all GP functions depend on a scalar input (as any activation function!) and therefore by just placing the inducing points in a dense grid gives a fast and accurate representation/compression of all GPs in terms of the inducing function values (denoted by U in the paper).  Of course then inference involves approximating the finite posterior over inducing function values U and the paper make use of the standard Gaussian approximations.   
       
In general I like the idea and I believe that it can lead to a very useful model. However, I have found the current paper quite preliminary and incomplete.  The authors need to address the following:  

First (very important): You need to show experimentally how your method compares against regular neural nets (with specific fixed forms for their activation functions such relus etc). At the moment in the last section you mention ""We have validated networks of Gaussian Process Neurons in a set of experiments, the details of which we submit in a subsequent publication. In those experiments, our model shows to be significantly less prone to overfitting than a traditional feed-forward network of same size, despite having more parameters."" ===>  Well all this needs to be included in the same paper.  

Secondly: Discuss the connection with Deep GPs (Damianou and Lawrence 2013). Your method seems to be connected with Deep GPs although there appear to be important differences as well. E.g. you place GPs on the scalar activation functions in an otherwise  heavily parametrized neural network (having interconnection weights between layers) while deep GPs model the full hidden layer mapping as a single GP (which does not require interconnection weights).  

Thirdly:  You need to better explain the propagation of uncertainly in section 3.2.2  and the central limit of distribution in section 3.2.1. This is the technical part of your paper which is a non-standard approximation. I will suggest to give a better intuition of the whole idea and move a lot of mathematical details to the appendix.  
","[5, 4, 7]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Good paper, accept']","[4, 5, 2]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']","The reviewer expresses interest in the paper's idea, stating ""In general I like the idea and I believe that it can lead to a very useful model."" This suggests a somewhat positive sentiment. However, they also point out significant shortcomings and call the paper ""quite preliminary and incomplete."" The reviewer lists three major points that need to be addressed, indicating a need for substantial revisions. The language used is direct and critical but remains professional and within the bounds of typical academic discourse.",40.0,60.0
Finding ReMO (Related Memory Object): A Simple neural architecture for Text based Reasoning,"['Jihyung Moon', 'Hyochang Yang', 'Sungzoon Cho']",Reject,2018,"[1, 2, 30]","[6, 2, 34]","[12, 2, 135]","[6, 0, 38]","[6, 2, 7]","[0, 0, 90]","This paper proposes an alternative to the relation network architecture whose computational complexity is linear in the number of objects present in the input. The model achieves good results on bAbI compared to memory networks and the relation network model. From what I understood, it works by computing a weighted average of sentence representations in the input story where the attention weights are the output of an MLP whose input is just a sentence and question (not two sentences and a question). This average is then fed to a softmax layer for answer prediction. I found it difficult to understand how the model is related to relation networks, since it no longer scores every combination of objects (or, in the case of bAbI, sentences), which is the fundamental idea behind relation networks. Why is the approach not evaluated on CLEVR, in which the interaction between two objects is perhaps more critical (and was the main result of the original relation networks paper)? The fact that the model works well on bAbI despite its simplicity is interesting, but it feels like the paper is framed to suggest that object-object interactions are not necessary to explicitly model, which I can't agree with based solely on bAbI experiments. I'd encourage the authors to do a more detailed experimental study with more tasks, but I can't recommend this paper's acceptance in its current form.

other questions / comments:
- ""we use MLP to produce the attention weight without any extrinsic computation between the input sentence and the question."" isn't this statement false because the attention computation takes as input the concatenation of the question and sentence representation?
- writing could be cleaned up for spelling / grammar (e.g., ""last 70 stories"" instead of ""last 70 sentences""), currently the paper is very hard to read and it took me a while to understand the model","[4, 4, 4]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer acknowledges the model's good performance on bAbI but expresses concerns about its applicability beyond this dataset and its framing as a relation network alternative. The reviewer finds the paper difficult to understand in parts and suggests further experiments and writing improvements. The tone is critical but professional, with specific suggestions for improvement.",-20.0,60.0
Learning To Generate Reviews and Discovering Sentiment,"['Alec Radford', 'Rafal Jozefowicz', 'Ilya Sutskever']",Reject,2018,"[3, 6, 12]","[7, 8, 17]","[28, 18, 99]","[9, 6, 49]","[19, 11, 45]","[0, 1, 5]","This paper shows that an LSTM language model trained on a large corpus of Amazon product reviews can learn representations that are useful for sentiment analysis. 
Given representations from the language model, a logistic regression classifier is trained with supervised data from the task of interest to produce the final model.
The authors evaluated their approach on six sentiment analysis datasets (MR, CR, SUBJ, MPQA, SST, and IMDB), and found that the proposed method is competitive with existing supervised methods. 
The results are mixed, and they understandably are better for test datasets from similar domains to the Amazon product reviews dataset used to train the language model.
An interesting finding is that one of the neurons captures sentiment property and can be used to predict sentiment as a single unit.

I think the main result of the paper is not surprising and does not show much beyond we can do pretraining on unlabeled datasets from a similar domain to the domain of interest. 
This semi-supervised approach has been known to improve in the low data regime, and pretraining an expressive neural network model with a lot of unlabeled data has also been shown to help in the past.
There are a few unanswered questions in the paper:
- What are the performance of the sentiment unit on other datasets (e.g., SST, MR, CR)? Is it also competitive with the full model?
- How does this method compare to an approach that first pretrains a language model on the training set of each corpus without using the labels, and then trains a logistic regression while fixing the language model? Is the large amount of unlabeled data important to obtain good performance here? Or is similarity to the corpus of interest more important?
- I assume that the reason to use byte LSTM is because it is cheaper than a word level LSTM. Is this correct or was there any performance issue with using the word directly?
- More analysis on why the proposed method does well on the binary classification task of SST, but performs poorly on the fine-grained classification would be useful. If the model is capturing sentiment as is claimed by the authors, why does it only capture binary sentiment instead of a spectrum of sentiment level?

The paper is also poorly written. There are many typos (e.g., ""This advantage is also its difficulty"", ""Much previous work on language modeling has evaluated "", ""We focus in on the task"", and others) so the writing needs to be significantly improved for it to be a conference paper, preferably with some help from a native English speaker.","[4, 2, 4]","[' Ok but not good enough - rejection', ' Strong rejection', ' Ok but not good enough - rejection']","[5, 5, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer acknowledges the technical soundness of the work, indicating a somewhat positive sentiment. However, they find the results ""not surprising"" and point out several unanswered questions, suggesting room for improvement. The criticism regarding the writing and typos significantly impacts the overall positivity. Therefore, the sentiment is closer to neutral than overly positive. The language, while direct and critical, maintains a professional and respectful tone, avoiding personal attacks or overly harsh language.",20.0,60.0
The Principle of Logit Separation,"['Gil Keren', 'Sivan Sabato', 'Björn Schuller']",Reject,2018,"[3, 12, 18]","[8, 17, 23]","[34, 78, 964]","[15, 32, 573]","[15, 32, 167]","[4, 14, 224]","The paper is well-written which makes it easy to understand its main
thrust - choosing loss functions so that at test time one can
accurately (and speedily) determine whether an example is in a given
class, ie loss functions which are aligned with the ""Principle of Logit
Separation (PoLS)"". 

When the ""Principle of logit separation"" was first given (second page)
I found it confusing and difficult to parse (too many ""any""s, I could
not work out how the quantification worked). However, the formal
definition (Definition 2.1) was fine. Why not just use this - and drop
the vague, wordy definition?

The paper is fairly 'gentle'. For example, we are taken through
examples of loss functions which satisfy ""PoLS"" and those which don't.
No 'deep' mathematical reasoning is required - but I don't see this as
a deficiency.

The experiments are reasonably chosen and, as expected, show the
benefits of using PoLS-aligned loss functions.

My criticism of the paper is that I don't think there is enough
motivation. We have that normal classification is linear in the number
of classes. This modest computational burden (ie just linear),
apparently, is too slow for certain applications.  I would like more
evidence for this, including some examples of this problem including
in the paper. This is lacking from the current version.


typos, etc

max-maring -> max-margin
the seconds term -> the second term

","[6, 4, 3]","[' Marginally above acceptance threshold', ' Ok but not good enough - rejection', ' Clear rejection']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer provides a generally positive assessment, praising the paper's clarity, illustrative examples, and well-chosen experiments. While they point out a lack of strong motivation regarding the computational burden of traditional classification, the overall tone is constructive and helpful rather than dismissive. The reviewer also provides minor corrections, indicating a close and engaged reading.",60.0,80.0
Learning Less-Overlapping Representations,"['Hongbao Zhang', 'Pengtao Xie', 'Eric Xing']",Reject,2018,"[6, 5, 18]","[11, 8, 23]","[113, 9, 625]","[41, 4, 339]","[65, 4, 218]","[7, 1, 68]","The paper studies a regularization method to promote sparsity and reduce the overlap among the supports of the weight vectors in the learned representations. The motivation of using this regularization is to enhance the interpretability of the learned representation and avoid overfitting of complex models. 

To reduce the overlap among the supports of the weight vectors, an existing method (Xie et al, 2017b) encouraging orthogonality is adopted to let the Gram matrix of the weight vectors to be close to the identity matrix (so that each weight vector is with unit norm and any pair of vectors are approximately orthogonal).

Neural network and sparse coding are considered as two case studies. The alternating algorithm for solving the regularized sparse coding formulation is common and less attracted. I think the major point is to see how much benefit that the regularization can afford for learning deep neural networks. To avoid overfitting, some off-the-shelf methods, e.g., dropout which can be viewed as a kind of regularization, are commonly used for deep neural networks. Are there any connections between the adopted regularization terms and the existing methods? Will these less overlapped parameters control the activation of different neurons? I think these are some straightforward questions while there are not much explanations on those aspects.

For training neural networks, a simple sub-gradient method is used because of the non-smoothness of the regularization terms. When training with large neural networks, will the sub-gradient method affect the efficiency a lot compared without using the regularizer? For example, in the image classification problem with ResNet.

It is better not to use dropout in the experiments (language modeling and image classification), because one of the motivation of using the proposed regularizer is to avoid overfitting while dropout does the same work and may affect the evaluation of effectiveness of the regularization.
","[4, 3, 5]","[' Ok but not good enough - rejection', ' Clear rejection', ' Marginally below acceptance threshold']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is critical of the paper's novelty and depth, particularly questioning the choice of algorithm and lack of comparison with existing methods. It raises valid concerns about the experimental setup and suggests improvements. While the reviewer provides constructive criticism, the overall tone leans towards the critical side, indicating potential areas of improvement for the paper. The language used is professional and polite, adhering to academic standards.",-20.0,70.0
Style Memory: Making a Classifier Network Generative,"['Rey Wiyatno', 'Jeff Orchard']",Reject,2018,"[1, 18]","[5, 23]","[8, 60]","[2, 35]","[5, 6]","[1, 19]","The paper proposes training an autoencoder such that the middle layer representation consists of the class label of the input and a hidden vector representation called ""style memory"", which would presumably capture non-class information. The idea of learning representations that decompose into class-specific and class-agnostic parts, and more generally ""style"" and ""content"", is an interesting and long-standing problem. The results in the paper are mostly qualitative and only on MNIST. They do not show convincingly that the network managed to learn interesting class-specific and class-agnostic representations. It's not clear whether the examples shown in figures 7 to 11 are representative of the network's general behavior. The tSNE visualization in figure 6 seems to indicate that the style memory representation does not capture class information as well as the raw pixels, but doesn't indicate whether that representation is sensible.

The use of fully connected networks on images may affect the quality of the learned representations, and it may be necessary to use convolutional networks to get interesting results. It may also be interesting to consider class-specific representations that are more general than just the class label. For example, see ""Learning a Nonlinear Embedding by Preserving Class Neighbourhood Structure"" by Salakhutdinov and Hinton, 2007, which learns hidden vector representations for both class-specific and class-agnostic parts. (This paper should be cited.)","[3, 3, 4]","[' Clear rejection', ' Clear rejection', ' Ok but not good enough - rejection']","[5, 5, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']","The review is critical of the paper's methodology and results. It points out several weaknesses, such as the lack of quantitative results, the limited scope of the experiments, and the unconvincing nature of the evidence presented. The reviewer also suggests alternative approaches that might yield more interesting results. However, the language used is professional and avoids harsh criticism. It acknowledges the potential of the research direction while highlighting its shortcomings.",-30.0,70.0
Bayesian Uncertainty Estimation for Batch Normalized Deep Networks,"['Mattias Teye', 'Hossein Azizpour', 'Kevin Smith']",Reject,2018,"[1, 7, 15]","[5, 12, 20]","[2, 51, 34]","[1, 19, 19]","[0, 27, 11]","[1, 5, 4]","This paper proposes an approximate method to construct Bayesian uncertainty estimates in networks trained with batch normalization.

There is a lot going on in this paper. Although the overall presentation is clean, there are few key shortfalls (see below). Overall, the reported functionality is nice, although the experimental results are difficult to intepret (despite laudable effort by the authors to make them intuitive).

Some open questions that I find crucial:

* How exactly is the “stochastic forward-pass” performed that gives rise to the moment estimates? This step is the real meat of the paper, yet I struggle to find a concrete definition in the text. Is this really just an average over a few recent weights during optimization? If so, how is this method specific to batch normalization? Maybe I’m showing my own lack of understanding here, but it’s worrying that the actual sampling technique is not explained anywhere. This relates to a larger point about the paper's main point: What, exactly, is the Bayesian interpretation of batch normalization proposed here? In Bayesian Dropout, there is an explicit variational objective. Here, this is replaced by an implicit regularizer. The argument in Section 3.3 seems rather weak to me. To paraphrase it: If the prior vanishes, so does the regularizer. Fine. But what's the regularizer that's vanishing? The sentence that ""the influence of the prior diminishes as the size of the training data increases"" is debatable for something as over-parametrized as a DNN. I wouldn't be surprised that there are many directions in the weight-space of a trained DNN along which the posterior is dominated by the prior.

* I’m confused about the statements made about the “constant uncertainty” baseline. First off, how is this (constant) width of the predictive region chosen? Did I miss this, or is it not explained anywhere? Unless I misunderstand the definition of CRPS and PLL, that width should matter, no? Then, the paragraph at the end of page 8 is worrying: The authors essentially say that the constant baseline is quite close to the estimate constructed in their work because constant uncertainty is “quite a reasonable baseline”. That can hardly be true (if it is, then it puts the entire paper into question! If trivial uncertainty is almost as good as this method, isn't the method trivial, too?). 
On a related point: What would Figure 2 look like for the constand uncertainty setting? Just a horizontal line in blue and red? But at which level?

I like this paper. It is presented well (modulo the above problems), and it makes some strong points. But I’m worried about the empirical evaluation, and the omission of crucial algorithmic details. They may hide serious problems.","[5, 6, 5]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Marginally below acceptance threshold']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer finds the paper interesting and well-presented (""I like this paper. It is presented well...""). However, they also raise serious concerns about the clarity of the methodology and the interpretation of the results. The concerns, particularly about the core sampling technique and the baseline comparison, suggest a lack of conviction in the paper's claims. Therefore, the sentiment is somewhat positive but tempered by significant reservations.",40.0,70.0
YellowFin and the Art of Momentum Tuning,"['Jian Zhang', 'Ioannis Mitliagkas', 'Christopher Re']",Reject,2018,"[6, 9]","[9, 14]","[26, 78]","[12, 30]","[13, 46]","[1, 2]","[Apologies for short review, I got called in late. Marking my review as ""educated guess"" since i didn't have time for a detailed review]

The paper proposes an algorithm to tune the momentum and learning rate for SGD. While the algorithm does not have a theory for general non-quadratic functions, experimental validation is extensive, making it a worthy contribution in my opinion. I have personally tried the algorithm when the paper came out and can vouch for the empirical results presented here.","[6, 4, 4]","[' Marginally above acceptance threshold', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[1, 3, 5]","["" The reviewer's evaluation is an educated guess"", ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The reviewer explicitly states the contribution is worthy and that they vouch for the results. This points to a positive sentiment. The reviewer also apologizes for the short review, which points towards politeness. However, the lack of detailed feedback and use of ""educated guess"" slightly reduces the politeness score.",70.0,60.0
Network Signatures from Image Representation of Adjacency Matrices: Deep/Transfer Learning for Subgraph Classification,"['Kshiteesh Hegde', 'Malik Magdon-Ismail', 'Ram Ramanathan', 'Bishal Thapa']",Reject,2018,"[3, 22, 23, 12]","[4, 27, 27, 16]","[7, 211, 86, 19]","[4, 112, 53, 14]","[3, 49, 8, 2]","[0, 50, 25, 3]","The paper proposed a subgraph image representation and validate it in image classification and transfer learning problems. The image presentation is a minor extension based on a method of producing permutation-invariant adjacency matrix. The experimental results supports the claim.

It is very positive that the figures are very helpful for delivering the information.

The work seems to be a little bit incremental. The proposed image representation is mainly based on a previous work of permutation-invariant adjacency matrix. A novelty of this work seems to be transforming a graph into an image. By the proposed representation, the authors are able to apply image classification methods (supervised or unsupervised) to subgraph classification. 

It will be better if the authors could provide more details in the methodology or framework section.

The experiments on 9 networks support the claims that the image embedding approaches with their image representation of the subgraph outperform the graph kernel and classical features based methods. It seem to be promising when using transfer learning.

The last two process figures in 1.1 can be improved. No caption or figure number is provided.

It will be better to make the notations easy to understand and avoid any notation in a sentence without explanation nearby.
For example:
""the test example is correctly classified if and only if its ground truth matches C.""(P5)
""We carry out this exercise 4 times and set n to 8, 16, 32 and 64 respectively.""(P6)

Some minor issues:
""Zhu et al.(2011) discuss heterogeneous transfer learning where in they use...""(P3)
""Each label vector (a tuple of label, label-probability pairs)."" (incomplete sentence?P5)","[6, 6, 3]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Clear rejection']","[3, 3, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review acknowledges the paper's contributions as positive, noting the helpful figures and promising results. However, it also points out areas for improvement, such as the need for more methodological details and clearer notation. The suggestions are constructive and presented matter-of-factly. Overall, the tone is balanced, leaning towards the positive side due to the acknowledgment of the work's value.",60.0,70.0
Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update,"['Su Young Lee', 'Sungik Choi', 'Sae-Young Chung']",Reject,2018,"[11, 3, 21]","[14, 7, 25]","[5, 7, 151]","[2, 2, 57]","[2, 3, 43]","[1, 2, 51]","This paper proposes a new way of sampling data for updates in deep-Q networks. The basic principle is to update Q values starting from the end of the episode in order to facility quick propagation of rewards back along the episode.

The paper is interesting, but it lacks the proper comparisons to previously published techniques.

The results presented by this paper shows improvement over the baseline. But the Atari results is still significantly worse than the current SOTA.

In the non-tabular case, the authors have actually moved away from Q learning and defined an objective that is both on and off-policy. Some (theoretical) analysis would be nice. It is hard to judge whether the objective defined in the non-tabular defines a contraction operator at all in the tabular case.

There has been a number of highly relevant papers. Prioritized replay, for example, could have a very similar effect to proposed approach in the tabular case.

In the non-tabular case, the Retrace algorithm, tree backup, Watkin's Q learning all bear significant resemblance to the proposed method. Although the proposed algorithm is different from all 3, the authors should still have compared to at least one of them as a baseline. The Retrace algorithm specifically has also been shown to help significantly in the Atari case, and it defines a convergent update rule.","[5, 6, 4]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Ok but not good enough - rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with a positive note, acknowledging the paper's interesting proposal. However, it then raises several concerns about the lack of proper comparisons, limited improvement over SOTA, and the departure from true Q-learning in the non-tabular case. The reviewer also points out the similarity to existing algorithms and the need for further analysis and comparisons. The language used is direct and critical, but it maintains a professional and constructive tone throughout.",20.0,60.0
Long Short-Term Memory as a Dynamically Computed Element-wise Weighted Sum,"['Omer Levy', 'Kenton Lee', 'Nicholas FitzGerald', 'Luke Zettlemoyer']",Reject,2018,"[7, 7, 8, 21]","[12, 12, 13, 26]","[122, 66, 25, 346]","[55, 27, 13, 169]","[62, 36, 10, 167]","[5, 3, 2, 10]","This paper proposes a simplified LSTM variants by removing the non-linearity of content item and output gate. It shows comparable results with standard LSTM.

I believe this is a updated version of https://arxiv.org/abs/1705.07393 (Recurrent Additive Networks) with stronger experimental results. 

However, the formulation is very similar to ""[1] Semi-supervised Question Retrieval with Gated Convolutions"" 2016 by Lei, and ""Deriving Neural Architectures from Sequence and Graph Kernels"" which give theoretical view from string kernel about why this type of networks works. Both of the two paper don't have output gate and non-linearity of ""Wx_t"" and results on PTB also stronger than this paper. It also have some visualization about how the model decay the weights. Other AnonReviewer also point out some similar work. I won't repeat it here. In the paper, the author argued ""we propose and evaluate the minimal changes..."" but I think the these type of analysis also been covered by [1], Figure 5. 

On the experimental side, to draw the conclusion, ""weighted sum"" is enough for LSTM. I think at least Machine Translation and other classification results should be added. I'm not very familiar with SQuAD dataset, but the results seems worse than ""Reading Wikipedia to answer open-domain questions"" Table 4 which seems use a vanilla LSTM setup. 

Update: the revised version of the paper addresses all my concerns about experiments. So I increased my score. 
","[6, 7, 5]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Marginally below acceptance threshold']","[4, 3, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The reviewer acknowledges the work as an ""updated version"" with ""stronger experimental results"" which points towards a somewhat positive sentiment. However, they also point out significant overlap with prior work, somewhat diminishing the novelty aspect. The language remains largely neutral and professional throughout, with constructive criticism offered. The reviewer also increased their score based on the authors addressing their concerns, indicating a collaborative approach. Therefore, the sentiment leans slightly positive, but not overwhelmingly so.",30.0,70.0
Deep Temporal Clustering: Fully unsupervised learning of time-domain features,"['Naveen Sai Madiraju', 'Seid M. Sadat', 'Dimitry Fisher', 'Homa Karimabadi']",Reject,2018,"[1, 9, 3, 15]","[3, 9, 3, 17]","[3, 2, 3, 19]","[0, 0, 0, 10]","[3, 1, 3, 2]","[0, 1, 0, 7]","The authors proposed an algorithm named Deep Temporal Clustering (DTC) that integrates autoencoder with time-series data clustering. Compared to existing methods, DTC used a network structure (CNN + BiLSTM) that suits time-series data. In addition, a new clustering loss with different similarity measures are adopted to DTC. Experiments on different time-series data show the effectiveness of DTC compared to complete link. 

Although the idea of applying deep learning for temporal clustering is novel and interesting, the optimization problem is not clearly stated and experiments section is not comprehensive enough.

Here are the detailed comments.
The methods are described in a higher level language. The formula of overall loss function and its optimization should be written down to avoid unclearness.
The framework adopt the K-medoid clustering idea. But complete-link is used for initialization and comparison. Is that a difference? In addition, how to generate K centroids from complete-link clustering is not described at all.
The author Dynamic Time Warping is too expensive to integrate into DTC. However, most of the evaluated dataset are with small time points. Even for the longer ones, DTC does dimensionality reduction to make the time-series shorter. I do not see why quadratic computation is a problem here. DTW is most effective similarity measure for time-series data clustering. There is no excuse to skip it.
Is DTC robust to hyperparameters? If not, are there any guidelines to tune the hyperparameters, which is very important for unsupervised clustering. 

In summary, the method need to be described clearer, state-of-the-arts need to be compared and the usability of the method needs to be discussed. Therefore, at the current stage the paper cannot be accepted in my opinion. 
","[3, 5, 4]","[' Clear rejection', ' Marginally below acceptance threshold', ' Ok but not good enough - rejection']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with a positive sentiment, acknowledging the novelty and interesting aspects of the proposed algorithm. However, it then expresses several concerns regarding the clarity of the optimization problem, the comprehensiveness of the experiments, and the justification for certain methodological choices. The reviewer also raises important questions about the robustness and usability of the method. The overall tone suggests significant revisions are needed, leading to a negative sentiment overall. The language used is direct and critical, pointing out flaws and demanding improvements, but it maintains a professional and constructive tone without resorting to personal attacks or disrespectful language.",-30.0,60.0
Subspace Network: Deep Multi-Task Censored Regression for Modeling Neurodegenerative Diseases,"['Mengying Sun', 'Inci M. Baytas', 'Zhangyang Wang', 'Jiayu Zhou']",Reject,2018,"[2, 5, 10, 7, 12]","[7, 10, 15, 12, 17]","[32, 18, 84, 518, 189]","[16, 8, 55, 219, 106]","[5, 6, 14, 251, 48]","[11, 4, 15, 48, 35]","This work proposes a multi task learning framework for the modeling of clinical data in neurodegenerative diseases. 
Differently from previous applications of machine learning in neurodegeneration modeling, the proposed approach models the clinical data accounting for the bounded nature of cognitive tests scores. The framework is represented by a feed-forward deep architecture analogous to a residual network. At each layer a low-rank constraint is enforced on the linear transformation, while the cost function is specified in order to differentially account for the bounds of the predicted variables.

The idea of explicitly accounting for the boundedness of clinical scores is interesting, although the assumption of the proposed model is still incorrect: clinical scores are defined on discrete scales. For this reason the Gaussian assumption for the cost function used in the method is still not appropriate for the proposed application. 
Furthermore, while being the main methodological drive of this work, the paper does not show evidence about improved predictive performance and generalisation when accounting for the boundedness of the regression targets. 
The proposed algorithm is also generally compared with respect to linear methods, and the authors could have provided a more rigorous benchmark including standard non-linear prediction approaches (e.g. random forests, NN, GP, …). 

Overall, the proposed methods seems to provide little added value to the large amount of predictive methods proposed so far for prediction in neurodegenerative disorders. Moreover, the proposed experimental paradigm appears flawed. What is the interest of predicting baseline (or 6 months at best) cognitive scores (relatively low-cost and part of any routine clinical assessment) from brain imaging data (high-cost and not routine)?

Other remarks. 

- In section 2.2 and 4 there is some confusion between iteration indices and samples indices “i”. 

- Contrarily to what is stated in the introduction, the loss functions proposed in page 3 (first two formulas) only accounts for the lower bound of the predicted variables.  

-  Figure 2, synthetic data. The scale of the improvement of the subspace difference is quite tiny, in the order of 1e-2 when compared to U, and of 1e-5 across iterations. The loss function of Figure 2.b also does not show a strong improvement across iterations, while indicating a rather large instability of the optimisation procedure. These aspects may be a sign of convergence issues. 

- The dimensionality of the subspace representation importantly depends on the choice of the rank R of U and V. This is a crucial parameters that is however not discussed nor analysed in the paper. 

- The synthetic example of page 7 is quite misleading and potentially biased towards the proposed model. The authors are generating the synthetic data according to the model, and it is thus not surprising that they managed to obtain the best performance.  In particular, due to the nonlinear nature of (1), all the competing linear models are expected to perform poorly in this kind of setting.

- The computation time for the linear model shown in Table 3 is quite surprising (~20 minutes for linear regression of 5k observations). Is there anything that I am missing?
","[4, 5, 5]","[' Ok but not good enough - rejection', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[5, 4, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review is critical of the paper's approach, methodology, and results. The reviewer finds the core idea interesting but ultimately sees little value in the proposed method. They point out flaws in the experimental design, question the choice of benchmark models, and find issues with the synthetic data generation. The language used does not resort to personal attacks and maintains a professional tone, even when pointing out significant flaws.",-50.0,60.0
SQLNet: Generating Structured Queries From Natural Language Without Reinforcement Learning,"['Xiaojun Xu', 'Chang Liu', 'Dawn Song']",Reject,2018,"[11, 11, 20]","[16, 14, 25]","[78, 66, 440]","[30, 40, 254]","[19, 22, 154]","[29, 4, 32]","This submission proposes a new seq2sel solution by adopting two new techniques, a sequence-to-set model and column attention mechanism. They show performance improve over existing studies on WikiSQL dataset.

While the paper is written clearly, the contributions of the work heavily depends on the WikiSQL dataset. It is not sure if the approach is generally applicable to other sequence-to-sql workloads. Detailed comments are listed below:

1. WikiSQL dataset contains only a small class of SQL queries, with aggregation over single table and various filtering conditions. It does not involve any complex operator in relational database system, e.g., join and groupby. Due to its simple structure, the problem of sequence-to-sql translation over WikiSQL is actually simplified as a parameter selection problem for a fixed template. This greatly limits the generalization of approaches only applicable to WikiSQL. The authors are encouraged to explore other datasets available in the literature.

2. The ""order-matters"" motivation is not very convincing. It is straightforward to employ a global ordering approach to rank the columns and filtering conditions based on certain rules, e.g., alphabetical order. That could ensure the orders in the SQL results are always consistent.

3. The experiments do not fully verify how the approaches bring performance improvements. In the current version, the authors only report superficial accuracy results on final outcomes, without any deep investigation into why and how their approach works. For instance, they could verify how much accuracy improvement is due to the insensitivity to order in filtering expressions.

4. They do not compare against state-of-the-art solution on column and expression selection. While their attention mechanism over the columns could bring performance improvement, they should have included experiments over existing solutions designed for similar purpose. In (Yin, et al., IJCAI 2016), for example, representations over the columns are learned to generate better column selection.

As a conclusion, I find the submission contains certain interesting ideas but lacks serious research investigations. The quality of the paper could be much enhanced, if the authors deepen their studies on this direction.","[4, 7, 5]","[' Ok but not good enough - rejection', ' Good paper, accept', ' Marginally below acceptance threshold']","[5, 4, 5]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The reviewer acknowledges the clarity of the paper and the potential of the proposed approach, but expresses concerns about its limited generalizability and lack of in-depth analysis. The reviewer finds the paper's reliance on a specific dataset and the lack of comparison with existing solutions to be major drawbacks. The language used, while direct, is professional and not disrespectful.",20.0,60.0
Online Hyper-Parameter Optimization,"['Damien Vincent', 'Sylvain Gelly', 'Nicolas Le Roux', 'Olivier Bousquet']",Reject,2018,"[7, 16]","[7, 20]","[7, 116]","[6, 84]","[1, 25]","[0, 7]","
# Summary of paper
The paper proposes an algorithm for hyperparameter optimization that can be seen as an extension of Franceschi 2017 were some estimates are warm restarted to increase the stability of the method. 

# Summary of review
I find the contribution to be incremental, and the validation weak. Furthermore, the paper discusses the algorithm using hand-waiving arguments and lacks the rigor that I would consider necessary on an optimization-based contribution. None of my comments are fatal, but together with the incremental contribution I'm inclined as of this revision towards marginal reject. 

# Detailed comments

1. The distinction between parameters and hyperparameters (section 3) should be revised. First, the definition of parameters should not include the word parameters. Second, it is not clear what ""parameters of the regularization"" means. Typically, the regularization depends on both hyperparameters and parameters. The real distinction between parameters and parameters is how they are estimated: hyperparameters cannot be estimated from the same dataset as the parameters as this would lead to overfitting and so need to be estimated using a different criterion, but both are ""begin learnt"", just from different datasets.

2. In Section 3.1, credit for the approach of computing the hypergradient by backpropagating through the training procedure is attributed to Maclaurin 2015. This is not correct. This approach was first proposed in Domke 2012 and refined by Maclaurin 2015 (as correctly mentioned in Maclaurin 2015).

3. Some quantities are not correctly specified. I should not need to guess from the context or related literature what the quantities refer to. theta_K for example is undefined (although I could understand its meaning from the context) and sometimes used with arguments, sometimes without (i.e., both theta_K(lambda, theta_0) and theta_K are used).

4. The hypothesis are not correctly specified. Many of the results used require smoothness of the second derivative (e.g., the implicit function theorem) but these are nowhere stated.

5. The algorithm introduces too many hyper-hyperparameters, although the authors do acknowledge this. While I do believe that projecting into a compact domain is necessary (see Pedregosa 2016 assumption A3), the other parameters should ideally be relaxed or estimated from the evolution of the algorithm.

# Minor

missing . after ""hypergradient exactly"".

""we could optimization the hyperparam-"" (typo)

References:
 Justin  Domke.    Generic  methods  for  optimization-based modeling.  In
International Conference on Artificial Intelligence and Statistics, 2012.
","[5, 4, 4]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[3, 3, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer finds the paper's contribution to be incremental and the validation weak. They criticize the paper for using hand-waving arguments and lacking rigor. While the reviewer acknowledges that none of their comments are fatal, they are inclined towards a marginal rejection due to the combination of these factors. The language used is critical but professional and polite.",-50.0,50.0
Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates,"['Leslie N. Smith', 'Nicholay Topin']",Reject,2018,"[6, 5]","[11, 9]","[21, 29]","[3, 12]","[15, 17]","[3, 0]","This paper discusses the phenomenon of a fast convergence rate for training resnet with cyclical learning rates under a few particular setting. It tries to provide an explanation for the phenomenon and a procedure to test when it happens. However, I don't find the paper of high significance or the proposed method solid for publication at ICLR.

The paper is based on the cyclical learning rates proposed by Smith (2015, 2017). I don't understand what is offered beyond the original papers. The ""super-convergence"" occurs under special settings of hyper-parameters for resnet only and therefore I am concerned if it is of general interest for deep learning models. Also, the authors do not give a conclusive analysis under what condition it may happen.

The explanation of the cause of ""super-convergence"" from the perspective of  transversing the loss function topology in section 3 is rather illustrative at the best without convincing support of arguments. I feel most content of this paper (section 3, 4, 5) is observational results, and there is lack of solid analysis or discussion behind these observations.","[4, 4, 4]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[3, 3, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer expresses multiple concerns about the paper, stating it lacks significance and solid analysis. They find the explanation unconvincing and the results observational. While the reviewer doesn't resort to personal attacks or overly negative language, the critique is quite direct and critical. ",-50.0,20.0
Connectivity Learning in Multi-Branch Networks,"['Karim Ahmed', 'Lorenzo Torresani']",Reject,2018,"[3, 22]","[7, 27]","[12, 154]","[6, 83]","[5, 61]","[1, 10]","The paper is clear and well written.
It is an incremental modification of prior work (ResNeXt) that performs better on several experiments selected by the author; comparisons are only included relative to ResNeXt.

This paper is not about gating (c.f., gates in LSTMs, mixture of experts, etc) but rather about masking or perhaps a kind of block sparsity, as the ""gates"" of the paper do not depend upon the input: they are just fixed masking matrices (see eq (2)).

The main contribution appears to be the optimisation procedure for the binary masking tensor g. But this procedure is not justified: does each step minimise the loss? This seems unlikely due to the sampling. Can the authors show that the procedure will always converge? It would be good to contrast this with other attempts to learn discrete random variables (for example, The Concrete Distribution: Continuous Relaxation of Continuous Random Variables, Maddison et al, ICLR 2017).
","[5, 5, 5]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review starts with positive remarks on the clarity and writing of the paper, indicating a positive sentiment. However, the reviewer then notes that the work is ""incremental"" and questions the novelty and justification of the core contribution (the optimization procedure). The use of terms like ""not justified"" and ""seems unlikely"" point towards a critical stance. The reviewer provides constructive suggestions for improvement, which balances the criticism. Overall, the sentiment leans slightly towards the negative due to the concerns raised. The language used is professional and polite throughout, suggesting a neutral politeness score.",-10.0,0.0
Data augmentation instead of explicit regularization,"['Alex Hernández-García', 'Peter König']",Reject,2018,"[2, 29]","[7, 34]","[22, 96]","[5, 30]","[17, 9]","[0, 57]","The paper proposes data augmentation as an alternative to commonly used regularisation techniques like weight decay and dropout, and shows for a few reference models / tasks that the same generalization performance can be achieved using only data augmentation.

I think it's a great idea to investigate the effects of data augmentation more thoroughly. While it is a technique that is often used in literature, there hasn't really been any work that provides rigorous comparisons with alternative approaches and insights into its inner workings. Unfortunately I feel that this paper falls short of achieving this.

Experiments are conducted on two fairly similar tasks (image classification on CIFAR-10 and CIFAR-100), with two different network architectures. This is a bit meager to be able to draw general conclusions about the properties of data augmentation. Given that this work tries to provide insight into an existing common practice, I think it is fair to expect a much stronger experimental section. In section 2.1.1 it is stated that this was a conscious choice because simplicity would lead to clearer conclusions, but I think the conclusions would be much more valuable if variety was the objective instead of simplicity, and if larger-scale tasks were also considered.

Another concern is that the narrative of the paper pits augmentation against all other regularisation techniques, whereas more typically these will be used in conjunction. It is however very interesting that some of the results show that augmentation alone can sometimes be enough.

I think extending the analysis to larger datasets such as ImageNet, as is suggested at the end of section 3, and probably also to different problems than image classification, is going to be essential to ensure that the conclusions drawn hold weight.



Comments:

- The distinction between ""explicit"" and ""implicit"" regularisation is never clearly enunciated. A bunch of examples are given for both, but I found it tricky to understand the difference from those. Initially I thought it reflected the intention behind the use of a given technique; i.e. weight decay is explicit because clearly regularisation is its primary purpose -- whereas batch normalisation is implicit because its regularisation properties are actually a side effect. However, the paper then goes on to treat data augmentation as distinct from other explicit regularisation techniques, so I guess this is not the intended meaning. Please clarify this, as the terms crop up quite often throughout the paper. I suspect that the distinction is somewhat arbitrary and not that meaningful.

- In the abstract, it is already implied that data augmentation is superior to certain other regularisation techniques because it doesn't actually reduce the capacity of the model. But this ignores the fact that some of the model's excess capacity will be used to model out-of-distribution data (w.r.t. the original training distribution) instead. Data augmentation always modifies the distribution of the training data. I don't think it makes sense to imply that this is always preferable over reducing model capacity explicitly. This claim is referred to a few times throughout the work.

- It could be more clearly stated that the reason for the regularising effect of batch normalisation is the noise in the batch estimates for mean and variance.

- Some parts of the introduction could be removed because they are obvious, at least to an ICLR audience (like ""the model would not be regularised if alpha (the regularisation parameter) equals 0"").

- The experiments with smaller dataset sizes would be more interesting if smaller percentages were used. 50% / 80% / 100% are all on the same order of magnitude and this setting is not very realistic. In practice, when a dataset is ""too small"" to be able to train a network that solves a problem reliably, it will generally be one or more orders of magnitude too small, not 2x too small.

- The choices of hyperparameters for ""light"" and ""heavy"" motivation seem somewhat arbitrary and are not well motivated. Some parameters which are sampled uniformly at random should be probably be sampled log-uniformly instead, because they represent scale factors. It should also be noted that much more extreme augmentation strategies have been used for this particular task in literature, in combination with padding (for example by Graham). It would be interesting to include this setting in the experiments as well.

- On page 7 it is stated that ""when combined with explicit regularization, the results are much worse than without it"", but these results are omitted from the table. This is unfortunate because it is a very interesting observation, that runs counter to the common practice of combining all these regularisation techniques together (e.g. L2 + dropout + data augmentation is a common combination). Delving deeper into this could make the paper a lot stronger.

- It is not entirely true that augmentation parameters depend only on the training data and not the architecture (last paragraph of section 2.4). Clearly more elaborate architectures benefit more from data augmentation, and might need heavier augmentation to perform optimally because they are more prone to overfitting (this is in fact stated earlier on in the paper as well). It is of course true that these hyperparameters tend to be much more robust to architecture changes than those of other regularisation techniques such as dropout and weight decay. This increased robustness is definitely useful and I think this is also adequately demonstrated in the experiments.

- Phrases like ""implicit regularization operates more effectively at capturing reality"" are too vague to be meaningful.

- Note that weight decay has also been found to have side effects related to optimization (e.g. in ""Imagenet classification with deep convolutional neural networks"", Krizhevsky et al.)

REVISION: I applaud the effort the authors have put in to address many of my and the other reviewers' comments. I think they have done so adequately for the most part, so I've decided to raise the rating from 3 to 5, for what it's worth.

The reason I have decided not to raise it beyond that, is that I still feel that for a paper like this, which studies an existing technique in detail, the experimental side needs to be significantly stronger. While ImageNet experiments may be a lot of work, some other (smaller) additional datasets would also have provided more interesting evidence. CIFAR-10 and CIFAR-100 are so similar that they may as well be considered variants of the same dataset, at least in the setting where they are used here.

I do really appreciate the variety in the experiments in terms of network architectures, regularisation techniques, etc. but I think for real-world relevance, variety in problem settings (i.e. datasets) is simply much more important. I think it would be fine if additional experiments on other datasets were not varied along all these other axes, to cut down on the amount of work this would involve. But not including them at all unfortunately makes the results much less impactful.","[5, 5, 5]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is mostly positive, acknowledging the value of investigating data augmentation and praising the interesting results. However, it also expresses significant concerns about the limited scope of the experiments and the lack of clarity in some of the paper's claims. The reviewer suggests several improvements, particularly emphasizing the need for experiments on larger and more diverse datasets. The language used is polite and professional, offering constructive criticism and suggestions for improvement rather than harsh negativity.",40.0,80.0
Improving Deep Learning by Inverse Square Root Linear Units (ISRLUs),"['Brad Carlile', 'Guy Delamarter', 'Paul Kinney', 'Akiko Marti', 'Brian Whitney']",Reject,2018,"[23, 13, 18, 2, 17]","[22, 12, 17, 1, 16]","[3, 3, 2, 2, 11]","[0, 1, 1, 0, 7]","[2, 2, 1, 2, 1]","[1, 0, 0, 0, 3]","
Summary:
- The paper proposes a new activation function that looks similar to ELU but much cheaper by using the inverse square root function.

Contributions:
- The paper proposes a cheaper activation and validates it with an MNIST experiment. The paper also shows major speedup compared to ELU and TANH (unit-wise speedup).

Pros:
- The proposed function has similar behavior as ELU but 4x cheaper.
- The authors also refer us to faster ways to compute square root functions numerically, which can be of general interests to the community for efficient network designs in the future.
- The paper is clearly written and key contributions are well present.

Cons:
- Clearly, the proposed function is not faster than ReLU. In the introduction, the authors explain the motivation that ReLU needs centered activation (such as BN). But the authors also need to justify that ISRLU (or ELU) doesn’t need BN. In fact, in a recent study of ELU-ResNet (Shah et al., 2016) finds that ELU without BN leads to gradient explosion. To my knowledge, BN (at least in training time) is much more expensive than the activation function itself, so the speedup get from ISRLU may be killed by using BN in deeper networks on larger benchmarks. At inference time, all of ReLU, ELU, and ISRLU can fuse BN weights into convolution weights, so again ISRLU will not be faster than ReLU. The core question here is, whether the smoothness and centered zero property of ELU can buy us any win, compared to ReLU? I couldn’t find it based on the results presented here.
- The authors need to validate on larger datasets (e.g. CIFAR, if not ImageNet) so that their proposed methods can be widely adopted.
- The speedup is only measured on CPU. For practical usage, especially in computer vision, GPU speedup is needed to show an impact.

Conclusion:
- Based on the comments above, I recommend weak reject.

References:
- Shah, A., Shinde, S., Kadam, E., Shah, H., Shingade, S.. Deep Residual Networks with Exponential Linear Unit. In Proceedings of the Third International Symposium on Computer Vision and the Internet (VisionNet'16).","[5, 4, 3]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Clear rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer acknowledges the contributions of the paper and its clear writing style, pointing out positive aspects like the cheaper activation function and its similarity to ELU. However, they express significant concerns about the practical benefits and limited validation, leading to a 'weak reject' recommendation. The cons section, longer than the pros, highlights these reservations. While the language is critical, it maintains a professional and analytical tone without resorting to personal attacks or disrespectful language.",-10.0,60.0
GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks,"['Zhao Chen', 'Vijay Badrinarayanan', 'Chen-Yu Lee', 'Andrew Rabinovich']",Reject,2018,"[2, 12, 24, 16]","[3, 15, 29, 18]","[6, 43, 74, 40]","[2, 18, 39, 18]","[4, 21, 26, 21]","[0, 4, 9, 1]","The paper addresses an important problem in multitask learning. But its current form has several serious issues. 

Although I get the high-level goal of the paper, I find Sec. 3.1, which describes the technical approach, nearly incomprehensible. There are many things unclear. For example:

-  it starts with talking about multiple tasks, and then immediately talks about a ""filter F"", without defining what the kind of network is being addressed. 

- Also it is not clear what L_grad is. It looks like a loss, but Equation 2 seems to define it to be the difference between the gradient norm of a task and the average over all tasks. It is not clear how it is used. In particular, it is not clear how it is used to ""update the task weights""

- Equation 2 seems sloppy. “j” appears as a free index on the right side, but it doesn’t appear on the left side. 

As a result, I am unable to understand how the method works exactly, and unable to judge its quality and originality.

The toy experiment is not convincing. 

- the evaluation metric is the sum of the relative losses, that is, the sum of the original losses weighted by the inverse of the initial loss of each task. This is different from the sum of the original losses, which seems to be the one used to train the “equal weight” baseline. A more fair baseline is to directly use the evaluation metric as the training loss. 
- the curves seem to have not converged.

The experiments on NYUv2 involves non-standard settings, without a good justification. So it is not clear if the proposed method can make a real difference on state of the art systems. 

And the reason that the proposed method outperforms the equal weight baseline seems to be that the method prevents overfitting on some tasks (e.g. depth). However, the method works by normalizing the norms of the gradients, which does not necessarily prevent overfitting — it can in fact magnify gradients of certain tasks and cause over-training and over-fitting. So the performance gain is likely dataset dependent, and what happens on NYU depth can be a fluke and does not necessarily generalize to other datasets. ","[4, 4, 6]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Marginally above acceptance threshold']","[4, 4, 2]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']","The review is highly critical of the paper, pointing out major flaws in clarity, methodology, and experimental setup. The reviewer clearly states they don't understand the core ideas and finds the experiments unconvincing. While the language is direct and points out serious issues, it avoids personal attacks and maintains a professional tone.",-75.0,50.0
Improving image generative models with human interactions,"['Andrew Kyle Lampinen', 'David So', 'Douglas Eck', 'Fred Bertsch']",Reject,2018,"[2, 2, 20, 2]","[7, 1, 24, 6]","[36, 1, 89, 9]","[12, 0, 59, 3]","[23, 1, 22, 6]","[1, 0, 8, 0]","Summary:
This paper proposes an approach to generate images which are more aesthetically pleasing, considering the feedback of users via user interaction. However, instead of user interaction, it models it by a simulated measure of the quality of user interaction and then feeds it to a Gan architecture. 

Pros:
+ The paper is well-written and has just a few typos: 2.1: “an Gan”.
+ The idea is very interesting. 

Cons:

- Page 2- section 2- The reasoning that a deep-RL could not be more successful is not supported by any references and it is not convincing.

- Page 3- para 3 - mathematically the statement does not sound since the 2 expressions are exactly equivalent. The slight improvement may be achieved only by chance and be due to computational inefficiency, or changing a seed.  

- Page 3- 2.2. Using a crowd-sourcing technique, developing a similarly small dataset (1000 images with 100 annotations) would normally cost less than 1k$.

- Page 3- 2.2.It is highly motivating to use users feedback in the loop but it is poorly explained how actually the user's' feedback is involved if it is involved at all. 

- Page 4- sec 3 "".. it should be seen as a success""; the claim is not supported well.

- Page 4- sec 3.2- last paragraph.
This claim lacks scientific support, otherwise please cite proper references. The claim seems like a subjective understanding of conscious perception and unconscious perception of affective stimuli is totally disregarded.
The experimental setup is not convincing.

- Page 4. 3.3) ""Note that.. outdoor images"" this is implicitly adding the designers' bias to the results. The statement lacks scientific support.

- Page 4. 3.3) the importance of texture and shape is disregarded. “In the Eye of the Beholder: Employing Statistical Analysis and Eye Tracking for Analyzing Abstract Paintings, Yanulevskaya et al”
The architecture may lead in overfitting to users' feedback (being over-fit on the data with PIR measures)

- Page 6-Sec 4.2) "" It had more difficulty optimizing for the three-color result"" why? please discuss it.

- The expectation which is set in the abstract and the introduction of the paper is higher than the experiments shown in the Experimental setup.
","[5, 4, 4]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[3, 4, 5]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review is highly critical of the paper, pointing out multiple flaws in the reasoning, methodology, and claims. The reviewer finds the paper's core idea interesting but ultimately unconvinced by the execution. The language used is quite direct and critical, focusing on the scientific shortcomings rather than couching the feedback in overly polite terms.",-50.0,20.0
"MACH: Embarrassingly parallel $K$-class classification in $O(d\log{K})$ memory and $O(K\log{K} + d\log{K})$ time, instead of $O(Kd)$","['Qixuan Huang', 'Anshumali Shrivastava', 'Yiqiu Wang']",Reject,2018,"[1, 8, 2]","[2, 13, 6]","[3, 164, 18]","[1, 74, 7]","[2, 86, 9]","[0, 4, 2]","Thanks to the authors for their feedback.
==============================
The paper presents a method for classification scheme for problems involving large number of classes in multi-class setting. This is related to the theme of extreme classification but the setting is restricted to that of multi-class classification instead of multi-label classification. The training process involves data transformation using R hash functions, and then learning R classifiers. During prediction the probability of a test instance belonging to a class is given by the sum of the probabilities assigned by the R meta-classifiers to the meta-class in the which the given class label falls. The paper demonstrates better results on ODP and Imagenet-21K datasets compared to LOMTree, RecallTree and OAA.

There are following concerns regarding the paper which don't seem to be adequately addressed :
 
 - The paper seems to propose a method in which two-step trees are being constructed based on random binning of labels, such that the first level has B nodes. It is not intuitively clear why such a method could be better in terms of prediction accuracy than OAA. The authors mention algorithms for training and prediction, and go on to mention that the method performs better than OAA. Also, please refer to point 2 below.

 - The paper repeatedly mentions that OAA has O(Kd) storage and prediction complexity. This is however not entirely true due to sparsity of training data, and the model. These statements seem quite misleading especially in the context of text datasets such as ODP. The authors are requested to check the papers [1] and [2], in which it is shown that OAA can perform surprisingly well. Also, exploiting the sparsity in the data/models, actual model sizes for WikiLSHTC-325K from [3] can be reduced from around 900GB to less than 10GB with weight pruning, and sparsity inducing regularizers. It is not clear if the 160GB model size reported for ODP took the above suggestions into considerations, and which kind of regularization was used. Was the solver used from vowpal wabbit or packages such as Liblinear were used for reporting OAA results.

 - Lack of empirical comparison - The paper lacks empirical comparisons especially on large-scale multi-class LSHTC-1/2/3 datasets [4] on which many approaches have been proposed. For a fair comparison, the proposed method must be compared against these datasets. It would be important to clarify if the method can be used on multi-label datasets or not, if so, it needs to be evaluated on the XML datasets [3].

[1] PPDSparse - http://www.kdd.org/kdd2017/papers/view/a-parallel-and-primal-dual-sparse-method-for-extreme-classification
[2] DiSMEC - https://arxiv.org/abs/1609.02521
[3] http://manikvarma.org/downloads/XC/XMLRepository.html
[4] http://lshtc.iit.demokritos.gr/LSHTC2_CFP","[6, 6, 6]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is critical of the paper, pointing out several flaws and areas for improvement. The reviewer questions the clarity and novelty of the proposed method, criticizes the lack of comparison with relevant work, and suggests additional experiments. The language used is professional and not overtly negative, but the tone is clearly one of skepticism and the reviewer expresses concerns about the paper's claims. Therefore, the sentiment is significantly negative, but not extremely so, as the reviewer does acknowledge the authors' feedback and provides concrete suggestions for improvement.",-60.0,60.0
Deterministic Policy Imitation Gradient Algorithm,"['Fumihiro Sasaki', 'Atsuo Kawaguchi']",Reject,2018,"[0, 32]","[5, 35]","[7, 7]","[4, 5]","[0, 0]","[3, 2]","The paper lists 5 previous very recent papers that combine IRL, adversarial learning, and stochastic policies. The goal of this paper is to do the same thing but with deterministic policies as a way of decreasing the sample complexity. The approach is related to that used in the deterministic policy gradient work. Imitation learning results on the standard control problems appear very encouraging.

Detailed comments:

""s with environment"" -> ""s with the environment""?

""that IL algorithm"" -> ""that IL algorithms"".

""e to the real-world environments"" -> ""e to real-world environments"".

"" two folds"" -> "" two fold"".

""adopting deterministic policy"" -> ""adopting a deterministic policy"".

""those appeared on the expert’s demonstrations"" -> ""those appearing in the expert’s demonstrations"".

""t tens of times less interactions"" -> ""t tens of times fewer interactions"".

Ok, I can't flag all of the examples of disfluency. The examples above come from just the abstract. The text of the paper seems even less well edited. I'd highly recommend getting some help proof reading the work.

""Thus, the noisy policy updates could frequently be performed in IL and make the learner’s policy poor. From this observation, we assume that preventing the noisy policy updates with states that are not typical of those appeared on the expert’s demonstrations benefits to the imitation."": The justification for filtering is pretty weak. What is the statistical basis for doing so? Is it a form of a standard variance reduction approach? Is it a novel variance reduction approach? If so, is it more generally applicable?

Unfortunately, the text in Figure 1 is too small. The smallest font size you should use is that of a footnote in the text. As such, it is very difficult to assess the results.

As best I can tell, the empirical results seem impressive and interesting.
","[5, 5, 6]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Marginally above acceptance threshold']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with a neutral summary of the paper's goals and relates it to existing literature. While it points out that the empirical results seem ""impressive and interesting,""  the numerous language and clarity issues throughout the paper detract significantly from the overall impression. The reviewer also raises a valid concern about the justification of a core technical aspect (""filtering""), indicating potential weakness in the paper's methodology.  The tone, while direct, maintains a professional and constructive approach. ",20.0,60.0
Recursive Binary Neural Network Learning Model  with 2-bit/weight Storage Requirement,"['Tianchan Guan', 'Xiaoyang Zeng', 'Mingoo Seok']",Reject,2018,"[6, 14, 12]","[10, 19, 17]","[18, 337, 155]","[10, 209, 99]","[4, 11, 8]","[4, 117, 48]","The idea of this work is fairly simple. Two main problems exist in end devices for deep learning: power and memory. There have been a series of works showing how to discretisize neural networks. This work, discretisize a NN incrementally. It does so in the following way: First, we train the network with the memory we have. Once we train and achieve a network with best performance under this constraint, we take the sign of each weight (and leave them intact), and use the remaining n-1 bits of each weight in order to add some new connections to the network. Now, we do not change the sign weights, only the new n-1 bits. We continue with this process (recursively) until we don't get any improvement in performance. 

Based on experiments done by the authors, on MNIST, having this procedure gives the same performance with 3-4 times less memory or increase in performance of 1% for the same memory as regular network. 

I like the idea, and I think it is indeed a good idea for IoT and end devices. The main problem with this method that there is undiscussed payment with current hardware architectures. I think there is a problem with optimizing the memory after each stage was trained. Also, current architectures do not support a single bit manipulations, but is much more efficient on large bits registers. So, in theory this might be a good idea, but I think this idea is not out-of-the-box method for implementation.

Also, as the authors say, more experiments are needed in order to understand the regime in which this method is efficient. To summarize, I like this idea, but more experiments are needed in order to understand this method merits. ","[7, 6, 5]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Marginally below acceptance threshold']","[4, 3, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer starts by acknowledging the idea as ""good"" and expressing positive sentiment (""I like the idea""). They also clearly state the potential of the idea for its intended application domain. However, they point out a significant drawback regarding practical implementation and call for more experiments. Overall, the sentiment leans positive due to the initial praise and the concluding encouragement, but the identified limitations hold it back from being overly positive.",60.0,70.0
The Information-Autoencoding Family: A Lagrangian Perspective on Latent Variable Generative Modeling,"['Shengjia Zhao', 'Jiaming Song', 'Stefano Ermon']",Reject,2018,"[4, 4, 10]","[8, 9, 15]","[54, 120, 406]","[28, 55, 199]","[26, 60, 200]","[0, 5, 7]","Update after rebuttal
==========
Thanks for your response on my questions. The stated usefulness of the method unfortunately do not answer my worry about the significance. It remains unclear to me how much ""real"" difference the presented results would make to advance the existing work on generative models. Also, the authors did not promised any major changes in the final version in this direction, which is why I have reduced my score.

I do believe that this work could be useful and should be resubmitted. There are two main things to improve. First, the paper need more work on improving the clarity. Second, more work needs to be added to show that the paper will make a real difference to advance/improve existing methods.

==========
Before rebuttal
==========
This paper proposes an optimization problem whose Lagrangian duals contain many existing objective functions for generative models. Using this framework, the paper tries to generalize the optimization problems by defining computationally-tractable family which can be expressed in terms of existing objective functions. 

The paper has interesting elements and the results are original. The main issue is that the significance is unclear. The writing in Section 3 is unclear for me, which further made it challenging to understand the consequences of the theorems presented in that section. 

Here is a big-picture question that I would like to know answer for. Do the results of sec 3 help us identify a more useful/computationally tractable model than exiting approaches? Clarification on this will help me evaluate the significance of the paper.

I have three main clarification points. First, what is the importance of T1, T2, and T3 classes defined in Def. 7, i.e., why are these classes useful in solving some problems? Second, is the opposite relationship in Theorem 1, 2, and 3 true as well, e.g., is every linear combination of beta-ELBO and VMI is equivalent to a likelihood-based computable-objective of KL info-encoding family? Is the same true for other theorems?

Third, the objective of section 3 is to show that ""only some choices of lambda lead to a dual with a tractable equivalent form"". Could you rewrite the theorems so that they truly reflect this, rather than stating something which only indirectly imply the main claim of the paper.

Some small comments:
- Eq. 4. It might help to define MI to remind readers.
- After Eq. 7, please add a proof (may be in the Appendix). It is not that straightforward to see this. Also, I suppose you are saying Eq. 3 but with f from Eq. 4.
- Line after Eq. 8, D_i is ""one"" of the following... Is it always the same D_i for all i or it could be different? Make this more clear to avoid confusion.
- Last line in Para after Eq. 15, ""This neutrality corresponds to the observations made in.."" It might be useful to add a line explaining that particular ""observation""
- Def. 7, the names did not make much sense to me. You can add a line explaining why this name is chosen.
- Def. 8, the last equation is unclear. Does the first equivalence impy the next one? 
- Writing in Sec. 3.3 can be improved. e.g., ""all linear operations on log prob."" is very unclear, ""stated computational constraints"" which constraints?
","[5, 4, 6]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Marginally above acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer finds the paper to have ""interesting elements"" and the results to be original, which points towards a somewhat positive sentiment. However, they have major concerns about the significance and clarity of the work, pulling the sentiment down. The repeated use of ""unclear"" and the need for clarification on core concepts suggests the paper needs significant revisions to be considered positive. 

The language used is quite typical of academic peer reviews. The reviewer points out areas for improvement directly and asks specific questions about the work. While direct, the language doesn't come across as rude or disrespectful. ",20.0,50.0
CNNs as Inverse Problem Solvers and Double Network Superresolution,"['Cem TARHAN', 'Gözde BOZDAĞI AKAR']",Reject,2018,"[1, 27]","[2, 32]","[2, 125]","[0, 89]","[1, 11]","[1, 25]","The method proposes a new architecture for solving image super-resolution task. They provide an analysis that connects aims to establish a connection between how CNNs for solving super resolution and solving sparse regularized inverse problems.

The writing of the paper needs improvement. I was not able to understand the proposed connection, as notation is inconsistent and it is difficult to figure out what the authors are stating. I am willing to reconsider my evaluation if the authors provide clarifications.

The paper does not refer to recent advances in the problem, which are (as far as I know), the state of the art in the problem in terms of quality of the solutions. This references should be added and the authors should put their work into context.

1) Arguably, the state of the art in super resolution are techniques that go beyond L2 fitting. Specifically, methods using perceptual losses such as:

Johnson, J. et al ""Perceptual losses for real-time style transfer and super-resolution."" European Conference on Computer Vision. Springer International Publishing, 2016.

Ledig, Christian, et al. ""Photo-realistic single image super-resolution using a generative adversarial network."" arXiv preprint arXiv:1609.04802 (2016).

PSNR is known to not be directly related to image quality, as it favors blurred solutions. This should be discussed.

2) The overall notation of the paper should be improved. For instance, in (1), g represents the observation (the LR image), whereas later in the text, g is the HR image. 

3) The description of Section 2.1 is quite confusing in my view. In equation (1), y is the signal to be recovered and K is just the downsampling plus blurring. So assuming an L1 regularization in this equation assumes that the signal itself is sparse. Equation (2) changes notation referring y as f. 

4) Equation (2) seems wrong. The term multiplying K^T is not the norm (should be parenthesis).

5) The first statement of Section 2.2. seems wrong. DL methods do state the super resolution problem as an inverse problem. Instead of using a pre-defined basis function they learn an over-complete dictionary from the data, assuming that natural images can be sparsely represented. Also, this section does not explain how DL is used for super resolution. The cited work by Yang et al learns a two coupled dictionaries (one for LR and HL), such that for a given patch, the same sparse coefficients can reconstruct both HR and LR patches. The authors just state the sparse coding problem.

6) Equation (10) should not contain the \leq \epsilon.

7) In the second paragraph of Section 3, the authors mention that the LR image has to be larger than the HR image to prevent border effects. This makes sense. However, with the size of the network (20 layers), the change in size seems to be quite large. Could you please provide the sizes? When measuring PSNR, is this taken into account? 

8) It would be very helpful to include an image explaining the procedure described in the second paragraph of Section 3.

9) I find the description in Section 3 quite confusing. The authors relate the training of a single filter (or neuron) to equation (7), but they define D, that is not used in all of Section 2.1. And K does not show in any of the analysis given in the last paragraph of page 4. However, D and K seem two different things (it is not just one for the other), see bellow.

10) I cannot understand the derivation that the authors do in the last paragraph of page 4 (and beginning of page 5). What is phi_l here? K in equation (7) seems to match to D here, but D here is a collection of patches and in (7) is a blurring and downsampling operator. I cannot review this section. I will wait for the author's response clarifications.

11) The authors describe a change in roles between the representations and atoms in the training and testing phase respectively. I do not understand this. If I understand correctly, the final algorithm, the authors train a CNN mapping LR to HR images. The network is used in the same way at training and testing.

12) It would be useful to provide more details about the training of the network. Please describe the training set used by Kim et al. Are the two networks trained independently? One could think of fine-tuning them jointly (including the aggregation).

13) The authors show the advantage of separating networks on a single image, Barbara. It would be good to quantify this better (maybe in terms of PSNR?). This observation might be true only because the training loss, say than the works cited above. Please comment on this.

14) In figures 3 and 4, the learned filters are those on the top (above the yellow arrow). It is not obvious to me that the reflect the predominant structure in the data. (maybe due to the low resolution).

15) This work is related to (though clearly different)  that of LISTA (Learned ISTA) type of networks, proposed in:

Gregor, K., & LeCun, Y. (2010). Learning fast approximations of sparse coding. In Proceedings of the 27th International Conference on Machine Learning (ICML) 

Which connect the network architecture with the optimization algorithm used for solving the sparse coding problem. Follow up works have used these ideas for solving inverse problems as well.
","[4, 3, 6]","[' Ok but not good enough - rejection', ' Clear rejection', ' Marginally above acceptance threshold']","[4, 5, 2]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']","The review is critical of the paper, pointing out several flaws in clarity, methodology, and comparison with existing literature. The reviewer finds the paper's core arguments difficult to follow and suggests significant revisions are needed. While the reviewer is willing to reconsider their evaluation if the authors address their concerns, the overall tone suggests a negative sentiment. The language used is professional and polite, focusing on constructive criticism and suggestions for improvement rather than direct attacks on the authors or their work.",-50.0,70.0
Domain Adaptation for Deep Reinforcement Learning in Visually Distinct Games,"['Dino S. Ratcliffe', 'Luca Citi', 'Sam Devlin', 'Udo Kruschwitz']",Reject,2018,"[2, 13, 10, 23]","[3, 17, 15, 28]","[2, 62, 91, 142]","[2, 48, 45, 107]","[0, 1, 26, 11]","[0, 13, 20, 24]","- This paper discusses an agent architecture which uses a shared representation to train multiple tasks with different sprite level visual statistics. The key idea is that the agent learns a shared representations for tasks with different visual statistics

- A lot of important references  touching on very similar ideas are missing. For e.g. ""Unsupervised Pixel-level Domain Adaptation with Generative Adversarial Networks"", ""Using Simulation and Domain Adaptation to Improve Efficiency of Deep Robotic Grasping"", ""Schema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics"". 

- This paper has a lot of orthogonal details. For instance sec 2.1 reviews the history of games and AI, which is besides the key point and does not provide any literary context. 

- Only single runs for the results are shown in plots. How statistically valid are the results?

- In the last section authors mention the intent to do future work on atari and other env. Given that this general idea has been discussed in the literature several times, it seems imperative to at least scale up the experiments before the paper is ready for publication","[4, 2, 3]","[' Ok but not good enough - rejection', ' Strong rejection', ' Clear rejection']","[5, 4, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review starts with a neutral summary of the paper's content. However, it lists several significant concerns, such as missing important references, irrelevant sections, lack of statistical validation, and limited experimental scope. The language, while direct, maintains a professional and respectful tone, avoiding harsh or derogatory terms.",-30.0,60.0
Learning Parsimonious Deep Feed-forward Networks,"['Zhourong Chen', 'Xiaopeng Li', 'Nevin L. Zhang']",Reject,2018,"[4, 20, 27]","[9, 25, 32]","[26, 123, 151]","[12, 48, 69]","[13, 16, 55]","[1, 59, 27]","The main strengths of the paper are the supporting experimental results in comparison to plain feed-forward networks (FNNs).  The proposed method is focused on discovering sparse neural networks.  The experiments show that sparsity is achieved and still the discovered sparse networks have comparable or better performance compared to dense networks.

The main weakness of the paper is lack of cohesion in contributions and difficulty in delineating the scope of their proposed approach.

Below are some suggestions for improving the paper:

Can you enumerate the paper’s contributions and specify the scope of this work?  Where is this method most applicable and where is it not applicable?

Why is the paper focused on these specific contributions?  What problem does this particular set of contributions solve that is not solvable by the baselines?  There needs to be a cohesive story that puts the elements together.  For example, you explain how the algorithm for creating the backbone can use unsupervised data.  On the other hand, to distinguish this work from the baselines you mention that this work is the first to apply the method to supervised learning problems.

The motivation section in the beginning of the paper motivates using the backbone structure to get a sparse network.  However, it does not adequately motivate the skip-path connections or applications of the method to supervised tasks.

Is this work extending the applicability of baselines to new types of problems?  Or is this work focused on improving the performance of existing methods?  Answers to these questions can automatically determine suitable experiments to run as well.  It's not clear if Pruned FNNs are the most suitable baseline for evaluating the results.  Can your work be compared experimentally with any of the constructive methods from the related work section?  If not, why?

When contrasting this work with existing approaches, can you explain how existing work builds toward the same solution that you are focusing on?  It would be more informative to explain how the baselines contribute to the solution instead of just citing them and highlighting their differences.

Regarding the experimental results, is there any insight on why the dense networks are falling short?  For example, if it is due to overfitting, is there a correlation between performance and size of FNNs?  Do you observe a similar performance vs FNNs in existing methods?  Whether this good performance is due to your contributions or due to effectiveness of the baseline algorithm, proper analysis and discussion is required and counts as useful research contribution.
","[5, 5, 4]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Ok but not good enough - rejection']","[2, 5, 2]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']","The review starts with positive sentiment, highlighting the strengths of the experimental results and the achievement of sparsity with comparable or better performance. However, it quickly points out a major weakness regarding the paper's clarity and cohesion in contributions and scope. The reviewer then provides constructive criticism and detailed suggestions for improvement, indicating a willingness to help the authors enhance their work. The language throughout is professional and polite, focusing on the paper's aspects rather than directly criticizing the authors.",20.0,80.0
Latent forward model for Real-time Strategy game planning with incomplete information,"['Yuandong Tian', 'Qucheng Gong']",Reject,2018,"[13, 2]","[18, 4]","[150, 13]","[69, 5]","[73, 7]","[8, 1]","Summary:

This paper studies learning forward models on latent representations of the environment, and use these for model-based planning (e.g. via MCTS) in partial-information real-time-strategy games. The testbed used is MiniRTS, a simulation environemnt for 1v1 RTS.

Forecasting the future suffers from buildup / propagation of prediction errors, hence the paper uses multi-step errors to stabilize learning.

The paper:
1. describes how to train strong agents that might have learned an informative latent representation of the observed state-space.
2. Evaluates how informative the latent states are via state reconstruction.
3. trains variatns of a forward model f on the hidden states of the various learned agents.
4. evaluates different f within MCTS for MiniRTS.

Pro:
- This is a neat idea and addresses the important question of how to learn accurate models of the environment from data, and how to integrate them with model-free methods.
- The experimental setting is very non-trivial and novel.

Con:
- The manuscript is unclear in many parts -- this should be greatly improved.
1. The different forward models are not explained well (what is MatchPi, MatchA, PredN?). Which forward model is trained from which model-free agent?
2. How is the forward model / value function used in MCTS? I assume it's similar to what AlphaGo does, but right now it's not clear at all how everything is put together.

- The paper devotes a lot of space (sect 4.1) on details of learning and behavior of the model-free agents X. Yet it is unclear how this informs us about the quality of the learned forward models f. It would be more informative to focus in the main text on the aspects that inform us about f, and put the training details in an appendix.

- As there are many details on how the model-free agents are trained and the system has many moving parts, it is not clear what is important and what is not wrt to the eventual winrate comparisons of the MCTS models. Right now, it is not clear to me why MatchA / PredN differ so much in Fig 8.

- The conclusion seems quite negative: the model-based methods fare *much* worse than the model-free agent. Is this because of the MCTS approach? Because f is not good? Because the latent h is not informative enough? This requires a much more thorough evaluation. 

Overall:
I think this is an interesting direction of research, but the current manuscript does provide a complete and clear analysis.

Detailed:
- What are the right prediction tasks that ensure the latent space captures enough of the forward model?
- What is the error of the raw h-predictions? Only the state-reconstruction error is shown now.
- Figure 6 / sect 4.2: which model-free agent is used? Also fig 6 misses captions.
- Figure 8: scrambled caption.
- Does scheduled sampling / Dagger (Ross et al.) improve the long-term stability in this case?
","[4, 5, 4]","[' Ok but not good enough - rejection', ' Marginally below acceptance threshold', ' Ok but not good enough - rejection']","[5, 4, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer provides both positive and negative points, but the overall tone leans towards the negative side due to the cons outweighing the pros. The reviewer finds the research direction interesting but criticizes the manuscript for lack of clarity and a thorough analysis. The cons highlight significant issues in the manuscript, making the review more negative than neutral. The language used is polite and professional, focusing on constructive criticism and suggestions for improvement rather than harsh language.",-25.0,75.0
Learning Weighted Representations for Generalization Across Designs,"['Fredrik D. Johansson', 'Nathan Kallus', 'Uri Shalit', 'David Sontag']",Reject,2018,"[7, 5, 10, 14]","[12, 10, 15, 19]","[51, 126, 60, 171]","[22, 44, 27, 88]","[24, 69, 26, 75]","[5, 13, 7, 8]","This paper proposes a deep learning architecture for joint learning of feature representation, a target-task mapping function, and a sample re-weighting function. Specifically, the method tries to discover feature representations, which are invariance in different domains, by minimizing the re-weighted empirical risk and distributional shift between designs.
Overall, the paper is well written and organized with good description on the related work, research background, and theoretic proofs. 

The main contribution can be the idea of learning a sample re-weighting function, which is highly important in domain shift. However, as stated in the paper, since the causal effect of an intervention T on Y conditioned on X is one of main interests, it is expected to add the related analysis in the experiment section.","[8, 5, 7]","[' Top 50% of accepted papers, clear accept', ' Marginally below acceptance threshold', ' Good paper, accept']","[3, 3, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with positive statements, praising the paper's writing, organization, and theoretical foundation. The reviewer acknowledges the main contribution as potentially significant. While the reviewer suggests an addition to the experimental analysis, this is presented as a recommendation rather than a harsh criticism. Overall, the tone is constructive and encouraging.",75.0,80.0
HybridNet: A Hybrid Neural Architecture to Speed-up Autoregressive  Models,"['Yanqi Zhou', 'Wei Ping', 'Sercan Arik', 'Kainan Peng', 'Greg Diamos']",Reject,2018,"[5, 9, 8, 2, 11]","[10, 14, 13, 4, 15]","[58, 56, 76, 15, 49]","[24, 26, 26, 7, 26]","[24, 30, 41, 8, 20]","[10, 0, 9, 0, 3]","By generating multiple samples at once with the LSTM, the model is introducing some independence assumptions between samples that are from neighbouring windows and are not conditionally independent given the context produced by Wavenet. This reduces significantly the generality of the proposed technique.

Pros:
- Attempting to solve the important problem of speeding up autoregressive generation.
- Clarity of the write-up is OK, although it could use some polishing in some parts.
- The work is in the right direction, but the paucity of results and lack of thoroughness reduces somewhat the work's overall significance.

Cons:
- The proposed technique is not particularly novel and it is not clear whether the technique can be used to get speed-ups beyond 2x - something that is important for real-world deployment of Wavenet.
- The amount of innovation is on the low side, as it involves mostly just fairly minor architectural changes.
- The absolute results are not that great (MOS ~3.8 is not close to the SOTA of 4.4 - 4.5)


","[4, 6, 4]","[' Ok but not good enough - rejection', ' Marginally above acceptance threshold', ' Ok but not good enough - rejection']","[5, 5, 5]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review starts with a critical point, highlighting a significant limitation of the proposed technique. While it acknowledges some positive aspects like clarity and direction, it focuses more on the drawbacks such as lack of novelty, limited speed-ups, and moderate results. The language, while direct, maintains a professional and constructive tone. Therefore, the sentiment leans towards the negative side, but not extremely so, due to the acknowledgment of the work's merits.",-30.0,60.0
Key Protected Classification for GAN Attack Resilient Collaborative Learning,"['Mert Bülent Sarıyıldız', 'Ramazan Gökberk Cinbiş', 'Erman Ayday']",Reject,2018,"[0, 13, 13]","[4, 18, 18]","[11, 61, 113]","[4, 28, 52]","[6, 22, 33]","[1, 11, 28]","Collaborative learning has been proposed as a way to learn over federated data while preserving privacy. However collaborative learning has been shown to be suscepti
ble to active attacks in which one of the participants uses a GAN to reveal information about another participant.

This paper proposes a collaborative learning framework (CLF) that mitigates the GAN attack. The framework involves using the neural net to learn a mapping of the inp
ut to a high-dimensional vector and computing the inner product of this vector to a random class-specific key (the final class prediction is the argmax of this inner product). The class-specific key can be chosen randomly by each participant. By choosing sufficiently long random keys, the probability of an attacker guessing the key can be reduced. Experiments on two datasets show that this scheme successfully avoids the GAN attack.
 
1. Some of the details of key sharing are not clear and would appear to be important for the scheme to work. For example, if participants have instances associated with the same class, then they would need to share the key. This would require a central key distribution scheme which would then allow the attacker to also get access to the key.

2. I would have  liked to see how the method works with an increasing fraction of adversarial participants (I could only see experiments with one adversary). Similarly, I would have liked to see experiments with and without the fixed dense layer to see its contribution to effective learning. ","[5, 4, 3]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Clear rejection']","[2, 4, 4]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review acknowledges the value of the paper's proposed solution to a known problem ('this scheme successfully avoids the GAN attack'). However, it also raises valid concerns and questions about the implementation details and experimental setup. This suggests the reviewer sees potential in the work but requires further clarification and experimentation to be fully convinced.",50.0,70.0
Statestream: A toolbox to explore layerwise-parallel deep neural networks,['Volker Fischer'],Reject,2018,['skipped'],['skipped'],['skipped'],['skipped'],['skipped'],['skipped'],"In this paper, the authors present an open-source toolbox to explore layerwise-parallel deep neural networks. They offer an interesting and detailed comparison of the temporal progression of layerwise-parallel and layerwise-sequential networks, and differences that can emerge in the results of these two computation strategies.

While the open-source toolbox introduced in this paper can be an excellent resource for the community interested in exploring these networks, the present submission offers relatively few results actually using these networks in practice. In order to make a more compelling case for these networks, the present submission could include more detailed investigations, perhaps demonstrating that they learn differently or better than other implementations on standard training sets.","[5, 3, 5]","[' Marginally below acceptance threshold', ' Clear rejection', ' Marginally below acceptance threshold']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with positive remarks, highlighting the value of the toolbox and the interesting comparison presented. However, it then transitions to suggest improvements, indicating that the paper in its current form might not be compelling enough due to the lack of practical results. The language used throughout is constructive and professional.",50.0,75.0
Automatic Parameter Tying in Neural Networks,"['Yibo Yang', 'Nicholas Ruozzi', 'Vibhav Gogate']",Reject,2018,"[10, 12, 15]","[15, 17, 20]","[97, 49, 94]","[37, 31, 68]","[48, 13, 18]","[12, 5, 8]","Approach is interesting however my main reservation is with the data set used for experiments and making general (!) conclusions. MNIST, CIFAR-10 are too simple tasks perhaps suitable for debugging but not for a comprehensive validation of quantization/compression techniques. Looking at the results, I see a horrific degradation of 25-43% relative to DC baseline despite being told about only a minimal loss in accuracy. A number of general statements is made based on MNIST data, such as on page 3 when comparing GMM and k-means priors, on page 7 and 8 when claiming that parameter tying and sparsity do not act strongly to improve generalization. In addition, by making a list of all hyper parameters you tuned I am not confident that your claim that this approach requires less tuning. 

Additional comments:

(a) you did not mention student-teacher training
(b) reference to previously not introduced K-means prior at the end of section 1
(c) what is that special version of 1-D K-means?
(d) Beginning of section 4.1 is hard to follow as you are referring to some experiments not shown in the paper.
(e) Where is 8th cluster hiding in Figure 1b?
(f) Any comparison to a classic compression technique would be beneficial.
(g) You are referring to a sparsity at the end of page 8 without formally defining it. 
(h) Can you label each subfigure in Figure 3 so I do not need to refer to the caption? Can you discuss this diagram in the main text, otherwise what is the point of dumping it in the appendix?
(i) I do not understand Figure 4 without explanation. ","[6, 6, 6]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer expresses interest in the approach but raises significant concerns about the validity of the results due to the choice of datasets. They find the performance degradation compared to the baseline unacceptable and question several claims made in the paper. The numerous specific comments further highlight issues with clarity, missing information, and potentially unsupported assertions. The tone is critical and points out flaws directly, but it maintains a professional and objective demeanor. ",-20.0,60.0
Adversarial reading networks for machine comprehension,"['Quentin Grail', 'Julien Perez']",Reject,2018,"[1, 11]","[4, 14]","[2, 36]","[1, 20]","[1, 9]","[0, 7]","Summary:

This paper proposes an adversarial learning framework for machine comprehension task. Specifically, authors consider a reader network which learns to answer the question by reading the passage and a narrator network which learns to obfuscate the passage so that the reader can fail in its task. Authors report results in 3 different reading comprehension datasets and the proposed learning framework results in improving the performance of GMemN2N.


My Comments:

This paper is a direct application of adversarial learning to the task of reading comprehension. It is a reasonable idea and authors indeed show that it works.

1. The paper needs a lot of editing. Please check the minor comments.

2. Why is the adversary called narrator network? It is bit confusing because the job of that network is to obfuscate the passage.

3. Why do you motivate the learning method using self-play? This is just using the idea of adversarial learning (like GAN) and it is not related to self-play.

4. In section 2, first paragraph, authors mention that the narrator prevents catastrophic forgetting. How is this happening? Can you elaborate more?

5. The learning framework is not explained in a precise way. What do you mean by re-initializing and retraining the narrator? Isn’t it costly to reinitialize the network and retrain it for every turn? How many such epochs are done? You say that test set also contains obfuscated documents. Is it only for the validation set? Can you please explain if you use obfuscation when you report the final test performance too? It would be more clear if you can provide a complete pseudo-code of the learning procedure.

6. How does the narrator choose which word to obfuscate? Do you run the narrator model with all possible obfuscations and pick the best choice?

7. Why don’t you treat number of hops as a hyper-parameter and choose it based on validation set? I would like to see the results in Table 1 where you choose number of hops for each of the three models based on validation set.

8. In figure 2, how are rounds constructed? Does the model sees the same document again and again for 100 times or each time it sees a random document and you sample documents with replacement? This will be clear if you provide the pseudo-code for learning.

9. I do not understand author's’ justification for figure-3. Is it the case that the model learns to attend to last sentences for all the questions? Or where it attends varies across examples?

10. Are you willing to release the code for reproducing the results?

Minor comments:

Page 1, “exploit his own decision” should be “exploit its own decision”
In page 2, section 2.1, sentence starting with “Indeed, a too low percentage …” needs to be fixed.
Page 3, “forgetting is compensate” should be “forgetting is compensated”.
Page 4, “for one sentences” needs to be fixed.
Page 4, “unknow” should be “unknown”.
Page 4, “??” needs to be fixed.
Page 5, “for the two first datasets” needs to be fixed.
Table 1, “GMenN2N” should be “GMemN2N”. In caption, is it mean accuracy or maximum accuracy?
Page 6, “dataset was achieves” needs to be fixed.
Page 7, “document by obfuscated this word” needs to be fixed.
Page 7, “overall aspect of the two first readers” needs to be fixed.
Page 8, last para, references needs to be fixed.
Page 9, first sentence, please check grammar.
Section 6.2, last sentence is irrelevant.
","[5, 4, 5]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[5, 5, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with a positive sentiment, acknowledging the paper's idea as reasonable and effective. However, the numerous questions and the amount of required editing suggest a less positive sentiment. Overall, the tone is critical but professional and polite, focusing on improvements. Therefore, the sentiment leans slightly towards the positive side due to the initial acknowledgment of the paper's merit, and the politeness remains high due to the constructive and professional tone.",60.0,75.0
DNN Model Compression Under Accuracy Constraints,"['Soroosh Khoram', 'Jing Li']",Reject,2018,"[2, 12]","[7, 17]","[10, 50]","[6, 34]","[2, 4]","[2, 12]","1. Summary

This paper introduced a method to learn a compressed version of a neural network such that the loss of the compressed network doesn't dramatically change.


2. High level paper

- I believe the writing is a bit sloppy. For instance equation 3 takes the minimum over all m in C but C is defined to be a set of c_1, ..., c_k, and other examples (see section 4 below). This is unfortunate because I believe this method, which takes as input a large complex network and compresses it so the loss in accuracy is small, would be really appealing to companies who are resource constrained but want to use neural network models.


3. High level technical

- I'm confused at the first and second lines of equation (19). In the first line, shouldn't the first term not contain \Delta W ? In the second line, shouldn't the first term be \tilde{\mathcal{L}}(W_0 + \Delta W) ?
- For CIFAR-10 and SVHN you're using Binarized Neural Networks and the two nice things about this method are (a) that the memory usage of the network is very small, and (b) network operations can be specialized to be fast on binary data. My worry is if you're compressing these networks with your method are the weights not treated as binary anymore? Now I know in Binarized Neural Networks they keep a copy of real-valued weights so if you're just compressing these then maybe all is alright. But if you're compressing the weights _after_ binarization then this would be very inefficient because the weights won't likely be binary anymore and (a) and (b) above no longer apply.
- Your compression ratio is much higher for MNIST but your accuracy loss is somewhat dramatic, especially for MNIST (an increase of 0.53 in error nearly doubles your error and makes the network worse than many other competing methods: http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#4d4e495354). What is your compression ratio for 0 accuracy loss? I think this is a key experiment that should be run as this result would be much easier to compare with the other methods.
- Previous compression work uses a lot of tricks to compress convolutional weights. Does your method work for convolutional layers?
- The first paper to propose weight sharing was not Han et al., 2015, it was actually:
Chen W., Wilson, J. T., Tyree, S., Weinberger K. Q., Chen, Y. ""Compressing Neural Networks with the Hashing Trick"" ICML 2015
Although they did not learn the weight sharing function, but use random hash functions.


4. Low level technical

- The end of Section 2 has an extra 'p' character
- Section 3.1: ""Here, X and y define a set of samples and ideal output distributions we use for training"" this sentence is a bit confusing. Here y isn't a distribution, but also samples drawn from some distribution. Actually I don't think it makes sense to talk about distributions at all in Section 3.
- Section 3.1: ""W is the learnt model...\hat{W} is the final, trained model"" This is unclear: W and \hat{W} seem to describe the same thing. I would just remove ""is the learnt model and""


5. Review summary

While the trust-region-like optimization of the method is nice and I believe this method could be useful for practitioners, I found the paper somewhat confusing to read. This combined with some key experimental questions I have make me think this paper still needs work before being accepted to ICLR.","[4, 3, 3]","[' Ok but not good enough - rejection', ' Clear rejection', ' Clear rejection']","[3, 3, 5]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The reviewer expresses a somewhat positive sentiment overall. They find the core idea of the method interesting and potentially useful for practitioners. However, they also point out several areas of improvement, particularly regarding the clarity of writing, potential technical issues, and missing experiments. The language used is polite and professional, providing constructive criticism throughout the review. Considering these points, the sentiment score leans slightly towards positive, while the politeness score is highly positive.",40.0,80.0
Autonomous Vehicle Fleet Coordination With Deep Reinforcement Learning,['Cane Punma'],Reject,2018,['no_match'],['no_match'],['no match'],['no match'],['no match'],['no match'],"

This paper proposes to use deep reinforcement learning to solve a multiagent coordination task. In particular, the paper introduces a benchmark domain to model fleet coordination problems as might be encountered in taxi companies. 

The paper does not really introduce new methods, and as such, this paper should be seen more as an application paper. I think that such a paper could have merits if it would really push the boundary of the feasible, but I do not think that is really the case with this paper: the task still seems quite simplistic, and the empirical evaluation is not convincing (limited analysis, weak baselines). As such, I do not really see any real grounds for acceptance.

Finally, there are also many other weaknesses. The paper is quite poorly written in places, has poor formatting (citations are incorrect and half a bibtex entry is inlined), and is highly inadequate in its treatment of related work. For instance, there are many related papers on:

-taxi fleet management (e.g., work by Pradeep Varakantham)
 
-coordination in multi-robot systems for spatially distributed tasks (e.g., Gerkey and much work since)

-scaling up multiagent reinforcement learning and multiagent MDPs (Guestrin et al 2002, Kok & Vlassis 2006, etc.)

-dealing with partial observability (work on decentralized POMDPs by Peshkin et al, 2000, Bernstein, Amato, etc.)

-multiagent deep RL has been very active last 1-2 years. E.g., see other papers by Foerster, Sukhbataar, Omidshafiei


Overall, I see this as a paper which with improvements could make a nice workshop contribution, but not as a paper to be published at a top-tier venue.

","[3, 4, 3]","[' Clear rejection', ' Ok but not good enough - rejection', ' Clear rejection']","[5, 4, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer finds little innovative value in the paper, deeming it more of an application paper that doesn't significantly push boundaries. The reviewer criticizes the simplistic task, unconvincing evaluation, poor writing, formatting errors, and inadequate handling of related work. While harsh in places, the language avoids personal attacks, suggesting areas for improvement and a more suitable venue (workshop) reflecting a less negative score.",-50.0,20.0
Deep Learning Inferences with Hybrid Homomorphic Encryption,"['Anthony Meehan', 'Ryan K L Ko', 'Geoff Holmes']",Reject,2018,"['no_match', 10, 'no_match']","['no_match', 15, 'no_match']","['no match', 85, 'no match']","['no match', 55, 'no match']","['no match', 11, 'no match']","['no match', 19, 'no match']","This paper proposes a hybrid Homomorphic encryption system that is well suited for privacy-sensitive data inference applications with the deep learning paradigm. 
The paper presents a well laid research methodology that shows a good decomposition of the problem at hand and the approach foreseen to solve it. It is well reflected in the paper and most importantly the rationale for the implementation decisions taken is always clear.

The results obtained (as compared to FHEW) seem to indicate well thought off decisions taken to optimize the different gates' operations as clearly explained in the paper. For example, reducing bootstrapping operations by two-complementing both the plaintext and the ciphertext, whenever the number of 1s in the plain bit-string is greater than the number of 0s (3.4/Page 6).

Result interpretation is coherent with the approach and data used and shows a good understanding of the implications of the implementation  decisions made in the system and the data sets used.
Overall, fine work, well organized, decomposed, and its rationale clearly explained. The good results obtained support the design decisions made.
Our main concern is regarding thorough comparison to similar work and provision of comparative work assessment to support novelty claims.

Nota: 
     - In Figure 4/Page 4: AND Table A(1)/B(0), shouldn't  A And B be 0?
     - Unlike Figure 3/Page 3, in Figure 2/page 2, shouldn't  operations' precedence prevail (No brackets), therefore 1+2*2=5?","[4, 4, 4]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[4, 5, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review starts with very positive statements about the paper's methodology, clarity, and results. The reviewer acknowledges the good work and clear explanations. While they raise a concern about the comparison to similar works, this is presented as an area for improvement rather than a severe flaw. The tone throughout is constructive and respectful.",75.0,85.0
Neural Clustering By Predicting And Copying Noise,"['Sam Coope', 'Andrej Zukov-Gregoric', 'Yoram Bachrach']",Reject,2018,"[2, 3, 14]","[5, 5, 19]","[14, 8, 150]","[8, 6, 98]","[6, 2, 31]","[0, 0, 21]","This paper presents an algorithm for clustering using DNNs. The algorithm essentially alternates over two steps: a step that trains the DNN to predict random targets, and another step that reassigns the targets based on the overall matching with the DNN outputs. The second step also shrinks the number of targets over time to achieve clustering. Intuitively, the randomness in target may achieve certain regularization effect.

My concerns:
1. There is no analysis on what the regularization effect is. What advantage does the proposed algorithm offer to an user that a more deterministic algorithm cannot?
2. The delete-and-copy step also introduces randomness, and since the algorithm removes targets over time, it is not clear if the algorithm consistently optimizes one objective throughout. Without a consistent objective function, the algorithm seems somewhat heuristic.
3. Due to the randomness from multiple operations, the experiments need to be run multiple times, and see if the output clustering is sensitive to it. If it turns out the algorithm is quite robust to the randomness, it is then an interesting question why this is the case.
4. Does the  Hungarian algorithm used for matching scales to much larger datasets?
5. While the algorithm empirically improve over k-means, I believe at this point combinations of DNN with classical clustering algorithms already exist and comparisons with such stronger baselines are missing. The authors have listed a few related algorithms in the last paragraph on page 1. I think the following one is also relevant:
-- Law et al. Deep spectral clustering learning. ICML 2015.

","[5, 5, 5]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer raises several valid concerns about the paper, questioning the algorithm's novelty, clarity in objective function, robustness to randomness, scalability, and lack of strong baselines. While the reviewer acknowledges the empirical improvement over k-means, the overall tone suggests a need for significant improvements and clarifications. The concerns are presented in a constructive and neutral manner, focusing on specific areas for improvement.",20.0,60.0
Generative Discovery of Relational Medical Entity Pairs,"['Chenwei Zhang', 'Yaliang Li', 'Nan Du', 'Wei Fan', 'Philip S. Yu']",Reject,2018,"[6, 8, 13, 20, 39]","[11, 13, 18, 24, 44]","[104, 178, 176, 236, 1879]","[45, 100, 108, 161, 991]","[38, 55, 38, 33, 389]","[21, 23, 30, 42, 499]","In the medical context, this paper describes the classic problem of ""knowledge base completion"" from structured data only (no text).  The authors argue for the advantages of a generative VAE approach (but without being convincing).  They do not cite the extensive literature on KB completion.  They present experimental results on their own data set, evaluating only against simpler baselines of their own VAE approach, not the pre-existing KB methods.

The authors seem unaware of a large literature on ""knowledge base completion.""  E.g. [Bordes, Weston, Collobert, Bengio, AAAI, 2011],  [Socher et al 2013 NIPS], [Wang, Wang, Guo 2015 IJCAI], [Gardner, Mitchell 2015 EMNLP], [Lin, Liu, Sun, Liu, Zhu AAAI 2015], [Neelakantan, Roth, McCallum 2015], 

The paper claims that operating on pre-structured data only (without using text) is an advantage.  I don't find the argument convincing.  There are many methods that can operate on pre-structured data only, but also have the ability to incorporate text data when available, e.g. ""universal schema"" [Riedel et al, 2014].

The paper claims that ""discriminative approaches"" need to iterate over all possible entity pairs to make predictions.  In their generative approach they say they find outputs by ""nearest neighbor search.""  But the same efficient search is possible in many of the classic ""discriminatively-trained"" KB completion models also.

It is admirable that the authors use an interesting (and to my knowledge novel) data set.  But the method should also be evaluated on multiple now-standard data sets, such as FB15K-237 or NELL-995.  The method is evaluated only against their own VAE-based alternatives.  It should be evaluated against multiple other standard KB completion methods from the literature, such as Jason Weston's Trans-E, Richard Socher's Tensor Neural Nets, and Neelakantan's RNNs.
","[2, 4, 4]","[' Strong rejection', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[5, 3, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is highly critical of the paper. The reviewer points out several major flaws, including a lack of familiarity with existing literature, unconvincing arguments, and limited evaluation. The reviewer does acknowledge the novelty of the dataset but overall finds the paper lacking.",-70.0,20.0
Data Augmentation by Pairing Samples for Images Classification,['Hiroshi Inoue'],Reject,2018,[22],[26],[95],[40],[4],[51],"The paper investigates a method of data augmentation for image classification, where two images from the training set are averaged together as input, but the label from only one image is used as a target.  Since this scheme is asymmetric and uses quite unrealistic input images, a training scheme is used where the technique is only enabled in the middle of training (not very beginning or end), and in an alternating on-off fashion.  This improves classification performance nicely on a variety of datasets.

This is a simple technique, and the paper is concise and to the point.  However, I would have liked to see a few additional comparisons.

First, this augmentation technique seems to have two components:  One is the mixing of inputs, but another is the effective dropping of labels from one of the two images in the pair.  Which of these are more important, and can they be separated?  What if some of the images' labels are changed at random, for half the images in a minibatch, for example?  This would have the effect of random label changes, but without the input mixing.  Likewise, what if both labels in the pair are used as targets (with 0.5 assigned to each in the softmax target)?  This would mix the images, but keep targets intact.

Second, the bottom of p.3 says that multiple training procedures were evaluated, but I'd be interested to see the results of some of these.  In particular, is it important to alternate enabling and disabling SamplePairing, or does it also work to mix samples with and without it in each minibatch (e.g. 3/4 of the minibatch with pairing augmentation, and 1/4 without it)?

I liked the experiment mixing images from within a restricted training set composed of a subset of the CIFAR images, compared to mixing these images with CIFAR training set images outside the restricted sample (p.5 and Fig 5).  This suggests to me, however, that it's possible the label manipulations may play an important role.  Or, is an explanation why this performs not as well that the network will train these mixing images to random targets (that of the training image in the pair), and never see this example again, whereas by using the training set alone, the mixing image is likely to be repeated with its correct label?  Some more discussion on this would be nice.

Overall, I think this is an interesting technique that appears to achieve nice results.  It could be investigated deeper at some key points.
","[5, 6, 4]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Ok but not good enough - rejection']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The reviewer provides constructive criticism and suggestions for improvement, indicating a positive view of the paper's core idea. They find the technique interesting and the paper well-written. However, the reviewer also suggests additional experiments and analysis, suggesting there's room for improvement.",60.0,90.0
Learning what to learn in a neural program,"['Richard Shin', 'Dawn Song']",Reject,2018,"[9, 20]","[13, 25]","[38, 440]","[21, 254]","[16, 154]","[1, 32]","In this paper, the authors consider the problem of generating a training data set for the neural programmer-interpreter from an executable oracle. In particular, they aim at generating a complete set that fully specifies the behavior of the oracle. The authors propose a technique that achieves this aim by borrowing ideas from programming language and abstract interpretation. The technique systematically interacts with the oracle using observations, which are abstractions of environment states, and it is guaranteed to produce a data set that completely specifies the oracle. The authors later describes how to improve this technique by further equating certain observations and exploring only one in each equivalence class. Their experiments show that this improve technique can produce complete training sets for three programs.

It is nice to see the application of ideas from different areas for learning-related questions. However, there is one thing that bothers me again and again. Why do we need a data-generation technique in the paper at all? Typically, we are given a set of data, not an oracle that can generate such data, and our task is to learn something from the data. If we have an executable oracle, it is now clear to me why we want to replicate this oracle by an instance of the neural programmer-interpreter. One thing that I can see is that the technique in the paper can be used when we do research on the neural programmer-interpreter. During research, we have multiple executable oracles and need to produce good training data from them. The authors' technique may let us do this data-generation easily. But this benefit to the researchers does not seem to be strong enough for the acceptance at ICLR'18.

 ","[4, 5, 5]","[' Ok but not good enough - rejection', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[4, 4, 2]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']","The review starts with a positive sentiment, highlighting the novelty of the approach (""It is nice to see...""). However, the main body raises a significant concern about the paper's premise, questioning the need for the proposed data generation technique. This concern, phrased directly but professionally, significantly impacts the overall sentiment, pushing it towards the negative side. While acknowledging a potential use case for researchers, the reviewer ultimately deems it insufficient for acceptance. The language remains polite throughout, employing a questioning tone rather than accusatory.",-30.0,60.0
Learning Independent Features with Adversarial Nets for Non-linear ICA,"['Philemon Brakel', 'Yoshua Bengio']",Reject,2018,"[8, 31]","[12, 36]","[32, 975]","[15, 405]","[14, 454]","[3, 116]","The focus of the paper is independent component analysis (ICA) and its nonlinear variants such as the post non-linear (PNL) ICA model. Motivated by the fact that estimating mutual information and similar dependency measures require density estimates and hard to optimize, the authors propose a Wasserstein GAN (generative adversarial network) based solution to tackle the problem, with illustrations on 6 (synthetic) and 3-dimemensional (audio) examples. The primary idea of the paper is to use the Wasserstein distance as an independence measure of the estimated source coordinates, and optimize it in a neural network (NN) framework.

Although finding novel GAN applications is an exciting topic, I am not really convinced that ICA with the proposed Wasserstein GAN based technique fulfills this goal.
 
Below I detail my reasons:

1)The ICA problem can be formulated as the minimization of pairwise mutual information [1] or one-dimensional entropy [2]. In other words, estimating the joint dependence of the source coordinates is not necessary; it is worthwhile to avoid it.

2)The PNL ICA task can be efficiently tackled by first 'removing' the nonlinearity followed by classical linear ICA; see for example [3].

3)Estimating information theoretic (IT) measures (mutual information, divergence) is a quite mature field with off-the-self techniques, see for example [4,5,6,8]. These methods do not estimate the underlying densities; it would be superfluous (and hard).

4)Optimizing non-differentiable IT measures can computationally quite efficiently carried out in the ICA context by e.g., Givens rotations [7]; differentiable ICA cost functions can be robustly handled by Stiefel manifold methods; see for example [8,9].

5)Section 3.1: This section is devoted to generating samples from the product of the marginals, even using separate generator networks. I do not see the necessity of these solutions; the subtask can be solved by independently shuffling all the coordinates of the sample.

6)Experiments (Section 6): 
i) It seems to me that the proposed NN-based technique has some quite serious divergence issues: 'After discarding diverged models, ...' or 'Unfortunately, the model selection procedure also didn't identify good settings for the Anica-g model...'.
ii) The proposed method gives pretty comparable results to the chosen baselines (fastICA, PNLMISEP) on the selected small-dimensional tasks. In fact, [7,8,9] are likely to provide more accurate (fastICA is a simple kurtosis based method, which is 
a somewhat crude 'estimate' of entropy) and faster estimates; see also 2).

References:
[1] Pierre Comon. Independent component analysis, a new concept? Signal Processing, 36:287-314, 1994.
[2] Aapo Hyvarinen and Erkki Oja. Independent Component Analysis: Algorithms and Applications. Neural Networks, 13(4-5):411-30, 2000. 
[3] Andreas Ziehe, Motoaki Kawanabe, Stefan Harmeling, and Klaus-Robert Muller. Blind separation of postnonlinear mixtures using linearizing transformations and temporal decorrelation. Journal of Machine Learning Research, 4:1319-1338, 2003.
[4] Barnabas Poczos, Liang Xiong, and Jeff Schneider. Nonparametric divergence: Estimation with applications to machine learning on distributions. In Conference on Uncertainty in Artificial Intelligence, pages 599-608, 2011.
[5] Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Scholkopf, Alexander Smola. A Kernel Two-Sample Test. Journal of Machine Learning Research, 13:723-773, 2012.
[6] Alan Wisler, Visar Berisha, Andreas Spanias, Alfred O. Hero. A data-driven basis for direct estimation of functionals of distributions. TR, 2017. (https://arxiv.org/abs/1702.06516) 
[7] Erik G. Learned-Miller, John W. Fisher III. ICA using spacings estimates of entropy. Journal of Machine Learning Research, 4:1271-1295, 2003.
[8] Francis R. Bach. Michael I. Jordan. Kernel Independent Component Analysis. Journal of Machine Learning Research 3: 1-48, 2002.
[9] Hao Shen, Stefanie Jegelka and Arthur Gretton. Fast Kernel-Based Independent Component Analysis, IEEE Transactions on Signal Processing, 57:3498-3511, 2009.
","[3, 5, 6]","[' Clear rejection', ' Marginally below acceptance threshold', ' Marginally above acceptance threshold']","[5, 5, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer outlines several significant concerns regarding the novelty and effectiveness of the proposed method compared to existing techniques. They find the paper's primary idea not convincing and provide detailed reasons why. The numerous references to existing literature that address the paper's shortcomings contribute to the negative sentiment.  While the reviewer maintains a professional tone, the criticism is substantial, indicating a negative sentiment.",-60.0,40.0
Image Quality Assessment Techniques Improve Training and Evaluation of Energy-Based Generative Adversarial Networks,"['Michael O. Vertolli', 'Jim Davies']",Reject,2018,"[5, 31]","[5, 35]","[7, 113]","[5, 78]","[1, 2]","[1, 33]","Summary: 
The paper extends the the recently proposed Boundary Equilibrium Generative Adversarial Networks (BEGANs), with the hope of generating images which are more realistic. In particular, the authors propose to change the energy function associated with the auto-encoder, from an L2 norm (a single number) to an energy function with multiple components. Their energy function is inspired by the structured similarity index (SSIM), and the three components they use are the L1 score, the gradient magnitude similarity score, and the chromium score. Using this energy function, the authors hypothesize, that it will force the generator to generate realistic images. They test their hypothesis on a single dataset, namely, the CelebA dataset. 

Review: 
While the idea proposed in the paper is somewhat novel and there is nothing obviously wrong about the proposed approach, I thought the paper is somewhat incremental. As a result I kind of question the impact of this result. My suspicion is reinforced by the fact that the experimental section is extremely weak. In particular the authors test their model on a single relatively straightforward dataset. Any reason why the authors did not try on other datasets involving natural images? As a result I feel that the title and the claims in the paper are somewhat misleading and premature: that the proposed techniques improves the training and evaluation of energy based gans. 

Over all the paper is clearly written and easy to understand. 

Based on its incremental nature and weak experiments, I'm on the margin with regards to its acceptance. Happy to change my opinion if other reviewers strongly think otherwise with good reason and are convinced about its impact. ","[6, 5, 5]","[' Marginally above acceptance threshold', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[3, 3, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer acknowledges the novelty of the idea but expresses concerns about the paper's incremental nature and limited experimental validation. They find the experiments weak due to the use of only one dataset and question the generalizability of the findings. The reviewer's tone is critical but professional, suggesting a willingness to reconsider their opinion based on other reviewers' feedback. Overall, the sentiment leans towards the negative side due to the reservations expressed.",-25.0,50.0
Contextual memory bandit for pro-active dialog engagement,"['julien perez', 'Tomi Silander']",Reject,2018,"[11, 29]","[14, 31]","[36, 42]","[20, 27]","[9, 6]","[7, 9]","The paper ""CONTEXTUAL MEMORY BANDIT FOR PRO-ACTIVE DIALOG ENGAGEMENT"" proposes to address the problem of pro-active dialog engagement by the mean of a bandit framework that selects dialog situations w.r.t. to the context of the system. Authors define a neural archiecture managing memory with the mean of a contextual attention mechanism.

My main concern about this paper is that the proposal is not enough well described. A very large amount of technical details are missing for allowing the reader to understand the model (and reproduce the experiments). The most important ones are about the exploration policies which are not described at all, while it is a very central point of the paper. The only discussion given w.r.t. the exploration policy is a very general overview about Thompson Sampling. But nothing is said about how it is implemented in the case of the proposed model. How is estimated p(\Theta|D) ? Ok  authors give p(\Theta|D) as a product between prior and likelihood. But it is not sufficient to get p(\Theta|D), the evidence should also been considered (for instance by using variational inference). Also, what is the prior of the parameters ? How is distributed r given a,x and \Theta ? 

Also, not enough justification is given about the general idea of the model. Authors should give more intuitions about the mechanism they propose. Figure 2 should be able to help, but no reference to this figure is given in the text, so it is very difficult to extract any information from it. Authors only (roughly) describe  the architecture without justifying their choices.

At last, the experiments really fail at demonstrating the relevance of the approach, as only questionable artificial data is used. On the first hand it appears mandatory to me to consider some (even minimal) experiments on real data for such proposal. On the other one, the simulated data used there cannot correspond to cues to validate the approach since they appear very far from real scenarios: the trajectories do not depend on what is recommended. Ok only the recommended places reveal some reward but it appears not as a sufficiently realistic scenario to me. Also, very too few baselines are considered: only different versions of the proposal and a random baseline are considered. A classical contextual bandit instance (such as LinUCB) would have been a minimum.

Other remarks:
      - the definition of q is not given
      - user is part of the context x  in the bandit section but not after where it is denoted as u.
      - the notion of time window should be more formally described
      - How is built the context is not clear in the experiments section

","[3, 3, 2]","[' Clear rejection', ' Clear rejection', ' Strong rejection']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The reviewer raises several significant concerns about the paper, including a lack of clarity in the model description, insufficient justification for design choices, and concerns about the artificial nature of the experimental data and lack of strong baselines. The language, while direct and critical, maintains a professional and appropriately academic tone.",-40.0,60.0
Policy Gradient For Multidimensional Action Spaces: Action Sampling and Entropy Bonus,"['Vuong Ho Quan', 'Yiming Zhang', 'Kenny Song', 'Xiao-Yue Gong', 'Keith W. Ross']",Reject,2018,"[1, 1, 1, 33]","[5, 1, 5, 37]","[9, 2, 6, 180]","[4, 1, 1, 102]","[5, 1, 3, 17]","[0, 0, 2, 61]","The authors present two autoregressive models for sampling action probabilities from a factorized discrete action space. On a multi-agent gridworld task and a multi-agent multi-armed bandit task, the proposed method seems to benefit from their lower-variance entropy estimator for exploration bonus. A few key citations were missing - notably the LSTM model they propose is a clear instance of an autoregressive density estimator, as in PixelCNN, WaveNet and other recently popular deep architectures. In that context, this work can be viewed as applying deep autoregressive density estimators to policy gradient methods. At least one of those papers ought to be cited. It also seems like a simple, obvious baseline is missing from their experiments - simply independently outputting D independent softmaxes from the policy network. Without that baseline it's not clear that any actual benefit is gained by modeling the joint distribution between actions, especially since the optimal policy for an MDP is provably deterministic anyway. The method could even be made to capture dependencies between different actions by adding a latent probabilistic layer in the middle of the policy network, inducing marginal dependencies between different actions. A direct comparison against one of the related methods in the discussion section would help better contextualize the paper as well. A final point on clarity of presentation - in keeping with the convention in the field, the readability of the tables could be improved by putting the top-performing models in bold, and Table 2 should almost certainly be replaced by a boxplot.","[5, 6, 5]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Marginally below acceptance threshold']","[4, 3, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review acknowledges the potential contribution of the paper (""seems to benefit..."") but also highlights significant shortcomings (""key citations missing"", ""simple, obvious baseline missing""). The reviewer suggests improvements and alternative approaches, indicating a critical but constructive stance. Overall, the sentiment leans slightly towards the negative due to the perceived flaws. The language is formal, academic, and respectful, without resorting to harsh criticism.",-20.0,70.0
TRL: Discriminative Hints for Scalable Reverse Curriculum Learning,"['Chen Wang', 'Xiangyu Chen', 'Zelin Ye', 'Jialu Wang', 'Ziruo Cai', 'Shixiang Gu', 'Cewu Lu']",Reject,2018,"['no_match', 10, 0, 3, 0, 7, 8]","['no_match', 15, 5, 8, 5, 12, 13]","['no match', 131, 11, 28, 3, 89, 252]","['no match', 65, 4, 13, 1, 38, 107]","['no match', 39, 5, 13, 2, 49, 116]","['no match', 27, 2, 2, 0, 2, 29]","This paper proposes a new method for reverse curriculum generation by gradually reseting the environment in phases and classifying states that tend to lead to success. It additionally proposes a mechanism for learning from human-provided ""key states"".

The ideas in this paper are quite nice, but the paper has significant issues with regard to clarity and applicability to real-world problems:
First, it is unclear is the proposed method requires access only high-dimensional observations (e.g. images) during training or if it additionally requires low-dimensional states (e.g. sufficient information to reset the environment). In most compelling problems settings where a low-dimensional representation that sufficiently explains the current state of the world is available during training, then it is also likely that one can write down a nicely shaped reward function using that state information during training, in which case, it makes sense to use such a reward function. This paper seems to require access to low-dimensional states, and specifically considers the sparse-reward setting, which seems contrived.
Second, the paper states that the assumption ""when resetting, the agent can be reset to any state"" can be satisfied in problems such as real-world robotic manipulation. This is not correct. If the robot could autonomously reset to any state, then we would have largely solved robotic manipulation. Further, it is not always realistic to assume access to low-dimensional state information during training on a real robotic system (e.g. knowing the poses of all of the objects in the world).
Third, the experiments section lacks crucial information needed to understand the experiments. What is the state, observation, and action space for each problem setting? What is the reward function for each problem setting? What reinforcement learning algorithm is used in combination with the curriculum and tendency rewards? Are the states and actions continuous or discrete? Without this information, it is difficult to judge the merit of the experimental setting.
Fourth, the proposed method seems to lack motivation, making the proposed scheme seem a bit ad hoc. Could each of the components be motivated further through more discussion and/or ablative studies?
Finally, the main text of the paper is substantially longer than the recommended page limit. It should be shortened by making the writing more concise.

Beyond my feedback on clarity and significance, here are further pieces of feedback with regard to the technical content, experiments, and related work:
I'm wondering -- can the reward shaping in Equation 2 be made to satisfy the property of not affecting the final policy? (see Ng et al. '09) If so, such a reward shaping would make the method even more appealing.
How do the experiments in section 5.4 compare to prior methods and ablations? Without such a comparison, it is impossible to judge the performance of the proposed method and the level of difficulty of these tasks. At the very least, the paper should compare the performance of the proposed method to the performance a random policy.

The paper is missing some highly relevant references. First, how does the proposed method compare to hindsight experience replay? [1] Second, learning from keyframes (rather than demonstrations) has been explored in the past [1]. It would be preferable to use the standard terminology of ""keyframe"".

[1] Andrychowicz et al. Hindsight Experience Replay. 2017
[2] Akgun et al. Keyframe-based Learning from Demonstration. 2012

In summary, I think this paper has a number of promising ideas and experimental results, but given the significant issues in clarity and significance to real world problems, I don't think that the current version of this paper is suitable for publication in ICLR.

More minor feedback on clarity and correctness:
- Abstract: ""Deep RL algorithms have proven successful in a vast variety of domains"" -- This is an overstatement.
- The introduction should be more clear with regard to the assumptions. In particular, it would be helpful to see discussion of requiring human-provided keyframes. As is, it is unclear what is meant by ""checkpoint scheme"", which is not commonly used terminology.
- ""This kind of spare reward, goal-oriented tasks are considered the most difficult challenges"" -- This is also an overstatement. Long-horizon tasks and high-dimensional observations are also very difficult. Also, the sentence is not grammatically correct.
- ""That is, environment"" -> ""That is, the environment""
- In the last paragraph of the intro, it would be helpful to more clearly state what the experiments can accomplish. Can they handle raw pixel inputs?
- ""diverse domains"" -> ""diverse simulated domains""
- ""a robotic grasping task"" -> ""a simulated robotic grasping task""
- There are a number of issues and errors in citations, e.g. missing the year, including the first name, incorrect reference
- Assumption 1: \mathcal{P} has not yet been defined.
- The last two paragraphs of section 3.2 are very difficult to understand without reading the method yet
- ""conventional RL solver tend"" -> ""conventional RL tend"", also should mention sparse reward in this sentence.
- Algorithm 1 and Figure 1 are not referenced in the text anywhere, and should be
- The text in Figure 1 and Figure 3 is extremely small
- The text in Figure 3 is extremely small


","[4, 4, 5]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review acknowledges the proposed method's merits but heavily criticizes its clarity, applicability, and experimental validation. The reviewer uses phrases like ""significant issues,"" ""unclear,"" ""not correct,"" ""lacks crucial information,"" ""seems to lack motivation,"" and ""a bit ad hoc,"" indicating a rather negative sentiment. While the reviewer provides constructive feedback and suggestions for improvement, the overall tone leans towards the critical side. However, the reviewer does not resort to personal attacks or disrespectful language, maintaining a professional and polite tone.",-30.0,60.0
NOVEL AND EFFECTIVE PARALLEL MIX-GENERATOR GENERATIVE ADVERSARIAL NETWORKS,"['Xia Xiao', 'Sanguthevar Rajasekaran']",Reject,2018,[32],[37],[318],[181],[19],[118],"Summary:
This paper proposes parallel GANs (PGANs). This is a new architecture which composes the generator based on a mixture of weak generators with the main intended purpose that each unique generator may suffer mode collapse, but as long as each generator collapses to a distinct mode, the combination of generators will cover the whole image distribution. The paper proposes a number of technical details to 1) ensure that each sub generator offers distinct information (adjustment component, C) and 2) to efficiently train the generators in parallel while accumulating information to update both the discriminator and the adjustment component. 
Results are shown on a synthetic dataset of gaussian mixtures, demonstrating that the model does indeed find all modes within the data, and on two small real image datasets: MNIST and CIFAR-10. Overall the parallel generator model results in ~x2 speedup in training time compared with a single complex generator model.

Strengths:
Mode collapse in GANs is a timely and unsolved problem. While most work aims to construct auxiliary loss function to prevent this collapse, this paper instead chooses to accept the collapse and instead encourage multiple models which collapse to unique modes. Though this does present a new problem in chooses the number of modes to estimate within a data source, the paper also presents a solution to systematically combine redundant modes over time, making the model more robust to the choice of number of generators overall. 

Weaknesses:
Organization - The paper is quite difficult to read. Some concepts are presented out of order. For example, the notion of an adjustment component is very natural but not introduced until after it is mentioned a few times. Similarly, G_{-k} is mentioned many times but not clearly defined.  I would suggest to the authors to reorder the subsections in the method part to first outline the main idea: (parallel generators to capture different parts of overall distribution), mention the need to prevent redundancy between the generators (C), and mention some technical overhead in determining how to process all generated images by D. All of this may be discussed within the context of Fig 1. Also Fig 1a-b may be combined and may aid in explanation. 

Experiments - Comparison is limited to single generator models. Many other generator approaches exist beyond a single generator/discriminator GAN. In particular, different loss functions for training the generator (LS-GAN etc). Missing some relevant details like why use HogWild or what it is. 

Minimal understanding - I would like to know what exactly each generator contributes in the real world datasets. Can you show some generations from each mode? Is there a human perceivable difference?

Figure 4: why does the inception score for the single generator models vary with the #generators?

Last paragraph before 4.2.1: Please clarify this sentence - “we designed a relatively strong discriminator with a high learning rate, since the gradient vanish problem is not observed in reverse KL GAN.” 

Typo: last line page 7: “we the use” → “we use the”","[6, 5, 3]","[' Marginally above acceptance threshold', ' Marginally below acceptance threshold', ' Clear rejection']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review acknowledges the importance of the problem tackled and the strengths of the paper's approach. While it points out weaknesses in organization, experimental comparisons, and clarity, the suggestions are constructive and aim to improve the paper. The reviewer also shows interest in understanding the model's workings better, indicating a positive engagement with the work.",60.0,80.0
The Mutual Autoencoder: Controlling Information in Latent Code Representations,"['Mary Phuong', 'Max Welling', 'Nate Kushman', 'Ryota Tomioka', 'Sebastian Nowozin']",Reject,2018,"[0, 19, 12, 13, 12]","[5, 24, 16, 18, 17]","[8, 390, 33, 76, 121]","[4, 190, 19, 38, 69]","[4, 166, 13, 24, 42]","[0, 34, 1, 14, 10]","This paper presents mutual autoencoders (MAE). MAE aims to address the limitation of regular variational autoencoders (VAE) for latent representation learning — VAE sometimes simply ignores the latent code z, especially with a powerful decoding distribution. The idea of MAE is to optimize the VAE objective subject to a constraint on the mutual information between the data x and latent code z: setting the mutual information constraints larger will force the latent code z to learn a meaningful representation of the data. An approximation strategy is employed to approximate the intractable mutual information. Experimental results on both synthetic data and movie review data demonstrate the effectiveness of the MAEs.  

Overall, the paper is well-written. The problem that VAEs fail to learn a meaningful representation is a well-known issue. This paper presents a simple, yet principled modification to the VAE objective to address this problem. I do, however, have two major concerns about the paper:

1. The proposed idea to add a mutual information constraint between the data x and latent code z is a very natural fix to the failure of regular VAEs. However, mutual information itself is not a quantity that is easy to comprehend and specify. This is not like, e.g., l2 regularization parameter, for which there exists a relatively clear way to specify and tune. For mutual information, at least it is not clear to me, how much mutual information is “enough” and I am pretty sure it is model/data-dependent. To make it worse, there exist no metrics in representation learning for us to easily tune this mutual information constraint. It seems the only way to select the mutual information constraint is to qualitative inspect the model fits. This makes the method less practical. 

2. The approximation to the mutual information seems rather loose. If I understand correctly, the optimization of MAE is similar to that of a regular VAE, with an additional parametric model r_w(z|x) which is used to approximate the infomax bound. (And this also adds an additional term to the gradient wrt \theta). r_w(z|x) is updated at the same time as \theta, which means r_w(z|x) is quite far from being an optimal r* as it is intended, especially early during the optimization. Further more, all the derivation following Eq (12-13) are based on r* being optimal, while in reality, it is probably not even close. This makes the whole approximation quite hand-waving. 

Related to 2, the discussion in Section 6 deserves more elaboration. It seems that having a flexible encoder is quite important, yet the authors only mention lightly that they use the approximate posterior from Cremer et al. (2017). Will MAE not work without this? How will VAE (without the mutual information constraint) work with this? A lot of the details seem to be glossed over. 

Furthermore, this work is also related to the deep variational information bottleneck of Alemi et al. 2017 (especially in the appendix they derived the VAE objective using information bottleneck principle). My intuition is that using a larger mutual information constraint in MAE is somewhat similar to setting the regularization \beta to be smaller than 1 — both are making the approximating posterior more concentrated. I wonder if the authors have explored this idea. 
 

Minor comments:

1. It would be more informative to include the running time in the presented results. 

2. Since the goal of r_w(z | x) is to approximate the posterior p(z | x), what about directly using q(z | x) to approximate it? 

3. In Algorithm 1, should line 14 and 15 be swapped? It seems samples are required in line 14 as well. 

4. Nitpicking: technically the model in Eq (1) is not a hierarchical model. 

","[5, 4, 4]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer acknowledges the paper's contribution and clear writing, indicating a positive sentiment. However, they raise major concerns about the practicality of the method and the approximation's validity, pulling the sentiment towards neutral. The language remains polite throughout, focusing on constructive criticism and suggestions for improvement.",40.0,80.0
UPS: optimizing Undirected Positive Sparse graph for neural graph filtering,"['Mikhail Yurochkin', 'Dung Thai', 'Hung Hai Bui', 'XuanLong Nguyen']",Reject,2018,"[3, 2, 23, 19]","[8, 7, 27, 24]","[70, 14, 82, 76]","[31, 7, 55, 33]","[38, 7, 13, 28]","[1, 0, 14, 15]","Learning adjacency matrix of a graph with sparsely connected undirected graph with nonnegative edge weights is the goal of this paper. A projected sub-gradient descent algorithm is used. The UPS optimizer by itself is not new.

Graph Polynomial Signal (GPS) neural network is proposed to address two shortcomings of GSP using linear polynomial graph filter. First, a nonlinear function sigma in (8) is used, and second, weights are shared among neighbors of every data points. There are some concerns about this network that need to be clarified:
1. sigma is never clarified in the main context or experiments
2. the shared weights should be relevant to the ordering of neighbors, instead of the set of neighbors without ordering, in which case, the sharing looks random.
3. another explanation about the weights as the rescaling to matrix A needs to further clarified. As authors mentioned that the magnitude of |A| from L1 norm might be detrimental for the prediction. What is the disagreement between L1 penalty and prediction quality? Why not apply these weights to L1 norm as a weighted L1 norm to control the scaling of A?
4. Authors stated that the last step is to build a mapping from the GPS features into the response Y. They mentioned that linear fully connected layer or a more complex neural network can be build on top of the GPS features. However, no detailed information is given in the paper. In the experiments, authors only stated that “we fit the GPS architecture using UPS optimizer for varying degree of the neighborhood of the graph”, and then the graph is used to train existing models as the input of the graph. Which architecture is used for building the mapping ?

In the experimental results, detailed definition or explanation of the compared methods and different settings should be clarified. For example, what is GPS 8, GCN_2 Eq. 9 in Table 1, and GCN_3 9 and GPS_1, GPS_2, GPS_3 and so on. More explanations of Figure 2 and the visualization method can be great helpful to understand the advantages of the proposed algorithm. 
","[6, 4, 3]","[' Marginally above acceptance threshold', ' Ok but not good enough - rejection', ' Clear rejection']","[3, 3, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review is critical of the paper, pointing out several areas that need clarification and improvement. The reviewer questions the clarity, novelty, and experimental validation of the proposed method. They find the explanation of key components like 'sigma' and the weight sharing mechanism insufficient. The reviewer also criticizes the lack of details about the mapping from GPS features to the response variable and the ambiguity surrounding the experimental setup. The use of phrases like ""need to be clarified,"" ""should be further clarified,"" and ""no detailed information is given"" indicates a negative sentiment. However, the language remains professional and avoids harsh or disrespectful tones.",-50.0,50.0
"Learning to Select: Problem, Solution, and Applications","['Heechang Ryu', 'Donghyun Kim', 'Hayong Shin']",Reject,2018,"[3, 'no_match', 26]","[7, 'no_match', 31]","[9, 'no match', 50]","[4, 'no match', 20]","[5, 'no match', 5]","[0, 'no match', 25]","The paper proposed a new framework called `Learning to select’, in which a best candidate needs to be identified in the decision making process such as job dispatching. A CNN architecture is designed, called `Variable-Length CNN’, to solve this problem.

My major concern is on the definition of the proposed concept of `learning-to-select’. Essentially, I’ve not seen its key difference from the classification problem. While `even in the case of completely identical candidates, the label can be 1 in some situations, and in some other situations the label can be 0’, why not including such `situations’ into your feature vector (i.e., x)? Once you do it, the gap between learning to select and classification will vanish. If this is not doable, you should better make more discussions, especially on what the so-called `situations’ are.  Furthermore, the application scope of the proposed framework is not very well discussed. If it is restricted to job dispatching scenarios, why do we need a new concept “learning to select”?

The proposed model looks quite straightforward. Standard CNN is able to capture the variable length input as is done in many NLP tasks. Dynamic computational graph is not new either. In this sense, the technical novelty of this work is somehow limited.

The experiments are weak in that the data are simulated and the baselines are not strong. I’ve not gained enough insights on why the proposed model could outperform the alternative approaches. More discussions and case studies are sorely needed.
","[4, 4, 4]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer raises several significant concerns about the paper, questioning the core concept's novelty and its difference from classification, finding the technical novelty limited, and criticizing the experiments as weak. The tone, while direct and critical, maintains a professional and academic demeanor.",-50.0,50.0
On the Construction and Evaluation of Color Invariant Networks,['Konrad Groh'],Reject,2018,[9],[12],[8],[2],[0],[6],"The authors investigate a modified input layer that results in color invariant networks. The proposed methods are evaluated on two car datasets. It is shown that certain color invariant ""input"" layers can improve accuracy for test-images from a different color distribution than the training images.


The proposed assumptions are not well motivated and seem arbitrary. Why is using a permutation of each pixels' color a good idea?

The paper is very hard to read. The message is unclear and the experiments to prove it are of very limited scope, i.e. one small dataset with the only experiment purportedly showing generalization to red cars.

Some examples of specific issues:
- the abstract is almost incomprehensible and it is not clear what the contributions are
- Some references to Figures are missing the figure number, eg. 3.2 first paragraph, 
- It is not clear how many input channels the color invariant functions use, eg. p1 does it use only one channel and hence has fewer parameters?
- are the training and testing sets all disjoint (sec 4.3)?
- at random points figures are put in the appendix, even though they are described in the paper and seem to show key results (eg ""tested on nored-test"")
- Sec 4.6: The explanation for why the accuracy drops for all models is not clear. Is it because the total number of training images drops? If that's the case the whole experimental setup seems flawed.
- Sec 4.6: the authors refer to the ""order net"" beating the baseline, however, from Fig 8 (right most) it appears as if all models beat the baseline. In the conclusion they say that weighted order net beats the baseline on all three test sets w/o red cars in the training set. Is that Fig 8 @0%? The baseline seems to be best performing on ""all cars"" and ""non-red cars""

In order to be at an appropriate level for any publication the experiments need to be much more general in scope.
","[3, 4, 3]","[' Clear rejection', ' Ok but not good enough - rejection', ' Clear rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with a neutral statement summarizing the paper's topic. However, it quickly transitions into a highly critical tone, using phrases like ""not well motivated,"" ""seem arbitrary,"" ""very hard to read,"" and ""unclear."" The reviewer finds significant flaws in the paper's clarity, methodology, and results. While the language avoids personal attacks, the overall tone is quite negative due to the severity and number of issues pointed out. ",-75.0,20.0
Spontaneous Symmetry Breaking in Deep Neural Networks,"['Ricky Fok', 'Aijun An', 'Xiaogang Wang']",Reject,2018,"[2, 25, 14]","[2, 30, 18]","[6, 174, 22]","[1, 117, 7]","[2, 14, 2]","[3, 43, 13]","The paper makes a mathematical analogy between deep neural networks and quantum field theory, and claims that this explains a large number of empirically observed phenomena.

I have a solid grasp of the relevant mathematics, and a superficial understanding of QFT, but I could not really make sense of this paper. The paper uses mathematics in a very loose manner. This is not always bad (an overly formal treatment can make a paper hard to read), but in this case it is not clear to me that the results are even ""correct modulo technicalities"" or have much to do with the reality of what goes on in deep nets.

The first thing I'm confused about is the nature and significance of the symmetries considered in this paper. At a very high level, there are two kinds of symmetries one could consider in DL: transformations of the input space that leave invariant the desired output, and transformations of the weight space that leave invariant the input/output mapping. These are not necessarily related. For instance, a translation or rotation of an image is an example of the former, whereas an arbitrary permutation of hidden units (and corresponding rows/columns of weight matrices) is an example of the latter. This paper is apparently dealing with groups that act on the input as well as the weight space, seemingly conflating the two.

Section 2.2 defines the action of symmetries on the input and weight space. For each layer t, we have a matrix Q_t in G, where G is an unspecified Lie group. Since all Q_t are elements of the same group, they have the same dimension, so all layers must have the same dimension as well. This is somewhat unrealistic. Furthermore, from the definitions in 2.2 it seems that in order to get covariance, the Q_t would have to be the same for all t, which is probably not what the authors had in mind.

For symmetries like rotation/translation of images, a better setup would probably involve a single group with different group actions or linear group representations for each layer. In that case, covariance of the weight layers is not automatic, but only holds for certain subspaces of weight space. For permutation or scale symmetries in weight space, a more sensible setup would be to say that each layer has a different group of symmetries, and the symmetry group of the whole network is the direct product of these groups.

It is stated that transformations in the affine group may not commute with nonlinearities, but rotations of feature maps do. This is correct (at least up to discretization errors), but the paper continues to talk about affine and orthogonal group symmetries. Later on an attempt is made to deal with this issue, by splitting the feature vectors into a part that is put to zero by a ReLU, and a part that is not, and the group is split accordingly. However, this does not make any sense because the pattern of zeros/non-zeros is different for each input, so one cannot speak of a ""remnant symmetry"" for a layer in general.

The connection between DL and QFT described in 2.3 is based on some kind of ""continuous limit"" of units and layers, i.e. having an uncountably infinite number of them. Even setting aside the enormous amount of technical difficulty involved in doing this math properly, I'm a bit skeptical that this has anything to do with real networks.

As an example of how loose the math is, ""theorem 1"" is only stated in natural language: ""Deep feedforward networks learn by breaking symmetries"". The proof involves assuming that the network is a sequence of affine transformations (no nonlinearities). Then it says that if we include a nonlinearity, it breaks the symmetry. Thus, since neural nets use nonlinearities, they break symmetries, and therefore learning works by breaking symmetries and the layers can learn a ""more generalized representation"" than an affine network could. The theorem is so vaguely stated that I don't know what it means, and the proof is inscrutable to me.

Theorem 2 states ""Let x^T x be an invariant under Aff(D)"". Clearly x^T x is not invariant under Aff(D).

The paper claims to explain many empirical facts, but it is not exactly clear which are the conspicuous and fundamental facts that need explaining. For instance, the IB phase transition claimed to happen in deep learning was recently called into question [1]. It appears that this phenomenon does not occur in ReLU nets but only in sigmoid nets, but the current paper purports to explain the phenomenon while assuming ReLUs. I would further note that the paper claims to explain a suspiciously large number of previously observed phenomena (Appendix A), but as far as I can tell does not make novel testable predictions.

The paper makes several strong claims, like ""we [...] illustrate that spontaneous symmetry breaking of affine symmetries is the sufficient and necessary condition for a deep network to attain its unprecedented power"", ""This phenomenon has profound implications"", ""we have solved one of the most puzzling mysteries of deep learning"", etc. In my opinion, unless it is completely obvious that this is indeed a breakthrough, one should refrain from making such statements.

[1] On the information bottleneck theory of deep learning. Anonymous ICLR2018 submission.","[3, 3, 3]","[' Clear rejection', ' Clear rejection', ' Clear rejection']","[3, 4, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review is highly critical of the paper, pointing out serious flaws in the mathematical reasoning and expressing skepticism about the paper's claims. The reviewer finds the paper's arguments unconvincing and its conclusions unsupported. While the reviewer maintains a professional tone, their criticism is quite strong, suggesting a negative sentiment. The language used is direct and critical, but not disrespectful.",-70.0,60.0
The Set Autoencoder: Unsupervised Representation Learning for Sets,['Malte Probst'],Reject,2018,[5],[10],[21],[6],[11],[4],"Summary:

This paper proposes an encoder-decoder framework for learning latent representations of sets of elements. The model utilizes the neural attention mechanism for set inputs proposed in (Vinyals et al., ICLR 2016) to encode a set into a fixed-length latent representation, and then employs an LSTM decoder to reconstruct the original set of elements, in which a stable matching algorithm is used to match decoder outputs to input elements. Experimental results on synthetic datasets show that the model learns meaningful representations and effectively handles permutation invariance.

Major Concerns:

1. Although the employed Gale-Shapely algorithm facilitates permutation-invariant set reconstruction, it has O(n^2) computational complexity during each back-propagation iteration, which might prevent it from scaling to sets of fairly big sizes. 

2. The experiments are only evaluated on synthetic datasets, and applications of the set autoencoder to real-world applications or scientific problems will make this work more interesting and significant.

3. The main contribution of this work is the adoption of the stable matching algorithm in the decoder. A strong set autoencoder baseline will be, the encoder employs the neural attention mechanism proposed in (Vinyals et al., ICLR 2016), but the decoder just uses a standard LSTM as in a seq2seq framework. Comparisons to this baseline will reveal the contribution of the stable matching procedure in the whole  framework of  the set autoencoder for learning representations. 

Minor issues:

On page 5, above Section 4, d_j -> o_j ?

the footnote on page 5: we not consider -> we do not consider?

on page 6 and 7,   6.000, 1.000 and 10.000 training examples ->  6000, 1000 and 10,000 training examples","[5, 4, 5]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review acknowledges the paper's contribution in proposing an encoder-decoder framework with a stable matching algorithm for set autoencoders. However, it raises major concerns regarding scalability, lack of real-world application, and a missing baseline comparison. While the review provides constructive criticism, the presence of 'major concerns' suggests a need for significant improvements. The language used is formal and polite, focusing on technical aspects and suggestions for improvement.",40.0,80.0
Now I Remember! Episodic Memory For Reinforcement Learning,"['Ricky Loynd', 'Matthew Hausknecht', 'Lihong Li', 'Li Deng']",Reject,2018,"[19, 10, 'no_match', 28]","[24, 15, 'no_match', 30]","[11, 53, 'no match', 329]","[6, 25, 'no match', 188]","[5, 24, 'no match', 36]","[0, 4, 'no match', 105]","The paper addresses an important problem of how ML systems can learn episodic memory.
Authors, first, criticize the existing approaches and benchmarks for episodic memory, arguing that the latter do not necessarily test episodic memory to the full extent of human-level intelligence.
Then, a new external memory augmented network (MEM) is proposed which is similar in the spirit to content-based retrieval architectures such as DNC and memory networks, but allows to explicitly exclude certain dimensions of memory vectors from matching. 
Authors evaluate the proposed MEM together with DNC and simple LSTM baselines on the game of Concentration where they find MEM to outperform concurrent approaches.

Unfortunately, I did not find enough of novelty, clarity or at least rigorous and interesting experiments in the paper to recommend acceptance.

Detailed comments:
1) When a new architecture is proposed, it is good to describe in detail, at least in the appendix. Currently, it is introduced only implicitly and a reader should infer the details from fig. 2.
2) It looks like the main difference between DNC and MEM is the way of addressing memories that allow explicit masking. If so, then to me this is a rather minor novelty and to justify it's importance authors should run a control experiment with the exact same architecture as in DNC, but with a masked similarity kernel. Besides that, an analysis of that is learned to be masked should be provided, how ""hard"" (i.e. strictly 0 and 1) are the masks, what influences them etc.
3) While the game of concentration clearly requires episodic memory to some extent, this only task is not enough for testing EM approaches, because there is always a risk that one of the evaluated systems somehow overfitted to this task by design. Especially to reason about human-level intelligence we need a variety of tasks.
4) To continue the previous point, humans would not perform well in the proposed task with random card labels, because it is very likely that familiar objects on cards help building associations and remembering them. Thus it is impossible to make a human baseline for this task and decide on how far are we below the human level. ","[4, 4, 4]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[4, 5, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The reviewer acknowledges the importance of the research topic and summarizes the authors' approach. However, they express disappointment with the novelty, clarity, and experimental rigor, ultimately not recommending the paper for acceptance. The reviewer lists several specific concerns, indicating a negative sentiment. The language remains professional and polite throughout, focusing on constructive criticism.",-60.0,70.0
Disentangled activations in deep networks,"['Mikael Kågebäck', 'Olof Mogren']",Reject,2018,"[5, 11]","[5, 16]","[10, 30]","[5, 16]","[4, 11]","[1, 3]","The authors propose a penalization term that enforces decorrelation between the dimensions of the representation 
They show that it can be included as additional term in cost functions to train generic models.
The idea is simple and it seems to work for the presented examples.

However, they talk about gradient descent using this extra term, but I'd like to see the derivatives of the 
proposed term depending on the parameters of the model (and this depends on the model!). On the other hand, 
given the expression of the proposed regulatization,
it seems to lead to non-convex optimization problems which are hard to solve. Any comment on that?.

Moreover, its results are not quantitatively compared to other Non-Linear generalizations of PCA/ICA designed for similar goals (e.g. those cited in the ""related work"" section or others which have been proved to be consistent non-linear generalizations of PCA such as: Principal Polynomial Analysis, Dimensionality Reduction via Regression that follow the family introduced in the book of Jolliffe, Principal Component Analysis).

Minor points: Fig.1 conveys not that much information.","[6, 5, 4]","[' Marginally above acceptance threshold', ' Marginally below acceptance threshold', ' Ok but not good enough - rejection']","[3, 4, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review starts with a positive sentiment, acknowledging the simplicity and functionality of the proposed idea. However, it quickly transitions into a more critical tone, expressing concerns about the non-convex optimization problems and the lack of quantitative comparisons with existing methods. The language used is professional and not impolite, but the criticism and demand for more rigorous analysis contribute to a less positive overall sentiment.",20.0,60.0
A Deep Learning Approach for Survival Clustering without End-of-life Signals,"['S Chandra Mouli', 'Bruno Ribeiro', 'Jennifer Neville']",Reject,2018,"[4, 18, 17]","[9, 23, 22]","[11, 122, 139]","[5, 57, 86]","[6, 51, 34]","[0, 14, 19]","Pros:
The paper is a nice read, clearly written, and its originality is well stated by the authors, “addressing the lifetime clustering problem without end-of-life signals for the first time”. I do not feel experienced enough in the field to evaluate the significance of this work.

The approach proposed in the manuscript is mainly based on a newly-designed nonparametric loss function using the Kuiper statistic and uses a feed-forward neural network to optimize the loss function. This approach does challenge some traditional assumptions, such as the presence of end-of-life signals or the artificial defined timeouts. Instead of giving a clear end-of-life signal, the authors specify a probability of end-of-life that permits us to take into account the associated uncertainty. By analyzing a large-scale social network dataset, it is shown that the proposed method performs better on average than the other two traditional models.

Cons:       
I think that the main drawback of the paper is that the structure of the neural network and the deep learning techniques used for optimizing the loss function are not explained in sufficient detail. ","[6, 6, 4]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Ok but not good enough - rejection']","[1, 5, 4]","["" The reviewer's evaluation is an educated guess"", ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer provides both positive and negative feedback, but the positive feedback outweighs the negative. The reviewer finds the paper well-written and acknowledges the originality of the work. While they point out a drawback, they do so constructively. The language used is formal and respectful throughout.",60.0,80.0
Iterative temporal differencing with fixed random feedback alignment support spike-time dependent plasticity in vanilla backpropagation for deep learning,"['Aras Dargazany', 'Kunal Mankodiya']",Reject,2018,"[9, 10]","[12, 15]","[10, 70]","[1, 40]","[7, 14]","[2, 16]","The paper falls far short of the standard expected of an ICLR submission. 

The paper has little to no content. There are large sections of blank page throughout. The algorithm, iterative temporal differencing, is introduced in a figure -- there is no formal description. The experiments are only performed on MNIST. The subfigures are not labeled. The paper over-uses acronyms; sentences like “In this figure, VBP, VBP with FBA, and ITD using FBA for VBP…” are painful to read. 


","[2, 3, 2]","[' Strong rejection', ' Clear rejection', ' Strong rejection']","[5, 4, 5]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review is extremely negative about the paper, using harsh language like ""falls far short"" and ""little to no content."" It points out numerous flaws with a condescending tone. Therefore, the sentiment is highly negative, and the politeness is very low.",-90.0,-80.0
Combining Model-based and Model-free RL via Multi-step Control Variates,"['Tong Che', 'Yuchen Lu', 'George Tucker', 'Surya Bhupatiraju', 'Shane Gu', 'Sergey Levine', 'Yoshua Bengio']",Reject,2018,"[7, 'no_match', 6, 2, 0, 10, 31]","[12, 'no_match', 10, 4, 1, 15, 36]","[38, 'no match', 75, 10, 1, 743, 975]","[14, 'no match', 34, 5, 1, 326, 405]","[18, 'no match', 39, 5, 0, 396, 454]","[6, 'no match', 2, 0, 0, 21, 116]","This paper presents a model-based approach to variance reduction in policy gradient methods.  The basic idea is to use a multi-step dynamics model as a ""baseline"" (more properly a control variate, as the terminology in the paper uses, but I think baselines are more familiar to the RL community) to reduce the variance of a policy gradient estimator, while remaining unbiased.  The authors also discuss how to best learn the type of multi-step dynamics that are well-suited to this problem (essentially, using off-policy data via importance weighting), and they demonstrate the effectiveness of the approach on four continuous control tasks.

This paper presents a nice idea, and I'm sure that with some polish it will become a very nice conference submission. But right now (at least as of the version I'm reviewing), the paper reads as being half-finished.  Several terms are introduced without being properly defined, and one of the key formalisms presented in the paper (the idea of ""embedding"" an ""imaginary trajectory"" remains completely opaque to me.  Further, the paper seems to simply leave out some portions: the introduction claims that one of the contributions is ""we show that techniques such as latent space trajectory embedding and dynamic unfolding can significantly boost the performance of the model based control variates,"" but I see literally no section that hints at anything like this (no mention of ""dynamic unfolding"" or ""latent space trajectory embedding"" ever occurs later in the paper).

In a bit more detail, the key idea of the paper, at least to the extent that I understood it, was that the authors are able to introduce a model-based variance-reduction baseline into the policy gradient term.  But because (unlike traditional baselines) introducing it alone would affect the actual estimate, they actually just add and subtract this term, and separate out the two terms in the policy gradient: the new policy gradient like term will be much smaller, and the other term can be computed with less variance using model-based methods and the reparameterization trick.  But beyond this, and despite fairly reasonable familiarity with the subject, I simply don't understand other elements that the paper is talking about.

The paper frequently refers to ""embedding"" ""imaginary trajectories"" into the dynamics model, and I still have no idea what this is actually referring to (the definition at the start of section 4 is completely opaque to me).  I also don't really understand why something like this would be needed given the understanding above, but it's likely I'm just missing something here.  But I also feel that in this case, it borders on being an issue with the paper itself, as I think this idea needs to be described much more clearly if it is central to the underlying paper.

Finally, although I do think the extent of the algorithm that I could follow is interesting, the second issue with the paper is that the results are fairly weak as they stand currently.  The improvement over TRPO is quite minor in most of the evaluated domains (other than possibly in the swimmer task), even with substantial added complexity to the approach.  And the experiments are described with very little detail or discussion about the experimental setup.

Nor are either of these issues simply due to space constraints: the paper is 2 pages under the soft ICLR limit, with no appendix.  Not that there is anything wrong with short papers, but in this case both the clarity of presentation and details are lacking.  My honest impression is simply that this is still work in progress and that the write up was done rather hastily.  I think it will eventually become a good paper, but it is not ready yet.","[4, 5, 5]","[' Ok but not good enough - rejection', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer acknowledges the paper presents a ""nice idea"" and could be a ""very nice conference submission"" with improvements. However, they find the paper ""half-finished"" with undefined terms and missing explanations. The reviewer expresses confusion about the ""imaginary trajectory embedding"" concept and finds the results ""fairly weak."" They criticize the lack of detail in the experiments and the overall rushed feel of the paper.  While not outright negative, the critique points to significant areas needing improvement, making the sentiment more negative than positive. The language, while direct, maintains a professional and respectful tone.",-25.0,50.0
APPLICATION OF DEEP CONVOLUTIONAL NEURAL NETWORK TO PREVENT ATM FRAUD BY FACIAL DISGUISE IDENTIFICATION,"['Suraj Nandkishor Kothawade', 'Sumit Baburao Tamgale']",Reject,2018,"['no_match', 'no_match']","['no_match', 'no_match']","['no match', 'no match']","['no match', 'no match']","['no match', 'no match']","['no match', 'no match']","
As one can see by the title, the originality (application of DCNN) and significance (limited to ATM domain) is very limited. If this is still enough for ICLR, the paper could be okay. However, even so one can clearly see that the architecture, the depth, the regularization techniques, and the evaluation are clearly behind the state of the art. Especially for this problem domain, drop-out and data augmentation should be investigated.

Only one dataset is used for the evaluation and it seems to be very limited and small. Moreover, it seems that the same subjects (even if it is other pictures) may appear in the training set and test set as they were randomly selected. Looking into the referece (to get the details of the dataset -  from a workshop of the IEEE International Conference on Computer Vision Workshops (ICCVW) 2017) reveals, that it has only 25 subjects and 10 disguises. This makes it even likely that the same subject with the same disguise appears in the training and test set.

A very bad manner, which unfortunately is often performed by deep learning researchers with limited pattern recognition background, is that the accuracy on the test set is measured for every timestamp and finally the highest accuracy is reported. As such you perform an optimization of the paramerter #iterations on the test set, making it a validation set and not an independent test set. 

Minor issues:
make sure that the capitalization in the references is correct (ATM should be capital, e.g., by putting {ATM} - and many more things).","[2, 1, 3]","[' Strong rejection', ' Trivial or wrong', ' Clear rejection']","[4, 5, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review is highly critical of the paper, pointing out significant limitations in originality, methodology, and evaluation. The reviewer uses strong language like ""very limited,"" ""clearly behind the state of the art,"" ""very bad manner,"" and ""unfortunately is often performed by deep learning researchers with limited pattern recognition background"" to express their negative opinion. While they offer some constructive feedback, the tone is predominantly negative and somewhat condescending.",-75.0,-40.0
A Goal-oriented Neural Conversation Model by Self-Play,"['Wei Wei', 'Quoc V. Le', 'Andrew M. Dai', 'Li-Jia Li']",Reject,2018,"[14, 8]","[19, 13]","[299, 74]","[143, 28]","[145, 42]","[11, 4]","Summary: The paper proposes a self-play model for goal oriented dialog generation, aiming to enforce a stronger coupling between the task reward and the language model.

Contributions:

While there are architectural changes (e.g. the customer agent and client agent have different roles and parameters; the parameters of both agents are updated via self-play training), the information isolation claim is not clear. Both the previous work (Lewis et al., 2017) and the proposed approach pitch two agents against each other and the agents communicate via language utterances alone (e.g. rather than exchanging hidden states). In the previous work, the two agents share a set of initial conditions (the set of objects to be divided; this is required by the nature of the task: negotiation), but the goals of each agent are hidden and the negotiation process and outcome are only revealed through natural language. Could you expand on your claim regarding information isolation? Could you design an experiment which highlights the contribution and provide a comparison with the previous approach?

Furthermore, divergence from natural language when optimizing the task reward remains an issue. As a result, both methods require alternate training between the supervised loss and the reinforcement loss.

Experiments:

1. Minor question: During self-play ""we conduct 1 supervised training using the training data every time we make a reinforcement update"". One iteration or one epoch of supervised training?

2. The method is only evaluated on a toy dataset where both the structure of the dialog is limited (see figure 2) and the sentences themselves (the number of language templates is not provided). The referenced negotiation paper uses data collected from mechanical turk ensuring more diversity and the dataset is publicly available. Couldn't your method be applied to that setting for comparison?

3. The qualitative evaluation shows compelling examples from the model. Are the results hand-picked to highlight the two outcomes? I wish more examples and some statistics regarding the diversity of produced dialogs were provided (e.g. how many times to they result in a booked flight vs. unfulfilled request and compare that with the training data).

4. What is the difference between evaluation reward reported in Table 4 and self-play evaluation reward reported in Table 5? (Is the former obtained by conditioning on target utterances?). Is there a reason to not report the itemized rewards in Table 5 as well (Eval flight, Eval action) etc?

5. The use of the value network vs. the policy network is not clarified in the model description nor in the experiments. Is the value network used to reduce the variance in the reward?

Finally, there are several typos or grammatical errors, including:
- Page 4, t and i should be the same.
- Page 4. Use p(u_t |  t_{<t-1}; \theta) instead of p(u_t |  t_{<t-1} | \theta).
- Page 2, second paragraph: ""which correctness"" -> ""whose correctness"".
- Page 2, second-to-last paragraph: ""access to a pieces"" -> ""access to pieces"", ""to the best of it can"" -> ""as good as it can"".
- Page 4. ""feeded"" -> fed
- Page 5, second-to-last paragraph: ""dataset is consists of"" -> ""dataset consists of"".
- Page 7/8: Both examples are labeled ""Sample dialog 1""
- Dataset & experiments: Table 3 and Table 3
- Experiments: ""to see how our model performs qualitative"" -> ""to see how our model performs qualitatively""
- Related work: ""... of studying dialog system is to ..."" -> ""dialog systems""
- Conclusion: ""In those scenario"" -> ""In those scenarios""","[4, 3, 6]","[' Ok but not good enough - rejection', ' Clear rejection', ' Marginally above acceptance threshold']","[3, 4, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review is quite critical of the paper, pointing out several areas where the claims are not well-supported and suggesting alternative approaches and experiments. While the reviewer acknowledges some positive aspects (e.g., ""compelling examples""), the overall tone and the numerous concerns raised indicate a negative sentiment. However, the language remains professional and polite throughout, focusing on the scientific aspects and offering constructive criticism. Therefore, the politeness score leans towards the positive side.",-40.0,60.0
Achieving Strong Regularization for Deep Neural Networks,"['Dae Hoon Park', 'Chiu Man Ho', 'Yi Chang']",Reject,2018,"[12, 1, 16]","[14, 5, 21]","[21, 22, 218]","[11, 9, 139]","[6, 13, 44]","[4, 0, 35]","The authors studied the behavior that a strong regularization parameter may lead to poor performance in training of deep neural networks. Experimental results on CIFAR-10 and CIFAR-100 were reported using AlexNet and VGG-16. The results seem to show that a delayed application of the regularization parameter leads to improved classification performance.

The proposed scheme, which delays the application of regularization parameter, seems to be in contrast of the continuation approach used in sparse learning. In the latter case, a stronger parameter is applied, followed by reduced regularization parameter. One may argue that the continuation approach is applied in the convex optimization case, while the one proposed in this paper is for non-convex optimization. It would be interesting to see whether deep networks can benefit from the continuation approach, and the strong regularization parameter may not be an issue because the regularization parameter decreases as the optimization progress goes on.

One limitation of the work, as pointed by the authors, is that experimental results on big data sets such as ImageNet is not reported. 
","[6, 4, 5]","[' Marginally above acceptance threshold', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[2, 5, 5]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review starts with a neutral summary of the paper's contents. While it points out a potential contradiction with another approach, it does so in a neutral and questioning way (""It would be interesting to see...""). It also acknowledges the authors' own admission of a limitation. Overall, the tone is one of mild interest with suggestions for improvement rather than harsh criticism.",40.0,80.0
Improving generalization by regularizing in $L^2$ function space,"['Ari S Benjamin', 'Konrad Kording']",Reject,2018,"[1, 19]","[5, 24]","[9, 107]","[2, 29]","[6, 35]","[1, 43]","I have read comments and rebuttal - i do not have the luxury of time to read in depth the revision.
It seems that the authors have made an effort to accommodate reviewers' comments. I upgraded the rating.

-----------------------------------------------------------------------------------------------------------------------

Summary: The paper considers the use of natural gradients for learning. The added twist is the substitution of the KL divergence with the Wasserstein distance, as proposed in GAN training. The authors suggest that Wasserstein regularization improves generalization over SGD with a little extra cost.

The paper is structured as follows:
1. KL divergence is used as a similarity measure between two distributions.
2. Regularizing the objective with KL div. seems promising, but expensive.
3. We usually approximate the KL div. with its 2nd order approximation - this introduces the Hessian of the KL divergence, known as Fisher information matrix.
4. However, computing and inverting the Fisher information matrix is computationally expensive.
5. One solution is to approximate the solution F^{-1} J using gradient descent. However, still we need to calculate F. There are options where F could be formed as the outer product of a collection gradients of individual examples ('empirical Fisher').
6. This paper does not move towards Fisher information, but towards Wasserstein distance: after a ""good"" initialization via SGD is obtained, the inner loop continues updating that point using the Wasserstein regularized objective. 
7. No large matrices need to be formed or inverted, however more passes needed per outer step.

Importance:
Somewhat lack of originality and poor experiments lead to low importance.

Clarity:
The paper needs major revision w.r.t. presenting and highlighting the new main points. E.g., one needs to get to page 5 to understand that the paper is just based on the WGAN ideas in Arjovsky et al., but with a different application (not GANS).

Originality/Novelty:
The paper, based on WGAN motivation, proposes Wasserstein distance regularization over KL div. regularization for training of simple models, such as neural networks. Beyond this, the paper does not provide any futher original idea. So, slight to no novelty.

Main comments:
1. Would the approximation of C_0 by its second-order Taylor expansion (that also introduces a Hessian) help? This would require the combination of two Hessian matrices.

2. Experiments are really demotivating: it is not clear whether using plain SGD or the proposed method leads to better results. 

Overall:
Rejection.
","[5, 4, 6]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Marginally above acceptance threshold']","[4, 3, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer finds the paper lacking in originality and with poor experiments. They state the paper needs major revisions in clarity and highlighting its main points.  They are also not convinced by the experiments. They ultimately recommend rejection, which points towards a negative sentiment. The language used does not contain any personal attacks or overly negative phrasing, suggesting a neutral politeness level.",-50.0,0.0
On Batch Adaptive Training for Deep Learning: Lower Loss and Larger Step Size,"['Runyao Chen', 'Kun Wu', 'Ping Luo']",Reject,2018,"['no_match', 20, 'no_match']","['no_match', 25, 'no_match']","['no match', 106, 'no match']","['no match', 53, 'no match']","['no match', 14, 'no match']","['no match', 39, 'no match']","The paper proposes a generalization of an algorithm by Yin et al. (2017), which performs SGD with adaptive batch sizes. The present paper generalizes the algorithm to SGD with momentum. Since the original algorithm was already formulated with a general utility function, the proposed algorithm is similar in structure but replaces the utility function so that it takes momentum into account. Experiments on an image classification task show improvements in the training loss. However, no test accuracies are reported and the learning curves have suspicious artifacts, see below. Experiments on a relation extraction task show little improvement over SGD with momentum and constant batch size.


COMMENTS:

The paper discusses a relevant issue. While adaptive learning algorithms are popular in deep learning, most algorithms adapt the learning rate or the momentum coefficient, but not the batch size. It appears to me that the main idea and the overall structure of the proposed algorithm is the same as in the one published by Yin et al. (2017), and that only few changes were necessary to include momentum. Given the incremental process, I find the presentation unnecessarily involved, and experiments not convincing enough.

Concerning the presentation, the paper dedicates two full pages on a review of the algorithm by Yin et al. (2017). The first page of this review states that, for large enough batch sizes, the change of the objective function in SGD is normal distributed with a variance that is inversely proportional the batch size. It seems to me that this is a direct consequence of the central limit theorem. The derivation, however, is quite technical and introduces some quantities that are never used (e.g., $\vec{\xi}_j$ is never used individually, only the combined term $\epsilon_t$ defined below Eq. 12 is). The second page of the review seems to discuss the main part of the algorithm, but I could not follow it. First, a ""state"" $s_t$ (also written as $S$) is introduced, which, according to the text, is ""the objective value"", which was earlier denoted by $F$. Nevertheless, the change of $s_t$, Eq. 5, appears to obey a different probability distribution than the change of $F$. The paper provides a verbal explanation for this discrepancy, saying that it is possible that $S$ is first reduced to the minimum $S^*$ of the objective and then increased again. However, in my understanding, the minimum of the objective is only realized at a singular point in parameter space. Crossing this point in an update step should have zero probability as long as the model has more than one parameter. The explanation also does not make it clear why the argument should apply to $S$ (or $s$) but not to $F$.

Page 5 provides pseudocode for the proposed algorithm. However, I couldn't find an explanation of the code. The code suggests that, for each update step, one gradually increases the batch size until it becomes larger or equal than a running estimate of the optimal batch size. While this may be a plausible strategy in practice, it seems to have a bias that is not addressed in the paper: the algorithm recalculates a noisy estimate of the optimal batch size after each increase of the batch size, and it terminates as soon as the noisy estimate happens to be small enough, resulting in a bias towards a smaller than optimal batch size. A probably more important issue is that the algorithm is sequential and hard to parallelize, where parallelization is usually the main motivation to use larger batch sizes. As the gradient noise scales inversely proportional to the batch size, I don't see why increasing the batch size should be preferred over decreasing the learning rate unless optimizations with a larger batch size can be parallelized. The experiments don't compare the two alternatives.

Concerning the experiments, it seems peculiar that the learning curves in Figure 1 remain at a constant value for a long time at the beginning of the optimization before they begin to drop. Do the authors understand this behavior? It could indicate that the magnitude of the random initialization was chosen too small. I.e., the parameters might have been initialized too close to zero, where the loss is stationary due to symmetries. Also, absolute values of the training loss can be deceptive since there is often no natural scale. A better indicator of convergence would be the test accuracy. The identification of the ""batch size boom"" is interesting.","[4, 5, 5]","[' Ok but not good enough - rejection', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[3, 3, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is critical of the paper's novelty, presentation clarity, and experimental validation. It points out potential flaws in the algorithm and questions the significance of the results. While the reviewer acknowledges the relevance of the topic, the numerous concerns and lack of strong positive remarks indicate a negative sentiment. However, the critique is delivered in a professional, analytical manner without resorting to personal attacks or disrespectful language.",-50.0,70.0
Multi-task Learning on MNIST Image Datasets,"['Po-Chen Hsieh', 'Chia-Ping Chen']",Reject,2018,"[2, 17]","[1, 22]","[1, 96]","[1, 79]","[0, 8]","[0, 9]","This paper presents a multi-task neural network for classification on MNIST-like datasets.

The main concern is that the technical innovation is limited. It is well known that multi-task learning can lead to performance improvement on similar tasks/datasets. This does not need to be verified in MNIST-like datasets. The proposed multi-task model is to fine tune a pretrained model, which is already a standard approach for multi-task and transfer learning. So the novelty of this paper is very limited.

The experiments do not bring too much insights.","[5, 4, 5]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[4, 5, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']","The review is negative about the paper, pointing out a lack of novelty and limited insights from the experiments. However, the language used is professional and not personally attacking the authors. Therefore, the sentiment is negative, but the politeness remains neutral.",-50.0,0.0
A Bayesian Nonparametric Topic Model with Variational Auto-Encoders,"['Xuefei Ning', 'Yin Zheng', 'Zhuxi Jiang', 'Yu Wang', 'Huazhong Yang', 'Junzhou Huang']",Reject,2018,"[2, 7, 3, 20, 13]","[7, 11, 6, 25, 18]","[46, 49, 7, 546, 392]","[20, 17, 1, 331, 213]","[20, 20, 3, 57, 113]","[6, 12, 3, 158, 66]","""topic modeling of text documents one of most important tasks""
Does this claim have any backing?

""inference of HDP is more complicated and not easy to be applied to new models""  Really an artifact of the misguided nature of earlier work. The posterior for the $\vec\pi$ of a elements of DP or HDP can be made a Dirichlet, made finite by keeping a ""remainder"" term and appropriate augmentation.  Hughes, Kim and Sudderth (2015) have avoided stick-breaking and CRPs altogether, as have others in earlier work. Extensive models building on simple HDP doing all sorts of things have been developed.

Variational stick-breaking methods never seemed to have worked well.  I suspect you could achieve better results by replacing them as well, but you would have to replace the tree of betas and extend your Kumaraswamy distribution, so it may not work.  Anyway, perhaps an avenue for future work.

""infinite topic models"" I've always taken the view that the use of the word ""infinite"" in machine learning is a kind of NIPSian machismo. In HDP-LDA at least, the major benefit in model performance comes from fitting what you call $\vec\pi$, which is uniform in vanilla LDA, and note that the number of topics ""found"" by a HDP-LDA sampler can be made to vary quite widely by varying what you call $\alpha$, so any statement about the ""right"" number of topics is questionable.  So the claim in 3rd paragraph of Section 2, ""superior"" and ""self-determined topic number"" I'd say are misguided.  Plenty of experimental work to support this.

In Related Work, you seem to only mention HDP for non-parametric topic models.  More work exists, for instance using Pitman-Yor distributions for modelling words and using Gibbs samplers that are efficient and don't rely on the memory hungry HCRP.

Good to see a prior is placed on the concentration parameter.  Very important and not well done in the community, usually.  
ADDED:  Originally done by Teh et al for HDP-LDA, and subsequently done
by several, including Kim et al 2016.   Others stress the importance of this.  You need to
cite at least Teh et al. in 5.4 to show this isn't new and the importance is well known.

The Prod version is a very nice idea.  Great results.  This looks original, but I'm not expert enough in the huge masses of new deep neural network research popping up.

You've upped the standard a bit by doing good experimental work.  Oftentimes this is done poorly and one is left wondering.  A lot of effort went into this.
ADDED:   usually like to see more data sets experimented with

What code is used for HDP-LDA?  Teh's original Matlab HCRP sampler does pretty well because at least he samples hyperparameters and can scale to 100k documents (yes, I tried). The comparison with LDA makes me suspicious. For instance, on 20News, a good non-parametric LDA will find well over 400 topics and roundly beat LDA on just 50 or 200.  If reporting LDA, or HDP-LDA, it should be standard to do hyperparameter fitting and you need to mention what you did as this makes a big difference.
ADDED:   20News results still poor for HPD, but its probably the implementation used ... their
        online variational algorithm only has advantages for large data sets 

Pros:   
* interesting new prod model with good results
* alternative ""deep"" approach to a HDL-LDA model
* good(-ish) experimental work
Cons:
* could do with a competitive non-parametric LDA implementation

ADDED:   good review responses generally
","[7, 3, 5]","[' Good paper, accept', ' Clear rejection', ' Marginally below acceptance threshold']","[4, 4, 2]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']","The review is quite critical of the paper's premise and methodology, particularly questioning the claims about ""infinite topic models"" and pointing out potential weaknesses in the experimental setup. While the reviewer acknowledges the ""Prod"" model as a good idea and praises the experimental effort, the overall tone suggests significant revisions are needed. The language, while direct, avoids being outright rude and maintains a professional tone.",-30.0,40.0
Network of Graph Convolutional Networks Trained on Random Walks,"['Sami Abu-El-Haija', 'Amol Kapoor', 'Bryan Perozzi', 'Joonseok Lee']",Reject,2018,"[6, 1, 8, 12]","[11, 4, 13, 17]","[36, 13, 82, 68]","[15, 5, 35, 38]","[19, 8, 43, 24]","[2, 0, 4, 6]","The paper presents a Network of Graph Convolutional Networks (NGCNs) that uses
random walk statistics to extract information from near and distant neighbors
in the graph.

The authors show that a 2-layer Graph Convolutional Network, with linear
activation and W0 as identity matrix, reduces to a one-step random walk.
They build on this notion to  introduce the idea to make the GCN directly operate
on random walk statistics to better model information across distant nodes.

Given that it is not clear how many steps of random walk to use a-priori it is
proposed to make a mixture of models whose outputs are combined by a
softmax classifier, or by an attention based mixing (learning the mixing coefficients).

I find that the comparison can be considered slightly unfair as NGCN has k-times
the number of GCN models in it. Did the authors compare with a deeper GCN, or
simply with a mixture of plain GCN using one-step random walk?
The datasets used for comparison are extremely simple, and I am glad that the
authors point out that this is a significant issue for benchmark driven research.
However, doing calibration on a subset of the validation nodes via gradient
descent is not very clean as by doing it one implicitly uses those nodes for training.
The improvement of the calibrated model on 5 nodes per class (Table 3) seems
to hint that this peeking into the validation is indeed happening.

The authors mention that feeding explicitly the information on distant nodes
makes learning easier and that otherwise such information it would be hard to
extract from stacking several GCN layers. While this is true for the small datasets
usually considered it is not clear at all whether this still holds when we will
have large scale graph benchmarks.

Experiments are well conducted but lack a comparison with GraphSAGE and MoNet,
which are the reference models for the selected benchmarks. A comparison would have made the contribution stronger in my opinion. Improvements in performance are minor
except for decimated inputs setting reported in Table 3. In this last case though
no statistics over multiple runs are shown.

Overall I like the interpretation, even if a bit forced, of GCN as using one-step
random walk statistics. The paper is clearly written.
The main issue I have with the approach is that it does not bring a very novel
way to perform deep learning on graphs, but rather improves marginally upon
a well established one.
","[6, 5, 5]","[' Marginally above acceptance threshold', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[5, 2, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer raises several valid concerns about the paper, questioning the fairness of comparisons, methodology, and significance of the improvements. While acknowledging the clear writing and interesting interpretation, the reviewer finds the overall contribution not very novel and only marginally improving upon existing methods. The tone is critical but professional and polite.",20.0,60.0
Training Autoencoders by Alternating Minimization,"['Sneha Kudugunta', 'Adepu Shankar', 'Surya Chavali', 'Vineeth Balasubramanian', 'Purushottam Kar']",Reject,2018,"[1, 'no_match', 13, 10]","[6, 'no_match', 18, 15]","[17, 'no match', 200, 69]","[5, 'no match', 110, 32]","[9, 'no match', 78, 32]","[3, 'no match', 12, 5]","In this paper an alternating optimization approach is explored for training Auto Encoders (AEs).
The authors treat each layer as a generalized linear model, and suggest to use the stochastic normalized GD of [Hazan et al., 2015] as the minimization algorithm in each (alternating) phase.
Then they apply the suggested method to several single layer and multi layer AEs, comparing its performance to standard SGD. The paper suggests an interesting approach and provides experimental evidence for its usefulness, especially for multi-layer AEs.


Some comments on the theoretical part:
-The theoretical part is partly misleading. While it is true that every layer can be treated a generalized linear model, the SLQC property only applies for the last layer.
Regarding the intermediate layers, we may indeed treat them as generalized linear models, but with non-monotone activations, and therefore the SLQC property does not apply.
The authors should mention this point.

-Showing that generalized ReLU is SLQC with a polynomial dependence on the domain is interesting. 

-It will be interesting if the authors can provide an analysis/relate to some theory related to alternating minimization of bi-quasi-convex objectives. Concretely: Is there any known theory for such objectives? What guarantees can we hope to achieve?


The extension to muti-layer AEs makes sense and seems to works quite well in practice.

The experimental part is satisfactory, and seems to be done in a decent manner. 
It will be useful if the authors could relate to the issue of parameter tuning for their algorithm.
Concretely: How sensitive/robust is their approach compared to SGD with respect to hyperparameter misspecification.
","[7, 6, 4]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Ok but not good enough - rejection']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with positive remarks, highlighting the interesting approach and experimental evidence. While it points out theoretical limitations, it maintains a constructive and helpful tone, suggesting improvements and further investigations. The language is formal and objective, typical of scientific discourse.",60.0,80.0
Improve Training Stability of Semi-supervised Generative Adversarial Networks with Collaborative Training,"['Dalei Wu', 'Xiaohua Liu']",Reject,2018,"[16, 18]","[21, 23]","[111, 113]","[57, 67]","[3, 6]","[51, 40]","* Summary *
The paper addresses the instability of GAN training. More precisely, the authors aim at improving the stability of the semi-supervised version of GANs presented in [1] (IGAN for short) . The paper presents a novel architecture for training adversarial networks in a semi-supervised settings (Algorithm 1). It further presents two theoretical results --- one (Theorem 2.1) showing that the generator's gradient vanish for IGAN, and the second (Theorem 3.1) showing that the proposed algorithm does not suffer this behaviour. Finally, experiments are provided (for MNIST and CIFAR10), which are meant to support empirically the claimed improved stability of the proposed method compared to the previous GAN implementations (including IGAN).

I need to say the paper is poorly written and not properly polished. Among many other things:

(1) It refers to non-existent results in other papers. Eq 2 is said to follow [1], meanwhile the objectives are totally different: the current paper seems to use the l2 loss, while Salimans et al. use the cross-entropy;

(2) Does not introduce notations in statements of theorems ($J_\theta$ in Theorem 2.1?) and provides unreadable proofs in appendix (proof of Theorem 2.1 is a sequence of inequalities involving the undefined notations with no explanations). In short, it is very hard to asses whether the proposed theoretical results are valid;

(3) Does not motivate, discuss, or comment the architecture of the proposed method at all (see Section 3).

Finally, in the experimental section it is unclear how exactly the authors measure the stability of training. The authors write ""unexpectedly high error rates and poor generate image quality"" (page 5), however, these things sounds very subjective and the authors never introduce a concrete metric. The authors only report ""0 fails"", ""one or two out of 10 runs fail"" etc. Moreover, for CIFAR10 it seems the authors make conclusions based only on 3 independent runs (page 6).

[1] Salimans et al, Improved Techniques for Training GANs, 2016","[3, 3, 2]","[' Clear rejection', ' Clear rejection', ' Strong rejection']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer highlights several significant shortcomings in the paper, including factual errors, unclear writing, lack of motivation for the proposed method, and questionable experimental methodology. They use phrases like ""poorly written,"" ""not properly polished,"" ""unreadable proofs,"" ""very hard to assess,"" ""unclear,"" ""sounds very subjective,"" which all point towards a negative sentiment. However, the criticism is presented in a professional and analytical manner, focusing on the paper's content rather than resorting to personal attacks. Therefore, the politeness score leans towards the neutral side of the spectrum.",-50.0,20.0
Linearly Constrained Weights: Resolving the Vanishing Gradient Problem by Reducing Angle Bias,['Takuro Kutsuna'],Reject,2018,[9],[14],[20],[12],[4],[4],"This paper studies the impact of angle bias on learning deep neural networks, where angle bias is defined to be the expected value of the inner product of a random vectors (e.g., an activation vector) and a given vector (e.g., a weight vector).  The angle bias is non-zero as long as the random vector is non-zero in expectation and the given vector is non-zero.  This suggests that the some of the units in a deep neural network have large values (either positive or negative) regardless of the input, which in turn suggests vanishing gradient.  The proposed solution to angle bias is to place a linear constraint such that the sum of the weight becomes zero.  Although this does not rule out angle bias in general, it does so for the very special case where the expected value of the random vector is a vector consisting of a common value.  Nevertheless, numerical experiments suggest that the proposed approach can effectively reduce angle bias and improves the accuracy for training data in the CIFAR-10 task.  Test accuracy is not improved, however.

Overall, this paper introduces an interesting phenomenon that is worth studying to gain insights into how to train deep neural networks, but the results are rather preliminary both on theory and experiments.

On the theoretical side, the linearly constrained weights are only shown to work for a very special case.  There can be many other approaches to mitigate the impact of angle bias.  For example, how about scaling each variable in a way that the mean becomes zero, instead of scaling it into [-1,+1] as is done in the experiments?  When the mean of input is zero, there is no angle bias in the first layer.  Also, what about if we include the bias term so that b + w a is the preactivation value?

On the experimental side, it has been shown that linearly constrained weights can mitigate the impact of angle bias on vanishing gradient and can reduce the training error, but the test error is unfortunately increased for the particular task with the particular dataset in the experiments.  It would be desirable to identify specific tasks and datasets for which the proposed approach outperforms baselines.  It is intuitively expected that the proposed approach has some merit in some domains, but it is unclear exactly when and where it is.

Minor comments:

In Section 2.2, is Layer 1 the input layer or the next?","[5, 5, 4]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Ok but not good enough - rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is mostly positive in its tone, acknowledging the interesting phenomenon presented and the potential of the research. However, it also points out significant limitations in both the theoretical grounding and experimental validation. The reviewer suggests alternative approaches and highlights the need for further investigation to establish the method's effectiveness. The language is constructive and professional throughout, suggesting areas for improvement without resorting to harsh criticism.",40.0,80.0
Unsupervised Deep Structure Learning by Recursive Dependency Analysis,"['Raanan Y. Yehezkel Rohekar', 'Guy Koren', 'Shami Nisimov', 'Gal Novik']",Reject,2018,"[14, 1, 1, 1]","[18, 5, 5, 5]","[15, 5, 11, 18]","[6, 2, 4, 6]","[7, 3, 7, 12]","[2, 0, 0, 0]","The paper proposes an unsupervised structure learning method for deep neural networks. It first constructs a fully visible DAG by learning from data, and decomposes variables into autonomous sets. Then latent variables are introduced and stochastic inverse is generated. Later a deep neural network structure is constructed based on the discriminative graph. Both the problem considered in the paper and the proposed method look interesting. The resulting structure seems nice.

However, the reviewer indeed finds a major technical flaw in the paper. The foundation of the proposed method is on preserving the conditional dependencies in graph G. And each step mentioned in the paper, as it claims, can preserve all the conditional dependencies. However, in section 2.2, it seems that the stochastic inverse cannot. In Fig. 3(b), A and B are no longer dependent conditioned on {C,D,E} due to the v-structure induced in node H_A and H_B. Also in Fig. 3(c), if the reviewer understands correctly, the bidirectional edge between H_A and H_B is equivalent to H_A <- h -> H_B, which also induces a v-structure, blocking the dependency between A and B. Therefore, the very foundation of the proposed method is shattered. And the reviewer requests an explicit explanation of this issue.

Besides that, the reviewer also finds unfair comparisons in the experiments.

1. In section 5.1, although the authors show that the learned structure achieves 99.04%-99.07% compared with 98.4%-98.75% for fully connected layers, the comparisons are made by keeping the number of parameters similar in both cases. The comparisons are reasonable but not very convincing. Observing that the learned structures would be much sparser than the fully connected ones, it means that the number of neurons in the fully connected network is significantly smaller. Did the authors compare with fully connected network with similar number of neurons? In such case, which one is better? (Having fewer parameters is a plus, but in terms of accuracy the number of neurons really matters for fair comparison. In practice, we definitely would not use that small number of neurons in fully connected layers.)

2. In section 5.2, it is interesting to observe that using features from conv10 is better than that from last dense layer. But it is not a fair comparison with vanilla network. In vanilla VGG-16-D, there are 3 more conv layers and 3 more fully connected layers. If you find that taking features from conv10 is good for the learned structure, then maybe it will also be good by taking features from conv10 and then apply 2-3 fully-connected layers directly (The proposed structure learning is not comparable to convolutional layers, and what it should really compare to is fully-connected layers.) In such case, which one is better? 
Secondly, VGG-16 is a large network designed for ImageNet data. For small dataset such as CIFAR10 and CIFAR100, it is really overkilled. That's maybe the reason why taking the output of shallow layers could achieve pretty good results.

3. In Fig. 6, again, comparing the learned structure with fully-connected network by keeping parameters to be similar and resulting in large difference of the number of neurons is unfair from my point of view.

Furthermore, all the comparisons are made with respect to fully-connected network or vanilla CNNs. No other structure learning methods are compared with. Reasonable baseline methods should be included.

In conclusion, due to the above issues both in method and experiments, the reviewer thinks that this paper is not ready for publication.
","[4, 5, 5]","[' Ok but not good enough - rejection', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[4, 2, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer starts with positive remarks, showing interest in the problem and approach, but then raises a major technical flaw that ""shatters the very foundation"" of the paper. This, combined with concerns about unfair comparisons in the experiments, makes the overall sentiment negative. While the reviewer points out significant issues, the language remains professional and not personally attacking, indicating a neutral politeness.",-50.0,0.0
Revisiting Bayes by Backprop,"['Meire Fortunato', 'Charles Blundell', 'Oriol Vinyals']",Reject,2018,"[4, 9, 12]","[8, 14, 17]","[14, 88, 209]","[5, 35, 101]","[8, 48, 98]","[1, 5, 10]","*Summary*

The paper applies variational inference (VI) with the 'reparameterisation' trick for Bayesian recurrent neural networks (BRNNs). The paper first considers the ""Bayes by Backprop"" approach of Blundell et al. (2015) and then modifies the BRNN model with a hierarchical prior over the network parameters, which then requires a hierarchical variational approximation with a simple linear recognition model. Several experiments demonstrate the quality of the prediction and the uncertainty over dropout.  

*Originality + significance*

To my knowledge, there is no other previous work on VI with the reparameterisation trick for BRNNs. However, one could say that this paper is, on careful examination, an application of reparameterisation gradient VI for a specific application. 

Nevertheless, the parameterisation of the conditional variational distribution q(\theta | \phi, (x, y)) using recognition model is interesting and could be useful in other models. However, this has not been tested or concretely shown in this paper. The idea of modifying the model by introducing variables to obtain a looser bound which can accommodate a richer variational family is also not new, see: hierarchical variational model (Ranganath et al., 2016) for example. 

*Clarity*

The paper is, in general, well-written. However, the presentation in 4 is hard to follow. I would prefer if appendix A3 was moved up front -- in this case, it would make it clear that the model is modified to contain \phi, a variational approximation over both \theta and \phi is needed, and a q that couples \theta, \phi and and the gradient of the log likelihood term wrt \phi is chosen. 

Additional comments:

Why is the variational approximation called ""sharpened""?

At test time, normal VI just uses the fixed q(\theta) after training. It's not clear to me how prediction is done when using 'posterior sharpening' -- how is q(\theta | \phi, x) in eqs. 19-20 parameterised? The first paragraph of page 5 uses q(\theta | \phi, (x, y)), but y is not known at test time.

What is C in eq. 9?

This comment ""variational typically underestimate the uncertainty in the posterior...whereas expectation propagation methods are mode averaging and so tend to overestimate uncertainty..."" is not precise. EP can do mode averaging as well as mode seeking, depending on the underlying and approximate factor graphs. In the Bayesian neural network setting when the likelihood is factorised point-wise and there is one factor for each likelihood, EP is just as mode-seeking as variational. On the other hand, variational methods can avoid modes too, see the mixture of Gaussians example in the ""Two problems with variational EM... "" paper by Turner and Sahani (2010).

There are also many hyperparameters that need to be chosen -- what would happen if these are optimised using the free-energy? Was there any KL reweighting scheduling as done in the original BBB paper? 

What is the significance of the difference between BBB and BBB with sharpening in the language modelling task? Was sharpening used in the image caption generation task?

What is the computational complexity of BBB with posterior sharpening? Twice that BBB? If this is the case, would BBB get to the same performance if we optimise it for longer? Would be interesting to see the time/accuracy frontier.","[5, 6, 6]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review is lukewarm positive. The reviewer acknowledges the originality of the paper and finds merit in its contributions, but also points out limitations and areas for improvement. The reviewer does not express strong excitement or enthusiasm for the work, but rather presents a balanced and measured assessment. The language used is polite and professional throughout the review. The reviewer provides constructive criticism and suggestions for improvement, maintaining a respectful and objective tone.",30.0,70.0
Continuous-fidelity Bayesian Optimization with Knowledge Gradient,"['Jian Wu', 'Peter I. Frazier']",Reject,2018,"[33, 12]","[38, 17]","[374, 110]","[181, 48]","[36, 37]","[157, 25]","This paper studies hyperparameter-optimization by Bayesian optimization, using the Knowledge Gradient framework and allowing the Bayesian optimizer to tune fideltiy against cost.

There’s nothing majorly wrong with this paper, but there’s also not much that is exciting about it. As the authors point out very clearly in Table 1, this setting has been addressed by several previous groups of authors. This paper does tick a previously unoccupied box in the problem-type-vs-algorithm matrix, but all the necessary steps are relatively straightforward.

The empirical results look good in comparison to the competing methods, but I suspsect an author of those competitors could find a way to make their own method look better in those plots, too.

In short: This is a neat paper, but it’s novelty is low. I don't think it would be a problem if this paper were accepted, but there are probably other, more groundbreaking papers in the batch.

Minor question: Why are there no results for 8-cfKG and Hyperband in Figure 2 for SVHN?","[5, 4, 6]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Marginally above acceptance threshold']","[4, 5, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The reviewer finds the paper ""neat"" and acknowledges its correctness (""nothing majorly wrong""). However, they also state the paper lacks excitement and novelty, with its contributions being relatively straightforward. The comparison to other work is acknowledged as potentially biased. Overall, this points to a lukewarm sentiment, leaning slightly towards the positive due to the lack of major criticism and acknowledgement of the paper's merit despite its limitations.",20.0,70.0
Preliminary theoretical troubleshooting in Variational Autoencoder,"['Shiqi Liu', 'Qian Zhao', 'Xiangyong Cao', 'Deyu Meng', 'Zilu Ma', 'Tao Yu']",Reject,2018,"[2, 7, 4, 14, 2, 'no_match']","[5, 12, 9, 19, 6, 'no_match']","[8, 101, 46, 340, 10, 'no match']","[0, 24, 5, 88, 0, 'no match']","[5, 38, 15, 111, 2, 'no match']","[3, 39, 26, 141, 8, 'no match']","This paper attempts to improve the beta-VAE (Higgins et al, 2017) by removing the trade-off between the quality of disentanglement in the latent representation and the quality of the reconstruction. The authors suggest doing so by explicitly modelling the noise of the reconstructed image Gaussian p(x|z). The authors assume that VAEs typically model the data using a Guassian distribution with a fixed noise. This, however, is not the case. Since the authors are trying to address a problem that does not actually exist, I am not sure what the contributions of the paper are. 

Apart from the major issue outlined above, the paper also makes other errors. For example, it suggests using D_KL(q(z)||p(z)) as a measure of disentanglement, with lower values being indicative of better disentanglement. This, however, is incorrect, since one can have tiny D_KL by encoding all the information into a single latent z_i. Such a representation would be highly entangled while still satisfying all of the conditions the authors propose for a disentangled representation. 

Given the points outlined above and the fact that the paper is hard to read and is excessively long, I do not believe it should be accepted.","[2, 5, 3]","[' Strong rejection', ' Marginally below acceptance threshold', ' Clear rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is extremely negative about the paper, stating a core assumption of the paper is flawed,  criticizing the methodology, clarity, and length. It directly recommends against acceptance. This points to a very negative sentiment.  While the reviewer uses harsh language like ""major issue"", ""incorrect"", and ""excessively long"", it doesn't contain personal attacks or disrespectful language. Therefore, the politeness is low but not extremely rude.",-80.0,-50.0
Learning Generative Models with Locally Disentangled Latent Factors,"['Brady Neal', 'Alex Lamb', 'Sherjil Ozair', 'Devon Hjelm', 'Aaron Courville', 'Yoshua Bengio', 'Ioannis Mitliagkas']",Reject,2018,"[1, 7, 5, 5, 18, 31, 9]","[5, 12, 9, 9, 23, 36, 14]","[7, 64, 27, 71, 309, 975, 78]","[2, 22, 13, 31, 135, 405, 30]","[5, 38, 13, 37, 160, 454, 46]","[0, 4, 1, 3, 14, 116, 2]","The paper investigates the potential of hierarchical latent variable models for generating images and image sequences. The paper relies on the ALI model from [Dumoulin et al, ICLR'16] as the main building block. The main innovation in the paper is to propose to train several ALI models stacked on top of each other to create a hierarchical representation of the data. The proposed hierarchical model is trained in stages. First stage is an original ALI model as in [Dumoulin et al]. Each subsequent stage is constructed by using the Z variables from the previous stage as the target data to be generated.

The paper constructs models for generatation of images and image sequences. The model for images is a 2-level ALI. The first level is similar to PatchGAN from [1] but is trained as an ALI model. The second layer is another ALI that is trained to generate latent variables from the first layer. 

[1] Isola et al. Image-to-Image Translation with Conditional Adversarial Networks, CVPR'17 

In the the model for image sequences the hierarchy is somewhat different. The top layer is directly generating images and not patches as in the image-generating model.

Summary: I think this paper presents a direct and somewhat straightforward extension of ALI. Therefore the novelty is limited. I think the paper would be stronger if it (1) demonstrated improvements when compared to ALI and (2) showed advantages of hierarchical training on other datasets, not just the somewhat simple datasets like CIFAR and Pacman. 

Other comments / questions: 

- baseline should probably be 1-level ALI from [Dumoulin et al.]. I believe in the moment the baseline is a standard GAN.

- I think the paper would be stronger if it directly reproduced the experiments from [Dumoulin et al.] and showed how hierarchy compares to standard ALI without hierarchy. 

- the reference Isola et al. [1] should ideally be cited since the model for image genration is similar to PatchGAN in [1]

- Why is the video model in this paper not directly extending the image model? Is it due to limitation of the implementation or direclty extending the iamge model didn't work? 
","[6, 3, 4]","[' Marginally above acceptance threshold', ' Clear rejection', ' Ok but not good enough - rejection']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer acknowledges the paper as a ""direct and somewhat straightforward extension of ALI"", which indicates a somewhat positive view of the work done. However, they also point out limitations in novelty and suggest several improvements. The suggestions for improvement and the directness in pointing out the limitations suggest the sentiment is not overly positive. Overall, the tone is neutral, leaning slightly towards positive.",20.0,70.0
Hybed: Hyperbolic Neural Graph Embedding,"['Benjamin Paul Chamberlain', 'James R Clough', 'Marc Peter Deisenroth']",Reject,2018,"[3, 6, 13]","[7, 11, 18]","[46, 47, 131]","[19, 20, 55]","[26, 20, 57]","[1, 7, 19]","== Preamble ==

As promised, I have read the updated paper from scratch and this is my revised review. My original review is kept below for reference. My original review had rating ""4: Ok but not good enough - rejection"".

== Updated review ==

The revised improves upon the original submission in several ways and, in particular, does a much better job at positioning itself within the existing body of literature. The new experiments also indicate that the proposed model offer some improvement over Nickel & Douwe, NIPS 2017).

I do have remaining concerns that unfortunately still prevent me from recommending acceptance:

- Throughout the paper it is argued that we should embed into a hyperbolic space. Such a space is characterized by its metric, but the proposed model do not use a hyperbolic metric. Rather it relies on a heuristic similarity measure that is inspired by the hyperbolic metric. I understand that this may be a practical choice, but then I find it misleading that the paper repeatedly states that points are embedded into a hyperbolic space (which is incorrect). This concern was also raised on this forum prior to the revision.

- The resulting optimization is one of the key selling points of the proposed method as it is unconstrained while Nickel & Douwe resort to a constrained optimization. Clearly unconstrained optimization is to be preferred. However, it is not entirely correct (from what I understand), that the resulting optimization is indeed unconstrained. Nickel & Douwe work under the constraint that |x| < 1, while the proposed model use polar coordinates (r, theta): r in (0, infinity) and theta in (0, 2 pi]. Note that theta parametrize a circle, and therefore wrapping may occur (this should really be mentioned in the paper). The constraints on theta are quite easy to cope with, so I agree with the authors that they have a more simple optimization problem. However, this is only true since points are embedded on the unit disk (2D). Should you want to embed into higher dimensional spaces, then theta need to be confined to live on the unit sphere, i.e. |theta| = 1 (the current setting is just a special-case of the unit sphere). While optimizing over the unit sphere is manageable it is most definitely a constrained optimization problem, and it is far from clear that it is much easier than working under the Nickel & Douwe constraint, |x| < 1.

Other comments:
- The sentence ""even infinite trees have nearly isometric embeddings in hyperbolic space (Gromov, 2007)"" sounds cool (I mean, we all want to cite Gromov), but what does it really mean? An isometric embedding is merely one that preserves a metric, so this statement only makes sense if the space of infinite trees had a single meaningful metric in the first place (it doesn't; that's a design choice).

- In the ""Contribution"" and ""Conclusion"" sections it is claimed that the paper ""introduce the new concept of neural embeddings in hyperbolic space"". I thought that was what Nickel & Douwe did... I understand that the authors are frustrated by this parallel work, but at this stage, I don't think the present paper can make this ""introducing"" claim.

- The caption in Figure 2 miss some indication that ""a"" and ""b"" refer to subfigures. I recommend ""a"" --> ""a)"" and ""b"" --> ""b)"".

- On page 4 it is mentioned that under the heuristic similarity measure some properties of hyperbolic spaces are lost while other are retained. From what I can read, it is only claimed that key properties are kept; a more formal argument (even if trivial) would have been helpful.


== Original Review ==

The paper considers embeddings of graph-structured data onto the hyperbolic Poincare ball. Focus is on word2vec style models but with hyperbolic embeddings. I am unable to determine how suitable an embedding space the Poincare ball really is, since I am not familiar enough with the type of data studied in the paper. I have a few minor comments/questions to the work, but my main concern is a seeming lack of novelty:
The paper argues that the main contribution is that this is the first neural embedding onto a hyperbolic space. From what I can see, the paper

  Poincaré Embeddings for Learning Hierarchical Representations
  https://arxiv.org/abs/1705.08039

consider an almost identical model to the one proposed here with an almost identical motivation and application set. Some technicalities appear different, but (to me) it seems like the main claimed novelties of the present paper has already been out for a while. If this analysis is incorrect, then I encourage the authors to provide very explicit arguments for this in the rebuttal phase.

Other comments:
*) It seems to me that, by construction, most data will be pushed towards the boundary of the Poincare ball during the embedding. Is that a property you want?
*) I found it rather surprising that the log-likelihood under consideration was pushed to an appendix of the paper, while its various derivatives are part of the main text. Given the not-so-tight page limits of ICLR, I'd recommend to provide the log-likelihood as part of the main text (it's rather difficult to evaluate the correctness of a derivative when its base function is not stated).
*) In the introduction must energy is used on the importance of large data sets, but it appears that only fairly small-scale experiments are considered. I'd recommend a better synchronization.
*) I find visual comparisons difficult on the Poincare ball as I am so trained at assuming Euclidean distances when making visual comparisons (I suspect most readers are as well). I think one needs to be very careful when making visual comparisons under non-trivial metrics.
*) In the final experiment, a logistic regressor is fitted post hoc to the embedded points. Why not directly optimize a hyperbolic classifier?

Pros:
+ well-written and (fairly) well-motivated.

Cons:
- It appears that novelty is very limited as highly similar work (see above) has been out for a while.

","[4, 4, 5, 7]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold', ' Good paper, accept']","[3, 3, 3, 2]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']","The review is mixed, leaning towards negative because the reviewer still has concerns preventing them from recommending acceptance. While acknowledging improvements from a previous version, the reviewer raises serious technical issues, questions novelty, and finds the core argument about hyperbolic space misleading. The language is direct and critical, pointing out flaws and demanding better justification, but maintains a professional tone without resorting to personal attacks.",-25.0,60.0
Model-based imitation learning from state trajectories,"['Subhajit Chaudhury', 'Daiki Kimura', 'Tadanobu Inoue', 'Ryuki Tachibana']",Reject,2018,"[5, 6, 24, 18]","[10, 10, 27, 21]","[46, 34, 20, 60]","[19, 20, 11, 37]","[22, 14, 7, 19]","[5, 0, 2, 4]","The paper presents a model-based imitation learning framework which learns the state transition distribution of the expert. A model-based policy is learned that should matches the expert transition dynamics. The approach can be used for imitation learning when the actions of the expert are not observed, but only the state transitions (which is an important special case).  

Pros:
- The paper concentrates on an interesting special case of imitation learning

Cons:
- The paper is written very confusingly and hard to understand. The algorithm needs to be better motivated and explained and the paper needs proof reading.
- The algorithm is based on many heuristics that are not well motivated. 
- The algorithm is only optimizing the one step error function for imitation learning but not the long term behavior. It heavily relies on the learned transition dynamics of the expert p(s_t+1|s_t). This transition model will be wrong if we go away from the expert's trajectories. Hence, I do not see why we should use p(s_t+1|s_t) to define the reward function. It does not prevent the single step 
errors of the policy to accumulate (which is the main goal of inverse reinforcement learning)
- The results are not convincing
- Other algorithms (such as GAIL) could be used in the same setup (no action observations). Comparisons to other imitation learning approaches are needed.

In summary, this is a poorly written paper that seems to rely on a lot of heuristics that are not well motivated. Also the results are not convincing. Clear reject.


More detailed comments
- It is unclear why a model-based and model-free policy need to be used. Is the model-based policy used at any time in the algorithm? If it is just used as final result, why train it iteratively? Why can we not just also use the model-based policy for data collection?
- It is unclear why the heuristic reward function makes sense. First of all, the defined reward is stochastic as \hat{s}_t+1 is a sample from the next state from the expert's transition model. Why do not we use the mean of the transition model here, then it would not be stochastic any more. Second, a much simpler reward could be used that essentially does the same thing. Instead of requiring a learned dynamics model f_E for predicting the next state, we can just use the experienced next state s_t+1. Note that the reward function for time step t can depend on s_t+1 in an MDP.  
- The objective that is optimized (Eq. 4) is not well defined. A function is not an objective function if we can only optimize part of it for theta while keeping theta fixed for the other part. It is unclear which objective the real algorithm optimizes
- There are quite a few confusions in terms of notation. Sometimes, a stochastic transition model p(s_t+1|s_t, a_t) is used and sometimes a deterministic model f_E(s,a). It is unclear how they relate. 
- Many other imitation learning techniques could be used in this setup including max-entropy inverse RL [1], IRL by distribution matching [2] and the approach given in [3] and GAIL. A comparison to at least a subset of these methods is needed

[1] B. Ziebart et al, Maximum Entropy Inverse Reinforcement Learning, AAAI 2008
[2] Arenz, O.; Abdulsamad, H.; Neumann, G. (2016). Optimal Control and Inverse Optimal Control by Distribution Matching, Proceedings of the International Conference on Intelligent Robots and Systems (IROS)
[3] P Englert, A Paraschos, J Peters, MP Deisenroth, Model-based Imitation Learning by Probabilistic Trajectory Matching, IEEE International Conference on Robotics and Automation","[3, 4, 7]","[' Clear rejection', ' Ok but not good enough - rejection', ' Good paper, accept']","[5, 4, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review is highly critical of the paper, pointing out significant flaws in clarity, methodology, and results. The reviewer uses strong language like ""confusingly,"" ""not well motivated,"" ""not convincing,"" and ""clear reject."" This indicates a negative sentiment. However, the criticism is presented in a professional and analytical manner, focusing on the paper's shortcomings rather than resorting to personal attacks.",-80.0,60.0
Training Neural Machines with Partial Traces,"['Matthew Mirman', 'Dimitar Dimitrov', 'Pavle Djordjevich', 'Timon Gehr', 'Martin Vechev']",Reject,2018,"[1, 'no_match', 'no_match', 4, 16]","[5, 'no_match', 'no_match', 8, 21]","[12, 'no match', 'no match', 22, 204]","[6, 'no match', 'no match', 19, 137]","[5, 'no match', 'no match', 2, 51]","[1, 'no match', 'no match', 1, 16]","Summary

This paper presents differentiable Neural Computational Machines (∂NCM), an abstraction of existing neural abstract machines such as Neural Turing Machines (NTMs) and Neural Random Access Machines (NRAMs). Using this abstraction, the paper proposes loss terms for incorporating supervision on execution traces. Adding supervision on execution traces in ∂NCM improves performance over NTM and NRAM which are trained end-to-end from input/output examples only. The observation that adding additional forms of supervision through execution traces improves generalization may be unsurprising, but from what I understand the main contribution of this paper lies in the abstraction of existing neural abstract machines to ∂NCM. However, this abstraction does not seem to be particularly useful for defining additional losses based on trace information. Despite the generic subtrace loss (Eq 8), there is no shared interface between ∂NCM versions of NTM and NRAM that would allow one to reuse the same subtrace loss in both cases. The different subtrace losses used for NTM and NRAM (Eq 9-11) require detailed knowledge of the underlying components of NTM and NRAM (write vector, tape, register etc.), which questions the value of ∂NCM as an abstraction.

Weaknesses

As explained in the summary, it is not clear to me why the abstraction to NCM is useful if one still needs to define specific subtrace losses for different neural abstract machines.
The approach seems to be very susceptible to the weight of the subtrace loss λ, at least when training NTMs. In my understanding each of the trace supervision information (hints, e.g. the ones listed in Appendix F) provides a sensible inductive bias we would the NTM to incorporate. Are there instances where these biases are noisy, and if not, could we incorporate all of them at the same time despite the susceptibility w.r.t λ?
NTMs and other recent neural abstract machines are often tested on rather toyish algorithmic tasks. I have the impression providing extra supervision in form of execution traces makes these tasks even more toyish. For instance, when providing input-output examples as well as the auxiliary loss in Eq6, what exactly is left to learn? What I like about Neural-Programmer Interpreters and Neural Programmer [1] is that they are tested on less toyish tasks (a computer vision and a question answering task respectively), and I believe the presented method would be more convincing for a more realistic downstream task where hints are noisy (as mentioned on page 5).

Minor Comments

p1: Why is Grefenstette et al. (2015) an extension of NTMs or NRAMs? While they took inspiration from NTMs, their Neural Stack has not much resemblance with this architecture.
p2: What is B exactly? It would be good to give a concrete example at this point. I have the feeling it might even be better to explain NCMs in terms of the communication between κ, π and M first, so starting with what I, O, C, B, Q are before explaining what κ and π are (this is done well for NTM as ∂NCM in the table on page 4). In addition, I think it might be better to explain the Controller before the Processor. Furthermore, Figure 2a should be referenced in the text here.
p4 Eq3: There are two things confusing in these equations. First, w is used as the write vector here, whereas on page 3 this is a weight of the neural network. Secondly, π and κ are defined on page 2 as having an element from W as first argument, which are suddenly omitted on page 4.
p4: The table for NRAM as ∂NCM needs a bit more explanation. Where does {1}=I come from? This is not obvious from Appendix B either.
p3 Fig2/p4 Eq4: Related to the concern regarding the usefulness of the ∂NCM abstraction: While I see how NTMs fit into the NCM abstraction, this is not obvious at all for NRAMs, particularly since in Fig 2c modules are introduced that do not follow the color scheme of κ and π in Fig 2a (ct, at, bt and the registers).
p5: There is related work for incorporating trace supervision into a neural abstract machine that is otherwise trained end-to-end from input-output examples [2].
p5: ""loss on example of difficulties"" -> ""loss on examples of the same difficulty""
p5: Do you have an example for a task and hints from a noisy source?
Citation style: sometimes citation should be in brackets, for example ""(Graves et al. 2016)"" instead of ""Graves et al. (2016)"" in the first paragraph of the introduction.

[1] Neelakantan et al. Neural programmer: Inducing latent programs with gradient descent. ICLR. 2015. 
[2] Bosnjak et al. Programming with a Differentiable Forth Interpreter. ICML. 2017.","[5, 4, 4]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[4, 5, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']","The review is critical of the paper's core contribution (the ∂NCM abstraction) and questions its usefulness. While acknowledging the potential of trace supervision, the reviewer finds its application in the paper limited and the experimental settings somewhat unrealistic. The reviewer maintains a professional and constructive tone throughout, providing specific examples and suggestions for improvement. Therefore, the sentiment leans towards the negative side, but the politeness remains high.",-30.0,80.0
Using Deep Reinforcement Learning to Generate Rationales for Molecules,"['Benson Chen', 'Connor Coley', 'Regina Barzilay', 'Tommi Jaakkola']",Reject,2018,"[0, 2, 21, 26]","[5, 7, 26, 31]","[6, 67, 247, 294]","[0, 12, 130, 149]","[5, 34, 89, 112]","[1, 21, 28, 33]","In this manuscript, the authors propose an interesting deep reinforcement learning approach via CNNs to learn the rationales associated to target chemical properties. The paper has merit, but in its current form does not match the acceptance criteria for ICLR.

In particular, the main issue lies in the poor performance reached by the systems, both overall and in comparison with baseline methods, which at the moment hardly justifies the effort required in setting up the DL framework. Moreover, the fact that test performances are sometimes (much) better than training results are quite suspicious in methodological terms.
Finally, the experimental part is quite limited (two small datasets), making it hard to evaluate the scalability (in all sense) of the proposed solution to much larger data. ","[5, 5, 5]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with a positive note by acknowledging the merit and interestingness of the proposed approach. However, it then expresses significant concerns about the poor performance and limited experimental validation, which ultimately leads to a negative sentiment. The language used, while direct, maintains a professional and respectful tone, suggesting areas for improvement without resorting to harsh criticism.",-30.0,60.0
Directing Generative Networks with Weighted Maximum Mean Discrepancy,"['Maurice Diesendruck', 'Guy W. Cole', 'Sinead Williamson']",Reject,2018,"[1, 1, 11]","[5, 2, 16]","[3, 4, 43]","[1, 1, 17]","[2, 3, 16]","[0, 0, 10]","This paper addresses the problem of sample selection bias in MMD-GANs. Instead of having access to an i.i.d. sample from the  distribution of interest, it is assumed that the dataset is subject to sample selection bias or the data has been gathered via a biased sample selection mechanism. Specifically, the observed data are drawn from the modified distribution T(x)P(x) where P(x) is the true distribution we aim to estimate and T(x) is an appropriately scaled ""thinning function"". Then, the authors proposed an estimate of the MMD between two distributions using weighted maximum mean discrepancy (MMD). The idea is in fact similar to an inverse probability weighting (IPW). They considered both when T(x) is known and when T(x) is unknown and must be estimated from the data. The proposed method was evaluated using both synthetic and real MNIST dataset. 

In brief, sample selection bias is generally a challenging problem in science, statistics, and machine learning, so the topic of this paper is interesting. Nevertheless, the motivation for investigating this problem specifically in MMD-GANs is not clear. What motivated you to study this problem specifically for GAN in the first place? How does solving this problem help us understand or solve the sample selection bias in general? Will it shed light on how to improve the stability of GAN? Also, the experiment results are too weak to make any justified conclusion.

Some comments and questions:

- How is sample selection bias related to the stability issue of training GAN? Does it worsen the stability?
- Have estimators in Eq. (2) and Eq. (3) been studied before? Are there any theoretical guarantees that this estimate will convergence to the true MMD? 
- On page 5, why T(men) = 1 and T(women) equals to the sample ratio of men to women in labeled subset?
- Can we use clustering to estimate the thinning function?","[4, 4, 4]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review acknowledges the importance of the paper's topic (sample selection bias) and finds the approach interesting. However, it raises significant concerns about the paper's motivation, clarity, and experimental validation. The reviewer asks several pointed questions indicating a lack of conviction in the paper's claims and methodology. The tone, while direct, remains professional and inquisitive rather than dismissive. ",20.0,60.0
Faster Reinforcement Learning with Expert State Sequences,"['Xiaoxiao Guo', 'Shiyu Chang', 'Mo Yu', 'Miao Liu', 'Gerald Tesauro']",Reject,2018,"[6, 8, 10, 8, 32]","[11, 13, 15, 13, 37]","[86, 212, 187, 50, 118]","[39, 106, 86, 24, 67]","[38, 95, 93, 23, 28]","[9, 11, 8, 3, 23]","The authors propose to speed up RL techniques, such as DQN, by utilizing expert demonstrations. The  expert demonstrations are sequences of consecutive states that do not include actions, which is closer to a real setting of imitation learning. The goal of this process is to extract a function that maps any given state to a subgoal. Subgoals are then used to learn different Q-value functions, one per subgoal. 
To learn the function that maps states into subgoals, the authors propose a surrogate reward model that corresponds to the angle between: the difference between two consecutive states (which captures velocity or direction) and a given subgoal. A von Mises- Fisher distribution policy is then assumed to be used by the expert to generate actions that guide the agent toward the subgoal. Finally, the mapping function state->subgoal is learned by performing a gradient descent on the expected total cost (based on the surrogate reward function, which also has free parameters that need to be learned).
Finally, the authors use the DQN platform to learn a Q-value function using the learned  surrogate reward function that guides the agent to specific subgoals, depending on the situation.
The paper is overall well-written, and the proposed idea seems interesting. However, there are rather little explanations provided to argue for the different modeling choices made, and the intuition behind them. From my understanding, the idea of subgoal learning boils down to a non-parametric (or kernel) regression where each state is mapped to a subgoal based on its closeness to different states in the expert's demonstration. It is not clear how this method would generalize to new situations. There is also the issue of keeping tracking of a large number of demonstration states in memory. This technique reminds me of some common methods in learning from demonstrations, such as those using GPs or GMMs, but the novelty of this technique is the fact that the subgoal mapping function is learned in an IRL fashion, by tacking into account the sum of surrogate rewards in the expert's demonstration. 
The architecture of the action value estimator does not seem novel, it's basically just an extension of DQN with an extra parameter (subgoal g).
The empirical evaluation seems rather mixed. Figure 3 shows that the proposed method learns faster than DQN, but Table I shows that the improvement is not statistically significant, except in two games, DefendCenter and PredictPosition. Are these the results after all agents had converged? 
Overall, this is a good paper, but focusing on only a single game (Doom) is a weakness that needs to be addressed because one cannot tell if the choices were tailored to make the method work well for this game. Since the paper does not provide significant theoretical or algorithmic contribution, at least more realistic and diverse experiments should be performed. ","[6, 5, 6]","[' Marginally above acceptance threshold', ' Marginally below acceptance threshold', ' Marginally above acceptance threshold']","[4, 5, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer finds the paper interesting and well-written, indicating a positive sentiment. However, they also express concerns about the clarity of explanations, novelty, and limited empirical evaluation. The reviewer acknowledges the paper's merits but suggests improvements, making the overall sentiment lean towards neutral positive. The language used is polite and professional, employing constructive criticism and avoiding harsh or disrespectful language.",60.0,80.0
Structured Deep Factorization Machine: Towards General-Purpose Architectures,"['José P. González-Brenes', 'Ralph Edezhath']",Reject,2018,"[9, 1]","[10, 2]","[20, 2]","[19, 2]","[0, 0]","[1, 0]","This paper proposes to improve time complexity of factorization machine. Unfortunately, the paper's claim that FM's time complexity is quadratic to feature size is wrong. Specifically, the dot product can be computed as (which is linear to feature size)

(\sum x_i \beta_i)^T (\sum x_i \beta_i) - \sum_i x_i^2 beta_i^T beta_i

The projection of feature group into one embedded space proposed in the paper can be viewed as another form of representing the same model when group equals one. When the number of feature groups do not equal one, they correspond to field aware factorization machine(FFM)","[3, 4, 4]","[' Clear rejection', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[5, 5, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']","The review is negative because it points out a fundamental flaw in the paper's premise, stating that the paper's claim about FM's time complexity is incorrect. The reviewer then goes on to explain how the proposed method is either equivalent to existing methods or corresponds to a different model (FFM). Despite the critical feedback, the language used is neutral and professional, avoiding harsh or disrespectful language.",-50.0,50.0
A novel method to determine the number of latent dimensions with SVD,"['Asana Neishabouri', 'Michel Desmarais']",Reject,2018,"[4, 32]","[7, 37]","[4, 96]","[4, 72]","[0, 4]","[0, 20]","The manuscript proposes to estimate the number of components in SVD by comparing the eigenvalues to those obtained on bootstrapped version of the input.

The paper has numerous flaws and is clearly below acceptance threshold for any scientific forum. Some of the more obvious issues, each alone sufficient for rejection, include:

1. Discrepancy between motivation and actual work. The method is specifically about determining the rank of a matrix, but the authors motivate it with way too general and vague relationships, such as ""determining the number of nodes in neural networks"". Somewhat oddly, the problem is highlighted to be of interest in supervised problems even though one would expect it to be much more important in unsupervised ones.

2. Complete lack of details for related work. Methods such as PA and MAP are described with vague one-sentences summaries that tell nothing about how they actually work. There would have been ample space to provide the mathematical formulations.

3. No technical contribution. The proposed method is trivial variant of randomised testing, described with single sentence ""Bootstrapped samples R_B are simply generated through random sampling with replacement of the values of R."" with literally no attempt of providing any sort of justification why this kind of random sampling would be good for the proposed task or what kind of assumptions it builds on.

4. Poor experiments using really tiny artificial data sets, reported in unprofessional manner (visual style in plots changes from figure to figure, tables report irrelevant numbers in hard-to-read format etc). No real improvement over the somewhat random choice of comparison methods that do not even represent the techniques people would typically use for this problem.","[1, 3, 2]","[' Trivial or wrong', ' Clear rejection', ' Strong rejection']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review is extremely negative about the manuscript, using very strong language to express its flaws. It calls the paper ""clearly below acceptance threshold"" and lists major issues that are ""each alone sufficient for rejection."" The reviewer doesn't attempt to soften the criticism, making it quite harsh.",-90.0,-80.0
Bias-Variance Decomposition for Boltzmann Machines,"['Mahito Sugiyama', 'Koji Tsuda', 'Hiroyuki Nakahara']",Reject,2018,"[9, 24, 24]","[14, 29, 28]","[55, 126, 35]","[27, 59, 10]","[20, 15, 4]","[8, 52, 21]","Summary of the paper:
The paper derives a lower bound on the expected  squared KL-divergence between a true distribution and the sample based maximum likelihood estimate (MLE) of that distribution modelled by an Boltzmann machine (BM) based on methods from information geometry. This  KL-divergence is first split into the squared KL-divergence between the true distribution and MLE of that distribution,  and the expected squared KL-divergence between the MLE of the true distribution and the sample based MLE (in a similar spirit to splitting the excess error into approximation and estimation error in statistical learning theory). The letter is than lower bounded (leading to a lower bound on the overall KL-divergence) by a term  which does not necessarily increase if the number of model parameters is increased. 


Pros:
- Using insights from information geometry  opens up a very interesting and (to my knowledge) new approach for analysing the generalisation ability of ML models.
- I am not an expert on information geometry and I did not find the time to follow all the steps of the proof in detail, but the analysis seems to be correct.

Cons:
- The fact that the lower bound does not necessary increase with a growing number of parameters does not guarantee that the same holds true for the KL-divergence (in this sense an upper bound would be more informative). Therefore, it is not clear how much of insights the theoretical analysis gives for practitioners (it could be nice to analyse the tightness of the bound for toy models).
- Another drawback reading the practical impact is, that the theorem bounds the expected  squared KL-divergence between a true distribution and the sample based MLE, while training minimises the divergence between the empirical distribution and the model distribution ( i.e. the sample based MLE in the optimal case),  and the theorem does not show the dependency on the letter. 

I found some parts difficulty to understand and clarity could be improved  e.g. by
- explaining why minimising KL(\hat P, P_B) is equivalent to minimising the KL-divergence between the empirical distribution and the Gibbs distribution \Phi.
- explaining in which sense the formula on page 4 is equivalent to “the learning equation of Boltzmann machines”.
- explaining what is the MLE of the true distribution (I assume the closest distribution in the set of distributions that can be modelled by the BM).

Minor comments:
- page 1: and DBMs….(Hinton et al., 2006) : The paper describes deep belief networks (DBNs) not DBMs 
- \theta is used to describe the function in eq. (2) as well as the BM parameters in Section 2.2 
- page 5: “nodes H is” -> “nodes H are” 



REVISION:
Thanks to the reviewers for replying to my comments and making the changes. I think they improved the paper. On the other hand the other reviewers raised valid questions, that led to my decision to not change the overall rating of the paper.","[5, 7, 5]","[' Marginally below acceptance threshold', ' Good paper, accept', ' Marginally below acceptance threshold']","[2, 5, 5]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review is mostly positive. The reviewer finds the approach interesting and the analysis correct. However, they have concerns about the practical implications of the theoretical results and suggest improvements for clarity. The reviewer also notes that other reviewers raised valid questions, preventing a higher rating. Overall, the tone is constructive and polite.",50.0,75.0
A dynamic game approach to training robust deep policies,['Olalekan Ogunmolu'],Reject,2018,[-3],[1],[1],[0],[1],[0],"The paper presents a method for evaluating the sensitivity and robustness of deep RL policies, and proposes a dynamic game approach for learning robust policies.

The paper oversells the approach in many ways. The authors claim that ""experiments confirm that state-of-the-art reinforcement learning algorithms fail in the presence of additive disturbances, making them brittle when used in situations that call for robustness"". However, their methods and experiments are only applied to Guided Policy Search (GPS), which seems like a specialized RL algorithm. Conclusions drawn from empirically running GPS on a problem cannot be generalized to all ""state-of-the-art RL algorithms"".

In Fig 3, the authors conclude that ""our algorithm uses lesser number for the GMMs and requires fewer samples to generalize to the real-world"". I'm not sure how this can be concluded from Fig 3 [LEFT]. The two line graphs for different values of gamma almost overlay each other, and the cost seems to go up and down, even with number of samples on a log scale. If this shows the variance in the procedure, then the authors should run enough repeats of the experient to smooth out the variance and show the true signal (with error bars if possible). All related conclusions with regards to the dynamic game achieving higher sample efficiency for GMM dynamics fitting need to be backed up with better experimental data (or perhaps clearer presentation, if such data already exists).

Figures 2 and 3 talk about optimal adversarial costs. The precise mathematical definition of this term should be clarified somewhere, since there are several cost functions described in the paper, and it's unclear which terms are actually being plotted here.

The structure of the global policies used in the experiments should be mentioned somewhere.

Note about anonymity: Citation [21] breaks anonymity, since it's referred to in the text as ""our abstract"". The link to the YouTube video breaks author anonymity. Further, the link to a shared dropbox folder breaks reviewer anonymity, hence I have not watched those videos.","[3, 5, 5]","[' Clear rejection', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[3, 4, 2]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']","The review is critical of the paper's claims and methodology. While it doesn't use overtly aggressive language, the criticism is clear and pointed, indicating a negative sentiment. For example, stating that the paper ""oversells the approach in many ways"" and pointing out flaws in the conclusions drawn from the figures suggests a negative view of the work. However, the reviewer provides constructive feedback and specific recommendations for improvement, which balances the negativity. Therefore, the sentiment is somewhat negative but not overly harsh. The language is formal and professional throughout, indicating a neutral to polite tone.",-40.0,50.0
VOCABULARY-INFORMED VISUAL FEATURE AUGMENTATION FOR ONE-SHOT LEARNING,"['jianqi ma', 'hangyu lin', 'yinda zhang', 'yanwei fu', 'xiangyang xue']",Reject,2018,"[3, 3, 'no_match', 9, 20]","[8, 6, 'no_match', 14, 25]","[16, 12, 'no match', 257, 411]","[5, 5, 'no match', 104, 235]","[9, 3, 'no match', 118, 107]","[2, 4, 'no match', 35, 69]","This paper proposes a feature augmentation method for one-shot learning.  The proposed approach is very interesting. However, the method needs to be further clarified and the experiments need to be improved. 

Details:
1. The citation format used in the paper is not appropriate, which makes the paper, especially the related work section, very inconvenient to read. 

2. The approach:
(1) Based on the discussion in the related work section and the approach section, it seems the proposed approach proposes to augment each instance in the visual feature space by adding more features, as shown by [x_i; x_i^A] in 2.3.  However, under one-shot learning, won’t this  make each class still have only one instance for training? 

(2) Moreover, the augmenting features x_i^A (regardless A=F, G, or H), are in the same space as the original features x_i. Hence x_i^A is rather an augmenting instance than additional features. What makes feature augmentation better than instance augmentation? 

(3) It is not clear how will the vocabulary-information be exploited? In particular, how to ensure the semantic space u to be same as the vocabulary semantic space? How to generate the neighborhood in Neigh(\hat{u}_i) on page 5? 

3.  In the experiments: 
(1) The authors didn’t compare the proposed method with existing state-of-the-art one-shot learning approaches, which makes the results not very convincing. 

(2) The results are reported for different numbers of augmented instances. Clarification is needed. 
","[4, 6, 5]","[' Ok but not good enough - rejection', ' Marginally above acceptance threshold', ' Marginally below acceptance threshold']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer finds the proposed approach interesting, which indicates a positive sentiment. However, they also express concerns about clarity and experimental design, suggesting a somewhat less positive stance than a strong acceptance. The language used is constructive and professional, focusing on areas for improvement without resorting to harsh or disrespectful language.",50.0,75.0
Modifying memories in a Recurrent Neural Network Unit,"['Vlad Velici', 'Adam Prügel-Bennett']",Reject,2018,"[-2, 26]","[1, 31]","[2, 107]","[0, 48]","[2, 21]","[0, 38]","Summary: This paper introduces a model that combines the rotation matrices with the LSTMs. They apply the rotations before the final tanh activation of the LSTM and before applying the output gate. The rotation matrix is a block-diagonal one where each block is a 2x2 rotations and those rotations are parametrized by another neural network that predicts the angle of the rotations. The paper only provides results on the bAbI task. 

Questions:
Have you compared against to the other parametrizations of the LSTMs and rotation matrices? (ablation study)
Have you tried on other tasks?
Why did you just apply the rotations only on d_{t}.

Pros:
Uses a simple parametrization of the rotation matrices.

Cons:
Not clear justification and motivations
The experiments are really lacking:
No ablation study
The results are only limited to single toy task.


General Comments:

This paper proposes to use the rotation matrices with LSTMs. However there is no clear justification why is this particular parametrization of rotation matrix is being used over others and why is it only applied before the output gate. The experiments are seriously lacking, an ablation study should have been made and the results are not good enough. The experiments are only limited to bAbI task which doesn’t tell you much. This paper is not ready for publication, and really feels like it is rushed.

Minor Comment:
This paper needs more proper proof-reading. There are some typos in it, e.g.:
1st page, senstence --> sentence
4th page, the the ... --> the

","[3, 4, 4]","[' Clear rejection', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[4, 3, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer raises several serious concerns about the paper, including lack of justification for design choices, limited experimental validation, and the need for further proofreading. They call the paper ""rushed"" and state it's ""not ready for publication."" This points to a highly negative sentiment. While the reviewer lists some pros and cons, the overall tone is critical. However, the language avoids personal attacks and maintains a professional, albeit negative, tone.",-75.0,25.0
A Tensor Analysis on Dense Connectivity via Convolutional Arithmetic Circuits,"['Emilio Rafael Balda', 'Arash Behboodi', 'Rudolf Mathar']",Reject,2018,"[3, 11, 29]","[5, 15, 33]","[13, 80, 314]","[7, 45, 221]","[5, 30, 39]","[1, 5, 54]","SUMMARY

Traditional convolutional neural networks consist of a sequence of information processing layers. However, one can relax this sequential design constraint so that higher layers receive inputs from one, some, or all preceding layers. This modification allows information to travel more freely throughout the network and has been shown to improve performance, e.g., in image recognition tasks. However, it is not clear whether this change in architecture truly increases representational capacity or it merely facilitates network training. 

In this paper, the authors present a theoretical analysis of the gain in representational capacity induced by additional inter-layer connections. The authors restrict their analysis to convolutional arithmetic circuits (ConvACs), a class of networks whose representational capacity has been studied previously. An important property of ConvACs is that the network mapping can be recast as a homogeneous polynomial over the input, with coefficients stored in a ""grid tensor"" $\mathcal{A}^y$. The grid tensor itself is a function of the hidden weight vectors $\mathbf{a}^{z,i}$. The authors first extend ConvACs to accommodate ""dense"" inter-layer connections and describe how adding dense connections affects the grid tensor. This analysis gives a potentially useful perspective for understanding the mappings that densely connected ConvACs compute.

The authors' main results (Theorems 5.1-5.3) analyze the ""dense gain"" of a densely connected ConvAC. This quantity roughly captures how much wider a standard ConvAC would need to be in order to represent the network mapping of a generic densely connected ConvAC. This is in a way a measure of the added representational power obtained from dense connections. The authors give upper bounds on this quantity, but also produce a case in which the upper bound is achieved. Importantly, the upper bounds are inversely proportional to a parameter $\lambda \leq 1$ controlling the rate at which hidden layer widths decay with increasing depth. The implication is that indeed densely connected ConvACs can have greater representational capacity, however the gain is limited to the case where hidden layers shrink exponentially with increasing depth.

These results are partly unsurprising, since densely connected ConvACs contain more trainable parameters than standard ConvACs. In Proposition 3, the authors give some criteria for evaluating when it is nonetheless worthwhile to add dense connections to a ConvAC.

COMMENTS

(1.) The authors address an interesting and important problem: explaining the empirical success of densely connected CNNs such as ResNets & DenseNets, relative to standard CNNs. The tensor algebra machinery built around ConvACs is impressive and seems to generate sound insights. However, I feel the current presentation fails to provide adequate intuition and interpretation of the results. Moreover, there is no overarching narrative linking the formal results together. This makes it difficult for the reader to grasp the main ideas and significance of the work without diving into all the details. For example:

- In Proposition 1, the authors comment that including a dense connection increases the rank of the grid tensor for a shallow densely connected convAC. However, the significance of grid tensor rank is not discussed.

- In Proposition 2, the authors do not explain why it is important that the added term $g(\mathbf{X})$ contains only polynomial terms of strictly smaller degree. It is not clear how Propositions 1 & 2 relate to the main Theorems 5.1-5.3. Is the characterization of the grid tensor in Proposition 1 used to obtain the bounds in the later Theorems?

- In Section 5, the authors introduce a parameter $\lambda \leq 1$ controlling how the widths of the hidden layers decay with increasing depth. This parameter seems central to the following bounds on dense gain, yet the authors do not motivate it, and there is no discussion of decaying hidden layer widths in previous sections.

- The practical significance of Proposition 3 is not sufficiently well explained. First, it is not clear how to use this result if all we have is an upper bound for $G_w$, as given by Theorems 5.1-5.2. It seems we would need a lower bound to be able to conclude that the ratio $\Delta P_{stand}/ \Delta P_{dense}$ is large. Second, it would be helpful if the authors commented on the implication for the special case $k=1$ and $r \leq (1/1+\lambda) \sqrt{M}$, where the dense gain is known.

(2.) Moreover, because the authors choose not to sketch the main proof ideas, it is difficult to identify the key novel insights, and how the special structure of densely connected ConvACs factors into the analysis. After studying the proofs in some detail, I have some specific concerns outlined below, which diminish the significance of the results and raise some doubts about soundness.

- In Theorem 5.1, the authors upper bound the dense gain by showing that arbitrary $(L, r, \lambda, k)$ dense ConvACs can be represented as standard $(L, r^\prime, \lambda, 0)$ ConvACs of sufficient width $r^\prime \geq G_w r$. The mechanism of the proof is to relate the grid tensor ranks of dense and standard ConvACs. However, a worst case bound on the grid tensor rank of a dense ConvAC is used, which does not seem to rely on the formulation of dense ConvACs. Thus, this result does not tell us anything in particular about dense ConvACs, but rather is a general result relating the expressive capacity of arbitrary depth-$L$ ConvACs and $(L, r^\prime, \lambda, 0)$ ConvACs with decaying widths.

- Central to Theorem 5.2 is the observation that a densely connected ConvAC can be viewed as a standard ConvAC, only with ""virtually enlarged"" hidden layers (of width $\tilde{r}_\ell = (1 + 1/\lambda)r_\ell$ for $k=1$, using the notation of the paper), and blocks of weights fixed to represent the identity mapping. This is a relatively simple idea, and one that seems to hold for general architectures. Thus, I believe Theorem 5.2 can be shown more simply and in greater generality, and without use of the tensor algebra machinery.

- There is some intuitive inconsistency in Theorem 5.3 which I would like some help resolving. We have seen that dense ConvACs can be viewed as standard ConvACs with larger hidden layers and some weights fixed. Effectively, the proof of Theorem 5.3 argues for a regime on $r, \lambda, M$ where this induced ConvAC uses its full representational capacity. This is surprising to me however, as I would have guessed that having some weights fixed makes this impossible. It would be very helpful if the authors could weigh in on this confusion. Perhaps there is an issue with the application of Lemmas 2 & 3 in the proof of Theorem 5.3. In Lemmas 2 & 3, we assume the tensors $\mathcal{A}$ and $\mathcal{B}$ are random. These Lemmas are applied in the proof of Theorem 5.3 to tensors $\phi^{\alpha, j, \gamma}$ appearing in the construction of the dense ConvAC grid tensor. However, the $\phi^{\alpha, j, \gamma}$ tensors do not seem completely random, as there are blocks of fixed weights. Can the authors please clarify how the randomness assumption is satisfied?

(3.) Lastly, I am concerned that the authors do not at least sketch how to generalize these results to architectures of more practical interest. As the authors point out, there is previous work generalizing theoretical results for ConvACs to convolutional rectifier networks. The authors should discuss whether a similar strategy might apply in this case.","[4, 4, 5]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[3, 3, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review is quite critical of the paper, pointing out a lack of clarity, potential flaws in the logic, and limited practical implications. While the reviewer acknowledges the interesting problem and impressive mathematical tools, the numerous concerns raised suggest significant revisions are needed. The tone, while direct and critical, maintains a professional and academic demeanor. ",-30.0,60.0
CAYLEYNETS: SPECTRAL GRAPH CNNS WITH COMPLEX RATIONAL FILTERS,"['Ron Levie', 'Federico Monti', 'Xavier Bresson', 'Michael M. Bronstein']",Reject,2018,"[7, 4, 17, 17]","[12, 9, 22, 22]","[35, 27, 97, 301]","[6, 9, 36, 131]","[24, 16, 35, 105]","[5, 2, 26, 65]","Summary: This paper proposes a new graph-convolution architecture, based on Cayley transform of the matrix. Succinctly, if L denotes the Laplacian of a graph, this filter corresponds to an operator that is a low degree polynomial of C(L) = (hL - i)/(hL+i), where h is a scalar and i denotes sqrt(-1). The authors contend that such filters are interesting because they can 'zoom' into a part of the spectrum, depending on the choice of h, and that C(L) is always a rotation matrix with eigenvalues with magnitude 1. The authors propose to compute them using Jacobi iteration (using the diagonal as a preconditioner), and present experimental results.

Opinion: Though the Cayley filters seem to have interesting properties,  I find the authors theoretical and experimental justification insufficient to conclude that they offer sufficient advantage over existing methods. I list my major criticisms below:
1. The comparison to Chebyshev filters  (small degree polynomials in the Chebyshev basis) at several places is unconvincing. The results on CORA (Fig 5a) compare filters with the same order, though Cayley filters have twice the number of variables for the same order as Chebyshev filters. Similarly for Fig 1, order 3 Cayley should be compared to Order 6 Chebyshev (roughly).

2. Since Chebyshev polynomials blow up exponentially when applied to values larger than 1, applying Chebyshev filters to unnormalized Laplacians (Fig 5b) is an unfair comparison.

3. The authors basically apply Jacobi iteration (gradient descent using a diagonal preconditioner) to estimate the Cayley filters, and contend that a constant number of iterations of Jacobi are sufficient. This ignores the fact that their convergence rate scales quadratically in h and the max-degree of the graph. Moreover, this means that the Filter is effectively a low degree polynomial in (D^(-1)A)^K, where A is the adjacency matrix of the graph, and K is the number of Jacobi iterations. It's unclear how (or why) a choice of K might be good, or why does it make sense to throw away all powers of D^(-1)Af, even though we're computing all of them.
Also, note that this means a K-fold increase in the runtime for each evaluation of the network, compared to the Chebyshev filter.

Among the other experimental results, the synthetic results do clearly convey a significant advantage at least over Chebyshev filters with the same number of parameters. The CORA results (table 2) do convey a small but clear advantage. The MNIST result seems a tie, and the comparison for MovieLens doesn't make it obvious that the number of parameters is the same. 

Overall, this leads me to conclude that the paper presents insufficient justification to conclude that Cayley filters offer a significant advantage over existing work.","[4, 6, 8]","[' Ok but not good enough - rejection', ' Marginally above acceptance threshold', ' Top 50% of accepted papers, clear accept']","[3, 3, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer raises several significant concerns about the paper's methodology and findings, particularly regarding the comparison with Chebyshev filters. They find the experimental justification lacking and point out potential flaws in the approach. While acknowledging some positive aspects, like the synthetic results, the overall tone suggests skepticism and a belief that the paper needs further work. Therefore, the sentiment leans towards the negative side. The language used is formal, academic, and devoid of any personal attacks, indicating a professional and polite tone.",-25.0,75.0
Lifelong Learning with Output Kernels,"['Keerthiram Murugesan', 'Jaime Carbonell']",Reject,2018,"[3, 12]","[8, 1]","[27, 1]","[12, 1]","[14, 0]","[1, 0]","The paper proposes a budgeted online kernel algorithm for multi-task learning. The main contribution of the paper is an online update of the output kernel, which measures similarity between pairs of tasks. The paper also proposes a removal strategy that bounds the number of support vectors in the kernel machine. The proposed algorithm is tested on 3 data sets and compared with several baselines.
  Positives:
- the output kernel update is well justified
- experimental results are encouraging
  Negatives:
- the methodological contribution of the paper is minimal
- the proposed approach to maintain the budget is simplistic
- no theoretical analysis of the proposed algorithm is provided
- there are issues with the experiments: the choice of data sets is questionable (all data sets are very small so there is not need for online learning or budgeting; newsgroups is a multi-class problem, so we would want to see comparisons with some good multi-class algorithms; spam data set might be too small), it is not clear what were hyperparameters in different algorithms and how they were selected, the budgeted baselines used in the experiments  are not state of the art (forgetron and random removal are known to perform poorly in practice, projectron usually works much better), it is not clear how a practitioner would decide whether to use update (2) or(3)","[4, 2, 3]","[' Ok but not good enough - rejection', ' Strong rejection', ' Clear rejection']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review acknowledges some positives like a well-justified output kernel update and encouraging results. However, it heavily criticizes the paper for minimal methodological contribution, a simplistic budgeting approach, lack of theoretical analysis, and questionable experimental setup. The numerous and specific concerns raised about the experiments suggest significant weaknesses in the paper. Overall, the negatives outweigh the positives.",-30.0,50.0
On the Generalization Effects of DenseNet Model Structures ,"['Yin Liu', 'Vincent Chen']",Reject,2018,"['no_match', 4]","['no_match', 8]","['no match', 12]","['no match', 8]","['no match', 4]","['no match', 0]","This paper analyzes the role of skip connections with respect to generalization in recent architectures such as ResNets or DenseNets. The authors perform an analysis of the performance of ResNets and DenseNets under data scarcity constraints and noisy training samples. They also run some experiments assessing the importance of the number of skip connections in such networks.

The presentation of the paper could be significantly improved. The motivation is difficult to grasp and the contributions do not seem compelling.

My main concern is about the contribution of the paper. The hypothesis that skip connections ease the training and improve the generalization has already been highlighted in the ResNet and DenseNet paper, see e.g. [a].

[a] https://arxiv.org/pdf/1603.05027.pdf

Moreover, the literature review is very limited. Although there is a vast existing literature on ResNets, DenseNets and, more generally, skip connections, the paper only references 4 papers. Many relevant papers could be referenced in the introduction as examples of successes in computer vision tasks,  identity mapping initialization, recent interpretations of ResNets/DensetNets, etc.

The title suggests that the analysis is performed on DenseNet architectures, but experiments focus on comparing both ResNets and DenseNets to sequential convolutional networks and assessing the importance of skip connections.

In section 3.1. (1st paragraph) proposes adding noise to groundtruth labels; however, in section 3.1.2,. it would seem that noise is added by changing the input images (by setting some pixel channels to 0). Could the authors clarify that? Wouldn’t the noise added to the groundtruth act as a regularizer?

In section 4, the paper claims to investigate the role of skip connections in vision tasks. However, experiments are performed on MNIST, CIFAR100, a curve fitting problem and a presumably synthetic 2D classification problem. Performing the analysis on computer vision datasets such as ImageNet would be more compelling to back the statement in section 4.
","[3, 3, 2]","[' Clear rejection', ' Clear rejection', ' Strong rejection']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review is critical of the paper's contribution, presentation, and experimental methodology. It points out several shortcomings, such as lack of novelty, limited literature review, and questionable experimental choices. While the reviewer provides constructive feedback and suggestions for improvement, the overall tone suggests that the paper needs significant revisions. The language used is formal and professional, without resorting to personal attacks or disrespectful remarks.",-50.0,70.0
Structured Exploration via Hierarchical Variational Policy Networks,"['Stephan Zheng', 'Yisong Yue']",Reject,2018,"[3, 12]","[8, 17]","[39, 230]","[10, 101]","[27, 110]","[2, 19]","This paper proposes an approach to improve exploration in multiagent reinforcement learning by allowing the policies of the individual agents to be conditioned on an external coordination signal \lambda. In order to find such parametrized policies, the approach combines deep RL with a variational inference approach (ELBO optimization). The paper presents an empirical evaluation, which seems encouraging, but that is also somewhat difficult to interpret given the lack of comparison to other state-of-the-art methods.

Overall, the paper seems interesting, but (in addition to the not completely convincing empirical evaluation), it has two main weaknesses: lack of clarity and grounding in related literature.

=Issues with clarity=

-""This problem has two equivalent solutions"". 
This is not so clear; depending on the movement of the preys it might well be that the optimal solution will switch to the other prey in certain cases?

-It is not clear what is really meant with the term ""structured exploration"". It just seems to mean 'improved'?

-It is not clear that the improvements are due to exploration; my feeling is that is is due to improved statistical strength on a more abstract state feature (which is learned), not unlike:
Geramifard, Alborz, et al. ""Online discovery of feature dependencies."" Proceedings of the 28th International Conference on Machine Learning (ICML-11). 2011.
However, there is no clear indication that there is an improved exploration policy.

-The problem setting is not quite clear:
The paper first introduces ""multi-agent RL"", which seems to correspond to a ""stochastic game"" (also ""Markov game""), but then moves on to restrict to the ""fully cooperative setting"" (which would make it a ""Multiagent MDP"", Boutilier '96).

It subsequently says it deals only with deterministic problems (which would reduce the problem further to a learning version of a multiagent classical planning problem), but in the experiments do consider stochastically moving preys.

-The paper says the problem is fully observable, but fails to make explicit if this is *individually* fully observable, or jointly. I am assuming the former, but is it not clear how the agents observe this full state in the experimental evaluation.

This is actually a crucial confusion, as it completely changes the interpretation of what the approach does: in the individually observable case, the approach is adding a redundant source of information which is more abstract and thus seems to facilitate faster learning. In the latter case, where agents would have individual observations, it is actually providing the agents with more information.

As such, I would really encourage the authors to better define the task they are considering. E.g., by building on the taxonomies of problems that researchers have developed in the community focusing on decentralized POMDPs, such as:
Goldman, Claudia V., and Shlomo Zilberstein. ""Decentralized control of cooperative systems: Categorization and complexity analysis."" (2004).

-""Compared to the single-agent RL setting, multi-agent RL poses unique difficulties. A central issue
is the exploration-exploitation trade-off""
That now in particular happens to be a central issue in single agent RL too.

-""Finding the true posteriors P (λ t |s t ) ∝ P (s t |λ t )P (λ t ) is intractable in general""
The paper did not explain how this inference task is required to solve the RL problem.

-In general, I found the technical description impossible to follow, even after carefully looking at the appending. For instance, (also) there the term P (λ |s ) is suddenly introduced without explaining what the term exactly is? Why is the term P(a|λ) not popping up here? That also needs to be optimized, right? I suppose \phi is the parameter vector of the variational approximation, but it is never really stated. The various shorthand notations introduced for clarity do not help at all, but only make the formulas very cryptic.

-The main text is not readable since definitions, e.g., L(Q_r,\tehta,\phi), that are in the appendix are now missing.

-It is not clear to me how the second term of (10) is now estimated?

-""Shared (shared actor-critic): agents share a deterministic hidden layer,""
What kind of layer is this exactly? How does it relate to \lambda ?

-""The key difference is that this model does not sample from the shared hidden layer""
Why would sampling help? Given that we are dealing with a fully observable multiagent MDP, there is no inherent need to randomize at all? (there should be a optimal deterministic joint policy?)

-""There is shared information between the agents""
What information is referred to exactly? 
Also: It is not quite clear if for these domains cloned would be better than completely independent learners (without shared weights)?

-I can't seem to find anywhere what is the actual shape (or type? I am assuming a vector of reals) of the used \lambda.

-in figure 5, rhs, what is being shown exactly? What do the colors mean? Why does there seem to be a \lambda *per* agent now?



=Related work=

I think the paper could/should be hugely improved in this respect. 

The idea of casting MARL as inference has also been considered by:

Learning for Decentralized Control of Multiagent Systems in Large, Partially-Observable Stochastic Environments.
M Liu, C Amato, EP Anesta, JD Griffith, JP How - AAAI, 2016

Stick-breaking policy learning in Dec-POMDPs
M Liu, C Amato, X Liao, L Carin, JP How
International Joint Conference on Artificial Intelligence (IJCAI) 2015

Wu, F.; Zilberstein, S.; and Jennings, N. R. 2013. Monte-carlo
expectation maximization for decentralized POMDPs. In Proc.
of the 23rd Int’l Joint Conf. on Artificial Intelligence (IJCAI-
13).

I do not think that these explicitly make use of a mechanism to coordinate the policies, since they address to true Dec-POMDP setting where each agent only gets its own observations, but in the Dec-POMDP literature, there also is the notion of 'correlation device', which is an additional controller (say corresponding to a dummy agent), which of which the states can be observed by other agents and used to condition their actions on:

Bernstein DS, Hansen EA, Zilberstein S. Bounded policy iteration for decentralized POMDPs. InProceedings of the nineteenth international joint conference on artificial intelligence (IJCAI) 2005 Jun 6 (pp. 52-57).

(and clearly this could be directly included in the aforementioned learning approaches). 


This notion of a correlation device also highlights to potential relation to methods to learn/compute correlated equilibria. E.g.,:

Greenwald A, Hall K, Serrano R. Correlated Q-learning. In ICML 2003 Aug 21 (Vol. 3, pp. 242-249).


A different connection between MARL and inference can be found in:

Zhang, Xinhua and Aberdeen, Douglas and Vishwanathan, S. V. N., ""Conditional Random Fields for Multi-agent Reinforcement Learning"", in (New York, NY, USA: ACM, 2007), pp. 1143--1150.


The idea of doing something hierarchical of course makes sense, but also here there are a number of related papers:

-putting ""hierarchical multiagent"" in google scholar finds works by Ghavamzadeh et al., Saira & Mahadevan, etc.

-Victor Lesser has pursued coordination for better exploration with a number of students.

I suppose that Guestrin et al.'s classical paper:
Guestrin, Carlos, Michail Lagoudakis, and Ronald Parr. ""Coordinated reinforcement learning."" ICML. Vol. 2. 2002.
would deserve a citation, and the MARL field is moving ahead fast, an explanation of the differences with COMA:
Counterfactual Multi-Agent Policy Gradients
J Foerster, G Farquhar, T Afouras, N Nardelli, S Whiteson
AAAI 2018
is probably also warranted.








","[4, 7, 5]","[' Ok but not good enough - rejection', ' Good paper, accept', ' Marginally below acceptance threshold']","[5, 3, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer finds the paper interesting but has significant concerns about its clarity, grounding in related literature, and the conclusiveness of the empirical evaluation. They point out multiple instances where the paper's explanations are unclear or incomplete. The reviewer also suggests a considerable amount of related work that should be discussed. While they don't explicitly reject the paper, their tone is quite critical. Therefore, the sentiment is somewhat positive but leans towards neutral due to the significant concerns raised.",20.0,50.0
Taking Apart Autoencoders: How do They Encode Geometric Shapes ?,"['Alasdair Newson', 'Andres Almansa', 'Yann Gousseau', 'Said Ladjal']",Reject,2018,"[7, 20, 17]","[11, 25, 22]","[30, 108, 36]","[13, 52, 14]","[10, 18, 9]","[7, 38, 13]","1. The idea is interesting, but the study is not comprehensive yet
2. need to visualize the input data space, with the training data, test data, the 'gaps' in training data [see a recent related paper - Stoecklein et al. Deep Learning for Flow Sculpting: Insights into Efficient Learning using Scientific Simulation Data. Scientific Reports 7, Article number: 46368 (2017).]. 
3. What's the effect of training data size? 
4. How do the intermediate feature maps look like? 
5. Is there an effect of number of layers? Maybe the network architecture is too deep for the simple data characteristics and size of training set. 
6. Other shapes are said to be part of future work, but I am not convinced that serious conclusions can be drawn from this study only? 
7. What about the possible effects of Batch normalization and dropout?  
8. size of 'd' is critical for autoencoders, only one example in appendix does not do justice, also it seems other color channels show up in the results (fig 10), wasn't it binary input?","[4, 4, 4]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[5, 4, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review starts with a mildly positive statement acknowledging the interesting idea. However, the rest of the review lists a series of concerns and suggestions for improvement, indicating that the reviewer finds the current study insufficient. The language is quite direct and to the point, but it doesn't contain any personal attacks or disrespectful phrasing. The reviewer focuses on the scientific content and provides concrete suggestions.",20.0,60.0
Tracking Loss: Converting Object Detector to Robust Visual Tracker,"['Zhenbin Yan', 'Jimmy Ren', 'Stephen Shaoyi Liao', 'Kai Yang']",Reject,2018,"['no_match', 9, 25, 'no_match']","['no_match', 14, 29, 'no_match']","['no match', 102, 97, 'no match']","['no match', 56, 44, 'no match']","['no match', 39, 3, 'no match']","['no match', 7, 50, 'no match']","In this paper, the authors propose a novel tracking loss to convert the RPN to a tracker. The internal structure of top layer features of RPN is exploited to treat feature points discriminatively. In addition, the proposed compression network speeds up the tracking algorithm. The experimental results on the VOT2016 dataset demonstrate its efficiency in tracking. 

This work is the combination of Faster R-CNN (Ren et al. PAMI 2015) and tracking-by-detection framework. The main contributions proposed in this paper are new tracking loss, network compression and results. 

There are numerous concerns with this work:

1.	The new tracking loss shown in equation 2 is similar with the original Faster R-CNN loss shown in equation 1. The only difference is to replace the regression loss with a predefined mask selection loss, which is of little sense that the feature processing can be further fulfilled through one-layer CNN. The empirical operation shown in figure 2 seems arbitrary and lack of theoretical explanation. There is no insight of why doing so. Simply showing the numbers in table 1 does not imply the necessity, which ought to be put in the experiment sections. 
2.	The network compression is engineering and lack insight as well. To remove part of the CNN and retrain is a common strategy in the CNN compression methods [a] [b]. There is a lack of discussion with the relationship with prior arts.
3.	The organization is not clear. Section 3.4 should be set in the experiments and Section 3.5 should be set at the beginning of the algorithm. The description of the network compression is not clear enough, especially the training details.  Meanwhile, the presentation is hard to follow. There is no clear expression of how the tracker performs in practice.
4.	In addition, VOT 2016, the method should evaluate on the OTB dataset with the following trackers [c] [d].
5.	The evaluation is not fair. In Sec 6, the authors indicate that MDNet runs at 1FPS while the proposed tracker runs at 1.6FPS. However, MDNet is based on Matlab and the proposed tracker is based on C++ (i.e., Caffe).

Reference:
[a] On Compressing Deep Models by Low Rank and Sparse Decomposition. Yu et al. CVPR 2017.
[b] Designing Energy-Efficient Convolutional Neural Network Using Energy-Aware Pruning. Yang et al. CVPR 2017.
[c] ECO: Efficient Convolution Operators for Tracking. Danelljan et al. CVPR 2017.
[d] Multi-Task Correlation Particle Filter For Robust Object Tracking. Zhang et al. CVPR 2017.
","[3, 4, 5]","[' Clear rejection', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is highly critical of the paper, pointing out significant flaws in the methodology, lack of novelty, and unfair comparisons. The reviewer lists five major concerns, questioning the significance of the contributions and highlighting the need for substantial revisions. The language, while direct and critical, maintains a professional tone.",-70.0,60.0
Forced Apart: Discovering Disentangled Representations Without Exhaustive Labels,"['Alexey Romanov', 'Anna Rumshisky']",Reject,2018,"[7, 17]","[10, 22]","[32, 97]","[21, 53]","[11, 35]","[0, 9]","This paper proposes two regularization terms to encourage learning disentangled representations. One term is applied to weight parameters of a layer just like weight decay. The other is applied to the activations of the target layer (e.g., the penultimate layer). The core part of both regularization terms is a compound hinge loss of which the input is the KL divergence between two softmax-normalized input arguments. Experiments demonstrate the proposed regularization terms are helpful in learning representations which significantly facilitate clustering performance.

Pros:
(1) This paper is clearly written and easy to follow.

(2) Authors proposed multiple variants of the regularization term which cover both supervised and unsupervised settings.

(3) Authors did a variety of classification experiments ranging from time serials, image and text data.

Cons:
(1) The design choice of the compound hinge loss is a bit arbitrary. KL divergence is a natural similarity measure for probability distribution. However, it seems that authors use softmax to force the weights or the activations of neural networks to be probability distributions just for the purpose of using KL divergence. Have you compared with other choices of similarity measure, e.g., cosine similarity? I think the comparison as an additional experiment would help explain the design choice of the proposed function.

(2) In the binary classification experiments, it is very strange to almost randomly group several different classes of images into the same category. I would suggest authors look into datasets where the class hierarchy is already provided, e.g., ImageNet or a combination of several fine-grained image classification datasets.

Additionally, I have the following questions:
(1) I am curious how the proposed method compares to other competitors in terms of the original classification setting, e.g., 10-class classification accuracy on CIFAR10. 
(2) What will happen for the multi-layer loss if the network architecture is very large such that you can not use large batch size, e.g., less than 10? 

(3) In drawing figure 2 and 3, if the nonlinear activation function is not ReLU, how would you exam the same behavior? Have you tried multi-class classification for the case “without proposed loss component” and does the similar pattern still happen or not?

Some typos:
(1) In introduction, “when the cosine between the vectors 1” should be “when the cosine between the vectors is 1”.

(2) In section 4.3, “we used the DBPedia ontology dataset dataset” should be “we used the DBPedia ontology dataset”. 

I would like to hear authors’ feedback on the issues I raised.
","[5, 4, 5]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer provides constructive criticism, acknowledges the strengths of the paper, and offers specific suggestions for improvement. They highlight both positive aspects (well-written, multiple variants, diverse experiments) and areas for improvement (design choices, dataset selection, comparisons). The questions posed are specific and intended to enhance the paper's clarity and depth. The tone is professional and objective, focusing on the scientific merit of the work.",50.0,75.0
Modeling Latent Attention Within Neural Networks,"['Christopher Grimm', 'Dilip Arumugam', 'Siddharth Karamcheti', 'David Abel', 'Lawson L.S. Wong', 'Michael L. Littman']",Reject,2018,"[2, 4, 2, 5, 13, 30]","[6, 9, 7, 9, 17, 35]","[17, 32, 32, 39, 45, 294]","[4, 12, 13, 21, 25, 166]","[11, 19, 18, 18, 17, 75]","[2, 1, 1, 0, 3, 53]","The authors of this paper proposed a data-driven black-box visualization scheme. The paper primarily focuses on neural network models in the experiment section. The proposed method iteratively optimize learnable masks for each training example to find the most relevant content in the input that was ""attended"" by the neural network.  The authors empirically demonstrated their method on image and text classification tasks. 

Strength:
           - The paper is well-written and easy to follow. 
           - The qualitative analysis of the experimental results nicely illustrated how the learnt latent attention masks match with our intuition about how neural networks make its classification predictions.

        Weakness:
           - Most of the experiments in the paper are performed on small neural networks and simple datesets. I found the method will be more compiling if the authors can show visualization results on ImageNet models. Besides simple object recognition tasks, other more interesting tasks to test out the proposed visualization method are object detection models like end-to-end fast R-CNN, video classification models, and image-captioning models. Overall, the current set of experiments are limited to showcase the effectiveness of the proposed method.
           - It is unclear how the hyperparameter is chosen for the proposed method. How does the \beta affect the visualization quality? It would be great to show a range of samples from high to low beta values. Does it require tuning for different visualization samples? Does it vary over different datasets?
  ","[5, 4, 7]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Good paper, accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review acknowledges the strengths of the paper, such as its clarity and insightful qualitative analysis. However, it also points out significant weaknesses, primarily the limited experimental scope and lack of clarity regarding hyperparameter selection. The reviewer suggests more challenging experiments and a deeper exploration of the method's sensitivity to its parameters. The tone is constructive and objective, suggesting areas for improvement without resorting to harsh language.",40.0,80.0
Training Deep AutoEncoders for Recommender Systems,"['Oleksii Kuchaiev', 'Boris Ginsburg']",Reject,2018,"[10, 17]","[15, 22]","[26, 69]","[11, 27]","[12, 40]","[3, 2]","This paper proposed to use deep AE to do rating prediction tasks in recommender systems.
Some of the conclusions of the paper, e.g. deep models perform bettern than shallow ones, the non-linear activation
function is important, dropout is necessary to prevent overfitting, are well known, and hence is of less novelty.
The proposed re-feeding algorithm to overcome natural sparseness of CF is interesting, however, I don't think it is enough to support being accepted by ICLR. 
Some reference about rating prediction are missing, such as ""A neural autoregressive approach to collaborative filtering, ICML2016"". And it would be better to show the performance of the model on implicit rating data, since it is more desirable in practice, since many industry applications have only implicit rating (e.g. whether the user watches the movie or not.).","[3, 4, 6]","[' Clear rejection', ' Ok but not good enough - rejection', ' Marginally above acceptance threshold']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review acknowledges some positive aspects of the paper, such as the interesting re-feeding algorithm. However, it also points out significant shortcomings, including lack of novelty and limited practical application. The reviewer suggests the paper is not strong enough for ICLR. The language is direct and critical but maintains a professional and respectful tone. ",-20.0,50.0
Predicting Auction Price of Vehicle License Plate with Deep Recurrent Neural Network,['Vinci Chow'],Reject,2018,[2],[4],[4],[1],[2],[1],"Summary: The authors take two pages to describe the data they eventually analyze - Chinese license plates (sections 1,2), with the aim of predicting auction price based on the ""luckiness"" of the license plate number.  The authors mentions other papers that use NN's to predict prices, contrasting them with the proposed model by saying they are usually shallow not deep, and only focus on numerical data not strings. Then the paper goes on to present the model which is just a vanilla RNN, with standard practices like batch normalization and dropout.  The proposed pipeline converts each character to an embedding with the only sentence of description being ""Each character is converted by a lookup table to a vector representation, known as character embedding.""   Specifics of the data,  RNN training, and the results as well as the stability of the network to hyperparameters is also examined. Finally they find a ""a feature vector for each plate by summing up the output of the last recurrent layer overtime."" and the use knn on these features to find other plates that are grouped together to try to explain how the RNN predicts the prices of the plates. In section 7,  the RNN is combined with a handcrafted feature model he criticized in a earlier section for being too simple to create an ensemble model that predicts the prices marginally better. 

Specific Comments on Sections: 
Comments: Sec 1,2
In these sections the author has somewhat odd references to specific economists that seem a little off topic, and spends a little too much time in my opinion setting up this specific data.

Sec 3
The author does not mention the following reference: ""Deep learning for stock prediction using numerical and textual information"" by Akita et al. that does incorporate non-numerical info to predict stock prices with deep networks.

Sec 4
What are the characters embedded with? This is important to specify. Is it Word2vec or something else? What does the lookup table consist of? References should be added to the relevant methods. 

Sec 5
I feel like there are many regression models that could have been tried here with word2vec embeddings that would have been an interesting comparison. LSTMs as well could have been a point of comparison. 

Sec 6
 Nothing too insightful is said about the RNN Model. 

Sec 7
The ensembling was a strange extension especially with the Woo model given that the other MLP architecture gave way better results in their table.

Overall: This is a unique NLP problem, and it seems to make a lot of sense to apply an RNN here, considering that word2vec is an RNN. However comparisons are lacking and the paper is not presented very scientifically.  The lack of comparisons made it feel like the author cherry picked the RNN to outperform other approaches that obviously would not do well.
","[4, 4, 6]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Marginally above acceptance threshold']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review is critical of the paper's structure, depth, and lack of comparisons to other methods. It points out several shortcomings without resorting to overtly negative language. The reviewer acknowledges the novelty of the problem and the appropriateness of using an RNN but finds the execution and presentation lacking.",-30.0,60.0
WHAT ARE GANS USEFUL FOR?,"['Pablo M. Olmos', 'Briland Hitaj', 'Paolo Gasti', 'Giuseppe Ateniese', 'Fernando Perez-Cruz']",Reject,2018,"[10, 4, 10, 23, 27]","[15, 9, 12, 28, 32]","[101, 21, 64, 142, 147]","[29, 7, 35, 69, 66]","[35, 12, 16, 51, 32]","[37, 2, 13, 22, 49]","The main take-away messages of this paper seem to be:

1. GANs don't really match the target distribution. Some previous theory supports this, and some experiments are provided here demonstrating that the failure seems to be largely in under-sampling the tails, and sometimes perhaps in introducing spurious modes.

2. Even if GANs don't exactly match the target distribution, their outputs might still be useful for some tasks.

(I wouldn't be surprised if you disagree with what the main takeaways are; I found the flow of the paper somewhat disjointed, and had something of a hard time identifying what the ""point"" was.)

Mode-dropping being a primary failure mode of GANs is already a fairly accepted hypothesis in the community (see, e.g. Mode Regularized GANs, Che et al ICLR 2017, among others), though some extra empirical evidence is provided here.

The second point is, in my opinion, simultaneously (i) an important point that more GAN research should take to heart, (ii) relatively obvious, and (iii) barely explored in this paper. The only example in the paper of using a GAN for something other than directly matching the target distribution is PassGAN, and even that is barely explored beyond saying that some of the spurious modes seem like reasonable-ish passwords.

Thus though this paper has some interesting aspects to it, I do not think its contributions rise to the level required for an ICLR paper.

Some more specifics:

Section 2.1 discusses four previous theoretical results about the convergence of GANs to the true density. This overview is mostly reasonable, and the discussion of Arora et al. (2017) and Liu et al. (2017) do at least vaguely support the conclusion in the last section of this paragraph. But this section is glaringly missing an important paper in this area: Arjovsky and Bottou (2017), cited here only in passing in the introduction, who proved that typical GAN architectures *cannot* exactly match the data distribution. Thus the question of metrics for convergence is of central importance, which it seems should be important to the topic of the present paper. (Figure 3 of Danihelka et al. https://arxiv.org/abs/1705.05263 gives a particularly vivid example of how optimizing different metrics can lead to very different results.) Presumably different metrics lead to models that are useful for different final tasks.

Also, although they do not quite fit into the framing of this section, Nowozin et al.'s local convergence proof and especially the convergence to a Nash equilibrium argument of Heusel et al. (NIPS 2017, https://arxiv.org/abs/1706.08500) should probably be mentioned here.

The two sample testing section of this paper, discussed in Section 2.2 and then implemented in Section 3.1.1, seems to be essentially a special case of what was previously done by Sutherland et al. (2017), except that it was run on CIFAR-10 as well. However, the bottom half of Table 1 demonstrates that something is seriously wrong with the implementation of your tests: using 1000 bootstrap samples, you should reject H_0 at approximately the nominal rate of 5%, not about 50%! To double-check, I ran a median-heuristic RBF kernel MMD myself on the MNIST test set with N_test = 100, repeating 1000 times, and rejected the null 4.8% of the time. My code is available at https://gist.github.com/anonymous/2993a16fbc28a424a0e79b1c8ff31d24 if you want to use it to help find the difference from what you did. Although Table 1 does indicate that the GAN distribution is more different from the test set than the test set is from itself, the apparent serious flaw in your procedure makes those results questionable. (Also, it seems that your entry labeled ""MMD"" in the table is probably n * MMD_b^2, which is what is computed by the code linked to in footnote 2.)

The appendix gives a further study of what went wrong with the MNIST GAN model, arguing based on nearest-neighbors that the GAN model is over-representing modes and under-representing the tails. This is fairly interesting; certainly more interesting than the rehash of running MMD tests on GAN outputs, in my opinion.

Minor:

In 3.1.1, you say ""ideally the null hypothesis H0 should never be rejected"" – it should be rejected at most an alpha portion of the time.

In the description of section 3.2, you should clarify whether the train-test split was done such that unique passwords were assigned to a single fold or not: did 123456 appear in both folds? (It is not entirely clear whether it should or not; both schemes have possible advantages for evaluation.)","[3, 3, 3]","[' Clear rejection', ' Clear rejection', ' Clear rejection']","[5, 4, 5]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review is critical of the paper, pointing out flaws in the methodology and lack of significant contributions. While it acknowledges some interesting aspects, the overall tone suggests the paper is not ready for publication in its current form. The reviewer provides concrete examples of issues and suggestions for improvement, indicating a willingness to engage with the work but not shying away from criticism. Therefore, the sentiment leans towards the negative side. The language used is direct and technical, typical of academic peer reviews. While the reviewer points out shortcomings, the language remains professional and avoids personal attacks, maintaining a neutral politeness level.",-30.0,0.0
Compact Encoding of Words for Efficient Character-level Convolutional Neural Networks Text Classification,"['Wemerson Marinho', 'Luis Marti', 'Nayat Sanchez-pi']",Reject,2018,"[1, 18, 13]","[6, 22, 17]","[2, 4, 64]","[2, 1, 50]","[0, 1, 1]","[0, 2, 13]","The manuscript proposed to use prefix codes to compress the input to a neural network for text classification. It builds upon the work by Zhang & LeCun (2015) where the same tasks are used.


There are several issues with the paper and I cannot recommend acceptance of the paper in the current state. 
- It looks like it is not finished.
- the datasets are not described properly. 
- It is not clear to me where the baseline results come from.
 They do not match up to the Zhang paper (I have tried to find the matching accuracies there).
- It is not clear to me what the baselines actually are or how I can found more info on those.
- the results are not remarkable. 

Because of this, the paper needs to be updated and cleaned up before it can be properly reviewed. 

On top of this, I do not enjoy the style the paper is written in, the language is convoluted. 
For example: “The effort to use Neural Convolution Networks for text classification tasks is justified by the possibility of appropriating tools from the recent developments of techniques, libraries and hardware used especially in the image classification “
I do not know which message the paper tries to get across here. 
As a reviewer my impression (which is subjective) is that the authors used difficult language to make the manuscript look more impressive.
The acknowledgements should not be included here either. 

","[3, 2, 4]","[' Clear rejection', ' Strong rejection', ' Ok but not good enough - rejection']","[5, 5, 5]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The reviewer clearly states multiple issues with the paper, ultimately recommending against acceptance. They find the results unremarkable and the paper unfinished. They also criticize the writing style, suspecting an attempt to mask a lack of substance with convoluted language. This points to a negative sentiment. While the reviewer lists concrete criticisms, the language remains professional and avoids personal attacks, suggesting a neutral politeness level.",-60.0,0.0
Deep Boosting of Diverse Experts,"['Wei Zhang', 'Qiuyu Chen', 'Jun Yu', 'Jianping Fan']",Reject,2018,"[14, 2, 'no_match', 19]","[18, 5, 'no_match', 24]","[49, 11, 'no match', 290]","[26, 5, 'no match', 132]","[12, 3, 'no match', 16]","[11, 3, 'no match', 142]","This paper consider a version of boosting where in each iteration only class weights are updated rather than sample weights and apply that to a series of CNNs for object recognition tasks.

While the paper is comprehensive in their derivations (very similar to original boosting papers and in many cases one to one translation of derivations), it lacks addressing a few fundamental questions:

- AdaBoost optimises exponential loss function via functional gradient descent in the space of weak learners. It's not clear what kind of loss function is really being optimised here. It feels like it should be the same, but the tweaks applied to fix weights across all samples for a class doesn't make it not clear what is that really gets optimised at the end.
- While the motivation is that classes have different complexities to learn and hence you might want each base model to focus on different classes, it is not clear why this methods should be better than normal boosting: if a class is more difficult, it's expected that their samples will have higher weights and hence the next base model will focus more on them. And crudely speaking, you can think of a class weight to be the expectation of its sample weights and you will end up in a similar setup.
- Choice of using large CNNs as base models for boosting isn't appealing in practical terms, such models will give you the ability to have only a few iterations and hence you can't achieve any convergence that often is the target of boosting models with many base learners.
- Experimentally, paper would benefit with better comparisons and studies: 1) state-of-the-art methods haven't been compared against (e.g. ImageNet experiment compares to 2 years old method) 2) comparisons to using normal AdaBoost on more complex methods haven't been studied (other than the MNIST) 3) comparison to simply ensembling with random initialisations.

Other comments:
- Paper would benefit from writing improvements to make it read better.
- ""simply use the weighted error function"": I don't think this is correct, AdaBoost loss function is an exponential loss. When you train the base learners, their loss functions will become weighted.
-  ""to replace the softmax error function (used in deep learning)"": I don't think we have softmax error function","[2, 6, 5]","[' Strong rejection', ' Marginally above acceptance threshold', ' Marginally below acceptance threshold']","[5, 3, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is critical of the paper's approach and findings. The reviewer raises several fundamental questions and suggests the paper lacks novelty and sufficient experimental validation. The tone, however, remains professional and focused on suggesting improvements.",-50.0,50.0
DENSELY CONNECTED RECURRENT NEURAL NETWORK FOR SEQUENCE-TO-SEQUENCE LEARNING,['Fei Tian'],Reject,2018,[10],[14],[69],[37],[18],[14],"This paper describes an attempt of improving information flow in deep networks (but is used and tested here with seq2seq models although it is reality unrelated to seq2seq models per se). Slightly different from Resnet the information flow is improved by not just adding the outputs from previous layers but instead concatenating the outputs from previous layers with the current outputs. The authors claim better convergence speed and better results for a similar number of parameters although the differences seems to be in the noise.  

Overall this is an OK technique but in my opinion not really novel enough to justify a whole paper about it as it seems more like a relatively minor architecture tweak. The results seem to indicate that there were some problems with getting deeper networks to work for the baseline (why is in Table 3 baseline-6L worse than baseline-4L?) for which the reason could be a multitude of issues probably related to hyper-parameter tuning. What is also missing is a an analysis of the negative consequences of this technique -- for example, doesn't the number of parameters increase with the depth of the network because of the concatenation? Also, it would have been good to see more experiments with smaller baseline networks as well to match the smaller DenseNet networks in Table 1 and 2. Finally, the writing of the paper could be improved a lot: The basic idea is not well described (however, many times repeated) and the grammar is often wrong and also there are some typos. ","[4, 6, 5]","[' Ok but not good enough - rejection', ' Marginally above acceptance threshold', ' Marginally below acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer finds the technique ""OK"" but ""not really novel enough"". They also point out potential flaws in the baseline comparisons and lack of analysis on negative consequences. While they see the work as potentially interesting, their criticism leans towards the negative. The language used is quite direct and critical, particularly phrases like ""seems more like a relatively minor architecture tweak"" and ""The writing [...] could be improved a lot"". However, it doesn't contain personal attacks or overly harsh language.",-20.0,20.0
LEAP: Learning Embeddings for Adaptive Pace,"['Vithursan Thangarasa', 'Graham W. Taylor']",Reject,2018,"[1, 15]","[6, 20]","[9, 174]","[3, 78]","[6, 77]","[0, 19]","While the idea is novel and I do agree that I have not seen other works along these lines there are a few things that are missing and hinder this paper significantly.

1. There are no quantitative numbers in terms of accuracy improvements, overhead in computation in having two networks.
2. The experiments are still at the toy level, the authors can tackle more challenging datasets where sampling goes from easy to hard examples like birdsnap. MNIST, FashionMNIST and CIFAR-10 are all small datasets where the true utility of sampling is not realized. Authors should be motivated to run the large scale experiments.

","[4, 6, 3]","[' Ok but not good enough - rejection', ' Marginally above acceptance threshold', ' Clear rejection']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review acknowledges the novelty of the paper's idea, which indicates a positive sentiment. However, it also points out significant weaknesses in the lack of quantitative results and the use of limited datasets. The language, while direct, maintains a professional and respectful tone.",-10.0,50.0
The Manifold Assumption and Defenses Against Adversarial Perturbations,"['Xi Wu', 'Uyeong Jang', 'Lingjiao Chen', 'Somesh Jha']",Reject,2018,"[11, 2, 6, 27]","[16, 7, 11, 32]","[50, 8, 33, 301]","[19, 4, 15, 175]","[22, 4, 17, 88]","[9, 0, 1, 38]","The authors argue that ""good"" classifiers naturally represent the classes in a classification as well-separated manifolds, and that adversarial examples are low-confidence examples lying near to one of these manifolds. The authors suggest ""fixing"" adversarial examples by projecting them back to the manifold, essentially by finding a point near the adversarial example that has high confidence.

There are numerous issues here, which taken together, make the whole story pretty unconvincing.

The term ""manifold"" is used very sloppily. To be fair, this is unfortunately common in modern machine learning. An actual manifold is a specific mathematical structure with specific properties. In ML, what is generally hypothesized is that the data (often per class) lives ""near"" to some ""low-dimensional"" structure. In this paper, even the low-dimensionality isn't used --- the ""manifold assumption"" is used as a stand-in for ""the regions associated with different classes are well-separated."" (This is partially discussed in Section 6, where the authors point out correctly that the same defense as used here could be used with a 1-nn model.) This is fine as far as it goes, but the paper refs Basri & Jacobs 2016 multiple times as if it says anything relevant about this paper: Basri & Jacobs is specifically about the ability of deep nets to fit data that falls on (actual, mathematical) manifolds. This reference doesn't add much to the present story.

The essential argument of the paper rests on the ""Postulate: (A good model) F is confident on natural points drawn from the manifolds, but has low confidence on points outside of the manifolds."" 

This postulate is sloppy and speculative. For instance, taken in its strong form, if believe the postulate, then a good model:
1. Can classify all ""natural points"" from all classes with 100% accuracy.
2. Can detect adversarial points with 100% accuracy because all high-confidence points are correct classifications and all low-confidence points are adversarial.
3. All adversarial examples will be low-confidence.

Point 1 makes it clear that no good model F fully satisfying the postulate exists --- models never achieve 100% accuracy on difficult real-world distributions. But the method for dealing with adversarial examples seems to require Points 2 and 3 being true.

To be fair, the paper more-or-less admits that how true these points are is not known and is important. Nevertheless, I think this paper comes pretty close to arguing something that I *think* is not true, and doesn't do much to back up its argument. Because of the quality of the writing (generally sloppy), it's hard to tell, but I believe the authors are basically arguing that:
a. You can generally easily detect adversarial points because they are low confidence.
b. If you go through a procedure to find a point near your adversarial point that is high-confidence, you'll get the ""correct"" (or perhaps ""original"") class back.

I think b follows from a, but a is extremely suspect. I do not personally work in adversarial examples, and briefly looking at the literature, it seems that most authors *do* focus on how something is classified and not its confidence, but I don't think it's *that* hard to generate high-confidence adversarial examples. Early work by Goodfellow et al. (""Explaining and Harnessing Adversarial Examples"", Figure 1, shows an example where the incorrect classification has very high confidence. The present paper only uses Carlini-Wagner attacks. From a read of Carlini-Wagner, it seems they are heavily concerned with finding *minimal* perturbations to achieve a given misclassification; this will of course produce low-confidence adversaries, but I see no reason why this is a general property of all adversarial examples.

The experiments are weak. I applaud the authors for mentioning the experiments are very preliminary, but that doesn't make them any less weak. 

What are we to make of the one image discussed at the end of Section 5 and shown in Figure 1? The authors note that the original image gives low-confidence for the correct class. (Does this mean that the classifier isn't ""good""? Is it evidence against some kind of manifold assumption?) The authors note the adversarial category has significantly higher confidence, and say ""in this case, it seems that it is the vagueness of the signals/data that lead to a natural difficulty."" But the signals and data are ALWAYS vague. If they weren't, machine learning would be easy. This paper proposes something, looks at a tiny number of examples, and already finds a counterexample to the theory. What's the evidence *for* the theory? 

A lot of writing is given over to how this method is ""semantic"", and I just don't buy it. The connection to manifolds is weak. The basic argument here is really ""(1) If our classifiers produce smooth well-separated high-confidence regions, (2) then we can detect adversaries because they're low-confidence, and (3) we can correct adversaries by projecting them back to high-confidence."" (1) seems vastly unlikely to me based on all my experience: neural nets often get things wrong, they often get things wrong with high confidence, and when they're right, the confidence is at least sometimes low. The authors use a sloppy postulate about good models and so could perhaps argue I've never seen a good model, but the methods of this paper require a good model. (2) seems to follow logically from (1). (3) is also suspect --- perturbations which are *minimal* can be corrected as this paper does (and Carlini-Wagner attacks are minimal by design), but there's no reason to expect general perturbations to be minimal.

The writing is poor throughout. It's generally readable, but the wordings are often odd, and sometimes so odd it's hard to tell what was meant. For instance, I spent awhile trying to decide whether the authors assumed common classifiers are ""good"" (according to the postulate) or whether this paper was about a way to *make* classifiers good (I eventually decided the former).","[3, 5, 4]","[' Clear rejection', ' Marginally below acceptance threshold', ' Ok but not good enough - rejection']","[3, 3, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review is highly critical of the paper, pointing out numerous flaws in the arguments, methodology, and writing. The reviewer finds the core postulate ""sloppy and speculative,"" the experiments ""weak,"" and the writing ""poor throughout."" The reviewer uses strong negative language like ""unconvincing,"" ""sloppy,"" ""suspect,"" and ""doesn't buy it."" ",-80.0,20.0
Understanding Local Minima in Neural Networks by Loss Surface Decomposition,"['Hanock Kwak', 'Byoung-Tak Zhang']",Reject,2018,"[3, 29]","[7, 34]","[11, 309]","[3, 190]","[8, 55]","[0, 64]","The authors propose investigating regions of the the parameter space under which the activations (over the entire training data set) remain unchanged.  They conjecture, and then attempt to argue for a simple network, that, over these regions, the loss function exhibits nice properties:  all local minima are global minima, all other local optima are saddle points, and the function is neither convex nor concave on these regions.  The proof of this statement seems relatively straightforward and appears to be correct.  Unfortunately it only applies to a special case.  Second, the authors argue that the loss function for their simple network has poor local minima.  Finally, the authors conclude with a simple set of experiments exploring the accuracy of random activations.  Overall, I found the main idea of the paper relatively straightforward, but the presentation is a bit awkward in places.

I think the work is heading in an interesting direction, but I found it somewhat incremental.  It's nice to know that the loss function (squared loss in this case) has these properties, but as there are exponentially many regions corresponding to the different activations, it is unclear what the practical consequences of these theoretical observations are.  Could the authors elaborate on this?

Another question:  is it really true that the non-differentiability of the functions involved creates significant issues in practice  (not theoretically) - isn't the set of all points with this property of measure zero?
","[5, 4, 4]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']",The reviewer acknowledges the interesting direction and straightforward proof but finds the work incremental and has reservations about its practical implications. The reviewer poses critical questions but maintains a generally neutral tone with suggestions for improvement.,10.0,50.0
On Convergence and Stability of GANs,"['Naveen Kodali', 'James Hays', 'Jacob Abernethy', 'Zsolt Kira']",Reject,2018,"[2, 30, 13, 15]","[1, 35, 18, 20]","[1, 121, 102, 130]","[0, 65, 55, 54]","[1, 43, 40, 71]","[0, 13, 7, 5]","This paper addresses the well-known stability problem encountered when training GANs. As many other papers, they suggest adding a regularization penalty on the discriminator which penalizes the gradient with respect to the data, effectively linearizing the data manifold.

Relevance: Although I think some of the empirical results provided in the paper are interesting, I doubt the scientific contribution of this paper is significant. First of all, the penalty the author suggest is the same as the one suggest by Gulrajani for Wasserstein GAN (there the motivation behind this penalty comes from the optimal transport plan). In this paper, the author apply the same penalty to the GAN objective with the alternative update rule which is also a lower-bound for the Wasserstein distance.

Justification: The authors justify the choice of their regularization saying it linearizes the objective along the data manifold and claim it reduces the number of non-optimal fixed points. This might be true in the data space but the GAN objective is optimized over the parameter space and it is therefore not clear to me their argument hold w.r.t to the network parameters. Can you please comment on this?

Regularizing the generator: Can the authors motivate their choice for regularizing the discriminator only, and not the generator? Following their reasoning of linearizing the objective, the same argument should apply to the generator.

Comparison to existing work: This is not the first paper that suggests adding a regularization. Given that the theoretical aspect of the paper are rather weak, I would at least expect a comparison to existing regularization methods, e.g.
Stabilizing training of generative adversarial networks through regularization. NIPS, 2017

Choice of hyper-parameters: The authors say that the suggested value for lambda is 10. Can you comment on the choice of this parameter and how it affect the results? Have you tried  annealing lambda? This is a common procedure in optimization (see e.g. homotopy or continuation methods).

Bogonet score: I very much like the experiment where the authors select 100 different architectures to compare their method against the vanilla GAN approach. I here have 2 questions:
- Did you do a deeper examination of your results, e.g. was there some architectures for which none of the method performed well?
- Did you try to run this experiment on other datasets?
","[4, 3, 5]","[' Ok but not good enough - rejection', ' Clear rejection', ' Marginally below acceptance threshold']","[5, 3, 2]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']","The review is critical of the paper's novelty and significance, pointing out similarities to existing work and raising concerns about the theoretical justification. The reviewer acknowledges some interesting empirical results but suggests the theoretical contributions are weak. The tone is direct and questioning but remains professional and within the bounds of academic discourse.",-30.0,50.0
Learning temporal evolution of probability distribution with Recurrent Neural Network,"['Kyongmin Yeo', 'Igor Melnyk', 'Nam Nguyen', 'Eun Kyung Lee']",Reject,2018,"[9, 6, 14, 9]","[13, 11, 19, 13]","[20, 46, 67, 30]","[4, 19, 43, 17]","[10, 23, 12, 0]","[6, 4, 12, 13]","Interesting ideas that extend LSTM to produce probabilistic forecasts for univariate time series, experiments are okay. Unclear if this would work at all in higher-dimensional time series. It is also unclear to me what are the sources of the uncertainties captured.


The author proposed to incorporate 2 different discretisation techniques into LSTM, in order to produce probabilistic forecasts of univariate time series. The proposed approach deviates from the Bayesian framework where there are well-defined priors on the model, and the parameter uncertainties are subsequently updated to incorporate information from the observed data, and propagated to the forecasts. Instead, the conditional density p(y_t|y_{1:t-1|, \theta}) was discretised by 1 of the 2 proposed schemes and parameterised by a LSTM. The LSTM was trained using discretised data and cross-entropy loss with regularisations to account for ordering of the discretised labels. Therefore, the uncertainties produced by the model appear to be a black-box. It is probably unlikely that the discretisation method can be generalised to high-dimensional setting?

Quality: The experiments with synthetic data sufficiently showed that the model can produce good forecasts and predictive standard deviations that agree with the ground truth. In the experiments with real data, it's unclear how good the uncertainties produced by the model are. It may be useful to compare to the uncertainty produced by a GP with suitable kernels. In Fig 6c, the 95pct CI looks more or less constant over time. Is there an explanation for that?

Clarity: The paper is well-written. The presentations of the ideas are pretty clear.

Originality: Above average. I think the regularisation techniques proposed to preserve the ordering of the discretised class label are quite clever.

Significance: Average. It would be excellent if the authors can extend this to higher dimensional time series.

I'm unsure about the correctness of Algorithm 1 as I don't have knowledge in SMC.","[6, 5, 6]","[' Marginally above acceptance threshold', ' Marginally below acceptance threshold', ' Marginally above acceptance threshold']","[2, 4, 4]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer provides a generally positive assessment, acknowledging the interesting ideas and clever techniques. While they have some reservations about the applicability to higher dimensions and the black-box nature of uncertainty, they find the experiments with synthetic data convincing and the paper well-written. The reviewer's direct questioning suggests areas for improvement rather than harsh criticism.",50.0,70.0
DNN Representations as Codewords: Manipulating Statistical Properties via Penalty Regularization,"['Daeyoung Choi', 'Changho Shin', 'Hyunghun Cho', 'Wonjong Rhee']",Reject,2018,"[1, 1, 5, 19]","[4, 6, 10, 24]","[8, 6, 8, 58]","[3, 2, 3, 22]","[3, 4, 3, 21]","[2, 0, 2, 15]","This paper presents a set of regularizers which aims for manipulating the statistical properties like sparsity, variance and covariance. While some of the proposed regularizers are applied to weights, most are applied to hidden representations of neural networks. Class-wise regularizations are also investigated for the purpose of fine-grained control of statistics within each class. Experiments over MNIST, CIFAR10 and CIFAR100 demonstrate the usefulness of this technique.

The following related work also studied the regularizations on hidden representations which are motivated from clustering perspective and share some similarities with the proposed one. It would be great to discuss the relationship.

Liao, R., Schwing, A., Zemel, R. and Urtasun, R., 2016. Learning deep parsimonious representations. NIPS.

Pros:
(1) The paper is clearly written.

(2) The visualizations of hidden activations are very helpful in understanding the effect of different regularizers.

(3) The proposed regularizations are simple and computationally efficient.

Cons:
(1) The novelty of the paper is limited as most of the proposed regularizers are more or less straightforward modifications over DeCov.

(2) When we manipulate the statistics of representations we aim for something, like improving generalization, interpretability. But as pointed out by authors, improvement of generalization performance is not the main focus. I also do not find significant improvement from all experiments. Then the question is what is the main benefit of manipulating various statistics? 

I have an additional question as below:
In measuring the ratio of dead units, I notice authors using the criterion of “not activated on all classes”. However, do you check this criterion over the whole epoch or just some mini-batches?

Overall, I think the paper is technically sound. But the novelty and significance are a bit unsatisfactory. I would like to hear authors’ feedback on the issues I raised.
","[5, 5, 5]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[5, 4, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer acknowledges the paper's clarity, helpful visualizations, and the simplicity and efficiency of the proposed regularizations. These aspects contribute to the positive sentiment. However, the reviewer also points out limitations in novelty, stating that the regularizers are ""more or less straightforward modifications over DeCov."" The lack of significant improvement in experiments and the unclear benefit of manipulating statistics further dampen the enthusiasm.  The reviewer's tone remains professional and polite throughout, engaging with the authors' work constructively by asking clarifying questions and seeking further elaboration on the raised issues. There are no instances of harsh criticism or disrespectful language. ",40.0,80.0
Sequence Transfer Learning for Neural Decoding,"['Venkatesh Elango*', 'Aashish N Patel*', 'Kai J Miller', 'Vikash Gilja']",Reject,2018,"[3, 2, 14, 13]","[3, 7, 18, 18]","[2, 5, 24, 30]","[2, 2, 9, 20]","[0, 1, 0, 0]","[0, 2, 15, 10]","This work addresses brain state  decoding (intent to move) based on intra-cranial ""electrocorticography (ECoG) grids"". ECoG signals are generally of much higher quality than more conventional EEG signals acquired on the skalp, hence it appears meaningful to invest significant effort to decode.  
Preprocessing is only descibed in a few lines in Section 2.1, and the the feature space is unclear (number of variables etc)

Linear discriminants, ""1-state and 2-state"" hidden markov models, and LSTMs are considered for classification (5 classes, unclear if prior odds are uniform). Data involves multiple subjects (4 selected from a larger pool). Total amount of data unclear. ""A validation set is not used due to the limited data size.""  The LSTM setup and training follows conventional wisdom.
""The model used for our analyses was constructed with 100 hidden units with no performance gain identified using larger or stacked networks.""
A simplistic but interesting  transfer scheme is proposed amounting to an affine transform of features(??) - the complexity of this transform is unclear.

While limited novelty is found in the methodology/engineering - novelty being mainly related to the affine transfer mechanism, results are disappointing.  
The decoding performance of the LSTMs does not convincingly exceed that of the simple baselines. 

When analyzing the transfer mechanism only the LSTMs are investigated and it remains unclear how well trans works.

There is an interesting visualization (t-SNE) of the latent representations. But very limited discussion of what we learn from it, or how such visualization could  be used to provide neuroscience insights.

In the discussion we find the claim: ""In this work, we have shown that LSTMs can model the variation within a neural sequence and are a good alternative to state-of-the-art decoders.""  I fail to see how it can be attractive to obtain similar performance with a model of 100x (?) the complexity



","[3, 4, 6]","[' Clear rejection', ' Ok but not good enough - rejection', ' Marginally above acceptance threshold']","[4, 5, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review starts with a neutral statement about the relevance of the research topic. However, it criticizes methodological choices (e.g., lack of validation set, unclear feature space) and questions the novelty and significance of the findings. The reviewer finds the results ""disappointing"" and criticizes the authors' claim about the superiority of LSTMs. The language, while direct, remains within the bounds of professional academic discourse.",-30.0,60.0
Towards Interpretable Chit-chat: Open Domain Dialogue Generation with Dialogue Acts,"['Wei Wu', 'Can Xu', 'Yu Wu', 'Zhoujun Li']",Reject,2018,"[27, 14, 'no_match', 21]","[32, 19, 'no_match', 26]","[733, 143, 'no match', 69]","[358, 62, 'no match', 44]","[57, 52, 'no match', 6]","[318, 29, 'no match', 19]","The topic discussed in this paper is interesting. Dialogue acts (DAs; or some other semantic relations between utterances) are informative to increase the diversity of response generation. It is interesting to see how DAs are used for conversational modeling, however this paper is difficult for me to follow. For example:

1) the caption of section 3.1 is about supervised learning, however the way of describing the model in this section sounds like reinforcement learning. Not sure whether it is necessary to formulate the problem with a RL framework, since the data have everything that the model needs as for a supervised learning.
2) the formulation in equation 4 seems to be problematic
3) ""simplify pr(ri|si,ai) as pr(ri|ai,ui−1,ui−2) since decoding natural language responses from long conversation history is challenging"" to my understanding, the only difference between the original and simplified model is the encoder part not the decoder part. Did I miss something?
4) about section 3.2, again I didn't get whether the model needs RL for training.
5) ""We train m(·, ·) with the 30 million crawled data through negative sampling."" not sure I understand the connection between training $m(\cdot, \cdot)$ and the entire model.
6) the experiments are not convincing. At least, it should show the generation texts were affected about DAs in a systemic way. Only a single example in table 5 is not enough.","[4, 7, 7]","[' Ok but not good enough - rejection', ' Good paper, accept', ' Good paper, accept']","[5, 3, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer starts with a mildly positive note, acknowledging the interesting topic and approach. However, the tone quickly shifts as the reviewer lists multiple concerns and points out areas of confusion. The numerous issues raised, particularly the questioning of fundamental aspects like the use of RL and clarity of the model, suggest a negative sentiment overall. While the language is not outright rude, the directness in pointing out flaws and lack of clarity leans towards the less polite side.",-30.0,-10.0
3C-GAN: AN CONDITION-CONTEXT-COMPOSITE GENERATIVE ADVERSARIAL NETWORKS FOR GENERATING IMAGES SEPARATELY,"['Yeu-Chern Harn', 'Vladimir Jojic']",Reject,2018,"[4, 15]","[5, 17]","[5, 27]","[0, 19]","[1, 3]","[4, 5]","Summary: This paper studied the conditional image generation with two-stream generative adversarial networks. More specifically, this paper proposed an unsupervised learning approach to generate (1) foreground region conditioned on class label and (2) background region without semantic meaning in the label. During training, two generators are competing against each other to hallucinate foreground region and background region with a physical gating operation. An auxiliary “label difference cost” was further introduced to encourage class information captured by the foreground generator. Experiments on MNIST, SVHN, and CelebA datasets demonstrated promising generation results with the unsupervised two-stream generation pipeline.

== Novelty/Significance ==
Controllable image generation is an important task in representation learning and computer vision. I also like the unsupervised learning through gating function and label difference cost. However, considering many other related work mentioned by the paper, the novelty in this paper is quite limited. For example, layered generation (Section 2.2.1) has been explored in Yan et al 2016 (VAEs) and Vondrick et al 2016 (GANs).

== Detailed comments ==
The proposed two-stream model is developed with the following two assumptions: (1) Single object in the scene; and (2) Class information is provided for the foreground/object region. Although the proposed method learns to distinguish foreground and background in an unsupervised fashion, it is limited in terms of applicability and generalizability. For example, I am not convinced if the two-stream generation pipeline can work well on more challenging datasets such as MS-COCO, LSUN, and ImageNet. 

Given the proposed method is controllable image generation, I would assume to see the following ablation studies: keeping two latent variables from (z_u, z_l, z_v) fixed, while gradually changing the value of the other latent variable. However, I didn’t see such detailed analysis as in the other papers on controllable image generation.

In Figure 7 and Figure 10, the boundary between foreground and background region is not very sharp. It looks like equation (5) and (6)  are insufficient for foreground and background separation (triplet/margin loss could work better). Also, in CelebA experiment, it is not a well defined experimental setting since only binary label (smiling/non-smiling) is conditioned. Is it possible to use all the binary attributes in the dataset.

Also, please either provide more qualitative examples or provide some type of quantitative evaluations (through user study , dataset statistics, or down-stream recognition tasks).

Overall, I believe the paper is interesting but not ready for publication. I encourage authors to investigate (1) more generic layered generation process and (2) better unsupervised boundary separation. Hopefully, the suggested studies will improve the quality of the paper in the future submission.

== Presentation ==
The paper is readable but not well polished. 

-- In Figure 1, the “G1” on the right should be “G2”;
-- Section 2.2.1, “X_f” should be “x_f”;
-- the motivation of having “z_v” should be introduced earlier;
-- Section 2.2.4, please use either “alpha” or “\alpha” but not both;
-- Section 3.3, the dataset information is incorrect: “20599 images” should be “202599 images”;

Missing reference:
-- Neural Face Editing with Intrinsic Image Disentangling, Shu et al. In CVPR 2017.
-- Domain Separation Networks, Bousmalis et al. In NIPS 2016.
-- Unsupervised Image-to-Image Translation Networks, Liu et al. In NIPS 2017.
","[5, 4, 4]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[5, 5, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer acknowledges the paper's interesting aspects (unsupervised learning, gating function) but expresses reservations about its novelty and applicability. They find limitations in the assumptions, lack of ablation studies, and quality of results. While encouraging further investigation, the overall tone suggests the paper is not ready for publication. The reviewer provides constructive criticism with specific recommendations, indicating a neutral-to-slightly negative sentiment. The language remains polite and professional throughout.",-20.0,60.0
On the difference between building and extracting patterns: a causal analysis of deep generative models.,"['Michel Besserve', 'Dominik Janzing', 'Bernhard Schoelkopf']",Reject,2018,"[12, 18, 24]","[17, 23, 29]","[42, 137, 777]","[16, 58, 380]","[19, 50, 286]","[7, 29, 111]","This paper examines the nature of convolutional filters in the encoder and a decoder of a VAE, and a generator and a discriminator of a GAN. The authors treat the inputs (X) and outputs (Y) of each filter throughout each step of the convolving process as a time series, which allows them to do a Discrete Time Fourier Transform analysis of the resulting sequences. By comparing the power spectral density of the input and the output, they get a Spectral Dependency Ratio (SDR) ratio that characterises a filter as spectrally independent (neutral), correlating (amplifies certain frequencies), or anti-correlating (dampens frequencies). This analysis is performed in the context of the Independence of Cause and Mechanism (ICM) framework. The authors claim that their analysis demonstrates a different characterisation of the inference/discriminator and generative networks in VAE and GAN, whereby the former are anti-causal and the latter are causal in line with the ICM framework. They also claim that this analysis can be used to improve the performance of the models.

Pros:
-- SDR characterisation of the convolutional filters is interesting
-- The authors show that filters with different characteristics are responsible for different aspects of image modelling

Cons:
-- The authors do not actually demonstrate how their analysis can be used to improve VAEs or GANs
-- Their proposed SDR analysis does not actually find much difference between the generator and the discriminator of the GAN 
-- The clarity of the writing could be improved (e.g. the discussion in section 3.1 seems inaccurate in the current form). Grammatical and spelling mistake are frequent. More background information could be helpful in section 2.2. All figures (but in particular Figure 3) need more informative captions
-- The authors talk a lot about disentangling in the introduction, but this does not seem to be followed up in the rest of the text. Furthermore, they are missing a reference to beta-VAE (Higgins et al, 2017) when discussing VAE-based approaches to disentangled factor learning


In summary, the paper is not ready for publication in its current form. The authors are advised to use the insights from their proposed SDR analysis to demonstrate quantifiable improvements the VAEs/GANs.","[2, 7, 7]","[' Strong rejection', ' Good paper, accept', ' Good paper, accept']","[4, 2, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer acknowledges the interesting aspects of the paper and highlights its potential but also points out significant shortcomings. They find the core analysis interesting but criticize the lack of practical application and clarity in writing. The reviewer suggests improvements and further work, indicating a mixed sentiment leaning towards the negative due to the need for major revisions. The language used is polite, providing constructive criticism and suggestions for improvement rather than harsh negativity.",-25.0,75.0
Evaluation of generative networks through their data augmentation capacity,"['Timothée Lesort', 'Florian Bordes', 'Jean-Francois Goudou', 'David Filliat']",Reject,2018,"[2, 3, 4, 20]","[6, 8, 7, 25]","[30, 14, 12, 112]","[6, 3, 6, 61]","[21, 10, 3, 33]","[3, 1, 3, 18]","The authors propose to evaluate how well generative models fit the training set by analysing their data augmentation capacity, namely the benefit brought by training classifiers on mixtures of real/generated data, compared to training on real data only. Despite the the idea of exploiting generative models to perform data augmentation is interesting, using it as an evaluation metric does not constitute an innovative enough contribution. 

In addition, there is a fundamental matter which the paper does not address: when evaluating a generative model, one should always ask himself what purpose the data is generated for. If the aim is to have realistic samples, a visual turing test is probably the best metric. If instead the purpose is to exploit the generated data for classification, well, in this case an evaluation of the impact of artificial data over training is a good option.

PROS:
The idea is interesting. 

CONS:
1. The authors did not relate the proposed evaluation metric to other metrics cited (e.g., the inception score, or a visual turing test, as discussed in the introduction). It would be interesting to understand how the different metrics relate. Moreover, the new metric is introduced with the following motivation “[visual Turing test and Inception Score] do not indicate if the generator collapses to a particular mode of the data distribution”. The mode collapse issue is never discussed elsewhere in the paper. 

2. Only two datasets were considered, both extremely simple: generating MNIST digits is nearly a toy task nowadays. Different works on GANs make use of CIFAR-10 and SVHN, since they entail more variability: those two could be a good start. 

3. The authors should clarify if the method is specifically designed for GANs and VAEs. If not, section 2.1 should contain several other works (as in Table 1). 

4. One of the main statements of the paper “Our approach imposes a high entropy on P(Y) and gives unbiased indicator about entropy of both P(Y|X) and P(X|Y)” is never proved, nor discussed.

5. Equation 2 (the proposed metric) is not convincing: taking the maximum over tau implies training many models with different fractions of generated data, which is expensive. Further, how many tau’s one should evaluate? In order to evaluate a generative model one should test on the generated data only (tau=1) I believe. In the worst case, the generator experiences mode collapse and performs badly. Differently, it can memorize the training data and performs as good as the baseline model. If it does actual data augmentation, it should perform better.

6. The protocol of section 3 looks inconsistent with the aim of the work, which is to evaluate data augmentation capability of generative models. In fact, the limit of training with a fixed dataset is that the model ‘sees’ the data multiple times across epochs with the risk of memorizing. In the proposed protocol, the model ‘sees’ the generated data D_gen (which is fixed before training) multiple time across epochs. This clearly does not allow to fully evaluate the capability of the generative model to generate newer and newer samples with significant variability.


Minor: 
Section 2.2 might be more readable it divided in two (exploitation and evaluation).   
","[3, 5, 3]","[' Clear rejection', ' Marginally below acceptance threshold', ' Clear rejection']","[5, 3, 5]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review is predominantly negative. While it acknowledges the interesting idea, it heavily criticizes the paper for lacking novelty, theoretical grounding, and a robust experimental setup. The reviewer also points out several inconsistencies and limitations in the proposed method. The language, however, remains largely polite and professional, focusing on constructive criticism rather than harsh language.",-40.0,60.0
Learning non-linear transform with discriminative and minimum information loss priors,"['Dimche Kostadinov', 'Slava Voloshynovskiy']",Reject,2018,"[5, 21]","[8, 26]","[42, 230]","[26, 160]","[16, 43]","[0, 27]","Overview:
This paper proposes a method for learning representations using a “non-linear transform”. Specifically, the approach is based on the form: Y =~ AX, where X is the original data, A is a projection matrix, and Y is the resulting representation. Using some assumptions, and priors/regularizers on Y and A, a joint objective is derived (eq. 10), and an alternating optimization algorithm is proposed (eq. 11 and 14). Both objective and algorithm use approximations due to hardness of the problem. Theoretical and empirical results on the quality and properties of the representation are presented.
Disclaimer: this is somewhat outside my area of expertise, so this is a rather high-level review. I have not thoroughly checked proofs and claims.

Comments:
-I found the presentation quality to be rather poor, making it hard to fully understand and evaluate the approach. In particular, the motivation and approach are not clear (sec. 1.2), making it hard to understand the proposed method. There is no explicit formulation, instead there are references to other models (e.g., sparsifying transform model) and illustrative figures (fig. 1 and 2). Those are useful following a formal definition, but cannot replace it. The separation between positive and negative elements of the representation is not motivated and explained in a footnote although it seems central to the proposed approach.
- The paper is 17 pages long (24 pages with the appendix), so I had to skim through some parts. Due to the extensive scope, perhaps a journal submission would be more appropriate.

Minors:
- Vu & Monga 2016b and 2016c are the same.
- p. 1: meaner => manner
- p. 1: refereed => referred
- p. 1: “a structural constraints”; p. 2: “a low rank constraints”, “a pairwise constraints”; p. 4: “a similarity concentrations”, “a numerical experiments”, and others...
- p. 2, 7: therms => terms
- p. 2: y_{c_1,k_2} => y_{c_1,k_1}?
- p. 3, 4: “a the”
- p. 5: “an parametric”
- p. 8: ether => either
- Other typos… the paper needs proofreading.
","[5, 4, 5]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[1, 2, 2]","["" The reviewer's evaluation is an educated guess"", ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']","The review is critical of the paper, pointing out issues in presentation, clarity, and length. However, it maintains a professional and constructive tone, suggesting improvements and not resorting to personal attacks. The reviewer acknowledges their limited expertise in the area, which further contributes to the neutral politeness.",-50.0,50.0
Image Transformer,"['Ashish Vaswani', 'Niki Parmar', 'Jakob Uszkoreit', 'Noam Shazeer', 'Lukasz Kaiser']",Reject,2018,"[13, 2, 11, 9, 14]","[17, 6, 15, 13, 18]",['skipped'],['skipped'],['skipped'],['skipped'],"In this paper the authors propose an autoregressive image generation model that incorporates a self-attention mechanism. The latter is inspired by the work of [Vaswani et al., 2016], which was proposed for sequences and is extended to 2D images in this work. The authors apply their model to super-resolution of face images, as well as image completion (aka inpainting) and generation, both unconditioned or conditioned on one of a small number of image classes from the CIFAR10 and ImageNet datasets. The authors evaluate their method in terms of visual quality of their generated images via an Amazon Mechanical Turk survey and quantitatively by reporting slightly improved log-likelihoods. 

While the paper is well written, the motivation for combining self-attention and autoregressive models remains unclear unfortunately, even more though as the reported quantitative improvement in terms of log-likelihood are only marginal. The technical exposition is at times difficult to follow with some design decisions of the network layout being quite ad hoc and not well motivated. Expressing the involved operations in mathematical terms would help comprehend some of the technical details and add to the reproducibility of the proposed model. 

Another concern is the experimental evaluation. While the reported log-likelihoods are only marginally better, the authors report a significant boost in how often humans are fooled by the generated images. While the image generation is conditioned on the low-resolution input, the workers in the Amazon Mechanical Turk study get to see the high-resolution images only. Of course, a human observer would pick the one image out of the two shown images which is more realistic although it might have nothing to do with the input image, which seems wrong. Instead, the workers should see the low-res input image and then have to decide which high-res image seems a better match or more likely.

Overall, the presented work looks quite promising and an interesting line of research. However, in its present form the manuscript doesn't seem quite ready for publication yet. Though, I would strongly encourage the authors to make the exposition more self-contained and accessible, in particular through rigorous mathematical terms, which would help comprehend the involved operations and help understand the proposed mechanism.

Additional comments:
- Abstract: ""we also believe to look pretty cool"". Please re-consider the wording here. Generating ""pretty cool"" images  should not be the goal of a scientific work.
","[5, 6, 3]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Clear rejection']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer acknowledges the paper's potential and finds the research direction interesting. However, they express several concerns, including unclear motivation, difficult-to-follow technical exposition, and questionable experimental evaluation. The reviewer suggests significant revisions are needed before the paper is ready for publication. While the reviewer provides constructive criticism, the overall tone leans towards the critical side, suggesting a sentiment score that is more negative than positive. The language used is professional and polite throughout the review.",-20.0,70.0
Baseline-corrected space-by-time non-negative matrix factorization for decoding single trial population spike trains,"['Arezoo Alizadeh', 'Marion Mutter', 'Thomas Münch', 'Arno Onken', 'Stefano Panzeri']",Reject,2018,"[-3, 'no_match', 2, 11, 24]","[1, 'no_match', 1, 16, 29]","[1, 'no match', 1, 21, 84]","[0, 'no match', 0, 5, 21]","[0, 'no match', 0, 9, 7]","[1, 'no match', 1, 7, 56]","In this contribution, the authors propose an improvement of a tensor decomposition method for decoding spike train. Relying on a non-negative matrix factorization, the authors tackle the influence of the baseline activity on the decomposition. The main consequence is that the retrieved components are not necessarily non-negative and the proposed decomposition rely on signed activation coefficients. An experimental validation shows that for high frequency baseline (> 0.7 Hz), the baseline corrected algorithm yields better classification results than non-corrected version (and other common factorization techniques). 

The objective function is defined with a Frobenius norm, which has an important influence on the obtained solutions, as it could be seen on Figure 2. The proposed method seems to provide a more discriminant factorization than the NMF one, at the expense of the sparsity of spatial and temporal components, impeding the biological interpretability.  A possible solution is to add a regularization term to the objective function to ensure the sparsity of the factorization.","[6, 4, 6]","[' Marginally above acceptance threshold', ' Ok but not good enough - rejection', ' Marginally above acceptance threshold']","[3, 4, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review acknowledges the authors' contribution and its positive aspects: addressing baseline activity influence and achieving better classification results in specific scenarios. However, it also points out a drawback regarding sparsity and interpretability, suggesting a potential improvement. The language is constructive and objective, employing a professional and respectful tone.",60.0,80.0
Toward predictive machine learning for active vision,['Emmanuel Daucé'],Reject,2018,[21],[25],[24],[13],[4],[7],"In this paper, the authors present a computational framework for the active vision problem. Motivating the study biologically, the authors explain how the control policy can be learned to reduce the entropy of the posterior belief, and present an application (MNIST digit classification) to substantiate their proposal.

I am not convinced about the novelty and contribution of the work. The active vision/sensing problem has been well studied and both the information theory and Bayes risk formulations have already been considered in previous works (see Najemnik and Geisler, 2005; Butko and Movellan, 2010; Ahmad and Yu, 2013).

The paper is also rife with spelling mistakes and grammatical errors and needs a thorough revision. Examples: foveate inspection the data (abstract), may allow to (motivation), tu put it clear (motivation), on contrary to animals retina (footnote 1), minimize at most the current uncertainty (perception-driven control), center an keep (fovea-based implementation), degrade te recognition (outlook and perspective). The citations are in non-standard format (section 1.2: Kalman (1960)).

Overall, I think the paper considers an important problem but the contribution to the state of the art is minimal, and editing highly lacking. 

1. J Najemnik and W S Geisler. Optimal eye movement strategies in visual search. Nature, 434(7031):387–91, 2005.
2. N J Butko and J R Movellan. Infomax control of eye movements. IEEE Transactions on Autonomous Mental Development, 2(2):91–107, 2010.
3. S Ahmad and A J Yu. Active sensing as Bayes-optimal sequential decision-making. Uncertainty in Artificial Intelligence, 2013.","[3, 3, 5]","[' Clear rejection', ' Clear rejection', ' Marginally below acceptance threshold']","[5, 4, 2]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']","The reviewer acknowledges the importance of the problem but expresses concerns about the novelty and contribution of the work. They find the paper to be lacking in originality and point out significant editing issues. The language used, while direct, is within the scope of professional academic feedback.",-50.0,20.0
Parametric Information Bottleneck to Optimize Stochastic Neural Networks,"['Thanh T. Nguyen', 'Jaesik Choi']",Reject,2018,"[6, 13]","[9, 18]","[8, 97]","[1, 51]","[4, 37]","[3, 9]","This paper presents a new way of training stochastic neural network following an information relevance/compression framework similar to the Information Bottleneck. A new training objective is defined as a sum of mutual informations (MI) between the successive stochastic hidden layers plus a sum of mutual informations between each layer and the relevance variable. 

The idea is interesting and to my knowledge novel. Experiments are carefully designed and presented in details, however assessing the impact of the proposed new objective is not straightforward. It would have been interesting to compare not only with SFNN but also to a model with the same architecture and same gradient estimator (Raiko et al. 2014) using maximum likelihood. This would allow to disentangle the impact of the learning mechanism from the impact of the learning objective. 

Why is it important to maximise I(X_l, Y) for every layer? Does that impact the MI of the final layer and Y?  

To estimate the MI between a hidden layer and the relevance variable, a multilayer generalisation of the variational bound from Alemi et al. 2016. Computation of the bound requires integration over multiple layers (equation 15). How is this achieved in practice? With high-dimensional hidden layers a Monte-Carlo estimate on the minibatch can be very noisy and the resulting estimation of MI could be poor.

Mutual information between the successive layers is decomposed as an entropy plus a conditional entropy term (eq 17). How is the conditional entropy term estimated? The entropy term is first bounded by conditioning on the previous layer and then estimated using Monte Carlo sampling with a plug-in estimator. Plug-in estimators are known to be inefficient in high dimensions even using a full dataset unless the number of samples is very large. It thus seems challenging to use mini batch MC, how does the mini batch estimation compare to an estimation using the full dataset? What is the variance of the mini batch estimate?

In the related work section, the IB problem can also be solved efficiently for meta-Gaussian distribution as explained in Rey et al. 2012 (Meta-gaussian information bottleneck). 

There is a small typo in (eq 5).
","[6, 4, 4]","[' Marginally above acceptance threshold', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with positive remarks, highlighting the novelty and careful experimental design. However, it then raises several methodological concerns and questions, suggesting potential weaknesses in the evaluation and implementation. The overall tone is critical but professional and aims to improve the paper. Therefore, the sentiment is mildly positive, and the politeness is highly positive.",60.0,80.0
The Variational Homoencoder: Learning to Infer High-Capacity Generative Models from Few Examples,"['Luke Hewitt', 'Andrea Gane', 'Tommi Jaakkola', 'Joshua B. Tenenbaum']",Reject,2018,"[4, 'no_match', 26, 25]","[8, 'no_match', 31, 30]","[3, 'no match', 294, 610]","[2, 'no match', 149, 353]","[1, 'no match', 112, 226]","[0, 'no match', 33, 31]","This paper presents an alternative approach to constructing variational lower bounds on data log likelihood in deep, directed generative models with latent variables. Specifically, the authors propose using approximate posteriors shared across groups of examples, rather than posteriors which treat examples independently. The group-wise posteriors allow amortization of the information cost KL(group posterior || prior) across all examples in the group, which the authors liken to the ""KL annealing"" tricks that are sometimes used to avoid posterior collapse when training models with strong decoders p(x|z) using current techniques for approximate variational inference in deep nets.

The presentation of the core idea is solid, though it did take two read-throughs before the equations really clicked for me. I think the paper could be improved by spending more time on a detailed description of the model for the Omniglot experiments (as illustrated in Figure 3). E.g., explicitly describing how group-wise and per-example posteriors are composed in this model, using Equations and pseudo-code for the main training loop, would have saved me some time. For readers less familiar with amortized variational inference in deep nets, the benefit would be larger.

I appreciate that the authors developed extensions of the core method to more complex group structures, though I didn't find the related experiments particularly convincing. 

Overall, I like this paper and think the underlying group-wise posterior construction trick is worth exploring further. Of course, the elephant in the room is how to determine the groups across which the posteriors can be shared and their information costs amortized.","[7, 5, 6]","[' Good paper, accept', ' Marginally below acceptance threshold', ' Marginally above acceptance threshold']","[4, 5, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer finds the core idea solid and worth exploring further, indicating a positive sentiment. While they have some suggestions for improvement and are not fully convinced by all experiments, their overall tone is constructive and polite.",60.0,70.0
Learning Deep Generative Models With Discrete Latent Variables,"['Hengyuan Hu', 'Ruslan Salakhutdinov']",Reject,2018,"[3, 16]","[8, 21]","[31, 419]","[14, 207]","[16, 201]","[1, 11]","Interesting work, but I’m not convinced by the arguments nor by the experiments. Similar models have been trained before; it’s not clear that the proposed pretraining procedure is a practical step forwards. And quite some decisions seem ad-hoc and not principled. 

Nevertheless, interesting work for everyone interested in RBMs as priors for “binary VAEs”. 

","[4, 5, 4]","[' Ok but not good enough - rejection', ' Marginally below acceptance threshold', ' Ok but not good enough - rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with ""Interesting work, but..."" which suggests a leaning towards the positive initially. However, it quickly transitions into skepticism about the novelty and effectiveness of the work. Phrases like ""not convinced"", ""not clear"", ""ad-hoc and not principled"" express doubt and criticism. While the reviewer acknowledges the work's relevance to a specific niche (""interesting work for everyone interested in RBMs...""), the overall tone conveys reservations rather than outright enthusiasm. The language avoids harsh or disrespectful vocabulary, maintaining a professional and critical tone.",-20.0,60.0
Machine Learning by Two-Dimensional Hierarchical Tensor Networks: A Quantum Information Theoretic Perspective on Deep Architectures,"['Ding Liu', 'Shi-Ju Ran', 'Peter Wittek', 'Cheng Peng', 'Raul Blázquez García', 'Gang Su', 'Maciej Lewenstein']",Reject,2018,"[18, 1, 10, 'no_match', 'no_match', 13, 25]","[23, 6, 14, 'no_match', 'no_match', 18, 29]","[203, 17, 63, 'no match', 'no match', 32, 20]","[92, 0, 29, 'no match', 'no match', 11, 1]","[42, 16, 17, 'no match', 'no match', 4, 8]","[69, 1, 17, 'no match', 'no match', 17, 11]","Authors of this paper derived an efficient quantum-inspired learning algorithm based on a hierarchical representation that is known as tree tensor network, which is inspired by the multipartite entanglement renormalization ansatz approach where the tensors in the TN are kept to be unitary during training. Some observations are: The limitation of learnability of TTN strongly depends on the physical indexes and the geometrical indexes determine how well the TTNs approximate the limit; TTNs exhibit same increase level of abstractions as CNN or DBN; Fidelity and entanglement entropy can be considered as some measurements of the network.

Authors introduced the two-dimensional hierarchical tensor networks for solving image recognition problems, which suits more the 2-D nature of images. In section 2, authors stated that the choice of feature function is arbitrary, and a specific feature map was introduced in Section 4. However, it is not straightforward to connect (10) to (1) or (2). It is better to clarify this connection because some important parameters such as the virtual bond and input bond are related to the complexity of the proposed algorithm as well as the limitation of learnability. For example, the scaling of the complexity O(dN_T(b_v^5 + b_i^4)) is not easy to understand. Is it related to specific feature map? How about the complexity of eigen-decomposition for one tensor at each iterates. And also, whether the tricks used to accelerate the computations will affect the convergence of the algorithm? More details on these problems are required for readers’ better understanding.

From Fig 2, it is difficult to see the relationship between learnability and parameters such input bond and virtual bond because it seems there are no clear trends in the Fig 2(a) and (b) to make any conclusion. It is better to clarify these relationships with either clear explanation or better examples.

From Fig 3, authors claimed that TN obtained the same levels of abstractions as in deep learning. However, from Fig 3 only, it is hard to make this conclusion. First, there are not too many differences from Fig 3(a) to Fig 3(e).  Second, there is no visualization result reported from deep learning on the same data for comparison. Hence, it is not convincing to draw this conclusion only from Fig 3. 

In Section 4.2, what strategy is used to obtain these parameters in Table 1?

In Section 5, it is interesting to see more experiments in terms of fidelity and entanglement entropy.
","[6, 3, 4]","[' Marginally above acceptance threshold', ' Clear rejection', ' Ok but not good enough - rejection']","[3, 2, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is fairly confident that the evaluation is correct']","The review is largely focused on requesting clarifications and expansions on the presented work. The reviewer points out areas where the paper lacks clarity and suggests specific improvements for better reader understanding. While the reviewer doesn't explicitly praise the work, the numerous constructive suggestions indicate they see potential and value in the research. The language used is formal, objective, and devoid of any negativity, suggesting a neutral to slightly positive sentiment overall.",20.0,80.0
Learning to play slot cars and Atari 2600 games in just minutes,"['Lionel Cordesses', 'Omar Bentahar', 'Julien Page']",Reject,2018,"[18, 1, 5]","[18, 3, 5]","[10, 4, 4]","[8, 3, 3]","[0, 0, 0]","[2, 1, 1]","In this paper the authors address the very important challenges of current deep learning approaches, which is that these algorithms typically need an extraordinarily large number of training rounds to learn their strategies.  The authors note that in real life, this type of training will outstrip both the training and time budget of most real world problems.  The solution they propose is to take a high level approach and to learn more like humans do by creating strategies that involve relationships between entities rather than trying to build up strategies from pixels. 
The authors credit their reframing of their approach to AI to the “continental philosophers” (e.g. Heidegger) in opposition to the “analytical philosophers” such as Wittgenstein.  The authors associate current machine learning approaches with the analytic philosophers, based on propositions that are either provably true or untrue and their own approach as in opposition to these, however from my reading of this paper what the authors are saying is that if you start learning with higher level concepts (relationships between entities) rather than doing analysis on low level information such as pixels.   Starting with low level concepts makes learning very difficult at first and leads to a path where many trials are required.  Staring from higher level concepts such as relationships between entities allows learning to happen quickly and in a manner much more similar in nature to what humans actually do.
While the authors bring up many valid points, and in essence I believe that they may be correct, the flaw in this paper is that they do not provide methods for teaching computers to learn these higher level concepts.  The algorithms they present all require human knowledge to be encoded in the algorithms to identify the higher level concepts.  The true power of the deep learning approach is that it can actually learn from low level data, without humans hand crafting the higher level entities on their behalf.

While I agree with Dreyfus that understanding what is important and interesting given a situation would be an incredible boon to any AI algorithm, it remains an unsolved problem as to how to teach a computer to understand what is interesting in a scene with the same intuition that a human has.  In the first experiment the authors need to pre-define the concepts of a straight road and a curved road and identify them for the algorithm.  They also need to tell the algorithm exactly how to count the number of sections that the track has.  In the second experiment, to identify the “Me” in the game, the authors instruct the computer to recognize “me” as the things that move when the controller is activated.  While in some ways this is clever, mimicking what a child might do to see what moves in the world when it issues a command to move from its own brain and thus learning what “me” is, children take year to develop a sense of “self” and part of that is learning that a “concept of self” is an interesting and useful thing to have.  In their work the authors know, from their human intelligence, what are the important concepts in the game (again from a human perspective) and devise simple methods for the computer to learn these.  Again the problem here is that the human has to define the important concepts for the computer and define a specific strategy for the computer to learn to identify these important policies.  Data intensive deep learning algorithms are able to infer strategies without these concepts being defined for them.

This reframing does point out a different and perhaps better path for AI, but it is not entirely new and this paper does not present a method for getting from sensed data to higher level concepts.  For each of the experiments, the strategies used rely on human intuition to define policies.  In the first experiment with slot cars, a human needs to provide n laps of driving to imitate.  The authors identify the “shortest lap” and store it for the “AI” to replay.  The only “learning” is from an optimization that minimizes the difference between the AI’s lap time and the best lap time (tbest) of the human by scaling that recorded sample of the human driving.  This results is a strategy that is essentially just trying to replicate (imitate) what the human is doing will not lead to a generalizable learning strategy that could ever exceed a human example.   This is at best a very limited form of imitation learning.  The learning process for the second example is explained in even less detail.
Overall, this paper presents a different way of thinking about AI, one in which the amount of training time and training data required for learning is greatly reduced, however what is missing Is a generalizable algorithmic strategy for implementing this framework.m  

 



","[2, 3, 3]","[' Strong rejection', ' Clear rejection', ' Clear rejection']","[5, 2, 1]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', "" The reviewer's evaluation is an educated guess""]","The reviewer acknowledges the importance of the paper's aim and finds the proposed approach potentially valid. However, they express significant concerns about the lack of concrete methods for teaching computers higher-level concepts, which forms the core of the authors' argument. The reviewer finds the reliance on human-defined concepts and strategies a major limitation, suggesting the proposed framework, while interesting, is incomplete without a generalizable algorithmic solution.",40.0,70.0
Simple Fast Convolutional Feature Learning,"['David Macêdo', 'Cleber Zanchettin', 'Teresa Ludermir']",Reject,2018,"[1, 16, 29]","[5, 21, 34]","[45, 110, 274]","[22, 72, 182]","[17, 19, 19]","[6, 19, 73]","This paper proposes a fast way to learn convolutional features that later can be used with any classifier. The acceleration of the training comes from a reduced number of training epocs and a specific schedule decay of the learning rate. 
In the evaluation the features are used with support vector machines (SVN) and extreme learning machines on MNIST and CIFAR10/100 datasets.

Pros:
The paper compares different classifiers on three datasets.

Cons:
- Considering an adaptive schedule of the learning decay is common practice in modern machine learning. Showing that by varying the learning rate the authors can reduce the number of training epocs and still obtain good performance is not a contribution and it is actually implemented in most of the recent deep learning libraries, like Keras or Pytorch.
- It is not clear why, once a CNN has been trained, one should want to change the last layer and use a SVN or other classifiers.
- There are many spelling errors
- Comparing CNN based methods with hand-crafted features as in Fig. 1 and Tab.3 is not interesting anymore. It is well known that CNN features are much better if enough data is available.
","[3, 3, 2]","[' Clear rejection', ' Clear rejection', ' Strong rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review acknowledges the work done but points out several significant flaws, indicating that the paper's contribution is limited and some aspects are outdated. The language, while direct, maintains a professional and respectful tone.",-30.0,60.0
"Model Specialization for Inference Via End-to-End Distillation, Pruning, and Cascades","['Daniel Kang', 'Karey Shi', 'Thao Ngyuen', 'Stephanie Mallard', 'Peter Bailis', 'Matei Zaharia']",Reject,2018,"[14, 'no_match', 'no_match', 'no_match', 9, 13]","[19, 'no_match', 'no_match', 'no_match', 13, 18]","[45, 'no match', 'no match', 'no match', 130, 194]","[13, 'no match', 'no match', 'no match', 39, 92]","[22, 'no match', 'no match', 'no match', 39, 65]","[10, 'no match', 'no match', 'no match', 52, 37]","This paper presents three different techniques for model specialization, i.e. adapting a pretrained network to a more specific task and reduce its computational cost while maintaining the performance. The three techniques are distillation, weight pruning and cascades. Evaluation compares how effective each technique is and how they interact with each other. In certain settings the obtained speed-up reaches 5x without loss of accuracy.

Pros:
- The idea of reducing the computational cost of specialized models makes sense.
- In some setting the speed-up can reach more than 5x, which is quite relevant.

Cons:
- The fact that the models are specialized to simpler tasks is not explicitly used in the approach. The authors should test what would happen when using their cascade for classification on all classes of ImageNet for instance. Would it be the gain in speed much lower?
- It is not clear if the distillation on smaller networks is really improving the models accuracy. The authors compared the distilled models with models trained from scratch. There should be an additional experiment with the small models trained on Imagenet first and then fine-tuned to the task. If in that case there is non gain, then, what is the advantage of distilling in these settings? ImageNet annotations need to be used anyway to train the teacher network.
- In section 3.2 it seems that the filters of a CNN are globally ranked based on their average activation values. Those with the lowest average activation will be removed. However, in my understanding, the ranking can work better if performed layer specific and not globally.
- In section 3.4, the title says ""end-to-end specialization pipeline"", but actually, the specialization is done in 3 steps, therefore in my understanding it is not end-to-end.
- There are some spelling errors, for instance in the beginning of section 4.1
- Pruning does not seem to produce much speed-up.
- The experimental part is difficult to read. In particular Fig. 4 should be better explained. There are some symbols in the legend that do not appear in the graph, and others (baselines only) that appear multiple times, but it is not clear what they represent. Also, at the end of the explanation of Fig. 4 the authors mention a gain of 8%, which in my understanding is not really relevant compared with the total speed-up, which can be in the order of 500%

Overall, the idea of model specialization seem interesting. However, in my understanding the main source of speed-up is a cascade approach with a reduced model, in which is not clear how much speed-up is actually due to the specialized task.","[4, 3, 6]","[' Ok but not good enough - rejection', ' Clear rejection', ' Marginally above acceptance threshold']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer provides a mixed opinion, acknowledging the merit of the paper's idea and the significant speed-up achieved in certain settings. However, they also raise several methodological concerns and criticisms regarding the clarity and presentation of the results. The language used is professional and typical of academic peer reviews, directly addressing the authors and providing constructive feedback.",20.0,70.0
Hallucinating brains with artificial brains,"['Peiye Zhuang', 'Alexander G. Schwing', 'Oluwasanmi Koyejo']",Reject,2018,"[0, 12, 10]","[5, 17, 15]","[15, 233, 156]","[7, 129, 71]","[8, 99, 71]","[0, 5, 14]","Quality

This is a very clear contribution which elegantly demonstrates the use of extensions of GAN variants in the context of neuroimaging.

Clarity

The paper is well-written. Methods and results are clearly described. The authors state significant improvements in classification using generated data. These claims should be substantiated with significance tests comparing classification on standard versus augmented datasets.

Originality

This is one of the first uses of GANs in the context of neuroimaging. 

Significance 

The approach outlined in this paper may spawn a new research direction.

Pros

Well-written and original contribution demonstrating the use of GANs in the context of neuroimaging.

Cons

The focus on neuroimaging might be less relevant to the broader AI community.","[8, 6, 5]","[' Top 50% of accepted papers, clear accept', ' Marginally above acceptance threshold', ' Marginally below acceptance threshold']","[5, 4, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review starts with very positive statements about the contribution and clarity of the work. While it mentions a need for additional significance tests, this is presented as a suggestion rather than a harsh criticism. The reviewer also highlights the originality and potential impact of the work. The 'Cons' section is fairly neutral, simply pointing out a potential limitation in scope rather than a flaw in the paper itself.",75.0,80.0
POLICY DRIVEN GENERATIVE ADVERSARIAL NETWORKS FOR ACCENTED SPEECH GENERATION,"['Prannay Khosla', 'Preethi Jyothi', 'Vinay P. Namboodiri', 'Mukundhan Srinivasan']",Reject,2018,"[2, 10, 15, 6]","[4, 15, 20, 7]","[4, 99, 221, 14]","[1, 62, 109, 11]","[2, 33, 90, 3]","[1, 4, 22, 0]","The contributions made by this paper is unclear. As one of the listed contributions, the authors propose using policy gradient. However, in this setting, the reward is a known differentiable function, and the action is continuous, and thus one could simply backpropagate through to get the gradients on the encoder. Also, it seems the reward is not a function of the future actions, which further questions the need for a reinforcement learning formulation.

The paper is written poorly. For instance, I don't understand what this sentence means: ""We condition the latent variables to come from rich distributions"". Observed accent labels are referred to as latent (hidden) variables.

While the independent Wasserstein critic is useful to study whether models are overfitting (by comparing train/heldout numbers), their use for comparing across different model types is not justified. Moreover, since GAN-based methods optimize the Wasserstein distance directly, it cannot serve as a metric to compare GAN-based models with other models.

All of the models compared against do not use accent information during training (table 2), so this is not a fair comparison.

Overall, the paper lacks any novel technical insight, contributions are not explained well, exposition is poor, and the evaluations are invalid.","[3, 4, 5]","[' Clear rejection', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is overwhelmingly negative, using strong language to criticize the paper's methodology, clarity, and overall contribution. Phrases like ""unclear,"" ""poorly written,"" ""not justified,"" ""not a fair comparison,"" ""lacks any novel technical insight,"" ""invalid,"" all point towards a highly critical sentiment. While the reviewer does not resort to personal attacks or disrespectful language, the tone is blunt and directly critical of the work's shortcomings. ",-80.0,-40.0
TOWARDS ROBOT VISION MODULE DEVELOPMENT WITH EXPERIENTIAL ROBOT LEARNING,"['Ahmed A Aly', 'Joanne Bechta Dugan']",Reject,2018,"[2, 35]","[1, 35]","[1, 55]","[1, 32]","[0, 0]","[0, 23]","This work explores some approaches in the object detection field of computer vision: (a) a soft attention map based on the activations on convolutional layers, (b) a classification regarding the location of an object in a 3x3 grid over the image, (c) an autoencoder that the authors claim to be aware of the multiple object instances in the image. These three proposals are presented in a framework of a robot vision module, although neither the experiments nor the dataset correspond to this domain.

From my perspective, the work is very immature and seems away from current state of the art on object detection, both in the vocabulary, performance or challenges. The proposed techniques are assessed in a dataset which is not described and whose results are not compared with any other technique. This important flaw in the evaluation prevents any fair comparison with the state of the art.

The text is also difficult to follow. The three contributions seem disconnected and could have been presented in separate works with a more deeper discussion. In particular, I have serious problems understanding:

1. What is exactly the contribution of the CNN pre-trained with IMageNet when learning the soft-attention maps ? The reference to a GAN architecture seems very forced and out of the scope.

2. What is the interest of the localization network ? The task it addresses seems very simple and in any case it requires a manual annotation of a dataset of objects in each of the predefined locations in the 3x3 grid.

3. The authors talk about an autoencoder architecture, but also on a classification network where the labels correspond to the object count. I could not undertstand what is exactly assessed in this section.

Finally, the authors violate the double-bind review policy by clearly referring to their previous work on Experiental Robot Learning.

I would encourage the authors to focus in one of the research lines they point in the paper and go deeper into it, with a clear understanding of the state of the art and the specific challenges these state of the art techniques may encounter in the case of robotic vision.","[2, 3, 2]","[' Strong rejection', ' Clear rejection', ' Strong rejection']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is highly critical of the work, pointing out major flaws in the methodology, evaluation, and clarity. The reviewer finds the work immature, disconnected, and lacking a clear understanding of the state-of-the-art. The language used, while direct and critical, maintains a professional and academic tone. There's no personal attack on the authors, only a critique of their work.",-75.0,50.0
SHADE: SHAnnon DEcay Information-Based Regularization for Deep Learning,"['Michael Blot', 'Thomas Robert', 'Nicolas Thome', 'Matthieu Cord']",Reject,2018,"[0, 3, 14, 21]","[2, 6, 19, 26]","[2, 11, 124, 282]","[1, 4, 69, 151]","[1, 7, 34, 87]","[0, 0, 21, 44]","the paper adapts the information bottleneck method where a problem has invariance in its structure. specifically, the constraint on the mutual information is changes to one on the conditional  entropy. the paper involves a technical discription how to develop proper estimators for this conditional entropy etc.

this is a nice and intuitive idea. how it interacts with classical regularizers or if it completely dominates classical regularizers would be interesting for the readers.","[7, 5, 4, 4]","[' Good paper, accept', ' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[3, 4, 3, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review starts by summarizing the paper's core idea. It then uses the phrase ""this is a nice and intuitive idea,"" which clearly indicates positivity. The suggestion to elaborate on the interaction with classical regularizers is constructive criticism, not a harsh negative point. Overall, the tone is encouraging and helpful rather than dismissive or overly critical.",60.0,80.0
Generative Models for Alignment and Data Efficiency in Language,"['Dustin Tran', 'Yura Burda', 'Ilya Sutskever']",Reject,2018,"[4, 5, 12]","[9, 10, 17]","[76, 17, 99]","[31, 8, 49]","[40, 9, 45]","[5, 0, 5]","This paper proposes a generative model called matching auto-encoder to carry out the learning from unaligned data.
However, it is very disappointed to read the contents after the introduction, since most of the contributions are overclaimed.

Detailed comments:
- Figure 1 is incorrect because the pairs (x, z) and (y, z) should be put into two different plates if  x and y are unaligned.

- Lots of contents in Sec. 3 are confusing to me. What is the difference between g_l(x) and g_l(y) if g_l : H_{l−1} → H_l and f_l: H_{l−1} → H_l are the same? What are e_x and e_y? Why is there a λ if it is a generative model?

- If the title is called 'text decipherment', there should be no parallel data at all, otherwise it is a huge overclaim on the decipherment tasks. Please add citations of Kevin Knight's recent papers on deciperment.

- Reading the experiment results of 'Sentiment Transfer' is a disaster to me. I couldn't get much information on 'sentiment transfer' from a bunch of ungrammatical unnatural language sentences. I would prefer to see some results of baseline models for comparison instead of a pure qualitative analysis.

- The claim on ""FMAEs are state of the art for neural machine translation with limited supervision on EN-DE and EN-FR"" is not exciting to me. Semi-supervised learning is interesting, but in the scenario of MT we do have enough parallel data for many language pairs. Unless the model is able to exceed the 'real' state-of-the-art that uses the full set of parallel data, otherwise we couldn't identify whether the models are able to benefit NMT.  Interestingly, the authors didn't provide any of the results that are experimented with full parallel data set. Possibly it is because the introduction of stochastic variables that prevent the models from overfitting on small datasets.

","[4, 2, 5]","[' Ok but not good enough - rejection', ' Strong rejection', ' Marginally below acceptance threshold']","[3, 3, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer expresses strong disappointment with the paper, finding many of the claims to be overstated and the experiments unconvincing.  They use terms like ""incorrect,"" ""confusing,"" ""disaster,"" and ""not exciting."" The reviewer also points out significant flaws in the methodology and results. While they don't resort to personal attacks, the tone is quite negative and critical throughout.",-75.0,20.0
Interpreting Deep Classification Models With Bayesian Inference,"['Hanshu Yan', 'Jiashi Feng']",Reject,2018,"[0, 9]","[5, 14]","[22, 542]","[7, 214]","[13, 235]","[2, 93]","The paper develops a technique to understand what nodes in a neural network are important
for prediction. The approach they develop consists of using an Indian Buffet Process 
to model a binary activation matrix with number of rows equal to the number of examples. 
The binary variables are estimated by taking a relaxed version of the 
asymptotic MAP objective for this problem. One question from the use of the 
Indian Buffet Process: how do the asymptotics of the feature allocation determine 
the number of hidden units selected? 

Overall, the results didn't warrant the complexity of the method. The results are neat, but 
I couldn't tell why this approach was better than others.

Lastly, can you intuitively explain the additivity assumption in the distribution for p(y')","[3, 3, 5]","[' Clear rejection', ' Clear rejection', ' Marginally below acceptance threshold']","[4, 3, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review starts with a neutral tone, describing the paper's focus and summarizing the methodology. However, the phrases ""Overall, the results didn't warrant the complexity of the method"" and ""I couldn't tell why this approach was better than others"" clearly indicate a negative sentiment. The reviewer finds the results interesting (""neat"") but unconvincing in terms of the method's advantage.  The question directed at the authors is framed neutrally, not in an attacking or demeaning way. ",-30.0,50.0
Deep Function Machines: Generalized Neural Networks for Topological Layer Expression,['William H. Guss'],Reject,2018,[3],[6],[17],[4],[13],[0],"The main idea of this paper is to replace the feedforward summation
y = f(W*x + b)
where x,y,b are vectors, W is a matrix
by an integral
\y = f(\int W \x + \b)
where \x,\y,\b are functions, and W is a kernel. A deep neural network with this integral feedforward is called a deep function machine. 

The motivation is along the lines of functional PCA: if the vector x was obtained by discretization of some function \x, then one encounters the curse of dimensionality as one obtains finer and finer discretization. The idea of functional PCA is to view \x as a function is some appropriate Hilbert space, and expands it in some appropriate basis. This way, finer discretization does not increase the dimension of \x (nor its approximation), but rather improves the resolution. 

This paper takes this idea and applies it to deep neural networks. Unfortunately, beyond rather obvious approximation results, the paper does not get major mileage out of this idea. This approach amounts to a change of basis - and therefore the resolution invariance is not surprising. In the experiments, results of this method should be compared not against NNs trained on the data directly, but against NNs trained on dimension reduced version of the data (eg: first fixed number of PCA components). Unfortunately, this was not done. I suspect that in this case, the results would be very similar. 

","[3, 7, 4]","[' Clear rejection', ' Good paper, accept', ' Ok but not good enough - rejection']","[4, 1, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', "" The reviewer's evaluation is an educated guess"", ' The reviewer is fairly confident that the evaluation is correct']","The review starts with a neutral, objective tone, explaining the core idea of the paper. However, it quickly transitions into a negative sentiment by using terms like ""Unfortunately"" and ""does not get major mileage out of this idea."" The reviewer finds the approach to be a basic change of basis and criticizes the lack of comparison with dimension-reduced data, suggesting the results might not be significant. The language, while critical, remains professional and polite throughout.",-30.0,60.0
Cheap DNN Pruning with Performance Guarantees ,"['Konstantinos Pitas', 'Mike Davies', 'Pierre Vandergheynst']",Reject,2018,"[5, 24, 24]","[9, 29, 29]","[8, 35, 277]","[2, 15, 132]","[6, 7, 69]","[0, 13, 76]","The problem of pruning DNNs is an active area of study.
This paper addresses this problem by posing the Net-trim objective function as  a Difference of convex(DC) function. This allows for an immediate application of DC function minimization using existing techniques. An analysis of Generalization error 
is also given. 

The main novelty seems to be the interesting connection to DC function minimization. The benefits seem to be a faster algorithm for pruning. 

About the generalization error the term C_2 needs to be more well defined otherwise the coefficient of  A would be -ve which may lead to complications.

Experimental investigations are reasonable and the results are convincing.

A list of Pros:
1. Interesting connection to DC function
2. Attempt to analyze generalization error 
3. Faster speed of convergence empirically

A list of Cons:
1. The contribution in posing the objective as a DC function looks limited as it is very straightforward. Also the algorithm is 
direct application
2. The time complexity analysis is imprecise. Since the proposed algorithm is iterative time complexity would depend on the number of iterations.




","[5, 6, 5]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Marginally below acceptance threshold']","[3, 3, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review acknowledges the paper's focus on a relevant problem and its main novelty (connection to DC function minimization). It finds the experimental results convincing. However, it also points out limitations, such as the straightforward nature of the DC function application and the imprecise time complexity analysis. The language is neutral, not overly praising nor overly critical.",50.0,50.0
Diffusing Policies : Towards Wasserstein Policy Gradient Flows,"['Pierre H. Richemond', 'Brendan Maginnis']",Reject,2018,"[2, 2]","[7, 3]","[18, 4]","[3, 0]","[15, 3]","[0, 1]","The main object of the paper is the (entropy regularized) policy updates. Policy iterations are viewed as a gradient flow in the small timestep limit. Using this, (and following Jordan et al. (1998)) the desired PDE (Equation 21) is obtained. The rest of the paper discusses the implications of Equation 21 including but not limited to what happens when the time derivative of the policy is zero, and the link to noisy gradients.

Even though the topic is interesting and would be of interest to the community, the paper mainly presents known results and provides an interpretation from the point of view of policy dynamics. I fail to see the significance nor the novelty in this work (esp. in light of  Jordan et al. (1998) and Peyre (2015)).

That said, I believe that exposing such connections will prove to be useful, and I encourage the authors to push the area forward. In particular, it would be useful to see demonstrations of the idea, and experimental justifications even in the form of references would be a welcome addition to the literature.","[4, 4, 5]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[4, 3, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review starts with a neutral summary of the paper's content. However, it then expresses concerns about the significance and novelty, stating ""I fail to see the significance nor the novelty in this work."" This suggests a negative sentiment. While the reviewer acknowledges potential usefulness and encourages further development, the lack of novelty concern weighs heavily. The language used is quite direct, particularly phrases like ""I fail to see,"" but maintains a professional and not disrespectful tone. Therefore, it leans slightly towards the negative side of politeness.",-30.0,-10.0
Large Batch Training of Convolutional Networks with Layer-wise Adaptive Rate Scaling,"['Boris Ginsburg', 'Igor Gitman', 'Yang You']",Reject,2018,"[17, 2, 11]","[22, 7, 16]","[69, 8, 102]","[27, 1, 34]","[40, 7, 35]","[2, 0, 33]","This paper provides an optimization approach for large batch training of CNN with layer-wise adaptive learning rates. 
It starts from the observation that the ratio between the L2-norm of parameters and that of gradients on parameters varies
significantly in the optimization,  and then introduce a local learning rate to consider this observation for a more stable and efficient optimization. Experimental results show improvements compared with the state-of-the-art algorithm.

Review:
(1) Pros
The proposed optimization method considers the dynamic self-adjustment of the learning rate in the optimization based on the ratio between the L2-norm of parameters and that of gradients on parameters  when the batch size increases, and shows improvements in experiments compared with previous methods.

(2) Cons
i) LR ""warm-up"" can mitigate the unstable training in the initial phase and the proposed method is also motivated by the stability but uses a different approach. However, it seems that the authors also combine with LR ""warm-up"" in your proposed method in the experimental part, e.g., Table 3. So does it mean that the proposed method cannot handle the problem in general?

ii) There is one coefficient that is independent from layers and needs to be set manually in the proposed local learning rate. The authors do not have a detail explanation and experiments about it. In fact, as can be seen in the Algorithm 1, this coefficient can be as an independent hyper-parameter (even is put with the global learning rate together as one fix term).

iii) In the section 6, when increase the training steps, experiments compared with previous methods should be implemented since they can also get better results with more epochs.

iv) Writing should be improved, e.g., the first paragraph in section 6. Some parts are confusing, for example, the authors claim that they use initial LR=0.01, but in Table 1(a) it is 0.02.  ","[5, 5, 4]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Ok but not good enough - rejection']","[3, 5, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review acknowledges the merits of the paper, highlighting its novel approach and experimental improvements as positives. However, it also raises several valid concerns and suggestions for improvement, indicating that the reviewer sees potential in the work but requires further clarification and experimentation. The language used is professional and typical of academic peer reviews, directly addressing the authors and their work without resorting to personal attacks or overly negative phrasing.",40.0,60.0
UNSUPERVISED METRIC LEARNING VIA NONLINEAR FEATURE SPACE TRANSFORMATIONS,"['Pin Zhang', 'Bibo Shi', 'JundongLiu']",Reject,2018,"[13, 8, 'no_match']","[17, 10, 'no_match']","[22, 26, 'no match']","[12, 20, 'no match']","[0, 3, 'no match']","[10, 3, 'no match']","This paper proposed a nonlinear unsupervised metric learning framework. The authors combine Coherent Point Drifting and the k-means approaches under the trace minimization framework. However, I am afraid that the novelty and insight of this work is not good enough for acceptance.

Pros:
The paper is well written and easy to follow.

Cons:
1 The novelty of this paper is limited.
The authors mainly combine Coherent Point Drifting and the k-means under the trace minimization framework. The trace minimization is then solved with an EM-like iterative minimization.
However, trace minimization is already well explored and this paper provides little insight. Furthermore, there is not any theoretical guarantee how this iterative minimization approach will converge to.

2 For a method with limited novelty, comprehensive experiments are needed to verify its effectiveness. However, the experimental setting of this paper is biased.
An important line of works, namely deep learning based clustering, are totally missing.
Comprehensive experiments with other deep learning based clustering are required.
","[4, 6, 4]","[' Ok but not good enough - rejection', ' Marginally above acceptance threshold', ' Ok but not good enough - rejection']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The reviewer finds the paper well-written and easy to follow, which indicates a positive attitude. However, they express strong concerns about the novelty and limited insight of the work. The use of ""However, I am afraid that..."" softens the criticism but still clearly conveys a negative sentiment. The reviewer also points out flaws in the experimental setup, further decreasing the positivity. Overall, the tone is professional and polite, suggesting constructive criticism rather than harsh negativity.",-30.0,70.0
Adversarial Examples for Natural Language Classification Problems,"['Volodymyr Kuleshov', 'Shantanu Thakoor', 'Tingfung Lau', 'Stefano Ermon']",Reject,2018,"[9, 1, 0, 10]","[14, 6, 3, 15]","[46, 17, 4, 406]","[20, 6, 1, 199]","[22, 11, 3, 200]","[4, 0, 0, 7]","Nice overview of adversarial techniques in natural language classification. The paper introduces the problem of adversarial perturbations, how they are constructed and demonstrate what effect they can have on a machine learning models. 

The authors study several real-world adversarial examples, such as spam filtering, sentiment analysis and fake news and use these examples to test several popular classification models in context of adversarial perturbations. 

Their results demonstrate the existence of adversarial perturbations in NLP and show that several different types of errors occur (syntactic, semantic, and factual). Studying each of these errors type can help defend and improve the classification algorithms via adversarial training.

Pros: Good analysis on real-world examples
Cons: I was expecting more actual solutions in addition to analysis","[6, 4, 4]","[' Marginally above acceptance threshold', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[3, 5, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with positive statements, highlighting the good aspects of the paper like the overview, analysis, and demonstration of adversarial techniques. The reviewer acknowledges the value of the study ('Good analysis on real-world examples'). However, the single 'Con' point, while politely phrased, expresses a desire for more from the paper. This suggests the reviewer finds the work good but potentially incomplete, leading to a score leaning towards positive but not overly enthusiastic.",60.0,80.0
Sparse Regularized Deep Neural Networks For Efficient Embedded Learning,['Jia Bi'],Reject,2018,[17],[21],[16],[9],[5],[2],"The authors present an l-1 regularized SVRG based training algorithm that is able to force many weights of the network to be 0, hence leading to good compression of the model.  The motivation for l-1 regularization is clear as it promotes sparse models, which lead to lower storage overheads during inference. The use of SVRG is motivated by the fact that it can, in some cases, provide faster convergence than SGD.

Unfortunately, the authors do not compare with some key literature. For example there has been several techniques that use sparsity, and group sparsity [1,2,3], that lead to the same conclusion as the paper here: models can be significantly sparsified while not affecting the test accuracy of the trained model.

Then, the novelty of the technique presented is also unclear, as essentially the algorithm is simply SVRG with l1 regularization and then some quantization. The experimental evaluation does not strongly support the thesis that the presented algorithm is much better than SGD with l1 regularization. In the presented experiments, the gap between the performance of SGD and SVRG is small (especially in terms of test error), and overall the savings in terms of the number of weights is similar to Deep compression. Hence, it is unclear how the use of SVRG over SGD improves things. Eg in figure 2 the differences in top-1 error of SGD and SVRG, for the same number of weights is very similar (it’s unclear also why Fig 2a uses top-1 and Fig 2b uses top-5 error). I also want to note that all experiments were run on LeNet, and not on state of the art models (eg ResNets).

Finally, the paper is riddled with typos. I attach below some of the ones I found in pages 1 and 2

Overall, although the topic is very interesting, the contribution of this paper is limited, and it is unclear how it compares with other similar techniques that use group sparsity regularization, and whether SVRG offers any significant advantages over l1-SGD.

typos:
“ This work addresses the problem by proposing methods Weight Reduction Quantisation”
-> This work addresses the problem by proposing a Weight Reduction Quantisation method

“Beside, applying with sparsity-inducing regularization”
-> Beside, applying sparsity-inducing regularization

“Our method that minibatch SVRG with l-1 regularization on non-convex problem”
-> Our minibatch SVRG with l-1 regularization method on non-convex problem

“As well as providing,l1 regularization is a powerful compression techniques to penalize some weights to be zero”
-> “l1 regularization is a powerful compression technique that forces some weights to be zero”

 The problem 1 can
->  The problem in Eq.(1) can

“it inefficiently encourages weight”
-> “it inefficiently encourages weights”

————

[1] Learning Structured Sparsity in Deep Neural Networks
http://papers.nips.cc/paper/6504-learning-structured-sparsity-in-deep-neural-networks.pdf

[2] Fast ConvNets Using Group-wise Brain Damage
https://arxiv.org/pdf/1506.02515.pdf

[3] Sparse Convolutional Neural Networks
https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Liu_Sparse_Convolutional_Neural_2015_CVPR_paper.pdf


","[4, 4, 2]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Strong rejection']","[5, 4, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer acknowledges the interesting topic and the clear motivation for using l-1 regularization and SVRG. However, they express concerns about the novelty of the approach and provide a somewhat negative assessment of the experimental evaluation. They find the improvements over SGD to be small and question the choice of LeNet over state-of-the-art models. The numerous typos further detract from the overall impression. Therefore, the sentiment is somewhat negative, although the tone remains professional and polite throughout.",-30.0,50.0
Enhancing Batch Normalized Convolutional Networks using Displaced Rectifier Linear Units: A Systematic Comparative Study,"['David Macêdo', 'Cleber Zanchettin', 'Adriano L. I. Oliveira', 'Teresa Ludermir']",Reject,2018,"[1, 16, 16, 29]","[5, 21, 21, 34]",['skipped'],['skipped'],['skipped'],['skipped'],"This paper describes DReLU, a shift version of ReLU. DReLU shifts ReLU from (0, 0) to (-\sigma, -\sigma). The author runs a few CIFAR-10/100 experiments with DReLU.

Comments:

1. Using expectation to explain why DReLU works well is not sufficient and convincing. Although DReLU’s expectation is smaller than expectation of ReLU, but it doesn’t explain why DReLU is better than very leaky ReLU, ELU etc.
2. CIFAR-10/100 is a saturated dataset and it is not convincing DReLU will perform will on complex task, such as ImageNet, object detection, etc.
3. In all experiments, ELU/LReLU are worse than ReLU, which is suspicious. I personally have tried ELU/LReLU/RReLU on Inception V3 with Batch Norm, and all are better than ReLU. 

Overall, I don’t think this paper meet ICLR’s novelty standard, although the authors present some good numbers, but they are not convincing. 


","[5, 3, 4]","[' Marginally below acceptance threshold', ' Clear rejection', ' Ok but not good enough - rejection']","[5, 5, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer highlights both positive and negative aspects of the paper. While acknowledging the ""good numbers,"" the reviewer ultimately finds the paper's novelty and the convincingness of the results lacking. The use of phrases like ""not sufficient,"" ""not convincing,"" and ""suspicious"" indicates a negative sentiment. However, the language remains professional and avoids harsh or disrespectful tones, suggesting a neutral-to-polite approach.",-40.0,60.0
Combination of Supervised and Reinforcement Learning For Vision-Based Autonomous Control,"['Dmitry Kangin', 'Nicolas Pugeault']",Reject,2018,"[6, 16]","[10, 21]","[24, 77]","[14, 41]","[5, 16]","[5, 20]","This paper proposes leveraging labelled controlled data to accelerate reinforcement-based learning of a control policy.  It provides two main contributions: pre-training the policy network of a DDPG agent in a supervised manner so that it begins in reasonable state-action distribution and regalurizing the Q-updates of the q-network to be biased towards existing actions.  The authors use the TORCS enviroment to demonstrate the performance of their method both in final cumulative return of the policy and speed of learning.

This paper is easy to understand but has a couple shortcomings and some fatal (but reparable) flaws:.

1) When using RL please try to standardize your notation to that used by the community, it makes things much easier to read.  I would strongly suggest avoiding your notation a(x|\Theta) and using \pi(x) (subscripting theta or making conditional is somewhat less important).  Your a(.) function seems to be the policy here, which is invariable denoted \pi in the RL literature.  There has been recent effort to clean up RL notation which is presented here: https://sites.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf. You have no obligation to use this notation but it does make reading of your paper much easier on others in the community.  This is more of a shortcoming than a fundamental issue.

2) More fatally, you have failed to compare your algorithm's performance against benchline implementations of similar algorithms.  It is almost trivial to run DDPG on Torcs using the openAI baselines package [https://github.com/openai/baselines].  I would have loved, for example, to see the effects of simply pre-training the DDPG actor on supervised data, vs. adding your mixture loss on the critic.  Using the baselines would have (maybe) made a very compelling graph showing DDPG, DDPG + actor pre-training, and then your complete method.

3) And finally, perhaps complementary to point 2), you really need to provide examples on more than one environment.  Each of these simulated environments has its own pathologies linked to determenism, reward structure, and other environment particularities.  Almost every algorithm I've seen published will often beat baselines on one environment and then fail to improve or even be wors on others, so it is important to at least run on a series of these.  Mujoco + AI Gym should make this really easy to do (for reference, I have no relatinship with OpenAI).  Running at least cartpole (which is a very well understood control task), and then perhaps reacher, swimmer, half-cheetah etc. using a known contoller as your behavior policy (behavior policy is a good term for your data-generating policy.)

4) In terms of state of the art you are very close to Todd Hester et. al's paper on imitation learning, and although you cite it, you should contrast your approach more clearly with the one in that paper.  Please also have a look at some more recent work my Matej Vecerik, Todd Hester & Jon Scholz: 'Leveraging Demonstrations for Deep Reinforcement Learning on Robotics Problems with Sparse Rewards' for an approach that is pretty similar to yours.

Overall I think your intuitions and ideas are good, but the paper does not do a good enough job justifying empirically that your approach provides any advantages over existing methods.  The idea of pre-training the policy net has been tried before (although I can't find a published reference) and in my experience will help on certain problems, and hinder on others, primarily because the policy network is already 'overfit' somewhat to the expert, and may have a hard time moving to a more optimal space.  Because of this experience I would need more supporting evidence that your method actually generalizes to more than one RL environment.","[4, 3, 5]","[' Ok but not good enough - rejection', ' Clear rejection', ' Marginally below acceptance threshold']","[5, 4, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer provides constructive criticism, acknowledges the paper's merits ('easy to understand', 'good intuitions and ideas'), but points out significant flaws that need to be addressed before publication. The language is direct and to the point, but not disrespectful.",20.0,60.0
Counterfactual Image Networks,"['Deniz Oktay', 'Carl Vondrick', 'Antonio Torralba']",Reject,2018,"[3, 9, 20]","[8, 14, 25]","[8, 122, 391]","[3, 50, 204]","[5, 65, 145]","[0, 7, 42]","This paper proposes a neural network architecture around the idea of layered scene composition.  Training is cast in the generative adversarial framework; a subnetwork is reused to generate and compose (via an output mask) multiple image layers; the resulting image is fed to a discriminator.  An encoder is later trained to map real images into the space of latent codes for the generator, allowing the system to be applied to real image segmentation tasks.

The idea is interesting and different from established approaches to segmentation.  Visualization of learned layers for several scene types (Figures 3, 7) shows that the network does learn a reasonable compositional scene model.

Experiments evaluate the ability to port the model learned in an unsupervised manner to semantic segmentation tasks, using a limited amount of supervision for the end task.  However, the included experiments are not nearly sufficient to establish the effectiveness of the proposed method.  Only two scene types (bedroom, kitchen) and four object classes (bed, window, appliance, counter) are used for evaluation.  This is far below the norm for semantic segmentation work in computer vision.  How does the method work on established semantic segmentation datasets with many classes, such as PASCAL?  Even the ADE20K dataset, from which this paper samples, is substantially larger and has an established benchmarking methodology (see http://placeschallenge.csail.mit.edu/).

An additional problem is that performance is not compared to any external prior work.  Only simple baselines (eg autoencoder, kmeans) implemented by this paper are included.  The range of prior work on semantic segmentation is extensive.  How well does the approach compare to supervised CNNs on an established segmentation task?  Note that the proposed method need not necessarily outperform supervised approaches, but the reader should be provided with some idea of the size of the gap between this unsupervised method and the state-of-the-art supervised approach.

In summary, the proposed method may be promising, but far more experiments are needed.
","[4, 4, 5]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with positive sentiment, acknowledging the novelty and potential of the proposed method. However, it quickly transitions into a critical assessment, pointing out significant limitations in experimental validation and lack of comparison with existing work. The review concludes by reiterating that the method seems promising but needs more substantial evidence to support its claims. Overall, the sentiment leans towards the skeptical side due to the lack of comprehensive evaluation. The language used is polite and professional throughout, providing constructive criticism and clear suggestions for improvement.",20.0,80.0
Estimation of cross-lingual news similarities using text-mining methods,"['Zhouhao Wang', 'Enda Liu', 'Hiroki Sakaji', 'Tomoki Ito', 'Kiyoshi Izumi', 'Kota Tsubouchi', 'Tatsuo Yamashita']",Reject,2018,"[3, 'no_match', 11, 3, 23, 14, 19]","[1, 'no_match', 16, 5, 28, 19, 22]","[1, 'no match', 79, 17, 106, 110, 16]","[1, 'no match', 61, 14, 83, 83, 14]","[0, 'no match', 3, 1, 2, 12, 0]","[0, 'no match', 15, 2, 21, 15, 2]","* PAPER SUMMARY *

This paper proposes a siamese net architecture to compare text in different languages. The proposed architecture builds upon siamese RNN by Mueller and Thyagarajan. The proposed approach is evaluated on cross lingual bitext retrieval.

* REVIEW SUMMARY * 

This paper is hard to read and need proof-reading by a person proficient in English. The experiments are extremely limited, on a toy task. No other baseline than (Mueller and Thyagarajan, 2016) is considered. The related work section lacks important references. It is hard to find positive points that would advocate for a presentation at ICLR.

* DETAILED REVIEW *

On related work, the authors need to consider related work on cross lingual retrieval, multilingual document representation:

Bai, Bing, et al. ""Learning to rank with (a lot of) word features."" Information retrieval 13.3 (2010): 291-314. (Section 4).

Schwenk, H., Tran, K., Firat, O., & Douze, M. Learning Joint Multilingual Sentence Representations with Neural Machine Translation, ACL Workshop on Representation Learning for NLP, 2017

Karl Moritz Hermann and Phil Blunsom.  Multilingual models for compositional distributed semantics. In ACL 2014. pages 58–68.

Hieu Pham, Minh-Thang Luong, and Christopher D. Manning. Learning distributed representations for multilingual text sequences. In Workshop
on Vector Space Modeling for NLP. 2015

Xinjie Zhou, Xiaojun Wan, and Jianguo Xiao. Cross-lingual sentiment classification with bilingual document representation learning. In ACL 2016

...

On evaluation, the authors need to learn about standard retrieval evaluation metrics such as precision at top 10, etc and use them. For instance, this book will be a good read.

Baeza-Yates, Ricardo, and Berthier Ribeiro-Neto. Modern information retrieval. Vol. 463. New York: ACM press, 1999.

On learning objective, the authors might want to read about learn-to-rank objectives for information retrieval, for instance, 

Liu, Tie-Yan. ""Learning to rank for information retrieval."" Foundations and Trends in Information Retrieval 3.3 (2009): 225-331.

Burges, Christopher JC. ""From ranknet to lambdarank to lambdamart: An overview."" Learning 11, no. 23-581 (2010): 81.

Chapelle, Olivier, and Yi Chang. ""Yahoo! learning to rank challenge overview."" Proceedings of the Learning to Rank Challenge. 2011.

Herbrich, Ralf, Thore Graepel, and Klaus Obermayer. ""Large margin rank boundaries for ordinal regression."" (2000).

On experimental setup, the authors want to consider a setup with more than 8k training documents. More importantly, ranking a document set of 1k documents is extremely small, toyish. For instance, (Schwenk et al 2017) search through 1.5 million sentences. (Bai, Bing, et al 2009) search through 140k documents. Since you mainly introduces 2 modifications with respect to (Mueller and Thyagarajan, 2016), i.e  (i) not sharing the parameters on both branch of the siamese and (ii) the fully connected net on top, I would suggest to measure the effect of each of them both on multilingual data and on the SICK dataset used in (Mueller and Thyagarajan, 2016).","[2, 6, 2]","[' Strong rejection', ' Marginally above acceptance threshold', ' Strong rejection']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is highly critical of the paper, pointing out numerous flaws in methodology, related work, and experimental setup. The reviewer finds the paper to be of low quality and doesn't see it fit for presentation at ICLR. The language used, while direct and critical, maintains a professional and objective tone. There's no personal attack or disrespectful language used.",-75.0,50.0
Interpretable Classification via Supervised Variational Autoencoders and Differentiable Decision Trees,"['Eleanor Quint', 'Garrett Wirka', 'Jacob Williams', 'Stephen Scott', 'N.V. Vinodchandran']",Reject,2018,"[0, 'no_match', 2, 24, 25]","[1, 'no_match', 7, 28, 30]","[1, 'no match', 10, 48, 117]","[0, 'no match', 3, 28, 52]","[1, 'no match', 2, 2, 34]","[0, 'no match', 5, 18, 31]","The paper tries to build an interpretable and accurate classifier via stacking a supervised VAE (SVAE) and a differentiable decision tree (DTT). The problem is important and interesting. The authors list the contributions of each part but it seems that only the final contribution, i.e. analysis of the interpretability, is interesting and should be further extended and emphasized. Here with the detailed comments.

1. I think Table 2 does not make sense at all. This is not only because the authors use the label information but also because the authors compare different quantities. The the previous methods evaluate log p(x) while the proposed method evaluates log p(x, y) which should be much lower as the proposed method potentially trains a separated model for each class of the x for evaluation.

2. The generation results of the SVAE shown in Figure 7 in Appendix A seem strange as the diversity of the samples is much less than those from the vanilla VAEs. Could the authors explain this mode collapse phenomenon? 

3. The results in Table 1 are not interesting.  It is most useful to interpret the state-of-the-art classifier while the results of the proposed methods are far from the state-of-the-art even on such simple MNIST dataset.

4. The most interesting results of this paper are shown in Figure 1. However, I think the results on the interpretability should be further extended. Several questions are as follows: 

Why other dimensions are not so interpretable, compared with 21?

Can we also interpret a VAE given labels by varying each dimension of the latent variables without jointly training a DTT? I personally think some of the dimensions of the latent variables of the vanilla VAEs can also be interpreted via interpolation in each dimension. 

Can these results be generalized to other datasets, consisting of natural images? 

Overall, this paper is below the acceptance threshold.
 ","[4, 5, 3]","[' Ok but not good enough - rejection', ' Marginally below acceptance threshold', ' Clear rejection']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The reviewer finds the paper's core idea interesting but ultimately deems the contributions and results insufficient for acceptance. While acknowledging the importance of interpretability analysis, the reviewer raises several significant concerns regarding the methodology, results interpretation, and lack of strong evidence supporting the proposed approach's effectiveness. The language, while direct and critical, maintains a professional and academic tone.",-50.0,50.0
Code Synthesis with Priority Queue Training,"['Daniel A. Abolafia', 'Quoc V. Le', 'Mohammad Norouzi']",Reject,2018,"[1, 14, 10]","[3, 19, 15]","[6, 299, 136]","[2, 143, 59]","[4, 145, 72]","[0, 11, 5]","This paper introduces a method for regularizing the REINFORCE algorithm by keeping around a small set of known high-quality samples as part of the sample set when performing stochastic gradient estimation.

I question the value of program synthesis in a language which is not human-readable. Typically, source code as function representation is desirable because it is human-interpretable. Code written in brainfuck is not  readable by humans. In the related work, a paper by Nachum et al is criticized for providing a sequence of machine instructions, rather than code in a language. Since code in brainfuck is essentially a sequence of pointer arithmetic operations, and does not include any concept of compositionality or modularity of code (e.g. functions or variables), it is not clear what advantage this representation presents. Neither am I particularly convinced by the benchmark of a GA for generating BF code. None of these programs are particularly complex: most of the examples found in table 4 are quite short, over half of them 16 characters or fewer. 500 million evaluations is a lot. There are no program synthesis examples demonstrating types of functions which perform complex tasks involving e.g. recursion, such as sorting operations.

There is also an odd attitude in the writing of this paper, reflected in the excerpt from the first paragraph describing that traditional approaches to program synthesis “… typically do not make use of machine learning and therefore require domain specific knowledge about the programming languages and hand-crafted heuristics to speed up the underlying combinatorial search. To create more generic programming tools without much domain specific knowledge …”. Why is this a goal? What is learned by restricting models to be unaware of obviously available domain-specific knowledge? 

All this said, the priority queue training presented here for reinforcement learning with sparse rewards is interesting, and appears to significantly improve the quality of results from a naive policy gradient approach. It would be nice to provide some sort of analysis of it, even an empirical one. For example, how frequently are the entries in the queue updated? Is this consistent over training time? How was the decision of K=10 reached? Is a separate queue per distributed training instance a choice made for implementation reasons, or because it provides helpful additional “regularization”? While the paper does demonstrate that PQT is helpful on this very particular task, it makes very little effort to investigate *why* it is helpful, or whether it will usefully generalize to other domains.

Some analysis, perhaps even on just a small toy problem, of e.g. the effect of the PQT on the variance of the gradient estimates produced by REINFORCE, would go a long way towards convincing a skeptical reader of the value of this approach. It would also help clarify under what situations one should or should not use this. Any insight into how one should best set the lambda hyperparameters would also be very appreciated.","[6, 6, 5]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Marginally below acceptance threshold']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review starts with a very skeptical and critical tone, questioning the fundamental premise of the paper's approach. While it acknowledges some positive aspects of the PQT method later on, the overall tone remains doubtful and unconvinced. The reviewer also criticizes the authors' choices and framing, indicating a negative sentiment. The language used is quite direct and critical, but it doesn't cross the line into being disrespectful or unprofessional. Therefore, the politeness score leans slightly towards the negative side of neutral.",-30.0,-10.0
SIC-GAN: A Self-Improving Collaborative GAN for Decoding Sketch RNNs,"['Chi-Chun Chuang', 'Zheng-Xin Weng', 'Shan-Hung Wu']",Reject,2018,"['no_match', 12]","['no_match', 16]","['no match', 40]","['no match', 26]","['no match', 3]","['no match', 11]","This paper baffles me. It appears to be a stochastic RNN with skip connections (so it's conditioned on the last two states rather than last one) trained by an adversarial objective (which is no small feat to make work for sequential tasks) with results shown on the firetruck category of the QuickDraw dataset. Yet the authors claim significantly more importance for the work than I think it merits.

First, there is nothing variational about their variational RNN. They seem to use the term to be equivalent to ""stochastic"", ""probabilistic"" or ""noisy"" rather than having anything to do with optimizing a variational bound. To strike the right balance between pretension and accuracy, I would suggest substituting the word ""stochastic""  everywhere ""variational"" is used.

Second, there is nothing self-improving or collaborative about their self-improving collaborative GAN. Once the architecture is chosen to share the weights between the weak and strong generator, the only difference between the two is that the weak generator has greater noise at the output. In this sense the architecture should really be seen as a single model with different noise levels at alternating steps. In this sense, I am not entirely clear on what the difference is between the SIC-GAN and their noisy GAN baseline - presumably the only difference is that the noisy GAN is conditioned on a single timestep instead of two at a time? The claim that these models are somehow ""self-improving"" baffles me as well - all machine learning models are self-improving, that is the point of learning. The authors make a comparison to AlphaGo Zero's use of self-play, but here the weak and strong generators are on the same side of the game, and because there are no game rules provided beyond ""reproduce the training set"", there is no possibility of discovery beyond what is human-provided, contrary to the authors' claim.

Third, the total absence of mathematical notation made it hard in places to follow exactly what the models were doing. While there are plenty of papers explaining the GAN framework to a novice, at least some clear description of the baseline architectures would be appreciated (for instance, a clearer explanation of how the SIC-GAN differs from the noisy GAN). Also the description of the soft $\ell_1$ loss (which the authors call the ""1-loss"" for some reason) would benefit from a clearer mathematical exposition.

Fourth, the experiments seem too focused on the firetruck category of the QuickDraw dataset. As it was the only example shown, it's difficult to evaluate their claim that this is a general method for improving variety without sacrificing quality. Their chosen metrics for variety and detail are somewhat subjective, as they depend on the fact that some categories in the QuickDraw dataset resemble firetrucks in the fine detail while others resemble firetrucks in outline. This is not a generalizable metric. Human evaluation of the relative quality and variety would likely suffice.

Lastly, the entire section on the strong-weak collaborative GAN seems to add nothing. They describe an entire training regiment for the model, yet never provide any actual experimental results using that model, so the entire section seems only to motivate the SIC-GAN which, again, seems like a fairly ordinary architectural extension to GANs with RNN generators.

The results presented on QuickDraw do seem nice, and to the best of my knowledge it is the first (or at least best) applications of GANs to QuickDraw - if they refocused the paper on GAN architectures for sketching and provided more generalizable metrics of quality and variety it could be made into a good paper.","[4, 7, 5]","[' Ok but not good enough - rejection', ' Good paper, accept', ' Marginally below acceptance threshold']","[5, 3, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer expresses a negative sentiment by using terms like ""baffles me"", questioning the authors' claims and suggesting the work is overselling its contributions. However, the reviewer does acknowledge some positive aspects, such as the quality of the results on QuickDraw and the novelty of applying GANs to this dataset. The language, while critical, maintains a professional and analytical tone without resorting to personal attacks or disrespectful language.",-50.0,60.0
Avoiding Catastrophic States with Intrinsic Fear,"['Zachary C. Lipton', 'Kamyar Azizzadenesheli', 'Abhishek Kumar', 'Lihong Li', 'Jianfeng Gao', 'Li Deng']",Reject,2018,"[5, 3, 'no_match', 16, 19, 28]","[10, 8, 'no_match', 20, 24, 30]","[203, 90, 'no match', 170, 542, 329]","[77, 26, 'no match', 89, 263, 188]","[117, 56, 'no match', 66, 247, 36]","[9, 8, 'no match', 15, 32, 105]","The paper studies catastrophic forgetting, which is an important aspect of deep reinforcement learning (RL). The problem formulation is connected to safe RL, but the emphasis is on tasks where a DQN is able to learn to avoid catastrophic events as long as it avoids forgetting. The proposed method is novel, but perhaps the most interesting aspect of this paper is that they demonstrate that “DQNs  are susceptible to periodically repeating mistakes”. I believe this observation, though not entirely novel, will inspire many researchers to study catastrophic forgetting and propose improved strategies for handling these issues.

The paper is accurate, very well written (apart from a small number of grammatical mistakes) and contains appealing motivations to its key contributions. In particular, I find the basic of idea of introducing a component that represents fear natural, promising and novel. 

Still, many of the design choices appear quite arbitrary and can most likely be improved upon. In fact, it is not difficult to design examples for which the proposed algorithm would be far from optimal. Instead I view the proposed techniques mostly as useful inspiration for future papers to build on. As a source of inspiration, I believe that this paper will be of considerable importance and I think many people in our community will read it with great interest. The theoretical results regarding the properties of the proposed algorithm are also relevant, and points out some of its benefits, though I do not view the results as particularly strong. 

To conclude, the submitted manuscript contains novel observations and results and is likely to draw additional attention to an important aspect of deep reinforcement learning. A potential weakness with the paper is that the proposed strategies appear to be simple to improve upon and that they have not convinced me that they would yield good performance on a wider set of problems. 
","[7, 5, 5]","[' Good paper, accept', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[3, 4, 5]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The reviewer explicitly states that the paper is a good source of inspiration and will be of considerable importance to the community. They find the problem interesting, the proposed method novel, and the observation about DQNs important. While they have concerns about the design choices and believe they can be improved, the overall tone is positive and encouraging. Therefore, the sentiment is scored as moderately positive. The language used is polite and professional throughout the review, with constructive criticism offered in a respectful manner.",60.0,80.0
Reinforcement Learning via Replica Stacking of Quantum Measurements for the Training of Quantum Boltzmann Machines,"['Anna Levit', '\x06 Daniel Crawford', 'Navid Ghadermarzy', 'Jaspreet S. Oberoi', 'Ehsan Zahedinejad', 'Pooya Ronagh']",Reject,2018,"[3, 4, 6, 8, 4, 4]","[3, 4, 7, 10, 6, 9]","[3, 4, 7, 7, 6, 19]","[0, 1, 0, 2, 0, 0]","[2, 2, 5, 4, 5, 12]","[1, 1, 2, 1, 1, 7]","The paper is easy to read  for a physicist, but I am not sure how useful it would be for ICLR... it is not clear for me it there is an interest for quantum problems in this conference. This is something I will let to the Area Chair to deceede. Other than this, the paper is interesting, certainly correct, and provides a nice perspective on the future of learning with quantum computers. I like the  quantum ""boltzmann machine"" problems. 

I feel, however, but it might be a bit far from the main interest of the conference.

Comments:

* What the authors called ""Free energy-based reinforcement learning"" seems to me just the minimization / maximiation of the free energy. This is simply maximum likelihood applied to the free energy and I think that calling it ""reinforcement learning"" is not only wrong, but also is very confusing, given this is usually reserved to an entirely different learning process.

* While i liked the introduction of the quantum Boltzmann machine, I would be happy to learn what they can do? Are these useful, for instance, to study correlated fermions/bosons? The paper does not explain why one should be concerns with these devices.

* The fact that the simulation on a classical computer agrees with the one on a quantum computer is promising, but I would say that this shows that, so far, there is not yet a clear advantage in using a quantum computer. This might change, but in the mean time, what is the benefits for the ICLR community?
","[6, 4, 4]","[' Marginally above acceptance threshold', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[4, 3, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer finds the paper interesting and well-written, with a ""nice perspective."" However, they express uncertainty about its relevance to the ICLR conference and suggest it might be ""a bit far from the main interest."" The reviewer also offers constructive criticism and questions, indicating a willingness to engage with the work. Overall, the tone is neutral to mildly positive, leaning towards politeness in their suggestions.",20.0,60.0
Kernel Graph Convolutional Neural Nets,"['Giannis Nikolentzos', 'Polykarpos Meladianos', 'Antoine J-P Tixier', 'Konstantinos Skianis', 'Michalis Vazirgiannis']",Reject,2018,"[4, 4, 3, 4, 26]","[9, 5, 8, 8, 31]","[55, 19, 30, 20, 316]","[28, 14, 14, 12, 181]","[21, 5, 16, 7, 77]","[6, 0, 0, 1, 58]","The authors propose a method for graph classification by combining graph kernels and CNNs. In a first step patches are extracted via community detection algorithms.  These are then transformed into vector representation using graph kernels and fed to a neural network. Multiple graph kernels may serve as different channels. The approach is evaluated on synthetic and real-world graphs.

The article is well-written and easily comprehensible, but suffers from several weak points:

* Features are not learned directly from the graphs, but the approach merely weights graph kernel features.
* The weights refer to the RKHS and filters are not easily interpretable.
* The approach is similar in spirit to Niepert, Ahmed, Kutzkov, ICML 2016 and thus incremental.
* The experiments are not convincing: The improvement over the existing work is small on real-world data sets. The synthetic classification task essentially is to distinguish a clique from star graph and not very meaningful. Moreover, a comparison to at least one of the recent approaches similar to ""Convolutional Networks on Graphs for Learning Molecular Fingerprints"" (Duvenaud et al., NIPS 2015) or ""Message Passing Neural Networks"" (Gilmer et al., 2017)  would be desirable.

Therefore, I cannot recommend the paper for acceptance.","[4, 5, 5]","[' Ok but not good enough - rejection', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[5, 5, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with a neutral summary of the paper's contributions. However, it lists several significant weaknesses, including lack of novelty, unconvincing experiments, and the need for comparison with more recent approaches. The reviewer explicitly recommends rejecting the paper. All this points to a negative sentiment. The language used is formal, polite, and provides constructive criticism without resorting to harsh or disrespectful language.",-60.0,70.0
On the Use of Word Embeddings Alone to Represent Natural Language Sequences,"['Dinghan Shen', 'Guoyin Wang', 'Wenlin Wang', 'Martin Renqiang Min', 'Qinliang Su', 'Yizhe Zhang', 'Ricardo Henao', 'Lawrence Carin']",Reject,2018,"[2, 2, 16, 12, 10, 17, 15, 22]","[6, 7, 20, 17, 15, 22, 20, 27]","[66, 65, 64, 79, 61, 107, 136, 602]","[31, 28, 33, 37, 30, 55, 72, 306]","[34, 36, 24, 32, 23, 51, 51, 173]","[1, 1, 7, 10, 8, 1, 13, 123]","This paper presents a very thorough empirical exploration of the qualities and limitations of very simple word-embedding based models. Average and/or max pooling over word embeddings (which are initialized from pretrained embeddings) is used to obtain a fixed-length representation for natural language sequences, which is then fed through a single layer MLP classifier. In many of the 9 evaluation tasks, this approach is found to match or outperform single-layer CNNs or RNNs.

The varied findings are very clearly presented and helpfully summarized, and for each task setting the authors perform an insightful analysis.

My only criticism would be the fact that the study is limited to English, even though the conclusions are explicitly scoped in light of this. Moreover, I wonder how well the findings would hold in a setting with a more severe OOV problem than is perhaps present in the studied datasets.

Besides concluding from the presented results that these SWEMs should be considered a strong baseline in future work, one might also conclude that we need more challenging datasets!

Minor things:
- It wasn't entirely clear how the text matching tasks are encoded. Are the two sequences combined into a single sequence before applying the model, or something else? I might have missed this detail.

- Given the two ways of using the Glove embeddings for initialization (direct update vs mapping them with an MLP into the task space), it would be helpful to know which one ended up being used (i.e. optimal) in each setting.

- Something went wrong with the font size for the remainder of the text near Figure 1.

** Update **
Thanks for addressing my questions in the author response.

After following the other discussion thread about the novelty claims, I believe I didn't weigh that aspect strongly enough in my original rating, so I'm revising it. I remain of the opinion that this paper offers a useful systematic comparison that goes sufficiently beyond the focus of the two related papers mentioned in that thread (fasttext and Parikh's).
","[7, 6, 5]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Marginally below acceptance threshold']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer provides a positive sentiment with minor criticisms. They highlight the thoroughness, clarity, and insightful analysis of the paper. While they point out limitations like the focus on English data and potential issues with OOV words, these are presented as areas for further exploration rather than strong drawbacks. The reviewer even acknowledges the value of the paper in prompting the need for more challenging datasets. The tone throughout is constructive and polite, suggesting areas for improvement and seeking clarification.",75.0,90.0
Discovering the mechanics of hidden neurons,"['Simon Carbonnelle', 'Christophe De Vleeschouwer']",Reject,2018,"[1, 23]","[5, 28]","[5, 164]","[2, 88]","[3, 33]","[0, 43]","--------------------
Review updates:
Rating 6 -> 7
Confidence 2 -> 4

The rebuttal and update addressed a number of my concerns, cleared up confusing sections, and moved the paper materially closer to being publication-worthy, thus I’ve increased my score.
--------------------

I want to love this paper. The results seem like they may be very important. However, a few parts were poorly explained, which led to this reviewer being unable to follow some of the jumps from experimental results to their conclusions. I would like to be able to give this paper the higher score it may deserve, but some parts first need to be further explained.

Unfortunately, the largest single confusion I had is on the first, most basic set of gradient results of section 4.1. Without understanding this first result, it’s difficult to decide to what extent the rest of the paper’s results are to be believed.

Fig 1 shows “the histograms of the average sign of partial derivatives of the loss with respect to activations, as collected over training for a random neuron in five different layers.” Let’s consider the top-left subplot of Fig 1, showing a heavily bimodal distribution (modes near -1 and +1.). Is this plot made using data from a single neuron or from  multiple neurons? For now let’s assume it is for a single neuron, as the caption and text in 4.1 seem to suggest. If it is for a single neuron, then that neuron will have, for a single input example, a single scalar activation value and a single scalar gradient value. The sign of the gradient will either be +1 or -1. If we compute the sign for each input example and then AGGREGATE over all training examples seen by this neuron over the course of training (or a subset for computational reasons), this will give us a list of signs. Let’s collect these signs into a long list: [+1, +1, +1, -1, +1, +1, …]. Now what do we do with this list? As far as I can tell, we can either average it (giving, say, .85 if the list has far more +1 values than -1 values) OR we can show a histogram of the list, which would just be two bars at -1 and +1. But we can’t do both, indicating that some assumption above was incorrect. Which assumption in reading the text was incorrect?

Further in this direction, Section 4.1 claims “Zero partial derivatives are ignored to make the signal more clear.” Are these zero partial derivatives of the post-relu or pre-relu? The text (Sec 3) points to activations as being post-relu, but in this case zero-gradients should be a very small set (only occuring if all neurons on the next layer had either zero pre-relu gradients, which is common for individual neurons but, I would think, not for all at once). Or does this mean the pre-relu gradient is zero, e.g. the common case where the gradient is zeroed because the pre-activation was negative and the relu at that point has zero slope? In this case we would be excluding a large set (about half!) of the gradient values, and it didn’t seem from the context in the paper that this would be desirable.

It would be great if the above could be addressed. Below are some less important comments.

Sec 5.1: great results!

Fig 3: This figure studies “the first and last layers of each network”. Is the last layer really the last linear layer, the one followed by a softmax? In this case there is no relu and the 0 pre-activation is not meaningful (softmax is shift invariant). Or is the layer shown (e.g. “stage3layer2”) the penultimate layer? Minor: in this figure, it would be great if the plots could be labeled with which networks/datasets they are from.

Sec 5.2 states “neuron partitions the inputs in two distinct but overlapping categories of quasi equal size.” This experiment only shows that this is true in aggregate, not for specific neurons? I.e. the partition percent for each neuron could be sampled from U(45, 55) or from U(10, 90) and this experiment would not tell us which, correct? Perhaps this statement could be qualified.

Table 1: “52th percentile vs actual 53 percentile shown”. 

> Table 1: The more fuzzy, the higher the percentile rank of the threshold

This is true for the CIFAR net but the opposite is true for ResNet, right?
","[7, 4, 5]","[' Good paper, accept', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer explicitly states wanting to give a higher score, but cannot do so until major confusions are addressed. This indicates an overall positive sentiment, but hampered by a lack of clarity. The reviewer's tone is quite polite, using phrases like ""It would be great if..."" and ""Perhaps this statement could be qualified."" The numerous detailed questions, while critical, are presented in a constructive and inquisitive manner rather than accusatory.",40.0,80.0
TD Learning with Constrained Gradients,"['Ishan Durugkar', 'Peter Stone']",Reject,2018,"[6, 29]","[10, 34]","[23, 671]","[12, 437]","[11, 121]","[0, 113]","Summary: This paper tackles the issue of combining TD learning methods with function approximation. The proposed algorithm constrains the gradient update to deal with the fact that canonical TD with function approximation ignores the impact of changing the weights on the target of the TD learning rule. Results with linear and non-linear function approximation highlight the attributes of the method.

Quality: The quality of the writing, notation, motivation, and results analysis is low. I will give a few examples to highlight the point. The paper motivates that TD is divergent with function approximation, and then goes on to discuss MSPBE methods that have strong convergence results, without addressing why a new approach is needed. There are many missing references: ETD, HTD, mirror-prox methods, retrace, ABQ. Q-sigma. This is a very active area of research and the paper needs to justify their approach. The paper has straightforward technical errors and naive statements: e.g. the equation for the loss of TD takes the norm of a scalar. The paper claims that it is not well-known that TD with function approximation ignores part of the gradient of the MSVE. There are many others.

The experiments have serious issues. Exp1 seems to indicate that the new method does not converge to the correct solution. The grid world experiment is not conclusive as important details like the number of episodes and how parameters were chosen was not discussed. Again exp3 provides little information about the experimental setup.

Clarity: The clarity of the text is fine, though errors make things difficult sometimes. For example The Bhatnagar 2009 reference should be Maei.
 
Originality: As mentioned above this is a very active research area, and the paper makes little effort to explain why the multitude of existing algorithms are not suitable. 

Significance: Because of all the things outlined above, the significance is below the bar for this round. ","[2, 3, 4]","[' Strong rejection', ' Clear rejection', ' Ok but not good enough - rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is highly critical of the paper, pointing out serious flaws in the quality of the work, lack of originality, and unconvincing experimental validation. The reviewer uses strong language like ""straightforward technical errors,"" ""naive statements,"" and ""serious issues"" to express their negative opinion. However, the criticism is directed towards the work itself and not the authors, maintaining a professional tone.",-75.0,25.0
A Neural Method for Goal-Oriented Dialog Systems to interact with Named Entities,"['Janarthanan Rajendran', 'Jatin Ganhotra', 'Xiaoxiao Guo', 'Mo Yu', 'Satinder Singh']",Reject,2018,"[4, 4, 6, 10, 28]","[9, 8, 11, 15, 33]","[31, 29, 86, 187, 291]","[11, 13, 39, 86, 189]","[18, 13, 38, 93, 72]","[2, 3, 9, 8, 30]","Properly capturing named entities for goal oriented dialog is essential, for instance location, time and cuisine for restaurant reservation. Mots successful approaches have argued for separate mechanism for NE captures, that rely on various hacks and tricks. This paper attempt to propose a comprehensive approach offers intriguing new ideas, but is too preliminary, both in the descriptions and experiments. 

The proposed methods and experiments are not understandable in the current way the paper is written: there is not a single equation, pseudo-code algorithm or pointer to real code to enable the reader to get a detailed understanding of the process. All we have a besides text is a small figure (figure 1). Then we have to trust the authors that on their modified dataset, the accuracies of the proposed method is around 100% while not using this method yields 0% accuracies?

The initial description (section 2)  leaves way too many unanswered questions:
- What embeddings are used for words detected as NE? Is it the same as the generated representation?
- What is the exact mechanism of generating a representation for NE EECS545? (end of page 2)
- Is it correct that the same representation stored in the NE table is used twice? (a) To retrieve the key (a vector) given the value (a string)  as the encoder input. (b) To find the value that best matches a key at the decoder stage?
- Exact description of the column attention mechanism: some similarity between a key embedding and embeddings representing each column? Multiplicative? Additive?
- How is the system supervised? Do we need to give the name of the column the Attention-Column-Query attention should focus on? Because of this unknown, I could not understand the experiment setup and data formatting!

The list goes on...

For such a complex architecture, the authors must try to analyze separate modules as much as possible. As neither the QA and the Babi tasks use the RNN dialog manager, while not start with something that only works at the sentence level

The Q&A task could be used to describe a simpler system with only a decoder accessing the DB table. Complexity for solving the Babi tasks could be added later.
","[3, 6, 4]","[' Clear rejection', ' Marginally above acceptance threshold', ' Ok but not good enough - rejection']","[3, 3, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer expresses intrigue in the paper's ideas but finds the work too preliminary. They heavily criticize the lack of clarity in the methodology, citing missing equations, pseudo-code, and detailed explanations. The numerous unanswered questions and overall confusion lead to a negative sentiment. However, the reviewer provides constructive feedback and suggestions for improvement, indicating a professional and polite tone despite the criticisms.",-50.0,50.0
On Optimality Conditions for Auto-Encoder Signal Recovery,"['Devansh Arpit', 'Yingbo Zhou', 'Hung Q. Ngo', 'Nils Napp', 'Venu Govindaraju']",Reject,2018,"[8, 9, 20, 14, 30]","[13, 14, 25, 18, 35]","[45, 93, 127, 46, 352]","[20, 41, 63, 32, 261]","[25, 46, 29, 5, 12]","[0, 6, 35, 9, 79]","*Summary*
The paper studies recovery guarantees within the context of auto-encoders. Assuming a noise-corrupted linear model for the inputs x's, the paper looks at some sufficient properties (e.g., over the generating dictionary denoted by W) to recover the true underlying sparse signals (denoted by h). Several settings of increasing complexity are considered (from binary signals with no noise to noisy continuous signals). Evaluations are carried out on synthetic examples to highlight the theoretical findings.

The paper is overall difficult to read. Moreover, and importantly, no algorithmic perspectives are presented in the paper, in the sense that we do not know whether practical procedures would lead to W's satisfying the appropriate properties (unlike (not-mentioned) recent results for dictionary learning/ICA; see detailed comments). Also, assumptions are made (e.g., knowledge about expectations of h and x) for which it is unclear to see how practical/limiting they are. Finally (and as further discussed below), the paper does not sufficiently discuss related work.

(note: I have not reviewed the appendix and supplementary material)

*Detailed comments*

-I think there is an insufficient literature review about recent recovery results in the context of sparse coding, dictionary learning and ICA (see some references at the bottom of the review). I think this is all the more important as the paper tries to draw connections with ICA (see Sec. 4.4).
Given that the paper positions itself on a theoretical level, detailed comparisons with existing sample complexities obtained in previous work for related models (e.g., sparse coding, dictionary learning and ICA) must be provided.

-To the best of my understanding of the paper, the guarantees are about h_hat and the true h. It therefore seems that the paper's approach is very close to standard sparse inverse problems, up to the difference due to the (non-identity) activation function. If this is indeed the case, the paper should discuss its results when the activation is identity to see whether known results are recovered. 

-""...we consider linear activation s_d because it is a more general case."": Just after this statement, it is mentioned that non-linear activations are used in practice. Could this statement be therefore clarified?

-Sec. 2 is unclear. For instance, it is not easy to see how one go from (1) to (2). Moreover, the concept of ""AE framework"" is not well defined.

-In the bottom of page 3, why are p_i and (1-p_i) discarded?

-In practice, how can we set the appropriate value of b_i?

-What is the practical sense of being able to have access to E_h[x], E_x[x], and E_h[h]?

-In Proposition 1 and 2, if the noise e is indeed random, it means the right-hand sides are also random variables. Then, what does the probability statement Pr mean on the left-hand side? Is is conditioned on the draw of e? Some clarifications are required.

-Typo page 7: ""...that used to generate the data."" --> ""... used to generate the data.""
-Typo page 9: ""...data are then generate..."" --> ""...data are then generated...""

-In Sec. 5.3, to match W_hat and W, the Hungarian algorithm can probably be used.

*References*

(Arora2012) Arora, S.; Ge, R.; Moitra, A. & Sachdeva, S. Provable ICA with unknown Gaussian noise, with implications for Gaussian mixtures and autoencoders Advances in Neural Information Processing Systems (NIPS), 2012, 2375-2383

(Arora2013) Arora, S.; Ge, R. & Moitra, A. New algorithms for learning incoherent and overcomplete dictionaries preprint arXiv:1308.6273, 2013

(Chatterji2017) Chatterji, N. S. & Bartlett, P. L. Alternating minimization for dictionary learning with random initialization preprint arXiv:1711.03634, 2017

(Gribonval2015) Gribonval, R.; Jenatton, R. & Bach, F. Sparse and spurious: dictionary learning with noise and outliers IEEE Transactions on Information Theory, 2015, 61, 6298-6319

(Sun2015) Sun, J.; Qu, Q. & Wright, J. Complete dictionary recovery over the sphere Sampling Theory and Applications (SampTA), 2015 International Conference on, 2015, 407-410
","[4, 5, 5]","[' Ok but not good enough - rejection', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with a neutral summary of the paper's topic and content. However, it quickly transitions into a critique of the paper's clarity, practicality, and related work discussion. The reviewer uses phrases like ""difficult to read,"" ""importantly, no algorithmic perspectives,"" ""unclear,"" ""limiting,"" and ""insufficient"" which all point towards a negative sentiment. Despite the criticism, the language remains professional and avoids harsh or disrespectful tones, indicating a polite approach.",-50.0,50.0
Tree2Tree Learning with Memory Unit,"['Ning Miao', 'Hengliang Wang', 'Ran Le', 'Chongyang Tao', 'Mingyue Shang', 'Rui Yan', 'Dongyan Zhao']",Reject,2018,"[1, 'no_match', 0, 2, 1, 15, 'no_match']","[6, 'no_match', 3, 7, 6, 20, 'no_match']","[18, 'no match', 5, 88, 17, 365, 'no match']","[8, 'no match', 5, 44, 10, 158, 'no match']","[10, 'no match', 0, 42, 7, 44, 'no match']","[0, 'no match', 0, 2, 0, 163, 'no match']","This paper presents a model to encode and decode trees in distributed representations. 
This is not the first attempt of doing these encoders and decoders. However, there is not a comparative evalution with these methods.
In fact, it has been demonstrated that it is possible to encode and decode trees in distributed structures without learning parameters, see ""Decoding Distributed Tree Structures"" and ""Distributed tree kernels"".
The paper should present a comparison with such kinds of models.
","[4, 5, 2]","[' Ok but not good enough - rejection', ' Marginally below acceptance threshold', ' Strong rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is critical of the paper for not comparing their model with existing parameter-free models, which indicates a negative sentiment. However, the language used is factual and not directly insulting or dismissive, suggesting a neutral politeness level.",-50.0,0.0
Convergence rate of sign stochastic gradient descent for non-convex functions,"['Jeremy Bernstein', 'Kamyar Azizzadenesheli', 'Yu-Xiang Wang', 'Anima Anandkumar']",Reject,2018,"[2, 3, 8, 13]","[7, 8, 13, 18]","[23, 90, 143, 419]","[10, 26, 62, 154]","[13, 56, 68, 223]","[0, 8, 13, 42]","The paper presents convergence rate of a quantized SGD, with biased quantization - simply taking a sign of each element of gradient.

The stated Theorem 1 is incorrect. Even if the stated result was correct, it presents much worse rate for a weaker notion of convergence.

Major flaws:
1. As far as I can see, Theorem 1 should depend on 4th root of N_K, the last (omitted) step from the proof is done incorrectly. This makes it much worse than presented.
2. Even if this was correct, the main point is that this is ""only"" d times worse - see eq (11). That is enormous difference, particularly in settings where such gradient compression can be relevant. Also, it is lot more worse than just d times:
3. Again in eq (11), you compare different notions of convergence - E[||g||_1]^2 vs. E[||g||_2^2]. In particular, the one for signSGD is the weaker notion - squared L1 norm can be d times bigger again. If this is not the case for some reason, more detailed explanation is needed.

Other than that, the paper contains several attempts at intuitive explanation, which I don't find correct. Inclusion of Assumption 3 would in particular require better justification.

Experiments are also inconclusive, as the plots show convergence to significantly worse accuracy than what the models converged to in original contributions.","[4, 4, 5]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review is highly critical of the paper, pointing out major flaws in the theorem, comparing it unfavorably to existing work, and finding the experiments inconclusive. The language used is direct and critical, but maintains a professional tone without resorting to personal attacks.",-80.0,20.0
Deep Lipschitz networks and Dudley GANs,"['Ehsan Abbasnejad', 'Javen Shi', 'Anton van den Hengel']",Reject,2018,"[8, 12, 21]","[13, 17, 26]","[90, 170, 398]","[40, 67, 166]","[41, 61, 149]","[9, 42, 83]","The authors propose a different type of GAN--the Dudley GAN--that is related to the Dudley metric. In fact, it is very much like the WGAN, but rather than just imposing the function class to have a bounded gradient, they also impose it to be bounded itself. This is argued to be more stable than the WGAN, as gradient clipping is said not necessary for the Dudley GAN. The authors empirically show that the Dudley GAN achieves a greater LL than WGAN for the MNIST and CIFAR-10 datasets.

The main idea [and its variants] looks solid, but with the plethora of GANs in the literature now, after reading I'm still left wondering why this GAN is significantly better than others [BEGAN, WGAN, etc.]. It is clear that imposing the quadratic penalty in equation (3) is really the same constraint as the Dudley norm? The big contribution of the paper seems to be that adding some L_inf regularization to the function class helps preclude gradient clipping, but after reading I'm unsure why this is ""the right thing"" to do in this case. We know that convergence in the Wasserstein metric is stronger than the Dudley metric, so why is using the weaker metric overweighed by the benefits in training?

Nits: Since the function class is parameterized by a NN, the IPM is not actually the Dudley metric between the two distributions. One would have to show that the NN is dense in Dudley unit ball w.r.t. L_inf norm, but this sort of misnaming had started with the ""Wasserstein"" GAN.","[5, 8, 5]","[' Marginally below acceptance threshold', ' Top 50% of accepted papers, clear accept', ' Marginally below acceptance threshold']","[3, 4, 1]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', "" The reviewer's evaluation is an educated guess""]","The reviewer acknowledges the novelty of the proposed Dudley GAN and its potential benefits over existing GANs like WGAN. However, they express reservations about its overall significance and question the justification for certain design choices. While the reviewer maintains a professional tone, their skepticism and probing questions suggest a slightly negative sentiment.",-20.0,60.0
Reward Estimation via State Prediction,"['Daiki Kimura', 'Subhajit Chaudhury', 'Ryuki Tachibana', 'Sakyasingha Dasgupta']",Reject,2018,"[6, 5, 18, 7]","[10, 10, 21, 10]","[34, 46, 60, 34]","[20, 19, 37, 12]","[14, 22, 19, 15]","[0, 5, 4, 7]","The authors propose to solve the inverse reinforcement learning problem of inferring the reward function from observations of a behaving agent, i.e. trajectories, albeit without observing state-action pairs as is common in IRL but only with the state sequences. This is an interesting problem setting. But, apparently, this is not the problem the authors actually solve, according to eq. 1-5. Particularly eq. 1 is rather peculiar. The main idea of RL in MDPs is that agents do not maximize immediate rewards but instead long term rewards. I am not sure how this greedy action should result in maximizing the total discounted reward along a trajectory. 
Equation 3 seems to be a cost function penalizing differences between predicted and observed states. As such, it implements a sort of policy imitation, but that is quite different from the notion of reward in RL and IRL. Similarly, equation 4 penalizes differences between predicted and observed state transitions. 
Essentially, the current manuscript does not learn the reward function of an MDP in the RL setting, but it learns some sort of a shaping reward function to do policy imitation, i.e. copy the behavior of the demonstrator as closely as possible. This is not learning the underlying reward function. So, in my view, the manuscript does a nice job at policy fitting, but this is not reward estimation. The manuscript has to be rewritten that way. 
One could also argue that the manuscript would profit from a better theoretical analysis of the IRL problem, say:
C. A. Rothkopf, C. Dimitrakakis. Preference elicitation and inverse reinforcement learning. ECML 2011
Overall the manuscript leverages on deep learning’s power of function approximation and the simulation results are nice, but in terms of the soundness of the underlying RL and IRL theory there is some work to do.","[4, 5, 3]","[' Ok but not good enough - rejection', ' Marginally below acceptance threshold', ' Clear rejection']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer finds the paper's premise interesting but takes issue with the methodology and framing. They argue the approach is closer to policy imitation than true reward learning, suggesting a significant rewrite is needed. While acknowledging the positive aspects (use of deep learning, simulation results), the critique of the theoretical grounding pulls the overall sentiment down. The language, while direct, avoids overtly negative phrasing.",-20.0,60.0
Convolutional Mesh Autoencoders for 3D Face Representation,"['Anurag Ranjan', 'Timo Bolkart', 'Michael J. Black']",Reject,2018,"[4, 7, 33]","[9, 11, 38]","[40, 55, 383]","[19, 25, 205]","[20, 24, 116]","[1, 6, 62]","Paper summary:
Authors extend [1] to form an auto-encoder CNN network for face mesh representation. Face mesh graph is represented by Fourier basis of graph Laplacian and therefore convolution operator is defined in Fourier space. Chebyshev polynomial is used for faster computations. Max pooling on graph is done by using Graclus multilevel clustering algorithm. Binary tree generated in pooling layers are kept for unpooling layers in decoder network. Authors captured a new facial dataset for their evaluation and reported better results than PCA.

Positive points:
Authors tackle irregular data feature extraction and learning using CNNs which is a hot topic in deep learning.

Negative points:
Although proposed idea is interesting, paper has a number of critical problems. Firstly, experiments are the main weakness of the paper. Set of experiments does not prove claims of the paper. 
- It is not clear how authors uses PCA to reconstruct faces in the test set.
- Authors do not compare to any state of the art on 3D face representation and reconstruction (e.g. [2]) using public datasets (e.g. BU-3DFE). 
- How network behaves by introducing noise on vertices?
- What is the effect of network hyper-parameters?

Secondly, paper has a lack of novelty. It is a simple extension of [1] without considering and solving problems in [1]. Also, it is not mentioned what is the loss function to train the network. I suppose it is L2 norm loss, but it must be clear in the paper.

[1] M. Defferrard, X. Bresson, and P. Vandergheynst.  Convolutional neural networks on graphs with fast localized spectral filtering. In Advances in Neural Information Processing Systems, pp. 3844–3852, 2016.
[2] A. Brunton, T. Bolkart, and S. Wuhrer.  Multilinear wavelets: A statistical shape space for human faces. In European Conference on Computer Vision, pp. 297–312, 2014a.

After rebuttal:
The current version of the paper still needs significant amount of work regarding the experimental part.","[2, 6, 4]","[' Strong rejection', ' Marginally above acceptance threshold', ' Ok but not good enough - rejection']","[5, 3, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review acknowledges the interesting aspects of the paper but highlights significant concerns about the experimental validation and novelty. The reviewer lists several crucial missing comparisons and analyses, indicating a lack of rigor in the research. While the language is direct and critical, it maintains a professional and constructive tone, suggesting improvements rather than outright rejection.",-30.0,50.0
Unsupervised Adversarial Anomaly  Detection using One-Class Support Vector Machines,"['Prameesha Sandamal Weerasinghe', 'Tansu Alpcan', 'Sarah Monazam Erfani', 'Christopher Leckie']",Reject,2018,"['no_match', 18, 8, 28]","['no_match', 23, 13, 33]","['no match', 216, 127, 308]","['no match', 123, 73, 190]","['no match', 37, 45, 36]","['no match', 56, 9, 82]","Although the problem addressed in the paper seems interesting, but there lacks of evidence to support some of the arguments that the authors make. And the paper does not contribute novelty to representation learning, therefore, it is not a good fit for the conference. Detailed critiques are as following:
1. The idea proposed by the authors seems too quite simple. It is just performing random projections for 1000 times and choose the set of projection parameters that results in the highest compactness as the dimensionality reduction model parameter before one-class SVM.
2. It says in the experiments part that the authors have used 3 different S_{attack} values, but they only present results for S_{attack} = 0.5. It would be nicer if they include results for all S_{attack} values that they have used in their experiments, which would also give the reader insights on how the anomaly detection performance degrades when the S_attack value change.
3. The paper claims that the nonlinear random projection is a defence against adversary due to the randomness, but there is no results in the paper proving that other non-random projections are susceptible to adversary that is designed to target that projection mechanism and nonlinear random projection is able to get away with that. And PCA as a non-random projection would a nice baseline to compare against.
4. The paper seems to misuse the term “False positive rate” as the y label of figure 3(d/e/f). The definition of false positive rate is FP/(FP+TN), so if the FPR=1 it means that all negative samples are labeled as positive. So it is surprising to see FPR=1 in Figure 3(d) when feature dimension=784 while the f1 score is still high in Figure 3(a). From what I understand, the paper means to present the percentage of adversarial examples that are misclassified instead of all the anomaly examples that get misclassified. The paper should come up with a better term for that evaluation.
5. The conclusion, that robustness of the learned model increases wrt the integrity attacks increases when the projection dimension becomes lower, cannot be drawn from Figure 3(d). Need more experiment on more dimensionality to prove that. 
6. In the appendix B results part, sometimes the word ’S_attack’ is typed wrong. And the values in  “distorted/distorted” columns in Table 5 do not match up with the ones in Figure 3(c).","[4, 4, 4]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with a negative sentiment, stating the paper lacks evidence and novelty. However, the tone is critical but professional, focusing on specific improvements. The reviewer provides constructive feedback and suggestions for improvement, indicating a desire to see the paper strengthened rather than outright rejection.  Therefore, the sentiment leans slightly negative, but the politeness remains relatively neutral.",-20.0,20.0
Learning to navigate by distilling visual information and natural language instructions,"['Abhishek Sinha', 'Akilesh B', 'Mausoom Sarkar', 'Balaji Krishnamurthy']",Reject,2018,"[8, 1, 13, 31]","[13, 2, 18, 36]","[102, 2, 28, 97]","[38, 1, 12, 45]","[48, 1, 16, 51]","[16, 0, 0, 1]","Interesting Problem, but Limited Novelty and Flawed Evaluation


The paper considers the problem of following natural language instructions given an first-person view of an a priori unknown environment. The paper proposes a neural architecture that employs an RNN to encode the language input and a CNN to encode the visual input. The two modalities are fused and fed to an RNN policy network. The method is evaluated on a new dataset consisting of short, simple instructions conveyed in simple environments.

The problem of following free-form navigation instructions is interesting and has achieved a fair bit of attention, previously with ""traditional"" structured approaches (rule-based and learned) and more recently with neural models. Unlike most existing work, this paper reasons over the raw visual input (vs., a pre-processed representation such as a bag-of-words model). HoA notable exception is the work of Chaplot et al. 2017, which addresses the same problem with a nearly identical architecture (see below). Overall, this paper constitutes a reasonable first-pass at this problem, but there is significant room for improvement to address issues related to the stated contributions and flawed evaluations.

The paper makes several claims regarding the novelty and expressiveness of the model and the contributions of the paper that are either invalid or not justified by the experimental results. As noted, a neural approach to instruction following is not new (see Mei et al. 2016) nor is a multimodal fusion architecture that incorporates raw images (see Chaplot et al.). The paper needs to make the contributions and novelty relative to existing methods clear (e.g., those stated in the intro are nearly identical to those of Mei et al. and Chaplot et al.). This includes discussion of the attention mechanism, for which the contributions and novelty are justified only by simple visualizations that are not very insightful. Related, the paper omits a large body of work in language understanding from the NLP and robotics domains, e.g., the work of Yoav Artzi, Thomas Howard, and Stefanie Tellex, among others (see below). While the approaches are different, it is important to describe this work in the context of these methods.


There are important shortcomings with the evaluation. First, one of the two scenarios involves testing on instructions from the training set. The test set should only include held-out environments and instructions, which the paper incorrectly refers to as the ""zero-shot"" scenario. This test set is very small, with only 19 instructions. Related, there is no mention of a validation set, and the discussion seems to suggest that hyperparameters were tuned on the test set. Further, the method is compared to incomplete implementations of existing baselines that admittedly don't attempt to replicate the baseline architectures. Consequently, it isn't clear what if anything can be concluded from the evaluation. There is a



Comments/Questions

* The action space does not include an explicit stop action. Instead, a run is considered to be finished either when the agent reaches the destination or a timeout is exceeded. This is clearly not valid in practice. The model should determine when to stop, as with existing approaches.

* The paper makes strong claims regarding the sophistication of the dataset that are unfounded. Despite the claims, the environment is rather small and the instructions almost trivially simple. For example, compare to the SAIL corpus that includes multi-sentence instructions with an average of 5 sentences/instruction (vs. 2); 37 words/instruction (vs. a manual cap of 9); and a total of 660 words (vs. 40); and three ""large"" virtual worlds (vs. 10x10 grids with 3-6 objects).

* While the paper makes several claims regarding novelty, the contributions over existing approaches are unclear. For example, Chaplot et al. 2017 propose a similar architecture that also fuses a CNN-based representation of raw visual input with an RNN encoding of language, the result of which is fed to a RNN policy network. What is novel with the proposed approach and what are the advantages? The paper makes an incomplete attempt to evaluate the proposed model against Chaplot et al., but without implementing their complete architecture, little can be inferred from the comparison.

* The paper claims that the fusion method realizes a *minimalistic* representation, but this statement is only justified by an experiment that involves the inclusion of the visual representation, but it isn't clear what we can conclude from this comparison (e.g., was there enough data to train this new representation?).

* It isn't clear that much can be concluded from the attention visualizations in Figs. 6 and 7, particularly regarding its contribution. Regarding Fig 6. the network attends to the target object (large apple), but not the smaller apple, which would be necessary to reason over their relative size. Further, the attention figure in Fig. 7(b) seems to foveate on both bags. In both cases, the distractor objects are very close to the true target, and one would expect the behavior to be similar irrespective of which one was being attended to.

* The conclusion states that the method is ""highly flexible"" and able to handle a ""rich set of natural language instructions"". Neither of these claims are justified by the discussion (please elaborate on what makes the method ""highly flexible"", presumably the end-to-end nature of the architecture) or the experimental results.

* The significance of randomly moving non-target objects that the agent encounters is unclear. What happens when the objects are not moved, as in real scenarios?

* A stated contribution is that the ""textual representations are semantically meaningful"" but the importance is not justified.

* Figure captions should appear below the figure, not at top.

* Figures and tables should appear as close to their first reference as possible (e.g., Table 1 is 6 pages away from its reference at the beginning of Section 7).


* Many citations should be enclosed in parentheses.



References:

* Artzi and Zettlemoyer, Weakly Supervised Learning of Semantic Parsers for Mapping Instructions to Actions, TACL 2013

* Howard, Tellex, and Roy, A Natural Language Planner Interface for Mobile Manipulators, ICRA 2014

* Chung, Propp, Walter, and Howard, On the performance of hierarchical distributed correspondence graphs for efficient symbol grounding of robot instructions, IROS 2015

* Paul, Arkin, Roy, and Howard, Efficient Grounding of Abstract Spatial Concepts for Natural Language Interaction with Robot Manipulators, RSS 2016

* Tellex, Kollar, Dickerson, Walter, Banerjee, Teller and Roy, Understanding natural language commands for robotic navigation and mobile manipulation, AAAI 2011","[4, 4, 5]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[5, 4, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review is highly critical of the paper, pointing out significant flaws in the novelty, methodology, and evaluation. The reviewer uses strong language like ""invalid,"" ""flawed evaluations,"" ""unfounded,"" and ""unclear"" to express their negative opinion. However, the criticism is presented in a professional and constructive manner, suggesting specific areas for improvement. ",-50.0,50.0
Bayesian Time Series Forecasting with Change Point and Anomaly Detection,"['Anderson Y. Zhang', 'Miao Lu', 'Deguang Kong', 'Jimmy Yang']",Reject,2018,"[4, 16, 12, 8]","[9, 21, 17, 13]","[16, 41, 51, 5]","[1, 18, 40, 1]","[12, 9, 7, 1]","[3, 14, 4, 3]","Minor comments:
- page 3. “The observation equation and transition equations together (i.e., Equation (1,2,3)) together define “ - one “together” should be removed
- page 4. “From Figure 2, the joint distribution (i.e., the likelihood function ” - there should be additional bracket
- page 7. “We can further integral out αn “ -> integrate out

Major comments:
The paper is well-written. The paper considers structural time-series model with seasonal component and stochastic trend, which allow for change-points and structural breaks.

Such type of parametric models are widely considered in econometric literature, see e.g.
[1] Jalles, João Tovar, Structural Time Series Models and the Kalman Filter: A Concise Review (June 19, 2009). FEUNL Working Paper No. 541. Available at SSRN: https://ssrn.com/abstract=1496864 or http://dx.doi.org/10.2139/ssrn.1496864 
[2] Jacques J. F. Commandeur, Siem Jan Koopman, Marius Ooms. Statistical Software for State Space Methods // May 2011, Volume 41, Issue 1.
[3] Scott, Steven L. and Varian, Hal R., Predicting the Present with Bayesian Structural Time Series (June 28, 2013). Available at SSRN: https://ssrn.com/abstract=2304426 or http://dx.doi.org/10.2139/ssrn.2304426 
[4] Phillip G. Gould, Anne B. Koehler, J. Keith Ord, Ralph D. Snyder, Rob J. Hyndman, Farshid Vahid-Araghi, Forecasting time series with multiple seasonal patterns, In European Journal of Operational Research, Volume 191, Issue 1, 2008, Pages 207-222, ISSN 0377-2217, https://doi.org/10.1016/j.ejor.2007.08.024.
[5] A.C. Harvey, S. Peters. Estimation Procedures for structural time series models // Journal of Forecasting, Vol. 9, 89-108, 1990
[6] A. Harvey, S.J. Koopman, J. Penzer. Messy Time Series: A Unified approach // Advances in Econometrics, Vol. 13, pp. 103-143.

They also use Kalman filter and MCMC-based approaches to sample posterior to estimate hidden components.

There are also non-parametric approaches to extraction of components from quasi-periodic time-series, see e.g.
[7] Artemov A., Burnaev E. Detecting Performance Degradation of Software-Intensive Systems in the Presence of Trends and Long-Range Dependence // 16th International Conference on Data Mining Workshops (ICDMW), IEEE Conference Publications, pp. 29 - 36, 2016. DOI: 10.1109/ICDMW.2016.0013
[8] Alexey Artemov, Evgeny Burnaev and Andrey Lokot. Nonparametric Decomposition of Quasi-periodic Time Series for Change-point Detection // Proc. SPIE 9875, Eighth International Conference on Machine Vision, 987520 (December 8, 2015); 5 P. doi:10.1117/12.2228370;http://dx.doi.org/10.1117/12.2228370

In some of these papers models of structural brakes and change-points are also considered, see e.g. 
- page 118 in [6]
- papers [7, 8]

There were also Bayesian approaches for change-point detection, which are similar to the model of change-point, proposed in the considered paper, e.g.
[9] Ryan Prescott Adams, David J.C. MacKay. Bayesian Online Changepoint Detection // https://arxiv.org/abs/0710.3742
[10] Ryan Turner, Yunus Saatçi, and Carl Edward Rasmussen. Adaptive sequential Bayesian change point detection. In Zaïd Harchaoui, editor, NIPS Workshop on Temporal Segmentation, Whistler, BC, Canada, December 2009.

Thus,
- the paper does not provide comparison with relevant econometric literature on parametric structural time-series models,
- the paper does not provide comparison with relevant advanced change-point detection methods e.g. [7,8,9,10]. The comparison is provided only with very simple methods,
- the proposed model itself looks very similar to what can be found across econometric literature,
- the datasets, used for comparison, are very scarce. There are datasets for anomaly detection in time-series data, which should be used for extensive comparison, e.g. Numenta Anomaly Detection Benchmark.

Therefore, also the paper is well-written, 
- it lacks novelty,
- its topic does not perfectly fit topics of interest for ICLR,
So, I do not recommend this paper to be published.","[4, 5, 6]","[' Ok but not good enough - rejection', ' Marginally below acceptance threshold', ' Marginally above acceptance threshold']","[5, 5, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']","The review starts with a positive sentiment, praising the writing quality and acknowledging the use of established methods. However, it transitions into a negative sentiment by highlighting a lack of novelty and comparison with relevant literature. The reviewer lists several missing references and argues against the paper's suitability for ICLR. The final recommendation is to reject the paper. The numerous ""major comments"" and the lack of strong positive remarks suggest a negative overall sentiment.",-40.0,50.0
Training with Growing Sets: A Simple Alternative to Curriculum Learning and Self Paced Learning,"['Melike Nur Mermer', 'Mehmet Fatih Amasyali']",Reject,2018,"['no_match', 13]","['no_match', 18]","['no match', 60]","['no match', 39]","['no match', 6]","['no match', 15]","This paper addresses an interesting problem of curriculum/self-paced versus random order of samples for faster learning. Specifically, the authors argue that adding samples in random order is as beneficial as adding them with some curriculum strategy, i.e. from easiest to hardest, or reverse. 
The main learning strategy considered in this work is learning with growing sets, i.e. at each next stage a new portion of samples is added to the current available training set. At the last stage, all training samples are considered. The classifier is re-learned on each stage, where optimized weights in the previous stage are given as initial weights in the next stage. 

The work has several flaws. 
-First of all, it is not surprising that learning with more training samples at each next stage (growing sets) gets better - this is the basic principle of learning. The question is how fast the current classifier converges to the optimal Bayes level when using Curriculum strategy versus Random strategy. The empirical evaluations do not show evidence/disprove regarding this matter. For example, it could happen that the classifier converges to the optimal on the first stage already, so there is no difference when training in random versus curriculum order with growing sets. 
-Secondly, easyness/hardness of the samples are defined w.r.t. some pre-trained (external) ensemble method. It is not clear how this definition of easiness/hardness translates when training the 3-layer neural network (final classifier). For example, it could well happen that all the samples are equally easy for training the final classifier, so the curriculum order would be the same as random order. In the original work on self-paced learning, Kumar et al (2010), easiness of the samples is re-computed on each stage of the classifier learning. 
-The empirical evaluations are not clear. Just showing the wins across datasets without actual performance is not convincing (Table 2). 
-I wonder whether the section with theoretical explanation is needed. What is the main advantage of learning with growing sets (when re-training the classifier)  and (traditional) learning when using the whole training dataset (last stage, in this work)? 

","[4, 4, 6]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Marginally above acceptance threshold']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer points out several flaws in the paper, questioning the methodology and findings. They don't see the work as groundbreaking and find the empirical evaluations unconvincing. The tone is critical, but the reviewer provides concrete reasons for their skepticism. Overall, the review leans negative due to the identified flaws and lack of strong evidence supporting the paper's claims.",-50.0,50.0
Dense Recurrent Neural Network with Attention Gate,"['Yong-Ho Yoo', 'Kook Han', 'Sanghyun Cho', 'Kyoung-Chul Koh', 'Jong-Hwan Kim']",Reject,2018,"[12, 15, 'no_match', 24]","[16, 16, 'no_match', 29]","[21, 19, 'no match', 262]","[10, 14, 'no match', 170]","[1, 0, 'no match', 19]","[10, 5, 'no match', 73]","The authors propose an RNN that combines temporal shortcut connections from [Soltani & Jang, 2016] and Gated Recurrent Attention [Chung, 2014]. However, their justification about the novelty and efficacy of the model is not well demonstrated in the paper. The experiment part is modest with only one small dataset Penn Tree Bank is used. The results are not significant enough and no comparisons with models in [Soltani & Jang, 2016] and [Chung, 2014] are provided in the paper to show the effectiveness of the proposed combination. To conclude, this paper is an incremental work with limited contributions.

Some writing issues:
1. Lack of support in arguments,
2. Lack of referencing to previous works. For example, the sentence “By selecting the same dropout mask for feedforward, recurrent connections, respectively, the dropout can apply to the RNN, which is called a variational dropout” mentions “variational dropout” with no citing. Or “NARX-RNN and HO-RNN increase the complexity by increasing recurrent depth. Gated feedback RNN has the fully connection between two consecutive timesteps” also mentions a lot of models without any references at all.
3. Some related papers are not cited, e.g., Hierarchical Multiscale Recurrent Neural Networks [Chung, 2016]
","[4, 2, 4]","[' Ok but not good enough - rejection', ' Strong rejection', ' Ok but not good enough - rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is highly critical of the paper, pointing out significant weaknesses in novelty, experimental validation, and writing. The reviewer uses phrases like ""not well demonstrated,"" ""modest,"" ""not significant enough,"" ""limited contributions,"" and ""incremental work"" which all point towards a negative sentiment. However, the language remains professional and avoids harsh or disrespectful language, indicating a level of politeness. ",-60.0,60.0
DOUBLY STOCHASTIC ADVERSARIAL AUTOENCODER,['Mahdi Azarafrooz'],Reject,2018,[9],[9],[10],[3],[4],[3],"This manuscript explores the idea of adding noise to the adversary's play in GAN dynamics over an RKHS. This is equivalent to adding noise to the gradient update, using the duality of reproducing kernels. Unfortunately, the evaluation here is wholly unsatisfactory to justify the manuscript's claims. No concrete practical algorithm specification is given (only a couple of ideas to inject noise listed), only a qualitative one on a 2-dimensional latent space in MNIST, and an inconclusive one using the much-doubted Parzen window KDE method. The idea as stated in the abstract and introduction may well be worth pursuing, but not on the evidence provided by the rest of the manuscript.","[2, 3, 3]","[' Strong rejection', ' Clear rejection', ' Clear rejection']","[5, 5, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with a positive statement acknowledging the potential of the manuscript's idea. However, it quickly transitions into a highly critical assessment of the evaluation methods, using terms like ""wholly unsatisfactory,"" ""inconclusive,"" and questioning the validity of a used method (""much-doubted"").  The final statement, while suggesting the idea is worth pursuing, undermines the manuscript's contribution. This suggests an overall negative sentiment. The language, while critical, maintains a professional and academic tone, avoiding personal attacks or disrespectful language.",-60.0,50.0
GraphGAN: Generating Graphs via Random Walks,"['Aleksandar Bojchevski', 'Oleksandr Shchur', 'Daniel Zügner', 'Stephan Günnemann']",Reject,2018,"[2, 2, 4, 10]","[7, 6, 9, 15]","[38, 15, 39, 274]","[17, 6, 18, 145]","[19, 9, 20, 104]","[2, 0, 1, 25]","This paper proposes a WGAN formulation for generating graphs based on random walks. The proposed generator model combines node embeddings, with an LSTM architecture for modeling the sequence of nodes visited in a random walk; the discriminator distinguishes real from fake walks.

The model is learned from a single large input graph (for three real-world networks) and evaluated against one baseline generative graph model: degree-corrected stochastic block models. 

The primary claims of the paper are as follows:
i) The proposed approach is a generative model of graphs, specifically producing ""sibling"" graphs
ii) The learned latent representation provides an interpretation of generated graph properties
iii) The model generalizes well in terms of link and node classification

The proposed method is novel and the incorporated ideas are quite interesting (e.g., discriminating real from fake random walks, generating random walks from node embeddings and LSTMs). However, from a graph generation perspective, the problem formulation and evaluation do not sufficiently demonstrate the utility of proposed method. 

First, wrt claim (i) the problem of generating ""sibling"" graphs is ill-posed. Statistical graph models are typically designed to generate a probability distribution over all graphs with N nodes and, as such, are evaluated based on how well they model that distribution. The notion of a ""sibling"" graph used in this paper is not clearly defined, but it seems to only be useful if the sibling graphs are likely under the distribution. Unfortunately, the likelihood of the sampled graphs is not explicitly evaluated. On the other hand, since many of the edges are shared the ""siblings"" may be nearly isomorphic to the input graph, which is not useful from a graph modeling perspective. 

For claim (i), the comparison to related work is far from sufficient to demonstrate its utility as a graph generation model. There are many graph models that are superior to DC-SBM, including KPGMs, BETR, ERGMs, hierarchical random graph models and latent space models. Moreover, a very simple baseline to assess the LSTM component of the model, would be to produce a graph by sampling links repeatedly from the latent space of node embeddings. 

Next, the evaluation wrt to claim (ii) is novel and may help developers understand the model characteristics. However, since the properties are measured based on a set of random walks it is still difficult to interpret the impact on the generated graphs (since an arbitrary node in the final graph will have some structure determined from each of the regions). Do the various regions generate different parts of the final graph structure (i.e., focusing on only a subset of the nodes)?   

Lastly, the authors evaluate the learned model on link and node prediction tasks and state that the model's so-so performance supports the claim that the model can generalize. This is the weakest claim of the paper. The learned node embeddings appear to do significantly worse than node2vec, and the full model is worse than DC-SBM. Given that the proposed model is transductive (when there is significant edge overlap) it should do far better than DC-SBM which is inductive. 

Overall, while the paper includes a wide range of experimental evaluation, they are aimed too broadly (and the results are too weak) to support any specific claim of the work. If the goal is to generate transductively (with many similar edges), then it would be better to compare more extensively to alternative node embedding and matrix factorization approaches, and assess the utility of the various modeling choices (e.g., LSTM, in/out embedding). If the goal is to generate inductively, over the full distribution of graphs, then it would be better to (i) assess whether the sampled graphs are isomorphic, and (ii) compare more extensively to alternative graph models (many of which have been published since 2010).  
","[4, 6, 7]","[' Ok but not good enough - rejection', ' Marginally above acceptance threshold', ' Good paper, accept']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is predominantly negative. While it acknowledges the novelty and interesting ideas, it criticizes the paper for its ill-defined problem formulation, insufficient evaluation, and weak claims. The reviewer points out several shortcomings, including the lack of clarity regarding ""sibling"" graphs, insufficient comparison to related work, and weak performance on link and node prediction tasks. The language used, while direct, maintains a professional and constructive tone.",-40.0,60.0
Pointing Out SQL Queries From Text,"['Chenglong Wang', 'Marc Brockschmidt', 'Rishabh Singh']",Reject,2018,"[9, 9, 10]","[14, 14, 14]","[129, 75, 133]","[59, 41, 62]","[36, 29, 50]","[34, 5, 21]","This paper proposes a model for solving the WikiSQL dataset that was released recently.

The main issues with the paper is that its contributions are not new.

* The first claimed contribution is to use typing at decoding time (they don't say why but this helps search and learning). Restricting the type of the decoded tokens based on the programming language has already been done by the Neural Symbolic Machines of Liang et al. 2017. Then Krishnamurthy et al. expanded that in EMNLP 2017 and used typing in a grammar at decoding time. I don't really see why the authors say their approach is simpler, it is only simpler because the sub-language of sql used in wikisql makes doing this in an encoder-decoder framework very simple, but in general sql is not regular. Of course even for CFG this is possible using post-fix notation or fixed-arity pre-fix notation of the language as has been done by Guu et al. 2017 for the SCONE dataset, and more recently for CNLVR by Goldman et al., 2017.

So at least 4 papers have done that in the last year on 4 different datasets, and it is now close to being common practice so I don't really see this as a contribution.

* The authors explain that they use a novel loss function that is better than an RL based function used by Zhong et al., 2017. If I understand correctly they did not implement Zhong et al. only compared to their numbers which is a problem because it is hard to judge the role of optimization in the results.

Moreover, it seems that the problem they are trying to address is standard - they would like to use cross-entropy loss when there are multiple tokens that could be gold. the standard solution to this is to just have uniform distribution over all gold tokens and minimize the cross-entropy between the predicted distribution and the gold distribution which is uniform over all tokens. The authors re-invent this and find it works better than randomly choosing a gold token or taking the max. But again, this is something that has been done already in the context of pointer networks and other work like See  et al. 2017 for summarization and Jia et al., 2016 for semantic parsing.

* As for the good results - the data is new, so it is probable that numbers are not very fine-tuned yet so it is hard to say what is important and what not for final performance. In general I tend to agree that using RL for this task is probably unnecessary when you have the full program as supervision.","[3, 4, 7]","[' Clear rejection', ' Ok but not good enough - rejection', ' Good paper, accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer expresses a negative sentiment, outlining multiple areas where they believe the paper lacks novelty. They cite existing papers to demonstrate prior work on the proposed methods. While the language is direct and critical, it avoids personal attacks and maintains a professional tone.",-75.0,20.0
Semi-supervised Outlier Detection using Generative And Adversary Framework,"['Jindong Gu', 'Matthias Schubert', 'Volker Tresp']",Reject,2018,"[1, 19, 29]","[6, 24, 34]","[39, 111, 308]","[13, 84, 166]","[26, 18, 105]","[0, 9, 37]","The idea of the paper is to use a GAN-like training to learn a novelty detection approach. In contrast to traditional GANs, this approach does not aim at convergence, where the generator has nicely learned to fool the discriminator with examples from the same data distribution. The goal is to build up a series of generators that sample examples close the data distribution boundary but are regarded as outliers. To establish such a behavior, the authors propose early stopping as well as other heuristics. 

I like the idea of the paper, however, this paper needs a revision in various aspects, which I simply list in the following:
* The authors do not compare with a lot of the state-of-the-art in outlier detection and the obvious baselines: SVDD/OneClassSVM without PCA, Gaussian Mixture Model, KNFST, Kernel Density Estimation, etc
* The model selection using the AUC of ""inlier accepted fraction"" is not well motivated in my opinion. This model selection criterion basically leads too a probability distribution with rather steep borders and indirectly prevents the outlier to be too far away from the positive data. The latter is important for the GAN-like training.
* The experiments are not sufficient: Especially for multi-class classification tasks, it is easy to sample various experimental setups for outlier detection. This allows for robust performance comparison. 
* With the imbalanced training as described in the paper, it is quite natural that the confidence threshold for the classification decision needs to be adapted (not equal to 0.5)
* There are quite a few heuristic tricks in the paper and some of them are not well motivated and analyzed (such as the discriminator training from multiple generators)
* A cross-entropy loss for the autoencoder does not make much sense in my opinion (?)


Minor comments:
* Citations should be fixed (use citep to enclose them in ())
* The term ""AI-related task"" sounds a bit too broad
* The authors could skip the paragraph in the beginning of page 5 on the AUC performance. AUC is a standard choice for evaluation in outlier detection.
* Where is Table 1?
* There are quite a lot of typos.

*After revision statement*
I thank the authors for their revision, but I keep my rating. The clarity of the paper has improved but the experimental evaluation is lacking realistic datasets and further simple baselines (as also stated by the other reviewers)","[4, 3, 4]","[' Ok but not good enough - rejection', ' Clear rejection', ' Ok but not good enough - rejection']","[4, 5, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer explicitly states they ""like the idea"", which points towards a positive sentiment. However, they also list many points of improvement, indicating that the paper is not perfect. Therefore, the sentiment is positive but moderate. The language used is formal, professional, and respectful, indicating a high level of politeness.  Even though the reviewer expresses disagreement and criticism, they do so constructively and politely.",60.0,80.0
Graph Topological Features via GAN,"['Weiyi Liu', 'Hal Cooper', 'Min-Hwan Oh']",Reject,2018,"[27, 4]","[32, 8]",['skipped'],['skipped'],['skipped'],['skipped'],"The authors try to combine the power of GANs with hierarchical community structure detections. While the idea is sound, many design choices of the system is questionable. The problem is particularly aggravated by the poor presentation of the paper, creating countless confusions for readers. I do not recommend the acceptance of this draft.

Compared with GAN, traditional graph analytics is model-specific and non-adaptive to training data. This is also the case for hierarchical community structures. By building the whole architecture on the Louvain method, the proposed method is by no means truly model-agnostic. In fact, if the layers are fine enough, a significant portion of the network structure will be captured by the sum-up module instead of the GAN modules, rendering the overall behavior dominated by the community detection algorithm. 

The evaluation remains superficial with minimal quantitative comparisons. Treating degree distribution and clustering coefficient (appeared as cluster coefficient in draft) as global features is problematic. They are merely global average of local topological features which is incapable of capturing true long-distance structures in graphs. 

The writing of the draft leaves much to be desired. The description of the architecture is confusing with design choices never clearly explained. Multiple concepts needs better introduction, including the very name of their model GTI and the idea of stage identification. Not to mention numerous grammatical errors, I suggest the authors seek professional English writing services.","[3, 4, 4]","[' Clear rejection', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with a negative statement, pointing out questionable design choices and poor presentation. The reviewer explicitly recommends against acceptance. Throughout the review, the language is critical, using terms like ""questionable,"" ""problematic,"" and ""superficial."" However, it refrains from personal attacks or overly harsh language, maintaining a level of professional criticism. The recommendation to seek professional writing help, while direct, is a common suggestion in peer reviews.",-60.0,40.0
Fast and Accurate Inference with Adaptive Ensemble Prediction for Deep Networks,['Hiroshi Inoue'],Reject,2018,[22],[26],[95],[40],[4],[51],"In this paper it is described a method that can be used to speed up the prediction process of ensembles of classifiers that output probabilistic predictions. The method proposed is very simple and it is based on the observation that in the case that the individual predictors are very sure about the potential class label, ensembling many predictions is not particularly useful. It seems it is most useful when the individual classifier are most unsure, as measured by the output probabilities. The idea proposed by the authors is to compute an estimate of the probability that the class with the highest probability will not change after querying more predictors from the ensemble. This estimate is obtained by using a t-student distribution for the distribution of the average maximum probability.

The paper is generally well written with a few mistakes that can be easily corrected using any spell checking tool.

The experiments carried out by the authors are convincing. It seems that their proposed approach can speed up the predictions of the ensemble by an important factor. The benefits of using ensemble methods are also evident, since they always improve the performance of a single classifier.

As far as I know this work is original. However, it is true that several similar ensemble pruning techniques exist for multi-class problems in which one uses majority voting for computing the combined prediction of the ensemble. Therefore it is unclear what are the advantages of the proposed method with respect to those ones. This is, in my opinion, the weakest point of the paper.
","[6, 5, 5]","[' Marginally above acceptance threshold', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer provides a generally positive overview of the paper, highlighting its clarity, convincing experiments, and originality. While they point out a potential weakness regarding the comparison with existing techniques, the overall tone suggests a favorable disposition towards the work.",75.0,80.0
Neural Tree Transducers for Tree to Tree Learning,"['João Sedoc', 'Dean Foster', 'Lyle Ungar']",Reject,2018,"[3, 0, 31]","[8, 3, 36]","[70, 7, 226]","[32, 5, 138]","[36, 2, 38]","[2, 0, 50]","The authors propose to tackle the tree transduction learning problem using recursive NN architectures: the prediction of a node label is conditioned on the ancestors sequence and the nodes in the left sibling subtree  (in a serialized order)
Pros:
- they identify the issue of locality as important (sequential serialization distorts locality) and they move the architecture closer to the tree structure of the problem
- the architecture proposed moves the bar forward in the tree processing field
Cons: 
- there is still a serialization step (depth first) that can potentially create sharp dips to null probabilities for marginal changes in the conditioning sequence (the issue is not addressed or commented by the authors) 
- the experimental setup lacks a perturbation test: rather than a copy task, it would be of greater interest to assess the capacity to recover from noise in the labels (as the noise magnitude increases)
- a clearer and more articulated comparison of the pros/cons w.r.t. competitive architectures would improve the quality of the work: what are the properties (depth, vocabulary size, complexity of the underlying generative process, etc) that are best dealt with by the proposed approach? 
- it is not clear if the is the vocabulary size in their model needs to increase exponentially with the tree depth: a crucial vocabulary size  vs performance experiment is missing
","[7, 2, 3]","[' Good paper, accept', ' Strong rejection', ' Clear rejection']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review acknowledges the merits of the paper and its contribution (""moves the bar forward"") but also provides constructive criticism and points out areas of improvement. The language is formal and focused on the scientific content, suggesting a professional and neutral tone rather than overly negative or positive.",25.0,50.0
Explaining the Mistakes of Neural Networks with Latent Sympathetic Examples,"['Riaan Zoetmulder', 'Efstratios Gavves', ""Peter O'Connor""]",Reject,2018,"[-3, 9, 22]","[1, 14, 23]","[1, 137, 32]","[0, 64, 20]","[0, 62, 5]","[1, 11, 7]","Summary: The authors propose a method for explaining why neural networks make mistakes by learning how to modify an image on a mistaken classification to make it a correct classification. They do this by perturbing the image in an encoded latent space and then reconstructing the perturbed image.  The explanation is the difference between the reconstructed perturbed encoded image and the reconstructed original encoded image.

The title is too general as this paper only offers an explanation in the area of image classification, which by itself, is still interesting.  

A method for explaining the results of neural networks is still open ended and visually to the human eye, this paper does offer an explanation of why the 8 is misclassified. However, if this works very well for MNIST, more examples should be given. This single example is interesting but not sufficient to illustrate the success of this method.  

The examples from CelebA are interesting but inconclusive. For example, why should adding blue to the glasses fix the misclassification. If the explanation is simply visual for a human, then this explanation does not suffice. And the two examples with one receding and the other not receding hairlines look like their correct classifications could be flipped.

Regarding epsilon, it is unclear what a small euclidean distance for epsilon is without more examples. It would also help to see how the euclidean distance changes along the path.  But also it is not clear why we care about the size of epsilon, but rather the size of the perturbation that must be made to the original image, which is what is defined in the paper as the explanation. 

Since it is the encoded image that is perturbed, and this is what causes the perturbations to be selective to particular features of the image, an analysis of what features in the encoded space that are modified would greatly help in the interpretability of this explanation. The fact that perturbations are made in the latent space, and that this perturbation gets reflected in particular areas in the reconstructed image, is the most interesting part of this work.  More discussion around this would greatly enhance the paper, especially since the technical tools of this method are not very strong.

Pros: Interesting explanation, visually selects certain parts of the image relevant to classification rather than obscure pixels

Cons: No discussion or analysis about the latent space where perturbations occur. Only one easy example from MNIST shown and examples on CelebA are not great. No way (suggested) to use this tool outside of image recognition. ","[6, 4, 4]","[' Marginally above acceptance threshold', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[4, 5, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']","The review is mostly positive. The reviewer finds the idea interesting and the visual aspect of the explanations appealing. They praise the focus on relevant image features. However, they also point out several limitations, such as the lack of analysis of the latent space, the limited number of examples, and the unclear practical implications beyond image recognition. The language is constructive and not rude.",50.0,70.0
Enhancing the Transferability of Adversarial Examples with Noise Reduced Gradient,"['Lei Wu', 'Zhanxing Zhu', 'Cheng Tai', 'Weinan E']",Reject,2018,"[21, 10, 6, 19]","[26, 15, 10, 24]","[336, 100, 12, 111]","[149, 48, 2, 17]","[38, 44, 6, 64]","[149, 8, 4, 30]","This paper focuses on enhancing the transferability of adversarial examples from one model to another model. The main contribution of this paper is to factorize the adversarial perturbation direction into model-specific and data-dependent. Motivated by finding the data-dependent direction, the paper proposes the noise reduced gradient method. 

The paper is not mature. The authors need to justify their arguments in a more rigorous way, like why data-dependent direction can be obtained by averaging; is it true factorization of the perturbation direction? i.e. is the orthogonal direction is indeed model-specific? most of explanations are not rigorous and kind of superficial.


","[5, 4, 5]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with a neutral summary of the paper's focus and contribution. However, it quickly transitions into a negative sentiment, stating that the paper is 'not mature' and criticizing the arguments as lacking rigor and being 'superficial.' The use of 'not mature' and the directness of the criticism point towards a less polite tone.",-60.0,-20.0
Jiffy: A Convolutional Approach to Learning Time Series Similarity,"['Divya Shanmugam', 'Davis Blalock', 'John Guttag']",Reject,2018,"[1, -2]","[6, 2]","[15, 3]","[5, 1]","[10, 2]","[0, 0]","This paper presents a solid empirical analysis of a simple idea for learning embeddings of time series: training a convolutional network with a custom pooling layer that generates a fixed size representation to classify time series, then use the fixed size representation for other tasks. The primary innovation is a custom pooling operation that looks at a fraction of a sequence, rather than a fixed window. The experiments are fairly thorough (albeit with some sizable gaps) and show that the proposed approach outperforms DTW, as well as embeddings learned using Siamese networks. On the whole, I like the line of inquiry and the elegant simplicity of the proposed approach, but the paper has some flaws (and there are some gaps in both motivation and the experiments) that led me to assign a lower score. I encourage the authors to address these flaws as much as possible during the review period. If they succeed in doing so, I am willing to raise my score.

QUALITY

I appreciate this line of research in general, but there are some flaws in its motivation and in the design of the experiments. Below I list strengths (+) and weaknesses (-):

+ Time series representation learning is an important problem with a large number of real world applications. Existing solutions are often computationally expensive and complex and fail to generalize to new problems (particularly with irregular sampling, missing values, heterogeneous data types, etc.). The proposed approach is conceptually simple and easy to implement, faster to train than alternative metric learning approaches, and learns representations that admit fast comparisons, e.g., Euclidean distance.
+ The experiments are pretty thorough (albeit with some noteworthy gaps) -- they use multiple benchmark data sets and compare against strong baselines, both traditional (DTW) and deep learning (Siamese networks).
+ The proposed approach performs best on average!

- The custom pooling layer is the most interesting part and warrants additional discussion. In particular, the ""naive"" approach would be to use global pooling over the full sequence [4]. The authors should advance an argument to motivate %-length pooling and perhaps add a global pooling baseline to the experiments.
- Likewise, the authors need to fully justify the use of channel-wise (vs. multi-channel) convolutions and perhaps include a multi-channel convolution baseline.
- There is something incoherent about training a convolutional network to classify time series, then discarding the classification layer and using the internal representation as input to a 1NN classifier. While this yields an apples-to-apples comparison in the experiments, I am skeptical anyone would do this in practice. Why not simply use the classifier (I am dubious the 1NN would outperform it)? To address this, I recommend the authors do two things: (1) report the accuracy of the learned classifier; (2) discuss the dynamic above -- either admit to the reader that this is a contrived comparison OR provide a convincing argument that someone might use embeddings + KNN classifier instead of the learned classifier. If embeddings + KNN outperforms the learned classifier, that would surprise me, so that would warrant some discussion.
- On a related note, are the learned representations useful for tasks other than the original classification task? This would strengthen the value proposition of this approach. If, however, the learned representations are ""overfit"" to the classification task (I suspect they are), and if the learned classifier outperforms embeddings + 1NN, then what would I use these representations for?
- I am modestly surprised that this approach outperformed Siamese networks. The authors should report the Siamese architectures -- and how hyperparameters were tuned on all neural nets -- to help convince the reader that the comparison is fair.
- To that end, did the Siamese convolutional network use the same base architecture as the proposed classification network (some convolutions, custom pooling, etc.)? If not, then that experiment should be run to help determine the relative contributions of the custom pooling layer and the loss function.
- Same notes above re: triplet network -- the authors should report results in Table 2 and disclose architecture details.
- A stronger baseline would be a center loss [1] network (which often outperforms triplets).
- The authors might consider adding at least one standard unsupervised baseline, e.g., a sequence-to-sequence autoencoder [2,3].

CLARITY

The paper is clearly written for the most part, but there is room for improvement:

- The %-length pooling requires a more detailed explanation, particularly of its motivation. There appears to be a connection to other time series representations that downsample while preserving shape information -- the authors could explore this. Also, they should add a figure with a visual illustration of how it works (and maybe how it differs from global pooling), perhaps using a contrived example.
- How was the %-length pooling implemented? Most deep learning frameworks only provide pooling layers with fixed length windows, though I suspect it is probably straightforward to implement variable-width pooling layers in an imperative framework like PyTorch.
- Figure 1 is not well executed and probably unnecessary. The solid colored volumes do not convey useful information about the structure of the time series or the neural net layers, filters, etc. Apart from the custom pooling layer, the architecture is common and well understood by the community -- thus, the figure can probably be removed.
- The paper needs to fully describe neural net architectures and how hyperparameters were tuned.

ORIGINALITY

The paper scores low on originality. As the authors themselves point out, time series metric learning -- even using deep learning -- is an active area of research. The proposed approach is refreshing in its simplicity (rather than adding additional complexity on top of existing approaches), but it is straightforward -- and I suspect it has been used previously by others in practice, even if it has not been formally studied. Likewise, the proposed %-length pooling is uncommon, but it is not novel per se (dynamic pooling has been used in NLP [5]). Channel-wise convolutional networks have been used for time series classification previously [6].

SIGNIFICANCE

Although I identified several flaws in the paper's motivation and experimental setup, I think it has some very useful findings, at least for machine learning practitioners. Within NLP, there appears to be gradual shift toward using convolutional, instead of recurrent, architectures. I wonder if papers like this one will contribute toward a similar shift in time series analysis. Convolutional architectures are typically much easier and faster to train than RNNs, and the main motivation for RNNs is their ability to deal with variable length sequences. Convolutional architectures that can effectively deal with variable length sequences, as the proposed one appears to do, would be a welcome innovation.

REFERENCES

[1] Wen, et al. A Discriminative Feature Learning Approach for Deep Face Recognition. ECCV 2016.
[2] Fabius and van Amersfoort. Variational Recurrent Auto-Encoders. ICLR 2015 Workshop Track.
[3] Tikhonov and Yamshchikov. Music generation with variational recurrent autoencoder supported by history. arXiv.
[4] Hertel, Phan, and Mertins. Classifying Variable-Length Audio Files with All-Convolutional Networks and Masked Global Pooling. 
[5] Kalchbrenner, Grefenstette, and Blunsom. A Convolutional Neural Network for Modelling Sentences. ACL 2014.
[6] Razavian and Sontag. Temporal Convolutional Neural Networks for Diagnosis from Lab Tests. arXiv.","[6, 4, 8]","[' Marginally above acceptance threshold', ' Ok but not good enough - rejection', ' Top 50% of accepted papers, clear accept']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer provides a balanced perspective with both positive and negative points. They acknowledge the strengths of the paper, such as its simplicity and empirical results, but also highlight significant weaknesses in motivation, experimental design, and novelty. The reviewer's willingness to raise their score based on revisions suggests they are open to the research but need more convincing. Overall, the tone is critical but constructive, aiming to improve the paper. ",40.0,70.0
Building effective deep neural networks one feature at a time,"['Martin Mundt', 'Tobias Weis', 'Kishore Konda', 'Visvanathan Ramesh']",Reject,2018,"[3, 2, 7, 27]","[8, 6, 8, 32]","[32, 4, 17, 89]","[15, 2, 9, 59]","[13, 2, 7, 14]","[4, 0, 1, 16]","This paper aims to address the deep learning architecture search problem via incremental addition and removal of channels in intermediate layers of the network. Experiments are carried out on small-scale datasets such as MNIST and CIFAR, as well as an exploratory run on ImageNet (AlexNet).

Overall, I find the approach proposed in the paper interesting but a little bit thin in content. Essentially, one increases or decreases the number of features based on equation 2. It would be much valuable to see ablation studies to show the effectiveness of such criterion: for example, simple cases one can think of is to model (1) a data distribution of known rank, (2) simple MLP/CNN models to show the cross-layer relationships (e.g. sudden increase and decrease of the number of channels across layers will be penalized by c^l_{f^{l+1}, t}), etc.

The experimentation section uses small scale datasets and as a result, it is relatively unclear how the proposed approach will perform on real-world applications. One apparent shortcoming of such approach is that training takes much longer time, and the algorithm is not easily made parallel (the sgd steps limit the level of parallelization that can be carried out). As a result, I am not sure about the applicability of the proposed approach.","[5, 4, 8]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Top 50% of accepted papers, clear accept']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer finds the paper's approach ""interesting"" but suggests the content is ""a little bit thin."" They recommend additional studies and point out limitations such as increased training time and lack of parallelism. This suggests a slightly positive sentiment overall, as the reviewer sees potential but has reservations. The language used is professional and not rude, suggesting a neutral-to-polite tone.",20.0,50.0
Representing Entropy : A short proof of the equivalence between soft Q-learning and policy gradients,"['Pierre H. Richemond', 'Brendan Maginnis']",Reject,2018,"[2, 2]","[7, 3]","[18, 4]","[3, 0]","[15, 3]","[0, 1]","Summary
*******
The paper provides a collection of existing results in statistics.

Comments
********
Page 1: references to Q-learning and Policy-gradients look awkwardly recent, given that these have been around for several decades.

I dont get what is the novelty in this paper. There is no doubt that all the tools that are detailed here are extremely useful and powerful results in mathematical statistics. But they are all known.

The Gibbs variational principle is folklore, Proposition 1,2 are available in all good text books on the topic, 
and Proposition 4 is nothing but a transportation Lemma.
Now, Proposition 3 is about soft-Bellman operators. This perhaps is less standard because contraction property of soft-Bellman operator in infinite norm is more recent than for Bellman operators.
But as mentioned by the authors, this is not new either. 
Also I don't really see the point of providing the proofs of these results in the main material, and not for instance in appendix, as there is no novelty either in the proof techniques.

I don't get the sentence ""we have restricted so far the proof in the bandit setting"": bandits are not even mentioned earlier.

Decision
********
I am sorry but unless I missed something (that then should be clarified) this seems to be an empty paper: Strong reject.","[2, 5, 5]","[' Strong rejection', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[5, 5, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is extremely negative about the paper, using very strong language to criticize the paper's lack of novelty and contribution. Phrases like ""empty paper"", ""Strong reject"", ""I don't get what is the novelty"", and ""this seems to be an empty paper"" all point to a very negative sentiment. While the reviewer does not resort to personal attacks, the tone is quite blunt and dismissive, indicating low politeness. ",-90.0,-40.0
Thinking like a machine — generating visual rationales through latent space optimization,"['Jarrel Seah', 'Jennifer Tang', 'Andy Kitchen', 'Jonathan Seah']",Reject,2018,"[2, 3, 2, 1]","[6, 7, 2, 1]","[7, 15, 5, 1]","[3, 9, 2, 0]","[3, 5, 3, 1]","[1, 1, 0, 0]","* This paper models images with a latent code representation, and then tries to modify the latent code to minimize changes in image space, while changing the classification label. As the authors indicate, it lies in the space of algorithms looking to modify the image while changing the label (e.g. LIME etc).

* This is quite an interesting paper with a sensible goal. It seems like the method could be more informative than the other methods.  However, there are quite a number of problems, as explained below.

* The explanation of eqs 1 and 2 is quite poor. \alpha in (1) seems to be \gamma in Alg 1 (line 5). ""L_target is a target objective which can be a negative class probability .."" this assumes that the example is a positive class. Could we not also apply this to negative examples?

""or in the case of heart failure, predicted BNP level"" -- this doesn't make sense to me -- surely it would be necessary to target an adjusted BNP level? Also specific details should be reserved until a general explanation of the problem has been made.

* The trade-off parameter \gamma is a ""fiddle factor"" -- how was this set for the lung image and MNIST examples? Were these values different?

* In typical ICLR style the authors use a deep network to learn the encoder and decoder networks. It would be v interesting (and provide a good baseline) to use a shallow network (i.e. PCA) instead, and elucidate what advantages the deep network brings.

* The example of 4/9 misclassification seems very specific. Does this method also work on say 2s and 3s? Why have you not reported results for these kinds of tasks?

* Fig 2: better to show each original and reconstructed image close by (e.g. above below or side-by-side).

The reconstructions show poor detail relative to the originals.  This loss of detail could be a limitation.

* A serious problem with the method is that we are asked to evaluate it in terms of images like Fig 4 or Fig 8. A serious study would involve domain experts and ascertain if Fig 4 conforms with what they are looking for.

* The references section is highly inadequate -- no venues of publication are given. If these are arXiv give the proper ref. Others are published in conferences etc, e.g. Goodfellow et al is in Advances in Neural Information Processing Systems 27, 2014.

* Overall: the paper contains an interesting idea, but given the deficiencies raised above I judge that it falls below the ICLR threshold.

* Text:

sec 2 para 4. ""reconstruction loss on the validation set was similar to the reconstruction loss on the validation set."" ??

* p 3 bottom -- give size of dataset

* p 5 AUC curve -> ROC curve

* p 6 Fig 4 use text over each image to better specify the details given in the caption.



","[4, 8, 7]","[' Ok but not good enough - rejection', ' Top 50% of accepted papers, clear accept', ' Good paper, accept']","[3, 2, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review acknowledges the paper's interesting idea and potential but highlights significant weaknesses. The reviewer points out several issues with clarity, methodology, and evaluation, suggesting the paper falls below the ICLR threshold. The language is direct and critical, focusing on the paper's shortcomings, but maintains a professional tone without resorting to personal attacks or disrespectful language.",-20.0,60.0
Video Action Segmentation with Hybrid Temporal Networks,"['Li Ding', 'Chenliang Xu']",Reject,2018,"[2, 7]","[7, 12]","[22, 126]","[10, 58]","[9, 59]","[3, 9]","This paper discusses the problem of action segmentation in long videos, up to 10 minutes long. The basic idea is to use a temporal convolutional encoder-decoder architecture, where in the enconder 1-D temporal convolutions are used. In the decoder three variants are studied:

(1) One that uses only several bidirectional LSTMs, one after the other.
(2) One that first applies successive layers of deconvolutions to produce per frame feature maps. Then, in the end a bidirectional LSTM in the last layer.
(3) One that first applies a bidirectional LSTM, then applies successively 1-D deconvolution layer.

All variants end with a ""temporal softmax""  layer, which outputs a class prediction per frame.

Overall, the paper is of rather limited novelty, as it is very similar to the work of Lea et al., 2017, where now the decoder part also has the deconvolutions smoothened by (bidirectional) LSTMs. It is not clear what is the main novelty compared to the aforementioned paper, other than temporal smoothing of features at the decoder stage.

Although one of the proposed architectures (TricorNet) produces some modest improvements, it is not clear why the particular architectures are a good fit. Surely, deconvolutions and LSTMs can help incorporate some longer-term temporal elements into the final representations. However, to begin with, aren't the 1-D deconvolutions and the LSTMs (assuming they are computed dimension-wise) serving the same purpose and therefore overlapping? Why are both needed?

Second, what makes the particular architectures in Figure 3 the most reasonable choice for encoding long-term dependencies, is there a fundamental reason? What is the difference of the L_mid from the 1-D deconv layers afterward? Currently, the three variants are motivated in terms of what the Bi-LSTM can encode (high or low level details). 

Third, the qualitative analysis can be improved. For instance, the experiment with the ""cut lettuce"" vs ""peel cucumber"" is not persuasive enough. Indeed, longer temporal relationships can save incorrect future predictions. However, this works both ways, meaning that wrong past predictions can persist because of the long-term modelling. Is there a mechanism in the proposed approach to account for that fact?

All in all, I believe the paper indeed improves over existing baselines. However, the novelty is insufficient for a publication at this stage.","[3, 4, 3]","[' Clear rejection', ' Ok but not good enough - rejection', ' Clear rejection']","[5, 4, 5]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The reviewer acknowledges some merits of the paper, such as the modest improvements of the proposed architecture. However, they mainly point out its limitations: lack of significant novelty compared to existing work, unclear justifications for the architectural choices, and a need for a more robust qualitative analysis. The tone is critical, questioning the rationale behind the proposed methods, but remains professional and within the bounds of academic discourse.",-20.0,60.0
BinaryFlex: On-the-Fly Kernel Generation in Binary Convolutional Networks,"['Vincent W.-S. Tseng', 'Sourav Bhattachary', 'Javier Fernández Marqués', 'Milad Alizadeh', 'Catherine Tong', 'Nicholas Donald Lane']",Reject,2018,"[3, 29, 4, 7, 2, 16]","[6, 33, 9, 11, 6, 21]","[10, 117, 27, 14, 14, 247]","[8, 82, 14, 6, 4, 133]","[0, 9, 13, 6, 5, 80]","[2, 26, 0, 2, 5, 34]","The paper proposes a neural net architecture that uses a predefined orthogonal binary basis to construct the filter weights of the different convolutional layers. Since only the basis weights need to be stored this leads to an exponential reduction in memory. The authors propose to compute the filter weights on the fly in order to tradeoff memory for computation time. Experiments are performed on ImageNet, MNIST, CIFAR datasets with comparisons to BinaryConnect, Binary-weight-networks and studies showing the memory vs time vs accuracy tradeoff.

Positives
- The idea of using a predefined basis to estimate filter weights in a neural network is novel and leads to significant reduction in memory usage.

Negatives
- The proposed method seems significantly worse than other binary techniques on ImageNet, CIFAR and SVHN. On Imagenet in particular binary-weight-network is 21% better at only 2x the model size. Would a binary-weight-network of the same model size be better than the proposed approach? It would help to provide results using the proposed method with the same model size as binary-weight-networks on the different datasets.  
- The citation to binary-weight-networks is missing.
- The descriptions in section 3.3, 3.4 need to be more rigorous. For instance, how many basis weights are needed for a filter of size N. Does N need to be a power of 2 or are extra dimentions from the basis just ignored?
","[5, 3, 5]","[' Marginally below acceptance threshold', ' Clear rejection', ' Marginally below acceptance threshold']","[3, 4, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review acknowledges the novelty of the paper's proposed method and its potential to reduce memory usage, which is positive. However, it also points out significant drawbacks in terms of performance compared to other techniques. The reviewer suggests specific areas for improvement and additional experiments, indicating a desire to see the work strengthened rather than rejected outright. The tone is direct and critical but professional and provides concrete suggestions for improvement. Overall, this leans towards the neutral side of positive.",20.0,50.0
Associative Conversation Model: Generating Visual Information from Textual Information,"['Yoichi Ishibashi', 'Hisashi Miyamori']",Reject,2018,"[17, 21]","[22, 24]","[9, 36]","[6, 34]","[3, 0]","[0, 2]","**Strengths**
In general, the paper makes an important observation that even in textual dialog, it might often make sense to reason or “imagine” how visual instances look, and this can lead to better more grounded dialog. 

**Weakness**
In general, the paper has some major weaknesses in how the dataset has been constructed, details of the models provided and generally the novelty of the proposed model. While the model on its own is not very novel, the paper does make an interesting computational observation that it could help to reason about vision even in textual dialog, but the execution of the dataset curation is not satisfactory, making the computational contribution less interesting. 

More specific details below:
1. The paper does not write down an objective that they are optimizing for any of the three stages in the model, and it is unclear what is the objective especially for the video context prediction task -- the distribution over the space of images (or videos) for a given piece of text is likely multimodal and gaussian likelihoods might not be sufficient to model this properly. Not clear if the sequence to sequence models are used in teacher forcing model when training in Stage 1, or there is sampling going on. In general, the paper lacks rigor in writing down what it optimizes, and laying out details of the model clearly. 

2. The manner in which the dataset has been constructed is unsatisfying -- it assumes that two consecutive pieces of subtitles in news channels constitutes a dialog. This is very likely an incorrect and unsatisfying assumption which does not take into account narrative, context etc. Right now the dataset seems more like skip-thought vectors [A] which models the distribution over contextual sentences given a source sentence than any kind of dialog.

3. The setup and ultimately the motivation in context of the setup is fairly artificial -- the dataset does have images corresponding to each “dialog” so it is unclear why the associative model is needed in this case. Further, it would have been useful to see quantitative evaluation of the proposed approach or statistics of the dataset to establish context for the dataset being a valid benchmark, and providing a baseline / numerical checkpoint for future works to compare to. Without any of these things, the work seems fairly incomplete.

Clarity:
1. Figure 2 captions are pretty unclear and hard to understand what they are conveying.
2. For a large part the paper talks about how visual instances are not available for textual phrases and then proceeds to assume access to aligned text and visual data. It would be good to clarify from the start that the model does need paired videos and text, and state exactly how much aligned data is needed.
3. Already learned CNN (Page. 4, Sec. 2.2.1): Would be good to mention which CNN was used.
4. Page 4: “the textual and visual context vectors of the spider are generated, respectively”: Would be good to clarify that textual and visual context vectors for the spider are attended to, as opposed to saying they are generated.

References:

[A]: Kiros, Ryan, Yukun Zhu, Ruslan R. Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. “Skip-Thought Vectors.” In Advances in Neural Information Processing Systems 28, edited by C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, 3294–3302. Curran Associates, Inc.","[3, 4, 3]","[' Clear rejection', ' Ok but not good enough - rejection', ' Clear rejection']","[4, 5, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The reviewer acknowledges the paper's interesting observation about the potential benefits of visual reasoning in textual dialogue. However, they express significant concerns about the dataset construction, model details, and overall novelty. The lack of a clear objective, the questionable assumption behind the dataset, and the absence of quantitative evaluation contribute to a negative sentiment. While the reviewer provides constructive criticism and specific recommendations, the numerous and substantial weaknesses outweigh the acknowledged strength, leading to a negative sentiment. The language remains professional and polite throughout, focusing on the technical aspects and avoiding personal attacks.",-50.0,70.0
Continuous Convolutional Neural Networks for Image Classification,"['Vitor Guizilini', 'Fabio Ramos']",Reject,2018,"[11, 15]","[16, 20]","[87, 275]","[49, 162]","[29, 77]","[9, 36]","This paper formulates a variant of convolutional neural networks which models both activations and filters as continuous functions composed from kernel bases. A closed-form representation for convolution of such functions is used to compute in a manner than maintains continuous representations, without making discrete approximations as in standard CNNs.

The proposed continuous convolutional neural networks (CCNNs) project input data into a RKHS with a Gaussian kernel function evaluated at a set of inducing points; the parameters defining the inducing points are optimized via backprop. Filters in convolutional layers are represented in a similar manner, yielding a closed-form expression for convolution between input and filters. Experiments train CCNNs on several standard small-scale image classification datasets: MNIST, CIFAR-10, STL-10, and SVHN.

While the idea is interesting and might be a good alternative to standard CNNs, the paper falls short in terms of providing experimental validation that would demonstrate the latter point. It unfortunately only experiments with CCNN architectures with a small number (eg 3) layers. They do well on MNIST, but MNIST performance is hardly informative as many supervised techniques achieve near perfect results. The CIFAR-10, STL-10, and SVHN results are disappointing. CCNNs do not outperform the prior CNN results listed in Table 2,3,4. Moreover, these tables do not even cite more recent higher-performing CNNs. See results table in (*) for CIFAR-10 and SVHN results on recent ResNet and DenseNet CNN designs which far outperform the methods listed in this paper.

The problem appears to be that CCNNs are not tested in a regime competitive with the state-of-the-art CNNs on the datasets used. Why not? To be competitive, deeper CCNNs would likely need to be trained. I would like to see results for CCNNs with many layers (eg 16+ layers) rather than just 3 layers. Do such CCNNs achieve performance compatible with ResNet/DenseNet on CIFAR or SVHN? Given that CIFAR and SVHN are relatively small datasets, training and testing larger networks on them should not be computationally prohibitive.

In addition, for such experiments, a clear report of parameters and FLOPs for each network should be included in the results table. This would assist in understanding tradeoffs in the design space.

Additional questions:

What is the receptive field of the CCNNs vs those of the standard CNNs to which they are compared? If the CCNNs have effectively larger receptive field, does this create a cost in FLOPs compared to standard CNNs?

For CCNNs, why does the CCAE initialization appear to be essential to achieving high performance on CIFAR-10 and SVHN? Standard CNNs, trained on supervised image classification tasks do not appear to be dependent on initialization schemes that do unsupervised pre-training. Such dependence for CCNNs appears to be a weakness in comparison.","[4, 6, 5]","[' Ok but not good enough - rejection', ' Marginally above acceptance threshold', ' Marginally below acceptance threshold']","[4, 2, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is fairly confident that the evaluation is correct']","The review starts with a neutral summary of the paper's contributions. However, it expresses disappointment regarding the experimental validation, stating that the results are ""disappointing"" and highlighting the lack of comparison with state-of-the-art methods. The reviewer also raises several concerns and asks for additional experiments. The language used is direct and critical but remains professional and not personally offensive.",-30.0,60.0
Grouping-By-ID: Guarding Against Adversarial Domain Shifts,"['Christina Heinze-Deml', 'Nicolai Meinshausen']",Reject,2018,"[4, 13]","[7, 17]","[10, 26]","[4, 6]","[5, 11]","[1, 9]","The paper discusses ways to guard against adversarial domain shifts with so-called counterfactual regularization. The main idea is that in several datasets there are many instances of images for the same object/person, and that taking this into account by learning a classifier that is invariant to the superficial changes (or “style” features, e.g. hair color, lighting, rotation etc.) can improve the robustness and prediction accuracy. The authors show the benefit of this approach, as opposed to the naive way of just using all images without any grouping, in several toy experimental settings.

Although I really wanted to like the paper, I have several concerns. First and most importantly, the paper is not citing several important related work. Especially, I have the impression that the paper is focusing on a very similar setting (causally) to the one considered in  [Gong et al. 2016] (http://proceedings.mlr.press/v48/gong16.html), as can be seen from Fig. 1. Although not focusing on classification directly, this paper also tries to a function T(X) such that P(Y|T(X)) is invariant to domain change. Moreover, in that paper, the authors assume that even the distribution of the class can be changed in the different domains (or interventions in this paper).
Besides, there are also other less related papers, e.g. http://proceedings.mlr.press/v28/zhang13d.pdf, https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/10052/0, https://arxiv.org/abs/1707.09724, (or potentially https://arxiv.org/abs/1507.05333 and https://arxiv.org/abs/1707.06422), that I think may be mentioned for a more complete picture. Since there is some related work, it may be also worth to compare with it, or use the same datasets.

I’m also not very happy with the term “counterfactual”. As the authors mention in footnote, this is not the correct use of the term, since counterfactual means “against the fact”. For example, a counterfactual query is “we gave the patient a drug and the patient died, what would have happened if we didn’t give the drug?” In this case, these are just different interventions on possibly the same object. I’m not sure that in the practical applications one can assure that the noise variables stay the same, which, as the authors correctly mention, would make it a bit closer to counterfactuals. It may sound pedantic, but I don’t understand why use the wrong and confusing terminology for no specific reason, also because in practice the paper reduces to the simple idea of finding a classifier that doesn’t vary too much in the different images of the single object.

**EDIT**: I was satisfied with the clarifications from the authors and I appreciated the changes that they did with respect to the related work and terminology, so I changed my evaluation from a 5 (marginally below threshold) to a 7 (good paper, accept).","[7, 4, 5]","[' Good paper, accept', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[3, 5, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer expresses initial interest (""really wanted to like the paper""), but raises several concerns, indicating a negative sentiment. However, the final note mentions satisfaction with the author's revisions, suggesting a shift towards positive. The language used, while critical, is professional and not personally attacking, indicating politeness. The reviewer provides constructive feedback and engages with the authors' work, suggesting a collaborative approach. Therefore, the sentiment leans slightly negative due to the initial concerns, and the politeness is notably positive.",-10.0,70.0
MACHINE VS MACHINE: MINIMAX-OPTIMAL DEFENSE AGAINST ADVERSARIAL EXAMPLES,['Jihun Hamm'],Reject,2018,[15],[19],[50],[29],[15],[6],"This paper presents a sensitivity-penalized loss (the loss of the classifier has an additional term in squart of the gradient of the classifier w.r.t. perturbations of the inputs), and a minimax (or maximin) driven algorithm to find attacks and defenses. It has a lemma which claims that the ""minimax and the maximin solutions provide the best worst-case defense and attack models, respectively"", without proof, although that statement is supported experimentally.

+ Prior work seem adequately cited and compared to, but I am not really knowledgeable in the adversarial attacks subdomain.
- The experiments are on small/limited datasets (MNIST and CIFAR-10). Because of this, confidence intervals (over different initializations, for instance) would be a nice addition to Table 5.
- There is no exact (""alternating optimization"" could be considered one) evaluation of the impact of the sensitivy loss vs. the minimax/maximin algorithm.
- The paper is hard to follow at times (and probably that dealing with the point above would help in this regard), e.g. Lemma 1 and experimental analysis.
- It is unclear (from Figures 3 and 7) that ""alternative optimization"" and ""minimax"" converged fully, and/or that the sets of hyperparameters were optimal.
+ This paper presents a game formulation of learning-based attacks and defense in the context of adversarial examples for neural networks, and empirical findings support its claims.


Nitpicks:
the gradient descent -> gradient descent or the gradient descent algorithm
seeming -> seemingly
arbitrary flexible -> arbitrarily flexible
can name ""gradient descent that maximizes"": gradient ascent.
The mini- max or the maximin solution is defined -> are defined
is the follow -> is the follower
","[6, 5, 5]","[' Marginally above acceptance threshold', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[3, 3, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review acknowledges the paper's contributions (new loss function, minimax algorithm) and supports its claims based on empirical findings. However, it also points out several limitations, such as limited datasets, lack of confidence intervals, and clarity issues. The overall tone is constructive and suggestive of improvements rather than dismissive. Therefore, the sentiment is moderately positive, and the language is polite.",60.0,70.0
A comparison of second-order methods for deep convolutional neural networks,"['Patrick H. Chen', 'Cho-jui Hsieh']",Reject,2018,"[1, 11]","[6, 16]","[16, 355]","[8, 171]","[7, 158]","[1, 26]","The paper conducts an empirical study on 2nd-order algorithms for deep learning, in particular on CNNs to answer the question whether 2nd-order methods are useful for deep learning.  More modestly and realistically, the authors compared stochastic Newton method (SHG) and stochastic Quasi- Newton method (SR1, SQN) with stochastic gradient method (SGD).  The activation function ReLu is known to be singular at 0, which may lead to poor curvature information, but the authors gave a good numerical comparison between the performances of 2nd-order methods with ReLu and the smooth function, Tanh.  The paper presented a reasonably good overview of existing 2nd-order methods, with clear numerical examples and reasonably well written.

The paper presents several interesting empirical findings, which will no doubt lead to follow up work. However, there are also a few critical issues that may undermine their claims, and that need to be addressed before we can really answer the original question of whether 2nd-order methods are useful for deep learning. 

1. There is no complexity comparison, e.g. what is the complexity for a single step of different method.

2. Relatedly, the paper reports the performance over epochs, but it is not clear what ""per epoch"" means for 2nd-order methods.  In particular, it seems to me that they did not count the inner CG iterations, and it is known that this is crucial in running time and important for quality.  If so, then the comparison between 1st-order and 2nd-order methods are not fair or incomplete.

3. The results on 2nd-order methods behave similarly to 1st-order methods, which makes me wonder how many CG iterations they used for 2nd-order method in their experiment, and also the details of the data.  In particular, are they looking at parameter/hyperparameter settings for which 2nd-order methods aren't really necessary.

4. In deep learning setting, the training objective is non-convex, which means the Hessian can be non-PSD.  It is not clear how the stochastic inexact-Newton method mentioned in Section 2.1 could work.  Details on implementations of 2nd-order methods are important here.

5. For 2nd-order methods, the author used line search to tune the step size.  It is not clear in the line search, the author used the whole training objective or batch loss.  Assuming using the batch loss, I suspect the training curve will be very noisy (depending on how large the batch size is).  But the paper only show the average training curves, which might be misleading.

Here are other points.

1. There is no figure showing training/ test accuracy.  Aside from being interested in test error, it is also of interest to see how 2nd order methods are similar/different than 1st order methods on training versus test.

2. Since it is a comparison paper, it only compares three 2nd-order methods with SGD.  The choices made were reasonable, but 2nd-order methods are not as trivial to implement as SGD, and it isn't clear whether they have really ""spanned the space"" of second order methods

3. In the paper, the settings of LeNet, AlexNet are different with those in the original paper.  The authors did not give a reason.

4. The quality of figures is not good.

5. The setting of optimization is not clear, e.g. the learning rate of SGD, the parameter of backtrack line search.  It's hard to reproduce results when these are not described.

","[6, 5, 3]","[' Marginally above acceptance threshold', ' Marginally below acceptance threshold', ' Clear rejection']","[3, 5, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with a generally positive sentiment, acknowledging the paper's contributions and clear writing. However, it then raises several ""critical issues"" that question the validity of the paper's claims. The reviewer lists several specific concerns, indicating a shift towards a more critical perspective. While the language remains professional and provides constructive feedback, the presence of these major concerns suggests the overall sentiment leans towards the negative side. ",-20.0,70.0
A closer look at the word analogy problem,['Siddharth Krishna Kumar'],Reject,2018,[2],[3],[3],[1],[2],[0],"This paper proposes a method to solve the 'word analogy problem', which was proposed as a way of understanding and evaluating word embeddings by Mikolov et al. There are some nice analyses in the paper which, if better organised, could lead to an improved understanding of semantic word spaces in neural nets. 

comments: 

The word analogy task was developed as an interesting way to analyse and understand word embedding spaces, but motivation for learning word embeddings was as general-purpose representations for language processing tasks (as in collobert et al, 2011), not as a way of resolving analogy questions. The authors develop a specialist method for resolving analogies, and it works (mostly) better than using the additive geometry of word-embedding spaces. But I don't think that comparison is 'fair' - the analogy thing is just a side-effect of word embedding spaces. 

Given that the authors focus on the word-analogy problem as an end in itself, I think there should be much more justification of why this is a useful problem to solve. Analogy seems to be fundamental to human cognition and reasoning, so maybe that is part of the reason, but it's not clear to the reader. 

The algorithm seems to be simple and intuitive, but the presentation is overly formal and unclear. It would be much easier for the reader to simply put into plain terms what the algorithm does.

Using a POS-tagger to strip out nouns is a form of supervision (the pos-tagger was trained on labelled data) that word-embedding methods do not use, which should at least be acknowledged when making a comparison. Similarly, it is nice that the present method works on less data, but the beauty of word embeddings is that they can be trained on any text - i.e. data is not a problem, and 'work' for any word type. Stripping away everything but nouns clearly allows co-occurrence semantic patterns to emerge from less data, but at the cost of the supervision mentioned above. Moreover, I suspect that the use of wikipedia is important for the proposed algorithm, as the pertinent relations are often explicit in the first sentence of articles ""Paris the largest city and capital of France..."". Would the same method work on any text? I would expect this question to be explored, even if the answer is negative. 

The goal of understanding word2vec and embedding spaces in general (section 5) is a really important one (as it can tell us a lot about how language and meaning is encoded in deep learning models in general), and I think that's one of the strongest aspects of this work. However, the conclusions from this section (and other related conclusions in other sections) are a little unclear to me. Perhaps that is because I don't quite get algorithm 3, which would be mitigated by an intuitive explanation to complement the pseudocode. I'm also confused by the assertion that Vec(A) - Vec(B) conveys the 'common information' in A and B. How can a non-symmetric operation convey 'common information'. Surely it conveys something about the relationship between A and B?

Minor point:
""may not the be indicative of the model's ability to learn the relationship between a word pair the way a human does"" (Abstract)
- I'm not sure we know how humans learn the relationships between word pairs. Are you referring to formal semantic relations i.e. in taxonomies in WordNet? This sentence seems dangerous, and the claim about humans is not really treated in the article itself. 

The a+cknowledgements compromise the anonymity of the authors. ","[3, 3, 2]","[' Clear rejection', ' Clear rejection', ' Strong rejection']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The reviewer acknowledges some positive aspects of the paper, such as the interesting analyses and the importance of understanding word embeddings. However, the review is mostly critical, questioning the paper's core premise and finding the presentation unclear. The reviewer also points out potential flaws in the methodology. The language used is quite direct and critical, but it doesn't cross the line into rudeness. It maintains a professional tone throughout.",-30.0,60.0
A Simple Fully Connected Network for Composing Word Embeddings from Characters,"['Michael Traynor', 'Thomas Trappenberg']",Reject,2018,"[1, 25]","[3, 29]","[3, 69]","[2, 41]","[0, 6]","[1, 22]","This paper presents a new model for composing representations of characters into word embeddings. The starting point of their argument is to include position-specific embeddings of characters rather than just position-independent characters. By adding together position-specific vectors, reasonable results are obtained.

This is an interesting result, but I have a few recommendations to improve the paper.
1) It is a bit hard to assess since it is not evaluated on a standard datasets. There are a number standard datasets for open vocabulary language modeling. E.g., the MWC corpus (http://k-kawakami.com/research/mwc), or even the Penn Treebank (although it is conventionally modeled in closed vocabulary form).
2) There are many existing models for composing characters into words. In addition to those cited in the paper, see the citations listed below. Comparison with those is crucial in a paper like this.
3) Since the predictions are done at the word type level, it is unclear how vocabulary set of the corpus is determined, and what is done with OOV word types at test time (while it is possible to condition on them using the technique in the paper, it is not possible to use this technique for generation).
4) The analysis is interesting, but a more intuitive explanation would be to show nearest neighbor plots.

Some missing citations:

Composing characters into words:

dos Santos and Zadrozny. (2014 ICML) http://proceedings.mlr.press/v32/santos14.pdf
Ling et al. (2015 EMNLP) Finding Function in Form. https://arxiv.org/abs/1508.02096

Additionally, using explicit positional features in modeling language has been used:
Vaswani et al. (2017) Attention is all you need https://arxiv.org/abs/1706.03762
and a variety of other sources.","[4, 3, 5]","[' Ok but not good enough - rejection', ' Clear rejection', ' Marginally below acceptance threshold']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer provides constructive criticism, acknowledges the interesting aspects of the work ('This is an interesting result'), and offers concrete suggestions for improvement. This suggests a positive attitude towards the paper, aiming to guide the authors towards a stronger publication. The language is formal and polite throughout, without resorting to harsh or negative phrasing.",60.0,80.0
Don't encrypt the data; just approximate the model \ Towards Secure Transaction and Fair Pricing of Training Data,['Xinlei Xu'],Reject,2018,['skipped'],['skipped'],['skipped'],['skipped'],['skipped'],['skipped'],"Summary

The paper addresses the issues of fair pricing and secure transactions between model and data providers in the context of machine learning real-world application.

Major

The paper addresses an important issue regarding the real-world application of machine learning, that is, the transactions between data and model provider and the associated aspects of fairness, pricing, privacy, and security.

The originality and significance of the work reported in this paper are difficult to comprehend. This is largely due to the lack of clarity, in general, and the lack of distinction between what is known and what is proposed. I failed to find any clear description of the proposed approach and any evaluation of the main idea.

Most of the discussions in the paper are difficult to follow due to that many of the statements are vague or unclear. There are some examples of this vagueness illustrated under “minor issues”. Together, the many minor issues contribute to a major communication issue, which significantly reduces readability of the paper. A majority of the references included in the reference section lack some or all of the required meta data.

In my view, the paper is out of scope for ICLR. Neither the CFP overview nor the (non-exhaustive) list of relevant topics suggest otherwise. In very general terms, the paper could of course be characterised as dealing with machine learning implementation/platform/application but the issues discussed are more connected to privacy, security, fair transactions, and pricing.

In summary; although there is no universal rule on how to structure research papers, a more traditional structure (introduction, aim & scope, background, related work, method, results, analysis, conclusions & future work) would most certainly have benefitted the paper through improved clarity and readability. Although some interesting works on adversarial learning, federated learning, and privace-preserving training are cited in the paper, the review and use of these references did not contribute to a better understanding of the topic or the significance of the contribution in this paper. I was unable to find any support in the paper for the strong general result stated in the abstract (“We successfully show that without running the data through the model, one can approximate the value of the data”).

Minor issues (examples)

- “Models trained only a small scale of data” (missing word)
- “to prevent useful data from not being paid” (unclear meaning)
- “while the company may decline reciprocating gifts such as academic collaboration, while using the data for some other service in the future” (unclear meaning)
- “since any data given up is given up ” (unclear meaning)
- “a user of a centralized service who has given up their data will have trouble telling if their data exchange was fair at all (even if their evaluation was purely psychological)” (unclear meaning)
- “For a generally deployed model, it can take any form. Designing a transaction strategy for each one can be time-consuming and difficult to reason about” (unclear meaning)
- “(et al., 2017)” (unknown reference)
- “Osbert Bastani, Carolyn Kim, and Hamsa Bastani. Interpreting blackbox models via model extraction, 2017” (incomplete reference data)
- “Song Han, Huizi Mao, and William J. Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding, 2015.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network, 2015.
Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions, 2017.” (Incomplete reference data)
- “H. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Agera y Arcas. Communication-efficient learning of deep networks from decentralized data. 2016.” (Incomplete reference data)
- “et al. Richard Craid.” (Incorrect author reference style)
- “Ryo Yonetani, Vishnu Naresh Boddeti, Kris M. Kitani, and Yoichi Sato. Privacy-preserving visual learning using doubly permuted homomorphic encryption, 2017.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization, 2016.” (Incomplete reference data)","[2, 3, 4]","[' Strong rejection', ' Clear rejection', ' Ok but not good enough - rejection']","[4, 5, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review is highly critical of the paper, pointing out a lack of clarity, originality, and scope. The reviewer finds the paper difficult to understand and believes it is not relevant to the conference. The language used, while direct, is professional and within the bounds of academic criticism. There are no personal attacks or disrespectful remarks. The reviewer focuses on the paper's shortcomings and provides constructive feedback for improvement.",-75.0,50.0
Network Iterative Learning for Dynamic Deep Neural Networks via Morphism,"['Tao Wei', 'Changhu Wang', 'Chang Wen Chen']",Reject,2018,"[23, 15, 29]","[28, 19, 34]","[127, 156, 489]","[72, 97, 251]","[15, 47, 42]","[40, 12, 196]","This paper proposes an iterative approach to train deep neural networks based on morphism of the network structure into more complex ones. The ideas are rather simple, but could be potentially important for improving the performance of the networks. On the other hand, it seems that an important part of the work has already been done before (in particular Wei et al. 2016), and that the differences from there are very ad-hoc and intuition for why they work is not present. Instead, the paper justifies its approach by arguing that the experimental results are good. Personally, I am skeptical with that, because interesting ideas with great added value usually have some cool intuition behind them. The paper is easy to read, and there does not seem to exist major errors. Because I am not an active researcher in the topic, I cannot judge if the benefits that are shown in the experiments are enough for publication (the theoretical part is not the strongest of the paper).","[5, 7, 5]","[' Marginally below acceptance threshold', ' Good paper, accept', ' Marginally below acceptance threshold']","[2, 4, 3]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer acknowledges the potential importance of the paper's ideas but expresses skepticism due to lack of strong theoretical justification and novelty. The reviewer finds the experimental justification insufficient, indicating a somewhat negative sentiment. However, the language remains professional and courteous throughout.",-20.0,60.0
Evolutionary Expectation Maximization for Generative Models with Binary Latents,"['Enrico Guiraud', 'Jakob Drefs', 'Joerg Luecke']",Reject,2018,"[1, 1, 35]","[6, 5, 39]","[9, 10, 66]","[4, 3, 31]","[2, 4, 11]","[3, 3, 24]","The paper presents a combination of evolutionary computation (EC) and variational EM for models with binary latent variables represented via a particle-based approximation.

The scope of the paper is quite narrow as the proposed method is only applicable to very specialised models. Furthermore, the authors do not seem to present any realistic modelling problems where the proposed approach would clearly advance the state of the art. There are no empirical comparisons with state of the art, only between different variants of the proposed method.

Because of these limitations, I do not think the paper can be considered for acceptance.

Detailed comments:

1. When revising the paper for next submission, please make the title more specific. Papers with very broad titles that only solve a very small part of the problem are very annoying.

2. Your use of crossover operators seems quite unimaginative. Genomes have a linear order but in the case of 2D images you use it is not obvious how that should be mapped to 1D. Combining crossovers in different representations or 2D crossovers might fit your problem much better.

3. Please present a real learning problem where your approach advances state of the art.

4. For the results in Fig. 7, please run the algorithm until convergence or justify why that is not necessary.

5. Please clarify the notation: what is the difference between y^n and y^(n)?
","[4, 4, 4]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is mainly negative. The reviewer finds the paper's scope too narrow, lacking in realistic applications and comparisons to existing methods. While the reviewer provides constructive feedback, the overall tone, particularly in the first paragraph and point 1, leans towards negativity. The language used is direct and somewhat critical, but not overly rude.",-60.0,-20.0
Learning objects from pixels,['David Saxton'],Reject,2018,[35],[36],[20],[5],[7],[8],"This paper learns to construct masks and feature representations from an input image, in order to represent objects. This is applied to the relatively simple domain of Atari games video input (compared to natural images). The paper is completely inadequate in respect to related work; it re-invents known techniques like non-maximum suppression and matching for tracking; fails to learn convincing objects according to visual inspection; and fails to compare with earlier methods for these tasks. (The comment above about re-invention is the most charitable intepretation -- the worst case would be using these ideas without citation.)


1) The related work section is outrageous, containing no references before 2016.  Do the authors think researchers never tried to do this task before then? This is the bad side of the recent deep nets hype, and ICLR is particularly susceptible to this. Examples include

@article{wang-adelson-94,
  author        = ""Wang,  J. Y. A. and Adelson, E. H."",
  title         = {{Representing Moving Images with Layers}},
  journal       = {{IEEE Transactions on Image Processing}},
  year          = ""1994"",
  volume        = ""3(5)"",
  pages         = {625-638}
}
see http://persci.mit.edu/pub_pdfs/wang_tr279.pdf

and

@article{frey-jojic-03,
   author    = {Frey, B. J. and Jojic, N.},
   title     = {{Transformation Invariant Clustering Using the EM Algorithm}},
   journal   = {IEEE Trans Pattern Analysis and Machine Intelligence},
   year      = {2003},
   volume    = {25(1)},
   pages     = {1-17}
}
where mask and appearances for each object of interest are learned. There is a literature which follows on from the F&J paper.  The methods used in Frey & Jojic are different from what is proposed in the paper, but there needs to be comparisons.

The AIR paper also contains references to relevant previous work.

2) p 3 center -- this seems to be reinventing non-maximum suppression

3) p 4 eq 3 and sec 3.2 -- please justify *why* it makes sense to use
the concrete transform.  Can you explain better (e.g. in the supp mat)
the effect of this for different values of q_i?

4) Sec 3.5 Matching objects in successive frames using the Hungarian 
algorithm is also well known, e.g. it is in the matlab function
assignDetectionsToTracks .

5) Overall: in this paper the authors come up with a method for learning objects from Atari games video input. This is a greatly restricted setting compared to real images. The objects learned as shown in Appendix A are quite unconvincing, e.g. on p 9. For example for Boxing why are the black and white objects broken up into 3 pieces, and why do they appear coloured in col 4?

Also the paper lacks comparisons to other methods (including ones from before 2016) which have tackled this problem.

It may be that the methods in this paper can outperform previous ones -- that would be interesting, but it would need a lot of work to address the issues raised above.

Text corrections:

p 2 ""we are more precise"" -> ""we give more details""

p 3 and p 2 -- local maximum (not maxima) for a single maximum.  [occurs many times]
","[3, 4, 4]","[' Clear rejection', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review is extremely negative about the paper. The reviewer states that the paper ""re-invents known techniques"" and ""fails to compare with earlier methods."" They call the related work section ""outrageous"" for not citing anything before 2016. The reviewer also finds the results unconvincing. While the language is harsh, it doesn't contain personal attacks or insults, focusing instead on the scientific shortcomings of the work.",-80.0,-20.0
“Style” Transfer for Musical Audio Using Multiple Time-Frequency Representations,"['Shaun Barry', 'Youngmoo Kim']",Reject,2018,['no_match'],['no_match'],['no match'],['no match'],['no match'],['no match'],"This paper describes improvements to a system described in a blog post for musical style transfer.  Such a system is difficult to evaluate, but examples are presented where the style of one song is applied to the content of another.  These audio examples show that the system produces somewhat reasonable mixtures of two songs, but suggest that if the system instead followed the (mostly) accepted rules for cover song generation, it could make the output much more pleasant to listen to.  Additional evaluation includes measuring correlations between style songs and the output to ensure that it is not being used directly as well as some sort of measure of key invariance that is difficult to interpret.  The paper does not completely define the mathematical formulation of the system, making it difficult to understand what is really going on.

The current paper changes the timbre, rhythm, and harmony of the target content song.  Changing the harmony is problematic as it can end up clashing with the generated melody or just change the listener's perception of which song it is.  I suggest instead attempting to generate a cover version of the content song in the style of the style song. Cover songs are re-performances of an existing (popular) song by another artist.  For example, Jimi Hendrix covered Bob Dylan's ""All along the watchtower"" and the Hendrix version became more popular than the original.  This is essentially artist A performing a song by artist B, which is very similar to the goal of the current paper.  Cover songs almost always maintain the lyrics, melody, and harmony of the original, while changing the timbre, vocal style, tempo, and rhythmic information.  This seems like a good way to structure the problem of musical style transfer.  Many systems exist for identifying cover songs, see the relevant publications at the International Society for Music Information Retrieval (ISMIR) Conference.  Few systems do something with cover songs after they have been identified, but they could be used for training a system like the one proposed here, if it could be trained. 

Another musically questionable operation is pooling across frequency in the constant-Q transform representation.  In western music, adjacent notes are very different from one another and are usually not played in the same key, for example C and C#.  Thus, pooling them together to say that one of them is present seems to lose useful information.  As part of the pooling discussion, the paper includes an investigation of the key-invariance of the model.  Results from this are shown in figure 5, but it is difficult to interpret this figure.  What units is the mean squared error measured in?  What would be a big value?  What would be a small value?  What aspects of figure 5 specifically ""confirm that changing key between style [and] content has less of an effect on our proposed key-invariant content representations""?

Section 3.1, which describes the specifics of the model, is confusing.  What exactly are S, C, W, and G?  What are their two dimensions indexed by i and j?  How do you compute them from the input?  Which parameters in this model are learned and which are just calculated?  Is there any training or is L(X,C,S) just optimized at test time?

Finally, the evaluation of the texture generation part of the algorithm could be compared to existing texture generation algorithms (there are several) such as McDermott & Simoncelli (2011, NEURON), which even has code available online.



Minor comments
--------------

p2: ""in the this work"" typo

p2: ""an highly"" typo

p2: ""The first method... The latter class of methods"" confusing wording.  Is the second one a different method or referring back to the previous method?  If it's different, say ""The second method...""

p7: Please describe kernel sizes in real units (e.g., ms, Hz, cents) as well as numbers of bins



After revision/response
--------------------------------
The revisions of the paper have made it clearer as to what is going on, although the description of the algorithm itself could still be described more mathematically to really make it clear.  It is more clear what's going on in figure 5, although it could also be further clarified whether the green bars are showing the distance between log magnitude STFTs of the transposed ""style"" snippets and the untransposed ""content"" snippets directly and so provide an upper bound on the distances. My overall rating of the paper has not changed.","[6, 4, 7]","[' Marginally above acceptance threshold', ' Ok but not good enough - rejection', ' Good paper, accept']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer acknowledges the work and finds the audio examples ""somewhat reasonable,"" suggesting a slightly positive sentiment. However, they also express significant concerns about the methodology and evaluation, pulling the sentiment towards neutral. The language is quite direct with strong suggestions but maintains a professional and respectful tone overall.",20.0,60.0
Open Loop Hyperparameter Optimization and Determinantal Point Processes,"['Jesse Dodge', 'Kevin Jamieson', 'Noah A. Smith']",Reject,2018,"[7, 'no_match', 19]","[12, 'no_match', 24]","[44, 'no match', 441]","[22, 'no match', 250]","[20, 'no match', 160]","[2, 'no match', 31]","
This paper considers hyperparameter searches in which all of the
candidate points are selected in advance.  The most common approaches
are uniform random search and grid search, but more recently
low-discrepancy sequences have sometimes been used to try to achieve
better coverage of the space.  This paper proposes using a variant of
the determinantal point process, the k-DPP to select these points.
The idea is that the DPP provides an alternative form of diversity to
low-discrepancy sequences.

Some issues I have with this paper:

1. Why a DPP? It's pretty heavyweight. Why not use any of the other
(potentially cheaper) repulsive point processes that also achieve
diversity?  Is there anything special about it that justifies this
work?

2. What about all of the literature on space-filling designs, e.g.,
latin hypercube designs?  Statisticians have thought about this for a
long time.

3. The motivation for not using low-discrepancy sequences was discrete
hyperparameters.  In practice, people just chop up the space or round.
Is a simple kernel with one length scale on a one-hot coding adding
value? In this setup, each parameter can only contribute ""same or
different"" to the diversity assessment.  In any case, the evaluations
didn't have any discrete parameters.  Given that the discrete setting
was the motivation for the DPP over LDS, it seems strange not to even
look at that case.

4. How do you propose handling ordinal variables? They're a common
case of discrete variables but it wouldn't be sensible to use a
one-hot coding.

5. Why no low discrepancy sequence in the experimental evaluation of
section 5?  Since there's no discrete parameters, I don't see what the
limitation is.

6. Why not evaluate any other low discrepancy sequences than Sobol?

7. I didn't understand the novelty of the MCMC method relative to
vanilla M-H updates.  It seems out of place.

8. The figures really need error bars --- Figure 3 in particular.  Are
these differences statistically significant?
","[4, 4, 4]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[5, 5, 5]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The reviewer raises several valid concerns about the paper's novelty, methodology, and lack of comparisons. They question the choice of DPP and its justification compared to simpler methods. The reviewer also points out missing aspects like handling ordinal variables, comparisons with other low-discrepancy sequences, and the lack of error bars in the results. The tone, while direct and critical, maintains a professional and constructive approach by asking for clarifications and suggesting improvements. There's no use of harsh language or personal attacks.",-20.0,60.0
LSH Softmax: Sub-Linear Learning and Inference of the Softmax Layer in Deep Architectures,"['Daniel Levy', 'Danlu Chan', 'Stefano Ermon']",Reject,2018,"[13, 'no_match', 10]","[16, 'no_match', 15]","[21, 'no match', 406]","[10, 'no match', 199]","[10, 'no match', 200]","[1, 'no match', 7]","The paper proposes to use LSH to approximate softmax, which greatly speeds up classification with large output space. The paper is overall well-written. However, similar ideas have been proposed before, such as ""Deep networks with large output spaces"" by Vijayanarasimhan et. al. (ICLR 2015). And this manuscript does not provide any comparison to any of those similar methods.

A few questions about the implementation,
(1) As stated in the manuscript, the proposed method contains three steps, hashing, lookup and distance. GPU is not good at lookup, so the manuscript proposes to do lookup on CPU. Does that mean the data should go back and forth between CPU and GPU? Would this significantly increase the overhead?
(2) At page 6, the LSH structure returns m list of C candidates. Is it a typo? C is the total number of classes. And how do you guarantee that each LSH query returns the same amount of candidates?

Experiment-wise, the manuscript leaves something to be desired.
(1) More baselines be evaluated and compared. In this manuscript, only IS and NS are compared. And pure negative sampling is actually rarely used in language modeling. In addition to Vijayanarasimhan's LSH method, there are also a few other methods out there, such as hierarchical softmax, NCE, D-sothat ftmax (""Strategies for Training Large Vocabulary Neural Language Models"" by Chen et. al. ACL 2016), adaptive softmax (""Efficient softmax approximation for GPUs"" by Grave et. al).
(2) The results of the proposed method is not impressive. D-softmax and adaptive softmax can achieve 147 ppl on text 8 with 512 hidden units as described in other paper, while the proposed method can only achieve 224 ppl with 650 hidden units. Even the exact softmax have large difference in ppl. It looks like the authors do not tune the hyper-parameters well. With this suboptimal setting, it is hard to judge the significance of this manuscript.
(3) Why one billion word dataset is used in eval but not used for training? It is one of the best datasets to test the scalability of language models.
(4) We can see, as reported in the manuscript, that NS has bigger speedup than the proposed method. So it would be nice to show ppl vs time curve for all methods. Eventually, what we want is the best model given a fixed amount of training time. With the same amount of epochs, NS loses the advantage of being faster.","[5, 5, 5]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with a positive sentiment, acknowledging the paper's contribution and clarity. However, it quickly shifts to a more critical tone, pointing out the lack of novelty and comparison with existing methods. The reviewer then raises several specific concerns about the implementation, experimental setup, and results, suggesting that the paper's claims are not fully substantiated. The language used is professional and objective, but the overall tone conveys a sense of skepticism and suggests significant revisions are needed.",20.0,60.0
TESLA: Task-wise Early Stopping and Loss Aggregation for Dynamic Neural Network Inference,"['Chun-Min Chang', 'Chia-Ching Lin', 'Hung-Yi Ou Yang', 'Chin-Laung Lei', 'Kuan-Ta Chen']",Reject,2018,"[11, 12, 'no_match', 34, 14]","[16, 17, 'no_match', 39, 19]","[25, 33, 'no match', 146, 153]","[18, 15, 'no match', 86, 103]","[0, 3, 'no match', 1, 3]","[7, 15, 'no match', 59, 47]","The authors propose a method for reducing the computational burden when performing inference in deep neural networks. The method is based a previously-developed approach called incomplete dot products, which works by pruning some of the inputs in the dot products via the introduction of pre-specified coefficients. The authors of this paper extend the method by introducing a task-wise learning procedure that sequentially optimizes a loss function for decreasing percentage of included features in the dot product. 

Unfortunately, this paper was hard to follow for someone who does not actively work in this field, making it hard to judge if the contribution is significant or not. While the description of the problem itself is adequate, when it comes to describing the TESLA procedure and the alternative training procedure, the relevant passages are, in my opinion, too vague to allow other researchers to implement this procedure.

Positive points:
- The application seems relevant, and the task-wise procedure seems like an improvement over the original IDP proposal.
- Application to two well-known benchmarking datasets.

Negative points:
- The method is not described in sufficient detail to allow reproducibility, the algorithms are no more than sketches.
- It is not clear to me what the advantage of this approach is, as opposed to alternative ways of compressing the network (e.g. via group lasso regularization), or training an emulator on the full model for each task.

Minor point:
- Figure 1 is unclear and requires a better caption. ","[4, 5, 4]","[' Ok but not good enough - rejection', ' Marginally below acceptance threshold', ' Ok but not good enough - rejection']","[2, 2, 4]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer provides both positive and negative feedback, ultimately stating uncertainty about the significance of the contribution due to a lack of clarity in the methodology. While acknowledging the potential relevance and improvement over previous work, the reviewer criticizes the paper for vagueness that hinders reproducibility. The language used is critical but professional, avoiding personal attacks or disrespectful tones.",-10.0,50.0
Transfer Learning on Manifolds via Learned Transport Operators,"['Marissa Connor', 'Christopher Rozell']",Reject,2018,"[0, 20]","[5, 25]","[11, 105]","[4, 50]","[6, 20]","[1, 35]","Overview:
The paper aim to model non-linear, intrinsically low-dimensional structure, in data by estimating ""transport operators"" that predict how points move along the manifold. This is an old idea, and the stated contribution of the paper is:
""The main contribution of this paper is to show that the manifold representation learned in the transport operators is valuable both as a probabilistic model to improve general machine learning tasks as well as for performing transfer learning in classification tasks."" 
The paper provide nice illustrative experiments arguing why transport operators may be a useful modeling tool, but does not go beyond illustrative experiments.
While I follow the intuitions behind transport operators I am doubtful if they will generalize beyond very simple manifold structures (see detailed comments below).

Quality:
The paper is well-written and fairly easy to follow. In particular, I appreciate that the authors make no attempt to overclaim contributions. From a methodology point-of-view, the paper has limited novelty (transport operators, and learning thereof has been studied elsewhere), but there are some technical insights (likelihood model, use in data augmentation). Since the provided experiments are mostly illustrations, I would argue that the significance of the paper is limited. I'd say that to really convince a broader audience that this old idea is worth revisiting, the work must go beyond illustrations and apply to a real data problem.

Detailed Comments and Questions:
*) Equation 1 of the paper describe the key dynamics of the applied transport operators. Basically, the paper assume that the underlying data manifold is locally governed by a linear differential equation. This is a very suitable assumption, e.g., for the swiss roll data set, but it is unclear to this reader why it is a suitable assumption beyond such toy data. I would very much appreciate a detailed discussion of when this is a suitable modeling choice, and when it is not. My intuition is that this is mostly a suitable model when the data manifold appears due to simple transformations (e.g. rotations) of data. This is also exactly the type of data considered in the paper.
*) In Eq. 3, should it be ""expm"" instead of ""exp"" ?
*) The first two paragraphs of Sec. 2 are background material, whereas paragraph 3 and beyond describe material that is key to the paper. I would recommend introducing a \subsection (or something like it) to make this more clear.
*) The idea of working with transformations of data rather than the actual data is the corner-stone of Ulf Grenander's renowned ""Pattern Theory"". A citation to this seminal work would be appropriate.
*) In the first paragraph of the introduction links are drawn to the neuroscience literature; it would be appropriate to cite a suitable publication.

Pros(+) & Cons(-):
+ Well-written.
+ Good illustrative experiments.
- Real-life experiments are lacking.
- Limited methodology contribution.
- The assumed dynamics might be too simplistic (at least a discussing of this is missing).

For the AC:
The submitted paper acknowledges several grants (including grant numbers), which can directly be tied to the authors identity. This may be a violation of the double blind review policy. I did not use this information to determine the authors identity, though, so this review is still double blind.

Post-rebuttal comments:
The paper has improved with the incorporated revisions, but my main concerns remain. I find the Swiss Roll / rotated-USPS examples to be too contrived as the dynamics are exactly tailored to the linear ODE assumption. These are examples where the model assumptions are perfect. What is unclear is how the model behaves when the linear ODE assumption is not-quite-correct-but-also-not-totally-incorrect, i.e. how the model behaves in real life. I didn't get that from the newly added experiment. So, I'll keep my rating as is. ","[4, 4, 5]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Marginally below acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer acknowledges the paper's clarity and the value of the illustrative experiments. However, they express concerns about the limited generalizability of the proposed method beyond simple manifold structures and the lack of real-life experiments. The reviewer finds the core assumption (locally linear data manifold) to be potentially limiting. The overall tone suggests the reviewer leans towards skepticism, but they are constructive in their criticism and offer specific suggestions for improvement. Therefore, the sentiment is somewhat negative but not overly so.",-20.0,70.0
PARAMETRIZED DEEP Q-NETWORKS LEARNING: PLAYING ONLINE BATTLE ARENA WITH DISCRETE-CONTINUOUS HYBRID ACTION SPACE,"['Jiechao Xiong', 'Qing Wang', 'Zhuoran Yang', 'Peng Sun', 'Yang Zheng', 'Lei Han', 'Haobo Fu', 'Xiangru Lian', 'Carson Eisenach', 'Haichuan Yang', 'Emmanuel Ekwedike', 'Bei Peng', 'Haoyue Gao', 'Tong Zhang', 'Ji Liu', 'Han Liu']",Reject,2018,"[6, 18, 4, 17, 19, 19, 9, 4, 1, 17, 4, 'no_match', -1, 23, 15]","[10, 23, 9, 22, 24, 24, 14, 8, 5, 22, 9, 'no_match', 1, 28, 19]","[32, 349, 206, 276, 121, 221, 14, 33, 9, 66, 5, 'no match', 1, 393, 118]","[13, 165, 88, 130, 62, 106, 11, 16, 1, 31, 2, 'no match', 1, 174, 63]","[14, 23, 106, 32, 16, 28, 2, 16, 6, 16, 2, 'no match', 0, 147, 27]","[5, 161, 12, 114, 43, 87, 1, 1, 2, 19, 1, 'no match', 0, 72, 28]","This paper presents a new reinforcement learning approach to handle environments with a mix of discrete and
continuous action spaces. The authors propose a parameterized deep Q-network (P-DQN) and leverage learning
schemes from existing algorithms such as DQN and DDPG to train the network. The proposed loss function and
alternating optimization of the parameters are pretty intuitive and easy to follow. My main concern is
with lack of sufficient depth in empirical evaluation and analysis of the method.

Pros:
1. The setup is an interesting and practically useful one to investigate. Many real-world environments require individual actions
 that are further parameterized over a continuous space.
2. The proposed method is simple and intuitive.

Cons:
1. The evaluation is performed only on a single environment in a restricted fashion. I understand the authors are restricted in the choice of environments which require a hybrid action space. However,
 even domains like Atari could be used in a setting where the continuous parameter x_k refers to the number of
 repetitions for action k. This is similar to the work of Lakshminarayanan et al. (2017). Could you test your algorithm in such a setting?
2. Comparison of the algorithm is performed only against DDPG. Have you tried other options like PPO (Schulman et al., 2017)?
 Also, considering that the action space is simplified in the experimental setup (""we use the default parameters of skills provided by the game environment, usually pointing to
the opponent hero's location""), with only the move(\alpha) action being a hybrid, one could imagine discretizing the move
direction \alpha and training a DQN (or any other algorithms over discrete action spaces) as another baseline.
3. The reward structure seems to be highly engineered. With so many components in the reward, it is not clear
what the individual contributions are and what policies are actually learned.
4. The authors don't provide any analysis of the empirical results. Do the P-DQN and DDPG converge to the same policy?
What factor(s) contribute most to the faster learning of P-DQN? Do the values of \alpha and \beta for the two-timescale
updates affect the results considerably?
5. (minor) The writing contains a lot of grammatical errors which makes this draft below par for an ICLR paper.


Other Questions:
1. In eq. 5.3, the loss over \theta is defined as the sum of Q values over different k. Did you try other formulations of
the loss? (say, product of the Q values for instance) One potential issue with the sum could be that if some values of k dominate this sum, Q(s, k, x_k; w) might not be maximized for all k.
2. Some terms of the reward function seem to be overly dependent on historic actions (ex. difference in gold and hitpoints). This could swamp the
influence of the other terms which are more dependent on the current action a_t, which might be an issue, especially with the Markovian assumption?

References:
 Lakshminarayanan et al, 2017; Dynamic Action Repetition for Deep Reinforcement Learning; AAAI
 Schulman et al., 2017; Proximal Policy Optimization Algorithms; Arxiv
","[4, 5, 5]","[' Ok but not good enough - rejection', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review acknowledges the paper's merits, such as the interesting problem setup and intuitive method (Pros: 1, 2). However, it expresses significant concerns about the evaluation's depth, including limited environments, missing baselines, and lack of analysis (Cons: 1, 2, 3, 4). The reviewer also points out writing issues (Con: 5). While the reviewer provides constructive criticism and suggestions for improvement, the numerous and substantial concerns suggest a generally lukewarm reception to the paper.",20.0,70.0
Better Generalization by Efficient Trust Region Method,"['Xuanqing Liu', 'Jason D. Lee', 'Cho-Jui Hsieh']",Reject,2018,"[2, 12, 11]","[5, 17, 16]","[29, 192, 355]","[11, 77, 171]","[18, 106, 158]","[0, 9, 26]","The paper proposes training neural networks using a trust region method, in which at each iteration a (non-convex) quadratic approximation of the objective function is found, and the minimizer of this quadratic within a fixed radius is chosen as the next iterate, with the radius of the trust region growing or shrinking at each iteration based on how closely the gains of the quadratic approximation matched those observed on the objective function. The authors claim that this approach is better at avoiding ""narrow"" local optima, and therefore will tend to generalize better than minibatched SGD. The main novelty seems to be algorithm 2, which finds the minimizer of the quadratic approximation within the trust region by performing GD iterations until the boundary is hit (if it is--it might not, if the quadratic is convex), and then Riemannian GD along the boundary.

The paper contains several grammatical mistakes, and in my opinion could explain things more clearly, particularly when arguing that the algorithm 2 will converge. I had particular difficulty accepting that the phase 1 GD iterates would never hit the boundary if the quadratic was strongly convex, although I accept that it is true due to the careful choice of step size and initialization (assumptions 1 and 2).

The central claim of the paper, that a trust region method will be better at avoiding narrow basins, seems plausible, since if the trust region is sufficiently large then it will simply pass straight over them. But if this is the case, wouldn't that imply that the quadratic approximation to the objective function is poor, and therefore that line 5 of algorithm 1 should shrink the trust region radius? Additionally, at some times the authors seem to indicate that the trust region method should be good at escaping from narrow basins (as opposed to avoiding them in the first place), see for example the left plot of figure 4. I don't see why this is true--the quadratic approximation would be likely to capture the narrow basin only.

This skepticism aside, the experiments in figure 2 do clearly show that, while the proposed approach doesn't converge nearly as quickly as SGD in terms of training loss, it does ultimately find a solution that generalizes better, as long as both SGD and TR use the same batch size (but I don't see why they should be using the same batch size). How does SGD with a batch size of 1 compare to TR with the batch sizes of 512 (CIFAR10) or 1024 (STL10)?

Section 4.3 (Figure 3) contain a very nice experiment that I think directly explores this issue, and seems to show that SGD with a batch size of 64 generalizes better than TR at any of the considered batch sizes (but not as well as the proposed TR+SGD hybrid). Furthermore, 64 was the smallest batch size considered, but SGD was performing monotonically better as the batch size decreased, so one would expect it to be still better for 32, 16, etc.

Smaller comments:

You say that you base the Hessian and gradient estimates on minibatched samples. I assume that the same is true for the evaluations of F on line 4 of Algorithm 1? Do these all use the same minibatch, at each iteration?

On the top of page 3: ""M is the matrix size"". Is this the number of elements, or the number of rows/columns?

Lemma 1: This looks correct to me, but are these the KKT conditions, which I understand to be first order optimality conditions (these are second order)? You cite Nocedal & Wright, but could you please provide a page number (or at least a chapter)?

On the top of page 5, ""Line 10 of Algorithm 1"": I think you mean Line 11 of Algorithm 2.","[5, 6, 6]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[2, 3, 5]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review is quite balanced, leaning slightly towards the critical side. It acknowledges the potential merits of the paper's idea and the positive results in some experiments but also raises several concerns and questions about the approach and clarity. The reviewer maintains a professional and respectful tone throughout, even when expressing skepticism or pointing out flaws. There are no personal attacks or dismissive language. The critique focuses on the scientific content and presentation. Therefore, the sentiment is slightly negative, and the politeness is neutral to slightly positive.",-20.0,70.0
ENRICHMENT OF FEATURES FOR CLASSIFICATION USING AN OPTIMIZED LINEAR/NON-LINEAR COMBINATION OF INPUT FEATURES,"['Mehran Taghipour-Gorjikolaie', 'Seyyed Mohammad Razavi', 'Javad Sadri']",Reject,2018,"[0, 1, 15]","[3, 4, 18]","[6, 11, 28]","[0, 0, 12]","[1, 1, 0]","[5, 10, 16]","The paper presents a method for feature projection which uses a two level neural network like structure to generate new features from the input features. The weights of the NN like structure are optimised using a genetic search algorithm which optimises the cross-validation error of a nearest neighbor classifier. The method is tested on four simple UCI datasets. There is nothing interesting or novel about the paper. It is not clear whether the GA optimisation takes place on the level of cross validation error estimation or within an internal validation set as it should have been the case. The very high accuracies reported seem to hint the latter, which is a serious methodological error. The poor language and presentation does not help in clearing that, as it does not help in general. ","[1, 2, 3]","[' Trivial or wrong', ' Strong rejection', ' Clear rejection']","[5, 3, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with a neutral factual summary but quickly turns very negative.  Phrases like ""nothing interesting or novel"", ""serious methodological error"", and ""poor language and presentation""  indicate a highly critical stance. The reviewer also questions the methodology with skepticism. While direct, the language avoids overtly rude or unprofessional terms.",-85.0,-20.0
Correcting Nuisance Variation using Wasserstein Distance,"['Gil Tabak', 'Minjie Fan', 'Samuel J. Yang', 'Stephan Hoyer', 'Geoff Davis']",Reject,2018,"[4, 4, 4, 2, 26]","[1, 5, 9, 6, 30]","[1, 2, 6, 16, 4]","[0, 0, 1, 2, 2]","[0, 1, 3, 12, 1]","[1, 1, 2, 2, 1]","The authors present a method that aims to remove domain-specific information while preserving the relevant biological information between biological data measured in different experiments or ""batches"". A network is trained to learn the transformations that minimize the Wasserstein distance between distributions. The wasserstein distance is also called the ""earth mover distance"" and is traditionally formulated as the cost it takes for an optimal transport plan to move one distribution to another. In this paper they have a neural network compute the wasserstein distance using a different formulation that was used in Arjovsky et al. 2017, finds a lipschitz function f, which shows the maximal difference when evaluated on samples from the two distributions. Here these functions are formulated as affine transforms of the data with parameters theta that are computed by a neural network. Results are examined mainly by looking at the first two PCA components of the data.  


The paper presents an interesting idea and is fairly well written. However I have a few concerns:
1. Most of the ideas presented in the paper rely on works by Arjovsky et al. (2017), Gulrajani et al. (2017), and Gulrajani et al. (2017). Some selections, which are presented in the papers are not explained, for example, the gradient penalty, the choice of \lambda and the choice of points for gradient computation.
2. The experimental results are not fully convincing, they simply compare the first two PC components on this Broad Bioimage benchmark collection. This section could be improved by demonstrating the approach on more datasets.
3. There is a lack comparison to other methods such as Shaham et al. (2017). Why is using earth mover distance better than MMD based distance? They only compare it to a method named CORAL and to Typical
Variation Normalization (TVN). What about comparison to other batch normalization methods in biology such as SEURAT? 
4. Why is the affine transform assumption valid in biology? There can definitely be non-linear effects that are different between batches, such as ion detection efficiency differences. 
5. Only early stopping seems to constrain their model to be near identity. Doesn't this also prevent optimal results ? How does this compare to the near-identity constraints in resnets in Shaham et al. ?

","[4, 7, 5]","[' Ok but not good enough - rejection', ' Good paper, accept', ' Marginally below acceptance threshold']","[5, 3, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer starts with positive aspects by acknowledging the interesting idea and well-written manuscript. However, they express several concerns about the technical choices, limited experimental validation, lack of comparison with relevant methods, and questionable assumptions. The criticism, while direct, is presented in a professional and constructive manner, suggesting improvements rather than simply dismissing the work.  Therefore, the sentiment leans slightly positive due to the initial praise, but the numerous concerns pull it towards neutral. The language remains polite throughout, maintaining a professional academic tone.",20.0,70.0
TCAV: Relative concept importance testing with Linear Concept Activation Vectors,"['Been Kim', 'Justin Gilmer', 'Martin Wattenberg', 'Fernanda Viégas']",Reject,2018,"[9, 6, 24, 20]","[14, 11, 29, 25]","[74, 49, 76, 64]","[29, 18, 38, 33]","[41, 28, 20, 17]","[4, 3, 18, 14]","Strengths:
1. This paper proposes a novel method called Concept Activation Vectors (CAV) which facilitates interpretability of neural networks by explaining how much a specific concept influences model predictions. 
2. The proposed method tries to incorporate multiple desiderata, namely, accessibility to non ML experts, customizability w.r.t. being able to explain any concept of interest, plug-in readiness i.e., providing explanations
without requiring retraining of the model. 

Weaknesses:
1. While this work is conceptually interesting, the technical novelty and contributions seem fairly minimal. 
2. The presentation of this paper is one of its weakest points. The organization of the content is quite incoherent. The paper also makes a lot of claims (e.g., hypothesis testing) which are not really justified. 
3. The experimental evaluation of this paper is quite rudimentary. Lots of details are missing. 

Summary: This paper proposes a novel framework for explaining the functionality of neural networks by using a simple idea. The intuition behind the proposed approach is as follows: by using the weight vectors of linear classifiers, which take as inputs the activation layer outputs of a given neural network (NN) model and predict the concepts of interest, we can understand the influence of specific concepts of interest on the NN model behavior. The authors claim that this simple approach can be quite useful in providing explanations that can be useful for a variety of purposes including testing specific hypothesis which is never really demonstrated or explained well in the paper. Furthermore, lot of details are lacking in both the experimentation section and the methods section (detailed comments below). The experiments also do not correspond well to the claims made in the introduction and abstract. This paper is also very hard to read which makes understanding the proposed method and other details quite challenging. 

Novelty: The novelty of this paper mainly stems from its proposed method of using prototypes which serve as positive and negative examples w.r.t. a specific concept, and leveraging the weight vectors obtained when predicting the positive/negative classes using activation layer outputs to understand the influence of concepts of interest.  The technical novelty of the proposed approach is fairly minimal. The experiments also do not support a lot of novelty claims made about the proposed approach. 

Other detailed comments:
1. I would first encourage the authors to improve the overall presentation and organization of this paper. 
2. Please add some intuition about the approach in the introduction. Also, please be succinct in explaining what kind of interpretability is provided by the explanations. I would advise the authors to refrain from making very broad claims and using words such as hypothesis testing without discussing them in detail later in the paper. 
3. Sections 2.3 and 2.4 are quite confusing and can probably be organized and titled differently. In fact, I would advise the authors to structure related work as i. inherently interpretable models ii. global explanations 
iii. local explanations iv. neuron level investigation methods. Highlight how existing methods do not incorporate plug-in readiness and/or other desiderate wherever appropriate within these subsections. 
4. Additional related work on inherently interpretable models and global explanations: 
i. Interpretable classifiers using rules and Bayesian analysis, Annals of Applied Statistics, 2015
ii. Interpretable Decision Sets: A joint framework for description and prediction, KDD, 2016
iii. A Bayesian Framework for Learning Rule Sets for Interpretable Classification, JMLR, 2017
iv. Interpretable and Explorable Explanations of Black Box Models, FAT ML, 2017
5. In section 3, clearly identify what are the inputs and outputs of your method. Also, clearly highlight the various ways in which outputs of your method can be used to understand the model behavior. While Secction 3.2 and 3.3 attempt to describe how the CAV can be used to explain the model behavior, the presentation in these sections can be improved. 
6. I think the experimental sections suffers from the following shortcomings: i. it does not substantiate all the claims made in the introduction ii. some of the details about which layer outputs are being studied are missing through out the section. 

Overall, while this paper proposes some interesting ideas, I think it can be improved significantly in terms of its clarity, claims, and evaluation.  
","[4, 3, 5, 4]","[' Ok but not good enough - rejection', ' Clear rejection', ' Marginally below acceptance threshold', ' Ok but not good enough - rejection']","[3, 5, 2, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review acknowledges the interesting idea but highlights significant weaknesses in technical novelty, presentation, and experimental validation. It provides constructive criticism with detailed suggestions for improvement. The language is formal and professional, without resorting to harsh or disrespectful tones.",-20.0,80.0
Piecewise Linear Neural Networks verification: A comparative study,"['Rudy Bunel', 'Ilker Turkaslan', 'Philip H.S. Torr', 'Pushmeet Kohli', 'M. Pawan Kumar']",Reject,2018,"[3, 2, 28, 16, 17]","[8, 4, 33, 21, 22]","[43, 4, 623, 322, 118]","[16, 1, 299, 180, 57]","[24, 2, 241, 102, 43]","[3, 1, 83, 40, 18]","The paper compares some recently proposed method for validation of properties
of piece-wise linear neural networks and claims to propose a novel method for
the same. Unfortunately, the proposed ""branch and bound method"" does not explain
how to implement the ""bound"" part (""compute lower bound"") -- and has been used 
several times in the same application, incl.:

Ruediger Ehlers. Planet. https://github.com/progirep/planet,
Chih-Hong Cheng, Georg Nuhrenberg, and Harald Ruess.  Maximum resilience of artificial neural networks. Automated Technology for Verification and Analysis
Alessio Lomuscio and Lalit Maganti.  An approach to reachability analysis for feed-forward relu neural networks. arXiv:1706.07351

Specifically, the authors say: ""In our experiments, we use the result of 
minimising the variable corresponding to the output of the network, subject 
to the constraints of the linear approximation introduced by Ehlers (2017a)""
which sounds a bit like using linear programming relaxations, which is what
the approaches using branch and bound cited above use. If that is the case,
the paper does not have any original contribution. If that is not the case,
the authors may have some contribution to make, but have not made it in this
paper, as it does not explain the lower bound computation other than the one
based on LPs.

Generally, I find a jarring mis-fit between the motivation (deep learning
for driving, presumably involving millions or billions of parameters) and
the actual reach of the methods proposed (hundreds of parameters).
This reach is NOT inherent in integer programming, per se. Modern solvers
routinely solve instances with tens of millions of non-zeros in the constraint
matrix, but require a strong relaxation. The authors may hence consider
improving the LP relaxation, noting that the big-M constraint are notorious
for producing weak relaxations.","[3, 6, 5]","[' Clear rejection', ' Marginally above acceptance threshold', ' Marginally below acceptance threshold']","[5, 3, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is critical of the paper's lack of originality and limited practical relevance. The reviewer points out that the proposed method seems to be a rehash of existing techniques and questions the paper's contribution. The reviewer also criticizes the gap between the paper's ambitious motivation and the limited scale of the proposed method. While the reviewer uses strong language like ""jarring mis-fit"" and ""does not have any original contribution,"" the criticism is grounded in specific technical points and presented in a professional manner. ",-50.0,50.0
Iterative Deep Compression : Compressing Deep Networks for Classification and Semantic Segmentation,"['Sugandha Doda', 'Vitor Fortes Rey', 'Dr. Nadereh Hatami', 'Prof. Dr. Paul Lukowicz']",Reject,2018,"[2, -2, 'no_match', 'no_match']","[6, 3, 'no_match', 'no_match']","[2, 2, 'no match', 'no match']","[1, 0, 'no match', 'no match']","[1, 1, 'no match', 'no match']","[0, 1, 'no match', 'no match']","quality: this paper is of good quality
clarity: this paper is very clear
originality: this paper combines original ideas with existing approaches for pruning to obtain dramatic space reduction in NN parameters.
significance: this paper seems significant.

PROS
- a new approach to sparsifying that considers different thresholds for each layer
- a systematic, empirical method to obtain optimal sparsity levels for a given neural network on a task.
- Very interesting and extensive experiments that validate the reasoning behind the described approach, with a detailed analysis of each step of the algorithm.

CONS
- Pruning time. Although the authors argue that the pruning algorithm is not prohibitive, I would argue that >1 month to prune LeNet-5 for MNIST is certainly daunting in many settings. It would benefit the experimental section to use another dataset than MNIST (e.g. CIFAR-10) for the image recognition experiment.
- It is unclear whether this approach will always work well; for some neural nets, the currently used sparsification method (thresholding) may not perform well, leading to very little final sparsification to maintain good performance.
- The search for the optimal sparsity in each level seems akin to a brute-force search. Although possibly inevitable, it would be valuable to discuss whether or not this approach can be refined.

Main questions
- You mention removing ""unimportant and redundant weights"" in the pruning step; in this case, do unimportant and redundant have the same meaning (smaller than a given threshold), or does redundancy have another meaning (e.g. (Mariet, Sra, 2016))?
- Algorithm 1 finds the best sparsity for a given layer that maintains a certain accuracy. Have you tried using a binary search for the best sparsity instead of simply decreasing the sparsity by 1% at each step? If there is a simple correlation between sparsity and accuracy, that might be faster; if there isn't (which would be believable given the complexity of neural nets), it would be valuable to confirm this with an experiment.
- Have you tried other pruning methods than thresholding to decide on the optimal sparsity in each layer?
- Could you please report the final accuracy of both models in Table 2?

Nitpicks:
- paragraph break in page 4 would be helpful.","[6, 5, 4]","[' Marginally above acceptance threshold', ' Marginally below acceptance threshold', ' Ok but not good enough - rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is overall positive. The reviewer finds the paper interesting, significant, and well-written. They praise the approach, systematic methodology, and detailed experiments. While they raise valid concerns about pruning time, generalizability, and the search method's efficiency, these are presented constructively as areas for improvement rather than outright criticisms. The tone is professional and polite throughout, suggesting a willingness to see the paper published with revisions.",70.0,80.0
Discrete-Valued Neural Networks Using Variational Inference,"['Wolfgang Roth', 'Franz Pernkopf']",Reject,2018,"[40, 18]","[44, 23]","[24, 179]","[9, 110]","[11, 32]","[4, 37]","The authors consider the problem of ultra-low precision neural networks motivated by 
limited computation and bandwidth. Their approach first posits a Bayesian neural network
a discrete prior on the weights followed by central limit approximations to efficiently 
approximate the likelihood. The authors propose several tricks like normalization and cost 
rescaling to help performance. They compare their results on several versions of MNIST. The 
paper is promising, but I have several questions:

1) One major concern is that the experimental results are only on MNIST. It's important 
to have another (larger) dataset to understand how sensitive the approach is to 
characteristics of the data. It seems plausible that a more difficulty problem may 
require more precision.

2) Likelihood weighting is related to annealing and variational tempering

3) The structure of the paper could be improved:
 - The introduction contains way too many details about the method 
    and related work without a clear boundary.
 - I would add the model up front at the start of section 2
 - Section 2.1 could be reversed or equations 2-5 could be broken with text 
   explaining each choice 

4) What does training time look like? Is the Bayesian optimization necessary?","[5, 5, 6]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Marginally above acceptance threshold']","[4, 4, 1]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', "" The reviewer's evaluation is an educated guess""]","The review starts with positive wording like ""promising"" and acknowledges the paper's novelty. However, it also raises several significant concerns and suggestions for improvement, indicating that the reviewer sees potential but has reservations. Therefore, the sentiment is slightly positive but closer to neutral. The language used is constructive and professional throughout, suggesting a polite and respectful tone.",20.0,80.0
Revisiting Knowledge Base Embedding as Tensor Decomposition,"['Jiezhong Qiu', 'Hao Ma', 'Yuxiao Dong', 'Kuansan Wang', 'Jie Tang']",Reject,2018,"[5, 12, 9, 27, 16]","[10, 16, 14, 32, 21]","[41, 64, 128, 102, 497]","[18, 40, 67, 63, 278]","[21, 18, 47, 18, 105]","[2, 6, 14, 21, 114]","The paper proposes a new method to train knowledge base embeddings using a least-squares loss. For this purpose, the paper introduces a reweighting scheme of the entries in the original adjacency tensor. The reweighting is derived from an analysis of the cross-entropy loss. In addition, the paper discusses the connections of the margin and cross-entropy loss and evaluates the proposed method on WN18 and FB15k.

 The paper tackles an interesting problem, as learning from knowledge bases via embedding methods has become increasingly important for tasks such as question answering. Providing additional insight into current methods can be an important contribution to advance the state-of-the-art.

However, I'm concerned about several aspects in the current form of the paper. For instance, the derivation in Section 4 is unclear to me, as eq.4 suddenly introduces a weighted sum over expectations using the degrees of nodes. The derivation also seems to rely on a very specific negative sampling assumption (uniform sampling without checking whether the corrupted triple is a true negative). This sampling method isn't used consistently across models and also brings its own problems, e.g., see the LCWA discussion in [4]

In addition, the semantics that are introduced by the weighting scheme are not clear to me either. Using the proposed method, the probability of edges between high-degree nodes are down-weighted, since the ground-truth labels are divided by the node degrees. Since these weighted labels are then fitted using a least-squares loss, this implies that links between high-degree nodes should be less likely, which seems the opposite of what the scores should look like.

With regard to the significance of the contributions: Using a least-squares loss in combination with tensor methods is attractive because it enables ALS algorithms with closed-form updates that can be computed very fast. However, the proposed method still relies on SGD optimization. In this context, it is not clear to me why a tensor framework/least-squares loss would be preferable.

Further comments:
- The paper seems to equate ""tensor method"" with using a least squares loss. However, this doesn't have to be the case. For instance see [1,2] which propose Logistic and Poisson tensor factorizations, respectively.
- The distinction between tensor factorization and neural methods is unclear. Tensor factorization can be interpreted just as a particular scoring function. For instance, see [5] for a detailed discussion.
- The margin based ranking loss has been proposed earlier than in (Collobert et al, 2011). For instance see [3]
- p1: corrupted triples are not described entirely correct, typically only one of s or o is corrputed. 
- Closed-form tensor in Table 1: This should be least-squares loss of f(s,p,o) and log(...)?
- p6: Adding the constant to the tensor as proposed in (Levy & Goldberg, 2014) can done while gathering the minibatch and is therefore equivalent to the proposed approach.

[1] Nickel et al: Logistic Tensor Factorization for Multi-Relational Data, 2013.
[2] Chi et al: ""On tensors, sparsity, and nonnegative factorizations"", 2012
[3] Collobert et al: A unified architecture for natural language processing, 2008
[4] Dong et al: Knowledge Vault: A Web-Scale Approach to Probabilistic Knowledge Fusion, 2014
[5] Nickel et al: A Review of Relational Machine Learning for Knowledge Graphs, 2016.","[3, 5, 3]","[' Clear rejection', ' Marginally below acceptance threshold', ' Clear rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer acknowledges the relevance of the paper's topic and its potential but raises several concerns about the clarity of the derivation, the semantics of the weighting scheme, and the significance of the contributions. They find the derivation unclear, the weighting scheme's logic questionable, and the choice of a least-squares loss unconvincing. The reviewer also points out several factual inaccuracies and provides additional references to support their critique. The detailed critique, specific concerns, and additional references suggest a rather critical stance. However, the language remains professional and avoids harsh or disrespectful language.",-30.0,60.0
Autostacker: an Automatic Evolutionary Hierarchical  Machine Learning System,"['Boyuan Chen', 'Warren Mo', 'Ishanu Chattopadhyay', 'Hod Lipson']",Reject,2018,"[2, 1, 16, 23]","[6, 1, 21, 28]","[24, 2, 50, 197]","[10, 1, 26, 110]","[13, 1, 12, 36]","[1, 0, 12, 51]","The author present Autostacker, a new algorithm for combining the strength of different learning algorithms during hyper parameter search. During the first step, the hyperparameter search is done in a conventional way. At the second step, the output of each primitives is added to the features of the original dataset and the training and hyperparameter search starts again. This process is repeated for some number of steps. The experiments are performed on 15 small scale dataset and show that Autostacker is performing better  than random forest almost systematically and better than TPOT, the external baseline, 13 times out of 15. Also the speed comparison favor Autostacker vs TPOT.

This algorithm is not highly innovative. Using the output of some algorithms as the input of another one for learning was seen numerous time in the literature. The novelty here is how exactly it is performed, which is a bit ad hoc. 

While testing on numerous dataset is important to verify the strength of a learning algorithm, final statistical significance test should be provided e.g. Sign Test, Wilcoxon Signed Rank Test.

The experiment compares with a weak baseline and a baseline that is unknown to me. Also, the datasets are all small scale which is not representative of modern machine learning. This leaves me very uncertain about the actual quality of the proposed algorithm. 

The strength of the Random Forest baseline could easily be augmented by simply considering the best learning algorithm over validation across the hyper parameter search (i.e. the choice of the learning algorithm is also a hyperparameter). Also a very simple and fast ensemble could be considered by using Agnostic Bayesian Learning of Ensembles (Lacoste et. al.). It is also common to simply consider a linear combination of the output of the different estimator obtained during cross validation and very simple to implement. This would provide other interesting baselines.

Finally the writing of the paper could be highly improved. Many typos, including several badly formatted citations (consider using \citet and \citep for a proper usage of parenthesis).
","[4, 3, 4]","[' Ok but not good enough - rejection', ' Clear rejection', ' Ok but not good enough - rejection']","[5, 4, 5]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review starts with a neutral tone, presenting a summary of the paper's content. However, it quickly transitions into a negative sentiment, criticizing the algorithm's lack of novelty, the experimental setup (weak baselines, small datasets), and the writing quality. The reviewer acknowledges some positive aspects, such as the speed comparison favoring Autostacker, but these are overshadowed by the negative points. The language used is critical but remains within the bounds of professional academic discourse, suggesting suggestions for improvement rather than resorting to personal attacks.",-50.0,50.0
LEARNING SEMANTIC WORD RESPRESENTATIONS VIA TENSOR FACTORIZATION,"['Eric Bailey', 'Charles Meyer', 'Shuchin Aeron']",Reject,2018,"[2, 39, 15]","[1, 29, 20]","[1, 4, 137]","[0, 1, 53]","[1, 0, 60]","[0, 3, 24]","In this paper, the authors consider symmetric (3rd order) CP decomposition of a PPMI tensor M (from neighboring triplets), which they call CP-S. Additionally, they propose an extension JCP-S, for n-order tensor decompositions. This is then compared with random, word2vec, and NNSE, the latter of two which are matrix factorization based (or interpretable) methods. The method is shown to be superior in tasks of 3-way outlier detection, supervised analogy recovery, and sentiment analysis. Additionally, it is evaluated over the MEN and Mturk datasets.


For the JCP-S model, the loss function is unclear to me. L is defined for 3rd order tensors only;  how is the extended to n > 3? Intuitively it seems that L is redefined, and for, say, n = 4, the model is M(i,j,k,n) = \sum_1^R u_ir u_jr u_kr u_nr. However, the statement ""since we are using at most third order tensors in this work"" I am further confused. Is it just that JCP-S also incorporates 2nd order embeddings? I believe this requires clarification in the manuscript itself.

For the evaluations, there are no other tensor-based methods evaluated, although there exist several well-known tensor-based word embedding models existing:

Pengfei Liu, Xipeng Qiu∗ and Xuanjing Huang, Learning Context-Sensitive Word Embeddings with Neural Tensor Skip-Gram Model,  IJCAI 2015

Jingwei Zhang and Jeremy Salwen, Michael Glass and Alfio Gliozzo. Word Semantic Representations using Bayesian Probabilistic Tensor Factorization, EMNLP 2014

Mo Yu, Mark Dredze, Raman Arora, Matthew R. Gormley, Embedding Lexical Features via Low-Rank Tensors

to name a few via quick googling.

Additionally, since it seems the main benefit of using a tensor-based method is that you can use 3rd order cooccurance information, multisense embedding methods should also be evaluated. There are many such methods, see for example 

Jiwei Li, Dan Jurafsky, Do Multi-Sense Embeddings Improve Natural Language Understanding?

and citations within, plus quick googling for more recent works.

I am not saying that these works are equivalent to what the authors are doing, or that there is no novelty, but the evaluations seem extremely unfair to only compare against matrix factorization techniques, when in fact many higher order extensions have been proposed and evaluated, and especially so on the tasks proposed (in particular the 3-way outlier detection). 

Observe also that in table 2, NNSE gets the highest performance in both MEN and MTurk. Frankly this is not very surprising; matrix factorization is very powerful, and these simple word similarity tasks are well-suited for matrix factorization. So, statements like ""as we can see, our embeddings very clearly outperform the random embedding at this task"" is  an unnecessary inflation of a result that 1) is not good and 2) is reasonable to not be good. 

Overall, I think for a more sincere evaluation, the authors need to better pick tasks that clearly exploit 3-way information and compare against other methods proposed to do the same.

The multiplicative relation analysis is interesting, but at this point it is not clear to me why multiplicative is better than additive in either performance or in giving meaningful interpretations of the model. 

In conclusion, because the novelty is also not that big (CP decomposition for word embeddings is a very natural idea) I believe the evaluation and analysis must be significantly strengthened for acceptance. ","[5, 5, 5]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[3, 5, 5]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The reviewer raises several valid concerns, indicating a negative sentiment towards the paper. While the reviewer acknowledges some positive aspects (e.g., finding the multiplicative relation analysis interesting), the criticism regarding the evaluation, comparison with existing methods, and clarity of the model outweighs the positive points. The language used is critical but professional and remains within the bounds of academic politeness.",-40.0,60.0
How do deep convolutional neural networks learn from raw audio waveforms?,"['Yuan Gong', 'Christian Poellabauer']",Reject,2018,"[9, 20]","[14, 24]","[56, 144]","[21, 96]","[26, 16]","[9, 32]","The paper provides an analysis of the representations learnt in convolutional neural networks that take raw audio waveforms as input for a speaker emotion recognition task. Based on this analysis, an architecture is proposed and compared to other architectures inspired by other recent work. The proposed architecture overfits less on this task and thus performs better.

I think this work is not experimentally strong enough to draw the conclusions that it draws. The proposed architecture, aptly called ""SimpleNet"", is relatively shallow compared to the reference architectures, and the task that is chosen for the experiments is relatively small-scale. I think it isn't reasonable to draw conclusions about what convnets learn in general from training on a single task, and especially not a small-scale one like this. 

Moreover, SoundNet, which the proposed architecture is compared to, was trained on (and designed for) a much richer and more challenging task originally. So it is not surprising at all that it overfits dramatically to the tasks chosen here (as indicated in table 1), and that a much shallower network with fewer parameters overfits less. This seems obvious to me, and contrary to what's claimed in the paper, it provides no convincing evidence that shallow architectures are inherently better suited for raw audio waveform processing. This is akin to saying that LeNet-5 is a better architecture for image classification than Inception, because the latter overfits more on MNIST. Perhaps using the original SoundNet task, which is much more versatile, would have lent some more credibility to these claims.

The analysis in section 2.2 is in-depth, but also not very relevant: it ignores the effects of nonlinearities, which are an essential component of modern neural network architectures. Studying their effects in the frequency domain would actually be quite interesting. It is mentioned that the ReLU nonlinearity acts as a half-wave rectifier, but the claim that its effect in the frequency domain is small compared to aliasing is not demonstrated. The claim that ""ReLU and non-linear activations can improve the network performance, but they are not the main factors in the inner workings of CNNs"" is also unfounded.

The conclusion that stacking layers is not useful might make sense in the absence of nonlinearities, but when each layer includes a nonlinearity, the obvious point of stacking layers is to improve the expressivity of the network. Studying aliasing effects in raw audio neural nets is a great idea, but I feel that this work takes some shortcuts that make the analysis less meaningful.




Other comments:

The paper is quite lengthy (11 pages of text) and contains some sections that could easily be removed, e.g. 2.1.1 through 2.1.3 which explain basic signal processing concepts and could be replaced by a reference. In general, the writing could be much more concise in many places.

The paper states that ""it remains unknown what actual features CNNs learn from waveforms."". There is actually some prior work that includes some analysis on what is learnt in the earlier layers of convnets trained on raw audio: 
""Learning the Speech Front-end With Raw Waveform CLDNNs"", Sainath et al.
""Speech acoustic modeling from raw multichannel waveforms"", Hoshen et al.
""End-to-end learning for music audio"", Dieleman & Schrauwen
Only the first one is cited, but not in this context. I think saying ""it remains unknown"" is a bit too strong of an expression.

The meaning of the following comment is not clear to me: ""because in computer vision, the spatial frequency is not the only information the model can use"". Surely the frequency domain and the spatial domain are two different representations of the same information contained in an image or audio signal? So in that sense, spatial frequency does encompass all information in an image.

The implication that high-frequency information is less useful for image-based tasks (""the spatial frequency of images is usually low"") is incorrect. While lower frequencies dominate the spectrum more obviously in images than in audio, lots of salient information (i.e. edges, textures) will be high-frequency, so models would still have to learn high-frequency features to perform useful tasks.

WaveNet is mentioned (2.2.4) but not cited. WaveNet is a fairly different architecture than the ones discussed in this paper and it would be useful to at least discuss it in the related work section. A lot of the supposed issues discussed in this paper don't apply to WaveNet (e.g. there are no pooling layers, there is a multiplicative nonlinearity in each residual block).

The paper sometimes uses concepts without clearly defining them, e.g. ""front-end layers"". Please clearly define each concept when it is first introduced.

The paper seems to make a fairly arbitrary distinction between layers that perform signal filtering operations, and layers that don't - but every layer can be seen as a (possibly nonlinear) filtering operation. Even if SimpleNet has fewer ""front-end layers"", surely the later layers in the network can still introduce aliasing? I think the implicit assumption that later layers in the network perform a fundamentally different kind of operation is incorrect.

It has been shown that even random linear filters can be quite frequency-selective (see e.g. ""On Random Weights and Unsupervised Feature Learning"", Saxe et al.). This is why I think the proposed ""changing rate"" measure is a poor choice to show effective training. Moreover, optimization pathways don't have to be linear in parameter space, and oscillations can occur. Why not measure the difference with the initial values (at iteration 0)? It seems like that would prove the point a bit better.

Manually designing filters to initialize the weights of a convnet has been done in e.g. Sainath et al. (same paper as mentioned before), so it would be useful to refer to it again when this idea is discussed.

In SpecNet, have the magnitude spectrograms been log-scaled? This is common practice and it can make a dramatic difference in performance. If you haven't tried this, please do.
","[2, 3, 3]","[' Strong rejection', ' Clear rejection', ' Clear rejection']","[5, 4, 5]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review starts with a neutral tone, presenting the paper's topic and findings without judgment. However, it quickly transitions into a highly critical analysis, using strong negative qualifiers like ""not experimentally strong enough,"" ""not reasonable,"" ""obvious,"" ""unfounded,"" and ""incorrect."" The reviewer finds significant flaws in the paper's methodology, conclusions, and even writing style. While the reviewer acknowledges some positive aspects (in-depth analysis, interesting idea), the overall tone and numerous negative points contribute to a largely negative sentiment. Despite the critical tone, the language remains professional and avoids personal attacks, staying within the bounds of academic discourse.",-50.0,50.0
Learning Covariate-Specific Embeddings with Tensor Decompositions,"['Kevin Tian', 'Teng Zhang', 'James Zou']",Reject,2018,"[10, 13, 4]","[15, 18, 8]","[62, 174, 18]","[30, 89, 7]","[30, 21, 11]","[2, 64, 0]","This paper presents an embedding algorithm for text corpora that allows known
covariates, e.g. author information, to modify a shared embedding to take context
into account. The method is an extension of the GloVe method and in the case of
a single covariate value the proposed method reduces to GloVe. The covariate-dependent
embeddings are diagonal scalings of the shared embedding. The authors demonstrate
the method on a corpus of books by various authors and on a corpus of subreddits.
Though not technically difficult, the extension of GloVe to covariate-dependent
embeddings is very interesting and well motivated. Some of the experimental results
do a good job of demonstrating the advantages of the models. However, some of the
experiments are not obvious that the model is really doing a good job.

I have some small qualms with the presentation of the method. First, using the term
""size m"" for the number of values that the covariate can take is a bit misleading.
Usually the size of a covariate would be the dimensionality. These would be the same
if the covariate is one hot coded, however, this isn't obvious in the paper right now.
Additionally, v_i and c_k live in R^d, however, it's not really explained what
'd' is, is it the number of 'topics', or something else? Additionally, the functional
form chosen for f() in the objective was chosen to match previous work but with no
explanation as to why that's a reasonable form to choose. Finally, the authors
say toward the end of Section 2 that ""A careful comparision shows that this
approximation is precisely that which is implied by equation 4, as desired"". This is
cryptic, just show us that this is the case.

Regarding the experiments there needs to be more discussion about how the
different model parameters were determined. The authors say ""... and after tuning
our algorithm to emged this dataset, ..."", but this isn't enough. What type of
tuning did you do to choose in particular the latent dimensionality and the
learning rate? I will detail concerns for the specific experiments below.

Section 4.1:
- How does held-out data fit into the plot?

Section 4.2:
- For the second embedding, what exactly was the algorithm trained on? Just the
  book, or the whole corpus?
- What is the reader supposed to take away from Table 1? Are higher or lower
  values better? Maybe highlight the best scores for each column.


Section 4.3:
- Many of these distributions don't look sparse.
- There is a terminology problem in this section. Coordinates in a vector are
  not sparse, the vector itself is sparse if there are many zeros, but
  coordinates are either zero or not zero. The authors' use of 'sparse' when
  they mean 'zero' is really confusing.
- Due to the weird sparsity terminology Table 1 is very confusing. Based on how
  the authors use 'sparse' I think that Table 1 shows the fraction of zeros in
  the learned embedding vectors. But if so, then these vectors aren't sparse at all
  as most values are non-zero.

Section 5.1:
- I don't agree with the authors that the topics in Table 3 are interpretable.
  As such, I think it's a reach to claim the model is learning interpretable topics.
  This isn't necessarily a problem, it's fine for models to not do everything well,
  but it's a stretch for the authors to claim that these results are a positive
  aspect of the model. The results in Section 5.2 seem to make a lot of sense and
  show the big contribution of the model.

Section 5.3:
- What is the ""a : b :: c : d"" notation?
","[5, 5, 5]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[4, 3, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review is generally positive, highlighting the interesting and well-motivated nature of the work. The reviewer acknowledges the value of the method and its potential. While the reviewer points out areas for improvement in the presentation and experiments, the language used is constructive and polite. The reviewer offers specific suggestions and asks for clarification rather than stating flaws bluntly. ",60.0,70.0
Neural Task Graph Execution,"['Sungryull Sohn', 'Junhyuk Oh', 'Honglak Lee']",Reject,2018,"[6, 4, 14]","[11, 9, 19]","[26, 42, 273]","[12, 18, 145]","[12, 23, 123]","[2, 1, 5]","This paper proposes to train recursive neural network on subtask graphs in order to execute a series of tasks in the right order, as is described by the subtask graph's dependencies. Each subtask execution is represented by a (non-learned) option. Reward shaping allows the proposed model to outperform simpler baselines, and experiments show the model generalizes to unseen graphs.

While this paper is as far as I can tell novel in how it does what it does, the authors have failed to convey to me why this direction of research is relevant.
- We know finding options is the hard part about options
- We already have good algorithms that take subtask graphs and execute them in the right order from the planning litterature

An interesting avenue would be if the subtask graphs were instead containing some level of uncertainty, or representing stochasticity, or anything that more traditional methods are unable to deal with efficiently, then I would see a justification for the use of neural networks. Alternatively, if the subtask graphs were learned instead of given, that would open the door to scaling an general learning. Yet, this is not discussed in the paper.

Another interesting avenue would be to learn the options associated with each task, possibly using the information from the recursive neural networks to help learn these options.


The proposed algorithm relies on fairly involved reward shaping, in that it is a very strong signal of supervision on what the next action should be. Additionaly, it's not clear why learning seems to completely ""fail"" without the pre-trained policy. The justification given is that it is ""to address the difficulty of training due to the complex nature of the problem"" but this is not really satisfying as the problems are not that hard. This also makes me question the generality of the approach since the pre-trained policy is rather simple while still providing an apparently strong score.


In your experiments, you do not compare with any state-of-the-art RL or hierarchical RL algorithm on your domain, and use a new domain which has no previous point of reference. It it thus hard to properly evaluate your method against other proposed methods.

What the authors propose is a simple idea, everything is very clearly explained, the experiments are somewhat lacking but at least show an improvement over more a naive approach, however, due to its simplicity, I do not think that this paper is relevant for the ICLR conference. 

Comments:
- It is weird to use both a discount factor \gamma *and* a per-step penalty. While not disallowed by theory, doing both is redundant because they enforce the same mechanism.
- It seems weird that the smoothed logical AND/OR functions do not depend on the number of inputs; that is unless there are always 3 inputs (but it is not explained why; logical functions are usually formalised as functions of 2 inputs) as suggested by Fig 3.
- It does not seem clear how the whole training is actually performed (beyond the pre-training policy). The part about the actor-critic learning seems to lack many elements (whole architecture training? why is the policy a sum of ""p^{cost}"" and ""p^{reward}""? is there a replay memory? How are the samples gathered?). (On the positive side, the appendix provides some interesting details on the tasks generations to understand the experiments.)
- The experiments cover different settings with different task difficulties. However, only one type of tasks is used. It would be good to motivate (in addition to the paragraph in the intro) the cases where using the algorithm described in the paper may be (or not?) the only viable option and/or compare it to other algorithms. Even tough not mandatory, it would also be a clear good addition to also demonstrate more convincing experiments in a different setting.
- ""The episode length (time budget) was randomly set for each episode in a range such that 60% − 80% of subtasks are executed on average for both training and testing."" --> this does not seem very precise: under what policy is the 60-80% defined? Is the time budget different for each new generated environment?
- why wait until exactly 120 epochs for NTS-RProp before fine-tuning with actor-critic? It seems that much less would be sufficient from figure 4?
- In the table 1 caption, it is written ""same graph structure with training set"" --> do you mean ""same graph structure than the training set""?","[4, 6, 6]","[' Ok but not good enough - rejection', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer acknowledges the novelty of the paper but expresses significant concerns about its relevance and contribution. They find the problem addressed to be somewhat trivial and question the necessity of the proposed approach. The reviewer also criticizes the experimental methodology and lack of comparison with existing methods, suggesting the paper might not be suitable for ICLR. The language used, while direct, maintains a professional and respectful tone, focusing on the scientific content rather than resorting to personal attacks.",-50.0,50.0
Achieving morphological agreement with Concorde,"['Daniil Polykovskiy', 'Dmitry Soloviev']",Reject,2018,"[1, 8]","[6, 11]","[13, 4]","[6, 3]","[6, 0]","[1, 1]","The key contributions of this paper are:
(a) proposes to reduce the vocabulary size in large sequence to sequence mapping tasks (e.g., translation) by first mapping them into a ""standard"" form and then into their correct morphological form,
(b) they achieve this by clever use of character LSTM encoder / decoder that sandwiches a bidirectional LSTM which captures context,
(c) they demonstrate clear and substantial performance gains on the OpenSubtitle task, and
(d) they demonstrate clear and substantial performance gains on a dialog question answer task.

Their analysis in Section 5.3 shows one clear advantage of this model in the context of long sequences. 

As an aside, the authors should correct the numbering of their Figures (there is no Figure 3) and provide better captions to the Tables so the results shown can easily understood at a glance. 

The only drawback of the paper is that this does not advance representation learning per se though a nice application of current models.","[6, 5, 2]","[' Marginally above acceptance threshold', ' Marginally below acceptance threshold', ' Strong rejection']","[5, 4, 5]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review starts by highlighting the key contributions of the paper, using positive language such as ""clever use"" and ""clear and substantial performance gains."" The reviewer acknowledges the advantage of the model. While pointing out a drawback, the reviewer uses a neutral tone and doesn't diminish the paper's value. The suggestions for improvement are concrete and constructive. Overall, the tone is balanced and professional.",75.0,75.0
Multiscale Hidden Markov Models For Covariance Prediction,"['João Sedoc', 'Jordan Rodu', 'Dean Foster', 'Lyle Ungar']",Reject,2018,"[3, 7, 0, 31]","[8, 12, 3, 36]","[70, 9, 7, 226]","[32, 4, 5, 138]","[36, 3, 2, 38]","[2, 2, 0, 50]","The paper focuses on a very particular HMM structure which involves multiple, independent HMMs. Each HMM emits an unobserved output with an explicit duration period. This explicit duration modelling captures multiple scale of temporal resolution. The actual observations are a weighted linear combination of the emissions from each latent HMM. The structure allows for fast inference using a spectral approach.

I found the paper unclear and lacking in detail in several key aspects:

1. It is unclear to me from Algorithm 2 how the weight vectors w are estimated. This is not adequately explained in the section on estimation.

2. The authors make the assumption that each HMM injects noise into the unobserved output which then gets propagated into the overall observation. What are reasons for his choice of model over a simpler model where the output of each HMM is uncorrupted?

3. The simulation example does not really demonstrate the ability of the MSHMM to do anything other than recover structure from data simulated under an MSHMM. It would be more interesting to apply to data simulated under non-Markovian or other setups that would enable richer frequency structures to be included and the ability of MSHMM to capture these.

4. The real data experiments shows some improvements in predictive accuracy with fast inference. However, the authors do not give a sufficiently broad exploration of the representations learnt by the model which allows us to understand the regimes in which the model would be advantageous.

Overall, the paper presents an interesting approach but the work lacks maturity. Furthermore, simulation and real data examples to explore the properties and utility of the method are required. 
","[5, 6, 6]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer acknowledges the interesting approach but expresses concerns about clarity, lack of detail, and limited experimental validation. The use of ""lacks maturity"" and the call for further exploration suggest a somewhat negative sentiment. However, the reviewer also points out the positive aspects, such as the interesting approach and fast inference, indicating a somewhat balanced perspective. The language used is professional and polite, providing constructive criticism without resorting to harsh or disrespectful language.",-20.0,70.0
Predicting Multiple Actions for Stochastic Continuous Control,"['Sanjeev Kumar', 'Christian Rupprecht', 'Federico Tombari', 'Gregory D. Hager']",Reject,2018,"['no_match', 'no_match', 15, 33]","['no_match', 'no_match', 20, 38]","['no match', 'no match', 347, 431]","['no match', 'no match', 162, 255]","['no match', 'no match', 129, 95]","['no match', 'no match', 56, 81]","This paper describes an approach to stochastic control using RL that extends DDPG with a stochastic policy.  A standard DDPG setup is extended such that the actor now produces M actions at each timestep.  Only one of the M actions will be executed in the environment using a uniform sampling.  The sampled action is the only that will receive a gradient update from the critic network. The authors demonstrate that such a stochastic policy performs better on average in a series of benchmark control tasks.

I find the general idea of the work compelling, but the particular approach is rather poor.  The fact that we are choosing the number of modes in the uniform distribution is a bit underwhelming (a more compelling architecture could have proposed a policy conditioned on gaussian noise for example, thus having better coverage of the distribution). I found the proposed apprach to be under-analyzed and the stochastic aspects of the policy are undervalued.   The main claim being argued in the paper is that the proposed stochastic policy has better final performance on average than a deterministic policy, but the only practical difference seems to be a slightly more structured approach to exploration.  
However, very little attention is paid to trying different exploration methods with the deterministic policy (incidentally, Ornstein-Uhlenbeck process noise is not something I'm familiar with, a citation to the use of this noise for exploration as well as a more explicit explanation would have been appreciated).  One interpretation is that each of the M sub-policies follows a different mode of the Q-value distribution over the action space.  But is this indeed the case?  There is a brief analysis of this with cartpole, but a more complete look at how actions are clustered in the action space would make this paper much more compelling.  Even in higher-dimensional action spaces, you could look at a t-SNE projection or cluster analysis to try and see how many modes the agent is reasoning over.  Additionally, the baseline agent should have used additional exploration methods as these can quickly change the performance of the agent.

I also think that better learning is not the only redeeming aspect of a stochastic policy.  In the face of a non-stationary environment, a stochastic policy will likely be much more robust.  Additionally, it will have much better performance against adversarial environments.  Given the remaining available space in the paper it would have been interesting to provide more insight into the proposed methods gains in these areas.","[4, 7, 3]","[' Ok but not good enough - rejection', ' Good paper, accept', ' Clear rejection']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer finds the general idea compelling, but has significant concerns about the approach and analysis. They find the approach ""rather poor"" and ""under-analyzed."" They also point out several areas where the paper could be improved, such as exploring different exploration methods and analyzing the robustness of the stochastic policy. While they see potential in the idea, the criticisms are substantial and suggest the paper needs significant revisions. Therefore, the sentiment is somewhat negative. The language used is critical but professional and not rude.",-30.0,60.0
Lung Tumor Location and Identification with AlexNet and a Custom CNN,"['Allison M Rossetto', 'Wenjin Zhou']",Reject,2018,"[2, 13]","[4, 16]","[5, 11]","[5, 10]","[0, 0]","[0, 1]","This paper compares 2 CNN architectures (Alexnet and a VGG variant) for the task of classifying images of lung cancer from CT scans. The comparison is trivial and does not go in depth to explain why one architecture works better than the other. Also, no effort is made to explain the data beyond some superficial description. No example of input data is given (what does an actual input look like). The authors mention ""the RCNN object detector"" in step 18, that presumably does post-processing after the CNN. But there is no explanation of that module anywhere. Instead the authors spend most of the paper listing in wordy details the architecture of their VGG variant. Also, a full page is devoted to detailed explanation of what precision-recall and Matthews Correlation Coefficient is! Overall, the paper does not provide any insight beyond: i tried this, i tried that and this works better than that; a strong reject.","[2, 3, 3]","[' Strong rejection', ' Clear rejection', ' Clear rejection']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is very negative about the paper, using terms like ""trivial"", ""superficial"", and ""strong reject"". It also criticizes the authors for including unnecessary details and failing to provide insights.  The language is quite direct and critical, but it doesn't use personal attacks or overly inflammatory language.",-80.0,-20.0
Large Scale Multi-Domain Multi-Task Learning with MultiModel,"['Lukasz Kaiser', 'Aidan N. Gomez', 'Noam Shazeer', 'Ashish Vaswani', 'Niki Parmar', 'Llion Jones', 'Jakob Uszkoreit']",Reject,2018,"[14, 2, 9, 13, 2, 3, 11]","[18, 6, 13, 17, 6, 8, 15]","[84, 29, 62, 54, 30, 24, 55]","[44, 9, 23, 32, 12, 9, 30]","[32, 18, 36, 20, 17, 12, 23]","[8, 2, 3, 2, 1, 3, 2]","The paper presents a multi-task, multi-domain model based on deep neural networks. The proposed model is able to take inputs from various domains (image, text, speech) and solves multiple tasks, such as image captioning, machine translation or speech recognition. The proposed model is composed of several features learning blocks (one for each input type) and of an encoder and an auto-regressive decoder, which are domain-agnostic. The model is evaluated on 8 different tasks and is compared with a model trained separately on each task, showing improvements on each task.

The paper is well written and easy to follow.

The contributions of the paper are novel and significant. The approach of having one model able to perform well on completely different tasks and type of input is very interesting and inspiring. The experiments clearly show the viability of the approach and give interesting insights. This is surely an important step towards more general deep learning models. 

Comments:

* In the introduction where the 8 databases are presented, the tasks should also be explained clearly, as several domains are involved and the reader might not be familiar with the task linked to each database. Moreover, some databases could be used for different tasks, such as WSJ or ImageNet.

* The training procedure of the model is not explained in the paper. What is the cost function and what is the strategy to train on multiple tasks ? The paper should at least outline the strategy.

* The experiments are sufficient to demonstrate the viability of the approach, but the experimental setup is not clear. Specifically, there is an issue about the speech recognition part of the experiment. It is not clear what the task exactly is: continuous speech recognition, isolated word recognition ? The metrics used in Table 1 are also not clear, they should be explained in the text. Also, if the task is continuous speech recognition, the WER (word error rate) metric should be used. Information about the detailed setup is also lacking, specifically which test and development sets are used (the WSJ corpus has several sets).

* Using raw waveforms as audio modality is very interesting, but this approach is not standard for speech recognition, some references should be provided, such as:
P. Golik, Z. Tuske, R. Schluter, H. Ney, Convolutional Neural Networks for Acoustic Modeling of Raw Time Signal in LVCSR, in: Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH), 2015, pp. 26–30.
D. Palaz, M. Magimai Doss and R. Collobert, (2015, April). Convolutional neural networks-based continuous speech recognition using raw speech signal. In Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on (pp. 4295-4299). IEEE.
T. N. Sainath, R. J. Weiss, A. Senior, K. W. Wilson, and O. Vinyals. Learning the Speech Front-end With Raw Waveform CLDNNs. Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH), 2015.

Revised Review:
The main idea of the paper is very interesting and the work presented is impressive. However, I tend to agree with Reviewer2, as a more comprehensive analysis should be presented to show that the network is not simply multiplexing tasks. The experiments are interesting, except for the WSJ speech task, which is almost meaningless. Indeed, it is not clear what the network has learned given the metrics presented, as the WER on WSJ should be around 5% for speech recognition.
I thus suggest to either drop the speech experiment, or the modify the network to do continuous speech recognition. A simpler speech task such as Keyword Spotting could also be investigated.
","[6, 3, 6]","[' Marginally above acceptance threshold', ' Clear rejection', ' Marginally above acceptance threshold']","[3, 5, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with positive statements, highlighting novelty and significance. However, it raises several important concerns about clarity, methodology, and results, particularly regarding the speech recognition experiment. The reviewer suggests dropping or significantly modifying this part. The tone is direct and professional, suggesting improvements without being overly negative.",50.0,70.0
Representing dynamically: An active process for describing sequential data,"['Juan Sebastian Olier', 'Emilia Barakova', 'Matthias Rauterberg', 'Carlo Regazzoni']",Reject,2018,"[4, 25, 30, 31]","[3, 30, 35, 36]","[5, 147, 230, 332]","[4, 96, 176, 219]","[0, 5, 3, 20]","[1, 46, 51, 93]","The paper proposes a hierarchical probabilistic model that learns both static representations and the dynamics of the data. The model iteratively updates its latent representations of the data in order to improve its generative power. The model is applicable to both static images (iterative improvements of the samples) and videos (predictive coding like repesentations).

Pros:

-- the motivation for the work and its connections to the cognitive/philosophical models of concepts and predictive coding is very interesting
-- the iterative improvements in the celebA samples in Fig. 3 and the corresponding improvements in the log-likelihood in Tbl. 1 vs the vanilla VAE baseline are promising and suggest that the approach has potential

Cons:

-- The major problem with this paper in my opinion is that the methods section is very confusing:
    1) The section is too brief and there is absolutely no description of the model until page 4
    2) The figures are hard to understand and the annotations are not informative (e.g. what is the difference between Fig.1 and Fig.2?) 
    3) The notation is unconventional and keeps changing (e.g. the generator is referred to as either \varphi_A, \varphi_X,  \varphi_X(Z_t), X_t|Z_t, or \mu_X; \sigma_X... the dimensionality of the image is denoted as i, N * c or Nc... I can go on).  
    4) The rescaling of the latent parameters seems engineered and arbitrary (e.g. \beta scaling factor in Eq. 8 is chosen so that the sigmoid reaches 0.75 when the value is 0.5\sigma of the threshold).

Due to the points above I failed to fully understand the model despite trying hard to do so. In particular, I did not understand the most important part of the paper addressing the iterative update of the latents vs backprop update of the generative weights. 

Minor points:
-- The introduction is too long and repetitive. The space saved should be used to describe the model more precisely.
-- The parametrisation of S_t should be described when it is first introduced, not 2 paragraphs later.
-- How does an inner product of two vectors result in a matrix (Sec. 3.3)?
-- GANs also do not have an encoder network (despite what the authors claim in Sec. 4.1) and should be used as a baseline
-- Why does the VAE baseline have a different decoder architecture than the proposed model?
-- What is the pre-processing done for CelebA?
-- What is the ground truth that was supposed to be matched by \mu_S_t in the dynamic dataset?
-- Figs. 4-5 are hard to understand. What do the different colours of the lines mean? The time stamps where the behaviours are changing should be marked in the plot (not just described in the text).

To conclude, the authors are advised to shorten the introduction and literature review sections and use the extra space to re-write and expand the methods section to make it very clear how their model works using the standard notation used in the literature. The results section detailing the dynamic setup of their approach needs to be made more clear as well. In the current form the paper is not ready for publication.






","[4, 3, 4, 6]","[' Ok but not good enough - rejection', ' Clear rejection', ' Ok but not good enough - rejection', ' Marginally above acceptance threshold']","[4, 3, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer acknowledges the interesting aspects of the work, particularly its motivation and potential demonstrated in image improvement. However, they express significant concerns about the clarity and presentation of the methods section, stating it's confusing and difficult to understand. The reviewer points out unconventional notation, lack of description, and arbitrary choices as reasons for this confusion. They also mention needing clarification in the results section related to the dynamic setup. While not explicitly negative, the numerous issues raised and the final statement about the paper not being ready for publication suggest a significantly negative sentiment. The language used is direct and critical but maintains a professional and constructive tone, suggesting a neutral politeness level.",-50.0,0.0
"Universality, Robustness, and Detectability of Adversarial Perturbations under Adversarial Training",['Jan Hendrik Metzen'],Reject,2018,[14],[19],[70],[33],[25],[12],"This paper investigates the effect of adversarial training. Based on experiments using CIFAR10, the authors show that adversarial training is effective in protecting against ""shared"" adversarial perturbation, in particular against universal perturbation. In contrast, it is less effective to protect against singular perturbations. Then they show that singular perturbation are less robust to image transformation, meaning after image transformation those perturbations are no longer effective. Finally, they show that singular perturbations can be easily detected.

I like the message conveyed in this paper. However, as the statements are mostly backed by experiments, then I think it makes sense to ask how statistically significant the present results are. Moreover, is CIFAR 10 experiments conclusive enough. ","[6, 3, 6]","[' Marginally above acceptance threshold', ' Clear rejection', ' Marginally above acceptance threshold']","[3, 4, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer explicitly states they ""like the message conveyed in this paper"", which points towards a positive sentiment. However, they also raise valid questions and ask for further investigation, indicating that the paper is not perfect. Therefore, the sentiment is positive but not overly enthusiastic. The language used is constructive, professional, and respectful, suggesting a polite tone.",60.0,80.0
Generative Entity Networks: Disentangling Entitites and Attributes in Visual Scenes using Partial Natural Language Descriptions,"['Charlie Nash', 'Sebastian Nowozin', 'Nate Kushman']",Reject,2018,"[3, 12, 12]","[8, 17, 16]","[19, 121, 33]","[7, 69, 19]","[10, 42, 13]","[2, 10, 1]","**Summary**
The paper proposes an extension of the attend, infer, repeat generative model of Eslami, 2016 and extends it to handle ``visual attribute descriptions. This straightforward extension is claimed to improve image quality and shown to improve performance on a previously introduced image caption ranking task. In general, the paper shows improvements on an image caption agreement task introduced in Kuhnle and Copestake, 2017.  The paper seems to have weaknesses pertaining to the approach taken, clarity of presentation and comparison to baselines which mean that the paper does not seem to meet the acceptance threshold for ICLR. See more detailed points below in Weaknesses.

**Strengths**
I like the high-level motivation of the work, that one needs to understand and establish that language or semantics can help learn better representations for images. I buy the premise and think the work addresses an important issue. 

**Weakness**

Approach:
* A major limitation of the model seems to be that one needs access to both images and attribute vectors at inference time to compute representations which is a highly restrictive assumption (since inference networks are discriminative). The paper should explain how/if one can compute representations given just the image, for instance, say by not using amortized inference. The paper does propose to use an image-only encoder but that is intended in general as a modeling choice to explain statistics which are not captured by the attributes (in this case location and orientation as explained in the Introduction of the paper).

Clarity:
* Eqn. 5, LHS can be written more clearly as \hat{a}_k. 

* It would also be good to cite the following related work, which closely ties into the model of Eslami 2016, and is prior work: 

Efficient inference in occlusion-aware generative models of images,
Jonathan Huang, Kevin Murphy.
ICLR Workshops, 2016

* It would be good to clarify that the paper is focusing on the image caption agreement task from Kuhnle and Copestake, as opposed to generic visual question answering.

* The claim that the paper works with natural language should be toned down and clarified. This is not natural language, firstly because the language in the dataset is synthetically generated and not “natural”. Secondly, the approach parses this “synthetic” language into structured tuples which makes it even less natural. Also, Page. 3. What does “partial descriptions” mean?

* Section 3: It would be good to explicitly draw out the graphical model for the proposed approach and clarify how it differs from prior work (Eslami, 2016).

* Sec. 3. 4 mentions that the “only image” encoder is used to obtain the representation for the image, but the “only image” encoder is expected to capture the “indescribable component” from the image, then how is the attribute information from the image captured in this framework? One cannot hope to do image caption association prediction without capturing the image attributes...

*, In general, the writing and presentation of the model seem highly fragmented, and it is not clear what the specifics of the overall model are. For instance, in the decoder, the paper mentions for the first time that there are variables “z”, but does not mention in the encoder how the variables “z” were obtained in the first place (Sec. 3.1). For instance, it is also not clear if the paper is modeling variable length sequences in a similar manner to Eslami, 2016 or not, and if this work also has a latent variable [z, z_pres] at every timestep which is used in a similar manner to Eqn. 2 in Eslami, 2016. Sec. 3.4 “GEN Image Encoder” has some typo, it is not clear what the conditioning is within q(z) term.

* Comparison to baselines: 
  1. How well does this model do against a baseline discriminative image caption ranking approach, similar to [D]? This seems like an important baseline to report for the image caption ranking task.
  2. Another crucial baseline is to train the Attend, Infer, Repeat model on the ShapeWorld images, and then take the latent state inferred at every step by that model, and use those features instead of the features described in Sec. 3.4 “Gen Image Encoder” and repeat the rest of the proposed pipeline. Does the proposed approach still show gains over Attend Infer Repeat?
  3. The results shown in Fig. 7 are surprising -- in general, it does not seem like a regular VAE would do so poorly. Are the number of parameters in the proposed approach and the baseline VAE similar? Are the choices of decoder etc. similar? Did the model used for drawing Fig. 7 converge? Would be good to provide its training curve. Also, it would be good to evaluate the AIR model from Eslami, 2016 on the same simple shapes dataset and show unconditional samples. If the claim from the work is true, that model should be just as bad as a regular VAE and would clearly establish that using language is helping get better image samples.

* Page 2: In general the notion of separating the latent space into content and style, where we have labels for the “content” is an old idea that has appeared in the literature and should be cited accordingly. See [B] for an earlier treatment, and an extension by [A]. See also the Bivcca-private model of [C] which has “private” latent variables for vision similar to this work (this is relevant to Sec. 3.2.)

References:
[A]: Upchurch, Paul, Noah Snavely, and Kavita Bala. 2016. “From A to Z: Supervised Transfer of Style and Content Using Deep Neural Network Generators.” arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1603.02003.

[B]: Kingma, Diederik P., Danilo J. Rezende, Shakir Mohamed, and Max Welling. 2014. “Semi-Supervised Learning with Deep Generative Models.” arXiv [cs.LG]. arXiv. http://arxiv.org/abs/1406.5298.

[C]: Wang, Weiran, Xinchen Yan, Honglak Lee, and Karen Livescu. 2016. “Deep Variational Canonical Correlation Analysis.” arXiv [cs.LG]. arXiv. http://arxiv.org/abs/1610.03454.

[D]: Kiros, Ryan, Ruslan Salakhutdinov, and Richard S. Zemel. 2014. “Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models.” arXiv [cs.LG]. arXiv. http://arxiv.org/abs/1411.2539.
","[5, 5, 4]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Ok but not good enough - rejection']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with a polite but negative sentiment, highlighting weaknesses in the paper's approach, clarity, and comparison to baselines. While acknowledging the good motivation, the reviewer lists several major concerns, suggesting the paper is not ready for acceptance. The language used is formal, professional, and typical of academic peer reviews. There's no personal criticism or disrespectful tone.",-25.0,75.0
Zero-shot Cross Language Text Classification,"['Dan Svenstrup', 'Jonas Meinertz Hansen', 'Ole Winther']",Reject,2018,"[2, 3, 26]","[1, 2, 31]","[2, 3, 140]","[1, 1, 53]","[1, 2, 46]","[0, 0, 41]","This paper proposes a language independent text encoding method for cross-language classification. The proposed approach demonstrates better performance than machine translation based classifier. 

The proposed approach  performs language independent common representation learning for cross-lingual text classification. Such representation learning based methods have been studied in the literature. The authors should provide a review and comparison to related methods. 

Technical contribution of the paper is very limited. The approach section is too short to provide a clear presentation of the model. Some descriptions about the input text representation are actually given in the experimental section. 

The proposed approach uses comparable texts across different languages to train the encoders, while using the topic information as auxiliary supervision label information. In the experiments, it shows the topics are actually fine-grained class information that are closely related to the target class categories. This makes the zero-shot learning scenario not to be very practical. With such fine-grained supervision knowledge, it is also unfair to compare to other cross-lingual methods that use much less auxiliary information. 

In the experiments, it states the data are collected by “For pages with multiple categories we select one at random”.  Won’t this produce false negative labels on the constructed data? How much will this affect the test performance?

The experimental results are not very convincing without empirical comparisons to the state-of-the-art cross-lingual text classification methods. ","[4, 3, 2]","[' Ok but not good enough - rejection', ' Clear rejection', ' Strong rejection']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with a mildly positive statement acknowledging the paper's aim and a potential advantage. However, the majority of the review points out significant shortcomings in terms of technical contribution, clarity, experimental setup, and lack of comparison with existing work. The reviewer raises valid concerns and questions the paper's claims. Overall, the sentiment leans towards the negative due to the seriousness of the issues raised.",-40.0,20.0
BLOCK-NORMALIZED GRADIENT METHOD: AN EMPIRICAL STUDY FOR TRAINING DEEP NEURAL NETWORK,"['Adams Wei Yu', 'Lei Huang', 'Qihang Lin', 'Ruslan Salakhutdinov', 'Jaime Carbonell']",Reject,2018,"[8, 7, 9, 16, 12]","[13, 12, 14, 21, 1]","[41, 62, 75, 419, 1]","[17, 29, 26, 207, 1]","[20, 23, 29, 201, 0]","[4, 10, 20, 11, 0]","This paper illustrates the benefits of using normalized gradients when training deep models.
Beyond exploring the ""vanilla"" normalized gradient algorithm they also consider adaptive versions, i.e., methods that employ per block (adaptive) learning rates using ideas from AdaGrad and Adam.
Finally, the authors provide a theoretical analysis of NG with adaptive step-size, showing convergence guarantees in the stochastic convex optimization setting.

I find this paper both very interesting and important. 
The normalized gradient method was previously shown to overcome some non-convex phenomena which are hurdles to SGD, yet there was still the gap of  combining NG with methods which automatically tune the learning rate.

The current paper addresses this gap by a very simple (yet clever) combination of NG with AdaGrad and Adam, and the authors do a great job by illustrating the benefits of their scheme by testing it over a very wide span of deep learning 
models. In light of their experiments it seems like AdamNG and NG should be adopted as the new state-of-the-art methods in deep-learning applications.

Additional comments:
-In the experiments the authors use the same parameters as is used by Adam/AdaGrad, etc..
Did the authors also try to fine tune the parameters of their NG versions? If so what is the benefit that they get by doing so?
-It will be useful if the authors can provide some intuition about why is the learning rate  chosen per block for NG?
Did the authors also try to choose a learning rate per weight vector rather than per block? If so, what is the behaviour that they see.
-I find the theoretical analysis a bit incomplete. The authors should spell out the choice of the learning rate in Thm. 1 and compare to AdaGrad.
","[9, 4, 2]","[' Top 15% of accepted papers, strong accept', ' Ok but not good enough - rejection', ' Strong rejection']","[5, 5, 5]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The reviewer clearly states that they find the paper ""very interesting and important"", highlighting the significance of the research. They praise the authors for addressing a gap in the field and commend the extensive experiments conducted. While they offer constructive criticism and suggestions for improvement, these are presented in a positive and encouraging manner. The reviewer even suggests that the proposed methods ""should be adopted as the new state-of-the-art"", indicating a strong positive sentiment overall.",90.0,90.0
Neural Networks for irregularly observed continuous-time Stochastic Processes,"['Francois W. Belletti', 'Alexander Ku', 'Joseph E. Gonzalez']",Reject,2018,"[4, 1, 10]","[8, 6, 15]","[23, 19, 265]","[11, 6, 109]","[11, 12, 138]","[1, 1, 18]","The authors proved that convolutional neural networks with Leaky ReLU activation function are nonlinear frames, and similar results hold for non-uniformly sampled time-series as well. My main concern on this part is that theory is too rough and its link to the later part of the paper is weak. Although frames are stable representations, the ones with lower bound much smaller than the upper bound are close to unstable. That's why in classical applications of frames in signal and image processing tight frames are vastly preferred. Furthermore, the authors did not explicitly state the reliance of the lower frame bound on the parameter alpha in Leaky ReLU. It seems to me that the representation gets more unstable as alpha decreases, and the lower bound will be zero when ReLU is used. 

In Section 2, the authors used CMF conditions to constraint filters which leads to a much more stable representation than in the previous section. The idea is very similar to previous work on data-driven tight frame (Cai et al. Applied & Computational Harmonic Analysis, vol. 37, no. 1, p. 89-105, 2014) and AdaFrame (Tai and E, arXiv:1507.04835). Furthermore, there has been multiple work on introducing tight-frame-like constraints to filters of convolutional neural networks (see for example Huang et al., arXiv:1710.02338). All of these work is not mentioned by the authors. Although the CMF constraints used by the authors seem new, the overall novelty is still weak in my opinion.

The experimental results are convincing, and the proposed architecture with wavelet-transform LSTM outperform the baseline model using standard LSTM. However, I am not familiar with the state-of-the-art models on the data sets used in the paper. Therefore, I am not certain whether the proposed method achieves state-of-the-art or not. ","[5, 5, 2]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Strong rejection']","[5, 4, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review is critical of the paper, pointing out several weaknesses such as the roughness of the theory, lack of novelty, and limited comparison with state-of-the-art methods. However, it does acknowledge the convincing experimental results. The language used is professional and not disrespectful, indicating a neutral rather than rude tone.",-20.0,50.0
Classifier-to-Generator Attack: Estimation of Training Data Distribution from Classifier,"['Kosuke Kusano', 'Jun Sakuma']",Reject,2018,"[2, 19]","[1, 24]","[1, 135]","[1, 85]","[0, 34]","[0, 16]","This paper proposed to learn a generative GAN model that generates the training data from the labels, given that only the black-box mapping $f$ from data to label is available, as well as an aux dataset that might and might not overlap with the training set. This approach can be regarded as a transfer learning version of ACGAN that generates data conditioned on its label.

Overall I feel it unclear to judge whether this paper has made substantial contributions. The performance critically relies on the structure of aux dataset and how the supervised model $f$ interacts with it. It would be great if the author could show how the aux dataset is partitioned according to the function $f, and what is the representative sample from aux dataset that maximizes a given class label. In Fig. 4, the face of Leonardo DiCaprio was reconstructed successfully, but is that because in the aux dataset there are other identities who look very similar to him and is classified as Leonardo, or it is because GAN has the magic to stitch characteristics of different face identities together?  Given the current version of the paper, it is not clear at all. From the results on EMNIST when the aux set and the training set are disjoint, the proposed model simply picks the most similar shapes as GAN generation, and is not that interesting. In summary, a lot of ablation experiments are needed for readers to understand the proposed method better.

The writing is ok but a bit redundant. For example, Eqn. 1 (and Eqn. 2) which shows the overall distribution of the training samples (and aux samples) as a linear combinations of the samples at each class, are not involved in the method. Do we really need Eqn. 1 and 2?","[4, 7, 4]","[' Ok but not good enough - rejection', ' Good paper, accept', ' Ok but not good enough - rejection']","[3, 3, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer expresses uncertainty about the paper's contributions, using phrases like ""it is unclear"" and questioning the novelty of the results. They find the results interesting only in specific cases and call for more experiments to understand the method better. This suggests a slightly negative sentiment. The reviewer does point out positive aspects like the writing being 'ok,' but the overall tone leans towards the critical side. The language used is professional and avoids harsh or disrespectful language, indicating a neutral to polite tone.",-20.0,50.0
AANN: Absolute Artificial Neural Network,['Animesh Karnewar'],Reject,2018,[2],[7],[10],[5],[5],[0],"This paper introduces a reversible network with absolute value used as
the activation function.  The network is run in the forward direction
to classify and in the reverse direction to generate.

The key points of the network are the use of the absolute value
activation function and the use of (free) normalization to match
target output. This allows the network to perfectly map inputs to any
point on a vector that goes through the one-hot encoding, allowing for
deterministic generation from different vectors (of different lengths)
with the same normalized output.

I think there are a lot of novel and interesting ideas in this paper
though they have not been fully explored.  The use of the absolute
value transfer function is new to me, though I was able to find a couple of old
references to its use.   In a paper by Gad et al. (2000), it is stated 
"" For example, the algorithm presented in Lin and
Unbehauen (1995) < I think they mean Lin and Unbehauen 1990)> 
 is used to train networks with a single hidden layer
employing the absolute value as the activation function of the hidden
neuron. This algorithm was further generalized to multilayer networks
with cascaded structures in Batruni (1991).""   Exploring the properties 
of the abs activation function seems worth exploring.

More details on the training are needed for full clarity in the paper.
(Though it is recognized that some of these could be determined from
links when made active, they should be included in the paper).  How
did you select the training parameters given at the bottom of page 5?
How many layers and units/layer did you use? And how were these
selected?  (The use of the links for providing code and visualizations (when active)
 is a nice feature of this paper).

Also, did you compare to using the leaky ReLU activation function --
That would be interesting as it also doesn't have any areas of zero
slope?  Did you compare the generated digits to those obtained using GANs?

I am also curious, how does accuracy on digit classification differ
when trained only to optimize the forward error?

The MNIST site referenced lists 60,000 training data and test data of
10,000.  How/why did you select 42,000 and then split it to 39900 in
the train set and 2100 in the dev set?

Also, the goal for the paper is presented as creating highly
interpretable representations of the input data.  My interpretation of
interpretable is that the hidden units are ""interpretable"" and that it
is clear how the combined hidden unit representations allow for
accurate classification.  Towards that end, it would be nice to see
some of the interpretations of the hidden unit representations.  In
the abstract it states "" ...These representations are generated by
penalizing the learning of the network in such a way that those
learned representations correspond to the respective labels present in
the labelled dataset used for supervised training"".  Does this
statement refer only to the encoding of the representation vector or
also the hidden layers?  If the former, isn't that true for all
supervised algorithms.  If the latter, you should show this.

Batruni, R. (1991). A multilayer neural network with piecewise-linear
structure and backpropagation learning. IEEE Transactions on Neural
Networks, 2, 395–403.

Lin, J.-N., & Unbehauen, R. (1995). Canonical piecewise-linear neural
networks. IEEE Transactions on Neural Networks, 6, 43–50.

Lin, J.-N, & Unbehauen, R. (1990). Adaptive Nonlinear Digital Filter with Canonical Piecewise-Linear Structure,
IEEE Transactions on Circuits and Systems, 37(3) 347-353.

Gad, E.F et al (2000). A new algorithm for learning in piecewise-linear neural networks.
Neural Networks 13,  485-505.
","[6, 3, 2]","[' Marginally above acceptance threshold', ' Clear rejection', ' Strong rejection']","[4, 5, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer finds the paper's ideas novel and interesting, using phrases like ""novel and interesting ideas"" and ""nice feature"". They also provide constructive criticism and suggestions for improvement, indicating a positive attitude towards the paper's potential. However, they also point out areas that need further exploration and clarification. Therefore, the sentiment is positive but not overly enthusiastic, leading to a score closer to moderately positive. The language is polite and professional throughout, with constructive criticism offered respectfully.",60.0,80.0
A Deep Predictive Coding Network for Learning Latent Representations,"['Shirin Dora', 'Cyriel Pennartz', 'Sander Bohte']",Reject,2018,"[6, 9, 19]","[11, 12, 24]","[25, 15, 85]","[11, 3, 37]","[4, 1, 23]","[10, 11, 25]","The paper attempts to extend the predictive coding model to a multilayer network.  The math is developed for a learning rule, and some demonstrations are shown for reconstructions of CIFAR-10 images.

The overall idea and approach being pursued here is a good one, but the model needs further development.  It could also use better theoretical motivation - i.e., what sorts of representations do you expect to emerge in higher layers?  Can you demonstrate this with a toy example and then extend to real data?

That the model can reconstruct images per se is not particularly interesting.  What we would like to see is that it has somehow learned a more useful or meaningful representation of the data.  For example, what do the learned weights look like?  That would tell you something about what has been learned.
","[3, 3, 4]","[' Clear rejection', ' Clear rejection', ' Ok but not good enough - rejection']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer provides a mixed opinion, acknowledging the merit of the idea but outlining significant areas for improvement. Phrases like ""good one, but"" and ""not particularly interesting"" indicate constructive criticism rather than outright negativity. The language remains professional and suggestive, focusing on improvements without resorting to harsh or disrespectful tones.",40.0,70.0
Softmax Supervision with Isotropic Normalization,"['Yue Zhao', 'Deli Zhao', 'Shaohua Wan', 'Bo Zhang']",Reject,2018,"['no_match', 15, 12, 13]","['no_match', 20, 17, 18]","['no match', 63, 35, 101]","['no match', 29, 11, 45]","['no match', 29, 2, 0]","['no match', 5, 22, 56]","In the centre loss, the centre is learned. Now it's calculated as the average of the last layer's features
To enable training with SGD, the authors calculate the centre within a mini batch","[4, 6, 4]","[' Ok but not good enough - rejection', ' Marginally above acceptance threshold', ' Ok but not good enough - rejection']","[5, 3, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is very short and doesn't offer explicit praise or criticism. It focuses solely on summarizing the paper's content related to a specific technical detail (centre loss calculation). Therefore, the sentiment is neutral. The language used is purely descriptive and objective, indicating a neutral politeness level.",0.0,0.0
Towards a Testable Notion of Generalization for Generative Adversarial Networks,"['Robert Cornish', 'Hongseok Yang', 'Frank Wood']",Reject,2018,"[2, 21, 23]","[6, 26, 28]","[10, 140, 153]","[5, 83, 71]","[5, 31, 71]","[0, 26, 11]","`The papers aims to provide a quality measure/test for GANs.   The objective is ambitious an deserve attention. As GANs are minimizing some f-divergence measure, the papers remarks that computing a  Wasserstein distance between two distributions made of a sum of Diracs is not a degenerate case and is tractable. So they propose evaluate the current approximation of a distribution learnt by a GAN by using this distance as a baseline performance (in terms of W distance and computed on a hold out dataset). 

A first remark is that the papers does not clearly develop the interest of puting things a trying to reach a treshold of performance in W distance rather than just trying to minimize the desired f-divergence. More specifically as they assess the performance in terms of W distance I would would be tempted to just minimize the given criterion. This would be very interesting to have arguments on why being better than the ""Dirac estimation"" in terms of W2 distance would lead to better performance for others tasks (as other f-divergences or image generation).

According to the authors the core claims are:
""1/ We suggest a formalisation of the goal of GAN training (/generative modelling more broadly) in terms of divergence minimisation. This leads to a natural, testable notion of generalisation. ""
Formalization in terms of divergence minimization is not new (see O. Bousquet & all https://arxiv.org/pdf/1701.02386.pdf ) and I do not feel like this paper actually performs any ""test"" (in a statistical sense). In my opinion the contribution is more about exhibiting a baseline which has to be defeated for any algorithm interesting is learning the distribution in terms of W2 distance.

""2/ We use this test to evaluate the success of GAN algorithms empirically, with the Wasserstein distance as our divergence.""
Here the distance does not seems so good because the performance in generation does not seems to only be related to W2 distance. Nevertheless, there is interesting observations in the paper about the sensitivity of this metric to the bluring of pictures. I would enjoyed more digging in this direction. The authors proposes to solve this issue by relying to an embedded space where the L2 distance makes more sense for pictures (DenseNet). This is of course very reasonable but I would expect anyone working on distribution over picture to work with such embeddings. Here I'm not sure if this papers opens a new way to improve the embedding making use on non labelled data. One could think about allowing the weights of the embeddings to vary while f-divergence is minimized but this is not done in the submitted work.

 ""3/ We find that whether our proposed test matches our intuitive sense of GAN quality depends heavily on the ground metric used for the Wasserstein distance.""
This claim is highly biased by who is giving the ""intuitive sense"". It would be much better evaluated thought a mechanical turk test.

 ""4/ We discuss how to use these insights to improve the design of WGANs more generally.""
As our understanding of the GANs dynamics are very coarse, I feel this is not a good thing to claim that ""doing xxx should improve things"" without actually trying it. 
","[5, 6, 4]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Ok but not good enough - rejection']","[3, 3, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is quite mixed. It acknowledges the paper's ambition and some interesting observations (e.g., sensitivity to blurring). However, it also raises several concerns, questioning the novelty and significance of the contributions. The reviewer finds some claims not well-supported and suggests alternative approaches. The language is formal and academic, directly addressing the paper's claims and methodology. While critical, it maintains a professional tone without resorting to personal attacks or overly negative phrasing.",-10.0,60.0
Gating out sensory noise in a spike-based Long Short-Term Memory network,"['Davide Zambrano', 'Isabella Pozzi', 'Roeland Nusselder', 'Sander Bohte']",Reject,2018,"[10, 1, 2, 19]","[15, 3, 3, 24]","[23, 3, 4, 85]","[11, 2, 2, 37]","[7, 1, 2, 23]","[5, 0, 0, 25]","The authors propose a first implementation of spiking LSTMs. This is an interesting and open problem. However, the present work somewhat incomplete, and requires further experiments and clarifications.

Pros:
1. To my best knowledge, this is the first mapping of LSTMs to spiking networks
2. The authors tackle an interesting and challenging problem.

Cons:
1. In the abstract the authors mention that another approach has been taken, but is never stated what’s the problem that this new one is trying to address. Also, H&S 1997 tested several tasks, which is the one that the authors are referring to?
2. Figure 1 is not very easy to read. The authors can spell out the labels of the axis (e.g. S could be input, S)
3. Why are output and forget gates not considered here?
4. A major point in mapping LSTMs to spiking networks is its biological plausibility. However, the authors do not seem to explore this. Of particular interest is its relationship to a recent proposal of a cortical implementation of LSTMs (Cortical microcircuits as gated-RNNs, NIPS 2017).
5. The text should be improved, for example in the abstract: “that almost all resulting spiking neural network equivalents correctly..”, please rephrase.
6. Current LSTMs are applied in much more challenging problems than the original ones. It would be important to test one of this, perhaps the relatively simple pixel-by-pixel MNIST task. If this is not feasible, please comment.

Minor comments:
1. Change in the abstract “can be substituted for” > “can be substituted by”
2. A new body of research aims at using backprop in spiking RNNs (e.g. Friedemann and Ganguli 2017). The present work gets around this by training the analog version instead. It would be of interesting to discuss how to train spiking-LSTMs as this is an important topic for future research. 
3. As the main promise of using spiking nets (instead of rate) is their potential efficiency in neuromorphic systems, it would be interesting to contrast in the text the two options for LSTMs, and give some more quantitative analyses on the gain of spiking-LSTM versus rate-LSTMs in terms of efficiency.","[5, 5, 4]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Ok but not good enough - rejection']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review acknowledges the novelty of the work and its potential but points out several areas for improvement. It highlights missing information, questions the depth of analysis (biological plausibility, comparison with rate-based LSTMs), and suggests additional experiments. The language, while direct, maintains a professional and constructive tone.",20.0,70.0
Hyperedge2vec: Distributed Representations for Hyperedges,"['Ankit Sharma', 'Shafiq Joty', 'Himanshu Kharkwal', 'Jaideep Srivastava']",Reject,2018,"['no_match', -3, -1, 33]","['no_match', 2, 2, 38]","['no match', 8, 2, 311]","['no match', 0, 0, 201]","['no match', 8, 1, 33]","['no match', 0, 1, 77]","This paper studies the problem of representation learning in hyperedges.  The author claims their novelty for using several different models to build hyperedge representations.  To generate representations for hyperedge, this paper proposes to use several different models such as Denoising AutoEncoder, tensor decomposition, word2vec or spectral embeddings. Experimental results show the effectiveness of these models in several different datasets. 

The author uses several different models (both recent studies like Node2Vec / sen2vec, and older results like spectral or tensor decomposition). The idea of studying embedding of a hypergraph is interesting and novel, and the results show that several different kinds of methods can all provide meaningful results for realistic applications. 

Despite the novel idea about hyperedge embedding generation, the paper is not easy to follow. 
The introduction of ``hypergraph`` takes too much spapce in preliminary, while the problem for generating embeddings of hyperedge is the key of paper. 

Further, the experiments only present several models this paper described. 
Some recent papers about hypergraph and graph structure (even though cannot generate embeddings directly) are still worth mention and compare in the experimental section. It will be persuasive to mention related methods in similar tasks. 

it would better better if the author can add some related work about hyperedge graph studies. ","[5, 5, 5]","[' Marginally below acceptance threshold', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[3, 3, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review acknowledges the novelty and interesting aspects of the paper's idea (positive sentiment). However, it also points out areas for improvement in clarity, structure, and completeness, suggesting the paper isn't perfect (neutralizing the sentiment towards the middle). The language used is constructive and suggestive, not demanding or aggressive, indicating politeness. ",40.0,70.0
Few-Shot Learning with Simplex,"['Bowen Zhang', 'Xifan Zhang', 'Fan Cheng', 'Deli Zhao']",Reject,2018,"[22, 'no_match', 7, 'no_match']","[27, 'no_match', 12, 'no_match']",['skipped'],['skipped'],['skipped'],['skipped'],"This paper proposes a geometric based approach to solving the problem of one-shot and few-shot learning. The basic idea is to use the feature vectors of a particular class to construct a simplex. (I am assuming the dimensions of the vectors are selected so as to exactly construct a simplex? It is not clearly written in the paper). The volume of the simplex is then taken to be a measure of class scatter, and classification happens by assigning the test feature vector to the nearest simplex, where the distances are normalized by the volume of the simplex. 

While the approach makes sense, I am not convinced that this geometric method plays an important role in increasing the performance on one-shot/few-shot tasks. In particular, one could try simpler approaches like k-NN where the distances to the cluster centers are also normalized by the variance within the clusters. I would suspect that this method is not superior to this simpler baseline. 

The other issue I have with this paper is misleading claims about being state of the art on Omniglot. In particular see Kaiser et al (ICLR 2017), where on 5-way-1-shot an accuracy of 98.4% is reached compared to 94.6% in this paper, and on 5-way-5-shot an accuracy of 99.6% is reached compared to 99.1% in this work. The paper also misses evaluations on various other data sets such as GNMT etc., on which Kaiser et al evaluated their approach.","[4, 6, 4]","[' Ok but not good enough - rejection', ' Marginally above acceptance threshold', ' Ok but not good enough - rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']",The reviewer acknowledges the approach makes sense but expresses skepticism about its effectiveness and novelty. They find the claims about state-of-the-art results misleading and point out missing evaluations compared to existing work. The language is critical but remains professional and within the bounds of academic discourse.,-20.0,60.0
Long Term Memory Network for Combinatorial Optimization Problems,"['Hazem A. A. Nomer', 'Abdallah Aboutahoun', 'Ashraf Elsayed']",Reject,2018,"[-1, 13, 10]","[2, 17, 15]","[2, 3, 14]","[1, 0, 4]","[0, 0, 0]","[1, 3, 10]","# Summary
This paper proposes a neural network framework for solving binary linear programs (Binary LP). The idea is to present a sequence of input-output examples to the network and train the network to remember input-output examples to solve a new example (binary LP). In order to store such information, the paper proposes an external memory with non-differentiable reading/writing operations. This network is trained through supervised learning for the output and reinforcement learning for discrete operations. The results show that the proposed network outperforms the baseline (handcrafted) solver and the seq-to-seq network baseline.

[Pros]
- The idea of approximating a binary linear program solver using neural network is new.

[Cons]
- The paper is not clearly written (e.g., problem statement, notations, architecture description). So, it is hard to understand the core idea of this paper.
- The proposed method and problem setting are not well-justified. 
- The results are not very convincing.

# Novelty and Significance
- The problem considered in this paper is new, but it is unclear why the problem should be formulated in such a way. To my understanding, the network is given a set of input (problem) and output (solution) pairs and should predict the solution given a new problem. I do not see why this should be formulated as a ""sequential"" decision problem. Instead, we can just give access to all input/output examples (in a non-sequential way) and allow the network to predict the solution given the new input like Q&A tasks. This does not require any ""memory"" because all necessary information is available to the network.
- The proposed method seems to require a set of input/output examples even during evaluation (if my understanding is correct), which has limited practical applications. 

# Quality
- The proposed reward function for training the memory controller sounds a bit arbitrary. The entire problem is a supervised learning problem, and the memory controller is just a non-differentiable decision within the neural network. In this case, the reward function is usually defined as the sum of log-likelihood of the future predictions (see [Kelvin Xu et al.] for training hard-attention) because this matches the supervised learning objective. It would be good to justify (empirically) the proposed reward function. 
- The results are not fully-convincing. If my understanding is correct, the LTMN is trained to predict the baseline solver's output. But, the LTMN significantly outperforms the baseline solver even in the training set. Can you explain why this is possible?

# Clarity
- The problem statement and model description are not described well. 
1) Is the network given a sequence of program/solution input? If yes, is it given during evaluation as well?
2) Many notations are not formally defined. What is the output (o_t) of the network? Is it the optimal solution (x_t)? 
3) There is no mathematical definition of memory addressing mechanism used in this paper.
- The overall objective function is missing. 

[Reference]
- Kelvin Xu et al., Show, Attend and Tell: Neural Image Caption Generation with Visual Attention","[4, 3, 4]","[' Ok but not good enough - rejection', ' Clear rejection', ' Ok but not good enough - rejection']","[2, 4, 1]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is confident but not absolutely certain that the evaluation is correct', "" The reviewer's evaluation is an educated guess""]","The reviewer raises several serious concerns about the paper, questioning its clarity, justification, and results. They find the core idea hard to understand and disagree with the sequential framing of the problem. The reviewer also criticizes the reward function, lack of mathematical rigor, and questions the validity of the results surpassing the baseline even in training. The tone, while critical, maintains a professional and analytical approach without resorting to personal attacks.",-50.0,50.0
Learning to Treat Sepsis with Multi-Output Gaussian Process Deep Recurrent Q-Networks,"['Joseph Futoma', 'Anthony Lin', 'Mark Sendak', 'Armando Bedoya', 'Meredith Clement', ""Cara O'Brien"", 'Katherine Heller']",Reject,2018,"[7, 3, 2, 2, 2, 14]","[10, 8, 6, 2, 6, 19]","[19, 14, 5, 2, 6, 73]","[10, 6, 2, 1, 3, 45]","[7, 3, 1, 0, 1, 23]","[2, 5, 2, 1, 2, 5]","This paper presents an important application of modern deep reinforcement learning (RL) methods to learning optimal treatments for sepsis from past patient encounters. From a methods standpoint, it offers nothing new but does synthesize best practice deep RL methods with a differentiable multi-task Gaussian Process (GP) input layer. This means that the proposed architecture can directly handle irregular sampling and missing values without a separate resampling step and can be trained end-to-end to optimize reward -- patient survival -- without a separate ad hoc preprocessing step. The experiments are thorough and the results promising. Overall, strong application work, which I appreciate, but with several flaws that I'd like the authors to address, if possible, during the review period. I'm perfectly willing to raise my score at least one point if my major concerns are addressed.

QUALITY

Although the core idea is derivative, the work is executed pretty well. Pros (+) and cons (-) are listed below:

+ discussion of the sepsis application is very strong. I especially appreciated the qualitative analysis of the individual case shown in Figure 4. While only a single anecdote, it provides insight into how the model might yield clinical insights at the bedside.
+ thorough comparison of competing baselines and clear variants -- though it would be cool to apply offline policy evaluation (OPE) to some of the standard clinical approaches, e.g., EGDT, discussed in the introduction.

- ""uncertainty"" is one of the supposed benefits of the MTGP layer, but it was not at all clear how it was used in practice, other than -- perhaps -- as a regularizer during training, similar to data augmentation.
- uses offline policy evaluation ""off-the-shelf"" and does not address or speculate the potential pitfalls or dangers of doing so. See ""Note on Offline Policy Evaluation"" below.
- although I like the anecdote, it tells us very little about the overall policy. The authors might consider some coarse statistical analyses, similar to Figure 3 in Raghu, et al. (though I'm sure you can come up with more and better analyses!). 
- there are some interesting patterns in Table 1 that the authors do not discuss, such as the fact that adding the MGP layer appears to reduce expected mortality more (on average) than adding recurrences. Why might this be (my guess is data augmentation)?

CLARITY

Paper is well-written, for the most part. I have some nitpicks about the writing, but in general, it's not a burden to read.

+ core ideas and challenges of the application are communicated clearly

- the authors did not detail how they chose their hyperparameters (number of layers, size of layers, whether to use dropout, etc.). This is critical for fully assessing the import of the empirical results.
- the text in the figures are virtually impossible to read (too small)
- the image quality in the figures is pretty bad (and some appear to be weirdly stretched or distorted)
- I prefer the X-axis labels that Raghu uses in their Figure 4 (with clinically interpretable increments) over the generic +1, +2, etc., labels used in Figure 3 here

Some nitpicks on the writing

* too much passive voice. Example: third paragraph in introduction (""Despite the promising results of EGDT, concerns arose.""). Avoid passive voice whenever possible.
* page 3, sec. 2.2 doesn't flow well. You bounce back and forth between discussion of the Markov assumption and full vs. partial observability. Try to focus on one concept at a time (and the solution offered by a proposed approach). Note that RNNs do NOT relax the Markov assumption -- they simply do an end run around it by using distributed latent representations.

ORIGINALITY

This work scores relatively low in originality. It really just combines ideas from two MLHC 2017 papers [1][2]. One could read those two papers and immediately conclude this paper's findings (the GP helps; RL helps; GP + RL is the best). This paper adds few (if any) new insights.

One way to address this would be to discuss in greater detail some potential explanations for why their results are stronger than those in Raghu and why the MTGP models outperform their simpler counterparts. Perhaps they could run some experiments to measure performance as a function of the number of MC samples (if perhaps grows with the number of samples, then it suggests that maybe it's largely a data augmentation effect).

SIGNIFICANCE

This paper's primary significance is that it provides further evidence that RL could be applied successfully to clinical data and problems, in particular sepsis treatment. However, this gets undersold (unsurprising, given the ML community's disdain for replication studies). It is also noteworthy that the MTGP gives such a large boost in performance for a relatively modest data set -- this property is worth exploring further, since clinical data are often small. However, again, this gets undersold.

One recommendation I would make is that the authors directly compare the results in this paper with those in Raghu and to point out, in particular, the confirmatory results. Interestingly, the shapes of the action vs. mortality rate plots (Figure 4 in Raghu, Figure 3 here) are quite similar -- that's not precisely replication, but it's comforting.

NOTE ON OFFLINE POLICY EVALUATION

This work has the same flaw that Raghu, et al., has -- neither justifies the use of offline policy evaluation. Both simply apply Jiang, et al.'s doubly robust approach [3] ""off the shelf"" without commenting on its accuracy in practice or discussing potential pitfalls (neither even considers [4] which seems to be superior in practice, especially with limited data). As far as I can tell (I'm not an RL expert), the DR approach carries stronger consistency guarantees and reduced variance but is still only as good the data it is trained on, and clinical data is known to have significant bias, particularly with respect to treatment, where clinicians are often following formulaic guidelines. Can we trust the mortality estimates in Table 1? Why or why not? Why shouldn't I think that RL is basically guaranteed to outperform non-RL approaches under an evaluation that is itself an RL model learned from the same training data!

While I'm willing to accept that this is the best we can do in this setting (we can't just try the learned policy on new patients!), I think this paper (and similar works, like Raghu, et al.) *must* provide a sober and critical discussion of its results, rather than simply applaud itself for getting the best score among competing approaches.

REFERENCES

[1] Raghu, et al. ""Continuous State-Space Models for Optimal Sepsis Treatment - a Deep Reinforcement Learning Approach."" MLHC 2017.
[2] Futoma, et al. ""An Improved Multi-Output Gaussian Process RNN with Real-Time Validation for Early Sepsis Detection."" MLHC 2017.
[3] Jiang, et al. ""Doubly robust off-policy value evaluation for reinforcement learning."" ICML 2016.
[4] Thompson and Brunskill. ""Data-Efficient Off-Policy Policy Evaluation for Reinforcement Learning."" ICML 2016.","[6, 3, 4]","[' Marginally above acceptance threshold', ' Clear rejection', ' Ok but not good enough - rejection']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer acknowledges the importance of the application and the thoroughness of the experiments, using terms like ""important application"", ""thorough comparison"", and ""promising results"". They also state their willingness to increase the score if major concerns are addressed, indicating an overall positive disposition. However, they also point out several flaws and areas for improvement, suggesting that the paper is not perfect. Therefore, the sentiment is positive but with reservations. The language used is quite direct and critical, but it maintains a professional and constructive tone. The reviewer provides specific examples and suggestions for improvement, indicating a desire to help the authors improve their work rather than simply criticize it.",60.0,70.0
Model Distillation with Knowledge Transfer from Face Classification to Alignment and Verification,"['Chong Wang', 'Xipeng Lan', 'Yangang Zhang']",Reject,2018,"[15, 2, 'no_match']","[20, 1, 'no_match']","[269, 3, 'no match']","[114, 1, 'no match']","[38, 2, 'no match']","[117, 0, 'no match']","This paper proposed to transfer the classifier from the model for face classification to the task of alignment and verification. The problem setting is interesting and valuable, however, the contribution is not clearly demonstrated. 

Specifically, it proposed to utilize the teacher model from classification to other tasks, and proposed a unified objective function to model the transferability as shown in Equation (5). The two terms in (5), (7) and (9) are used to transfer the knowledge from the teacher model. It maybe possible to claim that the different terms may play different roles for different tasks. However, there should be some general guidelines for choosing these different terms for regularization, rather than just make the claim purely based on the final results. In table 4 and table 5, the results seem to be not so consistent for using the distillation loss. The author mentioned that it is due to the weak teacher model. However, the teacher model just differs in performance with around 3% in accuracy. How could we define the “good” or “bad” of a teacher model for model distillation/transfer?

Besides, it seems that the improvement comes largely from the trick of initialization as mentioned in Section 3.2. Hence, it is still not clear which parts contribute to the final performance improvements. It could be better if the authors can report the results from each of the components together. 

 The authors just try the parameter (\alpha, \beta) to be (0,0), (1,0), (0,1) and (1,1). I think the range for both values could be any positive real value, and how about the performance for other sets of combinations, like (0.5, 0.5)?","[5, 3, 3]","[' Marginally below acceptance threshold', ' Clear rejection', ' Clear rejection']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with a neutral statement, acknowledging the interesting problem setting. However, it quickly points out a lack of clear contribution and provides constructive criticism throughout. The reviewer questions the methodology and results, suggesting improvements and further experiments. The tone, while direct, remains professional and focused on improving the paper. Therefore, the sentiment is slightly negative due to the concerns raised, and the politeness is neutral as it maintains a professional academic tone.",-20.0,0.0
A Painless Attention Mechanism for Convolutional Neural Networks,"['Pau Rodríguez', 'Guillem Cucurull', 'Jordi Gonzàlez', 'Josep M. Gonfaus', 'Xavier Roca']",Reject,2018,"[3, 3, 19, 9, 20]","[8, 7, 24, 13, 23]","[74, 16, 138, 16, 7]","[23, 5, 71, 5, 6]","[38, 8, 11, 4, 0]","[13, 3, 56, 7, 1]","Paper presents an interesting attention mechanism for fine-grained image classification. Introduction states that the method is simple and easy to understand. However, the presentation of the method is bit harder to follow. It is not clear to me if the attention modules are applied over all  pooling layers. How they are combined? 

Why use cross -correlation as the regulariser? Why not much stronger constraint such as orthogonality over elements of M in equation 1? What is the impact of this regularisation?

Why use soft-max in equation 1? One may use a Sigmoid as well? Is it better to use soft-max?

Equation 9 is not entirely clear to me. Undefined notations.

In Table 2, why stop from AD= 2 and AW=2?  What is the performance of AD=1, AW=1 with G? Why not perform this experiment over all 5 datasets? Is this performances, dataset specific?

The method is compared against 5 datasets. Obtained results are quite good.

","[6, 5, 5]","[' Marginally above acceptance threshold', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with a cautiously positive note, acknowledging the interesting aspects of the paper. However, it quickly delves into a series of critical questions and points out areas of improvement. The reviewer's use of phrases like ""not clear"" and ""why not"" indicates a critical but not disrespectful tone. The final sentence, while positive, suggests that the paper's merit lies in its results rather than its flawless execution.",40.0,60.0
A Spectral Approach to Generalization and Optimization in Neural Networks,"['Farzan Farnia', 'Jesse Zhang', 'David Tse']",Reject,2018,"[6, 0, 26]","[11, 5, 31]","[36, 20, 333]","[12, 10, 112]","[19, 10, 123]","[5, 0, 98]","This paper studies the generalization properties of 2-layer neural networks based on Fourier analysis. Studying the generalization property of neural network is an important problem and Fourier-based analysis is a promising direction, as shown in (Lee et al., 2017). However, I am not satisfied with the results in the current version.

1) The main theoretical results are on the sin activation functions instead of commonly used ReLU functions. 

2) Even if for sin activation functions, the analysis is NOT complete. The authors claimed in the abstract that gradient-based methods will converge to generalizable local minima. However, Corollary 3 is only a concentration bound on the gradient. There is a gap that how this corollary implies generalization. The paragraph below this corollary is only a high level intuition. 


","[4, 6, 6]","[' Ok but not good enough - rejection', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[4, 3, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review starts with a positive statement acknowledging the importance of the research topic and the chosen approach. However, it quickly dives into criticism of the work, pointing out limitations and gaps in the presented results. The reviewer's dissatisfaction is evident. While the language is critical, it maintains a professional and academic tone, avoiding personal attacks or disrespectful language. Therefore, the sentiment leans negative, but the politeness remains neutral.",-30.0,0.0
Link Weight Prediction with Node Embeddings,"['Yuchen Hou', 'Lawrence B. Holder']",Reject,2018,"[2, 31]","[3, 36]","[3, 180]","[1, 109]","[0, 13]","[2, 58]","The authors propose to use pretrained node embeddings in a deep learning model for link weight prediction in graphs. 
The embedding of the source node and the destination node are concatenated and fed into a fully connected neural network which produces the link weight as its output.
Existing work by Hou and Holder 2017 trains the same architecture, but the node embeddings are learned together with the weights of the neural network. In my professional opinion, the idea of using pretrained node embeddings and training only the neural network is not enough of a contribution.

Since the proposed method does not build on the SBM or pWSBM the detailed equations on page 2 are not necessary. Also, Figure 1, 2, and 3 are not necessary. Fully connected neural networks are widely used and can be explained briefly without drawing the architecture. 

Pros:
+ interesting problem
+ future work. evaluation of embeddings is indeed a hard problem worth solving.

Cons:
- not novel","[3, 3, 4]","[' Clear rejection', ' Clear rejection', ' Ok but not good enough - rejection']","[5, 4, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review is mainly negative. The reviewer does not see the novelty of the paper's proposed method and deems it ""not enough of a contribution."" While they praise the ""interesting problem"" and ""future work,"" the criticism outweighs the positive remarks. The language used is formal and professional, without resorting to harsh or impolite language.",-40.0,60.0
Generalization of Learning using Reservoir Computing,"['Sanjukta Krishnagopal', 'Yiannis Aloimonos', 'Michelle Girvan']",Reject,2018,"[5, 33, 6]","[10, 38, 10]","[17, 263, 25]","[5, 141, 4]","[9, 50, 16]","[3, 72, 5]","The paper uses an echo state network to learn to classify image transformations (between pairs of images) into one of fives classes.  The image data is artificially represented as a time series, and the goal is generalization of classification ability to unseen image pairs.  The network dynamics are studied and are claimed to have explanatory power.

The paper is well-written and easy to follow, but I have concerns about the claims it makes relative to how convincing the results are.  The focus is on one simple, and frankly now-overused data set (MNIST).  Further, treating MNIST data as a time series is artificial and clunky.  Why does the series go from left to right rather than right to left or top to bottom or inside out or something else?  How do the results change if the data is ""temporalized"" in some other way?

For training in Section 2.4, is M the number of columns for a pair of images?  It's not clear how pairs are input in parallel--- one after the other? Concatenated? Interleaved columns?  Something else? What are k, i, j in computing $\delta X_k$?  Later, in Section 3.2, it says, ""As in section 2.2, $xl(mn)$ is the differential reservoir state value of the $m$th reservoir node at time $n$ for input image $l$"", but nothing like this is discussed in Section 2.2; I'm confused.

The generalization results on this one simple data set seem pretty good.  But, how does this kind of approach do on other kinds of or more complex data?  I'm not sure that RC has historically had very good success scaling up to ""real-world"" problems to date.

Table 1 doesn't really say anything.  Of course, the diagonals are higher than the off diagonals because these are dot products.  True, they are dot products of averages over different inputs (which is why they are less than 1), but still.  Also, what Table 1 really seems to say is that the off-diagonals really aren't all that different than the diagonals, and that especially the differences between same and different digits is not very different, suggesting that what is learned is pretty fragile and likely won't generalize to harder problems.  I like the idea of using dynamical systems theory to attempt to explain what is going on, but I wonder if it is not being used a bit simplistically or naively.

Why were the five transform classes chosen?  It seems like the ""transforms"" a (same) and e (different) are qualitatively different than transforms b-d (rotated, scaled, blurred).  This seems like it should talked about.

""Thus, we infer, that the reservoir is in fact, simply training these attractors as opposed to training the entire reservoir space.""  What does this mean?  The reservoir isn't trained at all in ESNs (which is also stated explicitly for the model presented here)…

For 3.3, why did were those three classes chosen? Was this experiment tried with other subsets of three classes?  Why are results reported on only the one combination of rotated/blurred vs. rotated?  Were others tried?  If so, what were the results?  If not, why?  How does the network know when to take more than the highest output (so it can say that two transforms have been applied)?  In the case of combination, counting either transform as the correct output kind of seems like cheating a bit—it over states how well the model is doing.  Also, does the order in which the transforms are applied affect their relative representative strength in the reservoir?

The comparison with SNNs is kind of interesting, but I'm not sure that I'm (yet) convinced, as there is little detail on how the experiment was performed and what was done (or not) to try to get the SNN to generalize.  My suspicion is that with the proper approach, an SNN or similar non-dynamical system could generalize well on these tasks.  The need for a dynamical system could be argued to make sense for the camera task, perhaps, as video frames naturally form a time series; however, as already mentioned, for the MNIST data, this is not the case, and the fact that the SNN does not generalize here seems likely due to their under utilization rather than due to an inherent lack of capability.

I don't believe that there is sufficient support for this statement in the conclusion, ""[ML/deep networks] do not work as well for generalization of learning. In generalized learning, RCNs outperform them, due to their ability to function as a dynamical system with ‘memory’.""  First of all, ML is all about generalization, and there are lots and lots and lots of results showing that many ML systems generalize very well on a wide variety of problems, well beyond just classification, in fact.  And, I don't think the the paper has convincingly shown that a dynamical system 'memory' is doing something especially useful, given that the main task studied, that of character recognition (or classification of transformation or even transformation itself), does not require such a temporal ability.
","[4, 4, 4]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[4, 3, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The reviewer expresses a mixed sentiment, acknowledging the paper's strengths (well-written, easy to follow, good generalization on the tested dataset) but also raising significant concerns. They find the choice of MNIST and its artificial temporalization questionable, criticize the lack of detail and justification for several methodological choices, and find the claims regarding the superiority of ESNs over other methods, particularly regarding generalization and the role of 'memory,' to be insufficiently supported. The tone is critical but professional and not disrespectful.",-20.0,60.0
Exploring Sentence Vectors Through Automatic Summarization,"['Adly Templeton', 'Jugal Kalita']",Reject,2018,"[1, 35]","[4, 40]","[4, 218]","[2, 90]","[2, 44]","[0, 84]","This paper examines a number of sentence and document embedding methods for automatic summarization. It pairs a number of recent sentence embedding algorithms (e.g., Paragraph Vectors and Skip-Thought Vectors) with several simple summarization decoding algorithms for sentence selection, and evaluates the resulting output summary on DUC 2004 using ROUGE, based on the general intuition that the selected summary should be similar to the original document in the vector space induced by the embedding algorithm. It further provides a number of analyses of the sentence representations as they relate to summarization, and other aspects of the summarization process including the decoding algorithm.

The paper was well written and easy to understand. I appreciate the effort to apply these representation techniques in an extrinsic task.

However, the signficance of the results may be limited, because the paper does not respond to a long line of work in summarization literature which have addressed many of the same points. In particular, I worry that the paper may in part be reinventing the wheel, in that many of the results are quite incremental with respect to previous observations in the field.

Greedy decoding and non-redundancy: many methods in summarization use greedy decoding algorithms. For example, SumBasic (Nenkova and Vanderwende, 2005), and HierSum (Haghighi and Vanderwende, 2009) are two such papers. This specific topic has been thoroughly expanded on by the work on greedy decoding for submodular objective functions in summarization (Lin and Bilmes, 2011), as well as many papers which focus on how to optimize for both informativeness and non-redundancy (Kulesza and Taskar, 2012). 
	
The idea that the summary should be similar to the entire document is known as centrality. Some papers that exploit or examine that property include (Nenkova and Vanderwende, 2005; Louis and Nenkova, 2009; Cheung and Penn, 2013)
 
Another possible reading of the paper is that its novelty lies in the evaluation of sentence embedding models, specifically. However, these methods were not designed for summarization, and I don't see why they should necessarily work well for this task out of the box with simple decoding algorithms without finetuning. Also, the ROUGE results are so far from the SotA that I'm not sure what the value of analyzing this suite of techniques is.
  
In summary, I understand that this paper does not attempt to produce a state-of-the-art summarization system, but I find it hard to understand how it contributes to our understanding of future progress in the summmarization field. If the goal is to use summarization as an extrinsic evaluation of sentence embedding models, there needs to be better justification of this is a good idea when there are so many other issues in content selection that are not due to sentence embedding quality, but which affect summarization results.

References:

Nenkova and Vanderwende, 2005. The impact of frequency on summarization. Tech report.
Haghighi and Vanderwende, 2009. Exploring content models for multi-document summarization. NAACL-HLT 2009.
Lin and Bilmes, 2011. A class of submodular functions for document summarization. ACL-HLT 2011.
Kulesza and Taskar, 2012. Learning Determinantal Point Processes.
Louis and Nenkova, 2009. Automatically evaluating content selection in summarization without human models. EMNLP 2009.
Cheung and Penn, 2013. Towards Robust Abstractive Multi-Document Summarization: A Caseframe Analysis of Centrality and Domain. ACL 2013.

Other notes:
The acknowledgements seem to break double-blind reviewing.","[3, 2, 2]","[' Clear rejection', ' Strong rejection', ' Strong rejection']","[5, 5, 5]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review starts with positive remarks, praising the paper's clarity and the effort to apply representation techniques to an extrinsic task. This suggests a positive sentiment. However, the review quickly transitions into outlining significant limitations, arguing that the paper might be reinventing the wheel and doesn't offer much novelty. The reviewer also questions the value of the analysis due to low ROUGE scores and lack of contribution to future progress.  The numerous concerns and the conclusion that the reviewer finds it ""hard to understand how it contributes"" strongly indicate a negative sentiment overall. 

Despite the critical feedback, the language remains polite and professional throughout. The reviewer uses phrases like ""I appreciate,"" ""I worry that,"" and ""I find it hard to understand"" which express concerns without resorting to harsh or disrespectful language.",-50.0,70.0
Noise-Based Regularizers for Recurrent Neural Networks,"['Adji B. Dieng', 'Jaan Altosaar', 'Rajesh Ranganath', 'David M. Blei']",Reject,2018,"[3, 4, 10, 18]","[7, 9, 15, 23]","[22, 20, 105, 259]","[7, 10, 47, 140]","[14, 10, 51, 88]","[1, 0, 7, 31]","The authors of the paper advocate injecting noise into the activations of recurrent networks for regularisation. This is done  by replacing the deterministic units with stochastic ones.

The paper has several issues with respect to the method and related work. 

- The paper needs to mention [Graves 2011], which is one of the first works to inject noise into the dynamics of an RNN. It is also important to know how these two approaches differ. E.g.: Under what conditions are the two approaches equivalent? How do they compare experimentally?
- While [Bayer & Osendorfer, 2014] and [Chung et al, 2015] appear in the list of references, these works are not discussed in the main text. I personally think these are extremely related, pioneering the use of stochastic units in a recurrent context. In the end, the original paper can be cast in these frameworks approximately by removing the KL term of the ELBO. This might be ok by itself, but that the authors are apparently aware of the work (as it is in the list of references) and not discussing them in the main text makes me highly skeptical.
- The method is introduced for general exponential families, but a) not empirically evaluated for more than the Gaussian case and b) not a complete algorithm for e.g. the Bernoulli case. More specifically, the reader is left alone with the problem of estimating the gradients in the Bernoulli case, which is an active area of research by itself.
- The paper makes use of the reparameterisation trick, but does not cite the relevant literature, e.g. [Kingma 2013, Rezende 2014, and another one I currently struggle to find].
- The desiderate for noise seem completely arbitrary to me and are not justified. I don’t see why violation of any of them would lead to an inferior regularisation method.

### References
[Graves 2011] Graves, Alex. ""Practical variational inference for neural networks."" Advances in Neural Information Processing Systems. 2011.
[Kingma 2013] Kingma, Diederik P., and Max Welling. ""Auto-encoding variational bayes."" arXiv preprint arXiv:1312.6114 (2013).
[Rezende 2014] Rezende, Danilo Jimenez, Shakir Mohamed, and Daan Wierstra. ""Stochastic backpropagation and approximate inference in deep generative models."" arXiv preprint arXiv:1401.4082 (2014).
","[2, 5, 3]","[' Strong rejection', ' Marginally below acceptance threshold', ' Clear rejection']","[5, 4, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review is highly critical of the paper, pointing out several major flaws and omissions. The reviewer uses strong language like ""several issues,"" ""makes me highly skeptical,"" and ""completely arbitrary"" to express their negative opinion. However, the language remains within the bounds of academic discourse and avoids personal attacks.",-70.0,20.0
Soft Value Iteration Networks for Planetary Rover Path Planning,"['Max Pflueger', 'Ali Agha', 'Gaurav S. Sukhatme']",Reject,2018,"[4, 0, 26]","[6, 4, 31]",['skipped'],['skipped'],['skipped'],['skipped'],"Summary:
The submission proposes a simple modification to the Value Iteration Networks (VIN) method of Tamar et al., basically consisting of assuming a stochastic policy and replacing the max-over-actions in value iteration with an expectation that weights actions proportional to their exponentiated Q-values. Since this change removes the main nondifferentiability of VINs, it is hypothesized that the resulting method will be easier to train than VINs, and experiments seem to support this hypothesis.

Pros:
+ The proposed modification to VIN is simple, well-motivated, and addresses the nondifferentiability of VIN
+ Experiments on synthetic data demonstrate a significant improvement over the standard VIN method

Cons:
+ Some important references are missing (e.g., MaxEnt IOC with deep-learned features)
+ Although intuitive, more detailed justification could be provided for replacing the max-over-actions with an exponentially-weighted average
+ No baselines are provided for the experiments with real data
+ All the experimental scenarios are fairly simple (2D grid-worlds with discrete actions, 1-channel input features)

The proposed method is simple, well-motivated, and addresses a real concern in VINs, which is their nondifferentiability. Although many of the nonlinearities used in CNNs for computer vision applications are nondifferentiable, the theoretical grounds for using these in conjunction with gradient-based optimization is obviously questionable. Despite this, they are widely used for such applications because of strong empirical results showing that such nonlinearities are beneficial in image-processing applications. However, it would be incorrect to assume that because such nonlinearities work for image processing, they are also beneficial in the context of unrolling value iteration.

Replacing the max-over-actions with an exponentially-weighted average is an intuitively well-motivated alternative because, as the authors note, it incorporates the values of suboptimal actions during the training procedure. We would therefore expect better or faster training, as the values of these suboptimal actions can be updated more frequently. The (admittedly limited) experiments bear out this hypothesis.

Perhaps the most significant downside of this work is that it fails to acknowledge prior work in the RL and IOC literature that result in similar  “smoothed” or “softmax"" Bellman updates: in particular, MaxEnt IOC [A] and linearly-solvable MDPs [B] both fall in this category. Both of those papers clearly derive approximate Bellman equations from modified optimal control principles; although I believe this is also possible for the proposed update (Eq. 11), along the lines of the sentence after Eq. 11, this should be made more explicit/rigorous, and the result compared to [A,B].

Another important missing reference is [C], which learned cost maps with deep neural networks in a MaxEnt IOC framework. As far as I can tell, the application is identical to that of the present paper, and [C] may have some advantages: for instance, [C] features a principled, fully-differentiable training objective while also avoiding having to backprop through the inference procedure, as in VIN. Again, this raises the question of how the proposed method compares to MaxEnt IOC, both theoretically and experimentally.

The experiments are also a bit lacking in a few ways. First, a baseline is only provided for the experiments with synthetic data. Although that experiment shows a promising, significant advantage over VIN, the lack of baselines for the experiment with real data is disappointing. Furthermore, the setting for the experiments is fairly simple, consisting of a grid-world with 1-channel input features. The setting is simple enough that even shallow IOC methods (e.g., [D]) would probably perform well; however, the deep IOC methods of [C] is also applicable and should probably also be evaluated as a baseline.

In summary, although the method proposes an intuitively reasonable modification to VIN that seems to outperform it in limited experiments, the submission fails to acknowledge important related work (especially the MaxEnt IOC methods of [A,D]) that may have significant theoretical and practical advantages. Unfortunately, I believe the original VIN paper also failed to articulate the precise advantages of VIN over this prior work—which is not to say there are none, but it is clear that VINs applied to problems as simple as the one considered here have real competitors in prior work. Clarifying this connection, both theoretically and experimentally, would make this work much stronger and would be a valuable contribution to the literature.

[A] Ziebart, Brian D. Modeling purposeful adaptive behavior with the principle of maximum causal entropy. Carnegie Mellon University, 2010.
[B] Todorov, Emanuel. ""Linearly-solvable Markov decision problems."" Advances in neural information processing systems. 2007.
[C] Wulfmeier et al. Watch This: Scalable Cost-Function Learning for Path Planning in Urban Environments. IROS 2016
[D] Ratliff, Nathan D., David Silver, and J. Andrew Bagnell. ""Learning to search: Functional gradient techniques for imitation learning."" Autonomous Robots 27.1 (2009): 25-53.
","[4, 3, 3]","[' Ok but not good enough - rejection', ' Clear rejection', ' Clear rejection']","[4, 5, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']","The review acknowledges the merits of the proposed method, highlighting its simplicity, motivation, and improvement over the standard VIN method. However, it also points out significant downsides, including missing important references and limitations in the experiments. The reviewer suggests that clarifying the connection with prior work would make the work stronger. Overall, the sentiment is mixed, leaning towards the positive side due to the acknowledged merits, but with reservations. The language used is polite and professional, providing constructive criticism and suggestions for improvement.",50.0,80.0
Make SVM great again with Siamese kernel for  few-shot learning,['Bence Tilk'],Reject,2018,['no_match'],['no_match'],['no match'],['no match'],['no match'],['no match'],"Make SVM great again with Siamese kernel for few-shot learning 

** PAPER SUMMARY **

The author proposes to combine siamase networks with an SVM for pair classification. The proposed approach is evaluated on few shot learning tasks, on omniglot and timit.  


** REVIEW SUMMARY **

The paper is readable but it could be more fluent. It lacks a few references and important technical aspects are not discussed. It contains a few errors. Empirical contribution seems inflated on omniglot as the authors omit other papers reporting better results. Overall, the contribution is modest at best.

** DETAILED REVIEW **

On mistakes, it is wrong to say that an SVM is a parameterless classifier. It is wrong to cite (Boser et al 92) for the soft-margin SVM. I think slack variables come from (Cortes et al 95). ""consistent"" has a specific definition in machine learning https://en.wikipedia.org/wiki/Consistent_estimator , you must use a different word in 3.2. You mention that a non linear SVM need a similarity measure, it actually need a positive definite kernel which has a specific definition, https://en.wikipedia.org/wiki/Positive-definite_kernel .

On incompleteness, it is not obvious how the classifier is used at test time. Could you explain how classes are predicted given a test problem? The setup of the experiments on TIMIT is extremely unclear. What are the class you are interested in? How many classes and examples does the testing problems have? 

On clarity, I do not understand why you talk again about non-linear SVM in the last paragraph of 3.2. since you mention at the end of page 4 that you will only rely on linear SVMs for computational reasons. You need to mention explicitely somewhere that (w,\theta) are optimized jointly. The sentence ""this paper investigates only the one versus rest approach"" is confusing, as you have only two classes from the SVM perspective i.e. pairs (x1,x2) where both examples come from the same class and pairs (x1,x2) where they come from different class. So you use a binary SVM, not one versus rest. You need to find a better justification for using L2-SVM than ""L2-SVM loss variant is considered to be the best by the author of the paper"", did you try classical SVM and found them performing worse? Also could you motivate your choice for L1 norm as opposed to L2 in Eq 3?

On empirical evaluation, I already mentioned that it impossible to understand what the classification problem on TIMIT is. I suspect it might be speaker identification. So I will focus on the omniglot experiments. 

Few-Shot Learning Through an Information Retrieval Lens, Eleni Triantafillou, Richard Zemel, Raquel Urtasun, NIPS 2017 [arxiv July'17]

and the reference therein give a few more recent baselines than your table. Some of the results are better than your approach. I am not sure why you do not evaluate on mini-imagenet as well as most work on few shot learning generally do. This dataset offers a clearer experimental setup than your TIMIT setting and has abundant published baseline results. Also, most work typically use omniglot as a proof of concept and consider mini-imagenet as a more challenging set. ","[3, 5, 4]","[' Clear rejection', ' Marginally below acceptance threshold', ' Ok but not good enough - rejection']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review is critical of the paper, pointing out errors, incompleteness, lack of clarity, and potentially inflated empirical contributions. While the language is direct and points out shortcomings, it does not resort to personal attacks or disrespectful tone. The reviewer provides concrete examples and suggestions for improvement.",-30.0,60.0
What is image captioning made of?,"['Pranava Madhyastha', 'Josiah Wang', 'Lucia Specia']",Reject,2018,"[5, 10, 14]","[10, 14, 19]","[79, 38, 321]","[40, 23, 231]","[33, 10, 61]","[6, 5, 29]","This paper is an experimental paper. It investigates what sort of image representations are good for image captioning systems. 

Overall, the idea seems relevant and there are some good findings but I am sure that image captioning community is already aware of these findings.

The main issue of the paper is the lack of novelty. Even for an experimental paper, I would argue that novelty in the experimental methodology is an important fact. Unfortunately, I do not see any novel concept in the experimental setup.

I recomend this paper for a workshop presentation.
","[4, 4, 4]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[5, 4, 5]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The reviewer finds the paper's topic relevant and acknowledges some good findings. However, they clearly state the main issue is a lack of novelty, which significantly impacts their recommendation. The language used doesn't express strong negativity but rather leans towards neutrality with a clear message of lacking novelty.  Therefore, the sentiment is somewhat negative, but the politeness remains fairly neutral.",-20.0,20.0
The Context-Aware Learner,"['Conor Durkan', 'Amos Storkey', 'Harrison Edwards']",Reject,2018,"[1, 23, 3]","[6, 28, 8]","[14, 135, 22]","[4, 62, 9]","[9, 54, 12]","[1, 19, 1]","This paper proposes a model for learning to generate data conditional on attributes. Demonstrations show that the model is capable of learning to generate data with attribute combinations that were not present in conjunction at training time.

The model is interesting, and the results, while preliminary, suggest that the model is capable of making quite interesting generalizations (in particular, it can synthesize images that consist of settings of features that have not been seen before).

However, this paper is mercilessly difficult to read. The most serious problems are the extensive discussion of the fully unsupervised variant (rather than the semisupervised variant that is evaluated), poor use of examples when describing the model, nonstandard terminology (“concepts” and “context” are extremely vague terms that are not defined precisely) and discussions to vaguely related work that does not clarify but rather obscures what is going on in the paper.

For the evaluation, since this paper proposes a technique for learning a posterior recognition model, it would be extremely interesting to see if the model is capable of recognizing images appropriately that combine “contexts” that were not observed during training. The experiments show that the generation component is quite effective, but this is an obvious missing step.

Anyway, some other related work:
Lample et al. (2017 NIPS). Fader Networks. I realize this work is more ambitious since it seeks to be a fully generative model including of the contexts/attributes. But I mostly bring it up because it is an impressively clear presentation of a model and experimental set up.","[4, 4, 6]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Marginally above acceptance threshold']","[4, 3, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The reviewer finds the paper's core idea interesting and promising, evidenced by statements like ""The model is interesting"" and acknowledging the potential for ""quite interesting generalizations."" However, they express significant concerns about the clarity and presentation of the paper, using terms like ""mercilessly difficult to read,"" ""poor use of examples,"" ""nonstandard terminology,"" and ""obscures what is going on."" This suggests an overall mixed sentiment, leaning towards the negative due to the severity of the clarity issues. While the reviewer uses strong language to criticize the paper, it doesn't come across as personally offensive or disrespectful to the authors. The suggestions for improvement are presented matter-of-factly.",-20.0,40.0
Towards Building Affect sensitive Word Distributions,"['Kushal Chawla', 'Sopan Khosla', 'Niyati Chhaya', 'Kokil Jaidka']",Reject,2018,"[2, 3, 8, 9]","[7, 7, 13, 14]","[25, 27, 70, 68]","[13, 20, 51, 42]","[12, 7, 11, 7]","[0, 0, 8, 19]","This paper proposes integrating information from a semantic resource that quantifies the affect of different words into a text-based word embedding algorithm. 

The affect lexical seems to be a very interesting resource (although I'm not sure what it means to call it 'state of the art'), and definitely support the endeavour to make language models more reflective of complex semantic and pragmatic phenomena such as affect and sentiment. 

The justification for why we might want to do this with word embeddings in the manner proposed seems a little unconvincing to me:

- The statement that 'delighted' and 'disappointed' will have similar contexts is not evident to me at least (other then them both being participle / adjectives).

- Affect in language seems to me to be a very contextual phenomenon. Only a tiny subset of words have intrinsic and context-free affect. Most affect seems to me to come from the use of words in (phrasal, and extra-linguistic) contexts, so a more context-dependent model, in which affect is computed over phrases or sentences, would seem to be more appropriate. Consider words like 'expensive', 'wicked', 'elimination'...

The model proposes several applications (sentiment prediction, predicting email tone, word similarity) where the affect-based embeddings yield small improvements. However, in different cases, taking different flavours of affect information (V, A or D) produces the best score, so it is not clear what to conclude about what sort of information is most useful. 

It is not surprising to me that an algorithm that uses both WordNet and running text to compute word similarity scores improves over one that uses just running text. It also not surprising that adding information about affect improves the ability to predict sentiment and the tone of emails. 

To understand the importance of the proposed algorithm (rather than just the addition of additional data), I would like to see comparison with various different post-processing techniques using WordNet and the affect lexicon (i.e. not just Bollelaga et al.) including some much simpler baselines. For instance, what about averaging WordNet path-based distance metrics and distance in word embedding space (for word similarity), and other ways of applying the affect data to email tone prediction?

","[4, 4, 6]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Marginally above acceptance threshold']","[5, 4, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer provides a mixed opinion, acknowledging the interesting aspects of the paper's approach while expressing reservations about the justification and methodology. While they find the affect lexicon intriguing and support incorporating affect into language models, they find the paper's arguments for its specific application to word embeddings unconvincing. The reviewer also points out the lack of clear conclusions from the results and suggests additional comparisons with simpler baselines. The language used is polite and constructive, offering specific suggestions for improvement rather than outright criticism. Overall, the review leans slightly towards the negative due to the concerns raised, but it maintains a professional and respectful tone.",-20.0,70.0
Simple Nearest Neighbor Policy Method for Continuous Control Tasks,"['Elman Mansimov', 'Kyunghyun Cho']",Reject,2018,"[4, 9]","[9, 14]","[27, 396]","[10, 154]","[17, 215]","[0, 27]","SUMMARY
The paper deal with the problem of RL.  It proposes a non-parametric approach that maps trajectories to the optimal policy.  It avoids learning parameterized policies.  The fundamental idea is to store passed trajectories.  When a policy is to be executed, it does nearest neighbor search to find then closest trajectory and executes it.

COMMENTS

What happens if the agent finds it self  in a state that while is close to a state in the similar trajectory the action required to could be completely different.

Not certain about the claim that standard RL policy learning algorithms make it difficult to assess the difficulty of a problem. 

How do you execute a trajectory? Actions in RL are by definition stochastic, and this would make it unlikely that a same trajectory can be reproduced exactly.
","[4, 4, 3]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Clear rejection']","[5, 4, 5]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']",The review does not give an overall positive or negative impression and is rather neutral in its sentiment. It raises valid concerns and questions about the paper's approach in a neutral and professional manner. The reviewer asks important clarifications without dismissing the paper's core ideas.,0.0,50.0
AirNet: a machine learning dataset for air quality forecasting,"['Songgang Zhao', 'Xingyuan Yuan', 'Da Xiao', 'Jianyuan Zhang', 'Zhouyuan Li']",Reject,2018,"['no_match', 1, 14, 6, 7]","['no_match', 1, 18, 10, 9]","['no match', 2, 23, 6, 4]","['no match', 1, 8, 4, 1]","['no match', 1, 2, 0, 1]","['no match', 0, 13, 2, 2]","The paper is about open sourcing AirNet, a database that has interpolated air quality metrics in a spatial form along with matching meteorological data obtained elsewhere. In addition, the paper also develops a few baseline methods and evaluated using standard metrics such as detection rate, false alarms etc. The work is original and significant from an applications point of view. It looks like the dataset is useful but the model development and experimental sections are weak.

Strengths:
- open source data set for air quality monitoring that is significantly better than existing ones.
- baseline models using standard methods including RNN.

Weaknesses:
- The air quality data is measured at point locations (stations) which are interpolated to obtain spatial data. There is no evaluation on this step to make sure the interpolated data indeed reflects truth. 
- Experiments doesn't seem to be carefully done using hyper-parameter tuning/ cross-validation. The model results may be misleading.
- Writing and formatting needs to be improved. Some examples - ""quality of air quality"", ""people attempted to apply deep learning"", ""in the computer vision field ."", ""Some people also used the hidden Makov model"", ""radial of longitude"", ""in 2:00AM, January 23"". The paper in general was not easy to follow at many places.
- Is Table 3 incomplete with one box unlabeled?
- Figure 3 is not clear. It is suggested to follow standard notations to represent the RNN structure (see Jurgen Schmidhuber's paper)
- ""DEV"" in table 4 is not explained. Is this a development set? If so, what does it mean?
- It is said that ""reduced LSTM is improved than LSTM"". But the test results in Table 4 shows that LSTM is better.","[5, 4, 4]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer acknowledges the significance and originality of the work, particularly the value of the open-source dataset. This points towards a positive sentiment. However, they also point out significant weaknesses in the experimental methodology and presentation, suggesting a less positive view of these aspects. Overall, the sentiment is mixed but leans more towards the positive side due to the dataset's value. The language used is generally polite and professional. While the reviewer points out shortcomings, they do so in a constructive manner, suggesting specific improvements. The tone remains critical but not disrespectful.",60.0,70.0
 Explicit Induction Bias for Transfer Learning with Convolutional Networks,"['Xuhong LI', 'Yves GRANDVALET', 'Franck DAVOINE']",Reject,2018,"['no_match', 24, 25]","['no_match', 29, 30]","['no match', 66, 95]","['no match', 38, 66]","['no match', 7, 9]","['no match', 21, 20]","The paper proposes an analysis on different adaptive regularization techniques for deep transfer learning. 
Specifically it focuses on the use of an L2-SP condition that constraints the new parameters to be close to the
ones previously learned when solving a source task. 

+ The paper is easy to read and well organized
+ The advantage of the proposed regularization against the more standard L2 regularization is clearly visible 
from the experiments

- The idea per se is not new: there is a list of shallow learning methods for transfer learning based 
on the same L2 regularization choice
[Cross-Domain Video Concept Detection using Adaptive SVMs, ACM Multimedia 2007]
[Learning categories from few examples with multi model knowledge transfer, PAMI 2014]
[From n to n+ 1: Multiclass transfer incremental learning, CVPR 2013]
I believe this literature should be discussed in the related work section

- It is true that the L2-SP-Fisher regularization was designed for life-long learning cases with a 
fixed task, however, this solution seems to work quite well in the proposed experimental settings. 
From my understanding L2-SP-Fisher can be considered the best competitor of L2-SP so I think
the paper should dedicate more space to the analysis of their difference and similarities both
from the theoretical and experimental point of view. For instance:
--  adding the L2-SP-Fisher results in table 2
--  repeating the experiments of figure 2 and figure 3 with L2-SP-Fisher




","[6, 6, 7]","[' Marginally above acceptance threshold', ' Marginally above acceptance threshold', ' Good paper, accept']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer provides a mixed opinion, acknowledging the paper's strengths (well-written, clear advantage of the proposed method) but also pointing out significant shortcomings (lack of novelty, insufficient comparison with a relevant existing technique). The tone is critical but professional and offers constructive suggestions for improvement. ",20.0,60.0
Large Margin Neural Language Models,"['Jiaji Huang', 'Yi Li', 'Wei Ping', 'Sanjeev Satheesh', 'Gregory Diamos']",Reject,2018,"[7, 9, 10, 11]","[11, 14, 10, 15]","[41, 56, 24, 49]","[24, 26, 13, 26]","[14, 30, 10, 20]","[3, 0, 1, 3]","The main contribution of this paper are:
(a) replacing the typical maximum likelihood criterion in neural language model training with a discriminative criterion,
(b) propose two large margin criterion -- difference in likelihood and difference in rank (WER or BLUE ordered) hypotheses,
(c) demonstrate performance gains two standard tasks -- an ASR task on Wall Street Journal (small task) and an MT task.

In addition, they provide examples in Figure (1) and (2) that illustrate the effect of the cost function on training. Their illustration in Figure 4 is also helpful in seeing the impact of using a warm start with a generative model.
","[7, 5, 5]","[' Good paper, accept', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[5, 4, 5]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review explicitly points out the main contributions of the paper and acknowledges the helpfulness of the illustrations. It does not contain any negative statements. Therefore, the sentiment is positive. The language used is factual, professional, and void of any negative or informal phrasing, indicating a high level of politeness.",60.0,80.0
Anytime Neural Network: a Versatile Trade-off Between Computation and Accuracy,"['Hanzhang Hu', 'Debadeepta Dey', 'Martial Hebert', 'J. Andrew Bagnell']",Reject,2018,"[6, 10, 36, 18]","[10, 15, 41, 23]","[22, 66, 409, 195]","[5, 31, 282, 117]","[5, 31, 64, 58]","[12, 4, 63, 20]","This paper proposes an anytime neural network, which can predict anytime while training. To achieve that, the model includes auxiliary predictions which can make early predictions. Specifically, the paper presents a loss weighting scheme that considers high correlation among nearby predictions, an oscillating loss weighting scheme for further improvement, and an ensemble of anytime neural networks. In the experiments, test error of the proposed model was shown to be comparable to the optimal one at each time budget. 

It is an interesting idea to add auxiliary predictions to enable early predictions and the experimental results look promising as they are close to optimal at each time budget. 

1. In Section 3.2, there are some discussions on the parallel computations of EANN. The parallel training is not clear to me and it would be great to have more explanation on this with examples.  

2. It seems that EANN is not scalable because the depth is increasing exponentially. For example, given 10 machines, the model with the largest depth would have 2^10 layers, which is difficult to train. It would be great to discuss this issue.

3. In the experiments, it would be great to add a few alternatives to be compared for anytime predictions. 







","[7, 5, 5]","[' Good paper, accept', ' Marginally below acceptance threshold', ' Marginally below acceptance threshold']","[2, 4, 3]","[' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer provides a positive summary of the paper's contributions, highlighting the interesting idea and promising results. While they raise valid questions and suggestions for improvement, these are presented constructively and aim to enhance the paper's clarity and completeness. ",60.0,80.0
Learning Dynamic State Abstractions for Model-Based Reinforcement Learning,"['Lars Buesing', 'Theophane Weber', 'Sebastien Racaniere', 'S. M. Ali Eslami', 'Danilo Rezende', 'David Reichert', 'Fabio Viola', 'Frederic Besse', 'Karol Gregor', 'Demis Hassabis', 'Daan Wierstra']",Reject,2018,"[12, 11, 8, 7, 8, 12, 7, 10, 10, 15]","[17, 16, 13, 11, 13, 17, 10, 15, 15, 16]","[49, 70, 40, 54, 90, 67, 17, 42, 54, 70]","[21, 26, 11, 18, 28, 34, 6, 17, 10, 35]","[19, 38, 25, 32, 58, 18, 10, 24, 26, 27]","[9, 6, 4, 4, 4, 15, 1, 1, 18, 8]","Summary:

This paper studies how to learn (hidden)-state-space models of environment dynamics, and integrate them with Imagination-Augmented Agents (I2A). The paper considers single-agent problems and tests on Ms Pacman etc.

There are several variations of the hidden-state space [ds]SSM model: using det/stochastic latent variables + using det/stochastic decoders. In the stochastic case, learning is done using variational methods. 

[ds]SSM is integrated with I2A, which generates rollouts of future states, based on the inferred hidden states from the d/sSSM-VAE model. The rollouts are then fed into the agent's policy / value function.

Main results seem to be:
1. Experiments on learning the forward model, show that latent forward models work better and faster than naive AR models on several Atari games, and better than fully model-free baselines. 
2. I2A agents with latent codes work better than model-free models or I2A from pixels. Deterministic latent models seem to work better than stochastic ones.

Pro:
- Relatively straightforward idea: learn the forward model on hidden states, rather than raw states.
- Writing is clear, although a bit dense in places.

Con:
- Paper only shows training curves for MS Pacman. What about the other games from Table 1?
- The paper lacks any visualization of the latent codes. What do they represent? Can we e.g. learn a raw-state predictor from the latent codes?
- Are the latent codes relevant in the stochastic model? See e.g. the discussion in ""Variational Lossy Autoencoder"" (Chen et al. 2016)
- Experiments are not complete (e.g. for AR, as noted in the paper).
- The games used are fairly reactive (i.e. do not require significant long-term planning), and so the sequential hidden-state-space model does not have to capture long-term dependencies. It would be nice to see how this technique fares on Montezuma's revenge, for instance.

Overall:
The paper proposes a simple idea that seems to work well on reactive 1-agent games. However, the paper could give more insights into *how* this works: e.g. a better qualitative inspection of the learned latent model, and how existing questions surrounding sequential stochastic model affect the proposed method. Also, not all baseline experiments are done, and the impact on training is only evaluated on 1 game. 

Detailed:
-
","[6, 5, 8]","[' Marginally above acceptance threshold', ' Marginally below acceptance threshold', ' Top 50% of accepted papers, clear accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is generally positive in its assessment of the paper's core idea (learning forward models on hidden states) and acknowledges the clarity of the writing. However, it also raises several significant concerns and calls for more comprehensive experiments and analysis. The reviewer points out missing visualizations, incomplete baseline comparisons, and a lack of in-depth exploration of the learned latent model. The use of ""fairly reactive"" games is also noted as a limitation. Overall, the feedback leans towards the positive side due to the promising nature of the idea and clear presentation, but with significant reservations about the depth and completeness of the work.",40.0,80.0
Unsupervised Learning of Entailment-Vector Word Embeddings,['James Henderson'],Reject,2018,['skipped'],['skipped'],['skipped'],['skipped'],['skipped'],['skipped'],"This work proposes to learn word vectors that are intended to specifically model the lexical entailment relationship. This is achieved in an unsupervised manner from unstructured data, through an approach heavily influenced by recent work by Henderson and Popa, which ""reinterprets word2vec"" by modeling distributions over discrete latent ""pseudo-phrase"" vectors. That is, instead of using two vectors per word, as in word2vec, a latent representation is introduced that models the joint properties of the target and context words. While Henderson and Popa represent the latent vector as the evidence for the target and context, or the likelihood, this work suggests to represent it based on the posterior distribution instead. The resultant representations are evaluated on Weeds et al.'s (2014) version of BLESS, as well as the full BLESS dataset, where they do better than the original.

The paper is confusingly written, fails to mention a lot of related work, has a weak evaluation where it doesn't compare to related systems, and I feel that it would benefit from ""toning down"". Hence, I do not recommend it for acceptance. In more detail:

1. The idea behind Henderson and Popa's model, as well as the suggested modification, should be easy to explain, but I really had to struggle to make sense of it. This work relies very heavily on that paper, and would be better off if it was more standalone. I think part of the confusion stems from using y for the latent representation but not specifying whether it is a word or latent representation in Equation 1 - that only becomes obvious later. The exposition clearly needs more work, and more precise technical writing.

2. There is a lot of related work around word embeddings that is not mentioned, both on word2vec-style representation learning (e.g. it would be useful to relate this more to word2vec and what it learns, as in Omer Levy's work on ""interpreting"" word2vec, rather than reinterpreting) and word embeddings on hypernymy detection and lexical entailment (see e.g. Stephen Roller's thesis for references).

3. There has been a lot of work on the Weeds BLESS dataset that is not mentioned, or compared against, including unsupervised approaches (e.g. Levy's work, Santus's work, Kiela's work, Roller's work, etc.), that perform better than the numbers in Table 1. There are many other datasets that measure lexical entailment, none of which are evaluated on (apart from the original BLESS set, which is mentioned in passing). It would make sense to show that the method works on more than one dataset, and to do a thorough comparison against other work; especially given that:

4. The tone of the work appears to imply that word2vec was wrong and needs to be reinterpreted: the work leads to ""unprecedented results"" (not true), claims to be a completely novel method for inducing word representations (together with LSA, BOW and Word2Vec, third paragraph; not true), and suggests it has found ""the best way to extract information about the semantics of a word from this model"" (7th paragraph; not true). This, together with the ""reinterpretation of word2vec"" and the proposed ""new distributional semantic models"" almost makes it hard for me to take the work seriously.","[3, 3, 7]","[' Clear rejection', ' Clear rejection', ' Good paper, accept']","[5, 5, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer expresses significant concerns about the paper, ultimately recommending rejection. While they acknowledge the paper's aim and basis on previous work, they find the writing confusing, the literature review lacking, the evaluation weak, and the tone overly strong. The reviewer cites specific examples like the unclear explanation of Henderson and Popa's model, the omission of relevant related work, and the strong claims made about the novelty and performance of the proposed method. All these factors point towards a negative sentiment, although the language remains professional and provides constructive criticism.",-60.0,60.0
Variance Regularizing Adversarial Learning,"['Karan Grewal', 'R Devon Hjelm', 'Yoshua Bengio']",Reject,2018,"[2, 5, 31]","[6, 9, 36]",['skipped'],['skipped'],['skipped'],['skipped'],"The paper proposes variance regularizing adversarial learning (VRAL), a new method for training GANs.

The motivation is to ensure that the gradient for the generator does not vanish. The authors propose to use a discriminator whose output targets a mixture of two Gaussians (one component each for real and fake data).  The means and variances are fixed so that the discriminator does not overfit, which ensures that the generator learning is not hindered. 

The discriminator itself is trained through two additional meta-discriminators (!) Are the meta-discriminators really necessary? Have you tried matching moments or using other methods for comparing the distributions?

It would be useful to write down the actual loss function so that it's easier to compare with other GAN variants. In particular, I'm curious to understand the difference between VRAL and Fisher-GAN. The authors discuss this in the end of Section 3, but a more careful comparison is needed.

The experimental results are pretty limited and lack detailed quantitative evaluation, which makes it harder to compare the performance of the proposed variant to existing algorithms.

Overall, I think that the idea is interesting, but the paper needs more work and does not meet the ICLR acceptance bar.

FYI, another concurrent submission showed that gradient penalties stabilize training of GANs:
MANY PATHS TO EQUILIBRIUM: GANS DO NOT NEED TO DECREASE A DIVERGENCE AT EVERY STEP
https://openreview.net/pdf?id=ByQpn1ZA-","[4, 5, 6]","[' Ok but not good enough - rejection', ' Marginally below acceptance threshold', ' Marginally above acceptance threshold']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer finds the idea interesting but suggests the paper needs more work. The reviewer provides constructive criticism and suggestions for improvement, indicating a willingness to see the paper succeed with further development. The language is professional and respectful throughout.",20.0,80.0
Dynamic Integration of Background Knowledge in Neural NLU Systems,"['Dirk Weissenborn', 'Tomas Kocisky', 'Chris Dyer']",Reject,2018,"[6, 6, 20]","[10, 10, 24]","[38, 27, 268]","[18, 10, 161]","[17, 15, 90]","[3, 2, 17]","The main emphasis of this paper is how to add background knowledge so as to improve the performance of NLU (specifically QA and NLI) systems. They adopt the sensible perspective that background knowledge might most easily be added by providing it in text format. However, in this paper, the way it is added is simply by updating word representations based on this extra text. This seems too simple to really be the right way to add background knowledge. 

In practice, the biggest win of this paper turns out to be that you can get quite a lot of value by sharing contextualized word representations between all words with the same lemma (done by linguistic preprocessing; the paper never says exactly how, not even if you read the supplementary material). This seems a useful observation which it would be easy to apply everywhere and which shows fairly large utility from a bit of linguistically sensitive matching!  As the paper notes, this type of sharing is the main delta in this paper from simply using a standard deep LSTM (which the paper claims to not work on these data sets, though I'm not quite sure couldn't be made to work with more tuning).

pp. 6-7: The main thing of note seems to be that sharing of representations between words with the same lemma (which the tables refer to as ""reading"" is worth a lot (3.5-6.0%), in every case rather more than use of background knowledge (typically 0.3-1.5%). A note on the QA results: The QA results are certainly good enough to be in the range of ""good systems"", but none of the results really push the SOTA. The best SQuAD (devset) results are shown as several percent below the SOTA. In the table the TriviaQA results are shown as beating the SOTA, and that's fair wrt published work at the time of submission, but other submissions show that all of these results are below what you get by running the DrQA (Chen et al. 2017) system off-the-shelf on TriviaQA, so the real picture is perhaps similar to SQuAD, especially since DrQA is itself now considerably below the SOTA on SQUAD. Similar remarks perhaps apply to the NLI results.

p.7 In the additional NLI results, it is interesting and valuable to note that the lemmatization and knowledge help much more when amounts of data (and the covarying dimensionality of the word vectors) is much smaller, but the fact that the ideas of this paper have quite little (or even negative) effects when run on the full data with full word vectors on top of the ESIM model again draws into question whether enough value is being achieved from the world knowledge.

Biggest question:
 - Are word embeddings powerful enough as a form of memory to store the kind of relational facts that you are accessing as background knowledge?

Minor notes:
 - The paper was very well written/edited. The only real copyediting I noticed was in the conclusion: and be used ➔ and can be used; that rely on ➔ that relies on.
 - Should reference to (Manning et al. 1999) better be to (Manning et al. 2008) since the context here appears to be IR systems?
 - On p.3 above sec 3.1: What is u? Was that meant to be z?
 - On p.8, I'm a bit suspicious of the ""Is additional knowledge used?"" experiment which trains with knowledge and then tests without knowledge. It's not surprising that this mismatch might hurt performance, even if the knowledge provided no incremental value over what could be gained from standard word vectors alone.
 - In the supplementary material the paper notes that the numbers are from the best result from 3 runs. This seems to me a little less good experimental practice than reporting an average of k runs, preferably for k a bit bigger than 3.


","[5, 6, 5]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Marginally below acceptance threshold']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer acknowledges the paper's valuable observations and good writing but expresses skepticism about the core idea and findings. They find the method of adding background knowledge ""too simple"" and question the reliance on word embeddings for storing relational facts. The reviewer highlights that the biggest gain comes from sharing representations between words with the same lemma, not from the background knowledge itself. They also point out that the results, while decent, don't surpass existing systems and question the validity of some experiments. Overall, the review leans towards the critical side, suggesting potential but not fully convincing results.",20.0,70.0
"LSD-Net: Look, Step and Detect for Joint Navigation and Multi-View Recognition with Deep Reinforcement Learning",['N dinesh reddy'],Reject,2018,[6],[11],[20],[13],[7],[0],"Paper Summary: The paper proposes an approach to perform object classification and changing the viewpoint simultaneously. The idea is that the viewpoint changes until the object is recognized. The results have been reported on ModelNet40.

Paper Strength: The idea of combining active vision with object classification is interesting.

Paper Weaknesses:
I have the following concerns about this paper: (1) The paper performs the experiments on ModelNet40, which is a toy dataset for this task. The background is white and there is only a single object in each image. (2) The simple CNN baselines in MVCNN (Su et al., 2015) achieve higher performance than the proposed model, which is more complicated. (3) The paper seems unfinished. It mentions THOR and Active Vision, but there is no quantitative or qualitative results on them. (4) Some of the implementation details are unclear.

comments:

- It is unfair to use (Ammirato et al., 2017) as the citation for active vision. Active vision has been around for decades.

- It is not clear how the hierarchical soft-max layers have been implemented. There cannot be two consecutive soft-max layers. Also, for example, we cannot select an action from A, and then select an action from C since the operation is not differentiable. This should be clarified in the rebuttal.

- In Table 3, why is there a difference between the performance with and without LSTM in the first column? The LSTM does not see any history at the first step so the performance should be the same in both cases.

- According to Table 1 of MVCNN (Su et al., 2015), a simple CNN with one view as input achieves 83% accuracy (w/o fine-tuning), which is higher than the performance of the proposed method.

- It is better not to call the approach navigation. It is just changing the azimuth of the camera view.","[4, 3, 6]","[' Ok but not good enough - rejection', ' Clear rejection', ' Marginally above acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer highlights several significant weaknesses, pointing out methodological issues, lack of strong results, and unclear aspects in the paper. While they find the core idea interesting, the numerous concerns suggest a negative sentiment. However, the language remains professional and avoids harsh or disrespectful phrasing, indicating a neutral politeness level.",-40.0,0.0
Recurrent Auto-Encoder Model for Multidimensional Time Series Representation,"['Timothy Wong', 'Zhiyuan Luo']",Reject,2018,"[14, 29]","[19, 34]","[7, 76]","[4, 43]","[1, 7]","[2, 26]","The paper describes a sequence to sequence auto-encoder model which is used to learn sequence representations. The authors show that for their application, better performance is obtained when the network is only trained to reconstruct a subset of the data measurements. The paper also presents some visualizations the similarity structure of the learned representations and proposes a window-based method for processing the data.

According to the paper, the experiments are done using a data set which is obtained from measurements of an industrial production process. Figure 2 indicates that reconstructing fewer dimensions of this dataset leads to lower MSE scores. I don’t see how this is showing anything besides the obvious fact that reconstructing fewer dimensions is an easier task than reconstructing all of them.  The only conclusions I can draw from the visual analysis is that the context vectors are more similar to each other when they are obtained from time steps in the data stream which are close to each other. Since the paper doesn’t describe much about the privately owned data at all, there is no possibility to replicate the work. The paper doesn’t frame the work in prior research at all and the six papers it cites are only referred to in the context of describing the architecture.

I found it very hard to distil what the main contribution of this work was according to the paper. There were also not many details about the precise architecture used. It is implied that GRU networks and were used but the text doesn’t actually state this explicitly. By saying so little about the data that was used, it was also not clear what the temporal correlations of the context vectors are supposed to tell us. 

The paper describes how existing methods are applied to a specific data set. The benefit of only reconstructing a subset of the input dimensions seems very data specific to me and I find it hard to consider this a novel idea by itself. Presenting sequential data in a windowed format is a standard procedure and not a new idea either. All in all I don't think that the paper presents any new ideas or interesting results.

Pros:
* The visualizations look nice.

Cons:
* It is not clear what the main contribution is.
* Very little information about the data. 
* No clear experiments from which conclusions can be drawn.
* No new ideas.
* Not well rooted in prior work.
","[2, 2, 4]","[' Strong rejection', ' Strong rejection', ' Ok but not good enough - rejection']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is overwhelmingly negative. The reviewer finds little to no scientific value in the paper, questioning the methodology, novelty, and even the clarity of the presented work.  While the language avoids overtly aggressive terms, the criticisms are sharp, indicating a lack of politeness. The reviewer lists several 'cons' without a single redeeming 'pro' besides the visual presentation.",-75.0,-20.0
GATED FAST WEIGHTS FOR ASSOCIATIVE RETRIEVAL,"['Imanol Schlag', 'Jürgen Schmidhuber']",Reject,2018,"[2, 29]","[7, 34]","[19, 440]","[8, 232]","[11, 132]","[0, 76]","The authors present an evolution of the idea of fast weights: training a double recurrent neural network, one ""slow"" trained as usual and one ""fast"" that gets updated in every time-step based on the slow network. The authors generalize this idea in a nice  way and present results on 1 experiment. On the positive side, the paper is clearly written and while the fast-weights are not new, the details of the presented method are original. On the negative side, the experimental results are presented on only 1 experiment with a data-set and task made up by the authors. The results are good but the improvements are not too large, and they are measured over weak baselines implemented by the authors. For a convincing result, one would require an evaluation on a number of tasks, including long-studied ones like language modeling, and comparison to stronger related models, such as the Neural Turing Machine or the Transformer (from ""Attention is All You Need""). Without comparison to stronger baselines and with results only on 1 task constructed by the authors, we have to recommend rejection.","[3, 5, 4]","[' Clear rejection', ' Marginally below acceptance threshold', ' Ok but not good enough - rejection']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with a neutral summary of the paper's contributions. While acknowledging the originality and clarity of the work, the reviewer then raises significant concerns about the limited experimental validation, lack of comparison with established baselines, and reliance on a single, potentially contrived, dataset. The recommendation for rejection further underscores the negative sentiment. However, the language remains professional and avoids harsh or disrespectful phrasing.",-40.0,60.0
Toward learning better metrics for sequence generation training with policy gradient,"['Joji Toyama', 'Yusuke Iwasawa', 'Kotaro Nakayama', 'Yutaka Matsuo']",Reject,2018,"[3, 8, 14, 21]","[3, 13, 18, 26]","[2, 56, 44, 252]","[1, 29, 32, 156]","[1, 16, 6, 52]","[0, 11, 6, 44]","This paper describes an approach to generating time sequences by learning state-action values, where the state is the sequence generated so far, and the action is the choice of the next value.  Local and global reward functions are learned from existing data sequences and then the Q-function learned from a policy gradient.

Unfortunately, this description is a little vague, because the paper's details are quite difficult to understand.  Though the approach is interesting, and the experiments are promising, important explanation is missing or muddled.  Perhaps most confusing is the loss function in equation 7, which is quite inadequately explained.

This paper could be interesting, but substantial editing is needed before it is sufficient for publication.","[4, 7, 7]","[' Ok but not good enough - rejection', ' Good paper, accept', ' Good paper, accept']","[3, 1, 3]","[' The reviewer is fairly confident that the evaluation is correct', "" The reviewer's evaluation is an educated guess"", ' The reviewer is fairly confident that the evaluation is correct']","The review starts with a cautiously positive note by acknowledging the interesting approach and promising results. However, it quickly points out significant weaknesses regarding clarity and explanation, particularly with the loss function. The reviewer suggests substantial editing is needed, indicating significant revisions are required. Overall, the sentiment leans towards the negative side due to the severity of the clarity issues. The language used is professional and avoids harsh or disrespectful language.",-30.0,60.0
Towards Unsupervised Classification with Deep Generative Models,"['Dimitris Kalatzis', 'Konstantia Kotta', 'Ilias Kalamaras', 'Anastasios Vafeiadis', 'Andrew Rawstron', 'Dimitris Tzovaras', 'Kostas Stamatopoulos']",Reject,2018,"[3, 4, 5, 8, 'no_match', 'no_match', 6]","[7, 1, 10, 12, 'no_match', 'no_match', 10]","[9, 1, 27, 15, 'no match', 'no match', 18]","[4, 1, 17, 10, 'no match', 'no match', 9]","[5, 0, 0, 2, 'no match', 'no match', 0]","[0, 0, 10, 3, 'no match', 'no match', 9]","This paper addresses the question of unsupervised clustering with high classification performance. They propose a deep variational autoencoder architecture with categorical latent variables at the deepest layer and propose to train it with modifications of the standard variational approach with reparameterization gradients. The model is tested on a medical imagining dataset where the task is to distinguish healthy from pathological lymphocytes from blood samples. 

I am not an expert on this particular dataset, but to my eye the results look impressive. They show high sensitivity and high specificity. This paper may be an important contribution to the medical imaging community.

My primary concern with the paper is the lack of novelty and relatively little in the way of contributions to the ICLR community. The proposed model is a simple variant on the standard VAE models (see for example the Ladder VAE https://arxiv.org/abs/1602.02282 for deep models with multiple stochastic layers). This would be OK if a thorough evaluation on at least two other datasets showed similar improvements as the lymphocytes dataset. As it stands, it is difficulty for me to assess the value of this model.

Minor questions / concerns:

- The authors claim in the first paragraph of 3.2 that deterministic mappings lack expressiveness. Would be great to see the paper take this claim seriously and investigate it.
- In equation (13) it isn't clear whether you use q_phi to be the discrete mass or the concrete density. The distinction is discussed in https://arxiv.org/abs/1611.00712
- Would be nice to report the MCC in experimental results.","[4, 4, 4]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer finds the results impressive and potentially important for the medical imaging community (positive). However, they have concerns about the lack of novelty and limited evaluation beyond the medical dataset, which lessens their enthusiasm (negative). The reviewer uses phrases like ""My primary concern"" and ""it is difficult for me to assess"" which are direct but professional and polite.  Overall, the tone is balanced and professional.",40.0,70.0
Relational Multi-Instance Learning for Concept Annotation from Medical Time Series,"['Sanjay Purushotham', 'Zhengping Che', 'Bo Jiang', 'Tanachat Nilanon', 'Yan Liu']",Reject,2018,"[10, 5, 16, 3, 17]","[15, 10, 21, 2, 22]","[43, 42, 301, 2, 219]","[25, 21, 131, 2, 132]","[14, 18, 27, 0, 57]","[4, 3, 143, 0, 30]","This paper proposes a framework called 'multi-instance learning', in which a time series is treated as a 'set' of observations, and label is assigned to the full set, rather than individual observations. In this framework, authors propose to do set-level prediction (using pooling) and observation level predictions (using various attention mechanisms). 
They test their approach in a medical setting, where the goal is to annotate vital signs time series by clinical events. Their cohort is 2014 adults time series (average length 4 time steps), and their time series has dimension of 21 and their clinical events have dimension of 26. Their baselines are other 'multi-instance learning' prior work and results are achieved through cross-validation. A few of the relevant hyper-parameters are tuned and some important hyper-parameters (i.e. number of hidden states in the LSTMs, or optimization method and learning rate) are not tuned. 

Originality - I find the paper to be very incremental in terms of originality of the method. 

Quality and Significance - Due to small size of the cohort and lack of additional dataset, it is difficult to reliably access quality of experiments. Given that results are reported via cross-validation and without a true held-out dataset, and given that a number of hyperparameters are not even tuned, it is difficult to be confident that the differences of all the methods reported are significant. 

Clarity - The writing has good clarity.

Major issues with the paper: 
- Lack of reliable experiment section. Dataset is too small (2000 total samples), and model training is not described with enough details in terms of hyper-parameters tuned. 
","[3, 3, 6]","[' Clear rejection', ' Clear rejection', ' Marginally above acceptance threshold']","[3, 5, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer raises several major concerns, indicating a negative sentiment. They find the paper's originality lacking and criticize the experimental setup's quality and significance. While they acknowledge the writing's clarity, the overall tone suggests a negative view due to the significant methodological issues. The language used is formal and professional, without resorting to personal attacks or disrespectful phrasing.",-50.0,50.0
Generalized Graph Embedding Models,"['Qiao Liu', 'Xiaohui Yang', 'Rui Wan', 'Shouzhong Tu', 'Zufeng Wu']",Reject,2018,"['no_match', 'no_match', 12, 5, 1]","['no_match', 'no_match', 17, 6, 6]","['no match', 'no match', 12, 5, 4]","['no match', 'no match', 4, 4, 3]","['no match', 'no match', 4, 0, 0]","['no match', 'no match', 4, 1, 1]","The paper proposes a new method to compute embeddings of multirelational graphs. In particular, the paper proposes so-called E-Cells and R-Cells to answer queries of the form (h,r,?), (?,r,t), and(h,?,t). The proposed method (GEN), is evaluated on standard datasets for link prediction as well as datasets for node classification.

The paper tackles an interesting problem, as learning from graphs via embedding methods has become increasingly important. The experimental results of the proposed model, especially for the node classification tasks, look promising. Unfortunately, the paper makes a number of claims which are not justified or seem to result from misconceptions about related methods. For instance, the abstract labels prior work as ""ad hoc solutions"" and claims to propose a principled approach. However, I do not see how the proposed method is a more principled than previously proposed methods. For instance, methods such as RESCAL, TransE, HolE or ComplEx can be motivated as compositional models that reflect the compositional structure of relational data. Furthermore, RESCAL-like models can be linked to prior research in cognitive science on relational memory [3]. HolE explicitly motivates its modeling through its relation to models for associative memory. 

Furthermore, due to their compositional nature, these model are all able to answer the queries considered in the paper (i.e, (h,r,?), (h,?,t), (?,r,t)) and are implicitly trained to do so. The HolE paper discusses this for instance when relating the model to associative memory. For RESCAL, [4] shows how even more complicated queries involving logical connectives and quantification can be answered. It is therefore not clear how to proposed method improves over these models.

With regard to the evaluation: It is nice that the authors provided an evaluation which compares to several SOTA methods. However, it is unclear under which setting these results where obtained. In particular, how were the hyperparameter for each model chosen and which parameters ranges were considered in the grid search. Appendix B.2 in the supplementary seems to specify the parameter setting for GEN, but it is unclear whether the same parameters where chosen for the competing models and whether they were trained with similar methods (e.g., dropout, learning rate decay etc.). The big difference in performance of HolE and ComplEx is also surprising, as they are essentially the same model (e.g. see [1,2]). It is therefore not clear to me which conclusions we can draw from the reported numbers.

Further comments:
- p.3: The statement ""This is the actual way we humans learn the meaning of concepts expressed by a statement"" requires justification
- p.4: The authors state that the model is trained unsupervised, but eq. 10 clearly uses supervised information in form of labels.
- p.4: In 3.1, E-cells are responsible to answer queries of the form (h,r,?) and (?, r, t), while Section 3.2 says E-Cells are used to answer (h, ?, t). I assume in the later case, the task is actually to answer (h,r,?)?
- p.2: Making a closed-world assumption is quite problematic in this context, especially when taking a principled approach. Many graphs such as Freebase are very incomplete and make an explicit open-world assumption. 
- The paper uses a unusual definition of one-shot/multi-shot learning, which makes it confusing to read at first. The authors might consider using different terms to improve readability.
- Paper would benefit if the model is presented earlier. GEN Cells are defined only in Section 3.2, but the model is discussed earlier. Reversing the order might improve presentation.

[1] K. Hayashi et al: ""On the Equivalence of Holographic and Complex Embeddings for Link Prediction"", 2017
[2] T.Trouillon et al: ""Complex and holographic embeddings of knowledge graphs: a comparison"", 2017
[3] G. Halford et al: ""Processing capacity defined by relational complexity: Implications for comparative, developmental, and cognitive psychology"", 1998.
[4] D. Krompaß et al: ""Querying factorized probabilistic triple databases"", 2014","[3, 4, 6]","[' Clear rejection', ' Ok but not good enough - rejection', ' Marginally above acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is quite critical of the paper, pointing out several flaws in the claims and methodology. While it acknowledges the promising results, it raises serious concerns about the justification of the approach and the validity of the comparisons. The reviewer also points out factual errors and inconsistencies. The tone, while direct and critical, maintains a professional and academic decorum.",-30.0,60.0
Learning Gaussian Policies from Smoothed Action Value Functions,"['Ofir Nachum', 'Mohammad Norouzi', 'George Tucker', 'Dale Schuurmans']",Reject,2018,"[3, 10, 6, 30]","[8, 15, 10, 35]","[104, 136, 75, 286]","[42, 59, 34, 182]","[61, 72, 39, 82]","[1, 5, 2, 22]","This paper explores the idea of using policy gradients to learn a stochastic policy on complex control problems.  The central idea is to frame learning in terms of a new kind of Q-value that attempts to smooth out Q-values by framing them in terms of expectations over Gaussian policies.

To be honest, I didn't really ""get"" this paper.
* As far I understand, all of the original work policy gradients involved stochastic policies.  Many are/were Gaussian.
* All Q-value estimators are designed to marginalize out the randomness in these stochastic policies.
* As far as I can tell, this is equivalent to a slightly different formulation, where the agent emits a deterministic action (\mu,\Sigma) and the environment samples an action from that distribution.  In other words, it seems that if we just draw the box a bit differently, the environment soaks up the nondeterminism, instead of needing to define a new type of Q-value.

Ultimately, I couldn't discern /why/ this was a significant advance for RL, or even a meaningful new perspective on classic ideas.

I thought the little 2-mode MOG was a nice example of the premise of the model.

While I may or may not have understood the core technical contribution, I think the experiments can be critiqued: they didn't really seem to work out.  Figures 2&3 are unconvincing - the differences do not appear to be statistically significant.  Also, I was disappointed to see that the authors only compared to DDPG; they could have at least compared to TRPO, which they mention.  They dismiss it by saying that it takes 10 times as long, but gets a better answer - to which I respond, ""Very well, run your algorithm 10x longer and see where you end up!""  I think we need to see a more compelling demonstration of why this is a useful idea before it's ready to be published.

The idea of penalizing a policy based on KL-divergence from a reference policy was explored at length by Bert Kappen's work on KL-MDPs.  Perhaps you should cite that?
","[5, 6, 6]","[' Marginally below acceptance threshold', ' Marginally above acceptance threshold', ' Marginally above acceptance threshold']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer clearly states that they ""didn't really 'get' this paper"", which indicates a lack of understanding and therefore makes it hard to be positive. They find the experiments unconvincing and call for more compelling demonstrations of the proposed idea. While the reviewer does offer some constructive criticism and suggestions, the overall tone is rather skeptical and leans towards the negative. However, the language used is professional and avoids harsh or disrespectful vocabulary, staying within the bounds of academic politeness.",-30.0,60.0
Word2net: Deep Representations of Language,"['Maja Rudolph', 'Francisco Ruiz', 'David Blei']",Reject,2018,"[4, 'no_match']","[9, 'no_match']","[31, 'no match']","[10, 'no match']","[18, 'no match']","[3, 'no match']","The paper extends SGNS as follows. In SGNS, each word x is associated with vectors a_x and r_x. Given a set of context words C, the model calculates the probability that the target word is x by a dot product between a_x and the average of {r_c: c in C}.  The paper generalizes this computation to an arbitrary network: now each word x is associated with some network N_x whose input is a set of context words C and the output is the aforementioned probability. This is essentially an architectural change: from a bag-of-words model to a (3-layer) feedforward model. 

Another contribution of the paper is a new form of regularization by tying a subset of layers between different N_x. In particular, the paper considers incorporating POS tags by tying within each POS group. For instance, the parameters of the first layer are shared across all noun words. (This assumes that POS tags are given.)

While this is a natural extension to word2vec, the reviewer has some reservations about the execution of this work. Word embeddings are useful in large part because they can be used to initialize the parameters of a network. None of the chosen experiments shows this. Improvement in the log likelihood over SGNS is somewhat obvious because there are more parameters. The similarity between ""words"" now requires a selection of context vectors (7) which is awkward/arbitrary. The use of POS tags is not very compelling (though harmless). It's not necessary: contrary to the claim in the paper, word embeddings captures syntactic information if the context width is small and/or context information is provided. A more sensible experiment would be to actually plug in the entire pretrained word nets into an external model and see how much they help. 

EDIT: It's usually the case that even if the number of parameters is the same, extra nonlinearity results in better data fitting (e.g., Berg-Kirkpatrick et al, 2010), it's still not unexpected. 

All of this is closely addressed in the following prior work: 

Learning to Embed Words in Context for Syntactic Tasks (Tu et al., 2017)

Quality: Natural but questionable extension, see above. 

Clarity: Clear. 

Originality: Acceptable, but a very similar idea of embedding contexts is presented in Tu et al. (2017) which is not cited. 

Significance: Minor/moderate, see above. 

","[5, 4, 4]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer acknowledges the paper as a ""natural extension"" but expresses reservations about its execution and significance. They find the improvements somewhat obvious and the experiments lacking, suggesting more impactful ways to validate the approach. The reviewer also points out closely related prior work that diminishes the originality aspect. The tone, while critical, maintains a professional and analytical approach without resorting to personal attacks or disrespectful language.",-20.0,60.0
Egocentric Spatial Memory Network,"['Mengmi Zhang', 'Keng Teck Ma', 'Joo Hwee Lim', 'Shih-Cheng Yen', 'Qi Zhao', 'Jiashi Feng']",Reject,2018,"[5, 19, 27, 23, 4, 9]","[10, 20, 31, 27, 9, 14]","[30, 23, 252, 54, 59, 542]","[8, 14, 193, 34, 25, 214]","[20, 3, 19, 3, 22, 235]","[2, 6, 40, 17, 12, 93]","The paper is well written, well-motivated and the idea is very interesting for the computer vision and robotic communities. The technical contribution is original. The vision-based agent localization approach is novel compared to the methods of the literature. However, the experimental validation of the proposed approach could be more convincing (e.g. by testing on real data, with different training and testing splitting configurations).  

Major concern: 
1) The authors depict in section 2 “there is no existing end-to-end neural network for visual SLAM to our best knowledge” but they should discuss the positioning with respect to the paper of M Garon and JF Lalonde, “Deep 6-DOF Tracking”, ISMAR 2017 which propose a fully neural network based camera tracking method.

Minor concerns:
2) Table 3: the comparison is not rigorous in the sense that the proposed method estimates a 2D pose (3-DOF) while ORB-SLAM and EKF-SLAM are methods designed for 3D pose estimation (6-DOF). Is it possible to generalize your method to this case (6-DOF) for a more consistent comparison? At least, the fact that your method is more restrictive should be discussed in the paper. 

3) In the same vein than point 2), ORB-SLAM and EKF-SLAM are methods based on regression while the proposed method is restricted to the classification pose estimation. Is it possible to test your method with a regression task? 

4) It would be interesting to test the proposed method on real data to measure its robustness in terms of noise sensor and in terms of motion blur.

5) It would also be interesting to test the proposed method on datasets usually used in the SLAM community (e.g. using the sequences of the odometry benchmark of KITTI dataset).

6) In the SLAM context, the running time aspect on the test phase is crucial. Hence, the authors should compare the running time of their method with algorithms of literature (e.g. ORB-SLAM). 

","[5, 4, 3]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Clear rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer starts with positive affirmations about the paper's quality, writing, and novelty ('well written', 'well-motivated', 'very interesting', 'original', 'novel'). This suggests a positive sentiment. However, the review also raises several concerns regarding the experimental validation and comparison with other methods, indicating that the reviewer sees areas for improvement. While the reviewer points out limitations and suggests further experiments, the language remains constructive and polite throughout, using phrases like 'could be more convincing', 'it would be interesting', 'the authors should consider'. Therefore, the politeness score leans towards positive. Overall, the sentiment is positive due to the initial praise, but the numerous suggestions for improvement moderate it.",60.0,70.0
Exploring the Hidden Dimension in Accelerating Convolutional Neural Networks,"['Zhihao Jia', 'Sina Lin', 'Charles R. Qi', 'Alex Aiken']",Reject,2018,"[7, 7, 4, 33]","[12, 11, 9, 38]","[45, 10, 59, 217]","[24, 7, 28, 169]","[17, 3, 30, 20]","[4, 0, 1, 28]","The paper proposes an approach that offers speedup on common convolutional neural networks. It presents the approach well and shows results comparing with other popular frameworks used in the field.

Originality
- The automation of parallelism across the different dimensions in each of the layers appears somewhat new. Although parallelism across each of the individual dimensions has been explored (batch parallel is most common and best supported, height and width is discussed at least in the DistBelief paper), automatically exploring this to find the most efficient approach is new. The splitting across channels seems not to have been covered in a paper before.

Significance
- Paper shows a significant speedup over existing approaches on a single machine (16 GPUs). It is unclear how well this would translate across machines or to more devices, and also on newer devices - the experiments were all done on 16 K80s (3 generations old GPUs). While the approach is interesting, its impact also depends on the speedup on the common hardware used today.

Pros:
- Providing better parallelism opportunities for convolutional neural networks
- Simple approach to finding optimal global configurations that seems to work well
- Positive results with significant speedups across 3 different networks

Cons:
- Unclear if speedups hold on newer devices
- Useful to see how this scales across more than 1 machine
- Claim on overlapping computation with data transfer seems incorrect. I am pretty sure TensorFlow and possibly PyTorch supports this.

Questions:
- How long does finding the optimal global configuration take for each model?
","[5, 4, 7]","[' Marginally below acceptance threshold', ' Ok but not good enough - rejection', ' Good paper, accept']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is overall positive. The reviewer acknowledges the novelty and significance of the work, highlighting the speedup achieved. While they point out limitations and ask for clarifications, these are presented constructively and aim to improve the paper. The language is formal and polite throughout.",60.0,80.0
Dependent Bidirectional RNN with Extended-long Short-term Memory,"['Yuanhang Su', 'Yuzhong Huang', 'C.-C. Jay Kuo']",Reject,2018,"[4, 3, 32]","[5, 7, 37]","[10, 14, 1008]","[3, 6, 568]","[4, 7, 154]","[3, 1, 286]","
This paper introduces a different form of Memory cell for RNN which has more capabilities of long-term memorizing. Furthermore, it presents and efficient architecture for sequence-to-sequence mapping.

While the claim of the paper sounds very ambitious and good, the paper has several flaws. First of all, the mathematical analysis is a bit problematic. Of course, it is known that Simple Recurrent Networks (SRN) have a vanishing gradient problem. However, the way you proof it is not correct, as you ignore the application of f() for calculating the output (which is routed to the input) and you use an upper bound to show a general behaviour.
The analysis of the Memory capabilities of LSTM is a bit simplified, however, it is okay. Note, that various experiments by Schmidhuber's group, as well as Otte & al have shown that LSTM can generalize and memorize to sequences of more than a million time steps, if the learning rates is small enough.

The extended memory which the authors call SLSTM-I has similar memory capabilities as LSTM. The other one (SLSTM-II) looses the capability of forgetting as it seems. An analysis would be crucial in this paper to show the benefits mathematically. 

The authors should have a look at ""Evolving memory cell structures for sequence learning"" by Justin Bayer, Daan Wierstra, Julian Togelius and J¨urgen Schmidhuber, published in 2009. Note that the SLSTM belongs to the family of networks which could be generated by that paper as well.

Also ""Neural Architecture Search with Reinforcement Learning"" by Barret Zoph and Quoc V. Le would be interesting.

In your experiments it would be fair to compare to Cheng et al. 2016

I suggest the authors being more modest with the name of the memory cell as well as with the abstract (especially in the POS experiment, SLSTM is not superior)","[4, 3, 4]","[' Ok but not good enough - rejection', ' Clear rejection', ' Ok but not good enough - rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with a positive note, acknowledging the ambition and potential of the paper's idea. However, it quickly delves into a series of criticisms regarding the mathematical analysis, comparison with existing models, and even the naming choice. The reviewer suggests other publications to consider and points out flaws in the experiments. While the language is quite direct, it avoids harsh or disrespectful phrasing, maintaining a professional tone. Therefore, the sentiment leans towards the negative side due to the significant concerns raised, but the politeness remains relatively neutral.",-30.0,40.0
Gaussian Prototypical Networks for Few-Shot Learning on Omniglot,['Stanislav Fort'],Reject,2018,[2],[6],[33],[9],[24],[0],"This paper presents an interesting extension to Snell et al.'s prototypical networks, by introducing uncertainty through a parameterised estimation of covariance along side the image embeddings (means). Uncertainty may be particularly important in the few-shot learning case this paper examines, when it is helpful to extract more information from limited number of input samples.

However, several important concepts in the paper are not well explained or motivated. For example, it is a bit misleading to use the word ""covariance"" throughout the paper, when the best model only employs a scalar estimate of the variance. A related, and potentially technical problem is in computing the prototype's mean and variance (section 3.3). Eq. 5 and 6 are not well motivated, and the claim of ""optimal"" under eq.6 is not explained. More importantly, eq. 5 and 6 do not use any covariance information (off-diagonal elements of S) --- as a result, the model is likely to ignore the covariance structure even when using full covariance estimate. The distance function (eq. 4) is d Mahalanobis distance, instead of ""linear Euclidean distance"". While the paper emphasises the importance of the form of loss function, the loss function used in the model is given without explanation (and using cross-entropy over distances looks hacky).

In addition, the experiments are too limited to support the claimed benefits from encoding uncertainty. Since the accuracies on omniglot data from recent models are already close to perfect, it is unclear whether the marginally improved number reported here is significant. In addition, more analysis may better support existing claims. For example, showing subsampled images indeed had higher uncertainty, rather than only the histogram for all data points.

Pros:
-Interesting problem and interesting direction.
-Considers a number of possible alternative models
-Intuitive illustration in Fig. 1

Cons:
-Misleading use of ""covariance""
-The several important concepts including prototype mean/variance, distance, and loss are not well motivated or explained
-Evaluation is too limited","[4, 3, 3]","[' Ok but not good enough - rejection', ' Clear rejection', ' Clear rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with positive sentiment, acknowledging the interesting aspects of the paper. However, it quickly transitions into a  critical analysis, pointing out significant flaws in the methodology and explanation. The reviewer also expresses concerns about the limited experimental validation. While the language is direct and critical, it maintains a professional and academic tone without resorting to personal attacks or disrespectful language.",20.0,60.0
Demystifying overcomplete nonlinear auto-encoders: fast SGD convergence towards sparse representation from random initialization,"['Cheng Tang', 'Claire Monteleoni']",Reject,2018,"[18, 21]","[23, 24]","[50, 44]","[24, 28]","[5, 8]","[21, 8]","The authors study the convergence of a procedure for learning
an autoencoder with a ReLu non-linearity.  The procedure is akin
to stochastic gradient descent, with some parameters updated at
each iteration in a manner that performs optimization with respect
to the population risk.

The autoencoders that they study tie the weights of the decoder to
the weights of the encoder, which is a common practice.  There
are no bias terms in the decoder, however.  I do not see where they
motivate this restriction, and it seems to limit the usefulness of
the bias terms in the encoder.

Their analysis is with respect to a mixture model.  This is described
in the abstract as a sparse dictionary model, which it is, I guess.
They assume that the gaussians are very well separated.  

The statement of Theorem says that it concerns Algorithm 1.  The
description of Algorithm 1 describes a procedure, with an
aside that describes a ""version used in the analysis"".

They write in the text that the rows of W^t are projected onto
a ball of radius c in each update, but this is not included
in the description of Algorithm 1.  The statement of Theorem 1
includes the condition that all rows of W^t are always equal to
c, but this may not be consistent with the updates given
in Algorithm 1.  My best guess is that they intend of
the rows of W^t to be normalized after each update (which is
different than projecting onto the ball of radius c).  This
aspect of their procedure seems restrict its applicability.

Successful initialization looks like a very strong condition to
me, something that will occur exponentially rarely, as a function
of d. (See Fact 10 of ""Agnostically learning halfspaces"", by Kalai, et al.)
For each row of W^*, the probability that any one row of W^o will be
close enough is exponentially small, so exponentially many rows
are needed for the probability that any row is close enough to
be, say, 1/2.  I don't see anything in the conditions of Theorem 1
that says that n is large relative to d, so it seems like its
claim includes the case where k and n are constants, like 5.
But, in this case, it seems like the claim of the probability
of successful initialization cannot be correct when d is large.

It looks like, after ""successful initialization"", especially
given the strong separation condition, the model as already
""got it"".  In particular, the effect of the ReLUs seems to
be limited in this regime.

I have some other concerns about correctness, but I do not think
that the paper can be accepted even if they are unfounded.

The exposition is uneven.  They tell us that W^T is the transpose
of W, but do not indicate that 1_{a^t (x') > 0} is a componentwise
indicator function, and that x' 1_{a^t (x') > 0} is its
componentwise product with x' (if this is correct).


","[2, 2, 3]","[' Strong rejection', ' Strong rejection', ' Clear rejection']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review is highly critical of the paper, pointing out several flaws in the methodology, assumptions, and presentation. The reviewer finds the paper's core ideas questionable and believes the paper is not publishable in its current state. The language used, while direct and critical, maintains a professional and academic tone.",-75.0,50.0
Learning Representations for Faster Similarity Search,"['Ludwig Schmidt', 'Kunal Talwar']",Reject,2018,"[5, 18]","[10, 23]","[104, 182]","[50, 98]","[50, 67]","[4, 17]","This paper investigates learning representations for the problem of nearest neighbor (NN) search by exploring various deep learning architectural choices. The crux of the paper is the connection between NN and the angles between the closest neighbors -- the higher this angle, more data points need to be explored for finding the nearest one, and thus more computational expense. Thus, the paper proposes to learn a network that tries to reduce the angles between the inputs and the corresponding class vectors in a supervised framework using softmax cross-entropy loss. Three architectural choices are investigated, (i) controlling the norm of output layers of the CNN (using batch norm essentially), (ii) removing relu so that the outputs are well-distributed in both positive and negative orthants, and (iii) normalizing the class vectors. Experiments are given on multiMNIST and Sports 1M and show improvements. 

Pros:
1) The paper explores different architectural choices for the deep network to some depth and show extensive results.
2) The results do demonstrate clearly the advantage of the various choices and is useful
3) The theoretical connections between data angles and query times are quite interesting,

Cons:
1) Unclear Problem Statement. 
I find the problem statement a bit vague. Standard NN search finds a data point in the database closest to a query under some distance metric. While, the current paper uses the cosine similarity as the distance, the deep framework is trained on class vectors using cross-entropy loss. I do not think class labels are usually assumed to be given in the standard definition of NN, and it is not clear to me how the proposed setup can accommodate NN without class labels.  Thus as such, I see this paper is perhaps proposing a classification problem and not an NN problem per se. 

2) Lacks Focus
The paper lacks a good organization in my opinion. Things that are perhaps technically important are moved to the Appendix. For example, I find the theoretical part of the paper (e.g., Theorem 1) quite elegant and perhaps the main innovation in this paper. However, that is moved completely to the Appendix. So it cannot be really considered a contribution. It is also not clear if those theoretical results are novel. 

2) Disconnect/Unclear Assumptions
There seems to be some disconnect between LSH and deep learning architectures explored in Sections 2 and 3 respectively. Are the assumptions used in the theoretical results for LSH also assumed in the deep networks? For example, as far as I know, the standard LSH works assumes the projection hyperplanes are randomly chosen and the theoretical results are based on such assumptions. It is not clear how a softmax output of a CNN, which is trained in a supervised way, follow such assumptions. It would be important if the paper could clarify such assumptions to make sure the sections are congruent. 

3) No Related Work
There have been several efforts for adapting deep frameworks into KNN. The paper ignores all such works. Thus, it is not clear how significant is the proposed contribution. There are also not comparisons what-so-ever to competitive prior works.

4) Novelty
The main contribution of this paper is basically a set of experiments looking into architectural choices. However, the results of this study do not provide any surprises. It appears that batch normalization is essential for good performances, while using RELU is not so when one wants to use all directions for effective data encoding. Thus, as such, the novelty or the contributions of this paper are minor.

Overall, while I find there are some interesting theoretical bits in this paper, it lacks focus, the experiments do not offer any surprises, and there are no comparisons with prior literature. Thus, I do not think this paper is ready to be accepted in its present form.
","[4, 4, 4]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[5, 4, 5]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The reviewer raises several significant concerns about the paper, including a vague problem statement, lack of focus, disconnect between theory and experiments, absence of related work, and limited novelty. While acknowledging some interesting theoretical aspects, the reviewer finds the overall contribution minor and recommends rejecting the paper in its current form. The tone is critical but professional, without resorting to personal attacks or disrespectful language.",-50.0,50.0
Discovery of Predictive Representations With a Network of General Value Functions,"['Matthew Schlegel', 'Andrew Patterson', 'Adam White', 'Martha White']",Reject,2018,"[2, 16, 13, 10]","[7, 21, 18, 15]","[13, 30, 67, 135]","[6, 13, 22, 54]","[5, 13, 36, 70]","[2, 4, 9, 11]","I have to say that I do not have all the background of this paper, and the paper is not written very clearly. I think the major contribution of the paper is represented in a very vague way.","[4, 5, 4]","[' Ok but not good enough - rejection', ' Marginally below acceptance threshold', ' Ok but not good enough - rejection']","[1, 4, 4]","["" The reviewer's evaluation is an educated guess"", ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer expresses uncertainty and a lack of clarity understanding (""I do not have all the background...""). They also find the contribution unclear (""major contribution... very vague""). This suggests a negative, but not overly harsh, sentiment. The language is quite indirect in its criticism, suggesting politeness.",-60.0,60.0
Understanding and Exploiting the Low-Rank Structure of Deep Networks,"['Craig Bakker', 'Michael J. Henry', 'Nathan O. Hodas']",Reject,2018,"[2, 7, 9]","[6, 12, 12]","[17, 11, 52]","[5, 7, 16]","[5, 3, 35]","[7, 1, 1]","[Main comments]

* The authors made a really odd choice of notation, which made the equations hard to follow.
Apparently, that notation is used in differential geometry, but I have never seen it used in
an ML paper. If you talk about outer product structure, show some outer products!

* The function f that the authors differentiate is not even defined in the main manuscript!

* The low-rank structure they describe only holds for a single sample at a time.
I don't see how this would be ""understanding low rank structure of deep networks""
as the title claims... What is described is basically an implementation trick.

* Introducing cubic regularization seems interesting. However, either some
extensive empirical evidence or some some theoretical evidence that this is useful are needed.
The present paper has neither (the empirical evidence shown is very limited).

[Other minor comments]

* Strictly speaking Adagrad has not been designed for Deep Learning.
It is an online algorithm that became popular in the DL community later on.

* ""Second derivatives should suffice for now, but of course if a use arose for
third derivatives, calculating them would be a real option""

That sentence seems useless.

* Missing citation:

Gradient Descent Efficiently Finds the Cubic-Regularized Non-Convex Newton Step. 
Yair Carmon, John Duchi.
","[4, 2, 5]","[' Ok but not good enough - rejection', ' Strong rejection', ' Marginally below acceptance threshold']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is highly critical of the paper, pointing out major flaws in notation, missing definitions, and lack of both theoretical and empirical support for the main claims. The reviewer also questions the paper's relevance to its claimed topic. While the language is quite direct, it avoids personal attacks and maintains a professional tone.",-70.0,40.0
Feat2Vec:  Dense Vector Representation for Data with Arbitrary Features,"['Luis Armona', 'José P. González-Brenes', 'Ralph Edezhath']",Reject,2018,"[0, 9, 1]","[3, 10, 2]","[2, 20, 2]","[2, 19, 2]","[0, 0, 0]","[0, 1, 0]","This paper provides a clean way of learning embeddings for structured features that can be discrete -- indicating presence / absence of a certain quality. Further, these features can be structured i.e. a set of them are of the same 'type'. Unlike, word2vec there is no hard constraint that similar objects must have similar representations and so, the learnt embeddings reflect the likelihood of the observed features. Therefore, this can be used as a multi-label classifier by using two feature types -- the input and the set of categories. This proposed scheme is evaluated on two datasets -- movies and education in a retrieval setting. 

I would like to see an evaluation of these features in a classification setting to further demonstrate the utility of these embeddings as compared to directly embedding the discrete features and then performing a K-way classification. For example, I am aware of -- http://manikvarma.org/downloads/XC/XMLRepository.html contains some interesting datasets which have a large number of discrete features and classes. ","[7, 2, 7]","[' Good paper, accept', ' Strong rejection', ' Good paper, accept']","[5, 2, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is fairly confident that the evaluation is correct']","The review starts with positive statements, highlighting the novelty and cleanliness of the approach. It then provides constructive feedback by suggesting an additional evaluation setting. The language used is formal and suggestive, not demanding, which points to politeness.",60.0,70.0
Faster Distributed Synchronous SGD with Weak Synchronization,"['Cong Xie', 'Oluwasanmi O. Koyejo', 'Indranil Gupta']",Reject,2018,"[6, 10, 19]","[11, 15, 24]","[67, 156, 160]","[24, 71, 108]","[20, 71, 18]","[23, 14, 34]","This paper introduces a parameter server architecture to improve distributed training of CNNs in the presence of stragglers. Specifically, the paper proposes partial pulling where a worker only waits for first b blocks rather than all the blocks of the parameters. This technique is combined with existing methods such as partial pushing (Pan et. al. 2017) for a partial synchronous SGD method. The method is evaluated with Resnet -50 using synthetic delays.

Comments for the author:

The paper is well-written and easy to follow. The problem of synchronization costs being addressed is important but it is unclear how much of this is arising due to large blocks.

1) The partial pushing method (Pan et. al. 2017, section 3.1) shows a clear evidence for the problem using a real workload with a large number of workers. Unfortunately, in your Figure 2, this is not as obvious and not real since it is using simulated delays. More specifically, it is not clear how the workers behave in a real environment and whether you get a clear benefit from using a partial number of blocks as opposed to sending all of them. 

2) Did you modify your code to support block-wise sending of gradients (some description of how the framework was modified will be helpful)? The idea is to send partial parameter blocks and when 'b' blocks are received, compute the gradients. I feel that, with such a design, you may actually end up hurting the performance by sending a large number of small packets in the no failure case. For real, large data centers, this may cause a packet storm and subsequent throughput collapse (e.g. the incast problem). You need to show the evidence that you do not hurt the failure-free case for a large number of workers.

3) The evaluation is on fairly small workloads (CIFAR-10). Again, evaluating over Imagenet and demonstrating a clear speedup over existing sync methods will be helpful. Furthermore, a clear description of your “pull” configuration (such as in Figure 1) i.e. how many actual bytes or blocks are sent and what is the threshold will be helpful (beyond a vague 90%).

4) Another concern with partial synchronization methods that I have is that how do you pick these configurations (pull 0.75 etc). These appear to be dataset specific and finding the optimal configuration here requires significant experimentation that takes significantly more time than just running the baseline.

Overall, I feel there is not enough evidence for the problem specifically generating large blocks of gradients and this needs to be clearly shown. To propose a solution for stragglers, evaluation should be done in a datacenter environment with the presence of stragglers (and not small workloads with synthetic delays). Furthermore, the proposed technique despite the simplicity appears as a rather incremental contribution.","[4, 3, 4]","[' Ok but not good enough - rejection', ' Clear rejection', ' Ok but not good enough - rejection']","[5, 4, 5]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The reviewer acknowledges the importance of the problem tackled in the paper and finds the paper well-written and easy to follow. However, they raise several concerns regarding the evaluation methodology and the significance of the contribution. The reviewer points out the lack of real-world experiments and the use of synthetic delays, limited dataset size, and the need for more evidence to support the claims. The overall tone suggests that the reviewer leans towards recommending revisions rather than outright acceptance.",20.0,70.0
Alpha-divergence bridges maximum likelihood and reinforcement learning in neural sequence generation,"['Sotetsu Koyamada', 'Yuta Kikuchi', 'Atsunori Kanemura', 'Shin-ichi Maeda', 'Shin Ishii']",Reject,2018,"[5, 5, 12, 14, 24]","[10, 9, 15, 19, 29]","[8, 15, 48, 65, 189]","[2, 9, 28, 27, 88]","[6, 6, 3, 22, 12]","[0, 0, 17, 16, 89]","This paper considers a dichitomy between ML and RL based methods for sequence generation. It is argued that the ML approach has some ""discrepancy"" between the optimization objective and the learning objective, and the RL approach suffers from bad sample complexity. An alpha-divergence formulation is considered to combine both methods.

Unfortunately, I do not understand main points made in this paper and am thus not able to give an accurate evaluation of the technical content of this paper. I therefore have no option but to vote for reject of this paper, based on my educated guess. 

Below are the points that I'm particularly confused about:

1. For the ML formulation, the paper made several particularly confusing remarks. Some of them are blatantly wrong to me. For example, 

1.1 The q(.|.) distribution in Eq. (1) *cannot* really be the true distribution, because the true distribution is unknown and therefore cannot be used to construct estimators. From the context, I guess the authors mean ""empirical training distribution""?

1.2 I understand that the ML objective is different from what the users really care about (e.g., blue score), but this does not seem a ""discrepancy"" to me. The ML estimator simply finds a parameter that is the most consistent to the observed sequences; and if it fails to perform well in some other evaluation criterion such as blue score, it simply means the model is inadequate to describe the data given, or the model class is so large that the give number of samples is insufficient, and as a result one should change his/her modeling to make it more apt to describe the data at hand. In summary, I'm not convinced that the fact that ML optimizes a different objective than the blue score is a problem with the ML estimator.

In addition, I don't see at all why this discrepancy is a discrepancy between training and testing data. As long as both of them are identically distributed, then no discrepancy exists.

1.3 In point (ii) under the maximum likelihood section, I don't understand it at all and I think both sentences are wrong. First, the model is *not* trained on the true distribution which is unknown. The model is trained on an empirical distribution whose points are sampled from the true distribution. I also don't understand why it is evaluated using p_theta; if I understand correctly, the model is evaluated on a held-out test data, which is also generated from the underlying true distribution.

2. For the RL approach, I think it is very unclear as a formulation of an estimator. For example, in Eq. (2), what is r and what is y*? It is mentioned that r is a ""reward"" function, but I don't know what it means and the authors should perhaps explain further. I just don't see how one obtains an estimated parameter theta from the formulation in Eq. (2), using training examples.","[4, 4, 4]","[' Ok but not good enough - rejection', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[1, 5, 3]","["" The reviewer's evaluation is an educated guess"", ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer expresses strong confusion and disagreement with the paper's core arguments, ultimately leading to a reject recommendation. They find the paper's explanations unclear and some claims outright incorrect. This indicates a highly negative sentiment. However, the reviewer articulates their concerns in a professional, analytical manner, focusing on specific points of confusion and avoiding personal attacks. This suggests a neutral-to-polite tone despite the negative feedback.",-80.0,50.0
Towards Safe Deep Learning: Unsupervised Defense Against Generic Adversarial Attacks,"['Bita Darvish Rouhani', 'Mohammad Samragh', 'Tara Javidi', 'Farinaz Koushanfar']",Reject,2018,"[5, 2, 18, 19]","[10, 6, 22, 24]","[45, 33, 211, 295]","[21, 13, 104, 162]","[13, 13, 60, 72]","[11, 7, 47, 61]","Summary:
 The paper presents an unsupervised method for detecting adversarial examples of neural networks. The method includes two independent components: an ‘input defender’ which tried to inspect the input, and a ‘latent defender’ trying to inspect a hidden representation. Both are based on the claim that adversarial examples lie outside a certain sub-space occupied by the natural image examples, and modeling this sub-space hence enables their detection. The input defender is based on sparse coding, and the latent defender on modeling the latent activity as a mixture of Gaussians. Experiments are presented on MInst, Cifar10, and ImageNet.
 
-	Introduction: The motivation for detecting adversarial examples is not stated clearly enough. How can such examples be used by a malicious agent to cause damage to a system? Sketching some such scenarios would help the reader understand why the issue is practically important. I was not convinced it is. 
Page 4: 
-	Step 3 of the algorithm is not clear:
o	How exactly does HDDA model the data (formally) and how does it estimate the parameters? In the current version, the paper does not explain the HDDA formalism and learning algorithm, which is a main building block in the proposed system (as it provides the density score used for adversarial examples detection). Hence the paper cannot be read as a standalone document. I went on to read the relevant HDDA paper, but it is also not clear which of the model variants presented there is used in this paper.
o	What is the relation between the model learned at stage 2 (the centers c^i) and the model learnt by HDDA? Are they completely different models? Or are the C^I used when learning the HDDA model (and how)? 
If these are separate models, how are they used in conjunction to give a final density score? If I understand correctly, only the HDDA model is used to get the final score, and the C^i are only used to make the \phy(x) representation more class-seperable. Is that right?
-	Figure 4, b and c: it is not clear what the (x,y,z) measurements plotted in these 3D drawings are (what are the axis).
Page 5:
-	Section 2: the risk analysis is done in a standard Bayesian way and leads to a ratio of PDFs in equation 5. However, this form is not appropriate for the case presented at this paper, since the method presented only models one of these PDFs (Specifically p(x | W1)  - there is not generative model of p(x|W2)).  
-	The authors claim in the last sentence of the section that p(x|W2) is equivalent to 1-p(x|W1), but this is not true: these are two continuous densities, they do not sum to 1, and a model of p(x|W2) is not available (as far as I understand the method)
Page 6:
-	How is equation 7) optimized?
-	Which patchs are extracted from images, for training and at inference time? Are these patchs a dense coverage of the image? Sparsely sampled? Densely sampled with overlaps?
-	Its not clear enough what exactly is the ‘PSNR’ value which is used for the adversarial example detection, and what exactly is ‘profile the PSNR of legitimate samples within each class’. A formal definition of PSNR and’profiling’ is missing (does profiling simply mean finding a threshold for filtering?)
Page 7:
-	Figure 7 is not very informative. Given the ROC curves in figure 8  and table 1 it is redundant. 

Page 8:
-	The results in general indicate that the method is much better than chance, but it is not clear if it is practical, because the false alarm rates for high detection are quite high. For example on ImageNet, 14.2% of the innocent images are mistakenly rejected as malicious to get 90% detection rate. I do not think this working point is useful for a real application
-	Given the high flares alarm rate, it is surprising that experiments with multiple checkpoints are not presented (specifically as this case of multiple checkpoints is discussed explicitly in previous sections of the paper).  Experiments with multiple checkpoints are clear required to complete the picture regarding the empirical performance of this method
-	The experiments show that essentially, the latent defenders are stronger than the input defender in most cases. However, an ablation study of the latent defender is missing: Specifcially, it is not clear a) how much does stage 2 (model refinement with clusters)  contribute to the accuracy (how does the model do without it? And 3) how important is the HDDA and the specific variant used (which is not clear) important: is it important to model the Gaussians using a sub-space? Of which dimension?

Overall:
Pros:
-	 A nice idea with some novelty,  based on a non-trivial observation
-	The experimental results how the idea holds some promise
Cons
-	The method is not presented clearly enough: the main component modeling the network activity is not explained (the HDDA module used)
-	The results presented show that the method is probably not suitable for a practical application yet (high false alarm rate for good detection rate)
-	Experimental results are partial: results are not presented for multiple defenders, no ablation experiments


After revision:
Some of my comments were addressed, and some were not.
Specifically, results were presented for multiple defenders and some ablation experiments were highlihgted
Things not addressed:
 - The risk analysis is still not relevant. The authors removed a clearly flawed sentence, but the analysis still assumes that two densities (of 'good' and 'bad' examples) are modeled, while in the work presented only one of them is. Hence this analysis does not add anything to the paper-  it states a general case which does not fit the current scenario and its relation to the work is not clear. It would have been better to omit it and use the space to describe HDDA and the specific variant used in this work, as this is the main tool doing the distinction.

I believe the paper should be accepted.
","[7, 5, 3]","[' Good paper, accept', ' Marginally below acceptance threshold', ' Clear rejection']","[3, 3, 5]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review is mostly positive in its description of the paper's idea and potential. However, it also raises several significant concerns about the clarity of the methodology, the practicality of the results due to high false alarm rates, and the lack of certain experiments. The reviewer suggests the paper should be accepted after revision, indicating an overall positive sentiment, but not without reservations. The language used is formal, direct, and critical in an academic context, but not rude.",60.0,70.0
Small Coresets to Represent Large Training Data for Support Vector Machines,"['Cenk Baykal', 'Murad Tukan', 'Dan Feldman', 'Daniela Rus']",Reject,2018,"[5, -1, 13, 28]","[10, 4, 18, 33]","[35, 23, 124, 776]","[15, 7, 53, 489]","[16, 13, 47, 118]","[4, 3, 24, 169]","The paper studies the problem of constructing small coreset for SVM.
A coreset is a small subset of (weighted) points such that the optimal solution for the coreset is also a good approximation for the original point set. The notion of coreset was originally formulated in computational geometry by Agarwal et al.
(see e.g., [A])
Recently it has been extended to several clustering problems, linear algebra, and machine learning problems. This paper follows the important sampling approach first proposed in [B], and generalized by Feldman and Langberg. The key in this approach is to compute the sensitivity of points and bound the total sensitivity for the considered problem (this is also true for the present paper). For SVM, the paper presents a bad instance where the total sensitivity can be as bad as 2^d. Nevertheless,
the paper presents interesting upper bounds that depending on the optimal value and variance of the point set. The paper argues that in many data sets, the total sensitivity may be small, yielding small coreset. This makes sense and may have significant practical implications.

However, I have the following reservation for the paper.
(1) I don't quite understand the CHICKEN and EGG section. Indeed, it is unclear to me 
how to estimate the optimal value. The whole paragraph is hand-waving. What is exactly merge-and-reduce? From the proof of theorem 9, it appears that the interior point algorithm is run on the entire dataset, with running time O(n^3d). Then there is no point to compute a coreset as the optimal solution is already known.

(2) The running time of the algorithm is not attractive (in both theory and practice).
In fact, the experimental result on the running time is a bit weak. It seems that the algorithm is pretty slow (last in Figure 1). 

(3) The theoretical novelty is limited. The paper follows from now-standard technique for constructing coreset.

Overal, I don't recommend acceptance.

minor points:
It makes sense to cite the following papers where original ideas on constructing coresets were proposed initially.

[A]Geometric Approximation via Coresets
Pankaj K. Agarwal Sariel Har-Peled Kasturi R. Varadarajan

[B]Universal epsilon-approximators for integrals, by Langberg and Schulman

---------------------------------------------------------

After reading the response and the revised text, I understand the chicken-and-egg issue.
I think the experimental section is still a bit weak (given that there are several very competitive SVM algorithms that the paper didn't compare with).
I raised my score to 5. 

","[5, 7, 5]","[' Marginally below acceptance threshold', ' Good paper, accept', ' Marginally below acceptance threshold']","[4, 3, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer acknowledges the practical potential of the paper's findings, stating ""This makes sense and may have significant practical implications."" However, they express several reservations, particularly regarding the clarity of the ""CHICKEN and EGG"" section, the algorithm's efficiency, and the limited theoretical novelty. The reviewer ultimately recommends against acceptance, indicating a negative sentiment. While the reviewer provides constructive criticism and specific recommendations, the overall tone is rather critical, particularly in pointing out perceived flaws. The language remains professional and avoids personal attacks, indicating a neutral politeness level.",-30.0,0.0
Gated ConvNets for Letter-Based ASR,"['Vitaliy Liptchinsky', 'Gabriel Synnaeve', 'Ronan Collobert']",Reject,2018,"[7, 9, 18]","[11, 14, 23]","[25, 132, 125]","[11, 59, 70]","[12, 67, 47]","[2, 6, 8]","The paper describes some interesting work but for a combination of reasons I think it's more like a workshop-track paper.
There is not much that's technically new in the paper-- at least not much that's really understandable.   There is some text about a variant of CTC, but it does not explain very clearly what was done or what the motivation was.
There are also quite a few misspellings.  
Since the system is presented without any comparisons to alternatives for any of the individual components, it doesn't really shed any light on the significance of the various modeling decisions that were made.  That limits the value.
If rejected from here, it could perhaps be submitted as an ICASSP or Interspeech paper.","[6, 4, 3]","[' Marginally above acceptance threshold', ' Ok but not good enough - rejection', ' Clear rejection']","[5, 4, 5]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review starts with a backhanded compliment, saying the work is ""interesting"" but ultimately not suitable for the venue. The reviewer finds the technical contribution lacking clarity and novelty. Criticism about misspellings and lack of comparisons further points towards a negative sentiment. However, the language remains professional and avoids harsh or disrespectful phrasing, suggesting a more neutral politeness. The suggestion to submit to another venue slightly balances the negativity but doesn't erase the core criticism.",-50.0,20.0
From Information Bottleneck To Activation Norm Penalty,"['Allen Nie', 'Mihir Mongia', 'James Zou']",Reject,2018,"[2, 3, 9]","[7, 6, 14]","[21, 4, 208]","[7, 2, 78]","[12, 1, 109]","[2, 1, 21]","The paper puts forward Activation Norm Penalty (""ANR"", an L_2 type regularization on the activations), deriving it from the Information Bottleneck principle. As usual with Information Bottleneck style constructions, the loss takes on a variational form.

The experiments demonstrate small but consistent gains with ANR across a number of domains (Language modelling on small datasets, plus image classification) and baseline models.

A couple of things that could be improved:

- The abstract claims to ground the ANR in the variational dropout framework. When it is applied without dropout to image classification, shouldn't that be explained?

- Maybe dropping the determinant term also deserves some justification.

- Very recently, Activation Regularization by Merity (https://arxiv.org/abs/1708.01009) proposed a similar thing without theoretical justification. Maybe discuss it and the differences (if any) in the related work section?

- The Information Bottleneck section doesn't feel like an integral part of the paper.

My two cents: this work has both theoretical justification (a rare thing these days) and reasonable experimental results.

There are a number of typos and oversights:

- Abstract: ""variation dropout""
- Section 2:
  - x is never used
  - m in b = m + \sigma\epsilon is never defined (is it the x above?)
- In Section 3.2, equation 11 subscript of x_i is missing
- Section 6, Ungrammatical sentence: ""Even though L_2 ...""
","[7, 4, 4]","[' Good paper, accept', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[3, 3, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with a neutral summary of the paper's contributions. It then lists several constructive suggestions for improvement, indicating a positive attitude towards the paper's core ideas. The reviewer explicitly mentions appreciating the theoretical grounding and experimental validation. While the reviewer points out typos and minor issues, these are presented constructively and do not detract significantly from the overall positive sentiment. ",60.0,80.0
Multi-label Learning for Large Text Corpora using Latent Variable Model with Provable Gurantees,['Sayantan Dasgupta'],Reject,2018,[4],[2],[3],[1],[2],[0],"The paper addresses the problem of multi-label learning for text corpora and proposes to tackle the problem using tensor factorization methods. Some analysis and experimental results for the proposed algorithm are presented.

QUALITY: I find the quality of the results in this paper rather low. The proposed probabilistic model is defined ambiguously. The authors then look at joint probability distributions of co-occurence of two and three words, which gives a matrix and a tensor, respectively. They propose to match these matrix and tensor to their sample estimates and refer to such procedure as the moment matching method, which it is not. They then apply a standard two step technique from the moment matching literature consisting of whitening and orthogonal tensor factorization. However, in their case this does not have much statistical meaning. Indeed, whitening of the covariance matrix is usually justified by the scaling unidentifiability of the problem. In their case, the mathematics works because of the orthogonal unidentifiability of the square root of a matrix. Furthermore, the proposed sample estimators do not actually estimate densities they are dealing with (see, e.g., Eq. (16) and (17)). Their theoretical analysis seems like a straightforward extension of the analysis by Anandkumar, et al. (2012, 2014), however, I find it difficult to assess this analysis due to numerous ambiguities in the problem formulation and method development. This justifies my statement in the beginning of the paragraph.

CLARITY: The paper is not well written and, therefore, is difficult to assess. Many important details are omitted, the formulation of the model is self contradicting, the standard concepts and notations are sometimes abused, some statements are wrong. I provide some examples in the detailed comment below.

ORIGINALITY AND SIGNIFICANCE: The idea to apply tensor factorization approaches to the multi-label learning is novel up to my knowledge and is a pro of the paper. However, I have problems to find other pros in this submission because the clarity is quite low and in the present form there is no novelty in the proposed procedure. Moreover, the authors claim to work with densities, but end up estimating other quantities, which are not guaranteed to have the desirable form. They also emphasize the fact that there is the simplex constraint on the estimated parameters, but this constraint is completely ignored by the algorithm and, in general, won't be satisfied in practice. If think the authors should do some more work before this paper can be published.



DETAILED COMMENTS: Since I am quite critical about the paper, I point out some examples of drawbacks or flaws of this paper:

  - The proposed model (Section 2) is not well defined. In particular, the description in Section 2 is not sufficient to understand the proposed model; the plate diagram in Figure 2 is not consistent with the text. It is not mentioned how at least some conditional distributions behave (e.g., tokens given labels or states). The diagram in Fig. 1 does not help since it isn't consistent with the text (e.g. the elements of labels or states are not conditionally / independent). The model is very close to latent Dirichlet allocation by Blei, et al. (2003), but differences are not discussed.

  - The standard terminology is often abused. For example, the proposed approach is referred to as the method of moments when it is not. In Section 2.1, the authors aim to match joint distributions (not the moments) to their empirical approximations (which are also wrong; see below). The usage of tokes and documents is interchanged without any explanations.

  - The use of the whitening approach is not justified in their setting working with joint distributions of couples and triples and it has no statistical meaning. No explanation is provided. I would definitely not call this whitening.

  - In Section 2.2, the notation is not defined and is different from what is usually used in the literature. For example, Eq. (15) does not make much sense as is. One could guess from the context that they are talking about the eigenvectors of an orthogonal tensor as defined in, e.g. Anandkumar, et al. (2014).

  - In Section 3, the authors emphasize the fact that their parameters are constrained to the probability simplex, but this constraint is not ensured in the proposed algorithm (Alg. 1).

  - Importantly, the estimators of the matrix M_2 and tensor M_3 do not make much sense to me. For example, for estimating M_2 it would be reasonable to average over all word pairs, i.e. something like [M_2]_{ij} = 1/L \sum_{w_k \not = w_l} P(w_k = v_i, w_l = v_j), where L is the number of pairs. This is different from the expression in Eq. (16), which is just a rescaled non-central second moment. Similar issue is true for the order-3 estimator.

  - The factorization procedure does not ensure non-negativity of the obtained parameters and, therefore, the rescaling is not guaranteed to belong to the probability simplex. I could not find any explanations of this issue.

  - I explain good plots in the experimental section, potentially, by the fact that the authors do algorithmically something different from what they aim to do, because the estimators do not estimate the desired entities (i.e. are not consistent). The procedure looks to me quite similar to the procedure for LDA, hence the reasonable results. However, the authors do not justify their proposed method.","[4, 3, 4]","[' Ok but not good enough - rejection', ' Clear rejection', ' Ok but not good enough - rejection']","[5, 4, 5]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review is highly critical of the paper, pointing out serious flaws in the methodology, clarity, and originality. The reviewer finds the quality of the results low and the paper poorly written. While acknowledging the novelty of the idea, the reviewer's numerous concerns about the paper's execution result in a negative sentiment. Despite the critical tone, the language remains professional and avoids personal attacks, indicating a level of politeness.",-75.0,50.0
Self-Supervised Learning of Object Motion Through Adversarial Video Prediction,"['Alex X. Lee', 'Frederik Ebert', 'Richard Zhang', 'Chelsea Finn', 'Pieter Abbeel', 'Sergey Levine']",Reject,2018,"[6, 2, 11, 6, 17, 10]","[10, 6, 16, 11, 22, 15]","[24, 24, 4, 306, 608, 743]","[15, 10, 2, 128, 291, 326]","[8, 14, 2, 172, 291, 396]","[1, 0, 0, 6, 26, 21]","In this paper a neural-network based method for multi-frame video prediction is proposed. It builds on the previous work of [Finn et al. 2016] that uses a neural network to predict transformation parameters of an affine image transformation for future frame prediction, an idea akin to the Spatial Transformer Network paper of [Jaderberg et al., 2015]. What is new compared to [Finn et al. 2016] is that the authors managed to train the network in combination with an adversarial loss, which allows for the generation of more realistic images. Time series modelling is performed via convolutional LSTMs. The authors evaluate their method based on a mechanical turk survey, where humans are asked to judge the realism of the generated images; additionally, they propose to measure prediction quality by the distance between the manually annotated positions of objects within ground truth and predicted frames.

My main concerns with this paper are novelty, reproducibility and evaluation.

* Novelty. The network design builds heavily on the work of [Finn et al., 2106]. A number of design decisions (such as instance normalization) seem to help yield better results, but are minor contributions. A major contribution is certainly the combination with an adversarial loss, which is a non-trivial task. However, the authors claim that their method is the first to combine multi-frame video prediction with an adversarial loss, which is not true. A recent work, presented at CVPR this year also does multi-frame prediction featuring an adversarial loss and explicitly models and captures the full dense optical flow (though in the latent space) that allows non-trivial motion extrapolation to future frames. This work is neither mentioned in the related work nor compared to. 
 
Lu et al. , Flexible Spatio-Temporal Networks for Video Prediction, CVPR 2017

This recent work builds on another highly relevant work, that is also not mentioned in the paper:

Patraucean et al. Spatio-temporal video autoencoder with differentiable memory, arxiv 2017

Since this is prior state-of-the-art and directly applicable to the problem, a comparison is a must. 

* Reproducibility and evaluation
The description of the network is quite superficial. Even if the authors released their code used for training (which is not mentioned), I think the authors should aim for a more self-contained exposition. I doubt that a PhD student would be able to reimplement the method and achieve comparable results given the paper at hand only. It is also not mentioned whether the other methods that the authors compare to are re-trained on their newly proposed training dataset. Hence, it remains unclear to what extend the achieved improvements are due to the proposed network design changes or the particular dataset they use for training. The authors also don't show any results on previous datasets, which would allow for a more objective comparison to existing state of the art. Another point of criticism is the way the Amazon Mechanical Turk evaluation was performed. Since only individual images were shown, the evaluation mainly measures the quality of the generated images. Since the authors combine their method with a GAN, it is not surprising that the generated images look more realistic. However, since the task is *video* prediction, it seems more natural to show small video snippets rather than individual images, which would also evaluate temporal consistency.

* Further comments:
The paper contains a number of broken sentences, typos and requires a considerable amount of polishing prior to publication.
","[3, 3, 7, 3]","[' Clear rejection', ' Clear rejection', ' Good paper, accept', ' Clear rejection']","[5, 4, 5, 5]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review is critical of the paper due to concerns about novelty, reproducibility, and evaluation. The reviewer points out prior work that the authors overlooked and criticizes the evaluation methodology. While the reviewer acknowledges the use of adversarial loss as a significant contribution, the overall tone suggests that the paper needs substantial revisions before publication. The language used, while direct, maintains a professional and academic tone.",-50.0,50.0
Do Deep Reinforcement Learning Algorithms really Learn to Navigate?,"['Shurjo Banerjee', 'Vikas Dhiman', 'Brent Griffin', 'Jason J. Corso']",Reject,2018,"[3, 6, 7, 17]","[7, 11, 12, 22]","[7, 24, 28, 218]","[2, 10, 14, 120]","[5, 11, 12, 65]","[0, 3, 2, 33]","Science is about reproducible results and it is very commendable from scientists to hold their peers accountable for their work by verifying their results. It is also necessary to inspect claims that are made by researchers to avoid the community straying in the wrong direction. However, any critique needs to be done properly, by 1) attending to the actual claims that were made in the first place, by 2) reproducing the results in the same way as in the original work, 3) by avoiding introducing false claims based on a misunderstanding of terminology and 4) by extensively researching the literature before trying to affirm that a general method (here, Deep RL) cannot solve certain tasks.

This paper is a critique of deep reinforcement learning methods for learning to navigate in 3D environments, and seems to focus intensively on one specific paper (Mirowski et al, 2016, “Learning to Navigate in Complex Environments”) and one of the architectures (NavA3C+D1D2L) from that paper. It conducts an extensive assessment of the methods in the critiqued paper but does not introduce any alternative method. For this reason, I had to carefully re-read the critiqued paper to be able to assess the validity of the arguments made in this submission and to evaluate its merit from the point of view of the quality of the critique. The (Mirowski et al, 2016) paper shows that a neural network-based agent with LSTM-based memory and auxiliary tasks such as depth map prediction can learn to navigate in fixed environments (3D mazes) with a fixed goal position (what they call “static maze”), and in fixed mazes with changing goal environments (what they call “environments with dynamic elements” or “random goal mazes”).

This submission claims that:
[a] “[based on the critiqued paper] one might assume that DRL-based algorithms are able to 'learn to navigate' and are thus ready to replace classical mapping and path-planning algorithms”,
[b] “following training and testing on constant map structures, when trained and tested on the same maps, [the NavA3C+D1D2L algorithm] is able to choose the shorter paths to the goal”,
[c] “when tested on unseen maps the algorithm utilizes a wall-following strategy to find the goal without doing any mapping or path planning”,
[d] “this state-of-the-art result is shown to be successful on only one map, which brings into question the repeatability of the results”,
[e] “Do DRL-based navigation algorithms really 'learn to navigate'? Our results answer this question negatively.”
[f] “we are the first to evaluate any DRL-based navigation method on maps with unseen structures”

The paper also conducts an extensive analysis of the performance of a different version of the NavA3C+D1D2L algorithm (without velocity inputs, which probably makes learning path integration much more difficult), in the same environments but by introducing unjustified changes (e.g., with constant velocities and a different action space) and with a different reward structure (incorporating a negative reward for wall collisions). While the experimental setup does not match (Mirowski et al, 2016), thereby invalidating claim [d], the experiments are thorough and do show that that architecture does not generalize to unseen mazes. The use of attention heat maps is interesting.

The main problem however is that it seems that this submission completely misrepresents the intent of (Mirowski et al, 2016) by using a straw man argument, and makes a rather unacademic and unsubstantiated accusation of lack of repeatability of the results.

Regarding the former, I could not find any claim that the methods in (Mirowski et al, 2017) learn mapping and path planning in unseen environments, that could support claim [a]. More worryingly, when observing that the method of (Mirowski et al, 2017) may not generalize to unseen environments in claim [c], the authors of this submission seem to confuse navigation, cartography and SLAM, and attribute to that work claims that were never made in the first place, using a straw man argument. Navigation is commonly defined as the goal driven control of an agent, following localization, and is a broad skill that involves the determination of position and direction, with or without a map of the environment (Fox 1998, ” Markov Localization: A Probabilistic Framework for Mobile Robot Localization and Navigation”). This widely accepted definition of navigation does not preclude being limited to known environments only.

Regarding repeatability, the claim [d] is contradicted in section 5 when the authors demonstrate that the NavA3C+D1D2L algorithm does achieve a reduction in latency to goal in 8 out of 10 experiments on random goal, static map and random or static spawns. The experiments in section 5.3 are conducted in simple but previously unseen maps and cannot logically contradict results (Mirowski et al, 2016) achieved by training on static maps such as their “I-maze”. Moreover, claim [d] about repeatability is also invalidated by the fact that the experiments described in the paper use different observations (no velocity inputs), different action space, different reward structure, with no empirical evidence to support these changes. It seems, as the authors also claim in [b], that the work of (Mirowski et al, 2017), which was about navigation in known environments, actually is repeatable.

Additionally, some statements made by the authors are demonstrably untrue. First, the authors claim that they are the first to train DRL agents in all random mazes [f], but this has been already shown in at least two publications (Mnih et al, 2016 and Jaderberg et al, 2016).

Second, the title of the submission, “Do Deep Reinforcement Learning Algorithms Really Learn to Navigate” makes a broad statement [e] that cannot be logically invalidated by only one particular set of experiments on a particular model and environment, particularly since it directly targets one specific paper (out of several recent papers that have addressed navigation) and one specific architecture from that paper, NavA3C+D1D2L (incidentally, not the best-performing one, according to table 1 in that paper). Why did the authors not cite and consider (Parisotto et al, 2017, “Neural Map: Structured Memory for Deep Reinforcement Learning”), which explicitly claims that their method is “capable of generalizing to environments that were not seen during training”? It seems that the authors need to revise both their bibliography and their logical reasoning: one cannot invalidate a broad set of algorithms for a broad goal, simply by taking a specific example and showing that it does not fit a particular interpretation of navigation *in previously unseen environments*.
","[3, 3, 7]","[' Clear rejection', ' Clear rejection', ' Good paper, accept']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review expresses a negative sentiment towards the paper, criticizing its methodology and claims. It accuses the authors of misrepresenting previous work, making unsubstantiated claims, and drawing overly broad conclusions. The reviewer uses strong language, such as ""completely misrepresents,"" ""straw man argument,"" ""unacademic,"" and ""demonstrably untrue,"" indicating a lack of politeness. However, the review also acknowledges the thoroughness of the experiments and the interesting use of attention heat maps.",-60.0,-40.0
Bit-Regularized Optimization of Neural Nets,"['Mohamed Amer', 'Aswin Raghavan', 'Graham W. Taylor', 'Sek Chai']",Reject,2018,"[5, 7, 15, 4]","[1, 12, 20, 7]","[1, 29, 174, 8]","[0, 13, 78, 3]","[0, 14, 77, 3]","[1, 2, 19, 2]","This paper proposes to optimize neural networks considering the three different terms: original loss function, quantization error and the sum of bits. While the idea makes sense, the paper is not well executed, and I cannot understanding how gradient descend is performed based on the description of Section 4.

1. After equation (5), I don't understand how the gradient of L(tilde_W) w.r.t. B(i) is computed. B(i) is discrete. The update rule seems to be clearly wrong.
2. The experimental section of this paper needs improvement.
   a. End-to-end trained quantized networks have been studied in various previous works including stochastic neuron (Bengio et al 2013), quantization + fine tuning (Wu et al 2016 Quantized Convolutional Neural Networks for Mobile Devices), Binary connect (Courbariaux et al 2016) etc. None of these works have been compared with.
   b. All the baseline methods use 8 bits per value. This choice is quite ad-hoc.
   c. Only MNIST and CIFAR10 dataset with Lenet32 are used in the experiment. I find the findings not conclusive based on these.
   d. No wall-time and real memory numbers are reported.","[3, 4, 4]","[' Clear rejection', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer expresses that the core idea of the paper makes sense, but they find issues with the execution, particularly the clarity of the gradient descent description and the experimental setup. They list several shortcomings, indicating a negative sentiment. However, the language remains professional and focused on the scientific content, suggesting politeness. Therefore, the sentiment leans negative due to the critical points raised, but the politeness remains neutral to slightly positive due to the constructive framing of the criticism.",-30.0,20.0
Ego-CNN: An Ego Network-based Representation of Graphs Detecting Critical Structures,"['Ruo-Chun Tzeng', 'Shan-Hung Wu']",Reject,2018,"[0, 12]","[4, 16]","[6, 40]","[4, 26]","[2, 3]","[0, 11]","The authors proposed a convolutional framework based on merging ego-networks. It combines graph embedding layers with task driven output layers, producing interpretable results for critical structure detection. While based on existing embedding methods such as Patchy-San, the contribution of ego-centric convolution and multi-layer architechture is novel and has a lot of potential in applications. The overall presentation of the draft is also of high quality. I recommend its publication at ICLR.

Here is a list of suggested changes to further improve the draft,

1. The two panels of Figure 1 seems redundant.

2. Figure 4 does not provide useful information, especially in terms of how overlapping neighborhoods are aggregated at deeper layers.

3. There seems to be a mistake in Figure 5 with the top neighborhood in white

4. The connection between weight-tying and scale-free structure needs better explanation. Are the authors trying to say that fractal processes generates power-law degree distributions?

5. The visualization of critical structures are very helpful. However, it might be better to look into structures in high level layers for truly global signatures. This is especially the case for the reddit dataset, where visualizations at the node and edge level creates hairballs.

","[7, 4, 4]","[' Good paper, accept', ' Ok but not good enough - rejection', ' Ok but not good enough - rejection']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is very positive. The reviewer finds the proposed method novel and with high potential. They recommend publication at ICLR, which is a strong indicator of a positive sentiment. The language used is constructive and polite, suggesting specific areas of improvement without being demanding or disrespectful.",85.0,90.0
EXPLORING NEURAL ARCHITECTURE SEARCH FOR LANGUAGE TASKS,"['Minh-Thang Luong', 'David Dohan', 'Adams Wei Yu', 'Quoc V. Le', 'Barret Zoph', 'Vijay Vasudevan']",Reject,2018,"[9, 21, 8, 14, 4, 12]","[14, 26, 13, 19, 9, 16]","[53, 29, 41, 299, 66, 51]","[25, 13, 17, 143, 29, 25]","[26, 16, 20, 145, 35, 22]","[2, 0, 4, 11, 2, 4]","This paper experiments the application of NAS to some natural language processing tasks : machine translation and question answering.  

My main concern about this paper is its contribution. The difference with the paper of Zoph 2017 is really slight in terms of methodology. Moving from a language modeling task to machine translation is not very impressive neither really discussed. It could be interesting to change the NAS approach by taking into account this application shift.  

On the experimental part, the paper is not really convincing. The results on WMT are not state of the art. The best system of this year was a standard phrase based and has achieved 29.3 BLEU score (for BLEU cased, otherwise it's one point more). Therefore the results on mt tasks are difficult to interpret. 

At the end , the reader can be sure these experiments required a significant computational power. Beyond that it is difficult to really draw meaningful conclusions. ","[3, 4, 3]","[' Clear rejection', ' Ok but not good enough - rejection', ' Clear rejection']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is critical of the paper's contribution and experimental results. It points out the lack of significant novelty compared to prior work and the unconvincing performance on machine translation tasks. The reviewer also questions the value of the findings despite the computational effort involved. All of these points suggest a negative sentiment. However, the language used is professional and avoids harsh or disrespectful language, indicating a neutral politeness level.",-50.0,0.0
Feature Map Variational Auto-Encoders,"['Lars Maaløe', 'Ole Winther']",Reject,2018,"[4, 26]","[9, 31]","[28, 140]","[8, 53]","[19, 46]","[1, 41]","The description of the proposed method is very unclear. From the paper it is very difficult to make out exactly what architecture is proposed. I understand that the prior on the z_i in each layer is a pixel-cnn, but what is the posterior? Equations 8 and 9 would suggest it is of the same form (pixel-cnn) but this would be much too slow to sample during training. I'm guessing it is just a factorized Gaussian, with a separate factorized Gaussian pseudo-prior? That is, in figure 1 all solid lines are factorized Gaussians and all dashed lines are pixel-cnns?

* The word ""layers"" is sometimes used to refer to latent variables z, and sometimes to parameterized neural network layers in the encoder and decoder. E.g. ""The top stochastic layer z_L in FAME is a fully-connected dense layer"". No, z_L is a vector of latent variables. Are you saying the encoder produces it using a fully-connected layer?
* Section 2.2 starts talking about ""deterministic layers h"". Are these part of the encoder or decoder? What is meant by ""number of layers connecting the stochastic latent variables""?
* Section 2.3: What is meant by ""reconstruction data""?

If my understanding of the method is correct, the novelty is limited. Autoregressive priors were used previously in e.g. the Lossy VAE by Chen et al. and IAF-VAE by Kingma et al. The reported likelihood results are very impressive though, and would be reason for acceptance if correct. However, the quality of the sampled images shown for CIFAR-10 doesn't match the reported likelihood. There are multiple possible reasons for this, but after skimming the code I believe it might be due to a faulty implementation of the variational lower bound. Instead of calculating all quantities in the log domain, the code takes explicit logs and exponents and stabilizes them by adding small quantities ""eps"": this is not guaranteed to give the right result. Please fix this and re-run your experiments. (I.e. in _loss.py don't use x/(exp(y)+eps) but instead use x*exp(-y). Don't use log(var+eps) with var=softplus(x), but instead use var=softplus(x)+eps or parameterize the variance directly in the log domain).","[3, 6, 5]","[' Clear rejection', ' Marginally above acceptance threshold', ' Marginally below acceptance threshold']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with a very negative sentiment, expressing strong confusion and using phrases like ""very unclear"" and ""much too slow."" This suggests a negative initial impression. However, the reviewer acknowledges the potential significance of the findings (""impressive likelihood results"") and provides concrete advice for improvement. This mix of criticism and constructive feedback points to a neutral-to-positive overall sentiment. The language is direct and technical but maintains professional courtesy. There are no personal attacks or disrespectful remarks.",20.0,60.0
Spherical CNNs,"['Taco S. Cohen', 'Mario Geiger', 'Jonas Köhler', 'Max Welling']",Accept,2018,"[-3, 0, 14]","[2, 5, 19]","[2, 7, 82]","[1, 3, 11]","[1, 4, 52]","[0, 0, 19]","Summary:

The paper proposes a framework for constructing spherical convolutional networks (ConvNets) based on a novel synthesis of several existing concepts.  The goal is to detect patterns in spherical signals irrespective of how they are rotated on the sphere.  The key is to make the convolutional architecture rotation equivariant.

Pros:

+ novel/original proposal justified both theoretically and empirically
+ well written, easy to follow
+ limited evaluation on a classification and regression task is suggestive of the proposed approach's potential
+ efficient implementation

Cons:

- related work, in particular the first paragraph, should compare and contrast with the closest extant work rather than merely list them
- evaluation is limited; granted this is the nature of the target domain

Presentation:

While the paper is generally written well, the paper appears to conflate the definition of the convolutional and correlation operators?  This point should be clarified in a revised manuscript.  

In Section 5 (Experiments), there are several references to S^2CNN.  This naming of the proposed approach should be made clear earlier in the manuscript.  As an aside, this appears a little confusing since convolution is performed first on S^2 and then SO(3). 

Evaluation:

What are the timings of the forward/backward pass and space considerations for the Spherical ConvNets presented in the evaluation section?  Please provide specific numbers for the various tasks presented.

How many layers (parameters) are used in the baselines in Table 2?  If indeed there are much less parameters used in the proposed approach, this would strengthen the argument for the approach.  On the other hand, was there an attempt to add additional layers to the proposed approach for the shape recognition experiment in Sec. 5.3 to improve performance?

Minor Points:

- some references are missing their source, e.g., Maslen 1998 and Kostolec, Rockmore, 2007, and Ravanbakhsh, et al. 2016.

- some sources for the references are presented inconsistency, e.g., Cohen and Welling, 2017 and Dieleman, et al. 2017

- some references include the first name of the authors, others use the initial 

- in references to et al. or not, appears inconsistent

- Eqns 4, 5, 6, and 8 require punctuation

- Section 4 line 2, period missing before ""Since the FFT""

- ""coulomb matrix"" --> ""Coulomb matrix""

- Figure 5, caption: ""The red dot correcpond to"" --> ""The red dot corresponds to""

Final remarks:

Based on the novelty of the approach, and the sufficient evaluation, I recommend the paper be accepted.

","[8, 9, 7]","[' Top 50% of accepted papers, clear accept', ' Top 15% of accepted papers, strong accept', ' Good paper, accept']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer provides a generally positive assessment, highlighting the novelty, clarity, and potential of the proposed approach. While pointing out limitations in evaluation and suggesting improvements in presentation and referencing, the reviewer ultimately recommends acceptance. The language used is constructive, professional, and encouraging, indicating a positive disposition towards the paper.",75.0,80.0
Certifying Some Distributional Robustness with Principled Adversarial Training,"['Aman Sinha', 'Hongseok Namkoong', 'John Duchi']",Accept,2018,"[23, 3, 13]","[28, 8, 18]","[39, 31, 149]","[23, 13, 69]","[12, 16, 57]","[4, 2, 23]","This paper applies recently developed ideas in the literature of robust optimization, in particular distributionally robust optimization with Wasserstein metric, and showed that under this framework for smooth loss functions when not too much robustness is requested, then the resulting optimization problem is of the same difficulty level as the original one (where the adversarial attack is not concerned). I think the idea is intuitive and reasonable, the result is nice. Although it only holds when light robustness are imposed, but in practice, this seems to be more of the case than say large deviation/adversary exists. As adversarial training is an important topic for deep learning, I feel this work may lead to promising principled ways for adversarial training. ","[9, 9, 9]","[' Top 15% of accepted papers, strong accept', ' Top 15% of accepted papers, strong accept', ' Top 15% of accepted papers, strong accept']","[4, 4, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review is positive because the reviewer states ""the idea is intuitive and reasonable, the result is nice."" and predicts potential impact: ""this work may lead to promising principled ways for adversarial training."" The reviewer does point out a limitation (only holds for light robustness) but frames it positively as this ""seems to be more of the case in practice."" The language is polite and respectful throughout, without any negative framing or harsh criticism.",75.0,80.0
Learning Deep Mean Field Games for Modeling Large Population Behavior,"['Jiachen Yang', 'Xiaojing Ye', 'Rakshit Trivedi', 'Huan Xu', 'Hongyuan Zha']",Accept,2018,"[11, 11, 3, 16, 26]","[16, 16, 7, 21, 31]","[181, 65, 21, 253, 445]","[34, 24, 12, 112, 250]","[15, 25, 9, 56, 117]","[132, 16, 0, 85, 78]","The paper proposes a novel approach on estimating the parameters  
of Mean field games (MFG). The key of the method is a reduction of the unknown parameter MFG to an  unknown parameter Markov Decision Process (MDP).

This is an important class of models and I recommend the acceptance of the paper.

I think that the general discussion about the collective behavior application should be more carefully presented and some better examples of applications should be easy to provide.  In addition the authors may want to enrich their literature review and give references to alternative work on unknown MDP estimation methods cf. [1], [2] below. 

[1] Burnetas, A. N., & Katehakis, M. N. (1997). Optimal adaptive policies for Markov decision processes. Mathematics of Operations Research, 22(1), 222-255.

[2] Budhiraja, A., Liu, X., & Shwartz, A. (2012). Action time sharing policies for ergodic control of Markov chains. SIAM Journal on Control and Optimization, 50(1), 171-195.","[10, 8, 8]","[' Top 5% of accepted papers, seminal paper', ' Top 50% of accepted papers, clear accept', ' Top 50% of accepted papers, clear accept']","[5, 4, 3]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer explicitly recommends acceptance, a very positive sign. They also use positive language like ""important class of models."" While they suggest improvements, these are constructive and typical for a peer review. The suggestions do not detract from the overall positive sentiment. The language used is formal and professional, typical for academic discourse, without any rudeness.",85.0,75.0
Zero-Shot Visual Imitation,"['Deepak Pathak', 'Parsa Mahmoudieh', 'Guanghao Luo', 'Pulkit Agrawal', 'Dian Chen', 'Yide Shentu', 'Evan Shelhamer', 'Jitendra Malik', 'Alexei A. Efros', 'Trevor Darrell']",Accept,2018,"[3, 7, 4, 6, 10, 8]","[8, 12, 9, 11, 15, 13]","[22, 48, 52, 88, 175, 79]","[11, 18, 22, 29, 64, 29]","[11, 27, 28, 45, 52, 38]","[0, 3, 2, 14, 59, 12]","One of the main problems with imitation learning in general is the expense of expert demonstration. The authors here propose a method for sidestepping this issue by using the random exploration of an agent to learn generalizable skills which can then be applied without any specific pretraining on any new task. 

The proposed method has at its core a method for learning a parametric skill function (PSF) that takes as input a description of the initial state, goal state, parameters of the skill and outputs a sequence of actions (could be of varying length) which take the agent from initial state to goal state.

The skill function uses a RNN as function approximator and minimizes the sum of two losses i.e. the state mismatch loss over the trajectory (using an explicitly learnt forward model) and the action mismatch loss (using a model-free action prediction module) . This is hard to do in practice due to jointly learning both the forward model as well as the state mismatches. So first they are separately learnt and then fine-tuned together. 

In order to decide when to stop, an independent goal detector is trained which was found to be better than adding a 'goal-reached' action to the PSF.

Experiments on two domains are presented. 1. Visual navigation where images of start and goal states are given as input. 2. Robotic knot-tying with a loose rope where visual input of the initial and final rope states are given as input.

Comments:

- In the visual navigation task no numbers are presented on the comparison to slam-based techniques used as baselines although it is mentioned that it will be revisited.

- In the rope knot-tying task no slam-based or other classical baselines are mentioned.

- My main concern is that I am really trying to place this paper with respect to doing reinforcement learning first (either in simulation or in the real world itself, on-policy or off-policy) and then just using the learnt policy on test tasks. Or in other words why should we call this zero-shot imitation instead of simply reinforcement learnt policy being learnt and then used. The nice part of doing RL is that it provides ways of actively controlling the exploration. See this pretty relevant paper which attempts the same task and also claims to have the target state generalization ability. 

Target-driven Visual Navigation in Indoor Scenes using Deep Reinforcement Learning by Zhu et al.

I am genuinely curious and would love the authors' comments on this. It should help make it clearer in the paper as well.
 
Update:

After evaluating the response from the authors and ensuing discussion as well as the other reviews and their corresponding discussion, I am revising my rating for this paper up. This will be an interesting paper to have at the conference and will spur more ideas and follow-on work.","[7, 8, 8]","[' Good paper, accept', ' Top 50% of accepted papers, clear accept', ' Top 50% of accepted papers, clear accept']","[5, 3, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer raises valid concerns and questions the paper's framing and novelty compared to reinforcement learning approaches. However, they also acknowledge the paper's potential to stimulate further research and find it interesting enough to warrant presentation at the conference. This suggests the review leans slightly positive, but with reservations. The reviewer's willingness to revise their rating upwards after the author's response further indicates a positive shift in their perspective.",50.0,70.0
On the Convergence of Adam and Beyond,"['Sashank J. Reddi', 'Satyen Kale', 'Sanjiv Kumar']",Accept,2018,"[9, 15, 20]","[14, 20, 25]","[82, 127, 188]","[43, 73, 89]","[39, 41, 83]","[0, 13, 16]","This work identifies a mistake in the existing proof of convergence of
Adam, which is among the most popular optimization methods in deep
learning. Moreover, it gives a simple 1-dimensional counterexample with
linear losses on which Adam does not converge. The same issue also
affects RMSprop, which may be viewed as a special case of Adam without
momentum. The problem with Adam is that the ""learning rate"" matrices
V_t^{1/2}/alpha_t are not monotonically decreasing. A new method, called
AMSGrad is therefore proposed, which modifies Adam by forcing these
matrices to be decreasing. It is then shown that AMSGrad does satisfy
essentially the same convergence bound as the one previously claimed for
Adam. Experiments and simulations are provided that support the
theoretical analysis.

Apart from some issues with the technical presentation (see below), the
paper is well-written.

Given the popularity of Adam, I consider this paper to make a very
interesting observation. I further believe all issues with the technical
presentation can be readily addressed.



Issues with Technical Presentation:

- All theorems should explicitly state the conditions they require
  instead of referring to ""all the conditions in (Kingma & Ba, 2015)"".
- Theorem 2 is a repetition of Theorem 1 (except for additional
  conditions).
- The proof of Theorem 3 assumes there are no projections, so this
  should be stated as part of its conditions. (The claim in footnote 2
  that they can be handled seems highly plausible, but you should be up
  front about the limitations of your results.)
- The regret bound Theorem 4 establishes convergence of the optimization
  method, so it plays the role of a sanity check. However, it is
  strictly worse than the regret bound O(sqrt{T}) for online gradient
  descent [Zinkevich,2003], so it cannot explain why the proposed
  AMSgrad method might be adaptive. (The method may indeed be adaptive
  in some sense; I am just saying the *bound* does not express that.
  This is also not a criticism of the current paper; the same remark
  also applies to the previously claimed regret bound for Adam.)
- The discussion following Corollary 1 suggests that sum_i
  hat{v}_{T,i}^{1/2} might be much smaller than d G_infty. This is true,
  but we should always expect it to be at least a constant, because
  hat{v}_{t,i} is monotonically increasing by definition of the
  algorithm, so the bound does not get better than O(sqrt(T)).
  It is also suggested that sum_i ||g_{1:T,i}|| = sqrt{sum_{t=1}^T
  g_{t,i}^2} might be much smaller than dG_infty, but this is very
  unlikely, because this term will typically grow like O(sqrt{T}),
  unless the data are extremely sparse, so we should at least expect
  some dependence on T.
- In the proof of Theorem 1, the initial point is taken to be x_1 = 1,
  which is perfectly fine, but it is not ""without loss of generality"",
  as claimed. This should be stated in the statement of the Theorem.
- The proof of Theorem 6 in appendix B only covers epsilon=1. If it is
  ""easy to show"" that the same construction also works for other
  epsilon, as claimed, then please provide the proof for general
  epsilon.


Other remarks:

- Theoretically, nonconvergence of Adam seems a severe problem. Can you
  speculate on why this issue has not prevented its widespread adoption?
  Which factors might mitigate the issue in practice?
- Please define g_t \circ g_t and g_{1:T,i}
- I would recommend sticking with standard linear algebra notation for
  the sqrt and the inverse of a matrix and simply using A^{-1} and
  A^{1/2} instead of 1/A and sqrt{A}.
- In theorems 1,2,3, I would recommend stating the dimension (d=1) of
  your counterexamples, which makes them very nice!

Minor issues:

- Check accent on Nicol\`o Cesa-Bianchi in bibliography.
- Near the end of the proof of Theorem 6: I believe you mean Adam
  suffers a ""regret"" instead of a ""loss"" of at least 2C-4.
  Also 2C-4=2C-4 is trivial in the second but last display.
","[8, 8, 9]","[' Top 50% of accepted papers, clear accept', ' Top 50% of accepted papers, clear accept', ' Top 15% of accepted papers, strong accept']","[4, 3, 5]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review starts with positive statements, highlighting the significance of the paper's findings and praising its readability. While it points out technical issues, the reviewer believes these are addressable and doesn't diminish the paper's contribution. The tone throughout is constructive and professional, suggesting areas for improvement rather than harshly criticizing.",75.0,80.0
Breaking the Softmax Bottleneck: A High-Rank RNN Language Model,"['Zhilin Yang', 'Zihang Dai', 'Ruslan Salakhutdinov', 'William W. Cohen']",Accept,2018,"[16, 3, 16, 34]","[21, 7, 21, 39]","[84, 51, 430, 348]","[36, 25, 212, 220]","[35, 26, 207, 81]","[13, 0, 11, 47]","The authors has addressed my concerns, so I raised my rating. 

The paper is grounded on a solid theoretical motivation and the analysis is sound and quite interesting.

There are no results on large corpora such as 1 billion tokens benchmark corpus, or at least medium level corpus with 50 million tokens. The corpora the authors choose are quite small, the variance of the estimates are high, and similar conclusions might not be valid on a large corpus. 

[1] provides the results of character level language models on Enwik8 dataset, which shows regularization doesn't have much effect and needs less tuning. Results on this data might be more convincing.

The results of MOS is very good, but the computation complexity is much higher than other baselines. In the experiments, the embedding dimension of MOS is slightly smaller, but the number of mixture is 15. This will make it less usable, I think it's necessary to provide the training time comparison.

Finally experiments on machine translation or speech recognition should be done and to see what improvements the proposed method could bring for BLEU or WER. 

[1] Melis, Gábor, Chris Dyer, and Phil Blunsom. ""On the state of the art of evaluation in neural language models."" arXiv preprint arXiv:1707.05589 (2017).

[2] Joris Pelemans, Noam Shazeer, Ciprian Chelba, Sparse Non-negative Matrix Language Modeling,  Transactions of the Association for Computational Linguistics, vol. 4 (2016), pp. 329-342

[3] Shazeer et al. (2017). Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer. ICLR 2017
","[7, 8, 7]","[' Good paper, accept', ' Top 50% of accepted papers, clear accept', ' Good paper, accept']","[5, 4, 4]","[' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer's sentiment is positive overall. They explicitly mention raising their rating and call the paper's motivation ""solid"" and the analysis ""sound and quite interesting."" While they raise several constructive criticisms and suggestions for improvement, these are typical of peer reviews and do not negate the positive sentiment. The language used is polite and professional throughout, employing a neutral and objective tone. There are no instances of harsh language or personal attacks.",65.0,75.0
Neural Sketch Learning for Conditional Program Generation,"['Vijayaraghavan Murali', 'Letao Qi', 'Swarat Chaudhuri', 'Chris Jermaine']",Accept,2018,"[10, 6, 16, 20]","[15, 6, 21, 25]","[31, 2, 121, 126]","[19, 2, 70, 63]","[11, 0, 40, 20]","[1, 0, 11, 43]","This is a very well-written and nicely structured paper that tackles the problem of generating/inferring code given an incomplete description (sketch) of the task to be achieved. This is a novel contribution to existing machine learning approaches to automated programming that is achieved by training on a large corpus of Android apps. The combination of the proposed technique and leveraging of real data are a substantial strength of the work compared to many approaches that have come previously.

This paper has many strengths:
1) The writing is clear, and the paper is well-motivated
2) The proposed algorithm is described in excellent detail, which is essential to reproducibility
3) As stated previously, the approach is validated with a large number of real Android projects
4) The fact that the language generated is non-trivial (Java-like) is a substantial plus
5) Good discussion of limitations

Overall, this paper is a valuable addition to the empirical software engineering community, and a nice break from more traditional approaches of learning abstract syntax trees.","[7, 7, 8]","[' Good paper, accept', ' Good paper, accept', ' Top 50% of accepted papers, clear accept']","[3, 2, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review starts with very positive statements about the paper's contribution, writing style, and methodology. It lists five specific strengths, including clarity, detailed description, use of real data, non-trivial language generation, and a good discussion of limitations. The reviewer clearly states that this is a ""valuable addition"" to the field. All of this points to a very positive sentiment. The language used throughout is formal, respectful, and encouraging, indicating a high level of politeness.",90.0,90.0
Continuous Adaptation via Meta-Learning in Nonstationary and Competitive Environments,"['Maruan Al-Shedivat', 'Trapit Bansal', 'Yura Burda', 'Ilya Sutskever', 'Igor Mordatch', 'Pieter Abbeel']",Accept,2018,"[5, 5, 5, 12, 12, 17]","[9, 9, 10, 17, 17, 22]","[42, 31, 17, 100, 110, 617]","[19, 19, 8, 50, 49, 294]","[20, 12, 9, 45, 56, 296]","[3, 0, 0, 5, 5, 27]","This paper proposed a gradient-based meta-learning approach for continuous adaptation in nonstationary and adversarial environment. The idea is to treat a nonstationary task as a sequence of stationary tasks and train agents to exploit the dependencies between consecutive tasks such that they can deal with nonstationarities at test time. The proposed method was evaluated based on a nonstationary locomotion and within a competitive multi agent setting. For the later, this paper specifically designed the RomoSumo environment and defined iterated adaptation games to test various aspect of adaptation strategies. The empirical results in both cases demonstrate the efficacy of the proposed meta-learned adaptation rules over the baselines in the few-short regime. The superiority of meta-learners is further justified on a population level.

The paper addressed a very important problem for general AI and it is well-written. Careful experiment designs, and thorough comparisons make the results conniving. I

Further comments:

1. In the experiment the trajectory number seems very small, I wonder if directly using importance weight as shown in (9) will cause high variance in the performance?

2. One of the assumption in this work is that trajectories from T_i contain some information about T_{i+1}, I wonder what will happen if the mutually information is very small between them (The extreme case is that two tasks are independent), will current method still perform well?

P7, For the RL^2 policy, the authors mentioned that “…with a given environment (or an opponent), reset the state once the latter changes” How does the agent know when an environment (or opponent) changes? 

P10, “This suggests that it meta-learned a particular…” This sentence need to be rewritten.

P10, ELO is undefined
","[7, 8, 9]","[' Good paper, accept', ' Top 50% of accepted papers, clear accept', ' Top 15% of accepted papers, strong accept']","[4, 4, 2]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']","The reviewer starts with positive statements, highlighting the importance of the problem, the paper's quality, the experiment design, and the convincing results. They also use phrases like ""well-written"" and ""convincing."" However, they also raise several questions and point out areas for improvement, which indicates that the review is not entirely positive. Therefore, the sentiment is somewhat positive but not overly enthusiastic. The language used is polite and constructive. The reviewer asks questions and raises concerns respectfully and professionally.",60.0,80.0
Spectral Normalization for Generative Adversarial Networks,"['Takeru Miyato', 'Toshiki Kataoka', 'Masanori Koyama', 'Yuichi Yoshida']",Accept,2018,"[3, 6, 7, 24]","[8, 10, 12, 29]","[24, 11, 38, 222]","[9, 5, 14, 107]","[14, 3, 19, 72]","[1, 3, 5, 43]","This paper borrows the classic idea of spectral regularization, recently applied to deep learning by Yoshida and Miyato (2017) and use it to normalize GAN objectives. The ensuing GAN, coined SN-GAN, essentially ensures the Lipschitz property of the discriminator. This Lipschitz property has already been proposed by recent methods and has showed some success. However,  the authors here argue that spectral normalization is more powerful; it allows for models of higher rank (more non-zero singular values) which implies a more powerful discriminator and eventually more accurate generator. This is demonstrated in comparison to weight normalization in Figure 4. The experimental results are very good and give strong support for the proposed normalization.


While the main idea is not new to machine learning (or deep learning), to the best of my knowledge it has not been applied on GANs. The paper is overall well written (though check Comment 3 below), it covers the related work well and it includes an insightful discussion about the importance of high rank models. I am recommending acceptance, though I anticipate to see a more rounded evaluation of the exact mechanism under which SN improves over the state of the art. More details in the comments below.

Comments:
1. One concern about this paper is that it doesn’t fully answer the reasons why this normalization works better. I found the discussion about rank to be very intuitive, however this intuition is not fully tested.  Figure 4 reports layer spectra for SN and WN. The authors claim that other methods, like (Arjovsky et al. 2017) also suffer from the same rank deficiency. I would like to see the same spectra included. 
2. Continuing on the previous point: maybe there is another mechanism at play beyond just rank that give SN its apparent edge? One way to test the rank hypothesis and better explain this method is to run a couple of truncated-SN experiments. What happens if you run your SN but truncate its spectrum after every iteration in order to make it comparable to the rank of WN? Do you get comparable inception scores? Or does SN still win?
3. Section 4 needs some careful editing for language and grammar.
","[7, 8, 7]","[' Good paper, accept', ' Top 50% of accepted papers, clear accept', ' Good paper, accept']","[4, 3, 2]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']","The review is largely positive. The reviewer finds the idea interesting, the paper well-written, and the results promising. They recommend acceptance but suggest further experiments to strengthen the claims and explanations. The language used is constructive and professional.",75.0,80.0
Learning to Represent Programs with Graphs,"['Miltiadis Allamanis', 'Marc Brockschmidt', 'Mahmoud Khademi']",Accept,2018,"[-1, 7, 0, 15, 26]","[4, 12, 5, 20, 31]","[9, 32, 7, 122, 405]","[2, 17, 3, 62, 214]","[7, 11, 3, 47, 70]","[0, 4, 1, 13, 121]","The paper introduces an application of Graph Neural Networks (Li's Gated Graph Neural Nets, GGNNs, specifically) for reasoning about programs and programming. The core idea is to represent a program as a graph that a GGNN can take as input, and train the GGNN to make token-level predictions that depend on the semantic context. The two experimental tasks were: 1) identifying variable (mis)use, ie. identifying bugs in programs where the wrong variable is used, and 2) predicting a variable's name by consider its semantic context.

The paper is generally well written, easy to read and understand, and the results are compelling. The proposed GGNN approach outperforms (bi-)LSTMs on both tasks. Because the tasks are not widely explored in the literature, it could be difficult to know how crucial exploiting graphically structured information is, so the authors performed several ablation studies to analyze  this out. Those results show that as structural information is removed, the GGNN's performance diminishes, as expected. As a demonstration of the usefulness of their approach, the authors ran their model on an unnamed open-source project and claimed to find several bugs, at least one of which potentially reduced memory performance.

Overall the work is important, original, well-executed, and should open new directions for deep learning in program analysis. I recommend it be accepted.","[8, 8, 8]","[' Top 50% of accepted papers, clear accept', ' Top 50% of accepted papers, clear accept', ' Top 50% of accepted papers, clear accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is overwhelmingly positive. The reviewer explicitly states the work is ""important, original, well-executed"" and recommends acceptance. They praise the writing, clarity, results, and ablation studies. There are no negative remarks, only constructive suggestions.",95.0,100.0
Training and Inference with Integers in Deep Neural Networks,"['Shuang Wu', 'Guoqi Li', 'Feng Chen', 'Luping Shi']",Accept,2018,"[13, 13, 26, 11]","[18, 18, 31, 16]","[178, 190, 293, 51]","[85, 40, 135, 14]","[23, 51, 28, 14]","[70, 99, 130, 23]","The authors describe a method called WAGE, which quantize all operands and operators in a neural network, specifically, the weights (W), activations (A), gradients (G), and errors (E) . The idea is using quantizers with clipping (denoted in the paper with Q(x,k)) and some additional operators like shift (denoted with shift(x)) and stochastic rounding. The main motivation of the authors in this work is to reduce the number of bits for representation in a network for all the WAGE operations and operands which influences the power consumption and silicon area in hardware implementations.

After introducing the idea and related work, the authors in Section 3 give details about how to perform the quantization. They introduce the additional operators needed for training in such network. Since quantization may loss some information, the authors need to quantize the signals in the network around the dynamic range in order not to ""kill"" the signal. The authors describe how to do that. Afterward, as in other techniques for quantization, they describe how to initialize the network values. Also, they argue that batch normalization in this network is replaced with the shift-quantize operations, and what is matter in this case is (1) the relative values (“orientations”) and not the absolute values and (2) small values in errors are negligible.

Afterward, the authors conduct experiments on MNIST, SVHN, CIFAR10, and ILSVRC12 datasets, where they show promising results compared to the errors provided by previous works. The WAGE parameters (i.e., the quantized no. of bits used) are 2-8-8-8, respectively. For understand more the WAGE, the authors compare on CIFAR10 the test error rate with vanilla CNN and show is small loss in using their network. The authors investigate mainly the bitwidth of errors and gradients.

In overall, this paper is an accept since it shows good performance on standard problems and invent some nice tricks to implement NN in hardware, for *both* training and inference. For inference only, other works has more to offer but this is a promising technique for learning. The things that are still missing in this work are some power reduction estimates as well as area reduction estimations. This will give the hardware community a clear vision of how such methods may be implemented both in data centers as well as on end portable devices. 
","[7, 8, 7]","[' Good paper, accept', ' Top 50% of accepted papers, clear accept', ' Good paper, accept']","[3, 4, 4]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer explicitly recommends the paper for acceptance, stating ""In overall, this paper is an accept"". They highlight the good performance and novel techniques. While they point out areas for improvement, these are framed as suggestions rather than criticisms. The language is formal and objective, typical of scientific peer reviews.",75.0,50.0
Variance Reduction for Policy Gradient with Action-Dependent Factorized Baselines,"['Cathy Wu', 'Aravind Rajeswaran', 'Yan Duan', 'Vikash Kumar', 'Alexandre M Bayen', 'Sham Kakade', 'Igor Mordatch', 'Pieter Abbeel']",Accept,2018,"[4, 4, 11, 17, 18, 20, 12, 17]","[9, 9, 15, 22, 23, 25, 17, 22]","[34, 62, 52, 117, 257, 299, 110, 617]","[18, 26, 28, 53, 132, 145, 49, 294]","[13, 34, 19, 45, 52, 127, 56, 296]","[3, 2, 5, 19, 73, 27, 5, 27]","This paper presents methods to reduce the variance of policy gradient using an action dependent baseline. Such action dependent baseline can be used in settings where the action can be decomposed into factors that are conditionally dependent given the state. The paper:
(1) shows that using separate baselines for actions, each of which can depend on the state and other actions is bias-free
(2) derive the optimal action-dependent baseline, showing that it does not degenerate into state-only dependent baseline, i.e. there is potentially room for improvement over state-only baselines.
(3) suggests using marginalized action-value (Q) function as a practical baseline, generalizing the use of value function in state-only baseline case.
(4) suggests using MC marginalization and also using the ""average"" action to improve computational feasibility
(5) combines the method with GAE techniques to further improve convergence by trading off bias and variance

The suggested methods are empirically evaluated on a number of settings. Overall action-dependent baseline outperform state-only versions. Using a single average action marginalization is on par with MC sampling, which the authors attribute to the low quality of the Q estimate. Combining GAE shows that a hint of bias can be traded off with further variance reduction to further improve the performance.

I find the paper interesting and practical to the application of policy gradient in high dimensional action spaces with some level of conditional independence present in the action space. In light of such results, one might change the policy space to enforce such structure.

Notes:
- Elaborate further on the assumption made in Eqn 9. Does it mean that the actions factors cannot share (too many) parameters in the policy construction, or that shared parameters can only be applied to the state?
- Eqn 11 should use \simeq
- How can the notion of average be extended to handle multi-modal distributions, or categorical or structural actions? Consider expanding on that in section 4.5.
- The discussion on the DAG graphical model is lacking experimental analysis (where separate baselines models are needed). How would you train such baselines?
- Figure 4 is impossible to read in print. The fonts are too small for the numbers and the legends.
","[7, 6, 8]","[' Good paper, accept', ' Marginally above acceptance threshold', ' Top 50% of accepted papers, clear accept']","[4, 4, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer explicitly states that they find the paper ""interesting and practical."" They also provide constructive criticism and suggestions for improvement, indicating a positive attitude towards the work. The reviewer does not use harsh language and maintains a professional and respectful tone throughout the review. Therefore, the sentiment is positive, and the language is polite.",75.0,75.0
"Progressive Growing of GANs for Improved Quality, Stability, and Variation","['Tero Karras', 'Timo Aila', 'Samuli Laine', 'Jaakko Lehtinen']",Accept,2018,"[9, 16, 15, 16]","[14, 21, 20, 21]","[56, 70, 67, 73]","[27, 29, 30, 22]","[20, 20, 18, 20]","[9, 21, 19, 31]","The paper describes a number of modifications of GAN training that enable synthesis of high-resolution images. The modifications also support more automated longer-term training, and increasing variability in the results.

The key modification is progressive growing. First, a GAN is trained for image synthesis at very low resolution. Then a layer that refines the resolution is progressively faded in. (More accurately, a corresponding pair of layers, one in the generation and one in the discriminator.) This progressive fading in of layers is repeated, one octave at a time, until the desired resolution is reached.

Another modification reported in the paper is a simple parameter-free minibatch summary statistic feature that is reported to increase variation. Finally, the paper describes simple schemes for initialization and feature normalization that are reported to be more effective than commonly used initializers and batchnorm.

It's a very nice paper. It does share the ""bag of tricks"" nature of many GAN papers, but as such it is better than most of the lot. I appreciate that some of the tricks actually simplify training, and most are conceptually reasonable. The paper is also very well written.

My quibbles are minor. First, I would discuss [Huang et al., CVPR 2017] and the following paper more prominently:

[Zhang et al., ICCV 2017] H. Zhang, T. Xu, H. Li, S. Zhang, X. Wang, X. Huang, and D. Metaxas. StackGAN: Text to photo-realistic image synthesis with stacked generative adversarial networks. In ICCV, 2017.

I couldn't find a discussion of [Huang et al., CVPR 2017] at all, although it's in the bibliography. (Perhaps I overlooked the discussion.) And [Zhang et al., ICCV 2017] is quite closely related, since it also tackles high-resolution synthesis via multi-scale refinement. These papers don't diminish the submission, but they should be clearly acknowledged and the contribution of the submission relative to these prior works should be discussed.

Also, [Rabin et al., 2011] is cited in Section 5 but I couldn't find it in the bibliography.
","[8, 8, 1]","[' Top 50% of accepted papers, clear accept', ' Top 50% of accepted papers, clear accept', ' Trivial or wrong']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer clearly states ""It's a very nice paper"" and that they ""appreciate"" some of the work's aspects. They call their concerns ""quibbles"", which indicates an overall positive view. The reviewer does point out some missing references and comparisons, which slightly reduces the overall positivity. Hence, a sentiment score of 80 is appropriate. The language used is polite and professional throughout, justifying a politeness score of 100.",80.0,100.0
Synthetic and Natural Noise Both Break Neural Machine Translation,"['Yonatan Belinkov', 'Yonatan Bisk']",Accept,2018,"[6, 9]","[11, 14]","[131, 108]","[57, 51]","[65, 55]","[9, 2]","This paper investigates the impact of character-level noise on various flavours of neural machine translation. It tests 4 different NMT systems with varying degrees and types of character awareness, including a novel meanChar system that uses averaged unigram character embeddings as word representations on the source side. The authors test these systems under a variety of noise conditions, including synthetic scrambling and keyboard replacements, as well as natural (human-made) errors found in other corpora and transplanted to the training and/or testing bitext via replacement tables. They show that all NMT systems, whether BPE or character-based, degrade drastically in quality in the presence of both synthetic and natural noise, and that it is possible to train a system to be resistant to these types of noise by including them in the training data. Unfortunately, they are not able to show any types of synthetic noise helping address natural noise. However, they are able to show that a system trained on a mixture of error types is able to perform adequately on all types of noise.

This is a thorough exploration of a mostly under-studied problem. The paper is well-written and easy to follow. The authors do a good job of positioning their study with respect to related work on black-box adversarial techniques, but overall, by working on the topic of noisy input data at all, they are guaranteed novelty. The inclusion of so many character-based systems is very nice, but it is the inclusion of natural sources of noise that really makes the paper work. Their transplanting of errors from other corpora is a good solution to the problem, and one likely to be built upon by others. In terms of negatives, it feels like this work is just starting to scratch the surface of noise in NMT. The proposed meanChar architecture doesn’t look like a particularly good approach to producing noise-resistant translation systems, and the alternative solution of training on data where noise has been introduced through replacement tables isn’t extremely satisfying. Furthermore, the use of these replacement tables means that even when the noise is natural, it’s still kind of artificial. Finally, this paper doesn’t seem to be a perfect fit for ICLR, as it is mostly experimental with few technical contributions that are likely to be impactful; it feels like it might be more at home and have greater impact in a *ACL conference.

Regarding the artificialness of their natural noise - obviously the only solution here is to find genuinely noisy parallel data, but even granting that such a resource does not yet exist, what is described here feels unnaturally artificial. First of all, errors learned from the noisy data sources are constrained to exist within a word. This tilts the comparison in favour of architectures that retain word boundaries (such as the charCNN system here), while those systems may struggle with other sources of errors such as missing spaces between words. Second, if I understand correctly, once an error is learned from the noisy data, it is applied uniformly and consistently throughout the training and/or test data. This seems worse than estimating the frequency of the error and applying them stochastically (or trying to learn when an error is likely to occur). I feel like these issues should at least be mentioned in the paper, so it is clear to the reader that there is work left to be done in evaluating the system on truly natural noise.

Also, it is somewhat jarring that only the charCNN approach is included in the experiments with noisy training data (Table 6). I realize that this is likely due to computational or time constraints, but it is worth providing some explanation in the text for why the experiments were conducted in this manner. On a related note, the line in the abstract stating that “... a character convolutional neural network  is able to simultaneously learn representations robust to multiple kinds of noise” implies that the other (non-charCNN) architectures could not learn these representations, when in reality, they simply weren’t given the chance.

Section 7.2 on the richness of natural noise is extremely interesting, but maybe less so to an ICLR audience. From my perspective, it would be interesting to see that section expanded, or used as the basis for future work on improve architectures or training strategies.

I have only one small, specific suggestion: at the end of Section 3, consider deleting the last paragraph break, so there is one paragraph for each system (charCNN currently has two paragraphs).

[edited for typos]","[7, 7, 8]","[' Good paper, accept', ' Good paper, accept', ' Top 50% of accepted papers, clear accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is positive about the paper's novelty, thoroughness, and writing quality. It praises the exploration of natural noise and the use of error transplantation. However, it points out limitations such as the artificiality of the natural noise simulation, the limited exploration of noise-resistant architectures, and the lack of groundbreaking technical contributions. The reviewer suggests the paper might be better suited for an ACL conference due to its experimental nature. The language used is polite and constructive throughout, offering suggestions for improvement rather than harsh criticism.",60.0,80.0
Multi-Scale Dense Networks for Resource Efficient Image Classification,"['Gao Huang', 'Danlu Chen', 'Tianhong Li', 'Felix Wu', 'Laurens van der Maaten', 'Kilian Weinberger']",Accept,2018,"[7, 3, 3, 25, 14, 15]","[12, 4, 8, 30, 19, 20]","[215, 12, 27, 54, 118, 201]","[72, 5, 15, 22, 57, 108]","[90, 5, 11, 26, 48, 82]","[53, 2, 1, 6, 13, 11]","This paper presents a method for image classification given test-time computational budgeting constraints.  Two problems are considered:  ""any-time"" classification, in which there is a time constraint to evaluate a single example, and batched budgets, in which there is a fixed budget available to classify a large batch of images.  A convolutional neural network structure with a diagonal propagation layout over depth and scale is used, so that each activation map is constructed using dense connections from both same and finer scale features.  In this way, coarse-scale maps are constructed quickly, then continuously updated with feed-forward propagation from lower layers and finer scales, so they can be used for image classification at any intermediate stage.  Evaluations are performed on ImageNet and CIFAR-100.

I would have liked to see the MC baselines also evaluated on ImageNet --- I'm not sure why they aren't there as well?  Also on p.6 I'm not entirely clear on how the ""network reduction"" is performed --- it looks like finer scales are progressively dropped in successive blocks, but I don't think they exactly correspond to those that would be needed to evaluate the full model (this is ""lazy evaluation"").  A picture would help here, showing where the depth-layers are divided between blocks.

I was also initially a bit unclear on how the procedure described for batched budgeted evaluation achieves the desired result:  It seems this relies on having a batch that is both large and varied, so that its evaluation time will converge towards the expectation.  So this isn't really a hard constraint (just an expected result for batches that are large and varied enough).  This is fine, but could perhaps be pointed out if that is indeed the case.

Overall, this seems like a natural and effective approach, and achieves good results.
","[7, 10, 8]","[' Good paper, accept', ' Top 5% of accepted papers, seminal paper', ' Top 50% of accepted papers, clear accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer provides constructive criticism and suggestions for improvement, but ultimately finds the approach natural, effective, and yielding good results. This suggests a generally positive sentiment. The language used is polite and professional, employing phrases like ""I would have liked to see"" and ""it seems"" to express suggestions and observations without being overly critical.",60.0,80.0
AmbientGAN: Generative models from lossy measurements,"['Ashish Bora', 'Eric Price', 'Alexandros G. Dimakis']",Accept,2018,"[5, 12, 17]","[7, 17, 22]","[8, 122, 301]","[4, 60, 124]","[4, 58, 135]","[0, 4, 42]","The paper proposes an approach to train generators within a GAN framework, in the setting where one has access only to degraded / imperfect measurements of real samples, rather than the samples themselves. Broadly, the approach is to have a generator produce the ""full"" real data, pass it through a simulated model of the measurement process, and then train the discriminator to distinguish between these simulated measurements of generated samples, and true measurements of real samples. By this mechanism, the proposed method is able to train GANs to generate high-quality samples from only imperfect measurements.

The paper is largely well-written and well-motivated, the overall setup is interesting (I find the authors' practical use cases convincing---where one only has access to imperfect data in the first place), and the empirical results are convincing. The theoretical proofs do make strong assumptions (in particular, the fact that the true distribution must be uniquely constrained by its marginal along the measurement). However, in most theoretical analysis of GANs and neural networks in general, I view proofs as a means of gaining intuition rather than being strong guarantees---and to that end, I found the analysis in this paper to be informative.

I would make a  suggestions for possible further experimental analysis: it would be nice to see how robust the approach is to systematic mismatches between the true and modeled measurement functions (for instance, slight differences in the blur kernels, noise variance, etc.). Especially in the kind of settings the paper considers, I imagine it may sometimes also be hard to accurately model the measurement function of a device (or it may be necessary to use a computationally cheaper approximation for training). I think a study of how such mismatches affect the training procedure would be instructive (perhaps more so than some of the quantitative evaluation given that they at best only approximately measure sample quality).","[8, 7, 7]","[' Top 50% of accepted papers, clear accept', ' Good paper, accept', ' Good paper, accept']","[4, 4, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The reviewer provides positive feedback, calling the paper ""well-written and well-motivated"" and the results ""convincing."" They find the theoretical analysis informative, even acknowledging its limitations. While they suggest further experiments, they frame it as potential improvement rather than a flaw.",75.0,90.0
Beyond Word Importance:  Contextual Decomposition to Extract Interactions from LSTMs,"['W. James Murdoch', 'Peter J. Liu', 'Bin Yu']",Accept,2018,"[2, 3, 27]","[4, 8, 32]","[10, 35, 137]","[4, 12, 47]","[6, 21, 46]","[0, 2, 44]","This article aims at understanding the role played by the different words in a sentence, taking into account their order in the sentence. In sentiment analysis for instance, this capacity is critical to model properly negation.
As state-of-the-art approaches rely on LSTM, the authors want to understand which information comes from which gate. After a short remainder regarding LSTM, the authors propose a framework to disambiguate interactions between gates. In order to obtain an analytic formulation of the decomposition, the authors propose to linearize activation functions in the network.
In the experiment section, authors compare themselves to a standard logistic regression (based on a bag of words representation). They also check the unigram sentiment scores (without context).
The main issue consists in modeling the dynamics inside a sentence (when a negation or a 'used to be' reverses the sentiment). The proposed approach works fine on selected samples.


The related work section is entirely focused on deep learning while the experiment section is dedicated to sentiment analysis. This section should be rebalanced. Even if the authors claim that their approach is general, they also show that it fits well the sentiment analysis task in particular.

On top of that, a lot of fine-grained sentiment analysis tools has been developed outside deep-learning: the authors should refer to those works.

Finally, authors should provide some quantitative analysis on sentiment classification: a lot of standard benchmarks are widely use in the literature and we need to see how the proposed method performs with respect to the state-of-the-art.


Given the chosen tasks, this work should be compared to the beermind system:
http://deepx.ucsd.edu/#/home/beermind
and the associated publication
http://arxiv.org/pdf/1511.03683.pdf","[7, 7, 7]","[' Good paper, accept', ' Good paper, accept', ' Good paper, accept']","[3, 4, 2]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']","The review presents a mixed sentiment. While it acknowledges the relevance of the research question and the proposed framework, it points out significant shortcomings, particularly the lack of comprehensive comparison with existing methods and limited quantitative analysis. The language, however, remains polite and constructive throughout, suggesting areas of improvement rather than outright criticism.",-10.0,60.0
Wasserstein Auto-Encoders,"['Ilya Tolstikhin', 'Olivier Bousquet', 'Sylvain Gelly', 'Bernhard Schoelkopf']",Accept,2018,"[8, 10, 14, 6]","[13, 14, 19, 11]","[33, 64, 92, 67]","[11, 27, 58, 25]","[16, 24, 27, 30]","[6, 13, 7, 12]","This very well written paper covers the span between W-GAN and VAE. For a reviewer who is not an expert in the domain, it reads very well, and would have been of tutorial quality if space had allowed for more detailed explanations. The appendix are very useful, and tutorial paper material (especially A). 

While I am not sure description would be enough to reproduce and no code is provided, every aspect of the architecture, if not described, if referred as similar to some previous work. There are also some notation shortcuts (not explained) in the proof of theorems that can lead to initial confusion, but they turn out to be non-ambiguous. One that could be improved is P(P_X, P_G) where one loses the fact that the second random variable is Y.


This work contains plenty of novel material, which is clearly compared to previous work:
- The main consequence of the use of Wasserstein distance is the surprisingly simple and useful Theorem 1. I could not verify its novelty, but this seems to be a great contribution.
- Blending GAN and auto-encoders has been tried in the past, but the authors claim better theoretical foundations that lead to solutions that do not rquire min-max
- The use of MMD in the context of GANs has also been tried. The authors claim that their use in the latent space makes it more practival

The experiments are very convincing, both numerically and visually.

Source of confusion: in algorithm 1 and 2, \tilde{z} is ""sampled"" from Q_TH(Z|xi), some one is lead to believe that this is the sampling process as in VAEs, while in reality Q_TH(Z|xi) is deterministic in the experiments.","[8, 8, 8]","[' Top 50% of accepted papers, clear accept', ' Top 50% of accepted papers, clear accept', ' Top 50% of accepted papers, clear accept']","[3, 4, 3]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct']","The review starts with very positive statements about the paper's clarity and contribution ('very well written', 'tutorial quality', 'plenty of novel material', 'great contribution'). It also praises the experiments as 'very convincing'. While it points out areas for improvement, it frames them constructively. The language is formal and focuses on the paper's strengths and weaknesses without resorting to personal criticism.",85.0,90.0
Boosting Dilated Convolutional Networks with Mixed Tensor Decompositions,"['Nadav Cohen', 'Ronen Tamari', 'Amnon Shashua']",Accept,2018,"[7, 3, 31]","[12, 7, 36]","[57, 23, 144]","[24, 9, 84]","[28, 13, 40]","[5, 1, 20]","To date the theoretical advantage of deep learning has focused on the concept of ""expressive efficiency"" where one network must grow much larger to replicate functions that another ""more efficient"" network can produce. This has focused so far on depth (i.e. shallow networks have to grow much larger than deeper networks to express the same set of networks)

The authors explore another dimension here, namely that of ""connectivity"". They study dilated convolutional networks and show that intertwining two dilated convolutional networks A and B at various stages (formalized via mixed tensor decompositions) it is more expressively efficient than not intertwining. 

The authors' experiments support their theory showing that their mixed strategy leads to gains over a vanilla dilated convolutional net.

I found the paper very well written despite its level of mathematical depth (the authors provide many helpful pictures) and strongly recommend accepting this paper.

","[9, 8, 7]","[' Top 15% of accepted papers, strong accept', ' Top 50% of accepted papers, clear accept', ' Good paper, accept']","[4, 3, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is highly positive. The reviewer explicitly recommends acceptance and praises the paper's clarity, writing, and mathematical depth. They find the paper's exploration of ""connectivity"" in deep learning to be valuable and well-supported by experiments. The language used throughout is professional and courteous.",90.0,100.0
Emergence of Linguistic Communication from  Referential Games with Symbolic and Pixel Input,"['Angeliki Lazaridou', 'Karl Moritz Hermann', 'Karl Tuyls', 'Stephen Clark']",Accept,2018,"[8, 7, 17, 20]","[13, 9, 22, 25]","[65, 44, 242, 139]","[33, 22, 145, 94]","[30, 20, 53, 28]","[2, 2, 44, 17]","--------------
Summary:
--------------
This paper presents a series of experiments on language emergence through referential games between two agents. They ground these experiments in both fully-specified symbolic worlds and through raw, entangled, visual observations of simple synthetic scenes. They provide rich analysis of the emergent languages the agents produce under different experimental conditions. This analysis (especially on raw pixel images) make up the primary contribution of this work.


--------------
Evaluation:
--------------
Overall I think the paper makes some interesting contributions with respect to the line of recent 'language emergence' papers. The authors provide novel analysis of the learned languages and perceptual system across a number of environmental settings, coming to the (perhaps uncontroversial) finding that varying the environment and restrictions on language result in variations in the learned communication protocols. 

In the context of existing literature, the novelty of this work is somewhat limited -- consisting primarily of the extension of multi-agent reference games to raw-pixel inputs. While this is a non-trivial extension, other works have demonstrated language learning in similar referring-expression contexts (essentially modeling only the listener model [Hermann et.al 2017]). 

I have a number of requests for clarification in the weaknesses section which I think would improve my understanding of this work and result in a stronger submission if included by the authors.  

--------------
Strengths:
--------------
- Clear writing and document structure. 


- Extensive experimental setting tweaks which ablate the information and regularity available to the agents. The discussion of the resulting languages is appropriate and provides some interesting insights.


- A number of novel analyses are presented to evaluate the learned languages and perceptual systems. 


--------------
Weaknesses:
--------------
- How stable are the reported trends / languages across multiple runs within the same experimental setting? The variance of REINFORCE policy gradients (especially without a baseline) plus the general stochasticity of SGD on randomly initialized networks leads me to believe that multiple training runs of these agents might result is significantly different codes / performance. I am interested in hearing the author's experiences in this regard and if multiple runs present similar quantitative and qualitative results. I admit that expecting identical codes is unrealistic, but the form of the codes (i.e. primarily encoding position) might be consistent even if the individual mappings are not).


- I don't recall seeing descriptions of the inference-time procedure used to evaluate training / test accuracy. I will assume argmax decoding for both speaker and listener. Please clarify or let me know if I missed something.


- There is ambiguity in how the ""protocol size"" metric is computed. In Table 1, it is defined as 'the effective number of unique message used'. This comes back to my question about decoding I suppose, but does this count the 'inference-time' messages or those produced during training? 
Furthermore, Table 2 redefines ""protocol size"" as the percentage of novel message. I assume this is an editing error given the values presented and take these columns as counts. It also seems ""protocol size"" is replaced with the term ""lexicon"" from 4.1 onward.

- I'm surprised by how well the agents generalize in the raw pixel data experiments. In fact, it seems that across all games the test accuracy remains very close to the train accuracy. 

Given the dataset is created by taking all combinations of color / shape and then sampling 100 location / floor color variations, it is unlikely that a shape / color combo has not been seen in training. Such that the only novel variations are likely location and floor color. However, taking Game A as an example, the probe classifiers are relatively poor at these attributes -- indicating the speaker's representation is not capturing these attributes well. Then how do the agents effectively differentiate so well between 20 images leveraging primarily color and shape?

I think some additional analysis of this setting might shed some light on this issue. One thought is to compute upper-bounds based on ground truth attributes. Consider a model which knows shape perfectly, but cannot predict other attributes beyond chance. To compute the performance of such a model, you could take the candidate set, remove any instances not matching the ground truth shape, and then pick randomly from the remaining instances. Something similar could be repeated for all attributes independently as well as their combinations -- obviously culminating in 100% accuracy given all 4. It could be that by dataset construction, object location and shape are sufficient to achieve high accuracy because the odds of seeing the same shape at the same location (but different color) is very low. 

Given these are operations on annotations and don't require time-consuming model training, I hope to see this analysis in the rebuttal to put the results into appropriate context.


- What is random chance for the position and floor color probe classifiers? I don't think it is mentioned how many locations / floor colors are used in generation.  


- Relatively minor complaint: Both agents are trained via the REINFORCE policy gradient update rule; however, the listener agent makes a fairly standard classification decision and could be trained with a standard cross-entropy loss. That is to say, the listener policy need not make intermediate discrete policy decisions. This decision to withhold available supervision is not discussed in the paper (as far as I noticed), could the authors speak to this point?



--------------
Curiosities:
--------------
- I got the impression from the results (specifically the lack of discussion about message length) that in these experiments agents always issued full length messages even though they did not need to do so. If true, could the authors give some intuition as to why? If untrue, what sort of distribution of lengths do you observe?

- There is no long term planning involved in this problem, so why use reinforcement learning over some sort of differentiable sampler? With some re-parameterization (i.e. Gumbel-Softmax), this model could be end-to-end differentiable.


--------------
Minor errors:
--------------
[2.2 paragraph 1] LSTM citation should not be in inline form.
[3 paragraph 1] 'Note that these representations do care some' -> carry
[3.3.1 last paragraph] 'still able comprehend' --> to


-------
Edit
-------
Updating rating from 6 to 7.","[7, 9, 5]","[' Good paper, accept', ' Top 15% of accepted papers, strong accept', ' Marginally below acceptance threshold']","[4, 5, 4]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is confident but not absolutely certain that the evaluation is correct']","The review is overall positive. The reviewer finds the paper interesting and well-written, highlighting the extensive experiments and novel analyses as strengths. While acknowledging the work's novelty is somewhat limited and primarily lies in extending existing methods to raw-pixel inputs, the reviewer finds value in the research. The reviewer poses several clarifying questions and suggestions for improvement, indicating a desire to see the work strengthened rather than rejected. The reviewer's tone is constructive and polite, suggesting areas for improvement without resorting to harsh or disrespectful language.",60.0,80.0
On the insufficiency of existing momentum schemes for Stochastic Optimization,"['Rahul Kidambi', 'Praneeth Netrapalli', 'Prateek Jain', 'Sham M. Kakade']",Accept,2018,"[7, 9, 15, 20]","[11, 14, 20, 25]","[33, 126, 221, 299]","[16, 55, 113, 145]","[16, 64, 94, 127]","[1, 7, 14, 27]","I wonder how the ASGD compares to other optimization schemes applicable to DL, like Entropy-SGD, which is yet another algorithm that provably improves over SGD. This question is also valid when it comes to other optimization schemes that are designed for deep learning problems. For instance, Entropy-SGD and Path-SGD should be mentioned and compared with. As a consequence, the literature analysis is insufficient. 

Authors provided necessary clarifications. I am raising my score.




","[7, 7, 8]","[' Good paper, accept', ' Good paper, accept', ' Top 50% of accepted papers, clear accept']","[3, 4, 5]","[' The reviewer is fairly confident that the evaluation is correct', ' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']","The review starts with a critical point, questioning the positioning of the research against other optimization schemes. This suggests a somewhat negative sentiment. However, the language remains technical and lacks overtly negative phrasing. The second part of the review, where the reviewer mentions raising their score due to clarifications, indicates a shift towards a more positive view. Overall, the tone is critical but professional, suggesting room for improvement in the paper.",-20.0,50.0
Ask the Right Questions: Active Question Reformulation with Reinforcement Learning,"['Christian Buck', 'Jannis Bulian', 'Massimiliano Ciaramita', 'Wojciech Gajewski', 'Andrea Gesmundo', 'Neil Houlsby', 'Wei Wang.']",Accept,2018,"[11, 7, 19, 2, 10, 8, 2]","[15, 11, 24, 6, 15, 13, 6]","[30, 20, 59, 9, 36, 86, 12]","[18, 6, 42, 3, 16, 31, 6]","[8, 11, 13, 5, 19, 50, 6]","[4, 3, 4, 1, 1, 5, 0]","This paper formulates the Jeopardy QA as a query reformulation task that leverages a search engine.  In particular, a user will try a sequence of alternative queries based on the original question in order to find the answer.  The RL formulation essentially tries to mimic this process.  Although this is an interesting formulation, as promoted by some recent work, this paper does not provide compelling reasons why it's a good formulation.  The lack of serious comparisons to baseline methods makes it hard to judge the value of this work.

Detailed comments/questions:
	1. I am actually quite confused on why it's a good RL setting. For a human user, having a series of queries to search for the right answer is a natural process, but it's not natural for a computer program.  For instance, each query can be viewed as different formulation of the same question and can be issued concurrently. Although formulated as an RL problem,  it is not clear to me whether the search result after each episode has been used as the immediate environment feedback. As a result, the dependency between actions seems rather weak.
	2. I also feel that the comparisons to other baselines (not just the variation of the proposed system) are not entirely fair. For instance, the baseline BiDAF model has only one shot, namely using the original question as query.  In this case, AQA should be allowed to use the same budget -- only one query.  Another more realistic baseline is to follow the existing work on query formulation in the IR community.  For example, 20 shorter queries generated by methods like [1] can be used to compare the queries created by AQA.

[1] Kumaran & Carvalho. ""Reducing Long Queries Using Query Quality Predictors"". SIGIR-09
	
Pros:
	1. An interesting RL formulation for query reformulation

Cons:
	1. The use of RL is not properly justified
	2. The empirical result is not convincing that the proposed method is indeed advantageous 

---------------------------------------

After reading the author response and checking the revised paper, I'm both delighted and surprised that the authors improved the submission substantially and presented stronger results.  I believe the updated version has reached the bar and recommend accepting this paper.  ","[6, 7, 8]","[' Marginally above acceptance threshold', ' Good paper, accept', ' Top 50% of accepted papers, clear accept']","[4, 5, 3]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', ' The reviewer is fairly confident that the evaluation is correct']","The reviewer initially expresses confusion and skepticism towards the paper's approach, questioning its validity and fairness of comparisons. This, combined with the use of phrases like ""not compelling reasons,"" ""not entirely fair,"" and ""not convincing,"" suggests a negative sentiment. However, the tone is professional and provides constructive criticism. The final paragraph reveals a significant shift in sentiment after the authors addressed the reviewer's concerns. The reviewer's delight and surprise at the improvements, coupled with the recommendation for acceptance, indicate a now positive sentiment. The language throughout remains polite, offering suggestions and critiques rather than harsh judgments.",-10.0,70.0
Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality,"['Xingjun Ma', 'Bo Li', 'Yisen Wang', 'Sarah M. Erfani', 'Sudanthi Wijewickrema', 'Grant Schoenebeck', 'Dawn Song', 'Michael E. Houle', 'James Bailey']",Accept,2018,"[2, 10, 4, 8, 13, 15, 20, 34, 24]","[7, 15, 9, 13, 17, 20, 25, 38, 29]","[110, 302, 140, 129, 43, 112, 451, 108, 296]","[50, 126, 66, 74, 30, 59, 256, 69, 184]","[55, 148, 63, 46, 8, 43, 163, 7, 46]","[5, 28, 11, 9, 5, 10, 32, 32, 66]","The authors clearly describe the problem being addressed in the manuscript and motivate their solution very clearly. The proposed solution seems very intuitive and the empirical evaluations demonstrates its utility. My main concern is the underlying assumption (if I understand correctly) that the adversarial attack technique that the detector has to handle needs to be available at the training time of the detector. Especially since the empirical evaluations are designed in such a way where the training and test data for the detector are perturbed with the same attack technique. However, this does not invalidate the contributions of this manuscript.

Specific comments/questions:
- (Minor) Page 3, Eq 1: I think the expansion dimension cares more about the probability mass in the volume rather than the volume itself even in the Euclidean setting.
- Section 4: The different pieces of the problem (estimation, intuition for adversarial subspaces, efficiency) are very well described.
- Alg 1, L3: Is this where the normal exmaples are converted to adversarial examples using some attack technique? 
- Alg 1, L12: Is LID_norm computed using a leave-one-out estimate? Otherwise, r_1(.) for each point is 0, leading to a somewhat ""under-estimate"" of the true LID of the normal points in the training set. I understand that it is not an issue in the test set.
- Section 4 and Alg 1: S we do not really care about the ""labels/targets"" of the examples. All examples in the dataset are considered ""normal"" to start with. Is this assuming that the ""initial training set"" which is used to obtain the ""pre-trained DNN"" free of adversarial examples?
- Section 5, Experimental Setup: Seems like normal points in the test set would get lesser values if we are not doing the ""leave-one-out"" version of the estimation.
- Section 5: The authors have done a great job at evaluating every aspect of the proposed method.
","[7, 8, 6]","[' Good paper, accept', ' Top 50% of accepted papers, clear accept', ' Marginally above acceptance threshold']","[4, 3, 1]","[' The reviewer is confident but not absolutely certain that the evaluation is correct', ' The reviewer is fairly confident that the evaluation is correct', "" The reviewer's evaluation is an educated guess""]","The reviewer starts by highlighting the clarity and intuitiveness of the proposed solution, indicating a positive sentiment. While they raise a valid concern about the assumption of attack techniques being available during training, they acknowledge it doesn't invalidate the contributions. The use of phrases like ""very well described"" and ""great job"" further suggests a positive sentiment. The reviewer maintains a neutral and professional tone throughout, focusing on specific aspects of the work without resorting to personal opinions or harsh language.",60.0,80.0

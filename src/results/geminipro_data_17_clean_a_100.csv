review,reasoning,sentiment_score,politeness_score
"The paper address the problem of detecting if an example is misclassified or out-of-distribution. This is an very important topic and the study provides a good baseline. Although it misses strong novel methods for the task, the study contributes to the community.","The review acknowledges the importance of the topic and describes the work as a ""good baseline"". However, it also mentions that the study ""misses strong novel methods"".  Therefore, the sentiment is slightly positive but not enthusiastic. The language used is neutral and professional, without any negative or overly positive phrasing.",20,50
"This paper proposed a compare-aggregate model for the NLP tasks that require semantically comparing the text sequences, such as question answering and textual entailment. The basic framework of this model is to apply a convolutional neural network (aggregation) after a element-wise operation (comparison) over the attentive outputs of the LSTMs. The highlighted part is the comparison, where this paper compares several different methods for matching text sequences, and the element-wise subtraction/multiplication operations are demonstrated to achieve generally better performance on four different datasets. While the weak point is that this is an incremental work and a bit lack of innovation. A qualitative evaluation about how subtraction, multiplication and other comparison functions perform on varied kinds of sentences would be more interesting.","The review acknowledges the merits of the paper, such as its exploration of different comparison methods and its achievement of good performance. However, it also points out a significant weakness, stating that the work is incremental and lacks innovation. The reviewer also suggests an interesting direction for further exploration. Overall, the feedback is balanced, suggesting a slightly positive sentiment.",20,50
"Summary === This paper proposes the Neural Physics Engine (NPE), a network architecture which simulates object interactions. While NPE decides to explicitly represent objects (rather than video frames), it incorporates knowledge of physics almost exclusively through training data. It is tested in a toy domain with bouncing 2d balls. The proposed architecture processes each object in a scene one at a time. Pairs of objects are embedded in a common space where the effect of the objects on each other can be represented. These embeddings are summed and combined with the focus object*s state to predict the focus object*s change in velocity. Alternative baselines are presented which either forego the pairwise embedding for a single object embedding or encode a focus object*s neighbors in a sequence of LSTM states. NPE outperforms the baselines dramatically, showing the importance of architecture choices in learning to do this object based simulation. The model is tested in multiple ways. Ability to predict object trajectory over long time spans is measured. Generalization to different numbers of objects is measured. Generalization to slightly altered environments (difference shaped walls) is measured. Finally, the NPE is also trained to predict object mass using only interactions with other objects, where it also outperforms baselines. Comments === * I have one more clarifying question. Are the inputs to the blue box in figure 3 (b)/(c) the concatenation of the summed embeddings and state vector of object 3? Or is the input to the blue module some other combination of the two vectors? * Section 2.1 begins with *First, because physics does not change across inertial frames, it suffices to separately predict the future state of each object conditioned on the past states of itself and the other objects in its neighborhood, similar to Fragkiadaki et al. (2015).* I think this is an argument to forego the visual representation used by previous work in favor of an object only representation. This would be more clear if there were contrast with a visual representation. * As addressed in the paper, this approach is novel, though less so after taking into consideration the concurrent work of Battaglia et. al. in NIPS 2016 titled *Interaction Networks for Learning about Objects, Relations and Physics.* This work offers a different network architecture and set of experiments, as well as great presentation, but the use of an object based representation for learning to predict physical behavior is shared. Overall Evaluation === This paper was a pleasure to read and provided many experiments that offered clear and interesting conclusions. It offers a novel approach (though less so compared to the concurrent work of Battaglia et. al. 2016) which represents a significant step forward in the current investigation of intuitive physics.","The review explicitly states the paper ""was a pleasure to read"" and highlights its clarity, interesting conclusions, novel approach, and significance to the field. While it does mention a concurrent work, it doesn't detract from the paper's value. The reviewer also offers constructive questions and suggestions. All of this points towards a positive sentiment with polite and professional language.",80,90
"This paper addresses the problem of efficient neural stylization. Instead of training a separate network for N different styles (as is done, e.g., in Johnson et al.), this paper extends the instance normalization work of Ulyanov et al. to train a single network and learn a smaller set “conditional instance normalization” parameters dependent on the desired output style. The conditional instance normalization applies a learnt affine transformation on normalized feature maps at each layer in the network. Qualitative results are shown. I have not worked in this area, but I’m generally aware of the main issues in transferring artistic style. The paper addresses a known challenge of incorporating different styles into the same net, which have a number of practical benefits. As far as I can tell the results look compelling. As I’m less confident in my expertise in this area, I’m happy to support another reviewer who is willing to champion this paper. My main comments are on the paper writing. As far as I understand, the main novelty of the approach starts in Section 2.2, and before that is review of prior art. If this is indeed the case, one suggestion is to remove the subsection heading for 2.1 so it’s grouped with the first part of Section 2, and to cite related work for the feedforward network (e.g., Johnson et al.) in the text and in Fig 2 so it’s clear. In fact, I’m wondering if Figs 2 and 3 can be combined somehow so that the contribution is clearer in the figures. I was at first confused by Eq (5) as x and z are not defined anywhere. Also, it may be helpful to write out everything explicitly as is done in the instance normalization paper. In Eq (4), perhaps you could write T_s to emphasize that there are separate networks for different styles. Fig 5 left: I’m assuming the different colors correspond to the different styles. If so, perhaps mention this in the caption. Also, this figure is hard to read. Maybe instead show single curves with error bars that are averages over the loss curves for N-styles and individual styles. Typos: Page 1: Shouldn’t it be “VGG-16” network (not *VGG-19”)? Page 2: “newtork” => “network”. Paragraph after Eq. (5): “much less” => “fewer”.","The reviewer starts by acknowledging the paper addresses a relevant problem and finds the results compelling. They mention a lack of expertise in the specific area but seem open to accepting the paper. The rest of the review focuses on improving clarity and presentation, which points towards a positive sentiment overall. The language used is constructive and professional throughout the review.",60,80
"I like the idea in this paper that use not just one but multiple attentional vectors to extract multiple representations for a sentence. The authors have demonstrated consistent gains across three different tasks Age, Yelp, & SNLI. However, I*d like to see more analysis on the 2D representations (as concerned by another reviewer) to be convinced. Specifically, r=30 seems to be a pretty large value when applying to short sentences like tweets or those in the SNLI dataset. I*d like to see the effect of varying r from small to large value. With large r value, I suspect your models might have an advantage in having a much larger number of parameters (specifically in the supervised components) compare to other models. To make it transparent, the model sizes should be reported. I*d also like to see performances on the dev sets or learning curves. In the conclusion, the authors remark that *attention mechanism reliefs the burden of LSTM*. If the 2D representations are effective in that aspect, I*d expect that the authors might be able to train with a smaller LSTM. Testing the effect of LSTM dimension vs will be helpful. Lastly, there is a problem in the presentation of the paper in which there is no training objective defined. Readers have to read until the experimental sections to guess that the authors perform supervised learning and back-prop through the self-attention mechanism as well as the LSTM. * Minor comments: Typos: netowkrs, toghter, performd Missing year for the citation of (Margarit & Subramaniam) In figure 3, attention plotswith and without penalization look similar.","The reviewer starts with positive feedback, highlighting the novelty and consistent gains. However, they raise several concerns and request further analysis and experiments. The tone is constructive and suggestive, not dismissive. Therefore, the sentiment is slightly positive, but the numerous requests for improvement indicate it's not overwhelmingly positive.",60,70
"This is a good paper with an interesting probabilistic motivation for weighted bag of words models. The (hopefully soon) added comparison to Wang and Manning will make it stronger. Though it is sad that for sufficiently large datasets, NB-SVM still works better. In the second to last paragraph of the introduction you describe a problem of large cooccurrence counts which was already fixed by the Glove embeddings with their weighting function f. Minor comments: *The capturing the similarities* -- typo in line 2 of intro. *Recently, (Wieting et al.,2016) learned* -- use citet instead of parenthesized citation","The review starts with positive statements like ""This is a good paper"" and ""interesting probabilistic motivation"", indicating a positive sentiment. While it mentions a limitation (NB-SVM performing better) and some minor comments, the overall tone is constructive and encouraging. The language used is polite and professional throughout, with suggestions phrased as requests or observations rather than commands.",60,80
"The authors of the paper set out to answer the question whether chaotic behaviour is a necessary ingredient for RNNs to perform well on some tasks. For that question*s sake, they propose an architecture which is designed to not have chaos. The subsequent experiments validate the claim that chaos is not necessary. This paper is refreshing. Instead of proposing another incremental improvement, the authors start out with a clear hypothesis and test it. This might set the base for future design principles of RNNs. The only downside is that the experiments are only conducted on tasks which are known to be not that demanding from a dynamical systems perspective; it would have been nice if the authors had traversed the set of data sets more to find data where chaos is actually necessary.","The review is positive overall. It highlights the novelty and clarity of the paper's hypothesis and methodology. While it mentions a downside, it is presented as a suggestion for further research rather than a flaw. The language used is objective and professional.",75,80
"This paper provides an interesting idea, which extends GAN by taking into account bidirectional network. Totally, the paper is well-written, and easy to follow what is contribution of this paper. From the theoretical parts, the proposed method, BiGAN, inherits similar properties in GAN. The experimental results show that BiGAN is competitive with other methods. A drawback would a non-convex optimization problem in BiGAN, this paper is still suitable to be accepted in my opinion.","The review starts with positive statements such as ""interesting idea"", ""well-written"", and ""easy to follow"". It acknowledges the strengths of the paper, including its theoretical grounding and experimental results. While it points out a ""drawback"", it uses a softened tone (""A drawback would be..."") and ultimately recommends acceptance. Overall, the tone is balanced and constructive, leaning towards the positive.",60,70
"This paper has two main contributions: (1) Applying adversarial training to imagenet, a larger dataset than previously considered (2) Comparing different adversarial training approaches, focusing importantly on the transferability of different methods. The authors also uncover and explain the label leaking effect which is an important contribution. This paper is clear, well written and does a good job of assessing and comparing adversarial training methods and understanding their relation to one another. A wide range of empirical results are shown which helps elucidate the adversarial training procedure. This paper makes an important contribution towards understand adversarial training and believe ICLR is an appropriate venue for this work.","The review is overwhelmingly positive. The reviewer explicitly states the contributions of the paper, praises its clarity, writing quality, empirical approach, and contribution to the field. There is no negative sentiment expressed. The language used is formal and respectful, typical of scientific peer reviews.",90,100
"The authors propose to apply virtual adversarial training to semi-supervised classification. It is quite hard to assess the novelty on the algorithmic side at this stage: there is a huge available literature on semi-supervised learning (especially SVM-related literature, but some work were applied to neural networks too); unfortunately the authors do not mention it, nor relate their approach to it, and stick to the adversarial world. In terms of novelty on the adversarial side, the authors propose to add perturbations at the level of words embeddings, rather than the input itself (having in mind applications to NLP). Concerning the experimental section, authors focus on text classification methods. Again, comparison with the existing SVM-related literature is important to assess the viability of the proposed approach; for example (Wang et al, 2012) report 8.8% on IMBD with a very simple linear SVM (without transductive setup). Overall, the paper reads well and propose a semi-supervised learning algorithm which is shown to work in practice. Theoretical and experimental comparison with past work is missing.","The review acknowledges the paper's readability and practical demonstration of the algorithm (""The paper reads well and propose a semi-supervised learning algorithm which is shown to work in practice.""). However, it also points out significant shortcomings in terms of novelty and lack of comparison with existing literature, particularly SVM-related work. The language, while direct, maintains a professional and respectful tone.",20,50
This is a parallel work with BiGAN. The idea is using auto encoder to provide extra information for discriminator. This approach seems is promising from reported result.,"The reviewer states that the work is ""promising"" and provides no negative remarks. This indicates a positive sentiment. The language used is formal and devoid of any negative or overly positive language, suggesting a neutral politeness level.",60,0
"This paper proposes to use an actor-critic RL technique to train sequence to sequence tasks in natural language processing. In particular, experiments are shown in a synthetic denoising task as well as in machine translation. I like the idea of the paper, however, the experimental evaluation is not convincing. Why is the LL numbers in Ranzato et al. 2015 and your paper so different? Is the metric different? is it the scheduler? are the parameters different? If one extrapolates the numbers, it seems that MIXER will be much better than the proposed approach. I*d like to see a head-to-head comparison, either by reproducing the same setting or by running the mixer baseline. The authors should also compare their results to the state-of-the-art. How good is their machine translation system? Only comparing to a single baseline and without reproducing the numbers is not sufficient. While the idea makes sense, the authors needed to use many heuristics to make the model to work, e.g., using a delayed actor, update phi* with interpolation, penalize the variance, reducing the value of rare actions, etc. Furthermore, there is no in depth analysis of how much performance each of these heuristics brings. It seems that the authors need more work to make the model work without so many heuristics. The authors also mentioned several optimization difficulties (some of which are non-intuitive), 1) why does the critic assign very high value to actions with very low probability according to the actor? 2) why is a lower square error on Q resulting in much worst performance? The paper will benefit from a serious re-write. The technical part is not clearly written. The manuscript also assumes that the reader knows algorithms such as REINFORCE. I strongly suggest to include a brief description in the text. This will help the reader understand how to use the critic within this framework. Also the experimental section will benefit from dividing it by experiment. Right now is cumbersome to look at the details of each experiment as things are mixed up in the text. The paper criticizes the REINFORCE algorithm a lot, particularly for its high variance, however the best results in the real setting are achieved with this algorithm (+ the critic). How do you explain this? The text is also not consistent with what the results show. The discussion claims that using the critic on REINFORCE reduces the gap with the actor critic. However, it is better than the proposed approach. I*ll revise my score if the authors address my questions. In summary, an interesting idea, however many heuristics are used and the experimental evaluation is not sufficient.","The reviewer finds the idea interesting but has major concerns about the experimental evaluation and the many heuristics used. They find the results not convincing and not compared properly to state-of-the-art or existing baselines. The language is direct and critical, pointing out several flaws and demanding a better explanation and justification for the design choices. However, it maintains a professional tone and offers suggestions for improvement, indicating a willingness to revise the score based on the authors' response. Therefore, the sentiment is more negative due to the concerns raised, but the politeness remains relatively neutral.",-30,0
"This is an 18 page paper plus appendix which presents a mathematical derivation for infomax for an actual neural population with noise. The original Bell & Sejnowski infomax framework only considered the no noise case. Results are shown for natural image patches and the mnist dataset, which qualitatively resemble results obtained with other methods. This seems like an interesting and potentially more general approach to unsupervised learning. However the paper is quite long and it was difficult for me to follow all the twists and turns. For example the introduction of the hierarchical model was confusing and it took several iterations to understand where this was going. *Hierarchical* is probably not the right terminology here because it*s not like a deep net hierarchy, it*s just decomposing the tuning curve function into different parts. I would recommend that the authors try to condense the paper so that the central message and important steps are conveyed in short order, and then put the more complete mathematical development into a supplementary document. Also, the authors should look at the work of Karklin & Simoncelli 2011 which is highly related. They also use an infomax framework for a noisy neural population to derive on and off cells in the retina, and they show the conditions under which orientation selectivity emerges.","The reviewer finds the paper interesting and potentially general, suggesting it's a valuable contribution. However, they also point out that the paper is lengthy and difficult to follow at times. The reviewer suggests condensing the paper and points to related work, indicating a need for improvement but not a fundamental flaw. Overall, this suggests a positive sentiment, but not overly enthusiastic due to the need for revisions. The language used is constructive and professional, suggesting politeness in the feedback.",60,70
"In this paper a well known soft mixture of experts model is adapted for, and applied to, a specific type of transfer learning problem in reinforcement learning (RL), namely transfer of action policies and value functions between similar tasks. Although not treated as such, the experimental setup is reminiscent of hierarchical RL works, an aspect which the paper does not consider at length, regrettably. One possible implication of this work is that architecture and even learning algorithm choices could simply be stated in terms of the objective of the target task, rather than being hand-engineered by the experimenter. This is clearly an interesting direction of future work which the paper illuminates. Pros: The paper diligently explains how the network architecture fits in with various widely used reinforcement learning setups, which does facilitate continuation of this work. The experiments are good proofs of concept, but do not go beyond that i.m.h.o. Even so, this work provides convincing clues that collections of deep networks, which were trained on not entirely different tasks, generalize better to related tasks when used together rather than through conventional transfer learning (e.g. fine-tuning). Cons: As the paper well recounts in the related work section, libraries of fixed policies have long been formally proposed for reuse while learning similar tasks. Indeed, it is well understood in hierarchical RL literature that it can be beneficial to reuse libraries of fixed (Fernandez & Veloso 2006) or jointly learned policies which may not apply to the entire state space, e.g. options (Pricop et. al). What is not well understood is how to build such libraries, and this paper does not convincingly shed light in that direction, as far as I can tell. The transfer tasks have been picked to effectively illustrate the potential of the proposed architecture, but the paper does not tackle negative transfer or compositional reuse in well known challenging situations outlined in previous work (e.g. Parisotto et. al 2015, Rusu el. al 2015, 2016). Since the main contributions are of an empirical nature, I am curious how the results shown in figures 6 & 7 look plotted against wall-clock time, since relatively low data efficiency is not a limitation for achieving perfect play in Pong (see Mnih. et al, 2015). It would be more illuminating to consider tasks where final performance is plausibly limited by data availability. It would also be interesting if the presented results were achieved with reduced amounts of computation, or reduced representation sizes compared to learning from scratch, especially when one of the useful source tasks is an actual policy trained on the target task. Finally, it is perhaps underwhelming that it takes a quarter of the data required for learning Pong from scratch just to figure out that a perfect Pong policy is already in the expert library. Simply evaluating each expert for 10 episodes and using an average-score-weighted majority vote to mix action choices would probably achieve the same final performance for a smaller fraction of the data.","The review is cautiously optimistic about the paper's potential, but raises several concerns about its novelty, depth, and choice of experiments. While acknowledging the paper's contributions (clear explanation of architecture, proof-of-concept experiments), the reviewer finds the work somewhat incremental and lacking in rigorous evaluation on more challenging tasks. The language is formal and critical, but not disrespectful.",20,60
"The authors propose NVI for LDA variants. The authors compare NVI-LDA to standard inference schemes such as CGS and online SVI. The authors also evaluate NVI on a different model ProdLDA (not sure this model has been proposed before in the topic modeling literature though?) In general, I like the direction of this paper and NVI looks promising for LDA. The experimental results however confound model vs inference which makes it hard to understand the significance of the results. Furthermore, the authors don*t discuss hyper-parameter selection which is known to significantly impact performance of topic models. This makes it hard to understand when the proposed method can be expected to work. Can you maybe generate synthetic datasets with different Dirichlet distributions and assess when the proposed method recovers the true parameters? Figure 1: Is this prior or posterior? The text talks about sparsity whereas the y-axis reads *log p(topic proportions)* which is a bit confusing. Section 3.2: it is not clear what you mean by unimodal in softmax basis. Consider a Dirichlet on K-dimensional simplex with concentration parameter alpha/K where alpha<1 makes it multimodal. Isn*t the softmax basis still multimodal? None of the numbers include error bars. Are the results statistically significant? Minor comments: Last term in equation (3) is not *error*; reconstruction accuracy or negative reconstruction error perhaps? The idea of using an inference network is much older, cf. Helmholtz machine.","The reviewer starts with positive remarks, stating they ""like the direction"" and find NVI ""promising."" However, they raise several methodological concerns, questioning the clarity and significance of the results. The numerous suggestions for improvement, while constructive, indicate a lack of complete satisfaction with the paper's current state. Therefore, the sentiment leans slightly positive but is tempered by the critical feedback. The language remains polite throughout, employing a professional and undemanding tone even when pointing out flaws.",60,70
"EDIT: the revisions made to this paper are very thorough and address many of my concerns, and the paper is also easier to understand. i recommend the latest version of this paper for acceptance and have increased my score. This paper presents a way of interpreting LSTM models, which are notable for their opaqueness. In particular, the authors propose decomposing the LSTM*s predictions for a QA task into importance scores for words, which are then used to generate patterns that are used to find answers with a simple matching algorithm. On the WikiMovies dataset, the extracted pattern matching method achieves accuracies competitive with a normal LSTM, which shows the power of the proposed approach. I really like the motivation of the paper, as interpreting LSTMs is definitely still a work-in-progress, and the high performance of the pattern matching was surprising. However, several details of the pattern extraction process are not very clear, and the evaluation is conducted on a very specific task, where predictions are made at every word. As such, I recommend the paper in its current form as a weak accept but hope that the authors clarify their approach, as I believe the proposed method is potentially useful for NLP researchers. Comments: - Please introduce in more detail the specific QA tasks you are applying your models on before section 3.3, as it*s not clear at that point that the answer is an entity within the document. - 3.3: is the softmax predicting a 0/1 value (e.g., is this word the answer or not?) - 3.3: what are the P and Q vectors? do you just mean that you are transforming the hidden state into a 2-dimensional vector for binary prediction? - how does performance of the pattern matching change with different cutoff constant values? - 5.2: are there questions whose answers are not entities? - how could the proposed approach be used when predictions aren*t made at every word? is there any extension for, say, sentence-level sentiment classification?","The reviewer provides constructive criticism and acknowledges the paper's strengths (novelty, interesting results). While they have some concerns, they recommend it for acceptance after revisions, indicating an overall positive sentiment. The language is formal and polite, typical of academic peer reviews.",60,80
"This is a solid paper with good results. However, there aren*t many very interesting takeaways (most of the architecture seems a concatenation of standard elements to do well in the leaderboard) and some issues in the writing. The second paragraph of the introduction is very confusing. It*s clear the authors got really deep into their world and made no attempts to actually clearly explain their model, not even to an expert in the field, let a lone somebody who isn*t familiar with similar approaches. The authors keep referring to *previously popular attention paradigms* without any citation and then, I believe, incorrectly describe whatever those are supposed to be by writing that these unknown but popular approaches *summarize each modality into a single vector.* That*s one of the most incorrect descriptions I*ve yet seen for attention mechanisms. First, I don*t know what model works over several modalities in a single attention pass. Maybe the authors don*t know what a *modality* is? More importantly, the whole point of most attention mechanisms is that one does not simply summarize the whole input but instead can access all elements of it. So, this paper*s supposedly new way of using attention is pretty much exactly the standard way. Both modeling and modelling spellings are in the text. I understand the need to sometimes invent new terminology to describe a model but in this paragraph, the authors 3 times talk about a *modeling layer (RNN)*... It*s just an RNN, you don*t need to give an RNN another name, especially one that*s as nondescript as *modeling layer* all layers are part of a model? Typo: *let*s the modeling (RNN) layer to learn* This paragraph is supposed to give an overview of the model but just confuses readers. I would delete it. *Phrase embedding layer* -- terrible word choice as you are not embedding phrases here. It*s a standard bidirectional LSTM over words, not phrases. In all subsequent parts of the paper you just give examples of words embedded in context. No phrases. Please change this to *contextual word embedding layer* or something less incorrect. Your phrase layer embeddings only show single words, as expected in Table 2. Section 2: point 4. Second sentence needs citations for *popular* Typo: *from both *of* the context and query word* Typo: *aveaged* It seems like your output layer changes quite substantially so your claim in the abstract/intro of using the same model isn*t quite accurate. I*d say you*re changing one module or part of your model. Section 4: attention isn*t countable (no *a* in front of *huge attention*). Also, academic writing usually doesn*t include such adjectives in the first place.","The review starts with a positive statement, acknowledging the paper as ""solid"" with ""good results."" However, it quickly transitions into criticism, pointing out a lack of novelty and significant issues with the writing and clarity. The reviewer uses strong language like ""most incorrect descriptions I've yet seen"" and questions the authors' understanding of basic concepts. While the reviewer provides constructive feedback in the form of specific recommendations, the tone is predominantly negative due to the perceived flaws in the paper's presentation and originality.",-30,20
"This paper addresses one of the major shortcomings of generative adversarial networks - their lack of mechanism for evaluating held-out data. While other work such as BiGANs/ALI address this by learning a separate inference network, here the authors propose to change the GAN objective function such that the optimal discriminator is also an energy function, rather than becoming uninformative at the optimal solution. Training this new objective requires gradients of the entropy of the generated data, which are difficult to approximate, and the authors propose two methods to do so, one based on nearest neighbors and one based on a variational lower bound. The results presented show that on toy data the learned discriminator/energy function closely approximates the log probability of the data, and on more complex data the discriminator give a good measure of quality for held out data. I would say the largest shortcomings of the paper are practical issues around the scalability of the nearest neighbors approximation and accuracy of the variational approximation, which the authors acknowledge. Also, since entropy estimation and density estimation are such closely linked problems, I wonder if any practical method for EGANs will end up being equivalent to some form of approximate density estimation, exactly the problem GANs were designed to circumvent. Nonetheless, the elegant mathematical exposition alone makes the paper a worthwhile contribution to the literature. Also, some quibbles about the writing - it seems that something is missing in the sentence at the top of pg. 5 *Finally, let*s whose discriminative power*. I*m not sure what the authors mean to say here. And the title undersells the paper - it makes it sound like they are making a small improvement to training an existing model rather than deriving an alternative training framework.","The reviewer acknowledges the paper's address of a significant shortcoming in GANs and praises its elegant mathematical exposition, contributing worthwhilely to the literature. While pointing out practical limitations, the reviewer's overall tone remains positive. The language used is analytical and suggestive, not crossing into rudeness.",60,70
"This paper performs a very important service: exploring in a clear and systematic way the performance and trainability characteristics of a set of neural network architectures -- in particular, the basic RNN motifs that have recently been popular. Pros: * This paper addresses an important question I and many others would have liked to know the answer to but didn*t have the computational resources to thoroughly attack it. This is a nice use of Google*s resources to help the community. * The work appears to have been done carefully so that the results can be believed. * The basic answer arrived at (that, in the *typical training environment* LSTMs are reliable but basically GRUs are the answer) seems fairly decisive and practically useful. Of course the real answer is more complicated than my little summary here, but the subtleties are discussed nicely in the paper. * The insistence on a strong distinction between capacity and trainability helps nicely clear up a misconception about the reasons why gated architectures work. In sum, they*re much more easily trainable but somewhat lower capacity than vanilla RNNs, and in hard tasks, the benefits of better trainability far outweigh the costs of mildly lower capacity. * The point about the near-equivalence of capacity at equal numbers of parameters is very useful. * The paper makes it clear the importance of HP tuning, something that has sometimes gotten lost in the vast flow of papers about new architectures. * The idea of quantifying the fraction of infeasible parameters (e.g. those that diverge) is nice, because it*s a practical problem that everyone working with these networks has but often isn*t addressed. * The paper text is very clearly written. Cons: * The work on the UGRNNs and the +RNNs seems a bit preliminary. I don*t think that the authors have clearly shown that the +RNN should be *recommended* with the same generality as the GRU. I*d at the least want some better statistics on the significance of differences between +RNN and GRU performances quantifying the results in Figure 4 (8-layer panel). In a way the high standards for declaring an architecture useful that are set in the paper make the UGRNNs and +RNN contributions seem less important. I don*t really mind having them in the paper though. I guess the point of this paper is not really to be novel in the first place -- which is totally fine with me, though I don*t know what the ICLR area chairs will think. * The paper gives short shrift to the details of the HP algorithm itself. They do say: *Our setting of the tuner’s internal parameters was such that it uses Batched GP Bandits with an expected improvement acquisition function and a Matern 5/2 Kernel with feature scaling and automatic relevance determination performed by optimizing over kernel HPs* and give some good references, but I expect that actually trying to replicate this involves a lot of missing details. * I found some of the figures a bit hard to read at first, esp. Fig 4, mostly due to the panels being small, having a lot of details, and bad choices for visual cleanliness. * The neuroscience reference (*4.7 bits per synapse*) seems a little bit of a throw-away to me, because the connection between these results and the experimental neuroscience is very tenuous, or at any rate, not well explained. I guess it*s just in the discussion, but it seems gratuitous. Maybe it should couched in slightly less strong terms (nothing is really strongly shown to be *in agreement* here between computational architectures and neuroscience, but perhaps they could say something like -- *We wonder if it is anything other than coincidence that our 5 bits result is numerically similar to the 4.7 bits measurement from neuroscience.*)","The review is overwhelmingly positive. The reviewer highlights the importance, carefulness, and usefulness of the work. While they have minor suggestions and critiques, these are presented constructively and do not detract from the overall positive sentiment. The language is polite and professional throughout, with no instances of rudeness or disrespect.",85,90
"This paper introduces a continuous relaxation of categorical distribution, namely the the Gumbel-Softmax distribution, such that generative models with categorical random variables can be trained using reparameterization (path-derivative) gradients. The method is shown to improve upon other methods in terms of the achieved log-likelihoods of the resulting models. The main contribution, namely the method itself, is simple yet nontrivial and worth publishing, and seems effective in experiments. The paper is well-written, and I applaud the details provided in the appendix. The main application seems to be semi-supervised situations where you really want categorical variables. - P1: *differentiable sampling mechanism for softmax*. *sampling* => *approximate sampling*, since it*s technically sampling from the Gumbal-softmax. - P3: *backpropagtion* - Section 4.1: Interesting experiments. - It would be interesting to report whether there is any discrepancy between the relaxed and non-relaxed models in terms of log-likelihood. Currently, only the likelihoods under the non-relaxed models are reported. - It is slightly discouraging that the temperature (a nuisance parameter) is used differently across experiments. It would be nice to give more details on whether you were succesful in learning the temperature, instead of annealing it; it would be interesting if that hyper-parameter could be eliminated.","The review is positive overall. The reviewer finds the main contribution valuable and the paper well-written. While they have some suggestions for improvement, these are framed constructively and aim to enhance the paper's strength. The reviewer explicitly states the work is ""worth publishing."" ",75,100
"The work introduces a new regularization for learning domain-invariant representations with neural networks. The regularization aims at matching the higher order central moments of the hidden activations of the NNs of the source and target domain. The authors compared the proposed method vs MMD and two state-of-art NN domain adaptation algorithms on the Amazon review and office datasets, and showed comparable performance. The idea proposed is simple and straightforward, and the empirical results suggest that it is quite effective. The biggest limitation I can see with the proposed method is the assumption that the hidden activations are independently distributed. For example, this assumption will clearly be violated for the hidden activations of convolutional layers, where neighboring activations are dependent. I guess this is why the authors start with the output of dense layers for the image dataset. Do the authors have insight on if it is beneficial to start adaptation from lower level? If so, do the authors have insight on how to relax the assumption? In these scenarios, if MMD has an advantage as it does not make this assumption? Figure 3 does not seems to clearly support the boost of performance shown in table 2. The only class where the new regularization brings the source and target domain closer seem to be the mouse class pointed by the authors. Is the performance improvement only coming from this single class?","The reviewer provides generally positive feedback, acknowledging the simplicity and effectiveness of the proposed method. While they point out a limitation regarding the assumption of independent distributions, the tone is inquisitive rather than critical. The reviewer seeks clarification and further insights, suggesting potential areas of improvement. This suggests an overall positive sentiment leaning towards neutral due to the constructive criticism. The language used is polite and professional throughout, employing phrases like ""I guess"" and ""Do the authors have insight"" to express their queries respectfully.",60,80
"This is a very nicely written paper which unifies some value-based and policy-based (regularized policy gradient) methods, by pointing out connections between the value function and policy which have not been established before. The theoretical results are new and insightful, and will likely be useful in the RL field much beyond the specific algorithm being proposed in the paper. This being said, the paper does exploit the theory to produce a unified version of Q-learning and policy gradient, which proves to work on par or better than the state-of-art algorithms on the Atari suite. The empirical section is very well explained in terms of what optimization were done. One minor comment I had was related to the stationary distribution used for a policy - there are subtleties here between using a discounted vs non-discounted distribution which are not crucial in the tabular case, but will need to be addressed in the long run in the function approximation case. This being said, there is no major problem for the current version of the paper. Overall, the paper is definitely worthy of acceptance, and will likely influence a broad swath of RL, as it opens the door to further theoretical results as well as algorithm development.","The review starts with very positive statements about the paper's contribution, novelty, and potential impact. It praises both the theoretical and empirical aspects. While it raises a minor point, it clearly states it's not a major issue. The concluding sentence strongly advocates for acceptance and predicts a broad influence. All these points indicate a highly positive sentiment and polite, constructive language.",90,90
"Training highly non-convex deep neural networks is a very important practical problem, and this paper provides a great exploration of an interesting new idea for more effective training. The empirical evaluation both in the paper itself and in the authors’ comments during discussion convincingly demonstrates that the method achieves consistent improvements in accuracy across multiple architectures, tasks and datasets. The algorithm is very simple (alternating between training the full dense network and a sparse version of it), which is actually a positive since that means it may get adapted in practice by the research community. The paper should be revised to incorporate the additional experiments and comments from the discussion, particularly the accuracy comparisons with the same number of epochs.","The review is overwhelmingly positive. The reviewer highlights the significance of the paper's topic, praises the effectiveness of the proposed method, and appreciates the simplicity of the algorithm. The suggestions for revision are minor, focusing on incorporating additional experiments. There is no negative language used, and the tone is constructive and encouraging.",90,100
"This paper proposes a data noising technique for language modeling. The main idea is to noise a word history by using a probabilistic distribution based on N-gram smoothing techniques. The paper is clearly written and shows that such simple techniques improve the performance in various tasks including language modeling and machine translation. May main concern is that the method is too simple and sounds ad hoc, e.g., there is no theoretical justification of why n-gram smoothing based data noising would be effective for recurrent neural network based language modeling. Comments: - p. 3 “can be seen has a way” -> “can be seen as a way” (?) - p. 3. In general, the explanation about blank noising should be improved. Why does it avoid overfitting on specific contexts? - p. 4. It would be better to provide more detailed derivations for a general form of unigram and blank noising equations. - p. 5, Section 3.6: Is there any discussions about noising either/both input and output sequences with some numbers? This would be helpful information.","The reviewer finds the paper well-written and acknowledges the effectiveness of the proposed technique. However, they express concerns about the simplicity and lack of theoretical justification. The tone is generally neutral, with constructive criticism and suggestions for improvement.",50,50
"This paper introduces an approach for future frame prediction in videos by decoupling motion and content to be encoded separately, and additionally using multi-scale residual connections. Qualitative and quantitative results are shown on KTH, Weizmann, and UCF-101 datasets. The idea of decoupling motion and content is interesting, and seems to work well for this task. However, the novelty is relatively incremental given previous cited work on multi-stream networks, and it is not clear that this particular decoupling works well or is of broader interest beyond the specific task of future frame prediction. While results on KTH and Weizmann are convincing and significantly outperform baselines, the results are less impressive on less constrained UCF-101 dataset. The qualitative examples for UCF-101 are not convincing, as discussed in the pre-review question. Overall this is a well-executed work with an interesting though not extremely novel idea. Given the limited novelty of decoupling motion and content and impact beyond the specific application, the paper would be strengthened if this could be shown to be of broader interest e.g. for other video tasks.","The review starts with positive remarks about the paper's approach and findings ('interesting', 'seems to work well'). However, it also highlights limitations regarding novelty and broader impact. While acknowledging the paper as 'well-executed', the reviewer suggests further work to strengthen the claims. The language used is professional and constructive, without resorting to harsh criticism.",50,70
"The paper proposes a new function for computing arc score between two words in a sentence for dependency parsing. The proposed function is biaffine in the sense that it*s a combination of a bilinear score function and a bias term playing a role as prior. The paper reports new state-of-the-art dependency parsing performances on both English PTB and Chinese TB. The paper is very well written with impressive experimental results and analysis. However, the idea is hardly novel regarding to the theme of the conference: the framework that the paper uses is from Kiperwasser & Goldberg (2016), the use of bilinear score function for attention is from Luong et al (2015). Projecting BiLSTM outputs into different spaces using MLPs is a trivial step to make the model *deeper*, whereas adding linear bias terms isn*t confirmed to work in the experiments (table 2 shows that diag bilinear has a close performance to biaffine). I think that this paper is more proper for NLP conferences.","The review starts with positive statements, praising the writing, results, and analysis ('very well written', 'impressive experimental results'). However, it quickly transitions into a negative sentiment, criticizing the novelty and significance of the work ('hardly novel', 'trivial step', 'isn't confirmed to work'). The reviewer suggests the paper is a better fit for a different conference, indicating a lack of enthusiasm. Overall, the negative points outweigh the initial positives. The language used, while direct, avoids harsh or disrespectful language.",-30,40
"The paper expands a recent mean-field approximation of deep random neural networks to study depth-dependent information propagation, its phase-dependence and the influence of drop-out. The paper is extremely well written, the mathematical analysis is thorough and numerical experiments are included that underscore the theoretical results. Overall the paper stands out as one of the few papers that thoroughly analyses training and performance of deep nets.","The review is overwhelmingly positive. This is evidenced by phrases like:  ""extremely well-written"", ""thorough mathematical analysis"", ""numerical experiments [...] underscore the theoretical results"", and ""stands out as one of the few papers that thoroughly analyses"". The language used is formal and respectful.",95,100
"Authors describe implementation of TensorFlow Fold which allows one to run various computations without modifying computation graph. They achieve this by creating a generic scheduler as a TensorFlow computation graph, which can accept graph description as input and execute it. They show clear benefits to this approach for tasks where computation changes for each datapoint, such as the case with TreeRNN. In the experiments, they compare against having static batch (same graph structure repeated many times) and batch size 1. The reason my score is 7 and not higher is because they do not provide comparison to the main alternative of their method -- someone could create a new TensorFlow graph for each dynamic batch. In other words, instead of using their graph as the scheduling algorithm, one could explicitly create each non-uniform batch as a TensorFlow graph, and run that using standard TensorFlow.","The review is mostly positive, highlighting the clear benefits and clever implementation of the approach. However, it also points out a significant missing comparison that prevents a higher score. The language used is neutral and professional, lacking strong positive or negative connotations.",60,50
"The paper proposed a nice framework leveraging Tucker and Tensor train low-rank tensor factorization to induce parameter sharing for multi-task learning. The framework is nice and appealing. However, MTL is a very well studied problem and the paper considers simple task for different classification, and it is not clear if we really need ``Deep Learning* for these simple datasets. A comparison with existing shallow MTL is necessary to show the benefits of the proposed methods (and in particular being deep) on the dataset. The authors ignore them on the basis of speculation and it is not clear if the proposed framework is really superior to simple regularizations like the nuclear norm. The idea of nuclear norm regularization can also be extended to deep learning as gradient descent are popular in all methods.","The review starts with positive words like ""nice"" and ""appealing"" but then raises serious concerns about the novelty and significance of the work. The reviewer doubts the need for deep learning and suggests simpler methods might be as effective. The phrase ""The authors ignore them on the basis of speculation"" is a strong criticism, indicating flaws in the paper's argumentation. Overall, the sentiment leans negative due to the significant concerns raised. The language used, while direct, maintains a professional tone without resorting to personal attacks or overly harsh language.",-20,60
"An interesting architecture that accumulates and continuously corrects mistakes as you see more and more of a video sequence. Clarity: The video you generated seems very helpful towards understanding the information flow in your network, it would be nice to link to it from the paper. *Our model with hyperparameters optimized for KITTI underperforms the model of Finn et al. (2016), but outperforms the previous state-of-the-art model by Mathieu et al. (2016).* It is not clear how different are the train and test sequences at the moment, since standard benchmarks do not really exist for video prediction and each author picks his/her favorite. Underperforming Finn et al 206 at the H3.6m Walking videos is a bit disappointing.","The review starts with a positive statement, highlighting an ""interesting architecture."" However, it raises concerns about performance, particularly compared to Finn et al. 2016. While not entirely dismissive, the tone leans towards the critical side due to the underperformance issue. The language remains professional and polite throughout, suggesting areas for improvement rather than outright flaws.",20,70
"The authors propose a new software package for probabilistic programming, taking advantage of recent successful tools used in the deep learning community. The software looks very promising and has the potential to transform the way we work in the probabilistic modelling community, allowing us to perform rapid-prototyping to iterate through ideas quickly. The composability principles are used insightfully, and the extension of inference to HMC for example, going beyond VI inference (which is simple to implement using existing deep learning tools), makes the software even more compelling. However, the most important factor of any PPL is whether it is practical for real-world use cases. This was not demonstrated sufficiently in the submission. There are many example code snippets given in the paper, but most are not evaluated. The Dirichlet process mixture model example (Figure 12) is an important one: do the proposed black-box inference tools really work for this snippet? and will the GAN example (Figure 7) converge when optimised with real data? To convince the community of the practicality of the package it will be necessary to demonstrate these empirically. Currently the only evaluated model is a VAE with various inference techniques, which are not difficult to implement using pure TF. Presentation: * Paper presentation could be improved. For example the authors could use more signalling for what is about to be explained. On page 5 qbeta and qz are used without explanation - the authors could mention that an example will be given thereafter. * I would also suggest to the authors to explain in the preface how the layers are implemented, and how the KL is handled in VI for example. It will be useful to discuss what values are optimised and what values change as inference is performed (even before section 4.4). This was not clear for the majority of the paper. Experiments: * Why is the run time not reported in table 1? * What are the *difficulties around convergence* encountered with the analytical entropies? inference issues become more difficult to diagnose as inference is automated. Are there tools to diagnose these with the provided toolbox? * Did HMC give sensible results in the experiment at the bottom of page 8? only run time is reported. * How difficult is it to get the inference to work (eg HMC) when we don*t have full control over the computational graph structure and sampler? * It would be extremely insightful to give a table comparing the performance (run time, predictive log likelihood, etc) of the various inference tools on more models. * What benchmarks do you intend to use in the Model Zoo? the difficulty with probabilistic modelling is that there are no set benchmarks over which we can evaluate and compare many models. Model zoo is sensible for the Caffe ecosystem because there exist few benchmarks a large portion of the community was working on (ImageNet for example). What datasets would you use to compare the DPMM on for example? Minor comments: * Table 1: I would suggest to compare to Li & Turner with alpha=0.5 (the equivalent of Hellinger distance) as they concluded this value performs best. I*m not sure why alpha=-1 was chosen here. * How do you handle discrete distributions (eg Figure 5)? * x_real is not defined in Figure 7. * I would suggest highlighting M in Figure 8. * Comma instead of period after *rized), In* on page 8. In conclusion I would say that the software developments presented here are quite exciting, and I*m glad the authors are pushing towards practical and accessible *inference for all*. In its current form though I am forced to give the submission itself a score of 5.","The review starts with very positive remarks, highlighting the potential of the software and praising the composability principles and inference extension. However, the enthusiasm is tempered by concerns about the practicality of the software for real-world use cases, which is a crucial aspect. The reviewer finds the examples provided insufficient to demonstrate this practicality and calls for empirical evidence. The numerous constructive suggestions for improvement, particularly regarding the evaluation of different models and inference tools, further indicate that the review is not entirely positive.  The language is polite and professional throughout, with constructive criticism and suggestions for improvement. While the reviewer points out weaknesses, they do so respectfully and with the aim of guiding the authors towards a stronger submission.",50,75
"This is mainly a (well-written) toy application paper. It explains SGVB can be applied to state-space models. The main idea is to cast a state-space model as a deterministic temporal transformation, with innovation variables acting as latent variables. The prior over the innovation variables is not a function of time. Approximate inference is performed over these innovation variables, rather the states. This is a solution to a fairly specific problem (e.g. it doesn*t discuss how priors over the beta*s can depend on the past), but an interesting application nonetheless. The ideas could have been explained more compactly and more clearly; the paper dives into specifics fairly quickly, which seems a missed opportunity. My compliments for the amount of detail put in the paper and appendix. The experiments are on toy examples, but show promise. - Section 2.1: “In our notation, one would typically set beta_t = w_t, though other variants are possible” -> It’s probably better to clarify that if F_t and B_t and not in beta_t, they are not given a Bayesian treatment (but e.g. merely optimized). - Section 2.2 last paragraph: “A key contribution is […] forcing the latent space to fit the transition”. This seems rather trivial to achieve. - Eq 9: “This interpretation implies the factorization of the recognition model:..” The factorization is not implied anywhere: i.e. you could in principle use q(beta|x) = q(w|x,v)q(v)","The review starts with a somewhat backhanded compliment, calling it a ""well-written toy application paper."" This suggests the reviewer sees merit in the work but doesn't consider it a significant contribution. The reviewer acknowledges the detailed explanations and promising experiments but also points out missed opportunities for clarity and broader applicability. The tone is critical, questioning some choices and phrasing, but remains professional and offers concrete suggestions for improvement. Overall, the sentiment leans slightly positive due to the promising results and detailed work, but the criticism tempers the enthusiasm.",20,60
"Update: raised the score, because I think the arguments about adversarial examples are compelling. I think that the paper convincingly proves that this method acts as a decent regularizer, but I*m not convinced that it*s a competitive regularizer. For example, I don*t believe that there is sufficient evidence that it gives a better regularizer than dropout/normalization/etc. I also think that it will be much harder to tune than these other methods (discussed in my rebuttal reply). ---- Summary: If I understand correctly, this paper proposes to take the *bottleneck* term from variational autoencoders which pulls the latent variable towards a noise prior (like N(0,1)) and apply it in a supervised learning context where the reconstruction term log(p(x|z)) is replaced with the usual supervised cross-entropy objective. The argument is that this is an effective regularizer and increases robustness to adversarial attacks. Pros: -The presentation is quite good and the paper is easy to follow. -The idea is reasonable and the relationship to previous work is well described. -The robustness to adversarial examples experiment seems convincing, though I*m not an expert in this area. Is there any way to compare to an external quantitative baseline on robustness to adversarial examples? This would help a lot, since I*m not sure how the method here compares with other regularizers in terms of combatting adversarial examples. For example, if one uses a very high dropout rate, does this confer a comparable robustness to adversarial examples (perhaps at the expense of accuracy)? Cons: -MNIST accuracy results don*t seem very strong, unless I*m missing something. The Maxout paper from ICML 2013 listed many permutation invariant MNIST results with error rates below 1%. So the 1.13% error rate listed here doesn*t necessarily prove that the method is a competitive regularizer. I also suspect that tuning this method to make it work well is harder than other regularizers like dropout. -There are many distinct architectural choices with this method, particularly in how many hidden layers come before and after z. For example, the output could directly follow z, or there could be several layers between z and the output. As far as I can tell the paper says that p(y | z) is a simple logistic regression (i.e. one weight matrix followed by softmax), but it*s not obvious why this choice was made. Did it work best empirically? Other: -I wonder what would happen if you *trained against* the discovered adversarial examples while also using the method from this paper. Would it learn to have a higher variance p(z | x) when presented with an adversarial example?","The reviewer acknowledges the paper's merits, such as good presentation, a reasonable idea, and convincing experiments on adversarial robustness. However, they also raise concerns about the competitiveness of the proposed method as a regularizer, questioning its performance compared to established techniques like dropout. The reviewer's tone remains professional and polite throughout, expressing their points clearly and providing constructive criticism.",60,80
"This is a good paper, well written, that presents a simple but effective approach to predict code properties from input output pairs. The experiments show superiority to the baseline, with speedup factors between one to two orders of magnitude. This is a solid gain! The domain of programs is limited, so there is more work to do in trying such ideas on more difficult tasks. Using neural nets to augment the search is a good starting point and a right approach, instead of generating full complex code. I see this paper as being above the threshold for acceptance.","The review starts with positive statements like ""This is a good paper, well written"" and ""This is a solid gain!""  The reviewer clearly states the paper is ""above the threshold for acceptance."" indicating a positive sentiment. The language used is constructive and encouraging, suggesting a polite tone.",85,90
"This paper presents and evaluates a Scala-based deep learning framework called “DeepDSL,” describing the language’s syntactic and performance benefits with respect to existing frameworks. Pros: The use of Scala is unique among deep learning frameworks, to my knowledge, making this framework interesting for Scala users. The fact that Scala compiles to Java and therefore cross-platform support comes for free is also nice. The ability to inspect memory information as shown in Figure 3 is interesting and potentially useful for large networks or situations where memory is limited. DeepDSL compares favorably with existing frameworks in terms of memory use and speed for many common convolutional network architectures. Cons: There appears to be special privileged handling of parameters, gradients, and updates in the compilation process itself (as in Caffe), rather than having gradients/updates as a normal part of the full user-defined computation graph (as in Theano + TensorFlow). This makes certain applications, such as RNNs (which require parameter sharing) and GANs (which require gradients wrt multiple objectives), impossible to implement in DeepDSL without further extension of the underlying API. (Note: I might be wrong about this -- and please correct me if I am -- but all the examples in the paper are nets trained by gradient descent on a single objective, and do not share parameters or access gradients directly.) The paper repeatedly refers to line counts from the verbose Protobuf-based low-level representation of networks in Caffe to demonstrate the compactness of its own syntax. This is misleading as Caffe has officially supported a compact network definition style called “NetSpec” for years -- see a ~15 line definition of AlexNet [1]. Given that, Protobuf is essentially an intermediate representation for Caffe (as with TensorFlow), which happens to have a human-readable text format. DeepDSL is not especially novel when compared with existing frameworks, which is not a problem in and of itself, but some statements misleadingly or incorrectly oversell the novelty of the framework. Some examples: “This separation between network definition and training is an unique advantage of DeepDSL comparing to other tools.” This separation is not unique -- it’s certainly a feature of Caffe where the network definition is its own file, and can be attained in TensorFlow as well (though it’s not the default workflow there). “The difference [between our framework and Theano, TensorFlow, etc.] is that we do not model deep networks as ‘networks’ but as abstract ‘functions’.” There is no notion of a “network” in Theano or TensorFlow (not sure about the others) either -- there are only functions, like in DeepDSL. I asked about this statement, and the response didn’t convince me otherwise. The counterexample given was that in TensorFlow the number of input channels needs to be specified separately for each convolution. This is only true using the low-level API and can easily be worked around with higher-level wrappers like TensorFlow Slim -- e.g., see the definition of AlexNet [2]. It may be true that DeepDSL is more “batteries included” for writing compact network definitions than these other frameworks, but the paper’s claims seem to go beyond this. Overall, the DeepDSL framework seems to have real value in its use of Scala and its memory/speed efficiency as demonstrated by the experiments, but the current version of the paper contains statements that overclaim novelty in ways that are misleading and unfair to existing frameworks. I will consider upgrading my rating if these statements are removed or amended to be more technically precise. [1] https://github.com/BVLC/caffe/blob/master/examples/pycaffe/caffenet.py#L24 [2] https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/nets/alexnet.py#L92 ===================== Update: the authors have revised their paper to address the concerns that I considered grounds for rejection in my review. I*ve upgraded my rating from 5 (below threshold) to 7 (good paper, accept).","The review starts with positive remarks, highlighting the pros of the DeepDSL framework. However, it then delves into several significant concerns, pointing out misleading comparisons and potential limitations. While acknowledging the framework's merits, the reviewer emphasizes the need for revisions to address the overstated claims. The final paragraph reveals an upgrade in rating after the authors addressed the concerns, indicating an overall positive shift in sentiment.",50,70
"The paper presents an interesting and very detailed study of targeted and non-targeted adversarial examples in CNNs. I’m on the fence about this paper but am leaning towards acceptance. Such detailed empirical explorations are difficult and time-consuming to construct yet can serve as important stepping stones for future work. I see the length of the paper as a strength since it allows for a very in-depth look into the effectiveness and transferability of different kinds of adversarial examples. There are, however, some concerns: 1) While the length of the paper is a strength in my mind, the key contributions should be made much more clear. As evidenced by my comment earlier, I got confused at some point between the ensemble/non-ensemble method, and about the contribution of the Clarifai evaluation and what I should be focusing on where. I’d strongly suggest a radical revision which more clearly focuses the story: - First, we demonstrate that non-targeted attacks are easy while targeted attacks are hard (evidenced by a key experiment comparing the two; we refer to appendix or later sections for the extensive exploration of e.g., current Section 3) - Thus, we propose an ensemble method that is able to handle targeted attacks much better (evidenced by experiments focusing on the comparison between ensemble and non-ensemble method, both in a controlled setting and on Clarifai) - Also, here are all the other details and explorations. 2) Instead of using ResNet-152, Res-Net-101 and ResNet-50 as three of the five models, it would*ve been better to use one ResNet architecture and the other two, say, AlexNet and Network-in-Network. This would make the ensemble results a lot more compelling.","The reviewer explicitly states they are ""leaning towards acceptance"" and acknowledges the value of the work, even calling the paper's length a ""strength."" They offer constructive criticism and suggestions for improvement, rather than harsh negativity. The language is formal and professional throughout.",60,80
"This paper presents a clever way of training a generative model which allows for exact inference, sampling and log likelihood evaluation. The main idea here is to make the Jacobian that comes when using the change of variables formula (from data to latents) triangular - this makes the determinant easy to calculate and hence learning possible. The paper nicely presents this core idea and a way to achieve this - by choosing special *routings* between the latents and data such that part of the transformation is identity and part some complex function of the input (a deep net, for example) the resulting Jacobian has a tractable structure. This routing can be cascaded to achieve even more complex transformation. On the experimental side, the model is trained on several datasets and the results are quite convincing, both in sample quality and quantitive measures. I would be very happy to see if this model is useful with other types of tasks and if the resulting latent representation help with classification or inference such as image restoration. In summary - the paper is nicely written, results are quite good and the model is interesting - I*m happy to recommend acceptance.","The review is overwhelmingly positive. The reviewer uses terms like ""clever"", ""nicely"", ""convincing"", ""good"", and ""interesting"" to describe the paper and its results. They are also ""happy to recommend acceptance."" This indicates a very positive sentiment. The language used is polite and professional throughout, with no negative or disrespectful phrasing.",90,100
"This paper introduces a reinforcement learning framework for designing a neural network architecture. For each time-step, the agent picks a new layer type with corresponding layer parameters (e.g., #filters). In order to reduce the size of state-action space, they used a small set of design choices. Strengths: - A novel approach for automatic design of neural network architectures. - Shows quite promising results on several datasets (MNIST, CIFAR-10). Weakness: - Limited architecture design choices due to many prior assumptions (e.g., a set of possible number of convolution filters, at most 2 fully-connected layers, maximum depth, hard-coded dropout, etc.) - The method is demonstrated in tabular Q-learning setting, but it is unclear whether the proposed method would work in a large state-action space. Overall, this is an interesting and novel approach for neural network architecture design, and it seems to be worth publication despite some weaknesses.","The review acknowledges both strengths and weaknesses of the paper, ultimately recommending publication. While it points out limitations, it also highlights the novelty and promising results. The language is constructive and balanced.",60,70
"As discussed, the there are multiple concurrent contributions in different packages/submission by the authors that are in parts difficult to disentangle. Despite this fact, it is impressive to see a system learning from natural feedback in an online fashion. To the best of my knowledge, this is a new quality of result that was achieved - in particular as close to full supervision results are reached in some cases in this less constraint setting. several points were raised that were in turn addressed by the authors: 1. formalisation of the task (learning dialogue) is not precise. when can we declare success? The answer of the authors is partially satisfying. For this particular work, it might make sense to more precisely set goals e.g. to be as good as full supervision. 2. (along the line of the previous question:) dialogue can be seen as a form of noisy supervision. can you please report the classic supervision baselines for the particular model used? this would give a sense what fraction of the best case performance is achieved via dialogue learning. The authors provided additional information along those lines - and I think this helps to understand how much of the overall goal was achieved and open challenges. 3. is there an understanding of how much more difficult the MT setting is? feedback could be hand labeled as positive or negative for an analysis (?). or a handcrafted baseline could be tested, that either extracts the reward via template matching … or maybe even uses the length of the feedback as a proxy/baseline. (it looks to me that short feedback is highly correlated with high reward / correct answer (?)) The authors replied - but it would have been clearer if they could have quantified such suggested baseline, in order to confirm that there is no simple handcrafted baseline that would do well on the data - but these concerns are marginal. 4. relation to prior work Weston’16 is not fully clear. I understand that this submission should be understood as an independent submission of the prior work Weston’16 - and not replacing it. In this case Weston’16 makes this submission appear more incremental. my understanding is that the punch line of this submission is the online part that leads in turn to more exploration. Is there any analysis on how much this aspect matters? I couldn’t find this in the experiments. The authors clarified the raised issues. The application of reinforcement learning and in particular FP is convincing. There is a incremental nature to the paper - and the impression is emphasised by multiple concurrent contributions of the authors on this research thread. Comparison to prior work (in particular Weston*16), should be made more explicit. Not only in text but also in the experiments - as the authors partially do in their reply to the reviewers question. Nevertheless, this particular contribution is assessed as significant and worth sharing and seems likely to have impact on how we can learn in these less constraint setting.","The reviewer highlights several positive aspects: impressive system, novel results, close to full supervision performance. While the reviewer raises valid points about task formalization, baselines, and relation to prior work, these are presented constructively and the author's responses are acknowledged. The reviewer ultimately finds the contribution significant and impactful.",75,80
"The paper presents an application of deep learning to genomic SNP data with a comparison of possible approaches for dealing with the very high data dimensionality. The approach looks very interesting but the experiments are too limited to draw firm conclusions about the strengths of different approaches. The presentation would benefit from more precise math. Quality: The basic idea of the paper is interesting and the applied deep learning methodology appears reasonable. The experimental evaluation is rather weak as it only covers a single data set and a very limited number of cross validation folds. Given the significant variation in the performances of all the methods, it seems the differences between the better-performing methods are probably not statistically significant. More comprehensive empirical validation could clearly strengthen the paper. Clarity: The writing is generally good both in terms of the biology and ML, but more mathematical rigour would make it easier to understand precisely what was done. The different architectures are explained on an intuitive level and might benefit from a clear mathematical definition. I was ultimately left unsure of what the *raw end2end* model is - given so few parameters it cannot work on raw 300k dimensional input but I could not figure out what kind of embedding was used. The results in Fig. 3 might be clearer if scaled so that maximum for each class is 1 to avoid confounding from different numbers of subjects in different classes. In the text, please use the standard italics math font for all symbols such as N, N_d, ... Originality: The application and the approach appear quite novel. Significance: There is clearly strong interest for deep learning in the genomics area and the paper seeks to address some of the major bottlenecks here. It is too early to tell whether the specific techniques proposed in the paper will be the ultimate solution, but at the very least the paper provides interesting new ideas for others to work on. Other comments: I think releasing the code as promised would be a must.","The reviewer finds the paper interesting and novel, highlighting its potential in the field. However, they express concerns about the limited experimental validation and suggest improvements in clarity, particularly regarding mathematical rigor and presentation. The overall tone is constructive and helpful, suggesting ways to strengthen the paper rather than outright rejecting it.",50,75
"This paper presents a way of training deep generative models with discrete hidden variables using the reparameterization trick. It then applies it to a particular DBN-like architecture, and shows that this architecture achieves state-of-the-art density modeling performance on MNIST and similar datasets. The paper is well written, and the exposition is both thorough and precise. There are several appendices which justify various design decisions in detail. I wish more papers in our field would take this degree of care with the exposition! The log-likelihood results are quite strong, especially given that most of the competitive algorithms are based on continuous latent variables. Probably the main thing missing from the experiments is some way to separate out the contributions of the architecture and the inference algorithm. (E.g., what if a comparable architecture is trained with VIMCO, or if the algorithm is applied to a previously published discrete architecture?) I’m a bit concerned about the variance of the gradients in the general formulation of the algorithm. See my comment “variance of the derivatives of F^{-1}” below. I think the response is convincing, but the problem (as well as “engineering principles” for the smoothing distribution) are probably worth pointing out in the paper itself, since the problem seems likely to occur unless the user is aware of it. (E.g., my proposal of widely separated normals would be a natural distribution to consider until one actually works through the gradients — something not commonly done in the age of autodiff frameworks.) Another concern is how many sequential operations are needed for inference in the RBM model. (Note: is this actually an RBM, or a general Boltzmann machine?) The q distribution takes the form of an autoregressive model where the variables are processed one at a time. Section 3 mentions the possibility of grouping together variables in the q distribution, and this is elaborated in detail in Appendix A. But the solution requires decomposing the joint into a product of conditionals and applying the CDFs sequentially. So either way, it seems like we’re stuck handling all the variables sequentially, which might get expensive. Minor: the second paragraph of Section 3 needs a reference to Appendix A.","The review starts with very positive statements, praising the paper and its exposition. It then raises some valid concerns and suggestions for improvement, but these are framed as constructive feedback rather than harsh criticism. The reviewer acknowledges the authors' responses to some of their concerns, indicating a collaborative tone. The language used is professional and respectful throughout.",60,80
"In this paper, the authors present a partially asynchronous variant of the K-FAC method. The authors adapt/modify the K-FAC method in order to make it computationally tractable for optimizing deep neural networks. The method distributes the computation of the gradients and the other quantities required by the K-FAC method (2nd order statistics and Fisher Block inversion). The gradients are computed in synchronous manner by the ‘gradient workers’ and the quantities required by the K-FAC method are computed asynchronously by the ‘stats workers’ and ‘additional workers’. The method can be viewed as an augmented distributed Synchronous SGD method with additional computational nodes that update the approximate Fisher matrix and computes its inverse. The authors illustrate the performance of the method on the CIFAR-10 and ImageNet datasets using several models and compare with synchronous SGD. The main contributions of the paper are: 1) Distributed variant of K-FAC that is efficient for optimizing deep neural networks. The authors mitigate the computational bottlenecks of the method (second order statistic computation and Fisher Block inverses) by asynchronous updating. 2) The authors propose a “doubly-factored” Kronecker approximation for layers whose inputs are too large to be handled by the standard Kronecker-factored approximation. They also present (Appendix A) a cheaper Kronecker factored approximation for convolutional layers. 3) Empirically illustrate the performance of the method, and show: - Asynchronous Fisher Block inversions do not adversely affect the performance of the method (CIFAR-10) - K-FAC is faster than Synchronous SGD (with and without BN, and with momentum) (ImageNet) - Doubly-factored K-FAC method does not deteriorate the performance of the method (ImageNet and ResNet) - Favorable scaling properties of K-FAC with mini-batch size Pros: - Paper presents interesting ideas on how to make computationally demanding aspects of K-FAC tractable. - Experiments are well thought out and highlight the key advantages of the method over Synchronous SGD (with and without BN). Cons: - “…it should be possible to scale our implementation to a larger distributed system with hundreds of workers.” The authors mention that this should be possible, but fail to mention the potential issues with respect to communication, load balancing and node (worker) failure. That being said, as a proof-of-concept, the method seems to perform well and this is a good starting point. - Mini-batch size scaling experiments: the authors do not provide validation curves, which may be interesting for such an experiment. Keskar et. al. 2016 (On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima) provide empirical evidence that large-batch methods do not generalize as well as small batch methods. As a result, even if the method has favorable scaling properties (in terms of mini-batch sizes), this may not be effective. The paper is clearly written and easy to read, and the authors do a good job of communicating the motivation and main ideas of the method. There are a few minor typos and grammatical errors. Typos: - “updates that accounts for” — “updates that account for” - “Kronecker product of their inverse” — “Kronecker product of their inverses” - “where P is distribution over” — “where P is the distribution over” - “back-propagated loss derivativesas” — “back-propagated loss derivatives as” - “inverse of the Fisher” — “inverse of the Fisher Information matrix” - “which amounts of several matrix” — “which amounts to several matrix” - “The diagram illustrate the distributed” — “The diagram illustrates the distributed” - “Gradient workers computes” — “Gradient workers compute” - “Stat workers computes” — “Stat workers compute” - “occasionally and uses stale values” — “occasionally and using stale values” - “The factors of rank-1 approximations” — “The factors of the rank-1 approximations” - “be the first singular value and its left and right singular vectors” — “be the first singular value and the left and right singular vectors … , respectively.” - “Psi is captures” — “Psi captures” - “multiplying the inverses of the each smaller matrices” — “multiplying the inverses of each of the smaller matrices” - “which is a nested applications of the reshape” — “which is a nested application of the reshape” - “provides a computational feasible alternative” — “provides a computationally feasible alternative” - “according the geometric mean” — “according to the geometric mean” - “analogous to shrink” — “analogous to shrinking” - “applied to existing model-specification code” — “applied to the existing model-specification code” - “: that the alternative parametrization” — “: the alternative parameterization” Minor Issues: - In paragraph 2 (Introduction) the authors mention several methods that approximate the curvature matrix. However, several methods that have been developed are not mentioned. For example: 1) (AdaGrad) Adaptive Subgradient Methods for Online Learning and Stochastic Optimization (http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf) 2) Stochastic Quasi-Newton Methods for Nonconvex Stochastic Optimization (https://arxiv.org/abs/1607.01231) 3) adaQN: An Adaptive Quasi-Newton Algorithm for Training RNNs (http://link.springer.com/chapter/10.1007/978-3-319-46128-1_1) 4) A Self-Correcting Variable-Metric Algorithm for Stochastic Optimization (http://jmlr.org/proceedings/papers/v48/curtis16.html) 5) L-SR1: A Second Order Optimization Method for Deep Learning (https://openreview.net/pdf?id=By1snw5gl) - Page 2, equation s = WA, is there a dimension issue in this expression? - x-axis for top plots in Figures 3,4,5,7 (Updates x XXX) appear to be a headings for the lower plots. - “James Martens. Deep Learning via Hessian-Free Optimization” appears twice in References section.","The review is largely positive. The reviewer acknowledges the paper's interesting ideas, well-thought-out experiments, and clear writing. While the reviewer lists several 'Cons', these are presented constructively and point out potential areas of improvement rather than fundamental flaws. The language used is formal and polite, typical of academic peer reviews.",75,80
"Description. This paper describes experiments testing whether deep convolutional networks can be replaced with shallow networks with the same number of parameters without loss of accuracy. The experiments are performed on he CIFAR 10 dataset where deep convolutional teacher networks are used to train shallow student networks using L2 regression on logit outputs. The results show that similar accuracy on the same parameter budget can be only obtained when multiple layers of convolution are used. Strong points. - The experiments are carefully done with thorough selection of hyperparameters. - The paper shows interesting results that go partially against conclusions from the previous work in this area (Ba and Caruana 2014). - The paper is well and clearly written. Weak points: - CIFAR is still somewhat toy dataset with only 10 classes. It would be interesting to see some results on a more challenging problem such as ImageNet. Would the results for a large number of classes be similar? Originality: - This is mainly an experimental paper, but the question it asks is interesting and worth investigation. The experimental results are solid and provide new insights. Quality: - The experiments are well done. Clarity: - The paper is well written and clear. Significance: - The results go against some of the conclusions from previous work, so should be published and discussed. Overall: Experimental paper with interesting results. Well written. Solid experiments.","The review is overwhelmingly positive. It highlights the strengths of the paper, such as the careful experiments, interesting results, and clear writing. While it mentions a 'weak point,' it is phrased as a suggestion for future work rather than a criticism. The overall tone is encouraging and suggests the paper is worthy of publication.",85,90
"summary The paper explains dropout with a latent variable model where the dropout variable (0 or 1 depending on which units should be dropped) is not observed and is accordingly marginalised. Maximum likelihood under this model is not tractable but standard dropout then corresponds to a simple Monte Carlo approximation of ML for this model. The paper then introduces a theoretical framework for analysing the discrepancy (called inference gap) between the model at training (model ensemble, or here the latent variable model), and the model at testing (where usually what should be an expectation over the activations over many models becomes the activation of one model with averaged weights). This framework introduces several notions (e.g. expectation linearity) which allow the study of which transition functions (and more generally layers) can have a small inference gap. Theorem 3 gives a bound on the inference gap. Finally a new regularisation term is introduced to account for minimisation of the inference gap during learning. Experiments are performed on MNIST, CIFAR-10 and CIFAR-100 and show that the method has the potential to perform better than standard dropout and at the level of Monte Carlo Dropout (the standard method to compute the real dropout outputs consistently with the training assumption of an ensemble, of course quite expensive computationally) The study gives a very interesting theoretical model for dropout as a latent variable model where standard dropout is then a monte carlo approximation. This is very probably widely applicable to further studies of dropout. The framework for the study of the inference gap is interesting although maybe somewhat less widely applicable. The proposed model is convincing although 1. it is tested on simple datasets 2. the gains are relatively small and 3. there is an increased computational cost during training because a new hyper-parameter is introduced. p6 line 8 typo: expecatation","The review is positive about the paper's novelty and contribution (""very interesting theoretical model"", ""widely applicable"", ""convincing""). It acknowledges the limitations but frames them as opportunities for improvement rather than strong criticisms. The overall tone is constructive and encouraging.",75,80
"Paper Summary: The paper introduces a question answering model called Dynamic Coattention Network (DCN). It extracts co-dependent representations of the document and question, and then uses an iterative dynamic pointing decoder to predict an answer span. The proposed model achieves state-of-the-art performance, outperforming all published models. Paper Strengths: -- The proposed model introduces two new concepts to QA models -- 1) using attention in both directions, and 2) a dynamic decoder which iterates over multiple answer spans until convergence or maximum number of iterations. -- The paper also presents ablation study of the proposed model which shows the importance of their design choices. -- It is interesting to see the same idea of co-attention performing well in 2 different domains -- Visual Question Answering and machine reading comprehension. -- The performance breakdown over document and question lengths (Figure 6) strengthens the importance of attention for QA task. -- The proposed model achieves state-of-the-art result on SQuAD dataset. -- The model architecture has been clearly described. Paper Weaknesses / Future Thoughts: -- The paper provides model*s performance when the maximum number of iterations is 1 and 4. I would like to see how the performance of the model changes with the number of iterations, i.e., the model performance when that number is 2 and 3. Is there a clear trend? What type of questions is the model able to get correct with more iterations? -- As with many deep learning approaches, the overall architecture seems quite complex, and the design choices seem to be driven by performance numbers. As future work, authors might try to analyze qualitative advantages of different choices in the proposed model. What type of questions are correctly answered because of co-attention mechanism instead of attention in a single direction, when using Maxout Highway Network instead of a simple MLP, etc? Preliminary Evaluation: Novel and state-of-the-art question answering approach. Model is clearly described in detail. In my thoughts, a clear accept.","The review is overwhelmingly positive. The reviewer explicitly states ""In my thoughts, a clear accept."" and lists significantly more strengths than weaknesses. The weaknesses themselves are framed as constructive suggestions for improvement rather than harsh criticisms. The language used is objective and professional throughout.",90,100
"This paper explores ensemble optimisation in the context of policy-gradient training. Ensemble training has been a low-hanging fruit for many years in the this space and this paper finally touches on this interesting subject. The paper is well written and accessible. In particular the questions posed in section 4 are well posed and interesting. That said the paper does have some very weak points, most obviously that all of its results are for a very particular choice of domain+parameters. I eagerly look forward to the journal version where these experiments are repeated for all sorts of source domain/target domain/parameter combinations. <rant Finally a stylistic comment that the authors can feel free to ignore. I don*t like the trend of every paper coming up with a new acronymy wEiRDLY cAsEd name. Especially here when the idea is so simple. Why not use words? English words from the dictionary. Instead of *EPOpt* and *EPOpt-e*, you can write *ensemble training* and *robust ensemble training*. Is that not clearer? />","The review starts with positive statements, highlighting the relevance and good writing. However, it then points out a ""very weak point"" that significantly impacts the validity of the results. The reviewer's eagerness for a more comprehensive journal version suggests potential but ultimately leaves the current work somewhat lacking. The tone is mostly professional, but the rant at the end, although presented as optional, detracts from the overall politeness. ",20,50
"The paper presents an analysis of the ability of deep networks with ReLU functions to represent particular types of low-dimensional manifolds. Specifically, the paper focuses on what the authors call *monotonic chains of linear segments*, which are essentially sets of intersecting tangent planes. The paper presents a construction that efficiently models such manifolds in a deep net, and presents a basic error analysis of the resulting construction. While the presented results are novel to the best of my knowledge, they are hardly surprising (1) given what we already know about the representational power of deep networks and (2) given that the study selects a deep network architecture and a data structure that are very *compatible*. In particular, I have three main concerns with respect to the results presented in this paper: (1) In the last decade, there has been quite a bit of work on learning data representations from sets of local tangent planes. Examples that spring to mind are local tangent space analysis of Zhang & Zha (2002), manifold charting by Brand (2002) and alignment of local models by Verbeek, Roweis, and Vlassis (2003). None of this work is referred to in related work, even though it seems highly relevant to the analysis presented here. For instance, it would be interesting to see how these old techniques compare to the deep network trained to produce the embedding of Figure 6. This may provide some insight into the inductive biases the deep net introduces: does it learn better representations that non-parametric techniques because it has better inductive biases, or does it learn worse representations because the loss being optimized is non-convex? (2) It is difficult to see how the analysis generalizes to more complex data in which local linearity assumptions on the data manifold are vacuous given the sparsity of data in high-dimensional space, or how it generalizes to deep network architectures that are not pure ReLU networks. For instance, most modern networks use a variant of batch normalization; this already appears to break the presented analyses. (3) The error bound presented in Section 4 appears vacuous for any practical setting, as the upper bound on the error is exponential in the total curvature (a quantity that will be quite large in most practical settings). This is underlined by the analysis of the Swiss roll dataset, of which the authors state that the *bound for this case is very loose*. The fact that the bound is already so loose for this arguably very simple manifold makes that the error analysis may tell us very little about the representational power of deep nets. I would encourage the authors to address issue (1) in the revision of the paper. Issue (2) and (3) may be harder to address, but is essential that they are addressed for the line of work pioneered by this paper to have an impact on our understanding of deep learning. Minor comments: - In prior work, the authors only refer to fully supervised siamese network approaches. These approaches differ from that taken by the authors, as their approach is unsupervised. It should be noted that the authors are not the first to study unsupervised representation learners parametrized by deep networks: other important examples are deep autoencoders (Hinton & Salakhutdinov, 2006 and work on denoising autoencoders from Bengio*s group) and parametric t-SNE (van der Maaten, 2009). - What loss do the authors use in their experiments? Using *the difference between the ground truth distance ... and the distance computed by the network* seems odd, because it encourages the network to produce infinitely large distances (to get a loss of minus infinity). Is the difference squared?","The reviewer acknowledges the novelty of the work but expresses several significant concerns. They find the results ""hardly surprising"" and point out limitations in the analysis and applicability. The reviewer also criticizes the error bound as ""vacuous"" for practical use. While they offer suggestions for improvement, they emphasize the need to address these issues for the work to have a real impact. This suggests a lukewarm reception overall, leaning more towards the critical side. The language, while direct and critical, maintains a professional and respectful tone, avoiding personal attacks or dismissive language.",-20,60
"This paper discusses a method for computing vector representations for documents by using a skip-gram style learning mechanism with an added regularizer in the form of a global context vector with various bits of drop out. While none of the individual components proposed in this paper are new, I believe that the combination in this fashion is. Further, I appreciated the detailed analysis of model behaviour in section 3. The main downside to this submission is in its relative weakness on the empirical front. Arguably there are more interesting tasks than sentiment analysis and k-way classification! Likewise, why waste 2/3 of a page on t-sne projections rather than use that space for further analysis? While I am a bit disappointed by this reduced evaluation and agree with the other reviewers concerning soft baselines, I think this paper should be accepted: it*s an interesting algorithm, nicely composed and very efficient, so it*s reasonable to assume that other readers might have use for some of the ideas presented here.","The reviewer finds the core idea of the paper interesting and acknowledges the detailed analysis. They appreciate the novelty of the combination proposed, even though individual components are not new.  However, they express disappointment about the limited empirical evaluation and suggest improvements. Despite the criticism, the reviewer leans towards acceptance due to the algorithm's efficiency and potential usefulness for others. The language is critical but professional and not disrespectful.",60,60
"This paper presents a succinct argument that the principle of optimizing receptive field location and size in a simulated eye that can make saccades with respect to a classification error of images of data whose labels depend on variable-size and variable-location subimages, explains the existence of a foveal area in e.g. the primate retina. The argument could be improved by using more-realistic image data and drawing more direct correspondence with the number, receptive field sizes and eccentricities of retinal cells in e.g. the macaque, but the authors would then face the challenge of identifying a loss function that is both biologically plausible and supportive of their claim. The argument could also be improved by commenting on the timescales involved. Presumably the density of the foveal center depends on the number of of saccades allowed by the inference process, as well as the size of the target sub-images, and also has an impact on the overall classification accuracy. Why does the classification error rate of dataset 2 remain stubbornly at 24%? This seems so high that the model may not be working the way we’d like it to. It seems that the overall argument of the paper pre-supposes that the model can be trained to be a good classifier. If there are other training strategies or other models that work better and differently, then it raises the question of why do our eyes and visual cortex not work more like *those ones* if evolutionary pressures are applying the same pressure as our training objective. Why does the model with zooming powers out-do the translation-only model on dataset 1 (where all target images are the same size) and tie the translation-only model dataset 2 (where the target images have different sizes, for which the zooming model should be tailor-made?). Between this strange tie and the high classification rate on Dataset 2, I wonder if maybe one or both models isn’t being trained to its potential, which would undermine the overall claim. Comparing this model to other attention models (e.g. spatial transformer networks, DRAW) would be irrelevant to what I take to be the main point of the paper, but it would address the potential concerns above that training just didn’t go very well, or there was some problem with the model parameterization that could be easily fixed.","The reviewer acknowledges the paper's interesting idea but raises several concerns about the methodology and results. They suggest improvements like using more realistic data and addressing specific issues with the model's performance. The tone is critical but professional, focusing on constructive feedback. Therefore, the sentiment leans slightly negative due to the significant concerns, and the politeness remains neutral as the reviewer maintains professional language.",-20,0
"This paper is a parallel work to Improving Generative Adversarial Networks with Denoising Feature Matching. The main solution of both papers is introducing autoencoder into discriminator to improve the stability and quality of GAN. Different to Denoising Feature Matching, EBGAN uses encoder-decoder instead of denoising only, and use hingle loss to replace original loss function. The theoretical results are good, and empirical result of high resolution image is unique among all recent GAN advantages. I suggest to introduce Improving Generative Adversarial Networks with Denoising Feature Matching as related work.","The review acknowledges the paper's merits, stating its theoretical results are ""good"" and highlights the uniqueness of its empirical results. It doesn't use overtly negative phrasing and provides a concrete suggestion for improvement, indicating a constructive approach. Therefore, the sentiment leans positive, and the language is polite.",60,70
"Overview: This paper introduces a biasing term for SGD that, in theoretical results and a toy example, yields solutions with an approximately equal or lower generalization error. This comes at a computational cost of estimating the gradient of the biasing term for each iteration through stochastic gradient Langevin dynamics, approximating an MCMC sample of the log partition function of a modified Gibbs distribution. The cost is equivalent to adding an inner for-loop to the standard SGD algorithm for each minibatch. Pros: - Reviews and distills many results and theorems from past 2 decades that suggest a promising way forward for increasing the generalizability of deep neural networks - Generally very well written and well presented results, with interesting discussion of eigenvalues of Hessian as a way to characterize “flat” minima - Promising mathematical arguments suggest that E-SGD has generalization error bounded below by SGD, motivating further research in the area Cons / points suggested for a rebuttal: (1) One claim of the paper given in the abstract is ”experiments on competitive baselines demonstrate that Entropy-SGD leads to improved generalization and has the potential to accelerate training.“ This does not appear to be supported by the current set of experiments. As the authors comment in the discussion section, “In our experiments, Entropy-SGD results in a comparable generalization error as SGD, but always has a lower cross-entropy loss.” It*s not clear to me how to reconcile those two claims. (2) Similarly, the claim of accelerated training is not convincingly supported in the present version of the paper. Vanilla SGD requires a single forward pass through all M minibatches during one epoch for a parameter update, but the new method, E-SGD requires, L*M forward passes during one epoch where L is the number of Langevin updates, which require a minibatch sample each. This could in fact mean that E-SGD has worse computational complexity to reach the same point. In a remark on p.9, the authors note that a single epoch is defined to be “the number of parameter updates required to run through the dataset once.” It’s not clear to me how this answers the objection to a factor of L additional computations required for the inner-loop SGLD iterations. SGLD appears to introduces a potentially costly tradeoff that must be carefully managed by a user of E-SGD. (3) As the previous two points suggest, the paper could use some attention to the magnitude of the claims. For example, the introduction reads “Actively biasing towards wide valleys aids generalization, in fact, we can optimize solely the free energy term to obtain similar generalization error as SGD on the original loss function.“ According the the values reported on pp.9-10, only on MNIST is the generalization error, using only the free energy term (the log partition function of the modified Gibbs distribution), equivalent to using only the SGD loss function. This corresponds to setting rho to 0 in equation (6). On CIFAR-10, rho = 0.01 is used. (4) Another contribution of this paper, the characterization of the optimization landscape in terms of the eigenvalues of the Hessian and low generalization error being associated with flat local extrema, is helpful and interesting. I found the plots clear and useful. As another reviewer has already pointed out, there are high-level similarities to “Flat Minima” by Hochreiter and Schmidhuber (1997). The authors have responded already by adding a paragraph that helpfully explores some differences with H&S 1997. However, the similarities should also be carefully identified and mentioned. H&S 1997 includes detailed theoretical analysis that could be helpful for future work in this area, and has independently discovered a similar approach to training generalizable networks. (5) It*s not clear how the assumption about the eigenvalues that were made in section 4.4 / Appendix B affect the application of this result to real-world problems. What magnitude of c>0 needs to be chosen? Does this correspond to a measurable characteristic of the dataset? It*s a little mysterious in the current version of the paper.","The review is mostly positive, highlighting the paper's strengths such as its well-written nature, promising theoretical foundation, and interesting discussion of Hessian eigenvalues. However, it also raises several important concerns and suggestions for improvement. The reviewer questions the paper's claims regarding improved generalization and accelerated training, pointing out inconsistencies between the claims and the presented results. They also suggest a more in-depth comparison with related work and clarification on certain assumptions. The tone remains professional and constructive throughout, focusing on specific areas for improvement. Therefore, the sentiment leans towards the positive side, but not overwhelmingly so due to the constructive criticism.",60,80
"The paper presents a learning algorithm for micromanagement of battle scenarios in real-time strategy games. It focuses on a complex sub-problem of the full RTS problem. The assumptions and restrictions made (greedy MDP, distance-based action encoding, etc.) are clear and make sense for this problem. The main contribution of this paper is the zero-order optimization algorithm and how it is used for structured exploration. This is a nice new application of zero-order optimization meets deep learning for RL, quite well-motivated using similar arguments as DPG. The results show clear wins over vanilla Q-learning and REINFORCE, which is not hard to believe. Although RTS is a very interesting and challenging domain (certainly worthy as a domain of focused research!), it would have been nice to see results on other domains, mainly because it seems that this algorithm could be more generally applicable than just RTS games. Also, evaluation on such a complex domain makes it difficult to predict what other kinds of domains would benefit from this zero-order approach. Maybe the authors could add some text to clarify/motivate this. There are a few seemingly arbitrary choices that are justified only by *it worked in practice*. For example, using only the sign of w / Psi_{theta}(s^k, a^k). Again later: *Also we neglected the argmax operation that chooses the actions*. I suppose this and dividing by t could keep things nicely within or close to [-1,1] ? It might make sense to try truncating/normalizing w/Psi; it seems that much information must be lost when only taking the sign. Also lines such as *We did not extensively experiment with the structure of the network, but we found the maxpooling and tanh nonlinearity to be particularly important* and claiming the importance of adagrad over RMSprop without elaboration or providing any details feels somewhat unsatisfactory and leaves the reader wondering why.. e.g. could these only be true in the RTS setup in this paper? The presentation of the paper can be improved, as some ideas are presented without any context making it unnecessarily confusing. For example, when defining f(	ilde{s}, c) at the top of page 5, the w vector is not explained at all, so the reader is left wondering where it comes from or what its use is. This is explained later, of course, but one sentence on its role here would help contextualize its purpose (maybe refer later to the section where it is described fully). Also page 7: *because we neglected that a single u is sampled for an entire episode*; actually, no, you did mention this in the text above and it*s clear from the pseudo-code too. *perturbated* -> *perturbed* --- After response period: No rebuttal entered, therefore review remains unchanged.","The review is mostly positive, highlighting the novelty and potential of the algorithm. However, it also raises several concerns about the experimental methodology and clarity of the paper. The reviewer suggests additional experiments and clarifications to strengthen the paper. The language used is generally polite and professional, offering constructive criticism with phrases like ""it would have been nice to see"" or ""it might make sense to try."" However, there are instances where the reviewer expresses dissatisfaction, such as ""feels somewhat unsatisfactory"" or points out issues directly like ""actually, no, you did mention this.""",60,70
"The paper proposes a method for pruning weights in neural networks during training to obtain sparse solutions. The approach is applied to an RNN-based system which is trained and evaluated on a speech recognition dataset. The results indicate that large savings in test-time computations can be obtained without affecting the task performance too much. In some cases the method can actually improve the evaluation performance. The experiments are done using a state-of-the-art RNN system and the methodology of those experiments seems sound. I like that the effect of the pruning is investigated for networks of very large sizes. The computational gains are clearly substantial. It is a bit unfortunate that all experiments are done using a private dataset. Even with private training data, it would have been nice to see an evaluation on a known test set like the HUB5 for conversational speech. It would also have been nice to see a comparison with some other pruning approaches given the similarity of the proposed method to the work by Han et al. [2] to verify the relative merit of the proposed pruning scheme. While single-stage training looks more elegant at first sight, it may not save much time if more experiments are needed to find good hyperparameter settings for the threshold adaptation scheme. Finally, the dense baseline would have been more convincing if it involved some model compression tricks like training on the soft targets provided by a bigger network. Overall, the paper is easy to read. The table and figure captions could be a bit more detailed but they are still clear enough. The discussion of potential future speed-ups of sparse recurrent neural networks and memory savings is interesting but not specific to the proposed pruning algorithm. The paper doesn’t motivate the details of the method very well. It’s not clear to me why the threshold has to ramp up after a certain period time for example. If this is based on preliminary findings, the paper should mention that. Sparse neural networks have been the subject of research for a long time and this includes recurrent neural networks (e.g., sparse recurrent weight matrices were standard for echo-state networks [1]). The proposed method is also very similar to the work by Han et al. [2], where a threshold is used to prune weights after training, followed by a retraining phase of the remaining weights. While I think that it is certainly more elegant to replace this three stage procedure with a single training phase, the proposed scheme still contains multiple regimes that resemble such a process by first training without pruning followed by pruning at two different rates and finally training without further pruning again. The main novelty of the work would be the application of such a scheme to RNNs, which are typically more tricky to train than feedforward nets. Improving scalability is an important driving force of the progress in neural network research. While I don’t think the paper presents much novelty in ideas or scientific insight, it does show that weight pruning can be successfully applied to large practical RNN systems without sacrificing much in performance. The fact that this is possible with such a simple heuristic is a result worth sharing. Pros: The proposed method is successful at reducing the number of parameters in RNNs substantially without sacrificing too much in performance. The experiments are done using a state-of-the-art system for a practical application. Cons: The proposed method is very similar to earlier work and barely novel. There is no comparison with other pruning methods. The data is private and this prevents others from replicating the results. [1] Jaeger, H. (2001). The “echo state” approach to analyzing and training recurrent neural networks-with an erratum note. Bonn, Germany: German National Research Center for Information Technology GMD Technical Report, 148, 34. [2] Han, Song, Pool, Jeff, Tran, John, and Dally, William J. Learning both weights and connections for efficient neural networks. In Advances in Neural Information Processing Systems, 2015.","The review is mostly positive. The reviewer acknowledges the value of the paper's findings, especially the successful application of weight pruning to large RNNs with minimal performance loss. The reviewer uses phrases like ""The results indicate"", ""I like that"", ""The computational gains are clearly substantial"", ""The paper is easy to read"", ""interesting"", and ""a result worth sharing"" to express their positive sentiment. However, the reviewer also points out several limitations, such as lack of novelty, comparison with other methods, and use of a private dataset. Despite these limitations, the reviewer's overall tone is constructive and encouraging. The language used is polite and professional throughout the review.",60,80
"Authors propose a mechanism for selecting the design of filters in convolutional layers. The basic idea is that convolution should be applied to input feature dimensions that are highly correlated in order to detect rare events. For example, adjacent pixels in images are correlated and edges are rare events of interest to be detected. Authors argue that square filters are therefore appropriate in images. However, in data such as bird songs high correlations might exist between non-adjacent harmonics and a convolution filter should take a weighted summation over these input feature dimensions. Such an operation can thus be thought of computing data-dependent dilated convolutions. Paper theoretically motivates this choice using the idea of Gaussian complexity of the learner (i.e. a CNN in this case). The main idea being that choosing convolution filters that sum over correlated features results in lower Gaussian complexity and thus the learner has higher ability to generalize. While I am no expert in theoretical analysis of learning algorithms – there are parts of proof that look sound, but there are parts that are rather hand wavy (for eg, extension to networks using max-pooling from average pooling). Also, the theory is not directly applicable to choosing filters when number of layers are more than 1. I am willing to overlook the paucity in rigor in some parts of the theoretical arguments because the empirical evidence looks convincing. The method of choosing the filter shape can be briefly summarized as: (a) The covariance matrix of the input features is computed. (b) Using the covariance matrix, feature dimensions with highest correlations are determined by solving equation (7). A hard limit on maximum number of filter dimensions is imposed (typically ~ 10-15). This leads to choice of a single design for all filters in the layer. (c) Authors extend the framework to work with multiple layers in the following way: A subset of feature dimensions cannot account for all variance in the inputs and there is some residual variance. The filter design of the next layer attempts to minimize this residual variance. This process is repeated iteratively by solving eq (8) to obtain filter designs for all the layers. Ideally for determining filter designs of different layers – one should have computed the covariance statistics of outputs of the previous layer. However this assumes that filters of the previous layer are already known and this is not computationally feasible to implement. Authors instead use the method described in (c). A question which comes to my mind is – a single feature design is chosen for each layer. Have the authors considered using the process in (c) to chose different filter designs for different filters in the same layer as opposed to using the same filter design for all the filters? Regarding baselines: B1. It would be great to see a comparison with randomly chosen filter designs. Two comparisons could be made – (1a) A single random design is chosen for each layer. (1b) The design of each filter is chosen randomly (i.e. allowing for different designs of filter within each layer). B2. Since the theory is not really applicable to CNNs with more that one layer – I wonder how much of the benefit is obtained by choosing the filter design just in a single layer v/s all the layers. A good comparison would be when filter design of the first layer are chosen using the described method and filters in higher layers are chosen to be square. B3. Authors mention the use L1 regularization in the baselines. Was the L1 penalty cross-validated? If so, then upto what range? Somethings which are unclear: - “exclude data that represent obvious noise” - DFMax mentioned in the supplementary materials Overall I think this is very interesting idea for filter design. The authors have done a fair set of experiments but I would really like to see results of B1, B2 and the answer to question in B3. I have currently set my rating to a weak reject, but I am happy to raise my ratings to – “Good paper, accept” if the authors provide results of experiments and answers to questions in my comments above.","The reviewer provides constructive criticism, acknowledges the merits of the paper (interesting idea, convincing empirical evidence), but expresses concerns and requests clarifications/additional experiments. They are open to changing their rating to ""Good paper, accept"" based on the author's response. This suggests an overall positive sentiment, but with reservations. The language is polite and professional throughout.",60,80
"The paper details an implementation of sparse-full convolutions and a model to work out the potential speed-up of various sparsity levels for CNNs. The first contribution is more about engineering, but the authors make the source code available which is greatly appreciated. The second contribution is perhaps more interesting, as so far pruning methods have focused on saving memory, with very modest speed gains. Imbuing knowledge of running speed into a pruning algorithm seems like the proper way to tackle this problem. The authors are very methodical in how they build the model and evaluate it very thoroughly. It seems that the same idea could be used not just for pruning existing models, but also when building new architectures: selecting layers and their parameters as to achieve an optimal throughput rate. This could make for a nice direction for future work. One point that is missing is some discussion of how transferable the performance model is to GPUs. This would make the technique easier to adopt broadly. Other areas for improvement: The points in Figure 4 are hard to distinguish (e.g. small red circle vs. small red square), and overall the figure could be made bigger; specifying whether the *base learning rate* in Section 3 is the start or end rate of the annealing schedule; typos: *punning* (p.4), *spares* (p.5).","The review is positive about the paper, highlighting the contribution of making the source code available, the interesting approach to pruning for speed gains, and the thorough evaluation. The reviewer suggests potential future work and points out minor areas for improvement, but these are presented constructively. Overall, the tone is encouraging and appreciative of the work.",75,90
"The authors present a methodology for analyzing sentence embedding techniques by checking how much the embeddings preserve information about sentence length, word content, and word order. They examine several popular embedding methods including autoencoding LSTMs, averaged word vectors, and skip-thought vectors. The experiments are thorough and provide interesting insights into the representational power of common sentence embedding strategies, such as the fact that word ordering is surprisingly low-entropy conditioned on word content. Exploring what sort of information is encoded in representation learning methods for NLP is an important and under-researched area. For example, the tide of word-embeddings research was mostly stemmed after a thread of careful experimental results showing most embeddings to be essentially equivalent, culminating in *Improving Distributional Similarity with Lessons Learned from Word Embeddings* by Levy, Goldberg, and Dagan. As representation learning becomes even more important in NLP this sort of research will be even more important. While this paper makes a valuable contribution in setting out and exploring a methodology for evaluating sentence embeddings, the evaluations themselves are quite simple and do not necessarily correlate with real-world desiderata for sentence embeddings (as the authors note in other comments, performance on these tasks is not a normative measure of embedding quality). For example, as the authors note, the ability of the averaged vector to encode sentence length is trivially to be expected given the central limit theorem (or more accurately, concentration inequalities like Hoeffding*s inequality). The word-order experiments were interesting. A relevant citation for this sort of conditional ordering procedure is *Generating Text with Recurrent Neural Networks* by Sutskever, Martens, and Hinton, who refer to the conversion of a bag of words into a sentence as *debagging.* Although this is just a first step in better understanding of sentence embeddings, it is an important one and I recommend this paper for publication.","The reviewer explicitly states the paper makes a ""valuable contribution"" and recommends it for publication. They also use positive language such as ""thorough"" and ""interesting insights."" While the reviewer offers constructive criticism, they also acknowledge the importance of this research area. Therefore, the sentiment is positive, but not overwhelmingly so, as the reviewer also points out limitations. The language used is polite and professional throughout, with constructive criticism offered respectfully.",75,90
"Looking through the comment section here, I agree to a large degree with the author*s standpoint on many issues discussed. Points (1) through (4) in the authors comment below are, in my opinion, a good summary of the contributions of the paper. While I don*t think those contributions are groundbreaking, I believe they are significant enough to merit acceptance. The reason I am commenting here is because, having looked at several comment sections for this ICLR, I am seeing a general trend that reviews have a strong focus on performance, i.e. reviews tend to be very short and judge papers, to a large degree, on whether they are a few percentage points better or worse than the reported baseline. E.g. see the comments *the experimental evaluation is not convincing, e.g. no improvement on SVHN* or *the effect of drop-path seems to vanish with data augmentation* below. I believe that papers should be judged more on their scientific contributions (see points (1), (2) and (4) below), especially when those papers themselves state that their focus is on those scientific contributions, not on amazing performance. Further, I believe the trend to focus excessively on performance is problematic for a number of reasons: - The Deep Learning community has focused very heavily on a few datasets (MNIST, ImageNet, CIFAR-10, CIFAR-100, SVHN). This means that at any time, a large chunk of the deep learning literature is battling for 5 SOTA titles. Hence, expecting any new model to attain one of those titles is a very high bar. - It is an arbitrary standard. Say the SOTA on ImageNet improves by 2% a year. Then a paper that outperforms by 1% in 2014 would underperform by 1% in 2015. By the performance standard, the same paper with the same ideas and the same scientific merit would have declined drastically in value over that one year. Is that really true? - How does one even draw a *fair comparison* on these standard datasets at this point? The bag of tricks for neural networks includes: drop-out, l2, l1, ensembling, various forms of data augmentation, various forms of normalization and initialization, various non-linearities, various learning rate schedules, various forms of pooling, label smoothing, gradient clipping etc. etc. There are a gazillion ways to eke out fractions of percentage points of performance. And - every single paper has a unique combination of tricks that they use for their model, even though the tricks themselves are unrelated to the model. Hence, the only truly fair comparison would be to compare against every reference model with the exact trick combination that the paper presenting the reference model used, which would take an exorbitant amount of time. What*s worse, many papers do not even report all of the tricks they used. One would have to get the authors code and reverse engineer the model, not to mention slight differences introduced by using e.g. TensorFlow vs. Torch vs. Caffe. In this light, the request from one of the reviewers to have a baseline *against which the improvements can be clearly demonstrated by making isolated changes* seems unrealistic to me. - The ML community should not make excessive fine-tuning of models mandatory for publication. By requiring models to beat SOTA, we force each author to fine-tune their model ad nauseum, which leads to an arms race. To get a publications, authors would spend ever more time fine-tuning their models. This can not only lead to *training on the test set*, but also wastes the time of researcher that could be better spent exploring new ideas. - It gives too much power to bad research. In science, there is always a certain background rate of *bad* results published: either the numbers are outright fake or the experimental protocol was invalid, e.g. someone used the test set as a validation set or someone did an exorbitant number of random reruns and only published the best single result. What*s worse, these *bad* results are far more likely to hold the SOTA title at any given time than a *good* result. By requiring new publications to beat SOTA, we give too much power to bad results. - It punishes authors for reporting many or strong baselines. In this paper, authors were careful to report many recent results. Table 1 is thorough. And now they are criticized for not beating all of those baselines. I have a feeling that if the authors of this paper had been more selective about which baselines they report, i.e. those that they can beat, they would have received higher scores on the paper. I have written an in-depth review for another paper at this conference that used, in my opinion, very weak baselines and ended up getting high reviewer marks. I don*t think that was a coincidence. The same arguments apply, though I think to a lesser degree, to judging models excessively on how many parameters they have or their runtime. However, I agree with reviewers that more information about how models compare in terms of those metrics would enhance this paper. I would like to see a discussion of that in the final version. In general, I think this paper would benefit from an appendix with more details on model and training procedure. I also agree with reviewers that 80 layers, which is the deepest that authors can go while improving test error (Table 3), is not ultra-deep. Hence putting *ultra-deep* in the paper title seems exaggerated and I would recommend scaling back the language. However, I don*t think being ultra-deep (~1000 layers) is necessary, because as Veit et al showed, networks that appear ultra deep might not be ultra deep in practice. Training an 80-layer net that functions at test time without residual connections seems to be enough of an achievement. In summary, I think if a paper makes scientific contribution (see points (1), (2) and (4) below) independent of performance, then competitive performance should be enough for publication, instead of requiring SOTA. I believe this paper achieves that mark.","The reviewer expresses agreement with the authors' main points and acknowledges the significance of the paper's contributions, deeming them worthy of acceptance. While the reviewer doesn't find the contributions groundbreaking, they argue for the paper's value based on its scientific merit rather than solely on performance metrics. The reviewer's critique of the field's overemphasis on performance benchmarks further indicates a positive stance towards the paper, which aligns with their own values. The reviewer's tone remains professional and respectful throughout, engaging with the authors' arguments constructively.",60,80
"This paper focusses on attention for neural language modeling and has two major contributions: 1. Authors propose to use separate key, value, and predict vectors for attention mechanism instead of a single vector doing all the 3 functions. This is an interesting extension to standard attention mechanism which can be used in other applications as well. 2. Authors report that very short attention span is sufficient for language models (which is not very surprising) and propose an n-gram RNN which exploits this fact. The paper has novel models for neural language modeling and some interesting messages. Authors have done a thorough experimental analysis of the proposed ideas on a language modeling task and CBT task. I am convinced with authors’ responses for my pre-review questions. Minor comment: Ba et al., Reed & de Freitas, and Gulcehre et al. should be added to the related work section as well.","The review is positive overall. The reviewer finds the contributions interesting and novel, and appreciates the thorough experimental analysis. They are convinced by the authors' responses and only have a minor suggestion. The language used is neutral and professional.",75,50
"In supervised learning, a significant advance occurred when the framework of semi-supervised learning was adopted, which used the weaker approach of unsupervised learning to infer some property, such as a distance measure or a smoothness regularizer, which could then be used with a small number of labeled examples. The approach rested on the assumption of smoothness on the manifold, typically. This paper attempts to stretch this analogy to reinforcement learning, although the analogy is somewhat incoherent. Labels are not equivalent to reward functions, and positive or negative rewards do not mean the same as positive and negative labels. Still, the paper makes a worthwhile attempt to explore this notion of semi-supervised RL, which is clearly an important area that deserves more attention. The authors use the term *labeled MDP* to mean the typical MDP framework where the reward function is unknown. They use the confusing term *unlabeled MDP* to mean the situation where the reward is unknown, which is technically not an MDP (but a controlled Markov process). In the classical RL transfer learning setup, the agent is attempting to transfer learning from a source *labeled* MDP to a target *labeled* MDP (where both reward functions are known, but the learned policy is known only in the source MDP). In the semi-supervised RL setting, the target is an *unlabeled* CMP, and the source is both a *labeled* MDP and an *unlabeled* CMP. The basic approach is to use inverse RL to infer the unknown *labels* and then attempt to construct transfer. A further restriction is made to linearly solvable MDPs for technical reasons. Experiments are reported using three relatively complex domains using the Mujoco physics simulator. The work is interesting, but in the opinion of this reviewer, the work fails to provide a simple sufficiently general notion of semi-supervised RL that will be of sufficiently wide interest to the RL community. That remains to be done by a future paper, but in the interim, the work here is sufficiently interesting and the problem is certainly a worthwhile one to study.","The review starts with some criticism, pointing out that the analogy used might be incoherent and some terms are confusing. However, it highlights the importance of the research topic and acknowledges the value of the attempt. The reviewer finds the work interesting but ultimately believes it falls short of providing a widely applicable framework. Overall, the tone is balanced, suggesting a slightly positive sentiment.",20,70
"This paper provides an interesting idea to use the optimized MMD for generative model evaluation and learning. Starting from the test power, the authors justified the criterion. Moreover, they also provided an efficient implementation of perturbation tests for empirical MMD. Pros: 1) The criterion is principled which is derived from the test power. 2) The criterion can be used to detect the difference template by incorporating ARD technique. 3) By exploiting kernel in the objective, the generated algorithm, t-GMMN, training can be improved from the GMMN. Cons: 1) How to train the provided t objective is not clear. 2) The algorithm is only tested on MNIST dataset as model criticism and learning objective. Comprehensive empirical comparison to the state-of-the-art criteria, e.g., log-likelihood, and other learning objectives is missing.","The review starts with positive wording like ""interesting idea"" and ""justified criterion."" It lists both pros and cons, suggesting a balanced perspective. However, the cons point out significant limitations in training and testing, which slightly dampens the overall positivity. Therefore, the sentiment leans slightly towards positive but not overwhelmingly so. The language used is formal, objective, and devoid of any harsh or negative phrasing, indicating a polite tone.",60,80
"This work brings multiple discriminators into GAN. From the result, multiple discriminators is useful for stabilizing. The main problem of stabilizing seems is from gradient signal from discriminator, the authors motivation is using multiple discriminators to reduce this effect. I think this work indicates the direction is promising, however I think the authors may consider to add more result vs approach which enforce discriminator gradient, such as GAN with DAE (Improving Generative Adversarial Networks with Denoising Feature Matching), to show advantages of multiple discriminators.","The reviewer finds the work promising (""this work indicates the direction is promising"") but suggests improvements, indicating a generally positive but not overwhelmingly enthusiastic sentiment. The language used is constructive and professional, suggesting a neutral to polite tone.",60,50
"On the plus side, the paper proposes a mathematically interesting model for a context of a word (i.e., a Grassmanian manifold). On the minus side, the paper mostly ignores the long history of Word Sense Induction (WSI) and Word Sense Disambiguation (WSD), citing and comparing only some relatively recent papers. The experiments in this paper done on SemEval-2010 are not very persuasive. (It*s difficult to evaluate the experiments done on the 2016 data, since they are not directly comparable to published results). For example, going back to the SemEval-2010 WSI task in [1], the best system seems to be UoY [2]. The F-measure seems to be a poor metric: always assigning one sense to every word (*MFS*) yields the highest F-measure of 63.5%. The paper*s result with *2 clusters* (with an average of about 1.9) seems to be close to MFS. So I don*t think we can use F-measure to compare. The V-measure seems to be tilted towards systems that have high number of senses per word. UoY has V=15.7%, while the paper (with *5 clusters*) has 14.4%. That isn*t very convincing that the proposed method has captured the geometry of polysemy. In general, I have often wondered why people work on pure unsupervised WSI and WSD. The assessment is very difficult (as described above). More importantly, some very weakly supervised systems (with minimal labels) can work pretty well to bootstrap. See, e.g., the classic paper [3]. If the authors used the Grassmannian idea to solve higher-level NLP problems directly (such as analogies), that would be very persuasive. However, that*s a very different paper than what was submitted. For an example of application of Grassmannian manifolds to analogies, see [4]. References: 1. Manandhar, Suresh, et al. *SemEval-2010 task 14: Word sense induction & disambiguation.* Proceedings of the 5th international workshop on semantic evaluation. Association for Computational Linguistics, 2010. 2. Korkontzelos, Ioannis, and Suresh Manandhar. *Uoy: Graphs of unambiguous vertices for word sense induction and disambiguation.* Proceedings of the 5th international workshop on semantic evaluation. Association for Computational Linguistics, 2010. 3. Yarowsky, David. *Unsupervised word sense disambiguation rivaling supervised methods.* Proceedings of the 33rd annual meeting on Association for Computational Linguistics. Association for Computational Linguistics, 1995. 4. Mahadevan, Sridhar, and Sarath Chandar Reasoning about Linguistic Regularities in Word Embeddings using Matrix Manifolds https://arxiv.org/abs/1507.07636","The reviewer acknowledges some positive aspects of the paper, such as the mathematical model's interest. However, the review primarily criticizes the paper for overlooking existing research, using unconvincing experiments, and questioning the overall relevance of the work. The reviewer suggests a different application of the core idea, indicating a lack of enthusiasm for the presented research.",-30,50
"Summary: The paper presents low-rank bilinear pooling that uses Hadamard product (commonly known as element-wise multiplication). The paper implements low-rank bilinear pooling on an existing model (Kim et al., 2016b) and builds a model for Visual Question Answering (VQA) that outperforms the current state-of-art by 0.42%. The paper presents various ablation studies of the new VQA model they built. Strengths: 1. The paper presents new insights into element-wise multiplication operation which has been previously used in VQA literature (such as Antol et al., ICCV 2015) without insights on why it should work. 2. The paper presents a new model for the task of VQA that beats the current state-of-art by 0.42%. However, I have concerns about the statistical significance of the performance (see weaknesses below). 3. The various design choices made in model development have been experimentally verified. Weaknesses/Suggestions: 1. When authors explicitly (keeping rest of the model architecture same) compared low-rank bilinear pooling with compact bilinear pooling, they found that low-rank bilinear pooling performs worse. Hence, it could not be experimentally verified that low-rank bilinear pooling is better in performance than compact bilinear pooling (at least for the task of VQA). 2. The authors argue that low-rank bilinear pooling uses 25% less parameters than compact bilinear pooling. So, could the authors please explain how does the reduction in number of parameters help experimentally? Does the training time of the model reduce significantly? Can we train the model with less data? 3. One of the contributions of the paper is that the proposed model outperforms the current state-of-art on VQA by 0.42%. However, I am skeptical that the performance of the proposed model is statistically significantly better than the current state-of-art. 4. I would like the authors to explicitly mention the differences between MRN, MARN and MLB. It is not very clear from reading the paper. 5. In the caption for Table 1, fix the following: “have not” -> “have no” Review Summary: I like the insights about low-rank bilinear pooling using Hadamard product (element-wise multiplication) presented in the paper. However, it could not be justified that low-rank bilinear pooling leads to better performance than compact biliear pooling. It does lead to reduction in number of parameters but it is not clear how much that helps experimentally. So, to be more convinced I would like the authors to provide experimental justification of why low-rank bilinear pooling is better than other forms of pooling.","The reviewer acknowledges the strengths of the paper, such as new insights into element-wise multiplication and a new VQA model that outperforms the state-of-the-art. However, they also express concerns about the statistical significance of the performance improvement and the lack of experimental justification for the claimed benefits of low-rank bilinear pooling. The reviewer's tone is generally neutral, but they do point out weaknesses and request clarifications. Overall, the review leans slightly towards the positive side due to the acknowledgment of the paper's contributions.",20,70
"This paper proposes a new multiscale recurrent neural network, where each layer has different time scale, and the scale is not fixed but variable and determined by a neural network. The method is elegantly formulated within a recurrent neural network framework, and shows the state-of-the-art performance on several benchmarks. The paper is well written. Question) Can you extend it to bidirectional RNN?","The review highlights several positive aspects: ""elegantly formulated,"" ""state-of-the-art performance,"" and ""well written."" This suggests a positive sentiment. The single question posed is constructive and aims to explore potential extensions of the work, indicating a polite and engaged tone.",75,80
"Thank you for an interesting angle on highway and residual networks. This paper shows a new angle to how and what kind of representations are learnt at each layer in the aforementioned models. Due to residual information being provided at a periodic number of steps, each of the layers preserve feature identity which prevents lesioning unlike convolutional neural nets. Pros - the iterative unrolling view was extremely simple and intuitive, which was supported by theoretical results and reasonable assumptions. - Figure 3 gave a clear visualization for the iterative unrolling view Cons - Even though, the perspective is interesting few empirical results were shown to support the argument. The major experiments are image classification and language models trained on mutations of character-aware neural language models. - Figure 4 and 5 could be combined and enlarged to show the effects of batch normalization.","The review starts with positive remarks, highlighting the interesting angle and clarity of the paper. However, it also points out a significant drawback: the lack of empirical evidence. While the reviewer acknowledges the interesting perspective, the absence of strong empirical support dampens the overall enthusiasm. The language used is professional and constructive throughout, suggesting areas for improvement without resorting to harsh or disrespectful tones.",50,80
"Use of ML in ITP is an interesting direction of research. Authors consider the problem of predicting whether a given statement would be useful in a proof of a conjecture or not. This is posed as a binary classification task and authors propose a dataset and some deep learning based baselines. I am not an expert on ITP or theorem proving, so I will present a review from more of a ML perspective. I feel one of the goals of the paper should be to present the problem to a ML audience in a way that is easy for them to grasp. While most of the paper is well written, there are some sections that are not clear (especially section 2): - Terms such as LCF, OCaml-top level, deBruijn indices have been used without explaining or any references. These terms might be trivial in ITP literature, but were hard for me to follow. - Section 2 describes how the data was splits into train and test set. One thing which is unclear is – can the examples in the train and test set be statements about the same conjecture or are they always statements about different conjectures? It also unclear how the deep learning models are applied. Let’s consider the leftmost architecture in Figure 1. Each character is embedded into 256-D vector – and processed until the global max-pooling layer. Does this layer take a max along each feature and across all characters in the input? My another concern is only deep learning methods are presented as baselines. It would be great to compare with standard NLP techniques such as Bag of Words followed by SVM. I am sure these would be outperformed by neural networks, but the numbers would give a sense of how easy/hard the current problem setup is. Did the authors look at the success and failure cases of the algorithm? Are there any insights that can be drawn from such analysis that can inform design of future models? Overall I think the research direction of using ML for theorem proving is an interesting one. However, I also feel the paper is quite opaque. Many parts of how the data is constructed is unclear (atleast to someone with little knowledge in ITPs). If authors can revise the text to make it clearer – it would be great. The baseline models seem to perform quite well, however there are no insights into what kind of ability the models are lacking. Authors mention that they are unable to perform logical reasoning – but that’s a very vague statement. Some examples of mistakes might help make the message clearer. Further, since I am not well versed with the ITP literature it’s not possible for me to judge how valuable is this dataset. From the references, it seems like it’s drawn from a set of benchmark conjectures/proofs used in the ITP community – so its possibly a good dataset. My current rating is a weak reject, but if the authors address my concerns I would change to an accept.","The reviewer sees the potential in the research direction and acknowledges the value of the dataset and baselines. However, they express concerns about the clarity of the paper, particularly for readers unfamiliar with ITP. The reviewer finds the lack of insights into the models' limitations and the absence of comparisons with simpler methods as weaknesses. The language used is polite and suggests concrete ways to improve the paper, indicating a willingness to reconsider their rating upon addressing the concerns.",30,70
"A well known limitation in deep neural networks is that the same parameters are typically used for all examples, even though different examples have very different characteristics. For example, recognizing animals will likely require different features than categorizing flowers. Using different parameters for different types of examples has the potential to greatly reduce underfitting. This can be seen in recent results with generative models, where image quality is much better for less diverse datasets. However, it is difficult to use different parameters for different examples because we typically train using minibatches, which relies on using the same parameters for all examples in a minibatch (i.e. doing matrix multiplies in a fully-connected network). The hypernetworks paper cleverly proposes to get around this problem by adapting different *parameters* for different time steps in recurrent networks and different. The basic insight is that a minibatch will always include many different examples from the same time step or spatial position, so there is no computational issue involved with using different *parameters*. In this paper, the *parameters* are modified for different positions based on the output from a hypernetwork which conditions on the time step. Hypothetically, this hypernetwork could also condition on other features that are shared by all sequences in the minibatch. I expect this method to become standard for training RNNs, especially where the length of the sequences is the same during the training and testing phases. Penn Treebank is a highly competitive baseline, so the SOTA result reported here is impressive. The experiments on convolutional networks are less experimentally impressive. I suspect that the authors were aiming to achieve state of the art results here but settled with achieving a reduction in the number of parameters. It might even be worthwhile to consider a synthetic experiment where two completely different types of image are appended (i.e. birds on the left and flowers on the right) and show that the hypernetwork helps in this situation. It may be the case that for convnets, the cases where hypernetworks help are very specific. For RNNs, it seems to be the case that explicitly changing the nature of the computation depending on the position in the sequence greatly improves generalization. While a usual RNN could learn to store a counter (indicating the position in the sequence), the hypernetwork could be a more efficient way to add capacity. Applications to time series forecasting and modeling could be an interesting area for future work.","The review starts by acknowledging the importance of the problem being addressed and praises the cleverness of the proposed solution. The reviewer finds the results on Penn Treebank impressive, indicating a positive sentiment. While the reviewer is less impressed with the convolutional network experiments, they offer constructive suggestions for improvement. The reviewer concludes by suggesting interesting future work, further indicating a positive outlook on the paper.",75,80
"This was an interesting paper. The algorithm seems clear, the problem well-recognized, and the results are both strong and plausible. Approaches to hyperparameter optimization based on SMBO have struggled to make good use of convergence during training, and this paper presents a fresh look at a non-SMBO alternative (at least I thought it did, until one of the other reviewers pointed out how much overlap there is with the previously published successive halving algorithm - too bad!). Still, I*m excited to try it. I*m cautiously optimistic that this simple alternative to SMBO may be the first advance to model search for the skeptical practitioner since the case for random search > grid search (http://www.jmlr.org/papers/v13/bergstra12a.html, which this paper should probably cite in connection with their random search baseline.) I would suggest that the authors remove the (incorrect?) claim that this algorithm is *embarrassingly parallel* as it seems that there are number of synchronization barriers at which state must be shared in order to make the go-no-go decisions on whatever training runs are still in progress. As the authors themselves point out as future work, there are interesting questions around how to adapt this algorithm to make optimal use of a cluster (I*m optimistic that it should carry over, but it*s not trivial). For future work, the authors might be interested in Hutter et al*s work on Bayesian Optimization With Censored Response Data (https://arxiv.org/abs/1310.1947) for some ideas about how to use the dis-continued runs.","The reviewer expresses interest and excitement about the paper's approach, finding the results strong and plausible. They see it as a potential advancement in the field. While they point out a flaw (overlap with an existing algorithm), their overall tone remains positive, suggesting optimism about the algorithm's potential. The language used is constructive and encouraging, even when pointing out areas for improvement. There are no personal attacks or disrespectful remarks.",75,80
"This paper provides some theoretical guarantees for the identity parameterization by showing that 1) arbitrarily deep linear residual networks have no spurious local optima; and 2) residual networks with ReLu activations have universal finite-sample expressivity. This paper is well written and studied a fundamental problem in deep neural network. I am very positive on this paper overall and feel that this result is quite significant by essentially showing the stability of auto-encoder, given the fact that it is hard to provide concrete theoretical guarantees for deep neural networks. One of key questions is how to extent the result in this paper to the more general nonlinear actuation function case. Minors: one line before Eq. (3.1), U in R ? 	imes k","The reviewer explicitly states positive sentiment with ""I am very positive on this paper overall"" and ""this result is quite significant."" They also highlight the paper's strengths, such as being ""well written"" and addressing a ""fundamental problem."" The single minor comment is presented constructively and does not detract from the overall positive tone. Therefore, the sentiment is highly positive, and the language is polite and constructive. ",90,90
"This paper is well written, and well presented. This method is using denoise autoencoder to learn an implicit probability distribution helps reduce training difficulty, which is neat. In my view, joint training with an auto-encoder is providing extra auxiliary gradient information to improve generator. Providing auxiliary information may be a methodology to improve GAN. Extra comment: Please add more discussion with EBGAN in next version.","The review starts with positive statements like ""well written, and well presented"" and ""neat"". It also suggests improvements in a constructive manner using phrases like ""Please add"". Overall, the tone is positive and encouraging. ",75,100
"The authors present a simple method to affix a cache to neural language models, which provides in effect a copying mechanism from recently used words. Unlike much related work in neural networks with copying mechanisms, this mechanism need not be trained with long-term backpropagation, which makes it efficient and scalable to much larger cache sizes. They demonstrate good improvements on language modeling by adding this cache to RNN baselines. The main contribution of this paper is the observation that simply using the hidden states h_i as keys for words x_i, and h_t as the query vector, naturally gives a lookup mechanism that works fine without tuning by backprop. This is a simple observation and might already exist as folk knowledge among some people, but it has nice implications for scalability and the experiments are convincing. The basic idea of repurposing locally-learned representations for large-scale attention where backprop would normally be prohibitively expensive is an interesting one, and could probably be used to improve other types of memory networks. My main criticism of this work is its simplicity and incrementality when compared to previously existing literature. As a simple modification of existing NLP models, but with good empirical success, simplicity and practicality, it is probably more suitable for an NLP-specific conference. However, I think that approaches that distill recent work into a simple, efficient, applicable form should be rewarded and that this tool will be useful to a large enough portion of the ICLR community to recommend its publication.","The review acknowledges the simplicity of the method but highlights its effectiveness and scalability as strong points. The reviewer sees value in the contribution, particularly in its potential for broader application. While there's a suggestion that the work might be better suited for a more specialized conference, the overall tone is positive, recognizing the practical benefits and recommending publication.",75,80
"This paper proposes a novel exploration strategy that promotes exploration of under-appreciated reward regions. Proposed importance sampling based approach is a simple modification to REINFORCE and experiments in several algorithmic toy tasks show that the proposed model is performing better than REINFORCE and Q-learning. This paper shows promising results in automated algorithm discovery using reinforcement learning. However it is not very clear what is the main motivation of the paper. Is the main motivation better exploration for policy gradient methods? If so, authors should have benchmarked their algorithm with standard reinforcement learning tasks. While there is a huge body of literature on improving REINFORCE, authors have considered a simple version of REINFORCE on a non-standard task and say that UREX is better. If the main motivation is improving the performance in algorithm learning tasks, then the baselines are still weak. Authors should make it clear which is the main motivation. Also the action space is too small. In the beginning authors raise the concern that entropy regularization might not scale to larger action spaces. So a comparison of MENT and UREX in a large action space problem would give more insights on whether UREX is not affected by large action space. -------------------------- After rebuttal: I missed the action sequences argument when I pointed about small action space issue. For question regarding weak baseline, there are several tricks used in the literature to tackle high-variance issue for REINFORCE. For example, see Mnih & Gregor, 2014. I have increased my rating from 6 to 7. I still encourage the authors to improve their baseline.","The reviewer acknowledges the promising results and the paper's contribution to automated algorithm discovery. However, they express concerns about the clarity of the paper's main motivation and the strength of the baselines used. The reviewer suggests comparisons with standard reinforcement learning tasks and improvements to the baseline. The language used is critical but professional and provides constructive feedback. The reviewer also acknowledges the rebuttal and increases the rating, indicating a slightly more positive sentiment after clarification.",40,60
"The paper introduces a variation to the CNN-based texture synthesis procedure of Gatys et al. that matches correlations between spatially shifted feature responses in addition to the correlations between feature responses at the same position in the feature maps. The paper claims that this a) improves texture synthesis for textures with long-range regular structures, that are not preserved with the Gatys et al. method b) improves performance on texture inpainting tasks compared to the Gatys et al. method c) improves results in season transfer when combined with the style transfer method by Gatys et al. Furthermore the paper shows that d) by matching correlations between spatially flipped feature maps, symmetry properties around the flipping axis can be preserved. I agree with claim a). However, the generated textures still have some issues such as greyish regions so the problem is not solved. Additionally, the procedure proposed is very costly which makes an already slow texture synthesis method substantially slower. For example, in comparison, the concurrent work by Liu et al. (http://arxiv.org/abs/1605.01141) that tackles the same problem by adding constraints to the Fourier spectrum of the synthesised texture shows comparable or better results while being far more efficient. Also with b) the presented results constitute an improvement over the Gatys et al. method but again the results are not too exciting - one would not prefer this model to other inpainting algorithms. With c) I don’t see a clear advantage of the proposed method to the existing Gatys et al. algorithm. Finally, d) is a neat idea and the initial results look interesting but they don’t go much further than that. All in all I think it is decent work but neither its originality and technical complexity nor the quality of the results are convincing enough for acceptance. That said, I could imagine this to be a nice contribution to the workshop track though.","The reviewer acknowledges the paper's contributions and agrees with some of its claims but finds the improvements to be somewhat marginal ('decent work but...'). The reviewer also points out limitations and compares the paper to other work, suggesting it might be more suitable for a workshop rather than a main conference. Overall, this suggests a lukewarm reception leaning towards the negative.",-20,50
"There is a great deal of ongoing interest in compressing neural network models. One line of work has focused on using low-precision representations of the model weights, even down to 1 or 2 bits. However, so far these approaches have been accompanied by a significant impact on accuracy. The paper proposes an iterative quantization scheme, in which the network weights are quantized in stages---the largest weights (in absolute value) are quantized and fixed, while unquantized weights can adapt to compensate for any resulting error. The experimental results show this is extremely effective, yielding models with 4 bit or 3 bit weights with essentially no reduction in accuracy. While at 2 bits the accuracy decreases slightly, the results are substantially better than those achieved with other quantization approaches. Overall this paper is clear, the technique is as far as I am aware novel, the experiments are thorough and the results are very compelling, so I recommend acceptance. The paper could use another second pass for writing style and grammar. Also, the description of the pruning-inspired partitioning strategy could be clarified somewhat... e.g., the chosen splitting ratio of 50% only seems to be referenced in a figure caption and not the main text.","The review is overwhelmingly positive. The reviewer highlights the novelty and effectiveness of the proposed method, praises the clarity, thoroughness, and compelling results. The suggestions for improvement are minor, focusing on writing style and clarification, indicating a positive disposition towards the paper. The language used is constructive and professional throughout.",90,85
"This paper investigates the fact why deep networks perform well in practice and how modifying the geometry of pooling can make the polynomially sized deep network to provide a function with exponentially high separation rank (for certain partitioning.) In the authors* previous works, they showed the superiority of deep networks over shallows when the activation function is ReLu and the pooling is max/mean pooling but in the current paper there is no activation function after conv and the pooling is just a multiplication of the node values. Although for the experimental results they*ve considered both scenarios. Actually, the general reasoning for this problem is hard, therefore, this drawback is not significant and the current contribution adds a reasonable amount of knowledge to the literature. This paper studies the convolutional arithmetic circuits and shows how this model can address the inductive biases and how pooling can adjust these biases. This interesting contribution gives an intuition about how deep network can capture the correlation between the input variables when its size is polynomial but and correlation is exponential. It worth to note that although the authors tried to express their notation and definitions carefully where they were very successful, it would be helpful if they elaborate a bit more on their definitions, expressions, and conclusions in the sense to make them more accessible.","The review acknowledges the paper's valuable contribution to understanding deep network performance, particularly highlighting its insights into inductive biases and correlation capture. While it points out a drawback regarding the absence of an activation function, it doesn't consider it a major flaw. The reviewer suggests minor improvements in elaborating definitions and expressions for better clarity. Overall, the tone is constructive and appreciative.",60,70
"In this paper, the authors use a separate introspection neural network to predict the future value of the weights directly from their past history. The introspection network is trained on the parameter progressions collected from training separate set of meta learning models using a typical optimizer, e.g. SGD. Pros: + The organization is generally very clear + Novel meta-learning approach that is different than the previous learning to learn approach Cons: - The paper will benefit from more thorough experiments on other neural network architectures where the geometry of the parameter space are sufficiently different than CNNs such as fully connected and recurrent neural networks. - Neither MNIST nor CIFAR experimental section explained the architectural details - Mini-batch size for the experiments were not included in the paper - Comparison with different baseline optimizer such as Adam would be a strong addition or at least explain how the hyper-parameters, such as learning rate and momentum, are chosen for the baseline SGD method. Overall, due to the omission of the experimental details in the current revision, it is hard to draw any conclusive insight about the proposed method.","The reviewer acknowledges the novelty of the approach and clarity of the paper. However, they also point out several significant shortcomings related to experimental details and comparisons. The lack of these details prevents the reviewer from fully assessing the proposed method, making the overall sentiment rather critical.",-20,50
"The paper proposes a model for image generation where the back-ground is generated first and then the foreground is pasted in by generating first a foregound mask and corresponding appearance, curving the appearance image using the mask and transforming the mask using predicted affine transform to paste it on top of the image. Using AMTurkers the authors verify their generated images are selected 68% of the time as being more naturally looking than corresponding images from a DC-GAN model that does not use a figure-ground aware image generator. The segmentations masks learn to depict objects in very constrained datasets (birds) only, thus the method appears limited for general shape datasets, as the authors also argue in the paper. Yet, the architectural contributions have potential merit. It would be nice to see if multiple layers of foreground (occluding foregrounds) are ever generated with this layered model or it is just figure-ground aware.","The review is cautiously positive. It highlights both the strengths and limitations of the paper. For example, it acknowledges the potential merit of the architectural contributions but also points out the limited applicability to general shape datasets. The language is neutral, without strong positive or negative connotations.",50,50
"This paper proposes to learn decomposition of sequences (such as words) for speech recognition. It addresses an important issue and I forsee it being useful for other applications such as machine translation. While the approach is novel and well-motivated, I would very much like to see a comparison against byte pair encoding (BPE). BPE is a very natural (and important) baseline (i.e. dynamic vs fixed decomposition). The BPE performance should be obtained for various BPE vocab sizes. Minor points - Did the learned decompositions correspond to phonetically meaningful units? From the example in the appendix it*s hard to tell if the model is learning phonemes or just most frequent character n-grams. - Any thoughts on applications outside of speech recognition? If this is shown to be effective in other domains it would be a really strong contribution (but this is probably outside the scope for now).","The reviewer finds the paper's topic important and the approach novel and well-motivated, which points towards a positive sentiment. However, they also demand a comparison with another method (BPE) which suggests there are improvements to be made. They also have minor questions and suggestions, but these are presented constructively. Overall, the tone is polite and professional, without any harsh language or dismissive remarks.",60,80
"The paper addresses the problem of predicting learning curves. The key difference from prior work is that (1) the authors learn a neural network that generalizes across hyperparameter settings and (2) the authors use a Bayesian neural network with SGHMC. The authors demonstrate that the proposed approach is effective on extrapolating partially observed curves as well as predicting unobserved learning curves on various architectures (FC, CNN, LR and VAE). This seems very promising for Bayesian optimization, I*d love to see an experiment that evaluates the relative advantage of this proposed method :) Have you thought about ways to handle learning rate decays? Perhaps you could run the algorithm on a random subset of data and extrapolate from that? I was thinking of other evaluation measures in addition to MSE and LL. In practice, we care about the most promising run. Would it make sense to evaluate how accurately each method identified the best run? Minor comments: Fonts are too small and almost illegible on my hard copy. Please increase the font size for legends and axes in the figures. Fig 6: not all figures seem to have six lines. Are the lines overlapping in some cases?","The review starts with positive statements, highlighting the novelty and promising results. The reviewer expresses interest and suggests further experiments. While there are improvement suggestions, they are constructive and framed as questions/suggestions rather than harsh criticism. The tone remains encouraging throughout.",75,80
"This paper introduces MusicNet, a new dataset. Application of ML techniques to music have been limited due to scarcity of exactly the kind of data that is provided here: meticulously annotated, carefully verified and organized, containing enough *hours* of music, and where genre has been well constrained in order to allow for sufficient homogeneity in the data to help ensure usefulness. This is great for the community. The description of the validation of the dataset is interesting, and indicates a careful process was followed. The authors provide just enough basic experiments to show that this dataset is big enough that good low-level features (i.e. expected sinusoidal variations) can indeed be learned in an end-to-end context. One might argue that in terms of learning representations, the work presented here contributes more in the dataset than in the experiments or techniques used. However, given the challenges of acquiring good datasets, and given the essential role such datasets play for the community in moving research forward and providing baseline reference points, I feel that this contribution carries substantial weight in terms of expected future rewards. (If research groups were making great new datasets available on a regular basis, that would place this in a different context. But so far, that is not the case.) In otherwords, while the experiments/techniques are not necessarily in the top 50% of accepted papers (per the review criteria), I am guessing that the dataset is in the top 15% or better.","The review is overwhelmingly positive. The reviewer highlights the significance of the dataset, describing it as ""great for the community"" and acknowledging the challenges in creating such a resource. While the reviewer notes the experiments are basic, they emphasize the dataset's importance outweighs this, placing it potentially in the ""top 15% or better."" The language used is highly positive and encouraging, with no negative phrasing or criticisms.",90,100
"This paper presents an approach for skills transfer from one task to another in a control setting (trained by RL) by forcing the embeddings learned on two different tasks to be close (L2 penalty). The experiments are conducted in MuJoCo, with a set of experiments being from the state of the joints/links (5.2/5.3) and a set of experiments on the pixels (5.4). They exhibit transfer from arms with different number of links, and from a torque-driven arm to a tendon-driven arm. One limitation of the paper is that the authors suppose that time alignment is trivial, because the tasks are all episodic and in the same domain. Time alignment is one form of domain adaptation / transfer that is not dealt with in the paper, that could be dealt with through subsampling, dynamic time warping, or learning a matching function (e.g. neural network). General remarks: The approach is compared to CCA, which is a relevant baseline. However, as the paper is purely experimental, another baseline (worse than CCA) would be to just have the random projections for *f* and *g* (the embedding functions on the two domains), to check that the bad performance of the *no transfer* version of the model is due to over-specialisation of these embeddings. I would also add (for information) that the problem of learning invariant feature spaces is also linked to metric learning (e.g. [Xing et al. 2002]). More generally, no parallel is drawn with multi-task learning in ML. In the case of knowledge transfer (4.1.1), it may make sense to anneal alpha. The experiments feel a bit rushed. In particular, the performance of the baseline being always 0 (no transfer at all) is uninformative, at least a much bigger sample budget should be tested. Also, why does Figure 7.b contain no *CCA* nor *direct mapping* results? Another concern that I have with the experiments: (if/how) did the author control for the fact that the embeddings were trained with more iterations in the case of doing transfer? Overall, the study of transfer is most welcomed in RL. The experiments in this paper are interesting enough for publication, but the paper could have been more thorough.","The review is mostly positive, acknowledging the relevance of the paper's topic and the interesting results. The reviewer finds merit in the paper's contribution to the field of transfer learning in RL. However, the review also points out several areas for improvement, suggesting a need for more thorough experiments and comparisons. The language used is constructive and objective, typical of academic peer reviews.",60,70
"While my above review title is too verbose, it would be a more accurate title for the paper than the current one (an overall better title would probably be somewhere in between). The overall approach is interesting: all three of the key techniques (aux. tasks, skip/diagonal connections, and the use of internal labels for the kind of data available) make a lot of sense. I found some of the results hard to understand/interpret. Some of the explanation in the discussion below has been helpful (e.g. see my earlier questions about Fig 4 and 5); the paper would benefit from including more such explanations. It may be worthwhile very briefly mentioning the relationship of *diagonal* connections to other emerging terms for similar ideas (e.g. skip connections, etc). *Skip* seems to me to be accurate regardless of how you draw the network, whereas *diagonal* only makes sense for certain visual layouts. In response to comment in the discussion below: *leading to less over-segmentation of action bouts* (and corresponding discussion in section 5.1 of the paper): I would be like to have a bit more about this in the paper. I have assumed that *per-bout* refers to *per-action event*, but now I am not certain that I have understood this correctly (i.e. can a *bout* last for a few minutes?): given the readership, I think it would not be inappropriate to define some of these things explicitly. In response to comment about fly behaviours that last minutes vs milliseconds: This is interesting, and I would be curious to know how classification accuracy relates to the time-scale of the behaviour (e.g. are most of the mistakes on long-term behaviours? i realize that this would only tell part of the story, e.g. if you have a behaviour that has both a long-term duration, but that also has very different short-term characteristics than many other behaviours, it should be easy to classify accuractely despite being *long-term*). If easy to investigate this, I would add a comment about it; if this is hard to investigate, it*s probably not worth it at this point, although it*s something you might want to look at in future. In response to comment about scaling to human behavior: I agree that in principle, adding conv layers directly above the sensory input would be the right thing to try, but seriously: there is usually a pretty big gap between what *should* work and what actually works, as I am sure the authors are aware. (Indeed, I am sure the authors have a much more experiential and detailed understanding of the limitations of their work than I do). What I see presented is a nice system that has been demonstrated to handle spatiotemporal trajectories. The claims made should correspond to this. I would consider adjusting my rating to a 7 depending on future revisions.","The review is generally positive. The reviewer finds the approach interesting and sees merit in the work. While they point out areas for improvement and clarification, they also acknowledge the strengths and potential impact. The language is constructive and professional throughout.",60,80
"The paper proposes a novel approach for learning visual servoing based on Q-iteration. The main contributions of the paper are: 1. Bilinear dynamics model for predicting next frame (features) based on action and current frame 2. Formulation of servoing with a Q-function that learns weights for different feature channels 3. An elegant method for optimizing the Bellman error to learn the Q-function Pros: + The paper does a good job of exploring different ways to connect the action (u_t) and frame representation (y_t) to predict next frame features (y_{t+1}). They argue in favour of a locally connected bilinear model which strikes the balance between computation and expressive ability. Cons: - While, sec. 4 makes good arguments for different choices, I would have liked to see more experimental results comparing the 3 approaches: fully connected, convolutional and locally connected dynamics. Pros: + The idea of weighting different channels to capture the importance of obejcts in different channels seems more effective than treating errors across all channels equally. This is also validated experimentally, where unweighted performance suffers consistently. + Solving the Bellman error is a difficult problem in Q-learning approaches. The current paper presents a solid optimization scheme based on the key-observation that scaling Q-function parameters does not affect the best policy chosen. This enables a more elegant FQI approach as opposed to typical optimization schemes which (c_t + gamma min_u Q_{t+1}) fixed. Cons: - However, I would have liked to see the difference between FQI and such an iterative approach which holds the second term in Eq. 5 fixed. Experimental results: - Overall, I find the experimental results unsatisfying given the small scale and toy simulations. However, the lack of benchmarks in this domain needs to be recognized. - Also, as pointed out in pre-review section, the idea of modifying the VGG needs to be experimentally validated. In its current form, it is not clear whether the modified VGG would perform better than the original version. Overall, the contribution of the paper is solid in terms of technical novelty and problem formulations. However, the paper could use stronger experiments as suggested to earlier to bolster its claims.","The reviewer highlights both positive and negative aspects of the paper. They praise the novelty, the exploration of different approaches, and the elegant optimization scheme. However, they also express concerns about the lack of experimental validation for some claims and the limited scale of the experiments. Overall, the tone is constructive and suggests improvements, indicating a positive sentiment without being overly enthusiastic. The language used is formal, respectful, and typical of academic peer reviews, suggesting a neutral politeness level.",60,0
"The paper presents an end-to-end neural network model for the problem of designing natural language interfaces for database queries. The proposed approach uses only weak supervision signals to learn the parameters of the model. Unlike in traditional approaches, where the problem is solved by semantically parsing a natural language query into logical forms and executing those logical forms over the given data base, the proposed approach trains a neural network in an end-to-end manner which goes directly from the natural language query to the final answer obtained by processing the data base. This is achieved by formulating a collection of operations to be performed over the data base as continuous operations, the distributions over which is learnt using the now-standard soft attention mechanisms. The model is validated on the smallish WikiTableQuestions dataset, where the authors show that a single model performs worse than the approach which uses the traditional Semantic Parsing technique. However an ensemble of 15 models (trained in a variety of ways) results in comparable performance to the state of the art. I feel that the paper proposes an interesting solution to the hard problem of learning natural language interfaces for data bases. The model is an extension of the previously proposed models of Neelakantan 2016. The experimental section is rather weak though. The authors only show their model work on a single smallish dataset. Would love to see more ablation studies of their model and comparison against fancier version of memnns (i do not buy their initial response to not testing against memory networks). I do have a few objections though. -- The details of the model are rather convoluted and the Section 2.1 is not very clearly written. In particular with the absence of the accompanying code the model will be super hard to replicate. I wish the authors do a better job in explaining the details as to how exactly the discrete operations are modeled, what is the role of the *row selector*, the *scalar answer* and the *lookup answer* etc. -- The authors do a full attention over the entire database. Do they think this approach would scale when the data bases are huge (millions of rows)? Wish they experimented with larger datasets as well.","The reviewer finds the paper interesting and believes it proposes a novel solution to a complex problem. They praise the model as an extension of previous work. However, they also point out weaknesses in the experimental section, such as the use of only one small dataset and the lack of ablation studies. The reviewer also criticizes the clarity of the model description and questions the scalability of the approach. While the reviewer has some concerns, their overall impression seems positive due to the novelty and potential of the proposed approach.",60,50
"This paper introduces an approach for model-based control of stochastic dynamical systems with policy search, based on (1) learning the stochastic dynamics of the underlying system with a Bayesian deep neural network (BNN) that allows some of its inputs to be stochastic, and (2) a policy optimization method based on simulated rollouts from the learned dynamics. BNN training is carried out using alpha-divergence minimization, the specific form of which was introduced in previous work by the authors. Validation and comparison of the approach is undertaken on a simulated domain, as well as real-world scenarios. The paper is tightly written, and easy to follow. Its approach to fitting Bayesian neural networks with alpha divergence is interesting and appears novel in this context. The resulting application to model-based control appears to have significant practical impact, particularly in light of the explainability that a system model can bring to specific decisions made by the policy. As such, I think that the paper brings a valuable contribution to the literature. That said, I have a few questions and suggestions: 1) In section 2.2, it should be explained how the random z_n input is used by the neural network: is it just concatenated to the other inputs and used as-is, or is there a special treatment? 2) Moreover, much case is made for the need to have stochastic inputs, but only a scalar input seems to be provided throughout. Is this enough? How computationally difficult would providing stochastic inputs of higher dimensionality be? 3) How important is the normality assumption in z_n? How is the variance gamma established? 4) It is mentioned that the hidden layers of the neural network are made of rectifiers, but no further utilization of this fact is made in the paper. Is this assumption somehow important in the optimization of the alpha-divergence (beyond what we know about rectifiers to mitigate the vanishing gradient problem) ? 5) Equation (3), denominator mathbf{y} should be mathbf{Y} ? 6) Section 2.3: it would be helpful to have an overview or discussion of the computational complexity of training BNNs, to understand whether and when they can practicably be used. 7) Between eq (12) and (13), a citation to the statement of the time embedding theorem would be helpful, as well as an indication of how the embedding dimension should be chosen. 8) Figure 1: the subplots should have the letters by which they are referenced in the text on p. 7. 9) In section 4.2.1, it is not clear if the gas turbine data is publicly available, and if so where. In addition more details should be provided, such as the dimensionality of the variables E_t, N_t and A_t. 10) Perhaps the comparisons with Gaussian processes should include variants that support stochastic inputs, such as Girard et al. (2003), to provide some of the same modelling capabilities as what’s made use of here. At least, this strand of work should be mentioned in Section 5. References: Girard, A., Rasmussen, C. E., Quiñonero Candela, J., & Murray Smith, R. (2003). Gaussian process priors with uncertain inputs-application to multiple-step ahead time series forecasting. Advances in Neural Information Processing Systems, 545-552.","The reviewer provides constructive criticism and suggestions for improvement, indicating a positive view of the paper's core contributions. While they raise valid questions, these are geared towards enhancing the paper rather than dismissing it. The language is formal, respectful, and suggestive, not demanding, in tone.",75,80
"The paper introduces a simulator and a set of synthetic question answering tasks where interaction with the *teacher* via asking questions is desired. The motivation is that an intelligent agent can improve its performance by asking questions and getting corresponding feedback from users. The paper studies this problem in an offline supervised and an online reinforcement learning settings. The results show that the models improve by asking questions. -- The idea is novel, and is relatively unexplored in the research community. The paper serves as a good first step in that direction. -- The paper studies three different types of tasks where the agent can benefit from user feedback. -- The paper is well written and provides a clear and detailed description of the tasks, models and experimental settings. Other comments/questions: -- What is the motivation behind using both vanilla-MemN2N AND Cont-MemN2N? Is using both resulting in any conclusions which are adding to the paper*s contributions? -- In the Question Clarification setting, what is the distribution of misspelled words over question entity, answer entity, relation entity or none of these? If most of the misspelled words come from relation entities, it might be a much easier problem than it seems. -- The first point on Page 10 *The performance of TestModelAQ is worse than TestAQ but better than TestQA.* is not true for Task 2 from the numbers in Tables 2 and 4. -- What happens if the conversational history is smaller or none? -- Figure 5, Task 6, why does the accuracy for good student drop when it stops asking questions? It already knows the relevant facts, so asking questions is not providing any additional information to the good student. -- Figure 5, Task 2, the poor student is able to achieve almost 70% of the questions correct even without asking questions. I would expect this number to be quite low. Any explanation behind this? -- Figure 1, Task 2 AQ, last sentence should have a negative response *(-)* instead of positive as currently shown. Preliminary Evaluation: A good first step in the research direction of learning dialogue agents from unstructured user interaction.","The reviewer provides both positive feedback, such as stating the idea is novel and the paper is well-written, and constructive criticism in the form of specific questions and potential issues. The reviewer acknowledges the paper as a good first step, indicating a positive sentiment overall. The language used is formal and typical for academic peer reviews, suggesting a neutral politeness level.",60,0
"I have not much to add to my pre-review comments. It*s a very well written paper with an interesting idea. Lots of people currently want to combine RL with NLP. It is very en vogue. Nobody has gotten that to work yet in any really groundbreaking or influential way that results in actually superior performance on any highly relevant or competitive NLP task. Most people struggle with the fact that NLP requires very efficient methods on very large datasets and RL is super slow. Hence, I believe this direction hasn*t shown much promise yet and it*s not yet clear it ever will due to the slowness of RL. But many directions need to be explored and maybe eventually they will reach a point where they become relevant. It is interesting to learn the obviously inherent grammatical structure in language though sadly again, the trees here do not yet capture much of what our intuitions are. Regardless, it*s an interesting exploration, worthy of being discussed at the conference.","The reviewer finds the paper's idea interesting and acknowledges its relevance to a trending research area. They praise the writing quality. However, they express skepticism about the approach's effectiveness and long-term promise due to the inherent limitations of RL in NLP. Despite this, they recommend the paper for acceptance based on its exploratory nature and potential for discussion.",50,70
"This paper trains a generative model which transforms noise into model samples by a gradual denoising process. It is similar to a generative model based on diffusion. Unlike the diffusion approach: - It uses only a small number of denoising steps, and is thus far more computationally efficient. - Rather than consisting of a reverse trajectory, the conditional chain for the approximate posterior jumps to q(z(0) | x), and then runs in the same direction as the generative model. This allows the inference chain to behave like a perturbation around the generative model, that pulls it towards the data. (This also seems somewhat related to ladder networks.) - There is no tractable variational bound on the log likelihood. I liked the idea, and found the visual sample quality given a short chain impressive. The inpainting results were particularly nice, since one shot inpainting is not possible under most generative modeling frameworks. It would be much more convincing to have a log likelihood comparison that doesn*t depend on Parzen likelihoods. Detailed comments follow: Sec. 2: *theta(0) the* -> *theta(0) be the* *theta(t) the* -> *theta(t) be the* *what we will be using* -> *which we will be doing* I like that you infer q(z^0|x), and then run inference in the same order as the generative chain. This reminds me slightly of ladder networks. *q*. Having learned* -> *q*. [paragraph break] Having learned* Sec 3.3: *learn to inverse* -> *learn to reverse* Sec. 4: *For each experiments* -> *For each experiment* How sensitive are your results to infusion rate? Sec. 5: *appears to provide more accurate models* I don*t think you showed this -- there*s no direct comparison to the Sohl-Dickstein paper. Fig 4. -- neat!","The reviewer starts by praising the idea and results of the paper, particularly highlighting the impressive visual sample quality and inpainting results. They do express a desire for a more convincing log likelihood comparison. The remaining part of the review focuses on minor suggestions and typos, indicating a positive stance overall. ",75,90
"This paper shows that a deep RL approach augmented with auxiliary tasks improves performance on navigation in complex environments. Specifically, A3C is used for the RL problem, and the agent is simultaneously trained on an unsupervised depth prediction task and a self-supervised loop closure classification task. While the use of auxiliary tasks to improve training of models including RL agents is not new, the main contribution here is the use of tasks that encourage learning an intrinsic representation of space and movement that enables significant improvements on maze navigation tasks. The paper is well written, experiments are convincing, and the value of the auxiliary tasks for the problem are clear. However, the contribution is relatively incremental given previous work on RL for navigation and on auxiliary tasks. The work could become of greater interest provided broader analysis and insights on either optimal combinations of tasks for visual navigation (e.g. the value of other visual / geometry-based tasks), or on auxiliary tasks with RL in general. As it is, it is a useful demonstration of the benefit of geometry-based auxiliary tasks for navigation, but of relatively narrow interest.","The review is mostly positive, acknowledging the paper's clarity, convincing experiments, and the value of the auxiliary tasks. However, it also points out that the contribution is somewhat incremental and suggests avenues for broader impact. The language is constructive and professional throughout.",50,75
"This papers adds to the literature on learning optimizers/algorithms that has gained popularity recently. The authors choose to use the framework of guided policy search at the meta-level to train the optimizers. They also opt to train on random objectives and assess transfer to a few simple tasks. As pointed below, this is a useful addition. However, the argument of using RL vs gradients at the meta-level that appears below is not clear or convincing. I urge the authors to run an experiment comparing the two approaches and to present comparative results. This is a very important question, and the scalability of this approach could very well hinge on this fact. Indeed, demonstrating both scaling to large domains and transfer to those domains is the key challenge in this domain. In summary, the idea is a good one, but the experiments are weak.","The review starts with a positive note, acknowledging the relevance of the paper's topic and its contribution. However, it expresses concerns about the clarity and persuasiveness of a key argument, urging the authors to include an experiment for a stronger comparison. The reviewer also points out the limitations of the current experiments. While acknowledging the good idea, the overall tone suggests a need for significant improvement. Therefore, the sentiment leans slightly negative due to the weakness of the experiments. The language used is professional and polite, providing constructive criticism without resorting to harsh or disrespectful language.",-20,80
"This paper addresses the question of how to utilize physical interactions to answer questions about physical outcomes. This question falls into a popular stream in ML community -- understanding physics. The paper moved a step further and worked on experimental setups where there is no prior about the physical properties/rules and it uses a deep reinforcement learning (DRL) technique to address the problem. My overall opinion about this paper is: an interesting attempt and idea, yet without a clear contribution. The experimental setups are quite interesting. The goal is to figure out which blocks are heavier or which blocks are glued together -- only by pushing and pulling objects around without any prior. The paper also shows reasonable performances on each task with detailed scenarios. While these experiments and results are interesting, the contribution is unclear. My main question is: does this result bring us any new insight? While the scenarios are interesting and focused on physical experiments, this is not any more different (potentially easier) than learning from playing games (e.g. Atari). In other words, are the tasks really different from other typical popular DRL tasks? To this end, I would have been more excited if authors showed some more new insights or experiments on learned representations and etc. Currently, the paper only discusses the factual outcome. For example, it describes the experimental setup and how much performances an agent could achieve. The authors could probably dissect the learned representations further, or discuss how the experimental results are linked to the human behavior or physical properties/laws. I am very in-between for my overall rating. I think the paper could have a deeper analysis. I however recommend the acceptance because of its merit of the idea. The followings are some detailed questions (not directly impacting my overall rating): (1) Page 2 *we assume that the agent has no prior knowledge about the physical properties of objects, or the laws of physics, and hence must interact with the objects in order to learn to answer questions about these properties.*: why does one *must* interact with objects in order to learn about the properties? Can*t we also learn through observation? (2) Figure 1right is missing a Y-axis label. (3) Page 3: A relating to bandit is interesting, but the formal approach is all based on DRL. (4) Page 5 *which makes distinguishing between the two heaviest blocks very difficult*: I am a bit confused why having a small mass gap makes the task harder (unless it*s really close to 0). Shouldn*t a machine be possible to distinguish even a pixel difference of speed? If not, isn*t this just because of the network architecture? (5) Page 5 *Since the agents exhibit similar performance using pixels and features we conduct the remaining experiments in this section using feature observations, since these agents are substantially faster to train.*: How about at least showing a correlation of performances at the instance level (rather than average performances)? Even so, I think this is a bit of big conclusion. (6) Throughout the papers, I felt that many conclusions (e.g. difficulty and etc) are based on a particularly chosen training distribution. For example, how does an agent really know when the instance is any more difficult? Doesn*t this really depend on the empirically learned distribution of training samples (i.e. P(m_3 | m_1, m_2), where m_i indicates masses of object 1, 2, and 3)? In other words, does what*s hard/easy matter much unless this is more thoroughly tested over various types of distributions? (7) Any baseline approach?","The reviewer acknowledges the paper's interesting idea and experimental setup but expresses concerns about the clarity of the contribution and lack of deep analysis. They find the results interesting but not groundbreaking, comparing them to typical DRL tasks. The reviewer suggests further exploration of learned representations and links to human behavior or physical laws. Despite these reservations, they recommend acceptance due to the merit of the idea, making the sentiment cautiously positive. The language used is polite and professional, focusing on constructive criticism and suggestions for improvement.",40,70
"In this paper, the authors proposed a extension to the DQN algorithm by introducing both an upper and lower bound to the optimal Q function. The authors show experimentally that this approach improves the data efficiency quite dramatically such that they can achieve or even supersede the performance of DQN that is trained in 8 days. The idea is novel to the best of my knowledge and the improvement over DQN seems very significant. Recently, Remi et al have introduced the Retrace algorithm which can make use of multi-step returns to estimate Q values. As I suspect, some of the improvements that comes from the bounds is due to the fact that multi-step returns is used effectively. Therefore, I was wondering whether the authors have tried any approach like Retrace or Tree backup by Precup et al. and if so how do these methods stack up against the proposed method. The author have very impressive results and the paper proposes a very promising direction for future research and as a result I would like to make a few suggestions: First, it would be great if the authors could include a discussion about deterministic vs stochastic MDPs. Second, it would be great if the authors could include some kind of theoretically analysis about the approach. Finally, I would like to apologize for the late review.","The reviewer starts by praising the novelty of the idea and significance of the improvement (""The idea is novel to the best of my knowledge and the improvement over DQN seems very significant.""). They also clearly state the paper proposes a promising direction for future research. While they do pose a question and offer suggestions for improvement, the tone is constructive and inquisitive rather than critical. The language used is formal and respectful throughout.",85,90
"This paper proposed an integration of memory network with reinforcement learning. The experimental data is simple, but the model is very interesting and relatively novel. There are some questions about the model: 1. how does the model extend to the case with multiple variables in a single sentence? 2. If the answer is out of vocabulary, how would the model handle it? 3. I hope the authors can provide more analysis about the curriculum learning part, since it is very important for the RL model training. 4. In the training, in each iteration, how the data samples were selected, by random or from simple one depth to multiple depth?","The reviewer finds the model interesting and novel, which indicates a positive sentiment. They provide constructive questions and suggestions for improvement, rather than harsh criticism. The language is formal and respectful throughout.",60,80
"This paper proposes a new memory module for large scale life-long and one-shot learning. The module is general enough that the authors apply the module to several neural network architectures and show improvements in performance. Using k-nearest neighbors for memory access is not completely new. This has been recently explored in Rae et al., 2016 and Chandar et al., 2016. K-nearest neighbors based memory for one-shot learning has also been explored in [R1]. This paper provides experimental evidence that such an approach can be applied to a variety of architectures. Authors have addressed all my pre-review questions and I am ok with their response. Are the authors willing to release the source code to reproduce the results? At least for omniglot experiments and synthetic task experiments? References: [R1] Charles Blundell, Benigno Uria, Alexander Pritzel, Yazhe Li, Avraham Ruderman, Joel Z. Leibo, Jack Rae, Daan Wierstra, Demis Hassabis: Model-Free Episodic Control. CoRR abs/1606.04460 (2016)","The reviewer acknowledges the contribution of the paper stating that ""This paper provides experimental evidence that such an approach can be applied to a variety of architectures."", which points towards a positive sentiment. However, the reviewer also points out that the core idea is not entirely novel and cites existing literature. Overall, the tone is neutral, leaning slightly towards the positive side due to the acknowledgement of the paper's contribution. The language used is purely professional and polite.",20,50
"This paper shows that extending deep RL algorithms to decide which action to take as well as how many times to repeat it leads to improved performance on a number of domains. The evaluation is very thorough and shows that this simple idea works well in both discrete and continuous actions spaces. A few comments/questions: - Table 1 could be easier to interpret as a figure of histograms. - Figure 3 could be easier to interpret as a table. - How was the subset of Atari games selected? - The Atari evaluation does show convincing improvements over A3C on games requiring extended exploration (e.g. Freeway and Seaquest), but it would be nice to see a full evaluation on 57 games. This has become quite standard and would make it possible to compare overall performance using mean and median scores. - It would also be nice to see a more direct comparison to the STRAW model of Vezhnevets et al., which aims to solve some of the same problems as FiGAR. - FiGAR currently discards frames between action decisions. There might be a tradeoff between repeating an action more times and throwing away more information. Have you thought about separating these effects? You could train a model that does process intermediate frames. Just a thought. Overall, this is a nice simple addition to deep RL algorithms that many people will probably start using. -------------------- I*m increasing my score to 8 based on the rebuttal and the revised paper.","The review starts with positive statements, highlighting the thorough evaluation and positive results. While it lists several suggestions for improvement, these are presented constructively and aim to enhance the paper. The reviewer acknowledges the value of the work, stating ""this is a nice simple addition to deep RL algorithms that many people will probably start using."" The final score increase further indicates a positive sentiment.",75,80
"This work builds on top of STOKE (Schkufza et al., 2013), which is a superoptimization engine for program binaries. It works by starting with an existing program, and proposing modifications to it according to a proposal distribution. Proposals are accepted according to the Metropolis-Hastings criteria. The acceptance criteria takes into account the correctness of the program, and performance of the new program. Thus, the MCMC process is likely to converge to correct programs with high performance. Typically, the proposal distribution is fixed. The contribution of this work is to learn the proposal distribution as a function of the features of the program (bag of words of all the opcodes in the program). The experiments compare with the baselines of uniform proposal distribution, and a baseline where one just learns the weights of the proposal distribution but without conditioning on the features of the program. The evaluation shows that the proposed method has slightly better performance than the compared baselines. The significance of this work at ICLR seems to be quite low., both because this is not a progress in learning representations, but a straightforward application of neural networks and REINFORCE to yet another task which has non-differentiable components. The task itself (superoptimization) is not of significant interest to ICLR readers/attendees. A conference like AAAI/UAI seem a better fit for this work. The proposed method is seemingly novel. Typical MCMC-based synthesis methods are lacking due to their being no learning components in them. However, to make this work compelling, the authors should consider demonstrating the proposed method in other synthesis tasks, or even more generally, other tasks where MH-MCMC is used, and a learnt proposal distribution can be beneficial. Superoptimization alone (esp with small improvements over baselines) is not compelling enough. It is also not clear if there is any significant representation learning is going on. Since a BoW feature is used to represent the programs, the neural network cannot possibly learn anything more than just correlations between presence of opcodes and good moves. Such a model cannot possibly understand the program semantics in any way. It would have been a more interesting contribution if the authors had used a model (such as Tree-LSTM) which attempts to learn the semantics the program. The quite naive method of learning makes this paper not a favorable candidate for acceptance.","The review starts with a neutral factual summary of the paper's contributions. However, the reviewer expresses concerns about the significance and novelty of the work, stating that it's a ""straightforward application of neural networks"" and not a significant advancement in learning representations. They also find the task itself (superoptimization) not particularly interesting for the target audience. While the reviewer acknowledges the novelty of learning the proposal distribution, they suggest exploring other applications to make the work more compelling. The criticism regarding the use of BoW features and the lack of semantic understanding further contributes to the negative sentiment. The language used, while critical, maintains a professional and academic tone.",-40,60
"The paper introduces a novel memory mechanism for NTMs based on differentiable Lie groups. This allows to place memory elements as points on a manifold, while still allowing training with backpropagation. It*s a more general version of the NTM memory, and possibly allows for training a more efficient addressing schemes. Pros: - novel and interesting idea for memory access - nicely written Cons: - need to manually specify the Lie group to use (it would be better if network could learn the best way of accessing memory) - not clear if this really works better than standard NTM (compared only to simplified version) - not clear if this is useful in practice (no comparison on real tasks)","The reviewer finds the paper's idea novel and interesting, which indicates a positive sentiment. However, they also raise several concerns and suggest improvements, pointing towards a more neutral stance overall. The language used is professional and constructive, lacking any negative or disrespectful phrasing.",50,80
"This paper proposed a proximal (quasi-) Newton’s method to learn binary DNN. The main contribution is to combine pre-conditioning with binarization in a proximal framework. It is interesting to have a proximal Newton’s method to interpret the different DNN binarization schemes. This gives a new interpretation of existing approaches. However, the theoretical analysis is not very convincing or useful. The formulated optimization problem (3)-(4) is essentially a mixed integer programming. Even though the paper treats the integer part as a constraint and address it in proximal operators, the constraint set is still discrete and there is no guarantee that the proximal Newton algorithm could converge under practically useful conditions. In practice it is hard to verify the assumption [d_t^t]_k > _x0008_eta in Theorem 3.1. This relation could be hard to hold in DNN as the loss surface could be extremely complicated.","The review starts with positive remarks, highlighting the interesting aspects of the paper and its contribution ('interesting', 'gives a new interpretation'). However, it then transitions into a  critical analysis of the theoretical grounding, pointing out concerns and limitations ('not very convincing', 'no guarantee', 'hard to verify', 'could be hard to hold'). The criticism, while direct, maintains a professional and neutral tone.  Therefore, the sentiment leans slightly negative due to the significant concerns raised.",-20,20
"This work proposes a new approach for image compression using auto encoders. The results are impressive, besting the state of the art in this field. Pros: + Very clear paper. It should be possible to replicate these results should one be inclined to do so. + The results, when compared to other work in this field are very promising. I need to emphasize, and I think the authors should have emphasized this fact as well: this is very new technology and it should not be surprising it*s not better than the state of the art in image compression. It*s definitely better than other neural network approaches to compression, though. Cons: - The training procedure seems clunky. It requires multiple training stages, freezing weights, etc. - The motivation behind Figure 1 is a bit strange, as it*s not clear what it*s trying to illustrate, and may confuse readers (it talks about effects on JPEG, but the paper discusses a neural network architecture, not DCT quantization)","The review starts with positive remarks, highlighting the impressive results and clear presentation. While it mentions some drawbacks like the complex training procedure and a confusing figure, it acknowledges the novelty of the approach and its superiority over other neural network methods. The language is constructive and professional throughout. Therefore, the sentiment leans positive, and the politeness is high.",75,80
"The paper looks at the problem of locating the answer to a question in a text (For this task the answer is always part of the input text). For this the paper proposes to combine two existing works: Match-LSTM to relate question and text representations and Pointer Net to predict the location of the answer in the text. Strength: - The suggested approach makes sense for the task and achieves good performance, (although as the authors mention, recent concurrent works achieve better results) - The paper is evaluated on the SQuAD dataset and achieves significant improvements over prior work. Weaknesses: 1. It is unclear from the paper how well it is applicable to other problem scenarios where the answer is not a subset of the input text. 2. Experimental evaluation 2.1. It is not clear why the Bi-Ans-Ptr in Table 2 is not used for the ensemble although it achieves the best performance. 2.2. It would be interested if this approach generalizes to other datasets. Other (minor/discussion points) - The task and approach seem to have some similarity of locating queries in images and visual question answering. The authors might want to consider pointing to related works in this direction. - I am wondering how much this task can be seen as a “guided extractive summarization”, i.e. where the question guides the summarization process. - Page 6, last paragraph: missing “.”: “… searching This…” Summary: While the paper presents an interesting combination of two approaches for the task of answer extraction, the novelty is moderate. While the experimental results are encouraging, it remains unclear how well this approach generalizes to other scenarios as it seems a rather artificial task.","The review acknowledges the strengths of the paper, such as its logical approach and good performance. However, it also points out limitations, like the unclear generalizability and lack of certain experimental details. The tone is critical but professional and suggests areas for improvement rather than outright rejection. Overall, it leans slightly towards the positive side due to the acknowledged strengths.",20,60

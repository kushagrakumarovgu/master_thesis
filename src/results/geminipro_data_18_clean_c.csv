review,reasoning,sentiment_score,politeness_score
"This paper presents a solid empirical analysis of a simple idea for learning embeddings of time series: training a convolutional network with a custom pooling layer that generates a fixed size representation to classify time series, then use the fixed size representation for other tasks. The primary innovation is a custom pooling operation that looks at a fraction of a sequence, rather than a fixed window. The experiments are fairly thorough (albeit with some sizable gaps) and show that the proposed approach outperforms DTW, as well as embeddings learned using Siamese networks. On the whole, I like the line of inquiry and the elegant simplicity of the proposed approach, but the paper has some flaws (and there are some gaps in both motivation and the experiments) that led me to assign a lower score. I encourage the authors to address these flaws as much as possible during the review period. If they succeed in doing so, I am willing to raise my score.

QUALITY

I appreciate this line of research in general, but there are some flaws in its motivation and in the design of the experiments. Below I list strengths (+) and weaknesses (-):

+ Time series representation learning is an important problem with a large number of real world applications. Existing solutions are often computationally expensive and complex and fail to generalize to new problems (particularly with irregular sampling, missing values, heterogeneous data types, etc.). The proposed approach is conceptually simple and easy to implement, faster to train than alternative metric learning approaches, and learns representations that admit fast comparisons, e.g., Euclidean distance.
+ The experiments are pretty thorough (albeit with some noteworthy gaps) -- they use multiple benchmark data sets and compare against strong baselines, both traditional (DTW) and deep learning (Siamese networks).
+ The proposed approach performs best on average!

- The custom pooling layer is the most interesting part and warrants additional discussion. In particular, the ""naive"" approach would be to use global pooling over the full sequence [4]. The authors should advance an argument to motivate %-length pooling and perhaps add a global pooling baseline to the experiments.
- Likewise, the authors need to fully justify the use of channel-wise (vs. multi-channel) convolutions and perhaps include a multi-channel convolution baseline.
- There is something incoherent about training a convolutional network to classify time series, then discarding the classification layer and using the internal representation as input to a 1NN classifier. While this yields an apples-to-apples comparison in the experiments, I am skeptical anyone would do this in practice. Why not simply use the classifier (I am dubious the 1NN would outperform it)? To address this, I recommend the authors do two things: (1) report the accuracy of the learned classifier; (2) discuss the dynamic above -- either admit to the reader that this is a contrived comparison OR provide a convincing argument that someone might use embeddings + KNN classifier instead of the learned classifier. If embeddings + KNN outperforms the learned classifier, that would surprise me, so that would warrant some discussion.
- On a related note, are the learned representations useful for tasks other than the original classification task? This would strengthen the value proposition of this approach. If, however, the learned representations are ""overfit"" to the classification task (I suspect they are), and if the learned classifier outperforms embeddings + 1NN, then what would I use these representations for?
- I am modestly surprised that this approach outperformed Siamese networks. The authors should report the Siamese architectures -- and how hyperparameters were tuned on all neural nets -- to help convince the reader that the comparison is fair.
- To that end, did the Siamese convolutional network use the same base architecture as the proposed classification network (some convolutions, custom pooling, etc.)? If not, then that experiment should be run to help determine the relative contributions of the custom pooling layer and the loss function.
- Same notes above re: triplet network -- the authors should report results in Table 2 and disclose architecture details.
- A stronger baseline would be a center loss [1] network (which often outperforms triplets).
- The authors might consider adding at least one standard unsupervised baseline, e.g., a sequence-to-sequence autoencoder [2,3].

CLARITY

The paper is clearly written for the most part, but there is room for improvement:

- The %-length pooling requires a more detailed explanation, particularly of its motivation. There appears to be a connection to other time series representations that downsample while preserving shape information -- the authors could explore this. Also, they should add a figure with a visual illustration of how it works (and maybe how it differs from global pooling), perhaps using a contrived example.
- How was the %-length pooling implemented? Most deep learning frameworks only provide pooling layers with fixed length windows, though I suspect it is probably straightforward to implement variable-width pooling layers in an imperative framework like PyTorch.
- Figure 1 is not well executed and probably unnecessary. The solid colored volumes do not convey useful information about the structure of the time series or the neural net layers, filters, etc. Apart from the custom pooling layer, the architecture is common and well understood by the community -- thus, the figure can probably be removed.
- The paper needs to fully describe neural net architectures and how hyperparameters were tuned.

ORIGINALITY

The paper scores low on originality. As the authors themselves point out, time series metric learning -- even using deep learning -- is an active area of research. The proposed approach is refreshing in its simplicity (rather than adding additional complexity on top of existing approaches), but it is straightforward -- and I suspect it has been used previously by others in practice, even if it has not been formally studied. Likewise, the proposed %-length pooling is uncommon, but it is not novel per se (dynamic pooling has been used in NLP [5]). Channel-wise convolutional networks have been used for time series classification previously [6].

SIGNIFICANCE

Although I identified several flaws in the paper's motivation and experimental setup, I think it has some very useful findings, at least for machine learning practitioners. Within NLP, there appears to be gradual shift toward using convolutional, instead of recurrent, architectures. I wonder if papers like this one will contribute toward a similar shift in time series analysis. Convolutional architectures are typically much easier and faster to train than RNNs, and the main motivation for RNNs is their ability to deal with variable length sequences. Convolutional architectures that can effectively deal with variable length sequences, as the proposed one appears to do, would be a welcome innovation.

REFERENCES

[1] Wen, et al. A Discriminative Feature Learning Approach for Deep Face Recognition. ECCV 2016.
[2] Fabius and van Amersfoort. Variational Recurrent Auto-Encoders. ICLR 2015 Workshop Track.
[3] Tikhonov and Yamshchikov. Music generation with variational recurrent autoencoder supported by history. arXiv.
[4] Hertel, Phan, and Mertins. Classifying Variable-Length Audio Files with All-Convolutional Networks and Masked Global Pooling. 
[5] Kalchbrenner, Grefenstette, and Blunsom. A Convolutional Neural Network for Modelling Sentences. ACL 2014.
[6] Razavian and Sontag. Temporal Convolutional Neural Networks for Diagnosis from Lab Tests. arXiv.","The reviewer provides a balanced perspective with both positive and negative points. They acknowledge the strengths of the paper, such as its simplicity and empirical results, but also highlight significant weaknesses in motivation, experimental design, and novelty. The reviewer's willingness to raise their score based on revisions suggests they are open to the research but need more convincing. Overall, the tone is critical but constructive, aiming to improve the paper. ",40,70
"This paper aims to address the deep learning architecture search problem via incremental addition and removal of channels in intermediate layers of the network. Experiments are carried out on small-scale datasets such as MNIST and CIFAR, as well as an exploratory run on ImageNet (AlexNet).

Overall, I find the approach proposed in the paper interesting but a little bit thin in content. Essentially, one increases or decreases the number of features based on equation 2. It would be much valuable to see ablation studies to show the effectiveness of such criterion: for example, simple cases one can think of is to model (1) a data distribution of known rank, (2) simple MLP/CNN models to show the cross-layer relationships (e.g. sudden increase and decrease of the number of channels across layers will be penalized by c^l_{f^{l+1}, t}), etc.

The experimentation section uses small scale datasets and as a result, it is relatively unclear how the proposed approach will perform on real-world applications. One apparent shortcoming of such approach is that training takes much longer time, and the algorithm is not easily made parallel (the sgd steps limit the level of parallelization that can be carried out). As a result, I am not sure about the applicability of the proposed approach.","The reviewer finds the paper's approach ""interesting"" but suggests the content is ""a little bit thin."" They recommend additional studies and point out limitations such as increased training time and lack of parallelism. This suggests a slightly positive sentiment overall, as the reviewer sees potential but has reservations. The language used is professional and not rude, suggesting a neutral-to-polite tone.",20,50
"Summary
*******
The paper provides a collection of existing results in statistics.

Comments
********
Page 1: references to Q-learning and Policy-gradients look awkwardly recent, given that these have been around for several decades.

I dont get what is the novelty in this paper. There is no doubt that all the tools that are detailed here are extremely useful and powerful results in mathematical statistics. But they are all known.

The Gibbs variational principle is folklore, Proposition 1,2 are available in all good text books on the topic, 
and Proposition 4 is nothing but a transportation Lemma.
Now, Proposition 3 is about soft-Bellman operators. This perhaps is less standard because contraction property of soft-Bellman operator in infinite norm is more recent than for Bellman operators.
But as mentioned by the authors, this is not new either. 
Also I don't really see the point of providing the proofs of these results in the main material, and not for instance in appendix, as there is no novelty either in the proof techniques.

I don't get the sentence ""we have restricted so far the proof in the bandit setting"": bandits are not even mentioned earlier.

Decision
********
I am sorry but unless I missed something (that then should be clarified) this seems to be an empty paper: Strong reject.","The review is extremely negative about the paper, using very strong language to criticize the paper's lack of novelty and contribution. Phrases like ""empty paper"", ""Strong reject"", ""I don't get what is the novelty"", and ""this seems to be an empty paper"" all point to a very negative sentiment. While the reviewer does not resort to personal attacks, the tone is quite blunt and dismissive, indicating low politeness. ",-90,-40
"* This paper models images with a latent code representation, and then tries to modify the latent code to minimize changes in image space, while changing the classification label. As the authors indicate, it lies in the space of algorithms looking to modify the image while changing the label (e.g. LIME etc).

* This is quite an interesting paper with a sensible goal. It seems like the method could be more informative than the other methods.  However, there are quite a number of problems, as explained below.

* The explanation of eqs 1 and 2 is quite poor. \alpha in (1) seems to be \gamma in Alg 1 (line 5). ""L_target is a target objective which can be a negative class probability .."" this assumes that the example is a positive class. Could we not also apply this to negative examples?

""or in the case of heart failure, predicted BNP level"" -- this doesn't make sense to me -- surely it would be necessary to target an adjusted BNP level? Also specific details should be reserved until a general explanation of the problem has been made.

* The trade-off parameter \gamma is a ""fiddle factor"" -- how was this set for the lung image and MNIST examples? Were these values different?

* In typical ICLR style the authors use a deep network to learn the encoder and decoder networks. It would be v interesting (and provide a good baseline) to use a shallow network (i.e. PCA) instead, and elucidate what advantages the deep network brings.

* The example of 4/9 misclassification seems very specific. Does this method also work on say 2s and 3s? Why have you not reported results for these kinds of tasks?

* Fig 2: better to show each original and reconstructed image close by (e.g. above below or side-by-side).

The reconstructions show poor detail relative to the originals.  This loss of detail could be a limitation.

* A serious problem with the method is that we are asked to evaluate it in terms of images like Fig 4 or Fig 8. A serious study would involve domain experts and ascertain if Fig 4 conforms with what they are looking for.

* The references section is highly inadequate -- no venues of publication are given. If these are arXiv give the proper ref. Others are published in conferences etc, e.g. Goodfellow et al is in Advances in Neural Information Processing Systems 27, 2014.

* Overall: the paper contains an interesting idea, but given the deficiencies raised above I judge that it falls below the ICLR threshold.

* Text:

sec 2 para 4. ""reconstruction loss on the validation set was similar to the reconstruction loss on the validation set."" ??

* p 3 bottom -- give size of dataset

* p 5 AUC curve -> ROC curve

* p 6 Fig 4 use text over each image to better specify the details given in the caption.



","The review acknowledges the paper's interesting idea and potential but highlights significant weaknesses. The reviewer points out several issues with clarity, methodology, and evaluation, suggesting the paper falls below the ICLR threshold. The language is direct and critical, focusing on the paper's shortcomings, but maintains a professional tone without resorting to personal attacks or disrespectful language.",-20,60
"This paper discusses the problem of action segmentation in long videos, up to 10 minutes long. The basic idea is to use a temporal convolutional encoder-decoder architecture, where in the enconder 1-D temporal convolutions are used. In the decoder three variants are studied:

(1) One that uses only several bidirectional LSTMs, one after the other.
(2) One that first applies successive layers of deconvolutions to produce per frame feature maps. Then, in the end a bidirectional LSTM in the last layer.
(3) One that first applies a bidirectional LSTM, then applies successively 1-D deconvolution layer.

All variants end with a ""temporal softmax""  layer, which outputs a class prediction per frame.

Overall, the paper is of rather limited novelty, as it is very similar to the work of Lea et al., 2017, where now the decoder part also has the deconvolutions smoothened by (bidirectional) LSTMs. It is not clear what is the main novelty compared to the aforementioned paper, other than temporal smoothing of features at the decoder stage.

Although one of the proposed architectures (TricorNet) produces some modest improvements, it is not clear why the particular architectures are a good fit. Surely, deconvolutions and LSTMs can help incorporate some longer-term temporal elements into the final representations. However, to begin with, aren't the 1-D deconvolutions and the LSTMs (assuming they are computed dimension-wise) serving the same purpose and therefore overlapping? Why are both needed?

Second, what makes the particular architectures in Figure 3 the most reasonable choice for encoding long-term dependencies, is there a fundamental reason? What is the difference of the L_mid from the 1-D deconv layers afterward? Currently, the three variants are motivated in terms of what the Bi-LSTM can encode (high or low level details). 

Third, the qualitative analysis can be improved. For instance, the experiment with the ""cut lettuce"" vs ""peel cucumber"" is not persuasive enough. Indeed, longer temporal relationships can save incorrect future predictions. However, this works both ways, meaning that wrong past predictions can persist because of the long-term modelling. Is there a mechanism in the proposed approach to account for that fact?

All in all, I believe the paper indeed improves over existing baselines. However, the novelty is insufficient for a publication at this stage.","The reviewer acknowledges some merits of the paper, such as the modest improvements of the proposed architecture. However, they mainly point out its limitations: lack of significant novelty compared to existing work, unclear justifications for the architectural choices, and a need for a more robust qualitative analysis. The tone is critical, questioning the rationale behind the proposed methods, but remains professional and within the bounds of academic discourse.",-20,60
"The paper proposes a neural net architecture that uses a predefined orthogonal binary basis to construct the filter weights of the different convolutional layers. Since only the basis weights need to be stored this leads to an exponential reduction in memory. The authors propose to compute the filter weights on the fly in order to tradeoff memory for computation time. Experiments are performed on ImageNet, MNIST, CIFAR datasets with comparisons to BinaryConnect, Binary-weight-networks and studies showing the memory vs time vs accuracy tradeoff.

Positives
- The idea of using a predefined basis to estimate filter weights in a neural network is novel and leads to significant reduction in memory usage.

Negatives
- The proposed method seems significantly worse than other binary techniques on ImageNet, CIFAR and SVHN. On Imagenet in particular binary-weight-network is 21% better at only 2x the model size. Would a binary-weight-network of the same model size be better than the proposed approach? It would help to provide results using the proposed method with the same model size as binary-weight-networks on the different datasets.  
- The citation to binary-weight-networks is missing.
- The descriptions in section 3.3, 3.4 need to be more rigorous. For instance, how many basis weights are needed for a filter of size N. Does N need to be a power of 2 or are extra dimentions from the basis just ignored?
","The review acknowledges the novelty of the paper's proposed method and its potential to reduce memory usage, which is positive. However, it also points out significant drawbacks in terms of performance compared to other techniques. The reviewer suggests specific areas for improvement and additional experiments, indicating a desire to see the work strengthened rather than rejected outright. The tone is direct and critical but professional and provides concrete suggestions for improvement. Overall, this leans towards the neutral side of positive.",20,50
"**Strengths**
In general, the paper makes an important observation that even in textual dialog, it might often make sense to reason or “imagine” how visual instances look, and this can lead to better more grounded dialog. 

**Weakness**
In general, the paper has some major weaknesses in how the dataset has been constructed, details of the models provided and generally the novelty of the proposed model. While the model on its own is not very novel, the paper does make an interesting computational observation that it could help to reason about vision even in textual dialog, but the execution of the dataset curation is not satisfactory, making the computational contribution less interesting. 

More specific details below:
1. The paper does not write down an objective that they are optimizing for any of the three stages in the model, and it is unclear what is the objective especially for the video context prediction task -- the distribution over the space of images (or videos) for a given piece of text is likely multimodal and gaussian likelihoods might not be sufficient to model this properly. Not clear if the sequence to sequence models are used in teacher forcing model when training in Stage 1, or there is sampling going on. In general, the paper lacks rigor in writing down what it optimizes, and laying out details of the model clearly. 

2. The manner in which the dataset has been constructed is unsatisfying -- it assumes that two consecutive pieces of subtitles in news channels constitutes a dialog. This is very likely an incorrect and unsatisfying assumption which does not take into account narrative, context etc. Right now the dataset seems more like skip-thought vectors [A] which models the distribution over contextual sentences given a source sentence than any kind of dialog.

3. The setup and ultimately the motivation in context of the setup is fairly artificial -- the dataset does have images corresponding to each “dialog” so it is unclear why the associative model is needed in this case. Further, it would have been useful to see quantitative evaluation of the proposed approach or statistics of the dataset to establish context for the dataset being a valid benchmark, and providing a baseline / numerical checkpoint for future works to compare to. Without any of these things, the work seems fairly incomplete.

Clarity:
1. Figure 2 captions are pretty unclear and hard to understand what they are conveying.
2. For a large part the paper talks about how visual instances are not available for textual phrases and then proceeds to assume access to aligned text and visual data. It would be good to clarify from the start that the model does need paired videos and text, and state exactly how much aligned data is needed.
3. Already learned CNN (Page. 4, Sec. 2.2.1): Would be good to mention which CNN was used.
4. Page 4: “the textual and visual context vectors of the spider are generated, respectively”: Would be good to clarify that textual and visual context vectors for the spider are attended to, as opposed to saying they are generated.

References:

[A]: Kiros, Ryan, Yukun Zhu, Ruslan R. Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. “Skip-Thought Vectors.” In Advances in Neural Information Processing Systems 28, edited by C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, 3294–3302. Curran Associates, Inc.","The reviewer acknowledges the paper's interesting observation about the potential benefits of visual reasoning in textual dialogue. However, they express significant concerns about the dataset construction, model details, and overall novelty. The lack of a clear objective, the questionable assumption behind the dataset, and the absence of quantitative evaluation contribute to a negative sentiment. While the reviewer provides constructive criticism and specific recommendations, the numerous and substantial weaknesses outweigh the acknowledged strength, leading to a negative sentiment. The language remains professional and polite throughout, focusing on the technical aspects and avoiding personal attacks.",-50,70
"This paper formulates a variant of convolutional neural networks which models both activations and filters as continuous functions composed from kernel bases. A closed-form representation for convolution of such functions is used to compute in a manner than maintains continuous representations, without making discrete approximations as in standard CNNs.

The proposed continuous convolutional neural networks (CCNNs) project input data into a RKHS with a Gaussian kernel function evaluated at a set of inducing points; the parameters defining the inducing points are optimized via backprop. Filters in convolutional layers are represented in a similar manner, yielding a closed-form expression for convolution between input and filters. Experiments train CCNNs on several standard small-scale image classification datasets: MNIST, CIFAR-10, STL-10, and SVHN.

While the idea is interesting and might be a good alternative to standard CNNs, the paper falls short in terms of providing experimental validation that would demonstrate the latter point. It unfortunately only experiments with CCNN architectures with a small number (eg 3) layers. They do well on MNIST, but MNIST performance is hardly informative as many supervised techniques achieve near perfect results. The CIFAR-10, STL-10, and SVHN results are disappointing. CCNNs do not outperform the prior CNN results listed in Table 2,3,4. Moreover, these tables do not even cite more recent higher-performing CNNs. See results table in (*) for CIFAR-10 and SVHN results on recent ResNet and DenseNet CNN designs which far outperform the methods listed in this paper.

The problem appears to be that CCNNs are not tested in a regime competitive with the state-of-the-art CNNs on the datasets used. Why not? To be competitive, deeper CCNNs would likely need to be trained. I would like to see results for CCNNs with many layers (eg 16+ layers) rather than just 3 layers. Do such CCNNs achieve performance compatible with ResNet/DenseNet on CIFAR or SVHN? Given that CIFAR and SVHN are relatively small datasets, training and testing larger networks on them should not be computationally prohibitive.

In addition, for such experiments, a clear report of parameters and FLOPs for each network should be included in the results table. This would assist in understanding tradeoffs in the design space.

Additional questions:

What is the receptive field of the CCNNs vs those of the standard CNNs to which they are compared? If the CCNNs have effectively larger receptive field, does this create a cost in FLOPs compared to standard CNNs?

For CCNNs, why does the CCAE initialization appear to be essential to achieving high performance on CIFAR-10 and SVHN? Standard CNNs, trained on supervised image classification tasks do not appear to be dependent on initialization schemes that do unsupervised pre-training. Such dependence for CCNNs appears to be a weakness in comparison.","The review starts with a neutral summary of the paper's contributions. However, it expresses disappointment regarding the experimental validation, stating that the results are ""disappointing"" and highlighting the lack of comparison with state-of-the-art methods. The reviewer also raises several concerns and asks for additional experiments. The language used is direct and critical but remains professional and not personally offensive.",-30,60
"The paper discusses ways to guard against adversarial domain shifts with so-called counterfactual regularization. The main idea is that in several datasets there are many instances of images for the same object/person, and that taking this into account by learning a classifier that is invariant to the superficial changes (or “style” features, e.g. hair color, lighting, rotation etc.) can improve the robustness and prediction accuracy. The authors show the benefit of this approach, as opposed to the naive way of just using all images without any grouping, in several toy experimental settings.

Although I really wanted to like the paper, I have several concerns. First and most importantly, the paper is not citing several important related work. Especially, I have the impression that the paper is focusing on a very similar setting (causally) to the one considered in  [Gong et al. 2016] (http://proceedings.mlr.press/v48/gong16.html), as can be seen from Fig. 1. Although not focusing on classification directly, this paper also tries to a function T(X) such that P(Y|T(X)) is invariant to domain change. Moreover, in that paper, the authors assume that even the distribution of the class can be changed in the different domains (or interventions in this paper).
Besides, there are also other less related papers, e.g. http://proceedings.mlr.press/v28/zhang13d.pdf, https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/10052/0, https://arxiv.org/abs/1707.09724, (or potentially https://arxiv.org/abs/1507.05333 and https://arxiv.org/abs/1707.06422), that I think may be mentioned for a more complete picture. Since there is some related work, it may be also worth to compare with it, or use the same datasets.

I’m also not very happy with the term “counterfactual”. As the authors mention in footnote, this is not the correct use of the term, since counterfactual means “against the fact”. For example, a counterfactual query is “we gave the patient a drug and the patient died, what would have happened if we didn’t give the drug?” In this case, these are just different interventions on possibly the same object. I’m not sure that in the practical applications one can assure that the noise variables stay the same, which, as the authors correctly mention, would make it a bit closer to counterfactuals. It may sound pedantic, but I don’t understand why use the wrong and confusing terminology for no specific reason, also because in practice the paper reduces to the simple idea of finding a classifier that doesn’t vary too much in the different images of the single object.

**EDIT**: I was satisfied with the clarifications from the authors and I appreciated the changes that they did with respect to the related work and terminology, so I changed my evaluation from a 5 (marginally below threshold) to a 7 (good paper, accept).","The reviewer expresses initial interest (""really wanted to like the paper""), but raises several concerns, indicating a negative sentiment. However, the final note mentions satisfaction with the author's revisions, suggesting a shift towards positive. The language used, while critical, is professional and not personally attacking, indicating politeness. The reviewer provides constructive feedback and engages with the authors' work, suggesting a collaborative approach. Therefore, the sentiment leans slightly negative due to the initial concerns, and the politeness is notably positive.",-10,70
"This paper presents a sensitivity-penalized loss (the loss of the classifier has an additional term in squart of the gradient of the classifier w.r.t. perturbations of the inputs), and a minimax (or maximin) driven algorithm to find attacks and defenses. It has a lemma which claims that the ""minimax and the maximin solutions provide the best worst-case defense and attack models, respectively"", without proof, although that statement is supported experimentally.

+ Prior work seem adequately cited and compared to, but I am not really knowledgeable in the adversarial attacks subdomain.
- The experiments are on small/limited datasets (MNIST and CIFAR-10). Because of this, confidence intervals (over different initializations, for instance) would be a nice addition to Table 5.
- There is no exact (""alternating optimization"" could be considered one) evaluation of the impact of the sensitivy loss vs. the minimax/maximin algorithm.
- The paper is hard to follow at times (and probably that dealing with the point above would help in this regard), e.g. Lemma 1 and experimental analysis.
- It is unclear (from Figures 3 and 7) that ""alternative optimization"" and ""minimax"" converged fully, and/or that the sets of hyperparameters were optimal.
+ This paper presents a game formulation of learning-based attacks and defense in the context of adversarial examples for neural networks, and empirical findings support its claims.


Nitpicks:
the gradient descent -> gradient descent or the gradient descent algorithm
seeming -> seemingly
arbitrary flexible -> arbitrarily flexible
can name ""gradient descent that maximizes"": gradient ascent.
The mini- max or the maximin solution is defined -> are defined
is the follow -> is the follower
","The review acknowledges the paper's contributions (new loss function, minimax algorithm) and supports its claims based on empirical findings. However, it also points out several limitations, such as limited datasets, lack of confidence intervals, and clarity issues. The overall tone is constructive and suggestive of improvements rather than dismissive. Therefore, the sentiment is moderately positive, and the language is polite.",60,70
"The paper conducts an empirical study on 2nd-order algorithms for deep learning, in particular on CNNs to answer the question whether 2nd-order methods are useful for deep learning.  More modestly and realistically, the authors compared stochastic Newton method (SHG) and stochastic Quasi- Newton method (SR1, SQN) with stochastic gradient method (SGD).  The activation function ReLu is known to be singular at 0, which may lead to poor curvature information, but the authors gave a good numerical comparison between the performances of 2nd-order methods with ReLu and the smooth function, Tanh.  The paper presented a reasonably good overview of existing 2nd-order methods, with clear numerical examples and reasonably well written.

The paper presents several interesting empirical findings, which will no doubt lead to follow up work. However, there are also a few critical issues that may undermine their claims, and that need to be addressed before we can really answer the original question of whether 2nd-order methods are useful for deep learning. 

1. There is no complexity comparison, e.g. what is the complexity for a single step of different method.

2. Relatedly, the paper reports the performance over epochs, but it is not clear what ""per epoch"" means for 2nd-order methods.  In particular, it seems to me that they did not count the inner CG iterations, and it is known that this is crucial in running time and important for quality.  If so, then the comparison between 1st-order and 2nd-order methods are not fair or incomplete.

3. The results on 2nd-order methods behave similarly to 1st-order methods, which makes me wonder how many CG iterations they used for 2nd-order method in their experiment, and also the details of the data.  In particular, are they looking at parameter/hyperparameter settings for which 2nd-order methods aren't really necessary.

4. In deep learning setting, the training objective is non-convex, which means the Hessian can be non-PSD.  It is not clear how the stochastic inexact-Newton method mentioned in Section 2.1 could work.  Details on implementations of 2nd-order methods are important here.

5. For 2nd-order methods, the author used line search to tune the step size.  It is not clear in the line search, the author used the whole training objective or batch loss.  Assuming using the batch loss, I suspect the training curve will be very noisy (depending on how large the batch size is).  But the paper only show the average training curves, which might be misleading.

Here are other points.

1. There is no figure showing training/ test accuracy.  Aside from being interested in test error, it is also of interest to see how 2nd order methods are similar/different than 1st order methods on training versus test.

2. Since it is a comparison paper, it only compares three 2nd-order methods with SGD.  The choices made were reasonable, but 2nd-order methods are not as trivial to implement as SGD, and it isn't clear whether they have really ""spanned the space"" of second order methods

3. In the paper, the settings of LeNet, AlexNet are different with those in the original paper.  The authors did not give a reason.

4. The quality of figures is not good.

5. The setting of optimization is not clear, e.g. the learning rate of SGD, the parameter of backtrack line search.  It's hard to reproduce results when these are not described.

","The review starts with a generally positive sentiment, acknowledging the paper's contributions and clear writing. However, it then raises several ""critical issues"" that question the validity of the paper's claims. The reviewer lists several specific concerns, indicating a shift towards a more critical perspective. While the language remains professional and provides constructive feedback, the presence of these major concerns suggests the overall sentiment leans towards the negative side. ",-20,70
"This paper proposes a method to solve the 'word analogy problem', which was proposed as a way of understanding and evaluating word embeddings by Mikolov et al. There are some nice analyses in the paper which, if better organised, could lead to an improved understanding of semantic word spaces in neural nets. 

comments: 

The word analogy task was developed as an interesting way to analyse and understand word embedding spaces, but motivation for learning word embeddings was as general-purpose representations for language processing tasks (as in collobert et al, 2011), not as a way of resolving analogy questions. The authors develop a specialist method for resolving analogies, and it works (mostly) better than using the additive geometry of word-embedding spaces. But I don't think that comparison is 'fair' - the analogy thing is just a side-effect of word embedding spaces. 

Given that the authors focus on the word-analogy problem as an end in itself, I think there should be much more justification of why this is a useful problem to solve. Analogy seems to be fundamental to human cognition and reasoning, so maybe that is part of the reason, but it's not clear to the reader. 

The algorithm seems to be simple and intuitive, but the presentation is overly formal and unclear. It would be much easier for the reader to simply put into plain terms what the algorithm does.

Using a POS-tagger to strip out nouns is a form of supervision (the pos-tagger was trained on labelled data) that word-embedding methods do not use, which should at least be acknowledged when making a comparison. Similarly, it is nice that the present method works on less data, but the beauty of word embeddings is that they can be trained on any text - i.e. data is not a problem, and 'work' for any word type. Stripping away everything but nouns clearly allows co-occurrence semantic patterns to emerge from less data, but at the cost of the supervision mentioned above. Moreover, I suspect that the use of wikipedia is important for the proposed algorithm, as the pertinent relations are often explicit in the first sentence of articles ""Paris the largest city and capital of France..."". Would the same method work on any text? I would expect this question to be explored, even if the answer is negative. 

The goal of understanding word2vec and embedding spaces in general (section 5) is a really important one (as it can tell us a lot about how language and meaning is encoded in deep learning models in general), and I think that's one of the strongest aspects of this work. However, the conclusions from this section (and other related conclusions in other sections) are a little unclear to me. Perhaps that is because I don't quite get algorithm 3, which would be mitigated by an intuitive explanation to complement the pseudocode. I'm also confused by the assertion that Vec(A) - Vec(B) conveys the 'common information' in A and B. How can a non-symmetric operation convey 'common information'. Surely it conveys something about the relationship between A and B?

Minor point:
""may not the be indicative of the model's ability to learn the relationship between a word pair the way a human does"" (Abstract)
- I'm not sure we know how humans learn the relationships between word pairs. Are you referring to formal semantic relations i.e. in taxonomies in WordNet? This sentence seems dangerous, and the claim about humans is not really treated in the article itself. 

The a+cknowledgements compromise the anonymity of the authors. ","The reviewer acknowledges some positive aspects of the paper, such as the interesting analyses and the importance of understanding word embeddings. However, the review is mostly critical, questioning the paper's core premise and finding the presentation unclear. The reviewer also points out potential flaws in the methodology. The language used is quite direct and critical, but it doesn't cross the line into rudeness. It maintains a professional tone throughout.",-30,60
"This paper presents a new model for composing representations of characters into word embeddings. The starting point of their argument is to include position-specific embeddings of characters rather than just position-independent characters. By adding together position-specific vectors, reasonable results are obtained.

This is an interesting result, but I have a few recommendations to improve the paper.
1) It is a bit hard to assess since it is not evaluated on a standard datasets. There are a number standard datasets for open vocabulary language modeling. E.g., the MWC corpus (http://k-kawakami.com/research/mwc), or even the Penn Treebank (although it is conventionally modeled in closed vocabulary form).
2) There are many existing models for composing characters into words. In addition to those cited in the paper, see the citations listed below. Comparison with those is crucial in a paper like this.
3) Since the predictions are done at the word type level, it is unclear how vocabulary set of the corpus is determined, and what is done with OOV word types at test time (while it is possible to condition on them using the technique in the paper, it is not possible to use this technique for generation).
4) The analysis is interesting, but a more intuitive explanation would be to show nearest neighbor plots.

Some missing citations:

Composing characters into words:

dos Santos and Zadrozny. (2014 ICML) http://proceedings.mlr.press/v32/santos14.pdf
Ling et al. (2015 EMNLP) Finding Function in Form. https://arxiv.org/abs/1508.02096

Additionally, using explicit positional features in modeling language has been used:
Vaswani et al. (2017) Attention is all you need https://arxiv.org/abs/1706.03762
and a variety of other sources.","The reviewer provides constructive criticism, acknowledges the interesting aspects of the work ('This is an interesting result'), and offers concrete suggestions for improvement. This suggests a positive attitude towards the paper, aiming to guide the authors towards a stronger publication. The language is formal and polite throughout, without resorting to harsh or negative phrasing.",60,80
"Summary

The paper addresses the issues of fair pricing and secure transactions between model and data providers in the context of machine learning real-world application.

Major

The paper addresses an important issue regarding the real-world application of machine learning, that is, the transactions between data and model provider and the associated aspects of fairness, pricing, privacy, and security.

The originality and significance of the work reported in this paper are difficult to comprehend. This is largely due to the lack of clarity, in general, and the lack of distinction between what is known and what is proposed. I failed to find any clear description of the proposed approach and any evaluation of the main idea.

Most of the discussions in the paper are difficult to follow due to that many of the statements are vague or unclear. There are some examples of this vagueness illustrated under “minor issues”. Together, the many minor issues contribute to a major communication issue, which significantly reduces readability of the paper. A majority of the references included in the reference section lack some or all of the required meta data.

In my view, the paper is out of scope for ICLR. Neither the CFP overview nor the (non-exhaustive) list of relevant topics suggest otherwise. In very general terms, the paper could of course be characterised as dealing with machine learning implementation/platform/application but the issues discussed are more connected to privacy, security, fair transactions, and pricing.

In summary; although there is no universal rule on how to structure research papers, a more traditional structure (introduction, aim & scope, background, related work, method, results, analysis, conclusions & future work) would most certainly have benefitted the paper through improved clarity and readability. Although some interesting works on adversarial learning, federated learning, and privace-preserving training are cited in the paper, the review and use of these references did not contribute to a better understanding of the topic or the significance of the contribution in this paper. I was unable to find any support in the paper for the strong general result stated in the abstract (“We successfully show that without running the data through the model, one can approximate the value of the data”).

Minor issues (examples)

- “Models trained only a small scale of data” (missing word)
- “to prevent useful data from not being paid” (unclear meaning)
- “while the company may decline reciprocating gifts such as academic collaboration, while using the data for some other service in the future” (unclear meaning)
- “since any data given up is given up ” (unclear meaning)
- “a user of a centralized service who has given up their data will have trouble telling if their data exchange was fair at all (even if their evaluation was purely psychological)” (unclear meaning)
- “For a generally deployed model, it can take any form. Designing a transaction strategy for each one can be time-consuming and difficult to reason about” (unclear meaning)
- “(et al., 2017)” (unknown reference)
- “Osbert Bastani, Carolyn Kim, and Hamsa Bastani. Interpreting blackbox models via model extraction, 2017” (incomplete reference data)
- “Song Han, Huizi Mao, and William J. Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding, 2015.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network, 2015.
Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions, 2017.” (Incomplete reference data)
- “H. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Agera y Arcas. Communication-efficient learning of deep networks from decentralized data. 2016.” (Incomplete reference data)
- “et al. Richard Craid.” (Incorrect author reference style)
- “Ryo Yonetani, Vishnu Naresh Boddeti, Kris M. Kitani, and Yoichi Sato. Privacy-preserving visual learning using doubly permuted homomorphic encryption, 2017.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization, 2016.” (Incomplete reference data)","The review is highly critical of the paper, pointing out a lack of clarity, originality, and scope. The reviewer finds the paper difficult to understand and believes it is not relevant to the conference. The language used, while direct, is professional and within the bounds of academic criticism. There are no personal attacks or disrespectful remarks. The reviewer focuses on the paper's shortcomings and provides constructive feedback for improvement.",-75,50
"This paper proposes an iterative approach to train deep neural networks based on morphism of the network structure into more complex ones. The ideas are rather simple, but could be potentially important for improving the performance of the networks. On the other hand, it seems that an important part of the work has already been done before (in particular Wei et al. 2016), and that the differences from there are very ad-hoc and intuition for why they work is not present. Instead, the paper justifies its approach by arguing that the experimental results are good. Personally, I am skeptical with that, because interesting ideas with great added value usually have some cool intuition behind them. The paper is easy to read, and there does not seem to exist major errors. Because I am not an active researcher in the topic, I cannot judge if the benefits that are shown in the experiments are enough for publication (the theoretical part is not the strongest of the paper).","The reviewer acknowledges the potential importance of the paper's ideas but expresses skepticism due to lack of strong theoretical justification and novelty. The reviewer finds the experimental justification insufficient, indicating a somewhat negative sentiment. However, the language remains professional and courteous throughout.",-20,60
"The paper presents a combination of evolutionary computation (EC) and variational EM for models with binary latent variables represented via a particle-based approximation.

The scope of the paper is quite narrow as the proposed method is only applicable to very specialised models. Furthermore, the authors do not seem to present any realistic modelling problems where the proposed approach would clearly advance the state of the art. There are no empirical comparisons with state of the art, only between different variants of the proposed method.

Because of these limitations, I do not think the paper can be considered for acceptance.

Detailed comments:

1. When revising the paper for next submission, please make the title more specific. Papers with very broad titles that only solve a very small part of the problem are very annoying.

2. Your use of crossover operators seems quite unimaginative. Genomes have a linear order but in the case of 2D images you use it is not obvious how that should be mapped to 1D. Combining crossovers in different representations or 2D crossovers might fit your problem much better.

3. Please present a real learning problem where your approach advances state of the art.

4. For the results in Fig. 7, please run the algorithm until convergence or justify why that is not necessary.

5. Please clarify the notation: what is the difference between y^n and y^(n)?
","The review is mainly negative. The reviewer finds the paper's scope too narrow, lacking in realistic applications and comparisons to existing methods. While the reviewer provides constructive feedback, the overall tone, particularly in the first paragraph and point 1, leans towards negativity. The language used is direct and somewhat critical, but not overly rude.",-60,-20
"This paper learns to construct masks and feature representations from an input image, in order to represent objects. This is applied to the relatively simple domain of Atari games video input (compared to natural images). The paper is completely inadequate in respect to related work; it re-invents known techniques like non-maximum suppression and matching for tracking; fails to learn convincing objects according to visual inspection; and fails to compare with earlier methods for these tasks. (The comment above about re-invention is the most charitable intepretation -- the worst case would be using these ideas without citation.)


1) The related work section is outrageous, containing no references before 2016.  Do the authors think researchers never tried to do this task before then? This is the bad side of the recent deep nets hype, and ICLR is particularly susceptible to this. Examples include

@article{wang-adelson-94,
  author        = ""Wang,  J. Y. A. and Adelson, E. H."",
  title         = {{Representing Moving Images with Layers}},
  journal       = {{IEEE Transactions on Image Processing}},
  year          = ""1994"",
  volume        = ""3(5)"",
  pages         = {625-638}
}
see http://persci.mit.edu/pub_pdfs/wang_tr279.pdf

and

@article{frey-jojic-03,
   author    = {Frey, B. J. and Jojic, N.},
   title     = {{Transformation Invariant Clustering Using the EM Algorithm}},
   journal   = {IEEE Trans Pattern Analysis and Machine Intelligence},
   year      = {2003},
   volume    = {25(1)},
   pages     = {1-17}
}
where mask and appearances for each object of interest are learned. There is a literature which follows on from the F&J paper.  The methods used in Frey & Jojic are different from what is proposed in the paper, but there needs to be comparisons.

The AIR paper also contains references to relevant previous work.

2) p 3 center -- this seems to be reinventing non-maximum suppression

3) p 4 eq 3 and sec 3.2 -- please justify *why* it makes sense to use
the concrete transform.  Can you explain better (e.g. in the supp mat)
the effect of this for different values of q_i?

4) Sec 3.5 Matching objects in successive frames using the Hungarian 
algorithm is also well known, e.g. it is in the matlab function
assignDetectionsToTracks .

5) Overall: in this paper the authors come up with a method for learning objects from Atari games video input. This is a greatly restricted setting compared to real images. The objects learned as shown in Appendix A are quite unconvincing, e.g. on p 9. For example for Boxing why are the black and white objects broken up into 3 pieces, and why do they appear coloured in col 4?

Also the paper lacks comparisons to other methods (including ones from before 2016) which have tackled this problem.

It may be that the methods in this paper can outperform previous ones -- that would be interesting, but it would need a lot of work to address the issues raised above.

Text corrections:

p 2 ""we are more precise"" -> ""we give more details""

p 3 and p 2 -- local maximum (not maxima) for a single maximum.  [occurs many times]
","The review is extremely negative about the paper. The reviewer states that the paper ""re-invents known techniques"" and ""fails to compare with earlier methods."" They call the related work section ""outrageous"" for not citing anything before 2016. The reviewer also finds the results unconvincing. While the language is harsh, it doesn't contain personal attacks or insults, focusing instead on the scientific shortcomings of the work.",-80,-20
"This paper describes improvements to a system described in a blog post for musical style transfer.  Such a system is difficult to evaluate, but examples are presented where the style of one song is applied to the content of another.  These audio examples show that the system produces somewhat reasonable mixtures of two songs, but suggest that if the system instead followed the (mostly) accepted rules for cover song generation, it could make the output much more pleasant to listen to.  Additional evaluation includes measuring correlations between style songs and the output to ensure that it is not being used directly as well as some sort of measure of key invariance that is difficult to interpret.  The paper does not completely define the mathematical formulation of the system, making it difficult to understand what is really going on.

The current paper changes the timbre, rhythm, and harmony of the target content song.  Changing the harmony is problematic as it can end up clashing with the generated melody or just change the listener's perception of which song it is.  I suggest instead attempting to generate a cover version of the content song in the style of the style song. Cover songs are re-performances of an existing (popular) song by another artist.  For example, Jimi Hendrix covered Bob Dylan's ""All along the watchtower"" and the Hendrix version became more popular than the original.  This is essentially artist A performing a song by artist B, which is very similar to the goal of the current paper.  Cover songs almost always maintain the lyrics, melody, and harmony of the original, while changing the timbre, vocal style, tempo, and rhythmic information.  This seems like a good way to structure the problem of musical style transfer.  Many systems exist for identifying cover songs, see the relevant publications at the International Society for Music Information Retrieval (ISMIR) Conference.  Few systems do something with cover songs after they have been identified, but they could be used for training a system like the one proposed here, if it could be trained. 

Another musically questionable operation is pooling across frequency in the constant-Q transform representation.  In western music, adjacent notes are very different from one another and are usually not played in the same key, for example C and C#.  Thus, pooling them together to say that one of them is present seems to lose useful information.  As part of the pooling discussion, the paper includes an investigation of the key-invariance of the model.  Results from this are shown in figure 5, but it is difficult to interpret this figure.  What units is the mean squared error measured in?  What would be a big value?  What would be a small value?  What aspects of figure 5 specifically ""confirm that changing key between style [and] content has less of an effect on our proposed key-invariant content representations""?

Section 3.1, which describes the specifics of the model, is confusing.  What exactly are S, C, W, and G?  What are their two dimensions indexed by i and j?  How do you compute them from the input?  Which parameters in this model are learned and which are just calculated?  Is there any training or is L(X,C,S) just optimized at test time?

Finally, the evaluation of the texture generation part of the algorithm could be compared to existing texture generation algorithms (there are several) such as McDermott & Simoncelli (2011, NEURON), which even has code available online.



Minor comments
--------------

p2: ""in the this work"" typo

p2: ""an highly"" typo

p2: ""The first method... The latter class of methods"" confusing wording.  Is the second one a different method or referring back to the previous method?  If it's different, say ""The second method...""

p7: Please describe kernel sizes in real units (e.g., ms, Hz, cents) as well as numbers of bins



After revision/response
--------------------------------
The revisions of the paper have made it clearer as to what is going on, although the description of the algorithm itself could still be described more mathematically to really make it clear.  It is more clear what's going on in figure 5, although it could also be further clarified whether the green bars are showing the distance between log magnitude STFTs of the transposed ""style"" snippets and the untransposed ""content"" snippets directly and so provide an upper bound on the distances. My overall rating of the paper has not changed.","The reviewer acknowledges the work and finds the audio examples ""somewhat reasonable,"" suggesting a slightly positive sentiment. However, they also express significant concerns about the methodology and evaluation, pulling the sentiment towards neutral. The language is quite direct with strong suggestions but maintains a professional and respectful tone overall.",20,60
"
This paper considers hyperparameter searches in which all of the
candidate points are selected in advance.  The most common approaches
are uniform random search and grid search, but more recently
low-discrepancy sequences have sometimes been used to try to achieve
better coverage of the space.  This paper proposes using a variant of
the determinantal point process, the k-DPP to select these points.
The idea is that the DPP provides an alternative form of diversity to
low-discrepancy sequences.

Some issues I have with this paper:

1. Why a DPP? It's pretty heavyweight. Why not use any of the other
(potentially cheaper) repulsive point processes that also achieve
diversity?  Is there anything special about it that justifies this
work?

2. What about all of the literature on space-filling designs, e.g.,
latin hypercube designs?  Statisticians have thought about this for a
long time.

3. The motivation for not using low-discrepancy sequences was discrete
hyperparameters.  In practice, people just chop up the space or round.
Is a simple kernel with one length scale on a one-hot coding adding
value? In this setup, each parameter can only contribute ""same or
different"" to the diversity assessment.  In any case, the evaluations
didn't have any discrete parameters.  Given that the discrete setting
was the motivation for the DPP over LDS, it seems strange not to even
look at that case.

4. How do you propose handling ordinal variables? They're a common
case of discrete variables but it wouldn't be sensible to use a
one-hot coding.

5. Why no low discrepancy sequence in the experimental evaluation of
section 5?  Since there's no discrete parameters, I don't see what the
limitation is.

6. Why not evaluate any other low discrepancy sequences than Sobol?

7. I didn't understand the novelty of the MCMC method relative to
vanilla M-H updates.  It seems out of place.

8. The figures really need error bars --- Figure 3 in particular.  Are
these differences statistically significant?
","The reviewer raises several valid concerns about the paper's novelty, methodology, and lack of comparisons. They question the choice of DPP and its justification compared to simpler methods. The reviewer also points out missing aspects like handling ordinal variables, comparisons with other low-discrepancy sequences, and the lack of error bars in the results. The tone, while direct and critical, maintains a professional and constructive approach by asking for clarifications and suggesting improvements. There's no use of harsh language or personal attacks.",-20,60
"The paper proposes to use LSH to approximate softmax, which greatly speeds up classification with large output space. The paper is overall well-written. However, similar ideas have been proposed before, such as ""Deep networks with large output spaces"" by Vijayanarasimhan et. al. (ICLR 2015). And this manuscript does not provide any comparison to any of those similar methods.

A few questions about the implementation,
(1) As stated in the manuscript, the proposed method contains three steps, hashing, lookup and distance. GPU is not good at lookup, so the manuscript proposes to do lookup on CPU. Does that mean the data should go back and forth between CPU and GPU? Would this significantly increase the overhead?
(2) At page 6, the LSH structure returns m list of C candidates. Is it a typo? C is the total number of classes. And how do you guarantee that each LSH query returns the same amount of candidates?

Experiment-wise, the manuscript leaves something to be desired.
(1) More baselines be evaluated and compared. In this manuscript, only IS and NS are compared. And pure negative sampling is actually rarely used in language modeling. In addition to Vijayanarasimhan's LSH method, there are also a few other methods out there, such as hierarchical softmax, NCE, D-sothat ftmax (""Strategies for Training Large Vocabulary Neural Language Models"" by Chen et. al. ACL 2016), adaptive softmax (""Efficient softmax approximation for GPUs"" by Grave et. al).
(2) The results of the proposed method is not impressive. D-softmax and adaptive softmax can achieve 147 ppl on text 8 with 512 hidden units as described in other paper, while the proposed method can only achieve 224 ppl with 650 hidden units. Even the exact softmax have large difference in ppl. It looks like the authors do not tune the hyper-parameters well. With this suboptimal setting, it is hard to judge the significance of this manuscript.
(3) Why one billion word dataset is used in eval but not used for training? It is one of the best datasets to test the scalability of language models.
(4) We can see, as reported in the manuscript, that NS has bigger speedup than the proposed method. So it would be nice to show ppl vs time curve for all methods. Eventually, what we want is the best model given a fixed amount of training time. With the same amount of epochs, NS loses the advantage of being faster.","The review starts with a positive sentiment, acknowledging the paper's contribution and clarity. However, it quickly shifts to a more critical tone, pointing out the lack of novelty and comparison with existing methods. The reviewer then raises several specific concerns about the implementation, experimental setup, and results, suggesting that the paper's claims are not fully substantiated. The language used is professional and objective, but the overall tone conveys a sense of skepticism and suggests significant revisions are needed.",20,60
"The authors propose a method for reducing the computational burden when performing inference in deep neural networks. The method is based a previously-developed approach called incomplete dot products, which works by pruning some of the inputs in the dot products via the introduction of pre-specified coefficients. The authors of this paper extend the method by introducing a task-wise learning procedure that sequentially optimizes a loss function for decreasing percentage of included features in the dot product. 

Unfortunately, this paper was hard to follow for someone who does not actively work in this field, making it hard to judge if the contribution is significant or not. While the description of the problem itself is adequate, when it comes to describing the TESLA procedure and the alternative training procedure, the relevant passages are, in my opinion, too vague to allow other researchers to implement this procedure.

Positive points:
- The application seems relevant, and the task-wise procedure seems like an improvement over the original IDP proposal.
- Application to two well-known benchmarking datasets.

Negative points:
- The method is not described in sufficient detail to allow reproducibility, the algorithms are no more than sketches.
- It is not clear to me what the advantage of this approach is, as opposed to alternative ways of compressing the network (e.g. via group lasso regularization), or training an emulator on the full model for each task.

Minor point:
- Figure 1 is unclear and requires a better caption. ","The reviewer provides both positive and negative feedback, ultimately stating uncertainty about the significance of the contribution due to a lack of clarity in the methodology. While acknowledging the potential relevance and improvement over previous work, the reviewer criticizes the paper for vagueness that hinders reproducibility. The language used is critical but professional, avoiding personal attacks or disrespectful tones.",-10,50
"Overview:
The paper aim to model non-linear, intrinsically low-dimensional structure, in data by estimating ""transport operators"" that predict how points move along the manifold. This is an old idea, and the stated contribution of the paper is:
""The main contribution of this paper is to show that the manifold representation learned in the transport operators is valuable both as a probabilistic model to improve general machine learning tasks as well as for performing transfer learning in classification tasks."" 
The paper provide nice illustrative experiments arguing why transport operators may be a useful modeling tool, but does not go beyond illustrative experiments.
While I follow the intuitions behind transport operators I am doubtful if they will generalize beyond very simple manifold structures (see detailed comments below).

Quality:
The paper is well-written and fairly easy to follow. In particular, I appreciate that the authors make no attempt to overclaim contributions. From a methodology point-of-view, the paper has limited novelty (transport operators, and learning thereof has been studied elsewhere), but there are some technical insights (likelihood model, use in data augmentation). Since the provided experiments are mostly illustrations, I would argue that the significance of the paper is limited. I'd say that to really convince a broader audience that this old idea is worth revisiting, the work must go beyond illustrations and apply to a real data problem.

Detailed Comments and Questions:
*) Equation 1 of the paper describe the key dynamics of the applied transport operators. Basically, the paper assume that the underlying data manifold is locally governed by a linear differential equation. This is a very suitable assumption, e.g., for the swiss roll data set, but it is unclear to this reader why it is a suitable assumption beyond such toy data. I would very much appreciate a detailed discussion of when this is a suitable modeling choice, and when it is not. My intuition is that this is mostly a suitable model when the data manifold appears due to simple transformations (e.g. rotations) of data. This is also exactly the type of data considered in the paper.
*) In Eq. 3, should it be ""expm"" instead of ""exp"" ?
*) The first two paragraphs of Sec. 2 are background material, whereas paragraph 3 and beyond describe material that is key to the paper. I would recommend introducing a \subsection (or something like it) to make this more clear.
*) The idea of working with transformations of data rather than the actual data is the corner-stone of Ulf Grenander's renowned ""Pattern Theory"". A citation to this seminal work would be appropriate.
*) In the first paragraph of the introduction links are drawn to the neuroscience literature; it would be appropriate to cite a suitable publication.

Pros(+) & Cons(-):
+ Well-written.
+ Good illustrative experiments.
- Real-life experiments are lacking.
- Limited methodology contribution.
- The assumed dynamics might be too simplistic (at least a discussing of this is missing).

For the AC:
The submitted paper acknowledges several grants (including grant numbers), which can directly be tied to the authors identity. This may be a violation of the double blind review policy. I did not use this information to determine the authors identity, though, so this review is still double blind.

Post-rebuttal comments:
The paper has improved with the incorporated revisions, but my main concerns remain. I find the Swiss Roll / rotated-USPS examples to be too contrived as the dynamics are exactly tailored to the linear ODE assumption. These are examples where the model assumptions are perfect. What is unclear is how the model behaves when the linear ODE assumption is not-quite-correct-but-also-not-totally-incorrect, i.e. how the model behaves in real life. I didn't get that from the newly added experiment. So, I'll keep my rating as is. ","The reviewer acknowledges the paper's clarity and the value of the illustrative experiments. However, they express concerns about the limited generalizability of the proposed method beyond simple manifold structures and the lack of real-life experiments. The reviewer finds the core assumption (locally linear data manifold) to be potentially limiting. The overall tone suggests the reviewer leans towards skepticism, but they are constructive in their criticism and offer specific suggestions for improvement. Therefore, the sentiment is somewhat negative but not overly so.",-20,70
"This paper presents a new reinforcement learning approach to handle environments with a mix of discrete and
continuous action spaces. The authors propose a parameterized deep Q-network (P-DQN) and leverage learning
schemes from existing algorithms such as DQN and DDPG to train the network. The proposed loss function and
alternating optimization of the parameters are pretty intuitive and easy to follow. My main concern is
with lack of sufficient depth in empirical evaluation and analysis of the method.

Pros:
1. The setup is an interesting and practically useful one to investigate. Many real-world environments require individual actions
 that are further parameterized over a continuous space.
2. The proposed method is simple and intuitive.

Cons:
1. The evaluation is performed only on a single environment in a restricted fashion. I understand the authors are restricted in the choice of environments which require a hybrid action space. However,
 even domains like Atari could be used in a setting where the continuous parameter x_k refers to the number of
 repetitions for action k. This is similar to the work of Lakshminarayanan et al. (2017). Could you test your algorithm in such a setting?
2. Comparison of the algorithm is performed only against DDPG. Have you tried other options like PPO (Schulman et al., 2017)?
 Also, considering that the action space is simplified in the experimental setup (""we use the default parameters of skills provided by the game environment, usually pointing to
the opponent hero's location""), with only the move(\alpha) action being a hybrid, one could imagine discretizing the move
direction \alpha and training a DQN (or any other algorithms over discrete action spaces) as another baseline.
3. The reward structure seems to be highly engineered. With so many components in the reward, it is not clear
what the individual contributions are and what policies are actually learned.
4. The authors don't provide any analysis of the empirical results. Do the P-DQN and DDPG converge to the same policy?
What factor(s) contribute most to the faster learning of P-DQN? Do the values of \alpha and \beta for the two-timescale
updates affect the results considerably?
5. (minor) The writing contains a lot of grammatical errors which makes this draft below par for an ICLR paper.


Other Questions:
1. In eq. 5.3, the loss over \theta is defined as the sum of Q values over different k. Did you try other formulations of
the loss? (say, product of the Q values for instance) One potential issue with the sum could be that if some values of k dominate this sum, Q(s, k, x_k; w) might not be maximized for all k.
2. Some terms of the reward function seem to be overly dependent on historic actions (ex. difference in gold and hitpoints). This could swamp the
influence of the other terms which are more dependent on the current action a_t, which might be an issue, especially with the Markovian assumption?

References:
 Lakshminarayanan et al, 2017; Dynamic Action Repetition for Deep Reinforcement Learning; AAAI
 Schulman et al., 2017; Proximal Policy Optimization Algorithms; Arxiv
","The review acknowledges the paper's merits, such as the interesting problem setup and intuitive method (Pros: 1, 2). However, it expresses significant concerns about the evaluation's depth, including limited environments, missing baselines, and lack of analysis (Cons: 1, 2, 3, 4). The reviewer also points out writing issues (Con: 5). While the reviewer provides constructive criticism and suggestions for improvement, the numerous and substantial concerns suggest a generally lukewarm reception to the paper.",20,70
"The paper proposes training neural networks using a trust region method, in which at each iteration a (non-convex) quadratic approximation of the objective function is found, and the minimizer of this quadratic within a fixed radius is chosen as the next iterate, with the radius of the trust region growing or shrinking at each iteration based on how closely the gains of the quadratic approximation matched those observed on the objective function. The authors claim that this approach is better at avoiding ""narrow"" local optima, and therefore will tend to generalize better than minibatched SGD. The main novelty seems to be algorithm 2, which finds the minimizer of the quadratic approximation within the trust region by performing GD iterations until the boundary is hit (if it is--it might not, if the quadratic is convex), and then Riemannian GD along the boundary.

The paper contains several grammatical mistakes, and in my opinion could explain things more clearly, particularly when arguing that the algorithm 2 will converge. I had particular difficulty accepting that the phase 1 GD iterates would never hit the boundary if the quadratic was strongly convex, although I accept that it is true due to the careful choice of step size and initialization (assumptions 1 and 2).

The central claim of the paper, that a trust region method will be better at avoiding narrow basins, seems plausible, since if the trust region is sufficiently large then it will simply pass straight over them. But if this is the case, wouldn't that imply that the quadratic approximation to the objective function is poor, and therefore that line 5 of algorithm 1 should shrink the trust region radius? Additionally, at some times the authors seem to indicate that the trust region method should be good at escaping from narrow basins (as opposed to avoiding them in the first place), see for example the left plot of figure 4. I don't see why this is true--the quadratic approximation would be likely to capture the narrow basin only.

This skepticism aside, the experiments in figure 2 do clearly show that, while the proposed approach doesn't converge nearly as quickly as SGD in terms of training loss, it does ultimately find a solution that generalizes better, as long as both SGD and TR use the same batch size (but I don't see why they should be using the same batch size). How does SGD with a batch size of 1 compare to TR with the batch sizes of 512 (CIFAR10) or 1024 (STL10)?

Section 4.3 (Figure 3) contain a very nice experiment that I think directly explores this issue, and seems to show that SGD with a batch size of 64 generalizes better than TR at any of the considered batch sizes (but not as well as the proposed TR+SGD hybrid). Furthermore, 64 was the smallest batch size considered, but SGD was performing monotonically better as the batch size decreased, so one would expect it to be still better for 32, 16, etc.

Smaller comments:

You say that you base the Hessian and gradient estimates on minibatched samples. I assume that the same is true for the evaluations of F on line 4 of Algorithm 1? Do these all use the same minibatch, at each iteration?

On the top of page 3: ""M is the matrix size"". Is this the number of elements, or the number of rows/columns?

Lemma 1: This looks correct to me, but are these the KKT conditions, which I understand to be first order optimality conditions (these are second order)? You cite Nocedal & Wright, but could you please provide a page number (or at least a chapter)?

On the top of page 5, ""Line 10 of Algorithm 1"": I think you mean Line 11 of Algorithm 2.","The review is quite balanced, leaning slightly towards the critical side. It acknowledges the potential merits of the paper's idea and the positive results in some experiments but also raises several concerns and questions about the approach and clarity. The reviewer maintains a professional and respectful tone throughout, even when expressing skepticism or pointing out flaws. There are no personal attacks or dismissive language. The critique focuses on the scientific content and presentation. Therefore, the sentiment is slightly negative, and the politeness is neutral to slightly positive.",-20,70
"The paper presents a method for feature projection which uses a two level neural network like structure to generate new features from the input features. The weights of the NN like structure are optimised using a genetic search algorithm which optimises the cross-validation error of a nearest neighbor classifier. The method is tested on four simple UCI datasets. There is nothing interesting or novel about the paper. It is not clear whether the GA optimisation takes place on the level of cross validation error estimation or within an internal validation set as it should have been the case. The very high accuracies reported seem to hint the latter, which is a serious methodological error. The poor language and presentation does not help in clearing that, as it does not help in general. ","The review starts with a neutral factual summary but quickly turns very negative.  Phrases like ""nothing interesting or novel"", ""serious methodological error"", and ""poor language and presentation""  indicate a highly critical stance. The reviewer also questions the methodology with skepticism. While direct, the language avoids overtly rude or unprofessional terms.",-85,-20
"The authors present a method that aims to remove domain-specific information while preserving the relevant biological information between biological data measured in different experiments or ""batches"". A network is trained to learn the transformations that minimize the Wasserstein distance between distributions. The wasserstein distance is also called the ""earth mover distance"" and is traditionally formulated as the cost it takes for an optimal transport plan to move one distribution to another. In this paper they have a neural network compute the wasserstein distance using a different formulation that was used in Arjovsky et al. 2017, finds a lipschitz function f, which shows the maximal difference when evaluated on samples from the two distributions. Here these functions are formulated as affine transforms of the data with parameters theta that are computed by a neural network. Results are examined mainly by looking at the first two PCA components of the data.  


The paper presents an interesting idea and is fairly well written. However I have a few concerns:
1. Most of the ideas presented in the paper rely on works by Arjovsky et al. (2017), Gulrajani et al. (2017), and Gulrajani et al. (2017). Some selections, which are presented in the papers are not explained, for example, the gradient penalty, the choice of \lambda and the choice of points for gradient computation.
2. The experimental results are not fully convincing, they simply compare the first two PC components on this Broad Bioimage benchmark collection. This section could be improved by demonstrating the approach on more datasets.
3. There is a lack comparison to other methods such as Shaham et al. (2017). Why is using earth mover distance better than MMD based distance? They only compare it to a method named CORAL and to Typical
Variation Normalization (TVN). What about comparison to other batch normalization methods in biology such as SEURAT? 
4. Why is the affine transform assumption valid in biology? There can definitely be non-linear effects that are different between batches, such as ion detection efficiency differences. 
5. Only early stopping seems to constrain their model to be near identity. Doesn't this also prevent optimal results ? How does this compare to the near-identity constraints in resnets in Shaham et al. ?

","The reviewer starts with positive aspects by acknowledging the interesting idea and well-written manuscript. However, they express several concerns about the technical choices, limited experimental validation, lack of comparison with relevant methods, and questionable assumptions. The criticism, while direct, is presented in a professional and constructive manner, suggesting improvements rather than simply dismissing the work.  Therefore, the sentiment leans slightly positive due to the initial praise, but the numerous concerns pull it towards neutral. The language remains polite throughout, maintaining a professional academic tone.",20,70
"Strengths:
1. This paper proposes a novel method called Concept Activation Vectors (CAV) which facilitates interpretability of neural networks by explaining how much a specific concept influences model predictions. 
2. The proposed method tries to incorporate multiple desiderata, namely, accessibility to non ML experts, customizability w.r.t. being able to explain any concept of interest, plug-in readiness i.e., providing explanations
without requiring retraining of the model. 

Weaknesses:
1. While this work is conceptually interesting, the technical novelty and contributions seem fairly minimal. 
2. The presentation of this paper is one of its weakest points. The organization of the content is quite incoherent. The paper also makes a lot of claims (e.g., hypothesis testing) which are not really justified. 
3. The experimental evaluation of this paper is quite rudimentary. Lots of details are missing. 

Summary: This paper proposes a novel framework for explaining the functionality of neural networks by using a simple idea. The intuition behind the proposed approach is as follows: by using the weight vectors of linear classifiers, which take as inputs the activation layer outputs of a given neural network (NN) model and predict the concepts of interest, we can understand the influence of specific concepts of interest on the NN model behavior. The authors claim that this simple approach can be quite useful in providing explanations that can be useful for a variety of purposes including testing specific hypothesis which is never really demonstrated or explained well in the paper. Furthermore, lot of details are lacking in both the experimentation section and the methods section (detailed comments below). The experiments also do not correspond well to the claims made in the introduction and abstract. This paper is also very hard to read which makes understanding the proposed method and other details quite challenging. 

Novelty: The novelty of this paper mainly stems from its proposed method of using prototypes which serve as positive and negative examples w.r.t. a specific concept, and leveraging the weight vectors obtained when predicting the positive/negative classes using activation layer outputs to understand the influence of concepts of interest.  The technical novelty of the proposed approach is fairly minimal. The experiments also do not support a lot of novelty claims made about the proposed approach. 

Other detailed comments:
1. I would first encourage the authors to improve the overall presentation and organization of this paper. 
2. Please add some intuition about the approach in the introduction. Also, please be succinct in explaining what kind of interpretability is provided by the explanations. I would advise the authors to refrain from making very broad claims and using words such as hypothesis testing without discussing them in detail later in the paper. 
3. Sections 2.3 and 2.4 are quite confusing and can probably be organized and titled differently. In fact, I would advise the authors to structure related work as i. inherently interpretable models ii. global explanations 
iii. local explanations iv. neuron level investigation methods. Highlight how existing methods do not incorporate plug-in readiness and/or other desiderate wherever appropriate within these subsections. 
4. Additional related work on inherently interpretable models and global explanations: 
i. Interpretable classifiers using rules and Bayesian analysis, Annals of Applied Statistics, 2015
ii. Interpretable Decision Sets: A joint framework for description and prediction, KDD, 2016
iii. A Bayesian Framework for Learning Rule Sets for Interpretable Classification, JMLR, 2017
iv. Interpretable and Explorable Explanations of Black Box Models, FAT ML, 2017
5. In section 3, clearly identify what are the inputs and outputs of your method. Also, clearly highlight the various ways in which outputs of your method can be used to understand the model behavior. While Secction 3.2 and 3.3 attempt to describe how the CAV can be used to explain the model behavior, the presentation in these sections can be improved. 
6. I think the experimental sections suffers from the following shortcomings: i. it does not substantiate all the claims made in the introduction ii. some of the details about which layer outputs are being studied are missing through out the section. 

Overall, while this paper proposes some interesting ideas, I think it can be improved significantly in terms of its clarity, claims, and evaluation.  
","The review acknowledges the interesting idea but highlights significant weaknesses in technical novelty, presentation, and experimental validation. It provides constructive criticism with detailed suggestions for improvement. The language is formal and professional, without resorting to harsh or disrespectful tones.",-20,80
"The paper compares some recently proposed method for validation of properties
of piece-wise linear neural networks and claims to propose a novel method for
the same. Unfortunately, the proposed ""branch and bound method"" does not explain
how to implement the ""bound"" part (""compute lower bound"") -- and has been used 
several times in the same application, incl.:

Ruediger Ehlers. Planet. https://github.com/progirep/planet,
Chih-Hong Cheng, Georg Nuhrenberg, and Harald Ruess.  Maximum resilience of artificial neural networks. Automated Technology for Verification and Analysis
Alessio Lomuscio and Lalit Maganti.  An approach to reachability analysis for feed-forward relu neural networks. arXiv:1706.07351

Specifically, the authors say: ""In our experiments, we use the result of 
minimising the variable corresponding to the output of the network, subject 
to the constraints of the linear approximation introduced by Ehlers (2017a)""
which sounds a bit like using linear programming relaxations, which is what
the approaches using branch and bound cited above use. If that is the case,
the paper does not have any original contribution. If that is not the case,
the authors may have some contribution to make, but have not made it in this
paper, as it does not explain the lower bound computation other than the one
based on LPs.

Generally, I find a jarring mis-fit between the motivation (deep learning
for driving, presumably involving millions or billions of parameters) and
the actual reach of the methods proposed (hundreds of parameters).
This reach is NOT inherent in integer programming, per se. Modern solvers
routinely solve instances with tens of millions of non-zeros in the constraint
matrix, but require a strong relaxation. The authors may hence consider
improving the LP relaxation, noting that the big-M constraint are notorious
for producing weak relaxations.","The review is critical of the paper's lack of originality and limited practical relevance. The reviewer points out that the proposed method seems to be a rehash of existing techniques and questions the paper's contribution. The reviewer also criticizes the gap between the paper's ambitious motivation and the limited scale of the proposed method. While the reviewer uses strong language like ""jarring mis-fit"" and ""does not have any original contribution,"" the criticism is grounded in specific technical points and presented in a professional manner. ",-50,50
"quality: this paper is of good quality
clarity: this paper is very clear
originality: this paper combines original ideas with existing approaches for pruning to obtain dramatic space reduction in NN parameters.
significance: this paper seems significant.

PROS
- a new approach to sparsifying that considers different thresholds for each layer
- a systematic, empirical method to obtain optimal sparsity levels for a given neural network on a task.
- Very interesting and extensive experiments that validate the reasoning behind the described approach, with a detailed analysis of each step of the algorithm.

CONS
- Pruning time. Although the authors argue that the pruning algorithm is not prohibitive, I would argue that >1 month to prune LeNet-5 for MNIST is certainly daunting in many settings. It would benefit the experimental section to use another dataset than MNIST (e.g. CIFAR-10) for the image recognition experiment.
- It is unclear whether this approach will always work well; for some neural nets, the currently used sparsification method (thresholding) may not perform well, leading to very little final sparsification to maintain good performance.
- The search for the optimal sparsity in each level seems akin to a brute-force search. Although possibly inevitable, it would be valuable to discuss whether or not this approach can be refined.

Main questions
- You mention removing ""unimportant and redundant weights"" in the pruning step; in this case, do unimportant and redundant have the same meaning (smaller than a given threshold), or does redundancy have another meaning (e.g. (Mariet, Sra, 2016))?
- Algorithm 1 finds the best sparsity for a given layer that maintains a certain accuracy. Have you tried using a binary search for the best sparsity instead of simply decreasing the sparsity by 1% at each step? If there is a simple correlation between sparsity and accuracy, that might be faster; if there isn't (which would be believable given the complexity of neural nets), it would be valuable to confirm this with an experiment.
- Have you tried other pruning methods than thresholding to decide on the optimal sparsity in each layer?
- Could you please report the final accuracy of both models in Table 2?

Nitpicks:
- paragraph break in page 4 would be helpful.","The review is overall positive. The reviewer finds the paper interesting, significant, and well-written. They praise the approach, systematic methodology, and detailed experiments. While they raise valid concerns about pruning time, generalizability, and the search method's efficiency, these are presented constructively as areas for improvement rather than outright criticisms. The tone is professional and polite throughout, suggesting a willingness to see the paper published with revisions.",70,80
"The authors consider the problem of ultra-low precision neural networks motivated by 
limited computation and bandwidth. Their approach first posits a Bayesian neural network
a discrete prior on the weights followed by central limit approximations to efficiently 
approximate the likelihood. The authors propose several tricks like normalization and cost 
rescaling to help performance. They compare their results on several versions of MNIST. The 
paper is promising, but I have several questions:

1) One major concern is that the experimental results are only on MNIST. It's important 
to have another (larger) dataset to understand how sensitive the approach is to 
characteristics of the data. It seems plausible that a more difficulty problem may 
require more precision.

2) Likelihood weighting is related to annealing and variational tempering

3) The structure of the paper could be improved:
 - The introduction contains way too many details about the method 
    and related work without a clear boundary.
 - I would add the model up front at the start of section 2
 - Section 2.1 could be reversed or equations 2-5 could be broken with text 
   explaining each choice 

4) What does training time look like? Is the Bayesian optimization necessary?","The review starts with positive wording like ""promising"" and acknowledges the paper's novelty. However, it also raises several significant concerns and suggestions for improvement, indicating that the reviewer sees potential but has reservations. Therefore, the sentiment is slightly positive but closer to neutral. The language used is constructive and professional throughout, suggesting a polite and respectful tone.",20,80
"The paper proposes a new method to train knowledge base embeddings using a least-squares loss. For this purpose, the paper introduces a reweighting scheme of the entries in the original adjacency tensor. The reweighting is derived from an analysis of the cross-entropy loss. In addition, the paper discusses the connections of the margin and cross-entropy loss and evaluates the proposed method on WN18 and FB15k.

 The paper tackles an interesting problem, as learning from knowledge bases via embedding methods has become increasingly important for tasks such as question answering. Providing additional insight into current methods can be an important contribution to advance the state-of-the-art.

However, I'm concerned about several aspects in the current form of the paper. For instance, the derivation in Section 4 is unclear to me, as eq.4 suddenly introduces a weighted sum over expectations using the degrees of nodes. The derivation also seems to rely on a very specific negative sampling assumption (uniform sampling without checking whether the corrupted triple is a true negative). This sampling method isn't used consistently across models and also brings its own problems, e.g., see the LCWA discussion in [4]

In addition, the semantics that are introduced by the weighting scheme are not clear to me either. Using the proposed method, the probability of edges between high-degree nodes are down-weighted, since the ground-truth labels are divided by the node degrees. Since these weighted labels are then fitted using a least-squares loss, this implies that links between high-degree nodes should be less likely, which seems the opposite of what the scores should look like.

With regard to the significance of the contributions: Using a least-squares loss in combination with tensor methods is attractive because it enables ALS algorithms with closed-form updates that can be computed very fast. However, the proposed method still relies on SGD optimization. In this context, it is not clear to me why a tensor framework/least-squares loss would be preferable.

Further comments:
- The paper seems to equate ""tensor method"" with using a least squares loss. However, this doesn't have to be the case. For instance see [1,2] which propose Logistic and Poisson tensor factorizations, respectively.
- The distinction between tensor factorization and neural methods is unclear. Tensor factorization can be interpreted just as a particular scoring function. For instance, see [5] for a detailed discussion.
- The margin based ranking loss has been proposed earlier than in (Collobert et al, 2011). For instance see [3]
- p1: corrupted triples are not described entirely correct, typically only one of s or o is corrputed. 
- Closed-form tensor in Table 1: This should be least-squares loss of f(s,p,o) and log(...)?
- p6: Adding the constant to the tensor as proposed in (Levy & Goldberg, 2014) can done while gathering the minibatch and is therefore equivalent to the proposed approach.

[1] Nickel et al: Logistic Tensor Factorization for Multi-Relational Data, 2013.
[2] Chi et al: ""On tensors, sparsity, and nonnegative factorizations"", 2012
[3] Collobert et al: A unified architecture for natural language processing, 2008
[4] Dong et al: Knowledge Vault: A Web-Scale Approach to Probabilistic Knowledge Fusion, 2014
[5] Nickel et al: A Review of Relational Machine Learning for Knowledge Graphs, 2016.","The reviewer acknowledges the relevance of the paper's topic and its potential but raises several concerns about the clarity of the derivation, the semantics of the weighting scheme, and the significance of the contributions. They find the derivation unclear, the weighting scheme's logic questionable, and the choice of a least-squares loss unconvincing. The reviewer also points out several factual inaccuracies and provides additional references to support their critique. The detailed critique, specific concerns, and additional references suggest a rather critical stance. However, the language remains professional and avoids harsh or disrespectful language.",-30,60
"The author present Autostacker, a new algorithm for combining the strength of different learning algorithms during hyper parameter search. During the first step, the hyperparameter search is done in a conventional way. At the second step, the output of each primitives is added to the features of the original dataset and the training and hyperparameter search starts again. This process is repeated for some number of steps. The experiments are performed on 15 small scale dataset and show that Autostacker is performing better  than random forest almost systematically and better than TPOT, the external baseline, 13 times out of 15. Also the speed comparison favor Autostacker vs TPOT.

This algorithm is not highly innovative. Using the output of some algorithms as the input of another one for learning was seen numerous time in the literature. The novelty here is how exactly it is performed, which is a bit ad hoc. 

While testing on numerous dataset is important to verify the strength of a learning algorithm, final statistical significance test should be provided e.g. Sign Test, Wilcoxon Signed Rank Test.

The experiment compares with a weak baseline and a baseline that is unknown to me. Also, the datasets are all small scale which is not representative of modern machine learning. This leaves me very uncertain about the actual quality of the proposed algorithm. 

The strength of the Random Forest baseline could easily be augmented by simply considering the best learning algorithm over validation across the hyper parameter search (i.e. the choice of the learning algorithm is also a hyperparameter). Also a very simple and fast ensemble could be considered by using Agnostic Bayesian Learning of Ensembles (Lacoste et. al.). It is also common to simply consider a linear combination of the output of the different estimator obtained during cross validation and very simple to implement. This would provide other interesting baselines.

Finally the writing of the paper could be highly improved. Many typos, including several badly formatted citations (consider using \citet and \citep for a proper usage of parenthesis).
","The review starts with a neutral tone, presenting a summary of the paper's content. However, it quickly transitions into a negative sentiment, criticizing the algorithm's lack of novelty, the experimental setup (weak baselines, small datasets), and the writing quality. The reviewer acknowledges some positive aspects, such as the speed comparison favoring Autostacker, but these are overshadowed by the negative points. The language used is critical but remains within the bounds of professional academic discourse, suggesting suggestions for improvement rather than resorting to personal attacks.",-50,50
"In this paper, the authors consider symmetric (3rd order) CP decomposition of a PPMI tensor M (from neighboring triplets), which they call CP-S. Additionally, they propose an extension JCP-S, for n-order tensor decompositions. This is then compared with random, word2vec, and NNSE, the latter of two which are matrix factorization based (or interpretable) methods. The method is shown to be superior in tasks of 3-way outlier detection, supervised analogy recovery, and sentiment analysis. Additionally, it is evaluated over the MEN and Mturk datasets.


For the JCP-S model, the loss function is unclear to me. L is defined for 3rd order tensors only;  how is the extended to n > 3? Intuitively it seems that L is redefined, and for, say, n = 4, the model is M(i,j,k,n) = \sum_1^R u_ir u_jr u_kr u_nr. However, the statement ""since we are using at most third order tensors in this work"" I am further confused. Is it just that JCP-S also incorporates 2nd order embeddings? I believe this requires clarification in the manuscript itself.

For the evaluations, there are no other tensor-based methods evaluated, although there exist several well-known tensor-based word embedding models existing:

Pengfei Liu, Xipeng Qiu∗ and Xuanjing Huang, Learning Context-Sensitive Word Embeddings with Neural Tensor Skip-Gram Model,  IJCAI 2015

Jingwei Zhang and Jeremy Salwen, Michael Glass and Alfio Gliozzo. Word Semantic Representations using Bayesian Probabilistic Tensor Factorization, EMNLP 2014

Mo Yu, Mark Dredze, Raman Arora, Matthew R. Gormley, Embedding Lexical Features via Low-Rank Tensors

to name a few via quick googling.

Additionally, since it seems the main benefit of using a tensor-based method is that you can use 3rd order cooccurance information, multisense embedding methods should also be evaluated. There are many such methods, see for example 

Jiwei Li, Dan Jurafsky, Do Multi-Sense Embeddings Improve Natural Language Understanding?

and citations within, plus quick googling for more recent works.

I am not saying that these works are equivalent to what the authors are doing, or that there is no novelty, but the evaluations seem extremely unfair to only compare against matrix factorization techniques, when in fact many higher order extensions have been proposed and evaluated, and especially so on the tasks proposed (in particular the 3-way outlier detection). 

Observe also that in table 2, NNSE gets the highest performance in both MEN and MTurk. Frankly this is not very surprising; matrix factorization is very powerful, and these simple word similarity tasks are well-suited for matrix factorization. So, statements like ""as we can see, our embeddings very clearly outperform the random embedding at this task"" is  an unnecessary inflation of a result that 1) is not good and 2) is reasonable to not be good. 

Overall, I think for a more sincere evaluation, the authors need to better pick tasks that clearly exploit 3-way information and compare against other methods proposed to do the same.

The multiplicative relation analysis is interesting, but at this point it is not clear to me why multiplicative is better than additive in either performance or in giving meaningful interpretations of the model. 

In conclusion, because the novelty is also not that big (CP decomposition for word embeddings is a very natural idea) I believe the evaluation and analysis must be significantly strengthened for acceptance. ","The reviewer raises several valid concerns, indicating a negative sentiment towards the paper. While the reviewer acknowledges some positive aspects (e.g., finding the multiplicative relation analysis interesting), the criticism regarding the evaluation, comparison with existing methods, and clarity of the model outweighs the positive points. The language used is critical but professional and remains within the bounds of academic politeness.",-40,60
"The paper provides an analysis of the representations learnt in convolutional neural networks that take raw audio waveforms as input for a speaker emotion recognition task. Based on this analysis, an architecture is proposed and compared to other architectures inspired by other recent work. The proposed architecture overfits less on this task and thus performs better.

I think this work is not experimentally strong enough to draw the conclusions that it draws. The proposed architecture, aptly called ""SimpleNet"", is relatively shallow compared to the reference architectures, and the task that is chosen for the experiments is relatively small-scale. I think it isn't reasonable to draw conclusions about what convnets learn in general from training on a single task, and especially not a small-scale one like this. 

Moreover, SoundNet, which the proposed architecture is compared to, was trained on (and designed for) a much richer and more challenging task originally. So it is not surprising at all that it overfits dramatically to the tasks chosen here (as indicated in table 1), and that a much shallower network with fewer parameters overfits less. This seems obvious to me, and contrary to what's claimed in the paper, it provides no convincing evidence that shallow architectures are inherently better suited for raw audio waveform processing. This is akin to saying that LeNet-5 is a better architecture for image classification than Inception, because the latter overfits more on MNIST. Perhaps using the original SoundNet task, which is much more versatile, would have lent some more credibility to these claims.

The analysis in section 2.2 is in-depth, but also not very relevant: it ignores the effects of nonlinearities, which are an essential component of modern neural network architectures. Studying their effects in the frequency domain would actually be quite interesting. It is mentioned that the ReLU nonlinearity acts as a half-wave rectifier, but the claim that its effect in the frequency domain is small compared to aliasing is not demonstrated. The claim that ""ReLU and non-linear activations can improve the network performance, but they are not the main factors in the inner workings of CNNs"" is also unfounded.

The conclusion that stacking layers is not useful might make sense in the absence of nonlinearities, but when each layer includes a nonlinearity, the obvious point of stacking layers is to improve the expressivity of the network. Studying aliasing effects in raw audio neural nets is a great idea, but I feel that this work takes some shortcuts that make the analysis less meaningful.




Other comments:

The paper is quite lengthy (11 pages of text) and contains some sections that could easily be removed, e.g. 2.1.1 through 2.1.3 which explain basic signal processing concepts and could be replaced by a reference. In general, the writing could be much more concise in many places.

The paper states that ""it remains unknown what actual features CNNs learn from waveforms."". There is actually some prior work that includes some analysis on what is learnt in the earlier layers of convnets trained on raw audio: 
""Learning the Speech Front-end With Raw Waveform CLDNNs"", Sainath et al.
""Speech acoustic modeling from raw multichannel waveforms"", Hoshen et al.
""End-to-end learning for music audio"", Dieleman & Schrauwen
Only the first one is cited, but not in this context. I think saying ""it remains unknown"" is a bit too strong of an expression.

The meaning of the following comment is not clear to me: ""because in computer vision, the spatial frequency is not the only information the model can use"". Surely the frequency domain and the spatial domain are two different representations of the same information contained in an image or audio signal? So in that sense, spatial frequency does encompass all information in an image.

The implication that high-frequency information is less useful for image-based tasks (""the spatial frequency of images is usually low"") is incorrect. While lower frequencies dominate the spectrum more obviously in images than in audio, lots of salient information (i.e. edges, textures) will be high-frequency, so models would still have to learn high-frequency features to perform useful tasks.

WaveNet is mentioned (2.2.4) but not cited. WaveNet is a fairly different architecture than the ones discussed in this paper and it would be useful to at least discuss it in the related work section. A lot of the supposed issues discussed in this paper don't apply to WaveNet (e.g. there are no pooling layers, there is a multiplicative nonlinearity in each residual block).

The paper sometimes uses concepts without clearly defining them, e.g. ""front-end layers"". Please clearly define each concept when it is first introduced.

The paper seems to make a fairly arbitrary distinction between layers that perform signal filtering operations, and layers that don't - but every layer can be seen as a (possibly nonlinear) filtering operation. Even if SimpleNet has fewer ""front-end layers"", surely the later layers in the network can still introduce aliasing? I think the implicit assumption that later layers in the network perform a fundamentally different kind of operation is incorrect.

It has been shown that even random linear filters can be quite frequency-selective (see e.g. ""On Random Weights and Unsupervised Feature Learning"", Saxe et al.). This is why I think the proposed ""changing rate"" measure is a poor choice to show effective training. Moreover, optimization pathways don't have to be linear in parameter space, and oscillations can occur. Why not measure the difference with the initial values (at iteration 0)? It seems like that would prove the point a bit better.

Manually designing filters to initialize the weights of a convnet has been done in e.g. Sainath et al. (same paper as mentioned before), so it would be useful to refer to it again when this idea is discussed.

In SpecNet, have the magnitude spectrograms been log-scaled? This is common practice and it can make a dramatic difference in performance. If you haven't tried this, please do.
","The review starts with a neutral tone, presenting the paper's topic and findings without judgment. However, it quickly transitions into a highly critical analysis, using strong negative qualifiers like ""not experimentally strong enough,"" ""not reasonable,"" ""obvious,"" ""unfounded,"" and ""incorrect."" The reviewer finds significant flaws in the paper's methodology, conclusions, and even writing style. While the reviewer acknowledges some positive aspects (in-depth analysis, interesting idea), the overall tone and numerous negative points contribute to a largely negative sentiment. Despite the critical tone, the language remains professional and avoids personal attacks, staying within the bounds of academic discourse.",-50,50
"This paper presents an embedding algorithm for text corpora that allows known
covariates, e.g. author information, to modify a shared embedding to take context
into account. The method is an extension of the GloVe method and in the case of
a single covariate value the proposed method reduces to GloVe. The covariate-dependent
embeddings are diagonal scalings of the shared embedding. The authors demonstrate
the method on a corpus of books by various authors and on a corpus of subreddits.
Though not technically difficult, the extension of GloVe to covariate-dependent
embeddings is very interesting and well motivated. Some of the experimental results
do a good job of demonstrating the advantages of the models. However, some of the
experiments are not obvious that the model is really doing a good job.

I have some small qualms with the presentation of the method. First, using the term
""size m"" for the number of values that the covariate can take is a bit misleading.
Usually the size of a covariate would be the dimensionality. These would be the same
if the covariate is one hot coded, however, this isn't obvious in the paper right now.
Additionally, v_i and c_k live in R^d, however, it's not really explained what
'd' is, is it the number of 'topics', or something else? Additionally, the functional
form chosen for f() in the objective was chosen to match previous work but with no
explanation as to why that's a reasonable form to choose. Finally, the authors
say toward the end of Section 2 that ""A careful comparision shows that this
approximation is precisely that which is implied by equation 4, as desired"". This is
cryptic, just show us that this is the case.

Regarding the experiments there needs to be more discussion about how the
different model parameters were determined. The authors say ""... and after tuning
our algorithm to emged this dataset, ..."", but this isn't enough. What type of
tuning did you do to choose in particular the latent dimensionality and the
learning rate? I will detail concerns for the specific experiments below.

Section 4.1:
- How does held-out data fit into the plot?

Section 4.2:
- For the second embedding, what exactly was the algorithm trained on? Just the
  book, or the whole corpus?
- What is the reader supposed to take away from Table 1? Are higher or lower
  values better? Maybe highlight the best scores for each column.


Section 4.3:
- Many of these distributions don't look sparse.
- There is a terminology problem in this section. Coordinates in a vector are
  not sparse, the vector itself is sparse if there are many zeros, but
  coordinates are either zero or not zero. The authors' use of 'sparse' when
  they mean 'zero' is really confusing.
- Due to the weird sparsity terminology Table 1 is very confusing. Based on how
  the authors use 'sparse' I think that Table 1 shows the fraction of zeros in
  the learned embedding vectors. But if so, then these vectors aren't sparse at all
  as most values are non-zero.

Section 5.1:
- I don't agree with the authors that the topics in Table 3 are interpretable.
  As such, I think it's a reach to claim the model is learning interpretable topics.
  This isn't necessarily a problem, it's fine for models to not do everything well,
  but it's a stretch for the authors to claim that these results are a positive
  aspect of the model. The results in Section 5.2 seem to make a lot of sense and
  show the big contribution of the model.

Section 5.3:
- What is the ""a : b :: c : d"" notation?
","The review is generally positive, highlighting the interesting and well-motivated nature of the work. The reviewer acknowledges the value of the method and its potential. While the reviewer points out areas for improvement in the presentation and experiments, the language used is constructive and polite. The reviewer offers specific suggestions and asks for clarification rather than stating flaws bluntly. ",60,70
"This paper proposes to train recursive neural network on subtask graphs in order to execute a series of tasks in the right order, as is described by the subtask graph's dependencies. Each subtask execution is represented by a (non-learned) option. Reward shaping allows the proposed model to outperform simpler baselines, and experiments show the model generalizes to unseen graphs.

While this paper is as far as I can tell novel in how it does what it does, the authors have failed to convey to me why this direction of research is relevant.
- We know finding options is the hard part about options
- We already have good algorithms that take subtask graphs and execute them in the right order from the planning litterature

An interesting avenue would be if the subtask graphs were instead containing some level of uncertainty, or representing stochasticity, or anything that more traditional methods are unable to deal with efficiently, then I would see a justification for the use of neural networks. Alternatively, if the subtask graphs were learned instead of given, that would open the door to scaling an general learning. Yet, this is not discussed in the paper.

Another interesting avenue would be to learn the options associated with each task, possibly using the information from the recursive neural networks to help learn these options.


The proposed algorithm relies on fairly involved reward shaping, in that it is a very strong signal of supervision on what the next action should be. Additionaly, it's not clear why learning seems to completely ""fail"" without the pre-trained policy. The justification given is that it is ""to address the difficulty of training due to the complex nature of the problem"" but this is not really satisfying as the problems are not that hard. This also makes me question the generality of the approach since the pre-trained policy is rather simple while still providing an apparently strong score.


In your experiments, you do not compare with any state-of-the-art RL or hierarchical RL algorithm on your domain, and use a new domain which has no previous point of reference. It it thus hard to properly evaluate your method against other proposed methods.

What the authors propose is a simple idea, everything is very clearly explained, the experiments are somewhat lacking but at least show an improvement over more a naive approach, however, due to its simplicity, I do not think that this paper is relevant for the ICLR conference. 

Comments:
- It is weird to use both a discount factor \gamma *and* a per-step penalty. While not disallowed by theory, doing both is redundant because they enforce the same mechanism.
- It seems weird that the smoothed logical AND/OR functions do not depend on the number of inputs; that is unless there are always 3 inputs (but it is not explained why; logical functions are usually formalised as functions of 2 inputs) as suggested by Fig 3.
- It does not seem clear how the whole training is actually performed (beyond the pre-training policy). The part about the actor-critic learning seems to lack many elements (whole architecture training? why is the policy a sum of ""p^{cost}"" and ""p^{reward}""? is there a replay memory? How are the samples gathered?). (On the positive side, the appendix provides some interesting details on the tasks generations to understand the experiments.)
- The experiments cover different settings with different task difficulties. However, only one type of tasks is used. It would be good to motivate (in addition to the paragraph in the intro) the cases where using the algorithm described in the paper may be (or not?) the only viable option and/or compare it to other algorithms. Even tough not mandatory, it would also be a clear good addition to also demonstrate more convincing experiments in a different setting.
- ""The episode length (time budget) was randomly set for each episode in a range such that 60% − 80% of subtasks are executed on average for both training and testing."" --> this does not seem very precise: under what policy is the 60-80% defined? Is the time budget different for each new generated environment?
- why wait until exactly 120 epochs for NTS-RProp before fine-tuning with actor-critic? It seems that much less would be sufficient from figure 4?
- In the table 1 caption, it is written ""same graph structure with training set"" --> do you mean ""same graph structure than the training set""?","The reviewer acknowledges the novelty of the paper but expresses significant concerns about its relevance and contribution. They find the problem addressed to be somewhat trivial and question the necessity of the proposed approach. The reviewer also criticizes the experimental methodology and lack of comparison with existing methods, suggesting the paper might not be suitable for ICLR. The language used, while direct, maintains a professional and respectful tone, focusing on the scientific content rather than resorting to personal attacks.",-50,50
"The key contributions of this paper are:
(a) proposes to reduce the vocabulary size in large sequence to sequence mapping tasks (e.g., translation) by first mapping them into a ""standard"" form and then into their correct morphological form,
(b) they achieve this by clever use of character LSTM encoder / decoder that sandwiches a bidirectional LSTM which captures context,
(c) they demonstrate clear and substantial performance gains on the OpenSubtitle task, and
(d) they demonstrate clear and substantial performance gains on a dialog question answer task.

Their analysis in Section 5.3 shows one clear advantage of this model in the context of long sequences. 

As an aside, the authors should correct the numbering of their Figures (there is no Figure 3) and provide better captions to the Tables so the results shown can easily understood at a glance. 

The only drawback of the paper is that this does not advance representation learning per se though a nice application of current models.","The review starts by highlighting the key contributions of the paper, using positive language such as ""clever use"" and ""clear and substantial performance gains."" The reviewer acknowledges the advantage of the model. While pointing out a drawback, the reviewer uses a neutral tone and doesn't diminish the paper's value. The suggestions for improvement are concrete and constructive. Overall, the tone is balanced and professional.",75,75
"The paper focuses on a very particular HMM structure which involves multiple, independent HMMs. Each HMM emits an unobserved output with an explicit duration period. This explicit duration modelling captures multiple scale of temporal resolution. The actual observations are a weighted linear combination of the emissions from each latent HMM. The structure allows for fast inference using a spectral approach.

I found the paper unclear and lacking in detail in several key aspects:

1. It is unclear to me from Algorithm 2 how the weight vectors w are estimated. This is not adequately explained in the section on estimation.

2. The authors make the assumption that each HMM injects noise into the unobserved output which then gets propagated into the overall observation. What are reasons for his choice of model over a simpler model where the output of each HMM is uncorrupted?

3. The simulation example does not really demonstrate the ability of the MSHMM to do anything other than recover structure from data simulated under an MSHMM. It would be more interesting to apply to data simulated under non-Markovian or other setups that would enable richer frequency structures to be included and the ability of MSHMM to capture these.

4. The real data experiments shows some improvements in predictive accuracy with fast inference. However, the authors do not give a sufficiently broad exploration of the representations learnt by the model which allows us to understand the regimes in which the model would be advantageous.

Overall, the paper presents an interesting approach but the work lacks maturity. Furthermore, simulation and real data examples to explore the properties and utility of the method are required. 
","The reviewer acknowledges the interesting approach but expresses concerns about clarity, lack of detail, and limited experimental validation. The use of ""lacks maturity"" and the call for further exploration suggest a somewhat negative sentiment. However, the reviewer also points out the positive aspects, such as the interesting approach and fast inference, indicating a somewhat balanced perspective. The language used is professional and polite, providing constructive criticism without resorting to harsh or disrespectful language.",-20,70
"This paper describes an approach to stochastic control using RL that extends DDPG with a stochastic policy.  A standard DDPG setup is extended such that the actor now produces M actions at each timestep.  Only one of the M actions will be executed in the environment using a uniform sampling.  The sampled action is the only that will receive a gradient update from the critic network. The authors demonstrate that such a stochastic policy performs better on average in a series of benchmark control tasks.

I find the general idea of the work compelling, but the particular approach is rather poor.  The fact that we are choosing the number of modes in the uniform distribution is a bit underwhelming (a more compelling architecture could have proposed a policy conditioned on gaussian noise for example, thus having better coverage of the distribution). I found the proposed apprach to be under-analyzed and the stochastic aspects of the policy are undervalued.   The main claim being argued in the paper is that the proposed stochastic policy has better final performance on average than a deterministic policy, but the only practical difference seems to be a slightly more structured approach to exploration.  
However, very little attention is paid to trying different exploration methods with the deterministic policy (incidentally, Ornstein-Uhlenbeck process noise is not something I'm familiar with, a citation to the use of this noise for exploration as well as a more explicit explanation would have been appreciated).  One interpretation is that each of the M sub-policies follows a different mode of the Q-value distribution over the action space.  But is this indeed the case?  There is a brief analysis of this with cartpole, but a more complete look at how actions are clustered in the action space would make this paper much more compelling.  Even in higher-dimensional action spaces, you could look at a t-SNE projection or cluster analysis to try and see how many modes the agent is reasoning over.  Additionally, the baseline agent should have used additional exploration methods as these can quickly change the performance of the agent.

I also think that better learning is not the only redeeming aspect of a stochastic policy.  In the face of a non-stationary environment, a stochastic policy will likely be much more robust.  Additionally, it will have much better performance against adversarial environments.  Given the remaining available space in the paper it would have been interesting to provide more insight into the proposed methods gains in these areas.","The reviewer finds the general idea compelling, but has significant concerns about the approach and analysis. They find the approach ""rather poor"" and ""under-analyzed."" They also point out several areas where the paper could be improved, such as exploring different exploration methods and analyzing the robustness of the stochastic policy. While they see potential in the idea, the criticisms are substantial and suggest the paper needs significant revisions. Therefore, the sentiment is somewhat negative. The language used is critical but professional and not rude.",-30,60
"This paper compares 2 CNN architectures (Alexnet and a VGG variant) for the task of classifying images of lung cancer from CT scans. The comparison is trivial and does not go in depth to explain why one architecture works better than the other. Also, no effort is made to explain the data beyond some superficial description. No example of input data is given (what does an actual input look like). The authors mention ""the RCNN object detector"" in step 18, that presumably does post-processing after the CNN. But there is no explanation of that module anywhere. Instead the authors spend most of the paper listing in wordy details the architecture of their VGG variant. Also, a full page is devoted to detailed explanation of what precision-recall and Matthews Correlation Coefficient is! Overall, the paper does not provide any insight beyond: i tried this, i tried that and this works better than that; a strong reject.","The review is very negative about the paper, using terms like ""trivial"", ""superficial"", and ""strong reject"". It also criticizes the authors for including unnecessary details and failing to provide insights.  The language is quite direct and critical, but it doesn't use personal attacks or overly inflammatory language.",-80,-20
"The paper presents a multi-task, multi-domain model based on deep neural networks. The proposed model is able to take inputs from various domains (image, text, speech) and solves multiple tasks, such as image captioning, machine translation or speech recognition. The proposed model is composed of several features learning blocks (one for each input type) and of an encoder and an auto-regressive decoder, which are domain-agnostic. The model is evaluated on 8 different tasks and is compared with a model trained separately on each task, showing improvements on each task.

The paper is well written and easy to follow.

The contributions of the paper are novel and significant. The approach of having one model able to perform well on completely different tasks and type of input is very interesting and inspiring. The experiments clearly show the viability of the approach and give interesting insights. This is surely an important step towards more general deep learning models. 

Comments:

* In the introduction where the 8 databases are presented, the tasks should also be explained clearly, as several domains are involved and the reader might not be familiar with the task linked to each database. Moreover, some databases could be used for different tasks, such as WSJ or ImageNet.

* The training procedure of the model is not explained in the paper. What is the cost function and what is the strategy to train on multiple tasks ? The paper should at least outline the strategy.

* The experiments are sufficient to demonstrate the viability of the approach, but the experimental setup is not clear. Specifically, there is an issue about the speech recognition part of the experiment. It is not clear what the task exactly is: continuous speech recognition, isolated word recognition ? The metrics used in Table 1 are also not clear, they should be explained in the text. Also, if the task is continuous speech recognition, the WER (word error rate) metric should be used. Information about the detailed setup is also lacking, specifically which test and development sets are used (the WSJ corpus has several sets).

* Using raw waveforms as audio modality is very interesting, but this approach is not standard for speech recognition, some references should be provided, such as:
P. Golik, Z. Tuske, R. Schluter, H. Ney, Convolutional Neural Networks for Acoustic Modeling of Raw Time Signal in LVCSR, in: Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH), 2015, pp. 26–30.
D. Palaz, M. Magimai Doss and R. Collobert, (2015, April). Convolutional neural networks-based continuous speech recognition using raw speech signal. In Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on (pp. 4295-4299). IEEE.
T. N. Sainath, R. J. Weiss, A. Senior, K. W. Wilson, and O. Vinyals. Learning the Speech Front-end With Raw Waveform CLDNNs. Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH), 2015.

Revised Review:
The main idea of the paper is very interesting and the work presented is impressive. However, I tend to agree with Reviewer2, as a more comprehensive analysis should be presented to show that the network is not simply multiplexing tasks. The experiments are interesting, except for the WSJ speech task, which is almost meaningless. Indeed, it is not clear what the network has learned given the metrics presented, as the WER on WSJ should be around 5% for speech recognition.
I thus suggest to either drop the speech experiment, or the modify the network to do continuous speech recognition. A simpler speech task such as Keyword Spotting could also be investigated.
","The review starts with positive statements, highlighting novelty and significance. However, it raises several important concerns about clarity, methodology, and results, particularly regarding the speech recognition experiment. The reviewer suggests dropping or significantly modifying this part. The tone is direct and professional, suggesting improvements without being overly negative.",50,70
"The paper proposes a hierarchical probabilistic model that learns both static representations and the dynamics of the data. The model iteratively updates its latent representations of the data in order to improve its generative power. The model is applicable to both static images (iterative improvements of the samples) and videos (predictive coding like repesentations).

Pros:

-- the motivation for the work and its connections to the cognitive/philosophical models of concepts and predictive coding is very interesting
-- the iterative improvements in the celebA samples in Fig. 3 and the corresponding improvements in the log-likelihood in Tbl. 1 vs the vanilla VAE baseline are promising and suggest that the approach has potential

Cons:

-- The major problem with this paper in my opinion is that the methods section is very confusing:
    1) The section is too brief and there is absolutely no description of the model until page 4
    2) The figures are hard to understand and the annotations are not informative (e.g. what is the difference between Fig.1 and Fig.2?) 
    3) The notation is unconventional and keeps changing (e.g. the generator is referred to as either \varphi_A, \varphi_X,  \varphi_X(Z_t), X_t|Z_t, or \mu_X; \sigma_X... the dimensionality of the image is denoted as i, N * c or Nc... I can go on).  
    4) The rescaling of the latent parameters seems engineered and arbitrary (e.g. \beta scaling factor in Eq. 8 is chosen so that the sigmoid reaches 0.75 when the value is 0.5\sigma of the threshold).

Due to the points above I failed to fully understand the model despite trying hard to do so. In particular, I did not understand the most important part of the paper addressing the iterative update of the latents vs backprop update of the generative weights. 

Minor points:
-- The introduction is too long and repetitive. The space saved should be used to describe the model more precisely.
-- The parametrisation of S_t should be described when it is first introduced, not 2 paragraphs later.
-- How does an inner product of two vectors result in a matrix (Sec. 3.3)?
-- GANs also do not have an encoder network (despite what the authors claim in Sec. 4.1) and should be used as a baseline
-- Why does the VAE baseline have a different decoder architecture than the proposed model?
-- What is the pre-processing done for CelebA?
-- What is the ground truth that was supposed to be matched by \mu_S_t in the dynamic dataset?
-- Figs. 4-5 are hard to understand. What do the different colours of the lines mean? The time stamps where the behaviours are changing should be marked in the plot (not just described in the text).

To conclude, the authors are advised to shorten the introduction and literature review sections and use the extra space to re-write and expand the methods section to make it very clear how their model works using the standard notation used in the literature. The results section detailing the dynamic setup of their approach needs to be made more clear as well. In the current form the paper is not ready for publication.






","The reviewer acknowledges the interesting aspects of the work, particularly its motivation and potential demonstrated in image improvement. However, they express significant concerns about the clarity and presentation of the methods section, stating it's confusing and difficult to understand. The reviewer points out unconventional notation, lack of description, and arbitrary choices as reasons for this confusion. They also mention needing clarification in the results section related to the dynamic setup. While not explicitly negative, the numerous issues raised and the final statement about the paper not being ready for publication suggest a significantly negative sentiment. The language used is direct and critical but maintains a professional and constructive tone, suggesting a neutral politeness level.",-50,0
"This paper investigates the effect of adversarial training. Based on experiments using CIFAR10, the authors show that adversarial training is effective in protecting against ""shared"" adversarial perturbation, in particular against universal perturbation. In contrast, it is less effective to protect against singular perturbations. Then they show that singular perturbation are less robust to image transformation, meaning after image transformation those perturbations are no longer effective. Finally, they show that singular perturbations can be easily detected.

I like the message conveyed in this paper. However, as the statements are mostly backed by experiments, then I think it makes sense to ask how statistically significant the present results are. Moreover, is CIFAR 10 experiments conclusive enough. ","The reviewer explicitly states they ""like the message conveyed in this paper"", which points towards a positive sentiment. However, they also raise valid questions and ask for further investigation, indicating that the paper is not perfect. Therefore, the sentiment is positive but not overly enthusiastic. The language used is constructive, professional, and respectful, suggesting a polite tone.",60,80
"**Summary**
The paper proposes an extension of the attend, infer, repeat generative model of Eslami, 2016 and extends it to handle ``visual attribute descriptions. This straightforward extension is claimed to improve image quality and shown to improve performance on a previously introduced image caption ranking task. In general, the paper shows improvements on an image caption agreement task introduced in Kuhnle and Copestake, 2017.  The paper seems to have weaknesses pertaining to the approach taken, clarity of presentation and comparison to baselines which mean that the paper does not seem to meet the acceptance threshold for ICLR. See more detailed points below in Weaknesses.

**Strengths**
I like the high-level motivation of the work, that one needs to understand and establish that language or semantics can help learn better representations for images. I buy the premise and think the work addresses an important issue. 

**Weakness**

Approach:
* A major limitation of the model seems to be that one needs access to both images and attribute vectors at inference time to compute representations which is a highly restrictive assumption (since inference networks are discriminative). The paper should explain how/if one can compute representations given just the image, for instance, say by not using amortized inference. The paper does propose to use an image-only encoder but that is intended in general as a modeling choice to explain statistics which are not captured by the attributes (in this case location and orientation as explained in the Introduction of the paper).

Clarity:
* Eqn. 5, LHS can be written more clearly as \hat{a}_k. 

* It would also be good to cite the following related work, which closely ties into the model of Eslami 2016, and is prior work: 

Efficient inference in occlusion-aware generative models of images,
Jonathan Huang, Kevin Murphy.
ICLR Workshops, 2016

* It would be good to clarify that the paper is focusing on the image caption agreement task from Kuhnle and Copestake, as opposed to generic visual question answering.

* The claim that the paper works with natural language should be toned down and clarified. This is not natural language, firstly because the language in the dataset is synthetically generated and not “natural”. Secondly, the approach parses this “synthetic” language into structured tuples which makes it even less natural. Also, Page. 3. What does “partial descriptions” mean?

* Section 3: It would be good to explicitly draw out the graphical model for the proposed approach and clarify how it differs from prior work (Eslami, 2016).

* Sec. 3. 4 mentions that the “only image” encoder is used to obtain the representation for the image, but the “only image” encoder is expected to capture the “indescribable component” from the image, then how is the attribute information from the image captured in this framework? One cannot hope to do image caption association prediction without capturing the image attributes...

*, In general, the writing and presentation of the model seem highly fragmented, and it is not clear what the specifics of the overall model are. For instance, in the decoder, the paper mentions for the first time that there are variables “z”, but does not mention in the encoder how the variables “z” were obtained in the first place (Sec. 3.1). For instance, it is also not clear if the paper is modeling variable length sequences in a similar manner to Eslami, 2016 or not, and if this work also has a latent variable [z, z_pres] at every timestep which is used in a similar manner to Eqn. 2 in Eslami, 2016. Sec. 3.4 “GEN Image Encoder” has some typo, it is not clear what the conditioning is within q(z) term.

* Comparison to baselines: 
  1. How well does this model do against a baseline discriminative image caption ranking approach, similar to [D]? This seems like an important baseline to report for the image caption ranking task.
  2. Another crucial baseline is to train the Attend, Infer, Repeat model on the ShapeWorld images, and then take the latent state inferred at every step by that model, and use those features instead of the features described in Sec. 3.4 “Gen Image Encoder” and repeat the rest of the proposed pipeline. Does the proposed approach still show gains over Attend Infer Repeat?
  3. The results shown in Fig. 7 are surprising -- in general, it does not seem like a regular VAE would do so poorly. Are the number of parameters in the proposed approach and the baseline VAE similar? Are the choices of decoder etc. similar? Did the model used for drawing Fig. 7 converge? Would be good to provide its training curve. Also, it would be good to evaluate the AIR model from Eslami, 2016 on the same simple shapes dataset and show unconditional samples. If the claim from the work is true, that model should be just as bad as a regular VAE and would clearly establish that using language is helping get better image samples.

* Page 2: In general the notion of separating the latent space into content and style, where we have labels for the “content” is an old idea that has appeared in the literature and should be cited accordingly. See [B] for an earlier treatment, and an extension by [A]. See also the Bivcca-private model of [C] which has “private” latent variables for vision similar to this work (this is relevant to Sec. 3.2.)

References:
[A]: Upchurch, Paul, Noah Snavely, and Kavita Bala. 2016. “From A to Z: Supervised Transfer of Style and Content Using Deep Neural Network Generators.” arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1603.02003.

[B]: Kingma, Diederik P., Danilo J. Rezende, Shakir Mohamed, and Max Welling. 2014. “Semi-Supervised Learning with Deep Generative Models.” arXiv [cs.LG]. arXiv. http://arxiv.org/abs/1406.5298.

[C]: Wang, Weiran, Xinchen Yan, Honglak Lee, and Karen Livescu. 2016. “Deep Variational Canonical Correlation Analysis.” arXiv [cs.LG]. arXiv. http://arxiv.org/abs/1610.03454.

[D]: Kiros, Ryan, Ruslan Salakhutdinov, and Richard S. Zemel. 2014. “Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models.” arXiv [cs.LG]. arXiv. http://arxiv.org/abs/1411.2539.
","The review starts with a polite but negative sentiment, highlighting weaknesses in the paper's approach, clarity, and comparison to baselines. While acknowledging the good motivation, the reviewer lists several major concerns, suggesting the paper is not ready for acceptance. The language used is formal, professional, and typical of academic peer reviews. There's no personal criticism or disrespectful tone.",-25,75
"This paper proposes a language independent text encoding method for cross-language classification. The proposed approach demonstrates better performance than machine translation based classifier. 

The proposed approach  performs language independent common representation learning for cross-lingual text classification. Such representation learning based methods have been studied in the literature. The authors should provide a review and comparison to related methods. 

Technical contribution of the paper is very limited. The approach section is too short to provide a clear presentation of the model. Some descriptions about the input text representation are actually given in the experimental section. 

The proposed approach uses comparable texts across different languages to train the encoders, while using the topic information as auxiliary supervision label information. In the experiments, it shows the topics are actually fine-grained class information that are closely related to the target class categories. This makes the zero-shot learning scenario not to be very practical. With such fine-grained supervision knowledge, it is also unfair to compare to other cross-lingual methods that use much less auxiliary information. 

In the experiments, it states the data are collected by “For pages with multiple categories we select one at random”.  Won’t this produce false negative labels on the constructed data? How much will this affect the test performance?

The experimental results are not very convincing without empirical comparisons to the state-of-the-art cross-lingual text classification methods. ","The review starts with a mildly positive statement acknowledging the paper's aim and a potential advantage. However, the majority of the review points out significant shortcomings in terms of technical contribution, clarity, experimental setup, and lack of comparison with existing work. The reviewer raises valid concerns and questions the paper's claims. Overall, the sentiment leans towards the negative due to the seriousness of the issues raised.",-40,20
"This paper illustrates the benefits of using normalized gradients when training deep models.
Beyond exploring the ""vanilla"" normalized gradient algorithm they also consider adaptive versions, i.e., methods that employ per block (adaptive) learning rates using ideas from AdaGrad and Adam.
Finally, the authors provide a theoretical analysis of NG with adaptive step-size, showing convergence guarantees in the stochastic convex optimization setting.

I find this paper both very interesting and important. 
The normalized gradient method was previously shown to overcome some non-convex phenomena which are hurdles to SGD, yet there was still the gap of  combining NG with methods which automatically tune the learning rate.

The current paper addresses this gap by a very simple (yet clever) combination of NG with AdaGrad and Adam, and the authors do a great job by illustrating the benefits of their scheme by testing it over a very wide span of deep learning 
models. In light of their experiments it seems like AdamNG and NG should be adopted as the new state-of-the-art methods in deep-learning applications.

Additional comments:
-In the experiments the authors use the same parameters as is used by Adam/AdaGrad, etc..
Did the authors also try to fine tune the parameters of their NG versions? If so what is the benefit that they get by doing so?
-It will be useful if the authors can provide some intuition about why is the learning rate  chosen per block for NG?
Did the authors also try to choose a learning rate per weight vector rather than per block? If so, what is the behaviour that they see.
-I find the theoretical analysis a bit incomplete. The authors should spell out the choice of the learning rate in Thm. 1 and compare to AdaGrad.
","The reviewer clearly states that they find the paper ""very interesting and important"", highlighting the significance of the research. They praise the authors for addressing a gap in the field and commend the extensive experiments conducted. While they offer constructive criticism and suggestions for improvement, these are presented in a positive and encouraging manner. The reviewer even suggests that the proposed methods ""should be adopted as the new state-of-the-art"", indicating a strong positive sentiment overall.",90,90
"The authors proved that convolutional neural networks with Leaky ReLU activation function are nonlinear frames, and similar results hold for non-uniformly sampled time-series as well. My main concern on this part is that theory is too rough and its link to the later part of the paper is weak. Although frames are stable representations, the ones with lower bound much smaller than the upper bound are close to unstable. That's why in classical applications of frames in signal and image processing tight frames are vastly preferred. Furthermore, the authors did not explicitly state the reliance of the lower frame bound on the parameter alpha in Leaky ReLU. It seems to me that the representation gets more unstable as alpha decreases, and the lower bound will be zero when ReLU is used. 

In Section 2, the authors used CMF conditions to constraint filters which leads to a much more stable representation than in the previous section. The idea is very similar to previous work on data-driven tight frame (Cai et al. Applied & Computational Harmonic Analysis, vol. 37, no. 1, p. 89-105, 2014) and AdaFrame (Tai and E, arXiv:1507.04835). Furthermore, there has been multiple work on introducing tight-frame-like constraints to filters of convolutional neural networks (see for example Huang et al., arXiv:1710.02338). All of these work is not mentioned by the authors. Although the CMF constraints used by the authors seem new, the overall novelty is still weak in my opinion.

The experimental results are convincing, and the proposed architecture with wavelet-transform LSTM outperform the baseline model using standard LSTM. However, I am not familiar with the state-of-the-art models on the data sets used in the paper. Therefore, I am not certain whether the proposed method achieves state-of-the-art or not. ","The review is critical of the paper, pointing out several weaknesses such as the roughness of the theory, lack of novelty, and limited comparison with state-of-the-art methods. However, it does acknowledge the convincing experimental results. The language used is professional and not disrespectful, indicating a neutral rather than rude tone.",-20,50
"This paper proposed to learn a generative GAN model that generates the training data from the labels, given that only the black-box mapping $f$ from data to label is available, as well as an aux dataset that might and might not overlap with the training set. This approach can be regarded as a transfer learning version of ACGAN that generates data conditioned on its label.

Overall I feel it unclear to judge whether this paper has made substantial contributions. The performance critically relies on the structure of aux dataset and how the supervised model $f$ interacts with it. It would be great if the author could show how the aux dataset is partitioned according to the function $f, and what is the representative sample from aux dataset that maximizes a given class label. In Fig. 4, the face of Leonardo DiCaprio was reconstructed successfully, but is that because in the aux dataset there are other identities who look very similar to him and is classified as Leonardo, or it is because GAN has the magic to stitch characteristics of different face identities together?  Given the current version of the paper, it is not clear at all. From the results on EMNIST when the aux set and the training set are disjoint, the proposed model simply picks the most similar shapes as GAN generation, and is not that interesting. In summary, a lot of ablation experiments are needed for readers to understand the proposed method better.

The writing is ok but a bit redundant. For example, Eqn. 1 (and Eqn. 2) which shows the overall distribution of the training samples (and aux samples) as a linear combinations of the samples at each class, are not involved in the method. Do we really need Eqn. 1 and 2?","The reviewer expresses uncertainty about the paper's contributions, using phrases like ""it is unclear"" and questioning the novelty of the results. They find the results interesting only in specific cases and call for more experiments to understand the method better. This suggests a slightly negative sentiment. The reviewer does point out positive aspects like the writing being 'ok,' but the overall tone leans towards the critical side. The language used is professional and avoids harsh or disrespectful language, indicating a neutral to polite tone.",-20,50
"This paper introduces a reversible network with absolute value used as
the activation function.  The network is run in the forward direction
to classify and in the reverse direction to generate.

The key points of the network are the use of the absolute value
activation function and the use of (free) normalization to match
target output. This allows the network to perfectly map inputs to any
point on a vector that goes through the one-hot encoding, allowing for
deterministic generation from different vectors (of different lengths)
with the same normalized output.

I think there are a lot of novel and interesting ideas in this paper
though they have not been fully explored.  The use of the absolute
value transfer function is new to me, though I was able to find a couple of old
references to its use.   In a paper by Gad et al. (2000), it is stated 
"" For example, the algorithm presented in Lin and
Unbehauen (1995) < I think they mean Lin and Unbehauen 1990)> 
 is used to train networks with a single hidden layer
employing the absolute value as the activation function of the hidden
neuron. This algorithm was further generalized to multilayer networks
with cascaded structures in Batruni (1991).""   Exploring the properties 
of the abs activation function seems worth exploring.

More details on the training are needed for full clarity in the paper.
(Though it is recognized that some of these could be determined from
links when made active, they should be included in the paper).  How
did you select the training parameters given at the bottom of page 5?
How many layers and units/layer did you use? And how were these
selected?  (The use of the links for providing code and visualizations (when active)
 is a nice feature of this paper).

Also, did you compare to using the leaky ReLU activation function --
That would be interesting as it also doesn't have any areas of zero
slope?  Did you compare the generated digits to those obtained using GANs?

I am also curious, how does accuracy on digit classification differ
when trained only to optimize the forward error?

The MNIST site referenced lists 60,000 training data and test data of
10,000.  How/why did you select 42,000 and then split it to 39900 in
the train set and 2100 in the dev set?

Also, the goal for the paper is presented as creating highly
interpretable representations of the input data.  My interpretation of
interpretable is that the hidden units are ""interpretable"" and that it
is clear how the combined hidden unit representations allow for
accurate classification.  Towards that end, it would be nice to see
some of the interpretations of the hidden unit representations.  In
the abstract it states "" ...These representations are generated by
penalizing the learning of the network in such a way that those
learned representations correspond to the respective labels present in
the labelled dataset used for supervised training"".  Does this
statement refer only to the encoding of the representation vector or
also the hidden layers?  If the former, isn't that true for all
supervised algorithms.  If the latter, you should show this.

Batruni, R. (1991). A multilayer neural network with piecewise-linear
structure and backpropagation learning. IEEE Transactions on Neural
Networks, 2, 395–403.

Lin, J.-N., & Unbehauen, R. (1995). Canonical piecewise-linear neural
networks. IEEE Transactions on Neural Networks, 6, 43–50.

Lin, J.-N, & Unbehauen, R. (1990). Adaptive Nonlinear Digital Filter with Canonical Piecewise-Linear Structure,
IEEE Transactions on Circuits and Systems, 37(3) 347-353.

Gad, E.F et al (2000). A new algorithm for learning in piecewise-linear neural networks.
Neural Networks 13,  485-505.
","The reviewer finds the paper's ideas novel and interesting, using phrases like ""novel and interesting ideas"" and ""nice feature"". They also provide constructive criticism and suggestions for improvement, indicating a positive attitude towards the paper's potential. However, they also point out areas that need further exploration and clarification. Therefore, the sentiment is positive but not overly enthusiastic, leading to a score closer to moderately positive. The language is polite and professional throughout, with constructive criticism offered respectfully.",60,80
"The paper attempts to extend the predictive coding model to a multilayer network.  The math is developed for a learning rule, and some demonstrations are shown for reconstructions of CIFAR-10 images.

The overall idea and approach being pursued here is a good one, but the model needs further development.  It could also use better theoretical motivation - i.e., what sorts of representations do you expect to emerge in higher layers?  Can you demonstrate this with a toy example and then extend to real data?

That the model can reconstruct images per se is not particularly interesting.  What we would like to see is that it has somehow learned a more useful or meaningful representation of the data.  For example, what do the learned weights look like?  That would tell you something about what has been learned.
","The reviewer provides a mixed opinion, acknowledging the merit of the idea but outlining significant areas for improvement. Phrases like ""good one, but"" and ""not particularly interesting"" indicate constructive criticism rather than outright negativity. The language remains professional and suggestive, focusing on improvements without resorting to harsh or disrespectful tones.",40,70
"In the centre loss, the centre is learned. Now it's calculated as the average of the last layer's features
To enable training with SGD, the authors calculate the centre within a mini batch","The review is very short and doesn't offer explicit praise or criticism. It focuses solely on summarizing the paper's content related to a specific technical detail (centre loss calculation). Therefore, the sentiment is neutral. The language used is purely descriptive and objective, indicating a neutral politeness level.",0,0
"`The papers aims to provide a quality measure/test for GANs.   The objective is ambitious an deserve attention. As GANs are minimizing some f-divergence measure, the papers remarks that computing a  Wasserstein distance between two distributions made of a sum of Diracs is not a degenerate case and is tractable. So they propose evaluate the current approximation of a distribution learnt by a GAN by using this distance as a baseline performance (in terms of W distance and computed on a hold out dataset). 

A first remark is that the papers does not clearly develop the interest of puting things a trying to reach a treshold of performance in W distance rather than just trying to minimize the desired f-divergence. More specifically as they assess the performance in terms of W distance I would would be tempted to just minimize the given criterion. This would be very interesting to have arguments on why being better than the ""Dirac estimation"" in terms of W2 distance would lead to better performance for others tasks (as other f-divergences or image generation).

According to the authors the core claims are:
""1/ We suggest a formalisation of the goal of GAN training (/generative modelling more broadly) in terms of divergence minimisation. This leads to a natural, testable notion of generalisation. ""
Formalization in terms of divergence minimization is not new (see O. Bousquet & all https://arxiv.org/pdf/1701.02386.pdf ) and I do not feel like this paper actually performs any ""test"" (in a statistical sense). In my opinion the contribution is more about exhibiting a baseline which has to be defeated for any algorithm interesting is learning the distribution in terms of W2 distance.

""2/ We use this test to evaluate the success of GAN algorithms empirically, with the Wasserstein distance as our divergence.""
Here the distance does not seems so good because the performance in generation does not seems to only be related to W2 distance. Nevertheless, there is interesting observations in the paper about the sensitivity of this metric to the bluring of pictures. I would enjoyed more digging in this direction. The authors proposes to solve this issue by relying to an embedded space where the L2 distance makes more sense for pictures (DenseNet). This is of course very reasonable but I would expect anyone working on distribution over picture to work with such embeddings. Here I'm not sure if this papers opens a new way to improve the embedding making use on non labelled data. One could think about allowing the weights of the embeddings to vary while f-divergence is minimized but this is not done in the submitted work.

 ""3/ We find that whether our proposed test matches our intuitive sense of GAN quality depends heavily on the ground metric used for the Wasserstein distance.""
This claim is highly biased by who is giving the ""intuitive sense"". It would be much better evaluated thought a mechanical turk test.

 ""4/ We discuss how to use these insights to improve the design of WGANs more generally.""
As our understanding of the GANs dynamics are very coarse, I feel this is not a good thing to claim that ""doing xxx should improve things"" without actually trying it. 
","The review is quite mixed. It acknowledges the paper's ambition and some interesting observations (e.g., sensitivity to blurring). However, it also raises several concerns, questioning the novelty and significance of the contributions. The reviewer finds some claims not well-supported and suggests alternative approaches. The language is formal and academic, directly addressing the paper's claims and methodology. While critical, it maintains a professional tone without resorting to personal attacks or overly negative phrasing.",-10,60
"The authors propose a first implementation of spiking LSTMs. This is an interesting and open problem. However, the present work somewhat incomplete, and requires further experiments and clarifications.

Pros:
1. To my best knowledge, this is the first mapping of LSTMs to spiking networks
2. The authors tackle an interesting and challenging problem.

Cons:
1. In the abstract the authors mention that another approach has been taken, but is never stated what’s the problem that this new one is trying to address. Also, H&S 1997 tested several tasks, which is the one that the authors are referring to?
2. Figure 1 is not very easy to read. The authors can spell out the labels of the axis (e.g. S could be input, S)
3. Why are output and forget gates not considered here?
4. A major point in mapping LSTMs to spiking networks is its biological plausibility. However, the authors do not seem to explore this. Of particular interest is its relationship to a recent proposal of a cortical implementation of LSTMs (Cortical microcircuits as gated-RNNs, NIPS 2017).
5. The text should be improved, for example in the abstract: “that almost all resulting spiking neural network equivalents correctly..”, please rephrase.
6. Current LSTMs are applied in much more challenging problems than the original ones. It would be important to test one of this, perhaps the relatively simple pixel-by-pixel MNIST task. If this is not feasible, please comment.

Minor comments:
1. Change in the abstract “can be substituted for” > “can be substituted by”
2. A new body of research aims at using backprop in spiking RNNs (e.g. Friedemann and Ganguli 2017). The present work gets around this by training the analog version instead. It would be of interesting to discuss how to train spiking-LSTMs as this is an important topic for future research. 
3. As the main promise of using spiking nets (instead of rate) is their potential efficiency in neuromorphic systems, it would be interesting to contrast in the text the two options for LSTMs, and give some more quantitative analyses on the gain of spiking-LSTM versus rate-LSTMs in terms of efficiency.","The review acknowledges the novelty of the work and its potential but points out several areas for improvement. It highlights missing information, questions the depth of analysis (biological plausibility, comparison with rate-based LSTMs), and suggests additional experiments. The language, while direct, maintains a professional and constructive tone.",20,70
"This paper studies the problem of representation learning in hyperedges.  The author claims their novelty for using several different models to build hyperedge representations.  To generate representations for hyperedge, this paper proposes to use several different models such as Denoising AutoEncoder, tensor decomposition, word2vec or spectral embeddings. Experimental results show the effectiveness of these models in several different datasets. 

The author uses several different models (both recent studies like Node2Vec / sen2vec, and older results like spectral or tensor decomposition). The idea of studying embedding of a hypergraph is interesting and novel, and the results show that several different kinds of methods can all provide meaningful results for realistic applications. 

Despite the novel idea about hyperedge embedding generation, the paper is not easy to follow. 
The introduction of ``hypergraph`` takes too much spapce in preliminary, while the problem for generating embeddings of hyperedge is the key of paper. 

Further, the experiments only present several models this paper described. 
Some recent papers about hypergraph and graph structure (even though cannot generate embeddings directly) are still worth mention and compare in the experimental section. It will be persuasive to mention related methods in similar tasks. 

it would better better if the author can add some related work about hyperedge graph studies. ","The review acknowledges the novelty and interesting aspects of the paper's idea (positive sentiment). However, it also points out areas for improvement in clarity, structure, and completeness, suggesting the paper isn't perfect (neutralizing the sentiment towards the middle). The language used is constructive and suggestive, not demanding or aggressive, indicating politeness. ",40,70
"This paper proposes a geometric based approach to solving the problem of one-shot and few-shot learning. The basic idea is to use the feature vectors of a particular class to construct a simplex. (I am assuming the dimensions of the vectors are selected so as to exactly construct a simplex? It is not clearly written in the paper). The volume of the simplex is then taken to be a measure of class scatter, and classification happens by assigning the test feature vector to the nearest simplex, where the distances are normalized by the volume of the simplex. 

While the approach makes sense, I am not convinced that this geometric method plays an important role in increasing the performance on one-shot/few-shot tasks. In particular, one could try simpler approaches like k-NN where the distances to the cluster centers are also normalized by the variance within the clusters. I would suspect that this method is not superior to this simpler baseline. 

The other issue I have with this paper is misleading claims about being state of the art on Omniglot. In particular see Kaiser et al (ICLR 2017), where on 5-way-1-shot an accuracy of 98.4% is reached compared to 94.6% in this paper, and on 5-way-5-shot an accuracy of 99.6% is reached compared to 99.1% in this work. The paper also misses evaluations on various other data sets such as GNMT etc., on which Kaiser et al evaluated their approach.",The reviewer acknowledges the approach makes sense but expresses skepticism about its effectiveness and novelty. They find the claims about state-of-the-art results misleading and point out missing evaluations compared to existing work. The language is critical but remains professional and within the bounds of academic discourse.,-20,60
"# Summary
This paper proposes a neural network framework for solving binary linear programs (Binary LP). The idea is to present a sequence of input-output examples to the network and train the network to remember input-output examples to solve a new example (binary LP). In order to store such information, the paper proposes an external memory with non-differentiable reading/writing operations. This network is trained through supervised learning for the output and reinforcement learning for discrete operations. The results show that the proposed network outperforms the baseline (handcrafted) solver and the seq-to-seq network baseline.

[Pros]
- The idea of approximating a binary linear program solver using neural network is new.

[Cons]
- The paper is not clearly written (e.g., problem statement, notations, architecture description). So, it is hard to understand the core idea of this paper.
- The proposed method and problem setting are not well-justified. 
- The results are not very convincing.

# Novelty and Significance
- The problem considered in this paper is new, but it is unclear why the problem should be formulated in such a way. To my understanding, the network is given a set of input (problem) and output (solution) pairs and should predict the solution given a new problem. I do not see why this should be formulated as a ""sequential"" decision problem. Instead, we can just give access to all input/output examples (in a non-sequential way) and allow the network to predict the solution given the new input like Q&A tasks. This does not require any ""memory"" because all necessary information is available to the network.
- The proposed method seems to require a set of input/output examples even during evaluation (if my understanding is correct), which has limited practical applications. 

# Quality
- The proposed reward function for training the memory controller sounds a bit arbitrary. The entire problem is a supervised learning problem, and the memory controller is just a non-differentiable decision within the neural network. In this case, the reward function is usually defined as the sum of log-likelihood of the future predictions (see [Kelvin Xu et al.] for training hard-attention) because this matches the supervised learning objective. It would be good to justify (empirically) the proposed reward function. 
- The results are not fully-convincing. If my understanding is correct, the LTMN is trained to predict the baseline solver's output. But, the LTMN significantly outperforms the baseline solver even in the training set. Can you explain why this is possible?

# Clarity
- The problem statement and model description are not described well. 
1) Is the network given a sequence of program/solution input? If yes, is it given during evaluation as well?
2) Many notations are not formally defined. What is the output (o_t) of the network? Is it the optimal solution (x_t)? 
3) There is no mathematical definition of memory addressing mechanism used in this paper.
- The overall objective function is missing. 

[Reference]
- Kelvin Xu et al., Show, Attend and Tell: Neural Image Caption Generation with Visual Attention","The reviewer raises several serious concerns about the paper, questioning its clarity, justification, and results. They find the core idea hard to understand and disagree with the sequential framing of the problem. The reviewer also criticizes the reward function, lack of mathematical rigor, and questions the validity of the results surpassing the baseline even in training. The tone, while critical, maintains a professional and analytical approach without resorting to personal attacks.",-50,50
"This paper presents an important application of modern deep reinforcement learning (RL) methods to learning optimal treatments for sepsis from past patient encounters. From a methods standpoint, it offers nothing new but does synthesize best practice deep RL methods with a differentiable multi-task Gaussian Process (GP) input layer. This means that the proposed architecture can directly handle irregular sampling and missing values without a separate resampling step and can be trained end-to-end to optimize reward -- patient survival -- without a separate ad hoc preprocessing step. The experiments are thorough and the results promising. Overall, strong application work, which I appreciate, but with several flaws that I'd like the authors to address, if possible, during the review period. I'm perfectly willing to raise my score at least one point if my major concerns are addressed.

QUALITY

Although the core idea is derivative, the work is executed pretty well. Pros (+) and cons (-) are listed below:

+ discussion of the sepsis application is very strong. I especially appreciated the qualitative analysis of the individual case shown in Figure 4. While only a single anecdote, it provides insight into how the model might yield clinical insights at the bedside.
+ thorough comparison of competing baselines and clear variants -- though it would be cool to apply offline policy evaluation (OPE) to some of the standard clinical approaches, e.g., EGDT, discussed in the introduction.

- ""uncertainty"" is one of the supposed benefits of the MTGP layer, but it was not at all clear how it was used in practice, other than -- perhaps -- as a regularizer during training, similar to data augmentation.
- uses offline policy evaluation ""off-the-shelf"" and does not address or speculate the potential pitfalls or dangers of doing so. See ""Note on Offline Policy Evaluation"" below.
- although I like the anecdote, it tells us very little about the overall policy. The authors might consider some coarse statistical analyses, similar to Figure 3 in Raghu, et al. (though I'm sure you can come up with more and better analyses!). 
- there are some interesting patterns in Table 1 that the authors do not discuss, such as the fact that adding the MGP layer appears to reduce expected mortality more (on average) than adding recurrences. Why might this be (my guess is data augmentation)?

CLARITY

Paper is well-written, for the most part. I have some nitpicks about the writing, but in general, it's not a burden to read.

+ core ideas and challenges of the application are communicated clearly

- the authors did not detail how they chose their hyperparameters (number of layers, size of layers, whether to use dropout, etc.). This is critical for fully assessing the import of the empirical results.
- the text in the figures are virtually impossible to read (too small)
- the image quality in the figures is pretty bad (and some appear to be weirdly stretched or distorted)
- I prefer the X-axis labels that Raghu uses in their Figure 4 (with clinically interpretable increments) over the generic +1, +2, etc., labels used in Figure 3 here

Some nitpicks on the writing

* too much passive voice. Example: third paragraph in introduction (""Despite the promising results of EGDT, concerns arose.""). Avoid passive voice whenever possible.
* page 3, sec. 2.2 doesn't flow well. You bounce back and forth between discussion of the Markov assumption and full vs. partial observability. Try to focus on one concept at a time (and the solution offered by a proposed approach). Note that RNNs do NOT relax the Markov assumption -- they simply do an end run around it by using distributed latent representations.

ORIGINALITY

This work scores relatively low in originality. It really just combines ideas from two MLHC 2017 papers [1][2]. One could read those two papers and immediately conclude this paper's findings (the GP helps; RL helps; GP + RL is the best). This paper adds few (if any) new insights.

One way to address this would be to discuss in greater detail some potential explanations for why their results are stronger than those in Raghu and why the MTGP models outperform their simpler counterparts. Perhaps they could run some experiments to measure performance as a function of the number of MC samples (if perhaps grows with the number of samples, then it suggests that maybe it's largely a data augmentation effect).

SIGNIFICANCE

This paper's primary significance is that it provides further evidence that RL could be applied successfully to clinical data and problems, in particular sepsis treatment. However, this gets undersold (unsurprising, given the ML community's disdain for replication studies). It is also noteworthy that the MTGP gives such a large boost in performance for a relatively modest data set -- this property is worth exploring further, since clinical data are often small. However, again, this gets undersold.

One recommendation I would make is that the authors directly compare the results in this paper with those in Raghu and to point out, in particular, the confirmatory results. Interestingly, the shapes of the action vs. mortality rate plots (Figure 4 in Raghu, Figure 3 here) are quite similar -- that's not precisely replication, but it's comforting.

NOTE ON OFFLINE POLICY EVALUATION

This work has the same flaw that Raghu, et al., has -- neither justifies the use of offline policy evaluation. Both simply apply Jiang, et al.'s doubly robust approach [3] ""off the shelf"" without commenting on its accuracy in practice or discussing potential pitfalls (neither even considers [4] which seems to be superior in practice, especially with limited data). As far as I can tell (I'm not an RL expert), the DR approach carries stronger consistency guarantees and reduced variance but is still only as good the data it is trained on, and clinical data is known to have significant bias, particularly with respect to treatment, where clinicians are often following formulaic guidelines. Can we trust the mortality estimates in Table 1? Why or why not? Why shouldn't I think that RL is basically guaranteed to outperform non-RL approaches under an evaluation that is itself an RL model learned from the same training data!

While I'm willing to accept that this is the best we can do in this setting (we can't just try the learned policy on new patients!), I think this paper (and similar works, like Raghu, et al.) *must* provide a sober and critical discussion of its results, rather than simply applaud itself for getting the best score among competing approaches.

REFERENCES

[1] Raghu, et al. ""Continuous State-Space Models for Optimal Sepsis Treatment - a Deep Reinforcement Learning Approach."" MLHC 2017.
[2] Futoma, et al. ""An Improved Multi-Output Gaussian Process RNN with Real-Time Validation for Early Sepsis Detection."" MLHC 2017.
[3] Jiang, et al. ""Doubly robust off-policy value evaluation for reinforcement learning."" ICML 2016.
[4] Thompson and Brunskill. ""Data-Efficient Off-Policy Policy Evaluation for Reinforcement Learning."" ICML 2016.","The reviewer acknowledges the importance of the application and the thoroughness of the experiments, using terms like ""important application"", ""thorough comparison"", and ""promising results"". They also state their willingness to increase the score if major concerns are addressed, indicating an overall positive disposition. However, they also point out several flaws and areas for improvement, suggesting that the paper is not perfect. Therefore, the sentiment is positive but with reservations. The language used is quite direct and critical, but it maintains a professional and constructive tone. The reviewer provides specific examples and suggestions for improvement, indicating a desire to help the authors improve their work rather than simply criticize it.",60,70
"This paper proposed to transfer the classifier from the model for face classification to the task of alignment and verification. The problem setting is interesting and valuable, however, the contribution is not clearly demonstrated. 

Specifically, it proposed to utilize the teacher model from classification to other tasks, and proposed a unified objective function to model the transferability as shown in Equation (5). The two terms in (5), (7) and (9) are used to transfer the knowledge from the teacher model. It maybe possible to claim that the different terms may play different roles for different tasks. However, there should be some general guidelines for choosing these different terms for regularization, rather than just make the claim purely based on the final results. In table 4 and table 5, the results seem to be not so consistent for using the distillation loss. The author mentioned that it is due to the weak teacher model. However, the teacher model just differs in performance with around 3% in accuracy. How could we define the “good” or “bad” of a teacher model for model distillation/transfer?

Besides, it seems that the improvement comes largely from the trick of initialization as mentioned in Section 3.2. Hence, it is still not clear which parts contribute to the final performance improvements. It could be better if the authors can report the results from each of the components together. 

 The authors just try the parameter (\alpha, \beta) to be (0,0), (1,0), (0,1) and (1,1). I think the range for both values could be any positive real value, and how about the performance for other sets of combinations, like (0.5, 0.5)?","The review starts with a neutral statement, acknowledging the interesting problem setting. However, it quickly points out a lack of clear contribution and provides constructive criticism throughout. The reviewer questions the methodology and results, suggesting improvements and further experiments. The tone, while direct, remains professional and focused on improving the paper. Therefore, the sentiment is slightly negative due to the concerns raised, and the politeness is neutral as it maintains a professional academic tone.",-20,0
"Paper presents an interesting attention mechanism for fine-grained image classification. Introduction states that the method is simple and easy to understand. However, the presentation of the method is bit harder to follow. It is not clear to me if the attention modules are applied over all  pooling layers. How they are combined? 

Why use cross -correlation as the regulariser? Why not much stronger constraint such as orthogonality over elements of M in equation 1? What is the impact of this regularisation?

Why use soft-max in equation 1? One may use a Sigmoid as well? Is it better to use soft-max?

Equation 9 is not entirely clear to me. Undefined notations.

In Table 2, why stop from AD= 2 and AW=2?  What is the performance of AD=1, AW=1 with G? Why not perform this experiment over all 5 datasets? Is this performances, dataset specific?

The method is compared against 5 datasets. Obtained results are quite good.

","The review starts with a cautiously positive note, acknowledging the interesting aspects of the paper. However, it quickly delves into a series of critical questions and points out areas of improvement. The reviewer's use of phrases like ""not clear"" and ""why not"" indicates a critical but not disrespectful tone. The final sentence, while positive, suggests that the paper's merit lies in its results rather than its flawless execution.",40,60
"This paper studies the generalization properties of 2-layer neural networks based on Fourier analysis. Studying the generalization property of neural network is an important problem and Fourier-based analysis is a promising direction, as shown in (Lee et al., 2017). However, I am not satisfied with the results in the current version.

1) The main theoretical results are on the sin activation functions instead of commonly used ReLU functions. 

2) Even if for sin activation functions, the analysis is NOT complete. The authors claimed in the abstract that gradient-based methods will converge to generalizable local minima. However, Corollary 3 is only a concentration bound on the gradient. There is a gap that how this corollary implies generalization. The paragraph below this corollary is only a high level intuition. 


","The review starts with a positive statement acknowledging the importance of the research topic and the chosen approach. However, it quickly dives into criticism of the work, pointing out limitations and gaps in the presented results. The reviewer's dissatisfaction is evident. While the language is critical, it maintains a professional and academic tone, avoiding personal attacks or disrespectful language. Therefore, the sentiment leans negative, but the politeness remains neutral.",-30,0
"The authors propose to use pretrained node embeddings in a deep learning model for link weight prediction in graphs. 
The embedding of the source node and the destination node are concatenated and fed into a fully connected neural network which produces the link weight as its output.
Existing work by Hou and Holder 2017 trains the same architecture, but the node embeddings are learned together with the weights of the neural network. In my professional opinion, the idea of using pretrained node embeddings and training only the neural network is not enough of a contribution.

Since the proposed method does not build on the SBM or pWSBM the detailed equations on page 2 are not necessary. Also, Figure 1, 2, and 3 are not necessary. Fully connected neural networks are widely used and can be explained briefly without drawing the architecture. 

Pros:
+ interesting problem
+ future work. evaluation of embeddings is indeed a hard problem worth solving.

Cons:
- not novel","The review is mainly negative. The reviewer does not see the novelty of the paper's proposed method and deems it ""not enough of a contribution."" While they praise the ""interesting problem"" and ""future work,"" the criticism outweighs the positive remarks. The language used is formal and professional, without resorting to harsh or impolite language.",-40,60
"The paper uses an echo state network to learn to classify image transformations (between pairs of images) into one of fives classes.  The image data is artificially represented as a time series, and the goal is generalization of classification ability to unseen image pairs.  The network dynamics are studied and are claimed to have explanatory power.

The paper is well-written and easy to follow, but I have concerns about the claims it makes relative to how convincing the results are.  The focus is on one simple, and frankly now-overused data set (MNIST).  Further, treating MNIST data as a time series is artificial and clunky.  Why does the series go from left to right rather than right to left or top to bottom or inside out or something else?  How do the results change if the data is ""temporalized"" in some other way?

For training in Section 2.4, is M the number of columns for a pair of images?  It's not clear how pairs are input in parallel--- one after the other? Concatenated? Interleaved columns?  Something else? What are k, i, j in computing $\delta X_k$?  Later, in Section 3.2, it says, ""As in section 2.2, $xl(mn)$ is the differential reservoir state value of the $m$th reservoir node at time $n$ for input image $l$"", but nothing like this is discussed in Section 2.2; I'm confused.

The generalization results on this one simple data set seem pretty good.  But, how does this kind of approach do on other kinds of or more complex data?  I'm not sure that RC has historically had very good success scaling up to ""real-world"" problems to date.

Table 1 doesn't really say anything.  Of course, the diagonals are higher than the off diagonals because these are dot products.  True, they are dot products of averages over different inputs (which is why they are less than 1), but still.  Also, what Table 1 really seems to say is that the off-diagonals really aren't all that different than the diagonals, and that especially the differences between same and different digits is not very different, suggesting that what is learned is pretty fragile and likely won't generalize to harder problems.  I like the idea of using dynamical systems theory to attempt to explain what is going on, but I wonder if it is not being used a bit simplistically or naively.

Why were the five transform classes chosen?  It seems like the ""transforms"" a (same) and e (different) are qualitatively different than transforms b-d (rotated, scaled, blurred).  This seems like it should talked about.

""Thus, we infer, that the reservoir is in fact, simply training these attractors as opposed to training the entire reservoir space.""  What does this mean?  The reservoir isn't trained at all in ESNs (which is also stated explicitly for the model presented here)…

For 3.3, why did were those three classes chosen? Was this experiment tried with other subsets of three classes?  Why are results reported on only the one combination of rotated/blurred vs. rotated?  Were others tried?  If so, what were the results?  If not, why?  How does the network know when to take more than the highest output (so it can say that two transforms have been applied)?  In the case of combination, counting either transform as the correct output kind of seems like cheating a bit—it over states how well the model is doing.  Also, does the order in which the transforms are applied affect their relative representative strength in the reservoir?

The comparison with SNNs is kind of interesting, but I'm not sure that I'm (yet) convinced, as there is little detail on how the experiment was performed and what was done (or not) to try to get the SNN to generalize.  My suspicion is that with the proper approach, an SNN or similar non-dynamical system could generalize well on these tasks.  The need for a dynamical system could be argued to make sense for the camera task, perhaps, as video frames naturally form a time series; however, as already mentioned, for the MNIST data, this is not the case, and the fact that the SNN does not generalize here seems likely due to their under utilization rather than due to an inherent lack of capability.

I don't believe that there is sufficient support for this statement in the conclusion, ""[ML/deep networks] do not work as well for generalization of learning. In generalized learning, RCNs outperform them, due to their ability to function as a dynamical system with ‘memory’.""  First of all, ML is all about generalization, and there are lots and lots and lots of results showing that many ML systems generalize very well on a wide variety of problems, well beyond just classification, in fact.  And, I don't think the the paper has convincingly shown that a dynamical system 'memory' is doing something especially useful, given that the main task studied, that of character recognition (or classification of transformation or even transformation itself), does not require such a temporal ability.
","The reviewer expresses a mixed sentiment, acknowledging the paper's strengths (well-written, easy to follow, good generalization on the tested dataset) but also raising significant concerns. They find the choice of MNIST and its artificial temporalization questionable, criticize the lack of detail and justification for several methodological choices, and find the claims regarding the superiority of ESNs over other methods, particularly regarding generalization and the role of 'memory,' to be insufficiently supported. The tone is critical but professional and not disrespectful.",-20,60
"This paper examines a number of sentence and document embedding methods for automatic summarization. It pairs a number of recent sentence embedding algorithms (e.g., Paragraph Vectors and Skip-Thought Vectors) with several simple summarization decoding algorithms for sentence selection, and evaluates the resulting output summary on DUC 2004 using ROUGE, based on the general intuition that the selected summary should be similar to the original document in the vector space induced by the embedding algorithm. It further provides a number of analyses of the sentence representations as they relate to summarization, and other aspects of the summarization process including the decoding algorithm.

The paper was well written and easy to understand. I appreciate the effort to apply these representation techniques in an extrinsic task.

However, the signficance of the results may be limited, because the paper does not respond to a long line of work in summarization literature which have addressed many of the same points. In particular, I worry that the paper may in part be reinventing the wheel, in that many of the results are quite incremental with respect to previous observations in the field.

Greedy decoding and non-redundancy: many methods in summarization use greedy decoding algorithms. For example, SumBasic (Nenkova and Vanderwende, 2005), and HierSum (Haghighi and Vanderwende, 2009) are two such papers. This specific topic has been thoroughly expanded on by the work on greedy decoding for submodular objective functions in summarization (Lin and Bilmes, 2011), as well as many papers which focus on how to optimize for both informativeness and non-redundancy (Kulesza and Taskar, 2012). 
	
The idea that the summary should be similar to the entire document is known as centrality. Some papers that exploit or examine that property include (Nenkova and Vanderwende, 2005; Louis and Nenkova, 2009; Cheung and Penn, 2013)
 
Another possible reading of the paper is that its novelty lies in the evaluation of sentence embedding models, specifically. However, these methods were not designed for summarization, and I don't see why they should necessarily work well for this task out of the box with simple decoding algorithms without finetuning. Also, the ROUGE results are so far from the SotA that I'm not sure what the value of analyzing this suite of techniques is.
  
In summary, I understand that this paper does not attempt to produce a state-of-the-art summarization system, but I find it hard to understand how it contributes to our understanding of future progress in the summmarization field. If the goal is to use summarization as an extrinsic evaluation of sentence embedding models, there needs to be better justification of this is a good idea when there are so many other issues in content selection that are not due to sentence embedding quality, but which affect summarization results.

References:

Nenkova and Vanderwende, 2005. The impact of frequency on summarization. Tech report.
Haghighi and Vanderwende, 2009. Exploring content models for multi-document summarization. NAACL-HLT 2009.
Lin and Bilmes, 2011. A class of submodular functions for document summarization. ACL-HLT 2011.
Kulesza and Taskar, 2012. Learning Determinantal Point Processes.
Louis and Nenkova, 2009. Automatically evaluating content selection in summarization without human models. EMNLP 2009.
Cheung and Penn, 2013. Towards Robust Abstractive Multi-Document Summarization: A Caseframe Analysis of Centrality and Domain. ACL 2013.

Other notes:
The acknowledgements seem to break double-blind reviewing.","The review starts with positive remarks, praising the paper's clarity and the effort to apply representation techniques to an extrinsic task. This suggests a positive sentiment. However, the review quickly transitions into outlining significant limitations, arguing that the paper might be reinventing the wheel and doesn't offer much novelty. The reviewer also questions the value of the analysis due to low ROUGE scores and lack of contribution to future progress.  The numerous concerns and the conclusion that the reviewer finds it ""hard to understand how it contributes"" strongly indicate a negative sentiment overall. 

Despite the critical feedback, the language remains polite and professional throughout. The reviewer uses phrases like ""I appreciate,"" ""I worry that,"" and ""I find it hard to understand"" which express concerns without resorting to harsh or disrespectful language.",-50,70
"The authors of the paper advocate injecting noise into the activations of recurrent networks for regularisation. This is done  by replacing the deterministic units with stochastic ones.

The paper has several issues with respect to the method and related work. 

- The paper needs to mention [Graves 2011], which is one of the first works to inject noise into the dynamics of an RNN. It is also important to know how these two approaches differ. E.g.: Under what conditions are the two approaches equivalent? How do they compare experimentally?
- While [Bayer & Osendorfer, 2014] and [Chung et al, 2015] appear in the list of references, these works are not discussed in the main text. I personally think these are extremely related, pioneering the use of stochastic units in a recurrent context. In the end, the original paper can be cast in these frameworks approximately by removing the KL term of the ELBO. This might be ok by itself, but that the authors are apparently aware of the work (as it is in the list of references) and not discussing them in the main text makes me highly skeptical.
- The method is introduced for general exponential families, but a) not empirically evaluated for more than the Gaussian case and b) not a complete algorithm for e.g. the Bernoulli case. More specifically, the reader is left alone with the problem of estimating the gradients in the Bernoulli case, which is an active area of research by itself.
- The paper makes use of the reparameterisation trick, but does not cite the relevant literature, e.g. [Kingma 2013, Rezende 2014, and another one I currently struggle to find].
- The desiderate for noise seem completely arbitrary to me and are not justified. I don’t see why violation of any of them would lead to an inferior regularisation method.

### References
[Graves 2011] Graves, Alex. ""Practical variational inference for neural networks."" Advances in Neural Information Processing Systems. 2011.
[Kingma 2013] Kingma, Diederik P., and Max Welling. ""Auto-encoding variational bayes."" arXiv preprint arXiv:1312.6114 (2013).
[Rezende 2014] Rezende, Danilo Jimenez, Shakir Mohamed, and Daan Wierstra. ""Stochastic backpropagation and approximate inference in deep generative models."" arXiv preprint arXiv:1401.4082 (2014).
","The review is highly critical of the paper, pointing out several major flaws and omissions. The reviewer uses strong language like ""several issues,"" ""makes me highly skeptical,"" and ""completely arbitrary"" to express their negative opinion. However, the language remains within the bounds of academic discourse and avoids personal attacks.",-70,20
"Summary:
The submission proposes a simple modification to the Value Iteration Networks (VIN) method of Tamar et al., basically consisting of assuming a stochastic policy and replacing the max-over-actions in value iteration with an expectation that weights actions proportional to their exponentiated Q-values. Since this change removes the main nondifferentiability of VINs, it is hypothesized that the resulting method will be easier to train than VINs, and experiments seem to support this hypothesis.

Pros:
+ The proposed modification to VIN is simple, well-motivated, and addresses the nondifferentiability of VIN
+ Experiments on synthetic data demonstrate a significant improvement over the standard VIN method

Cons:
+ Some important references are missing (e.g., MaxEnt IOC with deep-learned features)
+ Although intuitive, more detailed justification could be provided for replacing the max-over-actions with an exponentially-weighted average
+ No baselines are provided for the experiments with real data
+ All the experimental scenarios are fairly simple (2D grid-worlds with discrete actions, 1-channel input features)

The proposed method is simple, well-motivated, and addresses a real concern in VINs, which is their nondifferentiability. Although many of the nonlinearities used in CNNs for computer vision applications are nondifferentiable, the theoretical grounds for using these in conjunction with gradient-based optimization is obviously questionable. Despite this, they are widely used for such applications because of strong empirical results showing that such nonlinearities are beneficial in image-processing applications. However, it would be incorrect to assume that because such nonlinearities work for image processing, they are also beneficial in the context of unrolling value iteration.

Replacing the max-over-actions with an exponentially-weighted average is an intuitively well-motivated alternative because, as the authors note, it incorporates the values of suboptimal actions during the training procedure. We would therefore expect better or faster training, as the values of these suboptimal actions can be updated more frequently. The (admittedly limited) experiments bear out this hypothesis.

Perhaps the most significant downside of this work is that it fails to acknowledge prior work in the RL and IOC literature that result in similar  “smoothed” or “softmax"" Bellman updates: in particular, MaxEnt IOC [A] and linearly-solvable MDPs [B] both fall in this category. Both of those papers clearly derive approximate Bellman equations from modified optimal control principles; although I believe this is also possible for the proposed update (Eq. 11), along the lines of the sentence after Eq. 11, this should be made more explicit/rigorous, and the result compared to [A,B].

Another important missing reference is [C], which learned cost maps with deep neural networks in a MaxEnt IOC framework. As far as I can tell, the application is identical to that of the present paper, and [C] may have some advantages: for instance, [C] features a principled, fully-differentiable training objective while also avoiding having to backprop through the inference procedure, as in VIN. Again, this raises the question of how the proposed method compares to MaxEnt IOC, both theoretically and experimentally.

The experiments are also a bit lacking in a few ways. First, a baseline is only provided for the experiments with synthetic data. Although that experiment shows a promising, significant advantage over VIN, the lack of baselines for the experiment with real data is disappointing. Furthermore, the setting for the experiments is fairly simple, consisting of a grid-world with 1-channel input features. The setting is simple enough that even shallow IOC methods (e.g., [D]) would probably perform well; however, the deep IOC methods of [C] is also applicable and should probably also be evaluated as a baseline.

In summary, although the method proposes an intuitively reasonable modification to VIN that seems to outperform it in limited experiments, the submission fails to acknowledge important related work (especially the MaxEnt IOC methods of [A,D]) that may have significant theoretical and practical advantages. Unfortunately, I believe the original VIN paper also failed to articulate the precise advantages of VIN over this prior work—which is not to say there are none, but it is clear that VINs applied to problems as simple as the one considered here have real competitors in prior work. Clarifying this connection, both theoretically and experimentally, would make this work much stronger and would be a valuable contribution to the literature.

[A] Ziebart, Brian D. Modeling purposeful adaptive behavior with the principle of maximum causal entropy. Carnegie Mellon University, 2010.
[B] Todorov, Emanuel. ""Linearly-solvable Markov decision problems."" Advances in neural information processing systems. 2007.
[C] Wulfmeier et al. Watch This: Scalable Cost-Function Learning for Path Planning in Urban Environments. IROS 2016
[D] Ratliff, Nathan D., David Silver, and J. Andrew Bagnell. ""Learning to search: Functional gradient techniques for imitation learning."" Autonomous Robots 27.1 (2009): 25-53.
","The review acknowledges the merits of the proposed method, highlighting its simplicity, motivation, and improvement over the standard VIN method. However, it also points out significant downsides, including missing important references and limitations in the experiments. The reviewer suggests that clarifying the connection with prior work would make the work stronger. Overall, the sentiment is mixed, leaning towards the positive side due to the acknowledged merits, but with reservations. The language used is polite and professional, providing constructive criticism and suggestions for improvement.",50,80
"Make SVM great again with Siamese kernel for few-shot learning 

** PAPER SUMMARY **

The author proposes to combine siamase networks with an SVM for pair classification. The proposed approach is evaluated on few shot learning tasks, on omniglot and timit.  


** REVIEW SUMMARY **

The paper is readable but it could be more fluent. It lacks a few references and important technical aspects are not discussed. It contains a few errors. Empirical contribution seems inflated on omniglot as the authors omit other papers reporting better results. Overall, the contribution is modest at best.

** DETAILED REVIEW **

On mistakes, it is wrong to say that an SVM is a parameterless classifier. It is wrong to cite (Boser et al 92) for the soft-margin SVM. I think slack variables come from (Cortes et al 95). ""consistent"" has a specific definition in machine learning https://en.wikipedia.org/wiki/Consistent_estimator , you must use a different word in 3.2. You mention that a non linear SVM need a similarity measure, it actually need a positive definite kernel which has a specific definition, https://en.wikipedia.org/wiki/Positive-definite_kernel .

On incompleteness, it is not obvious how the classifier is used at test time. Could you explain how classes are predicted given a test problem? The setup of the experiments on TIMIT is extremely unclear. What are the class you are interested in? How many classes and examples does the testing problems have? 

On clarity, I do not understand why you talk again about non-linear SVM in the last paragraph of 3.2. since you mention at the end of page 4 that you will only rely on linear SVMs for computational reasons. You need to mention explicitely somewhere that (w,\theta) are optimized jointly. The sentence ""this paper investigates only the one versus rest approach"" is confusing, as you have only two classes from the SVM perspective i.e. pairs (x1,x2) where both examples come from the same class and pairs (x1,x2) where they come from different class. So you use a binary SVM, not one versus rest. You need to find a better justification for using L2-SVM than ""L2-SVM loss variant is considered to be the best by the author of the paper"", did you try classical SVM and found them performing worse? Also could you motivate your choice for L1 norm as opposed to L2 in Eq 3?

On empirical evaluation, I already mentioned that it impossible to understand what the classification problem on TIMIT is. I suspect it might be speaker identification. So I will focus on the omniglot experiments. 

Few-Shot Learning Through an Information Retrieval Lens, Eleni Triantafillou, Richard Zemel, Raquel Urtasun, NIPS 2017 [arxiv July'17]

and the reference therein give a few more recent baselines than your table. Some of the results are better than your approach. I am not sure why you do not evaluate on mini-imagenet as well as most work on few shot learning generally do. This dataset offers a clearer experimental setup than your TIMIT setting and has abundant published baseline results. Also, most work typically use omniglot as a proof of concept and consider mini-imagenet as a more challenging set. ","The review is critical of the paper, pointing out errors, incompleteness, lack of clarity, and potentially inflated empirical contributions. While the language is direct and points out shortcomings, it does not resort to personal attacks or disrespectful tone. The reviewer provides concrete examples and suggestions for improvement.",-30,60
"This paper is an experimental paper. It investigates what sort of image representations are good for image captioning systems. 

Overall, the idea seems relevant and there are some good findings but I am sure that image captioning community is already aware of these findings.

The main issue of the paper is the lack of novelty. Even for an experimental paper, I would argue that novelty in the experimental methodology is an important fact. Unfortunately, I do not see any novel concept in the experimental setup.

I recomend this paper for a workshop presentation.
","The reviewer finds the paper's topic relevant and acknowledges some good findings. However, they clearly state the main issue is a lack of novelty, which significantly impacts their recommendation. The language used doesn't express strong negativity but rather leans towards neutrality with a clear message of lacking novelty.  Therefore, the sentiment is somewhat negative, but the politeness remains fairly neutral.",-20,20
"This paper proposes a model for learning to generate data conditional on attributes. Demonstrations show that the model is capable of learning to generate data with attribute combinations that were not present in conjunction at training time.

The model is interesting, and the results, while preliminary, suggest that the model is capable of making quite interesting generalizations (in particular, it can synthesize images that consist of settings of features that have not been seen before).

However, this paper is mercilessly difficult to read. The most serious problems are the extensive discussion of the fully unsupervised variant (rather than the semisupervised variant that is evaluated), poor use of examples when describing the model, nonstandard terminology (“concepts” and “context” are extremely vague terms that are not defined precisely) and discussions to vaguely related work that does not clarify but rather obscures what is going on in the paper.

For the evaluation, since this paper proposes a technique for learning a posterior recognition model, it would be extremely interesting to see if the model is capable of recognizing images appropriately that combine “contexts” that were not observed during training. The experiments show that the generation component is quite effective, but this is an obvious missing step.

Anyway, some other related work:
Lample et al. (2017 NIPS). Fader Networks. I realize this work is more ambitious since it seeks to be a fully generative model including of the contexts/attributes. But I mostly bring it up because it is an impressively clear presentation of a model and experimental set up.","The reviewer finds the paper's core idea interesting and promising, evidenced by statements like ""The model is interesting"" and acknowledging the potential for ""quite interesting generalizations."" However, they express significant concerns about the clarity and presentation of the paper, using terms like ""mercilessly difficult to read,"" ""poor use of examples,"" ""nonstandard terminology,"" and ""obscures what is going on."" This suggests an overall mixed sentiment, leaning towards the negative due to the severity of the clarity issues. While the reviewer uses strong language to criticize the paper, it doesn't come across as personally offensive or disrespectful to the authors. The suggestions for improvement are presented matter-of-factly.",-20,40
"This paper proposes integrating information from a semantic resource that quantifies the affect of different words into a text-based word embedding algorithm. 

The affect lexical seems to be a very interesting resource (although I'm not sure what it means to call it 'state of the art'), and definitely support the endeavour to make language models more reflective of complex semantic and pragmatic phenomena such as affect and sentiment. 

The justification for why we might want to do this with word embeddings in the manner proposed seems a little unconvincing to me:

- The statement that 'delighted' and 'disappointed' will have similar contexts is not evident to me at least (other then them both being participle / adjectives).

- Affect in language seems to me to be a very contextual phenomenon. Only a tiny subset of words have intrinsic and context-free affect. Most affect seems to me to come from the use of words in (phrasal, and extra-linguistic) contexts, so a more context-dependent model, in which affect is computed over phrases or sentences, would seem to be more appropriate. Consider words like 'expensive', 'wicked', 'elimination'...

The model proposes several applications (sentiment prediction, predicting email tone, word similarity) where the affect-based embeddings yield small improvements. However, in different cases, taking different flavours of affect information (V, A or D) produces the best score, so it is not clear what to conclude about what sort of information is most useful. 

It is not surprising to me that an algorithm that uses both WordNet and running text to compute word similarity scores improves over one that uses just running text. It also not surprising that adding information about affect improves the ability to predict sentiment and the tone of emails. 

To understand the importance of the proposed algorithm (rather than just the addition of additional data), I would like to see comparison with various different post-processing techniques using WordNet and the affect lexicon (i.e. not just Bollelaga et al.) including some much simpler baselines. For instance, what about averaging WordNet path-based distance metrics and distance in word embedding space (for word similarity), and other ways of applying the affect data to email tone prediction?

","The reviewer provides a mixed opinion, acknowledging the interesting aspects of the paper's approach while expressing reservations about the justification and methodology. While they find the affect lexicon intriguing and support incorporating affect into language models, they find the paper's arguments for its specific application to word embeddings unconvincing. The reviewer also points out the lack of clear conclusions from the results and suggests additional comparisons with simpler baselines. The language used is polite and constructive, offering specific suggestions for improvement rather than outright criticism. Overall, the review leans slightly towards the negative due to the concerns raised, but it maintains a professional and respectful tone.",-20,70
"SUMMARY
The paper deal with the problem of RL.  It proposes a non-parametric approach that maps trajectories to the optimal policy.  It avoids learning parameterized policies.  The fundamental idea is to store passed trajectories.  When a policy is to be executed, it does nearest neighbor search to find then closest trajectory and executes it.

COMMENTS

What happens if the agent finds it self  in a state that while is close to a state in the similar trajectory the action required to could be completely different.

Not certain about the claim that standard RL policy learning algorithms make it difficult to assess the difficulty of a problem. 

How do you execute a trajectory? Actions in RL are by definition stochastic, and this would make it unlikely that a same trajectory can be reproduced exactly.
",The review does not give an overall positive or negative impression and is rather neutral in its sentiment. It raises valid concerns and questions about the paper's approach in a neutral and professional manner. The reviewer asks important clarifications without dismissing the paper's core ideas.,0,50
"The paper is about open sourcing AirNet, a database that has interpolated air quality metrics in a spatial form along with matching meteorological data obtained elsewhere. In addition, the paper also develops a few baseline methods and evaluated using standard metrics such as detection rate, false alarms etc. The work is original and significant from an applications point of view. It looks like the dataset is useful but the model development and experimental sections are weak.

Strengths:
- open source data set for air quality monitoring that is significantly better than existing ones.
- baseline models using standard methods including RNN.

Weaknesses:
- The air quality data is measured at point locations (stations) which are interpolated to obtain spatial data. There is no evaluation on this step to make sure the interpolated data indeed reflects truth. 
- Experiments doesn't seem to be carefully done using hyper-parameter tuning/ cross-validation. The model results may be misleading.
- Writing and formatting needs to be improved. Some examples - ""quality of air quality"", ""people attempted to apply deep learning"", ""in the computer vision field ."", ""Some people also used the hidden Makov model"", ""radial of longitude"", ""in 2:00AM, January 23"". The paper in general was not easy to follow at many places.
- Is Table 3 incomplete with one box unlabeled?
- Figure 3 is not clear. It is suggested to follow standard notations to represent the RNN structure (see Jurgen Schmidhuber's paper)
- ""DEV"" in table 4 is not explained. Is this a development set? If so, what does it mean?
- It is said that ""reduced LSTM is improved than LSTM"". But the test results in Table 4 shows that LSTM is better.","The reviewer acknowledges the significance and originality of the work, particularly the value of the open-source dataset. This points towards a positive sentiment. However, they also point out significant weaknesses in the experimental methodology and presentation, suggesting a less positive view of these aspects. Overall, the sentiment is mixed but leans more towards the positive side due to the dataset's value. The language used is generally polite and professional. While the reviewer points out shortcomings, they do so in a constructive manner, suggesting specific improvements. The tone remains critical but not disrespectful.",60,70
"The paper proposes an analysis on different adaptive regularization techniques for deep transfer learning. 
Specifically it focuses on the use of an L2-SP condition that constraints the new parameters to be close to the
ones previously learned when solving a source task. 

+ The paper is easy to read and well organized
+ The advantage of the proposed regularization against the more standard L2 regularization is clearly visible 
from the experiments

- The idea per se is not new: there is a list of shallow learning methods for transfer learning based 
on the same L2 regularization choice
[Cross-Domain Video Concept Detection using Adaptive SVMs, ACM Multimedia 2007]
[Learning categories from few examples with multi model knowledge transfer, PAMI 2014]
[From n to n+ 1: Multiclass transfer incremental learning, CVPR 2013]
I believe this literature should be discussed in the related work section

- It is true that the L2-SP-Fisher regularization was designed for life-long learning cases with a 
fixed task, however, this solution seems to work quite well in the proposed experimental settings. 
From my understanding L2-SP-Fisher can be considered the best competitor of L2-SP so I think
the paper should dedicate more space to the analysis of their difference and similarities both
from the theoretical and experimental point of view. For instance:
--  adding the L2-SP-Fisher results in table 2
--  repeating the experiments of figure 2 and figure 3 with L2-SP-Fisher




","The reviewer provides a mixed opinion, acknowledging the paper's strengths (well-written, clear advantage of the proposed method) but also pointing out significant shortcomings (lack of novelty, insufficient comparison with a relevant existing technique). The tone is critical but professional and offers constructive suggestions for improvement. ",20,60
"The main contribution of this paper are:
(a) replacing the typical maximum likelihood criterion in neural language model training with a discriminative criterion,
(b) propose two large margin criterion -- difference in likelihood and difference in rank (WER or BLUE ordered) hypotheses,
(c) demonstrate performance gains two standard tasks -- an ASR task on Wall Street Journal (small task) and an MT task.

In addition, they provide examples in Figure (1) and (2) that illustrate the effect of the cost function on training. Their illustration in Figure 4 is also helpful in seeing the impact of using a warm start with a generative model.
","The review explicitly points out the main contributions of the paper and acknowledges the helpfulness of the illustrations. It does not contain any negative statements. Therefore, the sentiment is positive. The language used is factual, professional, and void of any negative or informal phrasing, indicating a high level of politeness.",60,80
"This paper proposes an anytime neural network, which can predict anytime while training. To achieve that, the model includes auxiliary predictions which can make early predictions. Specifically, the paper presents a loss weighting scheme that considers high correlation among nearby predictions, an oscillating loss weighting scheme for further improvement, and an ensemble of anytime neural networks. In the experiments, test error of the proposed model was shown to be comparable to the optimal one at each time budget. 

It is an interesting idea to add auxiliary predictions to enable early predictions and the experimental results look promising as they are close to optimal at each time budget. 

1. In Section 3.2, there are some discussions on the parallel computations of EANN. The parallel training is not clear to me and it would be great to have more explanation on this with examples.  

2. It seems that EANN is not scalable because the depth is increasing exponentially. For example, given 10 machines, the model with the largest depth would have 2^10 layers, which is difficult to train. It would be great to discuss this issue.

3. In the experiments, it would be great to add a few alternatives to be compared for anytime predictions. 







","The reviewer provides a positive summary of the paper's contributions, highlighting the interesting idea and promising results. While they raise valid questions and suggestions for improvement, these are presented constructively and aim to enhance the paper's clarity and completeness. ",60,80
"Summary:

This paper studies how to learn (hidden)-state-space models of environment dynamics, and integrate them with Imagination-Augmented Agents (I2A). The paper considers single-agent problems and tests on Ms Pacman etc.

There are several variations of the hidden-state space [ds]SSM model: using det/stochastic latent variables + using det/stochastic decoders. In the stochastic case, learning is done using variational methods. 

[ds]SSM is integrated with I2A, which generates rollouts of future states, based on the inferred hidden states from the d/sSSM-VAE model. The rollouts are then fed into the agent's policy / value function.

Main results seem to be:
1. Experiments on learning the forward model, show that latent forward models work better and faster than naive AR models on several Atari games, and better than fully model-free baselines. 
2. I2A agents with latent codes work better than model-free models or I2A from pixels. Deterministic latent models seem to work better than stochastic ones.

Pro:
- Relatively straightforward idea: learn the forward model on hidden states, rather than raw states.
- Writing is clear, although a bit dense in places.

Con:
- Paper only shows training curves for MS Pacman. What about the other games from Table 1?
- The paper lacks any visualization of the latent codes. What do they represent? Can we e.g. learn a raw-state predictor from the latent codes?
- Are the latent codes relevant in the stochastic model? See e.g. the discussion in ""Variational Lossy Autoencoder"" (Chen et al. 2016)
- Experiments are not complete (e.g. for AR, as noted in the paper).
- The games used are fairly reactive (i.e. do not require significant long-term planning), and so the sequential hidden-state-space model does not have to capture long-term dependencies. It would be nice to see how this technique fares on Montezuma's revenge, for instance.

Overall:
The paper proposes a simple idea that seems to work well on reactive 1-agent games. However, the paper could give more insights into *how* this works: e.g. a better qualitative inspection of the learned latent model, and how existing questions surrounding sequential stochastic model affect the proposed method. Also, not all baseline experiments are done, and the impact on training is only evaluated on 1 game. 

Detailed:
-
","The review is generally positive in its assessment of the paper's core idea (learning forward models on hidden states) and acknowledges the clarity of the writing. However, it also raises several significant concerns and calls for more comprehensive experiments and analysis. The reviewer points out missing visualizations, incomplete baseline comparisons, and a lack of in-depth exploration of the learned latent model. The use of ""fairly reactive"" games is also noted as a limitation. Overall, the feedback leans towards the positive side due to the promising nature of the idea and clear presentation, but with significant reservations about the depth and completeness of the work.",40,80
"This work proposes to learn word vectors that are intended to specifically model the lexical entailment relationship. This is achieved in an unsupervised manner from unstructured data, through an approach heavily influenced by recent work by Henderson and Popa, which ""reinterprets word2vec"" by modeling distributions over discrete latent ""pseudo-phrase"" vectors. That is, instead of using two vectors per word, as in word2vec, a latent representation is introduced that models the joint properties of the target and context words. While Henderson and Popa represent the latent vector as the evidence for the target and context, or the likelihood, this work suggests to represent it based on the posterior distribution instead. The resultant representations are evaluated on Weeds et al.'s (2014) version of BLESS, as well as the full BLESS dataset, where they do better than the original.

The paper is confusingly written, fails to mention a lot of related work, has a weak evaluation where it doesn't compare to related systems, and I feel that it would benefit from ""toning down"". Hence, I do not recommend it for acceptance. In more detail:

1. The idea behind Henderson and Popa's model, as well as the suggested modification, should be easy to explain, but I really had to struggle to make sense of it. This work relies very heavily on that paper, and would be better off if it was more standalone. I think part of the confusion stems from using y for the latent representation but not specifying whether it is a word or latent representation in Equation 1 - that only becomes obvious later. The exposition clearly needs more work, and more precise technical writing.

2. There is a lot of related work around word embeddings that is not mentioned, both on word2vec-style representation learning (e.g. it would be useful to relate this more to word2vec and what it learns, as in Omer Levy's work on ""interpreting"" word2vec, rather than reinterpreting) and word embeddings on hypernymy detection and lexical entailment (see e.g. Stephen Roller's thesis for references).

3. There has been a lot of work on the Weeds BLESS dataset that is not mentioned, or compared against, including unsupervised approaches (e.g. Levy's work, Santus's work, Kiela's work, Roller's work, etc.), that perform better than the numbers in Table 1. There are many other datasets that measure lexical entailment, none of which are evaluated on (apart from the original BLESS set, which is mentioned in passing). It would make sense to show that the method works on more than one dataset, and to do a thorough comparison against other work; especially given that:

4. The tone of the work appears to imply that word2vec was wrong and needs to be reinterpreted: the work leads to ""unprecedented results"" (not true), claims to be a completely novel method for inducing word representations (together with LSA, BOW and Word2Vec, third paragraph; not true), and suggests it has found ""the best way to extract information about the semantics of a word from this model"" (7th paragraph; not true). This, together with the ""reinterpretation of word2vec"" and the proposed ""new distributional semantic models"" almost makes it hard for me to take the work seriously.","The reviewer expresses significant concerns about the paper, ultimately recommending rejection. While they acknowledge the paper's aim and basis on previous work, they find the writing confusing, the literature review lacking, the evaluation weak, and the tone overly strong. The reviewer cites specific examples like the unclear explanation of Henderson and Popa's model, the omission of relevant related work, and the strong claims made about the novelty and performance of the proposed method. All these factors point towards a negative sentiment, although the language remains professional and provides constructive criticism.",-60,60
"The paper proposes variance regularizing adversarial learning (VRAL), a new method for training GANs.

The motivation is to ensure that the gradient for the generator does not vanish. The authors propose to use a discriminator whose output targets a mixture of two Gaussians (one component each for real and fake data).  The means and variances are fixed so that the discriminator does not overfit, which ensures that the generator learning is not hindered. 

The discriminator itself is trained through two additional meta-discriminators (!) Are the meta-discriminators really necessary? Have you tried matching moments or using other methods for comparing the distributions?

It would be useful to write down the actual loss function so that it's easier to compare with other GAN variants. In particular, I'm curious to understand the difference between VRAL and Fisher-GAN. The authors discuss this in the end of Section 3, but a more careful comparison is needed.

The experimental results are pretty limited and lack detailed quantitative evaluation, which makes it harder to compare the performance of the proposed variant to existing algorithms.

Overall, I think that the idea is interesting, but the paper needs more work and does not meet the ICLR acceptance bar.

FYI, another concurrent submission showed that gradient penalties stabilize training of GANs:
MANY PATHS TO EQUILIBRIUM: GANS DO NOT NEED TO DECREASE A DIVERGENCE AT EVERY STEP
https://openreview.net/pdf?id=ByQpn1ZA-","The reviewer finds the idea interesting but suggests the paper needs more work. The reviewer provides constructive criticism and suggestions for improvement, indicating a willingness to see the paper succeed with further development. The language is professional and respectful throughout.",20,80
"The main emphasis of this paper is how to add background knowledge so as to improve the performance of NLU (specifically QA and NLI) systems. They adopt the sensible perspective that background knowledge might most easily be added by providing it in text format. However, in this paper, the way it is added is simply by updating word representations based on this extra text. This seems too simple to really be the right way to add background knowledge. 

In practice, the biggest win of this paper turns out to be that you can get quite a lot of value by sharing contextualized word representations between all words with the same lemma (done by linguistic preprocessing; the paper never says exactly how, not even if you read the supplementary material). This seems a useful observation which it would be easy to apply everywhere and which shows fairly large utility from a bit of linguistically sensitive matching!  As the paper notes, this type of sharing is the main delta in this paper from simply using a standard deep LSTM (which the paper claims to not work on these data sets, though I'm not quite sure couldn't be made to work with more tuning).

pp. 6-7: The main thing of note seems to be that sharing of representations between words with the same lemma (which the tables refer to as ""reading"" is worth a lot (3.5-6.0%), in every case rather more than use of background knowledge (typically 0.3-1.5%). A note on the QA results: The QA results are certainly good enough to be in the range of ""good systems"", but none of the results really push the SOTA. The best SQuAD (devset) results are shown as several percent below the SOTA. In the table the TriviaQA results are shown as beating the SOTA, and that's fair wrt published work at the time of submission, but other submissions show that all of these results are below what you get by running the DrQA (Chen et al. 2017) system off-the-shelf on TriviaQA, so the real picture is perhaps similar to SQuAD, especially since DrQA is itself now considerably below the SOTA on SQUAD. Similar remarks perhaps apply to the NLI results.

p.7 In the additional NLI results, it is interesting and valuable to note that the lemmatization and knowledge help much more when amounts of data (and the covarying dimensionality of the word vectors) is much smaller, but the fact that the ideas of this paper have quite little (or even negative) effects when run on the full data with full word vectors on top of the ESIM model again draws into question whether enough value is being achieved from the world knowledge.

Biggest question:
 - Are word embeddings powerful enough as a form of memory to store the kind of relational facts that you are accessing as background knowledge?

Minor notes:
 - The paper was very well written/edited. The only real copyediting I noticed was in the conclusion: and be used ➔ and can be used; that rely on ➔ that relies on.
 - Should reference to (Manning et al. 1999) better be to (Manning et al. 2008) since the context here appears to be IR systems?
 - On p.3 above sec 3.1: What is u? Was that meant to be z?
 - On p.8, I'm a bit suspicious of the ""Is additional knowledge used?"" experiment which trains with knowledge and then tests without knowledge. It's not surprising that this mismatch might hurt performance, even if the knowledge provided no incremental value over what could be gained from standard word vectors alone.
 - In the supplementary material the paper notes that the numbers are from the best result from 3 runs. This seems to me a little less good experimental practice than reporting an average of k runs, preferably for k a bit bigger than 3.


","The reviewer acknowledges the paper's valuable observations and good writing but expresses skepticism about the core idea and findings. They find the method of adding background knowledge ""too simple"" and question the reliance on word embeddings for storing relational facts. The reviewer highlights that the biggest gain comes from sharing representations between words with the same lemma, not from the background knowledge itself. They also point out that the results, while decent, don't surpass existing systems and question the validity of some experiments. Overall, the review leans towards the critical side, suggesting potential but not fully convincing results.",20,70
"Paper Summary: The paper proposes an approach to perform object classification and changing the viewpoint simultaneously. The idea is that the viewpoint changes until the object is recognized. The results have been reported on ModelNet40.

Paper Strength: The idea of combining active vision with object classification is interesting.

Paper Weaknesses:
I have the following concerns about this paper: (1) The paper performs the experiments on ModelNet40, which is a toy dataset for this task. The background is white and there is only a single object in each image. (2) The simple CNN baselines in MVCNN (Su et al., 2015) achieve higher performance than the proposed model, which is more complicated. (3) The paper seems unfinished. It mentions THOR and Active Vision, but there is no quantitative or qualitative results on them. (4) Some of the implementation details are unclear.

comments:

- It is unfair to use (Ammirato et al., 2017) as the citation for active vision. Active vision has been around for decades.

- It is not clear how the hierarchical soft-max layers have been implemented. There cannot be two consecutive soft-max layers. Also, for example, we cannot select an action from A, and then select an action from C since the operation is not differentiable. This should be clarified in the rebuttal.

- In Table 3, why is there a difference between the performance with and without LSTM in the first column? The LSTM does not see any history at the first step so the performance should be the same in both cases.

- According to Table 1 of MVCNN (Su et al., 2015), a simple CNN with one view as input achieves 83% accuracy (w/o fine-tuning), which is higher than the performance of the proposed method.

- It is better not to call the approach navigation. It is just changing the azimuth of the camera view.","The reviewer highlights several significant weaknesses, pointing out methodological issues, lack of strong results, and unclear aspects in the paper. While they find the core idea interesting, the numerous concerns suggest a negative sentiment. However, the language remains professional and avoids harsh or disrespectful phrasing, indicating a neutral politeness level.",-40,0
"The paper describes a sequence to sequence auto-encoder model which is used to learn sequence representations. The authors show that for their application, better performance is obtained when the network is only trained to reconstruct a subset of the data measurements. The paper also presents some visualizations the similarity structure of the learned representations and proposes a window-based method for processing the data.

According to the paper, the experiments are done using a data set which is obtained from measurements of an industrial production process. Figure 2 indicates that reconstructing fewer dimensions of this dataset leads to lower MSE scores. I don’t see how this is showing anything besides the obvious fact that reconstructing fewer dimensions is an easier task than reconstructing all of them.  The only conclusions I can draw from the visual analysis is that the context vectors are more similar to each other when they are obtained from time steps in the data stream which are close to each other. Since the paper doesn’t describe much about the privately owned data at all, there is no possibility to replicate the work. The paper doesn’t frame the work in prior research at all and the six papers it cites are only referred to in the context of describing the architecture.

I found it very hard to distil what the main contribution of this work was according to the paper. There were also not many details about the precise architecture used. It is implied that GRU networks and were used but the text doesn’t actually state this explicitly. By saying so little about the data that was used, it was also not clear what the temporal correlations of the context vectors are supposed to tell us. 

The paper describes how existing methods are applied to a specific data set. The benefit of only reconstructing a subset of the input dimensions seems very data specific to me and I find it hard to consider this a novel idea by itself. Presenting sequential data in a windowed format is a standard procedure and not a new idea either. All in all I don't think that the paper presents any new ideas or interesting results.

Pros:
* The visualizations look nice.

Cons:
* It is not clear what the main contribution is.
* Very little information about the data. 
* No clear experiments from which conclusions can be drawn.
* No new ideas.
* Not well rooted in prior work.
","The review is overwhelmingly negative. The reviewer finds little to no scientific value in the paper, questioning the methodology, novelty, and even the clarity of the presented work.  While the language avoids overtly aggressive terms, the criticisms are sharp, indicating a lack of politeness. The reviewer lists several 'cons' without a single redeeming 'pro' besides the visual presentation.",-75,-20
"The authors present an evolution of the idea of fast weights: training a double recurrent neural network, one ""slow"" trained as usual and one ""fast"" that gets updated in every time-step based on the slow network. The authors generalize this idea in a nice  way and present results on 1 experiment. On the positive side, the paper is clearly written and while the fast-weights are not new, the details of the presented method are original. On the negative side, the experimental results are presented on only 1 experiment with a data-set and task made up by the authors. The results are good but the improvements are not too large, and they are measured over weak baselines implemented by the authors. For a convincing result, one would require an evaluation on a number of tasks, including long-studied ones like language modeling, and comparison to stronger related models, such as the Neural Turing Machine or the Transformer (from ""Attention is All You Need""). Without comparison to stronger baselines and with results only on 1 task constructed by the authors, we have to recommend rejection.","The review starts with a neutral summary of the paper's contributions. While acknowledging the originality and clarity of the work, the reviewer then raises significant concerns about the limited experimental validation, lack of comparison with established baselines, and reliance on a single, potentially contrived, dataset. The recommendation for rejection further underscores the negative sentiment. However, the language remains professional and avoids harsh or disrespectful phrasing.",-40,60
"This paper describes an approach to generating time sequences by learning state-action values, where the state is the sequence generated so far, and the action is the choice of the next value.  Local and global reward functions are learned from existing data sequences and then the Q-function learned from a policy gradient.

Unfortunately, this description is a little vague, because the paper's details are quite difficult to understand.  Though the approach is interesting, and the experiments are promising, important explanation is missing or muddled.  Perhaps most confusing is the loss function in equation 7, which is quite inadequately explained.

This paper could be interesting, but substantial editing is needed before it is sufficient for publication.","The review starts with a cautiously positive note by acknowledging the interesting approach and promising results. However, it quickly points out significant weaknesses regarding clarity and explanation, particularly with the loss function. The reviewer suggests substantial editing is needed, indicating significant revisions are required. Overall, the sentiment leans towards the negative side due to the severity of the clarity issues. The language used is professional and avoids harsh or disrespectful language.",-30,60
"This paper addresses the question of unsupervised clustering with high classification performance. They propose a deep variational autoencoder architecture with categorical latent variables at the deepest layer and propose to train it with modifications of the standard variational approach with reparameterization gradients. The model is tested on a medical imagining dataset where the task is to distinguish healthy from pathological lymphocytes from blood samples. 

I am not an expert on this particular dataset, but to my eye the results look impressive. They show high sensitivity and high specificity. This paper may be an important contribution to the medical imaging community.

My primary concern with the paper is the lack of novelty and relatively little in the way of contributions to the ICLR community. The proposed model is a simple variant on the standard VAE models (see for example the Ladder VAE https://arxiv.org/abs/1602.02282 for deep models with multiple stochastic layers). This would be OK if a thorough evaluation on at least two other datasets showed similar improvements as the lymphocytes dataset. As it stands, it is difficulty for me to assess the value of this model.

Minor questions / concerns:

- The authors claim in the first paragraph of 3.2 that deterministic mappings lack expressiveness. Would be great to see the paper take this claim seriously and investigate it.
- In equation (13) it isn't clear whether you use q_phi to be the discrete mass or the concrete density. The distinction is discussed in https://arxiv.org/abs/1611.00712
- Would be nice to report the MCC in experimental results.","The reviewer finds the results impressive and potentially important for the medical imaging community (positive). However, they have concerns about the lack of novelty and limited evaluation beyond the medical dataset, which lessens their enthusiasm (negative). The reviewer uses phrases like ""My primary concern"" and ""it is difficult for me to assess"" which are direct but professional and polite.  Overall, the tone is balanced and professional.",40,70
"This paper proposes a framework called 'multi-instance learning', in which a time series is treated as a 'set' of observations, and label is assigned to the full set, rather than individual observations. In this framework, authors propose to do set-level prediction (using pooling) and observation level predictions (using various attention mechanisms). 
They test their approach in a medical setting, where the goal is to annotate vital signs time series by clinical events. Their cohort is 2014 adults time series (average length 4 time steps), and their time series has dimension of 21 and their clinical events have dimension of 26. Their baselines are other 'multi-instance learning' prior work and results are achieved through cross-validation. A few of the relevant hyper-parameters are tuned and some important hyper-parameters (i.e. number of hidden states in the LSTMs, or optimization method and learning rate) are not tuned. 

Originality - I find the paper to be very incremental in terms of originality of the method. 

Quality and Significance - Due to small size of the cohort and lack of additional dataset, it is difficult to reliably access quality of experiments. Given that results are reported via cross-validation and without a true held-out dataset, and given that a number of hyperparameters are not even tuned, it is difficult to be confident that the differences of all the methods reported are significant. 

Clarity - The writing has good clarity.

Major issues with the paper: 
- Lack of reliable experiment section. Dataset is too small (2000 total samples), and model training is not described with enough details in terms of hyper-parameters tuned. 
","The reviewer raises several major concerns, indicating a negative sentiment. They find the paper's originality lacking and criticize the experimental setup's quality and significance. While they acknowledge the writing's clarity, the overall tone suggests a negative view due to the significant methodological issues. The language used is formal and professional, without resorting to personal attacks or disrespectful phrasing.",-50,50
"The paper proposes a new method to compute embeddings of multirelational graphs. In particular, the paper proposes so-called E-Cells and R-Cells to answer queries of the form (h,r,?), (?,r,t), and(h,?,t). The proposed method (GEN), is evaluated on standard datasets for link prediction as well as datasets for node classification.

The paper tackles an interesting problem, as learning from graphs via embedding methods has become increasingly important. The experimental results of the proposed model, especially for the node classification tasks, look promising. Unfortunately, the paper makes a number of claims which are not justified or seem to result from misconceptions about related methods. For instance, the abstract labels prior work as ""ad hoc solutions"" and claims to propose a principled approach. However, I do not see how the proposed method is a more principled than previously proposed methods. For instance, methods such as RESCAL, TransE, HolE or ComplEx can be motivated as compositional models that reflect the compositional structure of relational data. Furthermore, RESCAL-like models can be linked to prior research in cognitive science on relational memory [3]. HolE explicitly motivates its modeling through its relation to models for associative memory. 

Furthermore, due to their compositional nature, these model are all able to answer the queries considered in the paper (i.e, (h,r,?), (h,?,t), (?,r,t)) and are implicitly trained to do so. The HolE paper discusses this for instance when relating the model to associative memory. For RESCAL, [4] shows how even more complicated queries involving logical connectives and quantification can be answered. It is therefore not clear how to proposed method improves over these models.

With regard to the evaluation: It is nice that the authors provided an evaluation which compares to several SOTA methods. However, it is unclear under which setting these results where obtained. In particular, how were the hyperparameter for each model chosen and which parameters ranges were considered in the grid search. Appendix B.2 in the supplementary seems to specify the parameter setting for GEN, but it is unclear whether the same parameters where chosen for the competing models and whether they were trained with similar methods (e.g., dropout, learning rate decay etc.). The big difference in performance of HolE and ComplEx is also surprising, as they are essentially the same model (e.g. see [1,2]). It is therefore not clear to me which conclusions we can draw from the reported numbers.

Further comments:
- p.3: The statement ""This is the actual way we humans learn the meaning of concepts expressed by a statement"" requires justification
- p.4: The authors state that the model is trained unsupervised, but eq. 10 clearly uses supervised information in form of labels.
- p.4: In 3.1, E-cells are responsible to answer queries of the form (h,r,?) and (?, r, t), while Section 3.2 says E-Cells are used to answer (h, ?, t). I assume in the later case, the task is actually to answer (h,r,?)?
- p.2: Making a closed-world assumption is quite problematic in this context, especially when taking a principled approach. Many graphs such as Freebase are very incomplete and make an explicit open-world assumption. 
- The paper uses a unusual definition of one-shot/multi-shot learning, which makes it confusing to read at first. The authors might consider using different terms to improve readability.
- Paper would benefit if the model is presented earlier. GEN Cells are defined only in Section 3.2, but the model is discussed earlier. Reversing the order might improve presentation.

[1] K. Hayashi et al: ""On the Equivalence of Holographic and Complex Embeddings for Link Prediction"", 2017
[2] T.Trouillon et al: ""Complex and holographic embeddings of knowledge graphs: a comparison"", 2017
[3] G. Halford et al: ""Processing capacity defined by relational complexity: Implications for comparative, developmental, and cognitive psychology"", 1998.
[4] D. Krompaß et al: ""Querying factorized probabilistic triple databases"", 2014","The review is quite critical of the paper, pointing out several flaws in the claims and methodology. While it acknowledges the promising results, it raises serious concerns about the justification of the approach and the validity of the comparisons. The reviewer also points out factual errors and inconsistencies. The tone, while direct and critical, maintains a professional and academic decorum.",-30,60
"This paper explores the idea of using policy gradients to learn a stochastic policy on complex control problems.  The central idea is to frame learning in terms of a new kind of Q-value that attempts to smooth out Q-values by framing them in terms of expectations over Gaussian policies.

To be honest, I didn't really ""get"" this paper.
* As far I understand, all of the original work policy gradients involved stochastic policies.  Many are/were Gaussian.
* All Q-value estimators are designed to marginalize out the randomness in these stochastic policies.
* As far as I can tell, this is equivalent to a slightly different formulation, where the agent emits a deterministic action (\mu,\Sigma) and the environment samples an action from that distribution.  In other words, it seems that if we just draw the box a bit differently, the environment soaks up the nondeterminism, instead of needing to define a new type of Q-value.

Ultimately, I couldn't discern /why/ this was a significant advance for RL, or even a meaningful new perspective on classic ideas.

I thought the little 2-mode MOG was a nice example of the premise of the model.

While I may or may not have understood the core technical contribution, I think the experiments can be critiqued: they didn't really seem to work out.  Figures 2&3 are unconvincing - the differences do not appear to be statistically significant.  Also, I was disappointed to see that the authors only compared to DDPG; they could have at least compared to TRPO, which they mention.  They dismiss it by saying that it takes 10 times as long, but gets a better answer - to which I respond, ""Very well, run your algorithm 10x longer and see where you end up!""  I think we need to see a more compelling demonstration of why this is a useful idea before it's ready to be published.

The idea of penalizing a policy based on KL-divergence from a reference policy was explored at length by Bert Kappen's work on KL-MDPs.  Perhaps you should cite that?
","The reviewer clearly states that they ""didn't really 'get' this paper"", which indicates a lack of understanding and therefore makes it hard to be positive. They find the experiments unconvincing and call for more compelling demonstrations of the proposed idea. While the reviewer does offer some constructive criticism and suggestions, the overall tone is rather skeptical and leans towards the negative. However, the language used is professional and avoids harsh or disrespectful vocabulary, staying within the bounds of academic politeness.",-30,60
"The paper extends SGNS as follows. In SGNS, each word x is associated with vectors a_x and r_x. Given a set of context words C, the model calculates the probability that the target word is x by a dot product between a_x and the average of {r_c: c in C}.  The paper generalizes this computation to an arbitrary network: now each word x is associated with some network N_x whose input is a set of context words C and the output is the aforementioned probability. This is essentially an architectural change: from a bag-of-words model to a (3-layer) feedforward model. 

Another contribution of the paper is a new form of regularization by tying a subset of layers between different N_x. In particular, the paper considers incorporating POS tags by tying within each POS group. For instance, the parameters of the first layer are shared across all noun words. (This assumes that POS tags are given.)

While this is a natural extension to word2vec, the reviewer has some reservations about the execution of this work. Word embeddings are useful in large part because they can be used to initialize the parameters of a network. None of the chosen experiments shows this. Improvement in the log likelihood over SGNS is somewhat obvious because there are more parameters. The similarity between ""words"" now requires a selection of context vectors (7) which is awkward/arbitrary. The use of POS tags is not very compelling (though harmless). It's not necessary: contrary to the claim in the paper, word embeddings captures syntactic information if the context width is small and/or context information is provided. A more sensible experiment would be to actually plug in the entire pretrained word nets into an external model and see how much they help. 

EDIT: It's usually the case that even if the number of parameters is the same, extra nonlinearity results in better data fitting (e.g., Berg-Kirkpatrick et al, 2010), it's still not unexpected. 

All of this is closely addressed in the following prior work: 

Learning to Embed Words in Context for Syntactic Tasks (Tu et al., 2017)

Quality: Natural but questionable extension, see above. 

Clarity: Clear. 

Originality: Acceptable, but a very similar idea of embedding contexts is presented in Tu et al. (2017) which is not cited. 

Significance: Minor/moderate, see above. 

","The reviewer acknowledges the paper as a ""natural extension"" but expresses reservations about its execution and significance. They find the improvements somewhat obvious and the experiments lacking, suggesting more impactful ways to validate the approach. The reviewer also points out closely related prior work that diminishes the originality aspect. The tone, while critical, maintains a professional and analytical approach without resorting to personal attacks or disrespectful language.",-20,60
"The paper is well written, well-motivated and the idea is very interesting for the computer vision and robotic communities. The technical contribution is original. The vision-based agent localization approach is novel compared to the methods of the literature. However, the experimental validation of the proposed approach could be more convincing (e.g. by testing on real data, with different training and testing splitting configurations).  

Major concern: 
1) The authors depict in section 2 “there is no existing end-to-end neural network for visual SLAM to our best knowledge” but they should discuss the positioning with respect to the paper of M Garon and JF Lalonde, “Deep 6-DOF Tracking”, ISMAR 2017 which propose a fully neural network based camera tracking method.

Minor concerns:
2) Table 3: the comparison is not rigorous in the sense that the proposed method estimates a 2D pose (3-DOF) while ORB-SLAM and EKF-SLAM are methods designed for 3D pose estimation (6-DOF). Is it possible to generalize your method to this case (6-DOF) for a more consistent comparison? At least, the fact that your method is more restrictive should be discussed in the paper. 

3) In the same vein than point 2), ORB-SLAM and EKF-SLAM are methods based on regression while the proposed method is restricted to the classification pose estimation. Is it possible to test your method with a regression task? 

4) It would be interesting to test the proposed method on real data to measure its robustness in terms of noise sensor and in terms of motion blur.

5) It would also be interesting to test the proposed method on datasets usually used in the SLAM community (e.g. using the sequences of the odometry benchmark of KITTI dataset).

6) In the SLAM context, the running time aspect on the test phase is crucial. Hence, the authors should compare the running time of their method with algorithms of literature (e.g. ORB-SLAM). 

","The reviewer starts with positive affirmations about the paper's quality, writing, and novelty ('well written', 'well-motivated', 'very interesting', 'original', 'novel'). This suggests a positive sentiment. However, the review also raises several concerns regarding the experimental validation and comparison with other methods, indicating that the reviewer sees areas for improvement. While the reviewer points out limitations and suggests further experiments, the language remains constructive and polite throughout, using phrases like 'could be more convincing', 'it would be interesting', 'the authors should consider'. Therefore, the politeness score leans towards positive. Overall, the sentiment is positive due to the initial praise, but the numerous suggestions for improvement moderate it.",60,70
"The paper proposes an approach that offers speedup on common convolutional neural networks. It presents the approach well and shows results comparing with other popular frameworks used in the field.

Originality
- The automation of parallelism across the different dimensions in each of the layers appears somewhat new. Although parallelism across each of the individual dimensions has been explored (batch parallel is most common and best supported, height and width is discussed at least in the DistBelief paper), automatically exploring this to find the most efficient approach is new. The splitting across channels seems not to have been covered in a paper before.

Significance
- Paper shows a significant speedup over existing approaches on a single machine (16 GPUs). It is unclear how well this would translate across machines or to more devices, and also on newer devices - the experiments were all done on 16 K80s (3 generations old GPUs). While the approach is interesting, its impact also depends on the speedup on the common hardware used today.

Pros:
- Providing better parallelism opportunities for convolutional neural networks
- Simple approach to finding optimal global configurations that seems to work well
- Positive results with significant speedups across 3 different networks

Cons:
- Unclear if speedups hold on newer devices
- Useful to see how this scales across more than 1 machine
- Claim on overlapping computation with data transfer seems incorrect. I am pretty sure TensorFlow and possibly PyTorch supports this.

Questions:
- How long does finding the optimal global configuration take for each model?
","The review is overall positive. The reviewer acknowledges the novelty and significance of the work, highlighting the speedup achieved. While they point out limitations and ask for clarifications, these are presented constructively and aim to improve the paper. The language is formal and polite throughout.",60,80
"
This paper introduces a different form of Memory cell for RNN which has more capabilities of long-term memorizing. Furthermore, it presents and efficient architecture for sequence-to-sequence mapping.

While the claim of the paper sounds very ambitious and good, the paper has several flaws. First of all, the mathematical analysis is a bit problematic. Of course, it is known that Simple Recurrent Networks (SRN) have a vanishing gradient problem. However, the way you proof it is not correct, as you ignore the application of f() for calculating the output (which is routed to the input) and you use an upper bound to show a general behaviour.
The analysis of the Memory capabilities of LSTM is a bit simplified, however, it is okay. Note, that various experiments by Schmidhuber's group, as well as Otte & al have shown that LSTM can generalize and memorize to sequences of more than a million time steps, if the learning rates is small enough.

The extended memory which the authors call SLSTM-I has similar memory capabilities as LSTM. The other one (SLSTM-II) looses the capability of forgetting as it seems. An analysis would be crucial in this paper to show the benefits mathematically. 

The authors should have a look at ""Evolving memory cell structures for sequence learning"" by Justin Bayer, Daan Wierstra, Julian Togelius and J¨urgen Schmidhuber, published in 2009. Note that the SLSTM belongs to the family of networks which could be generated by that paper as well.

Also ""Neural Architecture Search with Reinforcement Learning"" by Barret Zoph and Quoc V. Le would be interesting.

In your experiments it would be fair to compare to Cheng et al. 2016

I suggest the authors being more modest with the name of the memory cell as well as with the abstract (especially in the POS experiment, SLSTM is not superior)","The review starts with a positive note, acknowledging the ambition and potential of the paper's idea. However, it quickly delves into a series of criticisms regarding the mathematical analysis, comparison with existing models, and even the naming choice. The reviewer suggests other publications to consider and points out flaws in the experiments. While the language is quite direct, it avoids harsh or disrespectful phrasing, maintaining a professional tone. Therefore, the sentiment leans towards the negative side due to the significant concerns raised, but the politeness remains relatively neutral.",-30,40
"This paper presents an interesting extension to Snell et al.'s prototypical networks, by introducing uncertainty through a parameterised estimation of covariance along side the image embeddings (means). Uncertainty may be particularly important in the few-shot learning case this paper examines, when it is helpful to extract more information from limited number of input samples.

However, several important concepts in the paper are not well explained or motivated. For example, it is a bit misleading to use the word ""covariance"" throughout the paper, when the best model only employs a scalar estimate of the variance. A related, and potentially technical problem is in computing the prototype's mean and variance (section 3.3). Eq. 5 and 6 are not well motivated, and the claim of ""optimal"" under eq.6 is not explained. More importantly, eq. 5 and 6 do not use any covariance information (off-diagonal elements of S) --- as a result, the model is likely to ignore the covariance structure even when using full covariance estimate. The distance function (eq. 4) is d Mahalanobis distance, instead of ""linear Euclidean distance"". While the paper emphasises the importance of the form of loss function, the loss function used in the model is given without explanation (and using cross-entropy over distances looks hacky).

In addition, the experiments are too limited to support the claimed benefits from encoding uncertainty. Since the accuracies on omniglot data from recent models are already close to perfect, it is unclear whether the marginally improved number reported here is significant. In addition, more analysis may better support existing claims. For example, showing subsampled images indeed had higher uncertainty, rather than only the histogram for all data points.

Pros:
-Interesting problem and interesting direction.
-Considers a number of possible alternative models
-Intuitive illustration in Fig. 1

Cons:
-Misleading use of ""covariance""
-The several important concepts including prototype mean/variance, distance, and loss are not well motivated or explained
-Evaluation is too limited","The review starts with positive sentiment, acknowledging the interesting aspects of the paper. However, it quickly transitions into a  critical analysis, pointing out significant flaws in the methodology and explanation. The reviewer also expresses concerns about the limited experimental validation. While the language is direct and critical, it maintains a professional and academic tone without resorting to personal attacks or disrespectful language.",20,60
"The authors study the convergence of a procedure for learning
an autoencoder with a ReLu non-linearity.  The procedure is akin
to stochastic gradient descent, with some parameters updated at
each iteration in a manner that performs optimization with respect
to the population risk.

The autoencoders that they study tie the weights of the decoder to
the weights of the encoder, which is a common practice.  There
are no bias terms in the decoder, however.  I do not see where they
motivate this restriction, and it seems to limit the usefulness of
the bias terms in the encoder.

Their analysis is with respect to a mixture model.  This is described
in the abstract as a sparse dictionary model, which it is, I guess.
They assume that the gaussians are very well separated.  

The statement of Theorem says that it concerns Algorithm 1.  The
description of Algorithm 1 describes a procedure, with an
aside that describes a ""version used in the analysis"".

They write in the text that the rows of W^t are projected onto
a ball of radius c in each update, but this is not included
in the description of Algorithm 1.  The statement of Theorem 1
includes the condition that all rows of W^t are always equal to
c, but this may not be consistent with the updates given
in Algorithm 1.  My best guess is that they intend of
the rows of W^t to be normalized after each update (which is
different than projecting onto the ball of radius c).  This
aspect of their procedure seems restrict its applicability.

Successful initialization looks like a very strong condition to
me, something that will occur exponentially rarely, as a function
of d. (See Fact 10 of ""Agnostically learning halfspaces"", by Kalai, et al.)
For each row of W^*, the probability that any one row of W^o will be
close enough is exponentially small, so exponentially many rows
are needed for the probability that any row is close enough to
be, say, 1/2.  I don't see anything in the conditions of Theorem 1
that says that n is large relative to d, so it seems like its
claim includes the case where k and n are constants, like 5.
But, in this case, it seems like the claim of the probability
of successful initialization cannot be correct when d is large.

It looks like, after ""successful initialization"", especially
given the strong separation condition, the model as already
""got it"".  In particular, the effect of the ReLUs seems to
be limited in this regime.

I have some other concerns about correctness, but I do not think
that the paper can be accepted even if they are unfounded.

The exposition is uneven.  They tell us that W^T is the transpose
of W, but do not indicate that 1_{a^t (x') > 0} is a componentwise
indicator function, and that x' 1_{a^t (x') > 0} is its
componentwise product with x' (if this is correct).


","The review is highly critical of the paper, pointing out several flaws in the methodology, assumptions, and presentation. The reviewer finds the paper's core ideas questionable and believes the paper is not publishable in its current state. The language used, while direct and critical, maintains a professional and academic tone.",-75,50
"This paper investigates learning representations for the problem of nearest neighbor (NN) search by exploring various deep learning architectural choices. The crux of the paper is the connection between NN and the angles between the closest neighbors -- the higher this angle, more data points need to be explored for finding the nearest one, and thus more computational expense. Thus, the paper proposes to learn a network that tries to reduce the angles between the inputs and the corresponding class vectors in a supervised framework using softmax cross-entropy loss. Three architectural choices are investigated, (i) controlling the norm of output layers of the CNN (using batch norm essentially), (ii) removing relu so that the outputs are well-distributed in both positive and negative orthants, and (iii) normalizing the class vectors. Experiments are given on multiMNIST and Sports 1M and show improvements. 

Pros:
1) The paper explores different architectural choices for the deep network to some depth and show extensive results.
2) The results do demonstrate clearly the advantage of the various choices and is useful
3) The theoretical connections between data angles and query times are quite interesting,

Cons:
1) Unclear Problem Statement. 
I find the problem statement a bit vague. Standard NN search finds a data point in the database closest to a query under some distance metric. While, the current paper uses the cosine similarity as the distance, the deep framework is trained on class vectors using cross-entropy loss. I do not think class labels are usually assumed to be given in the standard definition of NN, and it is not clear to me how the proposed setup can accommodate NN without class labels.  Thus as such, I see this paper is perhaps proposing a classification problem and not an NN problem per se. 

2) Lacks Focus
The paper lacks a good organization in my opinion. Things that are perhaps technically important are moved to the Appendix. For example, I find the theoretical part of the paper (e.g., Theorem 1) quite elegant and perhaps the main innovation in this paper. However, that is moved completely to the Appendix. So it cannot be really considered a contribution. It is also not clear if those theoretical results are novel. 

2) Disconnect/Unclear Assumptions
There seems to be some disconnect between LSH and deep learning architectures explored in Sections 2 and 3 respectively. Are the assumptions used in the theoretical results for LSH also assumed in the deep networks? For example, as far as I know, the standard LSH works assumes the projection hyperplanes are randomly chosen and the theoretical results are based on such assumptions. It is not clear how a softmax output of a CNN, which is trained in a supervised way, follow such assumptions. It would be important if the paper could clarify such assumptions to make sure the sections are congruent. 

3) No Related Work
There have been several efforts for adapting deep frameworks into KNN. The paper ignores all such works. Thus, it is not clear how significant is the proposed contribution. There are also not comparisons what-so-ever to competitive prior works.

4) Novelty
The main contribution of this paper is basically a set of experiments looking into architectural choices. However, the results of this study do not provide any surprises. It appears that batch normalization is essential for good performances, while using RELU is not so when one wants to use all directions for effective data encoding. Thus, as such, the novelty or the contributions of this paper are minor.

Overall, while I find there are some interesting theoretical bits in this paper, it lacks focus, the experiments do not offer any surprises, and there are no comparisons with prior literature. Thus, I do not think this paper is ready to be accepted in its present form.
","The reviewer raises several significant concerns about the paper, including a vague problem statement, lack of focus, disconnect between theory and experiments, absence of related work, and limited novelty. While acknowledging some interesting theoretical aspects, the reviewer finds the overall contribution minor and recommends rejecting the paper in its current form. The tone is critical but professional, without resorting to personal attacks or disrespectful language.",-50,50
"I have to say that I do not have all the background of this paper, and the paper is not written very clearly. I think the major contribution of the paper is represented in a very vague way.","The reviewer expresses uncertainty and a lack of clarity understanding (""I do not have all the background...""). They also find the contribution unclear (""major contribution... very vague""). This suggests a negative, but not overly harsh, sentiment. The language is quite indirect in its criticism, suggesting politeness.",-60,60
"[Main comments]

* The authors made a really odd choice of notation, which made the equations hard to follow.
Apparently, that notation is used in differential geometry, but I have never seen it used in
an ML paper. If you talk about outer product structure, show some outer products!

* The function f that the authors differentiate is not even defined in the main manuscript!

* The low-rank structure they describe only holds for a single sample at a time.
I don't see how this would be ""understanding low rank structure of deep networks""
as the title claims... What is described is basically an implementation trick.

* Introducing cubic regularization seems interesting. However, either some
extensive empirical evidence or some some theoretical evidence that this is useful are needed.
The present paper has neither (the empirical evidence shown is very limited).

[Other minor comments]

* Strictly speaking Adagrad has not been designed for Deep Learning.
It is an online algorithm that became popular in the DL community later on.

* ""Second derivatives should suffice for now, but of course if a use arose for
third derivatives, calculating them would be a real option""

That sentence seems useless.

* Missing citation:

Gradient Descent Efficiently Finds the Cubic-Regularized Non-Convex Newton Step. 
Yair Carmon, John Duchi.
","The review is highly critical of the paper, pointing out major flaws in notation, missing definitions, and lack of both theoretical and empirical support for the main claims. The reviewer also questions the paper's relevance to its claimed topic. While the language is quite direct, it avoids personal attacks and maintains a professional tone.",-70,40
"This paper provides a clean way of learning embeddings for structured features that can be discrete -- indicating presence / absence of a certain quality. Further, these features can be structured i.e. a set of them are of the same 'type'. Unlike, word2vec there is no hard constraint that similar objects must have similar representations and so, the learnt embeddings reflect the likelihood of the observed features. Therefore, this can be used as a multi-label classifier by using two feature types -- the input and the set of categories. This proposed scheme is evaluated on two datasets -- movies and education in a retrieval setting. 

I would like to see an evaluation of these features in a classification setting to further demonstrate the utility of these embeddings as compared to directly embedding the discrete features and then performing a K-way classification. For example, I am aware of -- http://manikvarma.org/downloads/XC/XMLRepository.html contains some interesting datasets which have a large number of discrete features and classes. ","The review starts with positive statements, highlighting the novelty and cleanliness of the approach. It then provides constructive feedback by suggesting an additional evaluation setting. The language used is formal and suggestive, not demanding, which points to politeness.",60,70
"This paper introduces a parameter server architecture to improve distributed training of CNNs in the presence of stragglers. Specifically, the paper proposes partial pulling where a worker only waits for first b blocks rather than all the blocks of the parameters. This technique is combined with existing methods such as partial pushing (Pan et. al. 2017) for a partial synchronous SGD method. The method is evaluated with Resnet -50 using synthetic delays.

Comments for the author:

The paper is well-written and easy to follow. The problem of synchronization costs being addressed is important but it is unclear how much of this is arising due to large blocks.

1) The partial pushing method (Pan et. al. 2017, section 3.1) shows a clear evidence for the problem using a real workload with a large number of workers. Unfortunately, in your Figure 2, this is not as obvious and not real since it is using simulated delays. More specifically, it is not clear how the workers behave in a real environment and whether you get a clear benefit from using a partial number of blocks as opposed to sending all of them. 

2) Did you modify your code to support block-wise sending of gradients (some description of how the framework was modified will be helpful)? The idea is to send partial parameter blocks and when 'b' blocks are received, compute the gradients. I feel that, with such a design, you may actually end up hurting the performance by sending a large number of small packets in the no failure case. For real, large data centers, this may cause a packet storm and subsequent throughput collapse (e.g. the incast problem). You need to show the evidence that you do not hurt the failure-free case for a large number of workers.

3) The evaluation is on fairly small workloads (CIFAR-10). Again, evaluating over Imagenet and demonstrating a clear speedup over existing sync methods will be helpful. Furthermore, a clear description of your “pull” configuration (such as in Figure 1) i.e. how many actual bytes or blocks are sent and what is the threshold will be helpful (beyond a vague 90%).

4) Another concern with partial synchronization methods that I have is that how do you pick these configurations (pull 0.75 etc). These appear to be dataset specific and finding the optimal configuration here requires significant experimentation that takes significantly more time than just running the baseline.

Overall, I feel there is not enough evidence for the problem specifically generating large blocks of gradients and this needs to be clearly shown. To propose a solution for stragglers, evaluation should be done in a datacenter environment with the presence of stragglers (and not small workloads with synthetic delays). Furthermore, the proposed technique despite the simplicity appears as a rather incremental contribution.","The reviewer acknowledges the importance of the problem tackled in the paper and finds the paper well-written and easy to follow. However, they raise several concerns regarding the evaluation methodology and the significance of the contribution. The reviewer points out the lack of real-world experiments and the use of synthetic delays, limited dataset size, and the need for more evidence to support the claims. The overall tone suggests that the reviewer leans towards recommending revisions rather than outright acceptance.",20,70
"This paper considers a dichitomy between ML and RL based methods for sequence generation. It is argued that the ML approach has some ""discrepancy"" between the optimization objective and the learning objective, and the RL approach suffers from bad sample complexity. An alpha-divergence formulation is considered to combine both methods.

Unfortunately, I do not understand main points made in this paper and am thus not able to give an accurate evaluation of the technical content of this paper. I therefore have no option but to vote for reject of this paper, based on my educated guess. 

Below are the points that I'm particularly confused about:

1. For the ML formulation, the paper made several particularly confusing remarks. Some of them are blatantly wrong to me. For example, 

1.1 The q(.|.) distribution in Eq. (1) *cannot* really be the true distribution, because the true distribution is unknown and therefore cannot be used to construct estimators. From the context, I guess the authors mean ""empirical training distribution""?

1.2 I understand that the ML objective is different from what the users really care about (e.g., blue score), but this does not seem a ""discrepancy"" to me. The ML estimator simply finds a parameter that is the most consistent to the observed sequences; and if it fails to perform well in some other evaluation criterion such as blue score, it simply means the model is inadequate to describe the data given, or the model class is so large that the give number of samples is insufficient, and as a result one should change his/her modeling to make it more apt to describe the data at hand. In summary, I'm not convinced that the fact that ML optimizes a different objective than the blue score is a problem with the ML estimator.

In addition, I don't see at all why this discrepancy is a discrepancy between training and testing data. As long as both of them are identically distributed, then no discrepancy exists.

1.3 In point (ii) under the maximum likelihood section, I don't understand it at all and I think both sentences are wrong. First, the model is *not* trained on the true distribution which is unknown. The model is trained on an empirical distribution whose points are sampled from the true distribution. I also don't understand why it is evaluated using p_theta; if I understand correctly, the model is evaluated on a held-out test data, which is also generated from the underlying true distribution.

2. For the RL approach, I think it is very unclear as a formulation of an estimator. For example, in Eq. (2), what is r and what is y*? It is mentioned that r is a ""reward"" function, but I don't know what it means and the authors should perhaps explain further. I just don't see how one obtains an estimated parameter theta from the formulation in Eq. (2), using training examples.","The reviewer expresses strong confusion and disagreement with the paper's core arguments, ultimately leading to a reject recommendation. They find the paper's explanations unclear and some claims outright incorrect. This indicates a highly negative sentiment. However, the reviewer articulates their concerns in a professional, analytical manner, focusing on specific points of confusion and avoiding personal attacks. This suggests a neutral-to-polite tone despite the negative feedback.",-80,50
"Summary:
 The paper presents an unsupervised method for detecting adversarial examples of neural networks. The method includes two independent components: an ‘input defender’ which tried to inspect the input, and a ‘latent defender’ trying to inspect a hidden representation. Both are based on the claim that adversarial examples lie outside a certain sub-space occupied by the natural image examples, and modeling this sub-space hence enables their detection. The input defender is based on sparse coding, and the latent defender on modeling the latent activity as a mixture of Gaussians. Experiments are presented on MInst, Cifar10, and ImageNet.
 
-	Introduction: The motivation for detecting adversarial examples is not stated clearly enough. How can such examples be used by a malicious agent to cause damage to a system? Sketching some such scenarios would help the reader understand why the issue is practically important. I was not convinced it is. 
Page 4: 
-	Step 3 of the algorithm is not clear:
o	How exactly does HDDA model the data (formally) and how does it estimate the parameters? In the current version, the paper does not explain the HDDA formalism and learning algorithm, which is a main building block in the proposed system (as it provides the density score used for adversarial examples detection). Hence the paper cannot be read as a standalone document. I went on to read the relevant HDDA paper, but it is also not clear which of the model variants presented there is used in this paper.
o	What is the relation between the model learned at stage 2 (the centers c^i) and the model learnt by HDDA? Are they completely different models? Or are the C^I used when learning the HDDA model (and how)? 
If these are separate models, how are they used in conjunction to give a final density score? If I understand correctly, only the HDDA model is used to get the final score, and the C^i are only used to make the \phy(x) representation more class-seperable. Is that right?
-	Figure 4, b and c: it is not clear what the (x,y,z) measurements plotted in these 3D drawings are (what are the axis).
Page 5:
-	Section 2: the risk analysis is done in a standard Bayesian way and leads to a ratio of PDFs in equation 5. However, this form is not appropriate for the case presented at this paper, since the method presented only models one of these PDFs (Specifically p(x | W1)  - there is not generative model of p(x|W2)).  
-	The authors claim in the last sentence of the section that p(x|W2) is equivalent to 1-p(x|W1), but this is not true: these are two continuous densities, they do not sum to 1, and a model of p(x|W2) is not available (as far as I understand the method)
Page 6:
-	How is equation 7) optimized?
-	Which patchs are extracted from images, for training and at inference time? Are these patchs a dense coverage of the image? Sparsely sampled? Densely sampled with overlaps?
-	Its not clear enough what exactly is the ‘PSNR’ value which is used for the adversarial example detection, and what exactly is ‘profile the PSNR of legitimate samples within each class’. A formal definition of PSNR and’profiling’ is missing (does profiling simply mean finding a threshold for filtering?)
Page 7:
-	Figure 7 is not very informative. Given the ROC curves in figure 8  and table 1 it is redundant. 

Page 8:
-	The results in general indicate that the method is much better than chance, but it is not clear if it is practical, because the false alarm rates for high detection are quite high. For example on ImageNet, 14.2% of the innocent images are mistakenly rejected as malicious to get 90% detection rate. I do not think this working point is useful for a real application
-	Given the high flares alarm rate, it is surprising that experiments with multiple checkpoints are not presented (specifically as this case of multiple checkpoints is discussed explicitly in previous sections of the paper).  Experiments with multiple checkpoints are clear required to complete the picture regarding the empirical performance of this method
-	The experiments show that essentially, the latent defenders are stronger than the input defender in most cases. However, an ablation study of the latent defender is missing: Specifcially, it is not clear a) how much does stage 2 (model refinement with clusters)  contribute to the accuracy (how does the model do without it? And 3) how important is the HDDA and the specific variant used (which is not clear) important: is it important to model the Gaussians using a sub-space? Of which dimension?

Overall:
Pros:
-	 A nice idea with some novelty,  based on a non-trivial observation
-	The experimental results how the idea holds some promise
Cons
-	The method is not presented clearly enough: the main component modeling the network activity is not explained (the HDDA module used)
-	The results presented show that the method is probably not suitable for a practical application yet (high false alarm rate for good detection rate)
-	Experimental results are partial: results are not presented for multiple defenders, no ablation experiments


After revision:
Some of my comments were addressed, and some were not.
Specifically, results were presented for multiple defenders and some ablation experiments were highlihgted
Things not addressed:
 - The risk analysis is still not relevant. The authors removed a clearly flawed sentence, but the analysis still assumes that two densities (of 'good' and 'bad' examples) are modeled, while in the work presented only one of them is. Hence this analysis does not add anything to the paper-  it states a general case which does not fit the current scenario and its relation to the work is not clear. It would have been better to omit it and use the space to describe HDDA and the specific variant used in this work, as this is the main tool doing the distinction.

I believe the paper should be accepted.
","The review is mostly positive in its description of the paper's idea and potential. However, it also raises several significant concerns about the clarity of the methodology, the practicality of the results due to high false alarm rates, and the lack of certain experiments. The reviewer suggests the paper should be accepted after revision, indicating an overall positive sentiment, but not without reservations. The language used is formal, direct, and critical in an academic context, but not rude.",60,70
"The paper studies the problem of constructing small coreset for SVM.
A coreset is a small subset of (weighted) points such that the optimal solution for the coreset is also a good approximation for the original point set. The notion of coreset was originally formulated in computational geometry by Agarwal et al.
(see e.g., [A])
Recently it has been extended to several clustering problems, linear algebra, and machine learning problems. This paper follows the important sampling approach first proposed in [B], and generalized by Feldman and Langberg. The key in this approach is to compute the sensitivity of points and bound the total sensitivity for the considered problem (this is also true for the present paper). For SVM, the paper presents a bad instance where the total sensitivity can be as bad as 2^d. Nevertheless,
the paper presents interesting upper bounds that depending on the optimal value and variance of the point set. The paper argues that in many data sets, the total sensitivity may be small, yielding small coreset. This makes sense and may have significant practical implications.

However, I have the following reservation for the paper.
(1) I don't quite understand the CHICKEN and EGG section. Indeed, it is unclear to me 
how to estimate the optimal value. The whole paragraph is hand-waving. What is exactly merge-and-reduce? From the proof of theorem 9, it appears that the interior point algorithm is run on the entire dataset, with running time O(n^3d). Then there is no point to compute a coreset as the optimal solution is already known.

(2) The running time of the algorithm is not attractive (in both theory and practice).
In fact, the experimental result on the running time is a bit weak. It seems that the algorithm is pretty slow (last in Figure 1). 

(3) The theoretical novelty is limited. The paper follows from now-standard technique for constructing coreset.

Overal, I don't recommend acceptance.

minor points:
It makes sense to cite the following papers where original ideas on constructing coresets were proposed initially.

[A]Geometric Approximation via Coresets
Pankaj K. Agarwal Sariel Har-Peled Kasturi R. Varadarajan

[B]Universal epsilon-approximators for integrals, by Langberg and Schulman

---------------------------------------------------------

After reading the response and the revised text, I understand the chicken-and-egg issue.
I think the experimental section is still a bit weak (given that there are several very competitive SVM algorithms that the paper didn't compare with).
I raised my score to 5. 

","The reviewer acknowledges the practical potential of the paper's findings, stating ""This makes sense and may have significant practical implications."" However, they express several reservations, particularly regarding the clarity of the ""CHICKEN and EGG"" section, the algorithm's efficiency, and the limited theoretical novelty. The reviewer ultimately recommends against acceptance, indicating a negative sentiment. While the reviewer provides constructive criticism and specific recommendations, the overall tone is rather critical, particularly in pointing out perceived flaws. The language remains professional and avoids personal attacks, indicating a neutral politeness level.",-30,0
"The paper describes some interesting work but for a combination of reasons I think it's more like a workshop-track paper.
There is not much that's technically new in the paper-- at least not much that's really understandable.   There is some text about a variant of CTC, but it does not explain very clearly what was done or what the motivation was.
There are also quite a few misspellings.  
Since the system is presented without any comparisons to alternatives for any of the individual components, it doesn't really shed any light on the significance of the various modeling decisions that were made.  That limits the value.
If rejected from here, it could perhaps be submitted as an ICASSP or Interspeech paper.","The review starts with a backhanded compliment, saying the work is ""interesting"" but ultimately not suitable for the venue. The reviewer finds the technical contribution lacking clarity and novelty. Criticism about misspellings and lack of comparisons further points towards a negative sentiment. However, the language remains professional and avoids harsh or disrespectful phrasing, suggesting a more neutral politeness. The suggestion to submit to another venue slightly balances the negativity but doesn't erase the core criticism.",-50,20
"The paper puts forward Activation Norm Penalty (""ANR"", an L_2 type regularization on the activations), deriving it from the Information Bottleneck principle. As usual with Information Bottleneck style constructions, the loss takes on a variational form.

The experiments demonstrate small but consistent gains with ANR across a number of domains (Language modelling on small datasets, plus image classification) and baseline models.

A couple of things that could be improved:

- The abstract claims to ground the ANR in the variational dropout framework. When it is applied without dropout to image classification, shouldn't that be explained?

- Maybe dropping the determinant term also deserves some justification.

- Very recently, Activation Regularization by Merity (https://arxiv.org/abs/1708.01009) proposed a similar thing without theoretical justification. Maybe discuss it and the differences (if any) in the related work section?

- The Information Bottleneck section doesn't feel like an integral part of the paper.

My two cents: this work has both theoretical justification (a rare thing these days) and reasonable experimental results.

There are a number of typos and oversights:

- Abstract: ""variation dropout""
- Section 2:
  - x is never used
  - m in b = m + \sigma\epsilon is never defined (is it the x above?)
- In Section 3.2, equation 11 subscript of x_i is missing
- Section 6, Ungrammatical sentence: ""Even though L_2 ...""
","The review starts with a neutral summary of the paper's contributions. It then lists several constructive suggestions for improvement, indicating a positive attitude towards the paper's core ideas. The reviewer explicitly mentions appreciating the theoretical grounding and experimental validation. While the reviewer points out typos and minor issues, these are presented constructively and do not detract significantly from the overall positive sentiment. ",60,80
"The paper addresses the problem of multi-label learning for text corpora and proposes to tackle the problem using tensor factorization methods. Some analysis and experimental results for the proposed algorithm are presented.

QUALITY: I find the quality of the results in this paper rather low. The proposed probabilistic model is defined ambiguously. The authors then look at joint probability distributions of co-occurence of two and three words, which gives a matrix and a tensor, respectively. They propose to match these matrix and tensor to their sample estimates and refer to such procedure as the moment matching method, which it is not. They then apply a standard two step technique from the moment matching literature consisting of whitening and orthogonal tensor factorization. However, in their case this does not have much statistical meaning. Indeed, whitening of the covariance matrix is usually justified by the scaling unidentifiability of the problem. In their case, the mathematics works because of the orthogonal unidentifiability of the square root of a matrix. Furthermore, the proposed sample estimators do not actually estimate densities they are dealing with (see, e.g., Eq. (16) and (17)). Their theoretical analysis seems like a straightforward extension of the analysis by Anandkumar, et al. (2012, 2014), however, I find it difficult to assess this analysis due to numerous ambiguities in the problem formulation and method development. This justifies my statement in the beginning of the paragraph.

CLARITY: The paper is not well written and, therefore, is difficult to assess. Many important details are omitted, the formulation of the model is self contradicting, the standard concepts and notations are sometimes abused, some statements are wrong. I provide some examples in the detailed comment below.

ORIGINALITY AND SIGNIFICANCE: The idea to apply tensor factorization approaches to the multi-label learning is novel up to my knowledge and is a pro of the paper. However, I have problems to find other pros in this submission because the clarity is quite low and in the present form there is no novelty in the proposed procedure. Moreover, the authors claim to work with densities, but end up estimating other quantities, which are not guaranteed to have the desirable form. They also emphasize the fact that there is the simplex constraint on the estimated parameters, but this constraint is completely ignored by the algorithm and, in general, won't be satisfied in practice. If think the authors should do some more work before this paper can be published.



DETAILED COMMENTS: Since I am quite critical about the paper, I point out some examples of drawbacks or flaws of this paper:

  - The proposed model (Section 2) is not well defined. In particular, the description in Section 2 is not sufficient to understand the proposed model; the plate diagram in Figure 2 is not consistent with the text. It is not mentioned how at least some conditional distributions behave (e.g., tokens given labels or states). The diagram in Fig. 1 does not help since it isn't consistent with the text (e.g. the elements of labels or states are not conditionally / independent). The model is very close to latent Dirichlet allocation by Blei, et al. (2003), but differences are not discussed.

  - The standard terminology is often abused. For example, the proposed approach is referred to as the method of moments when it is not. In Section 2.1, the authors aim to match joint distributions (not the moments) to their empirical approximations (which are also wrong; see below). The usage of tokes and documents is interchanged without any explanations.

  - The use of the whitening approach is not justified in their setting working with joint distributions of couples and triples and it has no statistical meaning. No explanation is provided. I would definitely not call this whitening.

  - In Section 2.2, the notation is not defined and is different from what is usually used in the literature. For example, Eq. (15) does not make much sense as is. One could guess from the context that they are talking about the eigenvectors of an orthogonal tensor as defined in, e.g. Anandkumar, et al. (2014).

  - In Section 3, the authors emphasize the fact that their parameters are constrained to the probability simplex, but this constraint is not ensured in the proposed algorithm (Alg. 1).

  - Importantly, the estimators of the matrix M_2 and tensor M_3 do not make much sense to me. For example, for estimating M_2 it would be reasonable to average over all word pairs, i.e. something like [M_2]_{ij} = 1/L \sum_{w_k \not = w_l} P(w_k = v_i, w_l = v_j), where L is the number of pairs. This is different from the expression in Eq. (16), which is just a rescaled non-central second moment. Similar issue is true for the order-3 estimator.

  - The factorization procedure does not ensure non-negativity of the obtained parameters and, therefore, the rescaling is not guaranteed to belong to the probability simplex. I could not find any explanations of this issue.

  - I explain good plots in the experimental section, potentially, by the fact that the authors do algorithmically something different from what they aim to do, because the estimators do not estimate the desired entities (i.e. are not consistent). The procedure looks to me quite similar to the procedure for LDA, hence the reasonable results. However, the authors do not justify their proposed method.","The review is highly critical of the paper, pointing out serious flaws in the methodology, clarity, and originality. The reviewer finds the quality of the results low and the paper poorly written. While acknowledging the novelty of the idea, the reviewer's numerous concerns about the paper's execution result in a negative sentiment. Despite the critical tone, the language remains professional and avoids personal attacks, indicating a level of politeness.",-75,50
"In this paper a neural-network based method for multi-frame video prediction is proposed. It builds on the previous work of [Finn et al. 2016] that uses a neural network to predict transformation parameters of an affine image transformation for future frame prediction, an idea akin to the Spatial Transformer Network paper of [Jaderberg et al., 2015]. What is new compared to [Finn et al. 2016] is that the authors managed to train the network in combination with an adversarial loss, which allows for the generation of more realistic images. Time series modelling is performed via convolutional LSTMs. The authors evaluate their method based on a mechanical turk survey, where humans are asked to judge the realism of the generated images; additionally, they propose to measure prediction quality by the distance between the manually annotated positions of objects within ground truth and predicted frames.

My main concerns with this paper are novelty, reproducibility and evaluation.

* Novelty. The network design builds heavily on the work of [Finn et al., 2106]. A number of design decisions (such as instance normalization) seem to help yield better results, but are minor contributions. A major contribution is certainly the combination with an adversarial loss, which is a non-trivial task. However, the authors claim that their method is the first to combine multi-frame video prediction with an adversarial loss, which is not true. A recent work, presented at CVPR this year also does multi-frame prediction featuring an adversarial loss and explicitly models and captures the full dense optical flow (though in the latent space) that allows non-trivial motion extrapolation to future frames. This work is neither mentioned in the related work nor compared to. 
 
Lu et al. , Flexible Spatio-Temporal Networks for Video Prediction, CVPR 2017

This recent work builds on another highly relevant work, that is also not mentioned in the paper:

Patraucean et al. Spatio-temporal video autoencoder with differentiable memory, arxiv 2017

Since this is prior state-of-the-art and directly applicable to the problem, a comparison is a must. 

* Reproducibility and evaluation
The description of the network is quite superficial. Even if the authors released their code used for training (which is not mentioned), I think the authors should aim for a more self-contained exposition. I doubt that a PhD student would be able to reimplement the method and achieve comparable results given the paper at hand only. It is also not mentioned whether the other methods that the authors compare to are re-trained on their newly proposed training dataset. Hence, it remains unclear to what extend the achieved improvements are due to the proposed network design changes or the particular dataset they use for training. The authors also don't show any results on previous datasets, which would allow for a more objective comparison to existing state of the art. Another point of criticism is the way the Amazon Mechanical Turk evaluation was performed. Since only individual images were shown, the evaluation mainly measures the quality of the generated images. Since the authors combine their method with a GAN, it is not surprising that the generated images look more realistic. However, since the task is *video* prediction, it seems more natural to show small video snippets rather than individual images, which would also evaluate temporal consistency.

* Further comments:
The paper contains a number of broken sentences, typos and requires a considerable amount of polishing prior to publication.
","The review is critical of the paper due to concerns about novelty, reproducibility, and evaluation. The reviewer points out prior work that the authors overlooked and criticizes the evaluation methodology. While the reviewer acknowledges the use of adversarial loss as a significant contribution, the overall tone suggests that the paper needs substantial revisions before publication. The language used, while direct, maintains a professional and academic tone.",-50,50
"Science is about reproducible results and it is very commendable from scientists to hold their peers accountable for their work by verifying their results. It is also necessary to inspect claims that are made by researchers to avoid the community straying in the wrong direction. However, any critique needs to be done properly, by 1) attending to the actual claims that were made in the first place, by 2) reproducing the results in the same way as in the original work, 3) by avoiding introducing false claims based on a misunderstanding of terminology and 4) by extensively researching the literature before trying to affirm that a general method (here, Deep RL) cannot solve certain tasks.

This paper is a critique of deep reinforcement learning methods for learning to navigate in 3D environments, and seems to focus intensively on one specific paper (Mirowski et al, 2016, “Learning to Navigate in Complex Environments”) and one of the architectures (NavA3C+D1D2L) from that paper. It conducts an extensive assessment of the methods in the critiqued paper but does not introduce any alternative method. For this reason, I had to carefully re-read the critiqued paper to be able to assess the validity of the arguments made in this submission and to evaluate its merit from the point of view of the quality of the critique. The (Mirowski et al, 2016) paper shows that a neural network-based agent with LSTM-based memory and auxiliary tasks such as depth map prediction can learn to navigate in fixed environments (3D mazes) with a fixed goal position (what they call “static maze”), and in fixed mazes with changing goal environments (what they call “environments with dynamic elements” or “random goal mazes”).

This submission claims that:
[a] “[based on the critiqued paper] one might assume that DRL-based algorithms are able to 'learn to navigate' and are thus ready to replace classical mapping and path-planning algorithms”,
[b] “following training and testing on constant map structures, when trained and tested on the same maps, [the NavA3C+D1D2L algorithm] is able to choose the shorter paths to the goal”,
[c] “when tested on unseen maps the algorithm utilizes a wall-following strategy to find the goal without doing any mapping or path planning”,
[d] “this state-of-the-art result is shown to be successful on only one map, which brings into question the repeatability of the results”,
[e] “Do DRL-based navigation algorithms really 'learn to navigate'? Our results answer this question negatively.”
[f] “we are the first to evaluate any DRL-based navigation method on maps with unseen structures”

The paper also conducts an extensive analysis of the performance of a different version of the NavA3C+D1D2L algorithm (without velocity inputs, which probably makes learning path integration much more difficult), in the same environments but by introducing unjustified changes (e.g., with constant velocities and a different action space) and with a different reward structure (incorporating a negative reward for wall collisions). While the experimental setup does not match (Mirowski et al, 2016), thereby invalidating claim [d], the experiments are thorough and do show that that architecture does not generalize to unseen mazes. The use of attention heat maps is interesting.

The main problem however is that it seems that this submission completely misrepresents the intent of (Mirowski et al, 2016) by using a straw man argument, and makes a rather unacademic and unsubstantiated accusation of lack of repeatability of the results.

Regarding the former, I could not find any claim that the methods in (Mirowski et al, 2017) learn mapping and path planning in unseen environments, that could support claim [a]. More worryingly, when observing that the method of (Mirowski et al, 2017) may not generalize to unseen environments in claim [c], the authors of this submission seem to confuse navigation, cartography and SLAM, and attribute to that work claims that were never made in the first place, using a straw man argument. Navigation is commonly defined as the goal driven control of an agent, following localization, and is a broad skill that involves the determination of position and direction, with or without a map of the environment (Fox 1998, ” Markov Localization: A Probabilistic Framework for Mobile Robot Localization and Navigation”). This widely accepted definition of navigation does not preclude being limited to known environments only.

Regarding repeatability, the claim [d] is contradicted in section 5 when the authors demonstrate that the NavA3C+D1D2L algorithm does achieve a reduction in latency to goal in 8 out of 10 experiments on random goal, static map and random or static spawns. The experiments in section 5.3 are conducted in simple but previously unseen maps and cannot logically contradict results (Mirowski et al, 2016) achieved by training on static maps such as their “I-maze”. Moreover, claim [d] about repeatability is also invalidated by the fact that the experiments described in the paper use different observations (no velocity inputs), different action space, different reward structure, with no empirical evidence to support these changes. It seems, as the authors also claim in [b], that the work of (Mirowski et al, 2017), which was about navigation in known environments, actually is repeatable.

Additionally, some statements made by the authors are demonstrably untrue. First, the authors claim that they are the first to train DRL agents in all random mazes [f], but this has been already shown in at least two publications (Mnih et al, 2016 and Jaderberg et al, 2016).

Second, the title of the submission, “Do Deep Reinforcement Learning Algorithms Really Learn to Navigate” makes a broad statement [e] that cannot be logically invalidated by only one particular set of experiments on a particular model and environment, particularly since it directly targets one specific paper (out of several recent papers that have addressed navigation) and one specific architecture from that paper, NavA3C+D1D2L (incidentally, not the best-performing one, according to table 1 in that paper). Why did the authors not cite and consider (Parisotto et al, 2017, “Neural Map: Structured Memory for Deep Reinforcement Learning”), which explicitly claims that their method is “capable of generalizing to environments that were not seen during training”? It seems that the authors need to revise both their bibliography and their logical reasoning: one cannot invalidate a broad set of algorithms for a broad goal, simply by taking a specific example and showing that it does not fit a particular interpretation of navigation *in previously unseen environments*.
","The review expresses a negative sentiment towards the paper, criticizing its methodology and claims. It accuses the authors of misrepresenting previous work, making unsubstantiated claims, and drawing overly broad conclusions. The reviewer uses strong language, such as ""completely misrepresents,"" ""straw man argument,"" ""unacademic,"" and ""demonstrably untrue,"" indicating a lack of politeness. However, the review also acknowledges the thoroughness of the experiments and the interesting use of attention heat maps.",-60,-40
"This paper proposes to optimize neural networks considering the three different terms: original loss function, quantization error and the sum of bits. While the idea makes sense, the paper is not well executed, and I cannot understanding how gradient descend is performed based on the description of Section 4.

1. After equation (5), I don't understand how the gradient of L(tilde_W) w.r.t. B(i) is computed. B(i) is discrete. The update rule seems to be clearly wrong.
2. The experimental section of this paper needs improvement.
   a. End-to-end trained quantized networks have been studied in various previous works including stochastic neuron (Bengio et al 2013), quantization + fine tuning (Wu et al 2016 Quantized Convolutional Neural Networks for Mobile Devices), Binary connect (Courbariaux et al 2016) etc. None of these works have been compared with.
   b. All the baseline methods use 8 bits per value. This choice is quite ad-hoc.
   c. Only MNIST and CIFAR10 dataset with Lenet32 are used in the experiment. I find the findings not conclusive based on these.
   d. No wall-time and real memory numbers are reported.","The reviewer expresses that the core idea of the paper makes sense, but they find issues with the execution, particularly the clarity of the gradient descent description and the experimental setup. They list several shortcomings, indicating a negative sentiment. However, the language remains professional and focused on the scientific content, suggesting politeness. Therefore, the sentiment leans negative due to the critical points raised, but the politeness remains neutral to slightly positive due to the constructive framing of the criticism.",-30,20
"The authors proposed a convolutional framework based on merging ego-networks. It combines graph embedding layers with task driven output layers, producing interpretable results for critical structure detection. While based on existing embedding methods such as Patchy-San, the contribution of ego-centric convolution and multi-layer architechture is novel and has a lot of potential in applications. The overall presentation of the draft is also of high quality. I recommend its publication at ICLR.

Here is a list of suggested changes to further improve the draft,

1. The two panels of Figure 1 seems redundant.

2. Figure 4 does not provide useful information, especially in terms of how overlapping neighborhoods are aggregated at deeper layers.

3. There seems to be a mistake in Figure 5 with the top neighborhood in white

4. The connection between weight-tying and scale-free structure needs better explanation. Are the authors trying to say that fractal processes generates power-law degree distributions?

5. The visualization of critical structures are very helpful. However, it might be better to look into structures in high level layers for truly global signatures. This is especially the case for the reddit dataset, where visualizations at the node and edge level creates hairballs.

","The review is very positive. The reviewer finds the proposed method novel and with high potential. They recommend publication at ICLR, which is a strong indicator of a positive sentiment. The language used is constructive and polite, suggesting specific areas of improvement without being demanding or disrespectful.",85,90
"This paper experiments the application of NAS to some natural language processing tasks : machine translation and question answering.  

My main concern about this paper is its contribution. The difference with the paper of Zoph 2017 is really slight in terms of methodology. Moving from a language modeling task to machine translation is not very impressive neither really discussed. It could be interesting to change the NAS approach by taking into account this application shift.  

On the experimental part, the paper is not really convincing. The results on WMT are not state of the art. The best system of this year was a standard phrase based and has achieved 29.3 BLEU score (for BLEU cased, otherwise it's one point more). Therefore the results on mt tasks are difficult to interpret. 

At the end , the reader can be sure these experiments required a significant computational power. Beyond that it is difficult to really draw meaningful conclusions. ","The review is critical of the paper's contribution and experimental results. It points out the lack of significant novelty compared to prior work and the unconvincing performance on machine translation tasks. The reviewer also questions the value of the findings despite the computational effort involved. All of these points suggest a negative sentiment. However, the language used is professional and avoids harsh or disrespectful language, indicating a neutral politeness level.",-50,0
"The description of the proposed method is very unclear. From the paper it is very difficult to make out exactly what architecture is proposed. I understand that the prior on the z_i in each layer is a pixel-cnn, but what is the posterior? Equations 8 and 9 would suggest it is of the same form (pixel-cnn) but this would be much too slow to sample during training. I'm guessing it is just a factorized Gaussian, with a separate factorized Gaussian pseudo-prior? That is, in figure 1 all solid lines are factorized Gaussians and all dashed lines are pixel-cnns?

* The word ""layers"" is sometimes used to refer to latent variables z, and sometimes to parameterized neural network layers in the encoder and decoder. E.g. ""The top stochastic layer z_L in FAME is a fully-connected dense layer"". No, z_L is a vector of latent variables. Are you saying the encoder produces it using a fully-connected layer?
* Section 2.2 starts talking about ""deterministic layers h"". Are these part of the encoder or decoder? What is meant by ""number of layers connecting the stochastic latent variables""?
* Section 2.3: What is meant by ""reconstruction data""?

If my understanding of the method is correct, the novelty is limited. Autoregressive priors were used previously in e.g. the Lossy VAE by Chen et al. and IAF-VAE by Kingma et al. The reported likelihood results are very impressive though, and would be reason for acceptance if correct. However, the quality of the sampled images shown for CIFAR-10 doesn't match the reported likelihood. There are multiple possible reasons for this, but after skimming the code I believe it might be due to a faulty implementation of the variational lower bound. Instead of calculating all quantities in the log domain, the code takes explicit logs and exponents and stabilizes them by adding small quantities ""eps"": this is not guaranteed to give the right result. Please fix this and re-run your experiments. (I.e. in _loss.py don't use x/(exp(y)+eps) but instead use x*exp(-y). Don't use log(var+eps) with var=softplus(x), but instead use var=softplus(x)+eps or parameterize the variance directly in the log domain).","The review starts with a very negative sentiment, expressing strong confusion and using phrases like ""very unclear"" and ""much too slow."" This suggests a negative initial impression. However, the reviewer acknowledges the potential significance of the findings (""impressive likelihood results"") and provides concrete advice for improvement. This mix of criticism and constructive feedback points to a neutral-to-positive overall sentiment. The language is direct and technical but maintains professional courtesy. There are no personal attacks or disrespectful remarks.",20,60
"Summary:

The paper proposes a framework for constructing spherical convolutional networks (ConvNets) based on a novel synthesis of several existing concepts.  The goal is to detect patterns in spherical signals irrespective of how they are rotated on the sphere.  The key is to make the convolutional architecture rotation equivariant.

Pros:

+ novel/original proposal justified both theoretically and empirically
+ well written, easy to follow
+ limited evaluation on a classification and regression task is suggestive of the proposed approach's potential
+ efficient implementation

Cons:

- related work, in particular the first paragraph, should compare and contrast with the closest extant work rather than merely list them
- evaluation is limited; granted this is the nature of the target domain

Presentation:

While the paper is generally written well, the paper appears to conflate the definition of the convolutional and correlation operators?  This point should be clarified in a revised manuscript.  

In Section 5 (Experiments), there are several references to S^2CNN.  This naming of the proposed approach should be made clear earlier in the manuscript.  As an aside, this appears a little confusing since convolution is performed first on S^2 and then SO(3). 

Evaluation:

What are the timings of the forward/backward pass and space considerations for the Spherical ConvNets presented in the evaluation section?  Please provide specific numbers for the various tasks presented.

How many layers (parameters) are used in the baselines in Table 2?  If indeed there are much less parameters used in the proposed approach, this would strengthen the argument for the approach.  On the other hand, was there an attempt to add additional layers to the proposed approach for the shape recognition experiment in Sec. 5.3 to improve performance?

Minor Points:

- some references are missing their source, e.g., Maslen 1998 and Kostolec, Rockmore, 2007, and Ravanbakhsh, et al. 2016.

- some sources for the references are presented inconsistency, e.g., Cohen and Welling, 2017 and Dieleman, et al. 2017

- some references include the first name of the authors, others use the initial 

- in references to et al. or not, appears inconsistent

- Eqns 4, 5, 6, and 8 require punctuation

- Section 4 line 2, period missing before ""Since the FFT""

- ""coulomb matrix"" --> ""Coulomb matrix""

- Figure 5, caption: ""The red dot correcpond to"" --> ""The red dot corresponds to""

Final remarks:

Based on the novelty of the approach, and the sufficient evaluation, I recommend the paper be accepted.

","The reviewer provides a generally positive assessment, highlighting the novelty, clarity, and potential of the proposed approach. While pointing out limitations in evaluation and suggesting improvements in presentation and referencing, the reviewer ultimately recommends acceptance. The language used is constructive, professional, and encouraging, indicating a positive disposition towards the paper.",75,80
"This paper applies recently developed ideas in the literature of robust optimization, in particular distributionally robust optimization with Wasserstein metric, and showed that under this framework for smooth loss functions when not too much robustness is requested, then the resulting optimization problem is of the same difficulty level as the original one (where the adversarial attack is not concerned). I think the idea is intuitive and reasonable, the result is nice. Although it only holds when light robustness are imposed, but in practice, this seems to be more of the case than say large deviation/adversary exists. As adversarial training is an important topic for deep learning, I feel this work may lead to promising principled ways for adversarial training. ","The review is positive because the reviewer states ""the idea is intuitive and reasonable, the result is nice."" and predicts potential impact: ""this work may lead to promising principled ways for adversarial training."" The reviewer does point out a limitation (only holds for light robustness) but frames it positively as this ""seems to be more of the case in practice."" The language is polite and respectful throughout, without any negative framing or harsh criticism.",75,80
"The paper proposes a novel approach on estimating the parameters  
of Mean field games (MFG). The key of the method is a reduction of the unknown parameter MFG to an  unknown parameter Markov Decision Process (MDP).

This is an important class of models and I recommend the acceptance of the paper.

I think that the general discussion about the collective behavior application should be more carefully presented and some better examples of applications should be easy to provide.  In addition the authors may want to enrich their literature review and give references to alternative work on unknown MDP estimation methods cf. [1], [2] below. 

[1] Burnetas, A. N., & Katehakis, M. N. (1997). Optimal adaptive policies for Markov decision processes. Mathematics of Operations Research, 22(1), 222-255.

[2] Budhiraja, A., Liu, X., & Shwartz, A. (2012). Action time sharing policies for ergodic control of Markov chains. SIAM Journal on Control and Optimization, 50(1), 171-195.","The reviewer explicitly recommends acceptance, a very positive sign. They also use positive language like ""important class of models."" While they suggest improvements, these are constructive and typical for a peer review. The suggestions do not detract from the overall positive sentiment. The language used is formal and professional, typical for academic discourse, without any rudeness.",85,75
"One of the main problems with imitation learning in general is the expense of expert demonstration. The authors here propose a method for sidestepping this issue by using the random exploration of an agent to learn generalizable skills which can then be applied without any specific pretraining on any new task. 

The proposed method has at its core a method for learning a parametric skill function (PSF) that takes as input a description of the initial state, goal state, parameters of the skill and outputs a sequence of actions (could be of varying length) which take the agent from initial state to goal state.

The skill function uses a RNN as function approximator and minimizes the sum of two losses i.e. the state mismatch loss over the trajectory (using an explicitly learnt forward model) and the action mismatch loss (using a model-free action prediction module) . This is hard to do in practice due to jointly learning both the forward model as well as the state mismatches. So first they are separately learnt and then fine-tuned together. 

In order to decide when to stop, an independent goal detector is trained which was found to be better than adding a 'goal-reached' action to the PSF.

Experiments on two domains are presented. 1. Visual navigation where images of start and goal states are given as input. 2. Robotic knot-tying with a loose rope where visual input of the initial and final rope states are given as input.

Comments:

- In the visual navigation task no numbers are presented on the comparison to slam-based techniques used as baselines although it is mentioned that it will be revisited.

- In the rope knot-tying task no slam-based or other classical baselines are mentioned.

- My main concern is that I am really trying to place this paper with respect to doing reinforcement learning first (either in simulation or in the real world itself, on-policy or off-policy) and then just using the learnt policy on test tasks. Or in other words why should we call this zero-shot imitation instead of simply reinforcement learnt policy being learnt and then used. The nice part of doing RL is that it provides ways of actively controlling the exploration. See this pretty relevant paper which attempts the same task and also claims to have the target state generalization ability. 

Target-driven Visual Navigation in Indoor Scenes using Deep Reinforcement Learning by Zhu et al.

I am genuinely curious and would love the authors' comments on this. It should help make it clearer in the paper as well.
 
Update:

After evaluating the response from the authors and ensuing discussion as well as the other reviews and their corresponding discussion, I am revising my rating for this paper up. This will be an interesting paper to have at the conference and will spur more ideas and follow-on work.","The reviewer raises valid concerns and questions the paper's framing and novelty compared to reinforcement learning approaches. However, they also acknowledge the paper's potential to stimulate further research and find it interesting enough to warrant presentation at the conference. This suggests the review leans slightly positive, but with reservations. The reviewer's willingness to revise their rating upwards after the author's response further indicates a positive shift in their perspective.",50,70
"This work identifies a mistake in the existing proof of convergence of
Adam, which is among the most popular optimization methods in deep
learning. Moreover, it gives a simple 1-dimensional counterexample with
linear losses on which Adam does not converge. The same issue also
affects RMSprop, which may be viewed as a special case of Adam without
momentum. The problem with Adam is that the ""learning rate"" matrices
V_t^{1/2}/alpha_t are not monotonically decreasing. A new method, called
AMSGrad is therefore proposed, which modifies Adam by forcing these
matrices to be decreasing. It is then shown that AMSGrad does satisfy
essentially the same convergence bound as the one previously claimed for
Adam. Experiments and simulations are provided that support the
theoretical analysis.

Apart from some issues with the technical presentation (see below), the
paper is well-written.

Given the popularity of Adam, I consider this paper to make a very
interesting observation. I further believe all issues with the technical
presentation can be readily addressed.



Issues with Technical Presentation:

- All theorems should explicitly state the conditions they require
  instead of referring to ""all the conditions in (Kingma & Ba, 2015)"".
- Theorem 2 is a repetition of Theorem 1 (except for additional
  conditions).
- The proof of Theorem 3 assumes there are no projections, so this
  should be stated as part of its conditions. (The claim in footnote 2
  that they can be handled seems highly plausible, but you should be up
  front about the limitations of your results.)
- The regret bound Theorem 4 establishes convergence of the optimization
  method, so it plays the role of a sanity check. However, it is
  strictly worse than the regret bound O(sqrt{T}) for online gradient
  descent [Zinkevich,2003], so it cannot explain why the proposed
  AMSgrad method might be adaptive. (The method may indeed be adaptive
  in some sense; I am just saying the *bound* does not express that.
  This is also not a criticism of the current paper; the same remark
  also applies to the previously claimed regret bound for Adam.)
- The discussion following Corollary 1 suggests that sum_i
  hat{v}_{T,i}^{1/2} might be much smaller than d G_infty. This is true,
  but we should always expect it to be at least a constant, because
  hat{v}_{t,i} is monotonically increasing by definition of the
  algorithm, so the bound does not get better than O(sqrt(T)).
  It is also suggested that sum_i ||g_{1:T,i}|| = sqrt{sum_{t=1}^T
  g_{t,i}^2} might be much smaller than dG_infty, but this is very
  unlikely, because this term will typically grow like O(sqrt{T}),
  unless the data are extremely sparse, so we should at least expect
  some dependence on T.
- In the proof of Theorem 1, the initial point is taken to be x_1 = 1,
  which is perfectly fine, but it is not ""without loss of generality"",
  as claimed. This should be stated in the statement of the Theorem.
- The proof of Theorem 6 in appendix B only covers epsilon=1. If it is
  ""easy to show"" that the same construction also works for other
  epsilon, as claimed, then please provide the proof for general
  epsilon.


Other remarks:

- Theoretically, nonconvergence of Adam seems a severe problem. Can you
  speculate on why this issue has not prevented its widespread adoption?
  Which factors might mitigate the issue in practice?
- Please define g_t \circ g_t and g_{1:T,i}
- I would recommend sticking with standard linear algebra notation for
  the sqrt and the inverse of a matrix and simply using A^{-1} and
  A^{1/2} instead of 1/A and sqrt{A}.
- In theorems 1,2,3, I would recommend stating the dimension (d=1) of
  your counterexamples, which makes them very nice!

Minor issues:

- Check accent on Nicol\`o Cesa-Bianchi in bibliography.
- Near the end of the proof of Theorem 6: I believe you mean Adam
  suffers a ""regret"" instead of a ""loss"" of at least 2C-4.
  Also 2C-4=2C-4 is trivial in the second but last display.
","The review starts with positive statements, highlighting the significance of the paper's findings and praising its readability. While it points out technical issues, the reviewer believes these are addressable and doesn't diminish the paper's contribution. The tone throughout is constructive and professional, suggesting areas for improvement rather than harshly criticizing.",75,80
"The authors has addressed my concerns, so I raised my rating. 

The paper is grounded on a solid theoretical motivation and the analysis is sound and quite interesting.

There are no results on large corpora such as 1 billion tokens benchmark corpus, or at least medium level corpus with 50 million tokens. The corpora the authors choose are quite small, the variance of the estimates are high, and similar conclusions might not be valid on a large corpus. 

[1] provides the results of character level language models on Enwik8 dataset, which shows regularization doesn't have much effect and needs less tuning. Results on this data might be more convincing.

The results of MOS is very good, but the computation complexity is much higher than other baselines. In the experiments, the embedding dimension of MOS is slightly smaller, but the number of mixture is 15. This will make it less usable, I think it's necessary to provide the training time comparison.

Finally experiments on machine translation or speech recognition should be done and to see what improvements the proposed method could bring for BLEU or WER. 

[1] Melis, Gábor, Chris Dyer, and Phil Blunsom. ""On the state of the art of evaluation in neural language models."" arXiv preprint arXiv:1707.05589 (2017).

[2] Joris Pelemans, Noam Shazeer, Ciprian Chelba, Sparse Non-negative Matrix Language Modeling,  Transactions of the Association for Computational Linguistics, vol. 4 (2016), pp. 329-342

[3] Shazeer et al. (2017). Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer. ICLR 2017
","The reviewer's sentiment is positive overall. They explicitly mention raising their rating and call the paper's motivation ""solid"" and the analysis ""sound and quite interesting."" While they raise several constructive criticisms and suggestions for improvement, these are typical of peer reviews and do not negate the positive sentiment. The language used is polite and professional throughout, employing a neutral and objective tone. There are no instances of harsh language or personal attacks.",65,75
"This is a very well-written and nicely structured paper that tackles the problem of generating/inferring code given an incomplete description (sketch) of the task to be achieved. This is a novel contribution to existing machine learning approaches to automated programming that is achieved by training on a large corpus of Android apps. The combination of the proposed technique and leveraging of real data are a substantial strength of the work compared to many approaches that have come previously.

This paper has many strengths:
1) The writing is clear, and the paper is well-motivated
2) The proposed algorithm is described in excellent detail, which is essential to reproducibility
3) As stated previously, the approach is validated with a large number of real Android projects
4) The fact that the language generated is non-trivial (Java-like) is a substantial plus
5) Good discussion of limitations

Overall, this paper is a valuable addition to the empirical software engineering community, and a nice break from more traditional approaches of learning abstract syntax trees.","The review starts with very positive statements about the paper's contribution, writing style, and methodology. It lists five specific strengths, including clarity, detailed description, use of real data, non-trivial language generation, and a good discussion of limitations. The reviewer clearly states that this is a ""valuable addition"" to the field. All of this points to a very positive sentiment. The language used throughout is formal, respectful, and encouraging, indicating a high level of politeness.",90,90
"This paper proposed a gradient-based meta-learning approach for continuous adaptation in nonstationary and adversarial environment. The idea is to treat a nonstationary task as a sequence of stationary tasks and train agents to exploit the dependencies between consecutive tasks such that they can deal with nonstationarities at test time. The proposed method was evaluated based on a nonstationary locomotion and within a competitive multi agent setting. For the later, this paper specifically designed the RomoSumo environment and defined iterated adaptation games to test various aspect of adaptation strategies. The empirical results in both cases demonstrate the efficacy of the proposed meta-learned adaptation rules over the baselines in the few-short regime. The superiority of meta-learners is further justified on a population level.

The paper addressed a very important problem for general AI and it is well-written. Careful experiment designs, and thorough comparisons make the results conniving. I

Further comments:

1. In the experiment the trajectory number seems very small, I wonder if directly using importance weight as shown in (9) will cause high variance in the performance?

2. One of the assumption in this work is that trajectories from T_i contain some information about T_{i+1}, I wonder what will happen if the mutually information is very small between them (The extreme case is that two tasks are independent), will current method still perform well?

P7, For the RL^2 policy, the authors mentioned that “…with a given environment (or an opponent), reset the state once the latter changes” How does the agent know when an environment (or opponent) changes? 

P10, “This suggests that it meta-learned a particular…” This sentence need to be rewritten.

P10, ELO is undefined
","The reviewer starts with positive statements, highlighting the importance of the problem, the paper's quality, the experiment design, and the convincing results. They also use phrases like ""well-written"" and ""convincing."" However, they also raise several questions and point out areas for improvement, which indicates that the review is not entirely positive. Therefore, the sentiment is somewhat positive but not overly enthusiastic. The language used is polite and constructive. The reviewer asks questions and raises concerns respectfully and professionally.",60,80
"This paper borrows the classic idea of spectral regularization, recently applied to deep learning by Yoshida and Miyato (2017) and use it to normalize GAN objectives. The ensuing GAN, coined SN-GAN, essentially ensures the Lipschitz property of the discriminator. This Lipschitz property has already been proposed by recent methods and has showed some success. However,  the authors here argue that spectral normalization is more powerful; it allows for models of higher rank (more non-zero singular values) which implies a more powerful discriminator and eventually more accurate generator. This is demonstrated in comparison to weight normalization in Figure 4. The experimental results are very good and give strong support for the proposed normalization.


While the main idea is not new to machine learning (or deep learning), to the best of my knowledge it has not been applied on GANs. The paper is overall well written (though check Comment 3 below), it covers the related work well and it includes an insightful discussion about the importance of high rank models. I am recommending acceptance, though I anticipate to see a more rounded evaluation of the exact mechanism under which SN improves over the state of the art. More details in the comments below.

Comments:
1. One concern about this paper is that it doesn’t fully answer the reasons why this normalization works better. I found the discussion about rank to be very intuitive, however this intuition is not fully tested.  Figure 4 reports layer spectra for SN and WN. The authors claim that other methods, like (Arjovsky et al. 2017) also suffer from the same rank deficiency. I would like to see the same spectra included. 
2. Continuing on the previous point: maybe there is another mechanism at play beyond just rank that give SN its apparent edge? One way to test the rank hypothesis and better explain this method is to run a couple of truncated-SN experiments. What happens if you run your SN but truncate its spectrum after every iteration in order to make it comparable to the rank of WN? Do you get comparable inception scores? Or does SN still win?
3. Section 4 needs some careful editing for language and grammar.
","The review is largely positive. The reviewer finds the idea interesting, the paper well-written, and the results promising. They recommend acceptance but suggest further experiments to strengthen the claims and explanations. The language used is constructive and professional.",75,80
"The paper introduces an application of Graph Neural Networks (Li's Gated Graph Neural Nets, GGNNs, specifically) for reasoning about programs and programming. The core idea is to represent a program as a graph that a GGNN can take as input, and train the GGNN to make token-level predictions that depend on the semantic context. The two experimental tasks were: 1) identifying variable (mis)use, ie. identifying bugs in programs where the wrong variable is used, and 2) predicting a variable's name by consider its semantic context.

The paper is generally well written, easy to read and understand, and the results are compelling. The proposed GGNN approach outperforms (bi-)LSTMs on both tasks. Because the tasks are not widely explored in the literature, it could be difficult to know how crucial exploiting graphically structured information is, so the authors performed several ablation studies to analyze  this out. Those results show that as structural information is removed, the GGNN's performance diminishes, as expected. As a demonstration of the usefulness of their approach, the authors ran their model on an unnamed open-source project and claimed to find several bugs, at least one of which potentially reduced memory performance.

Overall the work is important, original, well-executed, and should open new directions for deep learning in program analysis. I recommend it be accepted.","The review is overwhelmingly positive. The reviewer explicitly states the work is ""important, original, well-executed"" and recommends acceptance. They praise the writing, clarity, results, and ablation studies. There are no negative remarks, only constructive suggestions.",95,100
"The authors describe a method called WAGE, which quantize all operands and operators in a neural network, specifically, the weights (W), activations (A), gradients (G), and errors (E) . The idea is using quantizers with clipping (denoted in the paper with Q(x,k)) and some additional operators like shift (denoted with shift(x)) and stochastic rounding. The main motivation of the authors in this work is to reduce the number of bits for representation in a network for all the WAGE operations and operands which influences the power consumption and silicon area in hardware implementations.

After introducing the idea and related work, the authors in Section 3 give details about how to perform the quantization. They introduce the additional operators needed for training in such network. Since quantization may loss some information, the authors need to quantize the signals in the network around the dynamic range in order not to ""kill"" the signal. The authors describe how to do that. Afterward, as in other techniques for quantization, they describe how to initialize the network values. Also, they argue that batch normalization in this network is replaced with the shift-quantize operations, and what is matter in this case is (1) the relative values (“orientations”) and not the absolute values and (2) small values in errors are negligible.

Afterward, the authors conduct experiments on MNIST, SVHN, CIFAR10, and ILSVRC12 datasets, where they show promising results compared to the errors provided by previous works. The WAGE parameters (i.e., the quantized no. of bits used) are 2-8-8-8, respectively. For understand more the WAGE, the authors compare on CIFAR10 the test error rate with vanilla CNN and show is small loss in using their network. The authors investigate mainly the bitwidth of errors and gradients.

In overall, this paper is an accept since it shows good performance on standard problems and invent some nice tricks to implement NN in hardware, for *both* training and inference. For inference only, other works has more to offer but this is a promising technique for learning. The things that are still missing in this work are some power reduction estimates as well as area reduction estimations. This will give the hardware community a clear vision of how such methods may be implemented both in data centers as well as on end portable devices. 
","The reviewer explicitly recommends the paper for acceptance, stating ""In overall, this paper is an accept"". They highlight the good performance and novel techniques. While they point out areas for improvement, these are framed as suggestions rather than criticisms. The language is formal and objective, typical of scientific peer reviews.",75,50
"This paper presents methods to reduce the variance of policy gradient using an action dependent baseline. Such action dependent baseline can be used in settings where the action can be decomposed into factors that are conditionally dependent given the state. The paper:
(1) shows that using separate baselines for actions, each of which can depend on the state and other actions is bias-free
(2) derive the optimal action-dependent baseline, showing that it does not degenerate into state-only dependent baseline, i.e. there is potentially room for improvement over state-only baselines.
(3) suggests using marginalized action-value (Q) function as a practical baseline, generalizing the use of value function in state-only baseline case.
(4) suggests using MC marginalization and also using the ""average"" action to improve computational feasibility
(5) combines the method with GAE techniques to further improve convergence by trading off bias and variance

The suggested methods are empirically evaluated on a number of settings. Overall action-dependent baseline outperform state-only versions. Using a single average action marginalization is on par with MC sampling, which the authors attribute to the low quality of the Q estimate. Combining GAE shows that a hint of bias can be traded off with further variance reduction to further improve the performance.

I find the paper interesting and practical to the application of policy gradient in high dimensional action spaces with some level of conditional independence present in the action space. In light of such results, one might change the policy space to enforce such structure.

Notes:
- Elaborate further on the assumption made in Eqn 9. Does it mean that the actions factors cannot share (too many) parameters in the policy construction, or that shared parameters can only be applied to the state?
- Eqn 11 should use \simeq
- How can the notion of average be extended to handle multi-modal distributions, or categorical or structural actions? Consider expanding on that in section 4.5.
- The discussion on the DAG graphical model is lacking experimental analysis (where separate baselines models are needed). How would you train such baselines?
- Figure 4 is impossible to read in print. The fonts are too small for the numbers and the legends.
","The reviewer explicitly states that they find the paper ""interesting and practical."" They also provide constructive criticism and suggestions for improvement, indicating a positive attitude towards the work. The reviewer does not use harsh language and maintains a professional and respectful tone throughout the review. Therefore, the sentiment is positive, and the language is polite.",75,75
"The paper describes a number of modifications of GAN training that enable synthesis of high-resolution images. The modifications also support more automated longer-term training, and increasing variability in the results.

The key modification is progressive growing. First, a GAN is trained for image synthesis at very low resolution. Then a layer that refines the resolution is progressively faded in. (More accurately, a corresponding pair of layers, one in the generation and one in the discriminator.) This progressive fading in of layers is repeated, one octave at a time, until the desired resolution is reached.

Another modification reported in the paper is a simple parameter-free minibatch summary statistic feature that is reported to increase variation. Finally, the paper describes simple schemes for initialization and feature normalization that are reported to be more effective than commonly used initializers and batchnorm.

It's a very nice paper. It does share the ""bag of tricks"" nature of many GAN papers, but as such it is better than most of the lot. I appreciate that some of the tricks actually simplify training, and most are conceptually reasonable. The paper is also very well written.

My quibbles are minor. First, I would discuss [Huang et al., CVPR 2017] and the following paper more prominently:

[Zhang et al., ICCV 2017] H. Zhang, T. Xu, H. Li, S. Zhang, X. Wang, X. Huang, and D. Metaxas. StackGAN: Text to photo-realistic image synthesis with stacked generative adversarial networks. In ICCV, 2017.

I couldn't find a discussion of [Huang et al., CVPR 2017] at all, although it's in the bibliography. (Perhaps I overlooked the discussion.) And [Zhang et al., ICCV 2017] is quite closely related, since it also tackles high-resolution synthesis via multi-scale refinement. These papers don't diminish the submission, but they should be clearly acknowledged and the contribution of the submission relative to these prior works should be discussed.

Also, [Rabin et al., 2011] is cited in Section 5 but I couldn't find it in the bibliography.
","The reviewer clearly states ""It's a very nice paper"" and that they ""appreciate"" some of the work's aspects. They call their concerns ""quibbles"", which indicates an overall positive view. The reviewer does point out some missing references and comparisons, which slightly reduces the overall positivity. Hence, a sentiment score of 80 is appropriate. The language used is polite and professional throughout, justifying a politeness score of 100.",80,100
"This paper investigates the impact of character-level noise on various flavours of neural machine translation. It tests 4 different NMT systems with varying degrees and types of character awareness, including a novel meanChar system that uses averaged unigram character embeddings as word representations on the source side. The authors test these systems under a variety of noise conditions, including synthetic scrambling and keyboard replacements, as well as natural (human-made) errors found in other corpora and transplanted to the training and/or testing bitext via replacement tables. They show that all NMT systems, whether BPE or character-based, degrade drastically in quality in the presence of both synthetic and natural noise, and that it is possible to train a system to be resistant to these types of noise by including them in the training data. Unfortunately, they are not able to show any types of synthetic noise helping address natural noise. However, they are able to show that a system trained on a mixture of error types is able to perform adequately on all types of noise.

This is a thorough exploration of a mostly under-studied problem. The paper is well-written and easy to follow. The authors do a good job of positioning their study with respect to related work on black-box adversarial techniques, but overall, by working on the topic of noisy input data at all, they are guaranteed novelty. The inclusion of so many character-based systems is very nice, but it is the inclusion of natural sources of noise that really makes the paper work. Their transplanting of errors from other corpora is a good solution to the problem, and one likely to be built upon by others. In terms of negatives, it feels like this work is just starting to scratch the surface of noise in NMT. The proposed meanChar architecture doesn’t look like a particularly good approach to producing noise-resistant translation systems, and the alternative solution of training on data where noise has been introduced through replacement tables isn’t extremely satisfying. Furthermore, the use of these replacement tables means that even when the noise is natural, it’s still kind of artificial. Finally, this paper doesn’t seem to be a perfect fit for ICLR, as it is mostly experimental with few technical contributions that are likely to be impactful; it feels like it might be more at home and have greater impact in a *ACL conference.

Regarding the artificialness of their natural noise - obviously the only solution here is to find genuinely noisy parallel data, but even granting that such a resource does not yet exist, what is described here feels unnaturally artificial. First of all, errors learned from the noisy data sources are constrained to exist within a word. This tilts the comparison in favour of architectures that retain word boundaries (such as the charCNN system here), while those systems may struggle with other sources of errors such as missing spaces between words. Second, if I understand correctly, once an error is learned from the noisy data, it is applied uniformly and consistently throughout the training and/or test data. This seems worse than estimating the frequency of the error and applying them stochastically (or trying to learn when an error is likely to occur). I feel like these issues should at least be mentioned in the paper, so it is clear to the reader that there is work left to be done in evaluating the system on truly natural noise.

Also, it is somewhat jarring that only the charCNN approach is included in the experiments with noisy training data (Table 6). I realize that this is likely due to computational or time constraints, but it is worth providing some explanation in the text for why the experiments were conducted in this manner. On a related note, the line in the abstract stating that “... a character convolutional neural network  is able to simultaneously learn representations robust to multiple kinds of noise” implies that the other (non-charCNN) architectures could not learn these representations, when in reality, they simply weren’t given the chance.

Section 7.2 on the richness of natural noise is extremely interesting, but maybe less so to an ICLR audience. From my perspective, it would be interesting to see that section expanded, or used as the basis for future work on improve architectures or training strategies.

I have only one small, specific suggestion: at the end of Section 3, consider deleting the last paragraph break, so there is one paragraph for each system (charCNN currently has two paragraphs).

[edited for typos]","The review is positive about the paper's novelty, thoroughness, and writing quality. It praises the exploration of natural noise and the use of error transplantation. However, it points out limitations such as the artificiality of the natural noise simulation, the limited exploration of noise-resistant architectures, and the lack of groundbreaking technical contributions. The reviewer suggests the paper might be better suited for an ACL conference due to its experimental nature. The language used is polite and constructive throughout, offering suggestions for improvement rather than harsh criticism.",60,80
"This paper presents a method for image classification given test-time computational budgeting constraints.  Two problems are considered:  ""any-time"" classification, in which there is a time constraint to evaluate a single example, and batched budgets, in which there is a fixed budget available to classify a large batch of images.  A convolutional neural network structure with a diagonal propagation layout over depth and scale is used, so that each activation map is constructed using dense connections from both same and finer scale features.  In this way, coarse-scale maps are constructed quickly, then continuously updated with feed-forward propagation from lower layers and finer scales, so they can be used for image classification at any intermediate stage.  Evaluations are performed on ImageNet and CIFAR-100.

I would have liked to see the MC baselines also evaluated on ImageNet --- I'm not sure why they aren't there as well?  Also on p.6 I'm not entirely clear on how the ""network reduction"" is performed --- it looks like finer scales are progressively dropped in successive blocks, but I don't think they exactly correspond to those that would be needed to evaluate the full model (this is ""lazy evaluation"").  A picture would help here, showing where the depth-layers are divided between blocks.

I was also initially a bit unclear on how the procedure described for batched budgeted evaluation achieves the desired result:  It seems this relies on having a batch that is both large and varied, so that its evaluation time will converge towards the expectation.  So this isn't really a hard constraint (just an expected result for batches that are large and varied enough).  This is fine, but could perhaps be pointed out if that is indeed the case.

Overall, this seems like a natural and effective approach, and achieves good results.
","The reviewer provides constructive criticism and suggestions for improvement, but ultimately finds the approach natural, effective, and yielding good results. This suggests a generally positive sentiment. The language used is polite and professional, employing phrases like ""I would have liked to see"" and ""it seems"" to express suggestions and observations without being overly critical.",60,80
"The paper proposes an approach to train generators within a GAN framework, in the setting where one has access only to degraded / imperfect measurements of real samples, rather than the samples themselves. Broadly, the approach is to have a generator produce the ""full"" real data, pass it through a simulated model of the measurement process, and then train the discriminator to distinguish between these simulated measurements of generated samples, and true measurements of real samples. By this mechanism, the proposed method is able to train GANs to generate high-quality samples from only imperfect measurements.

The paper is largely well-written and well-motivated, the overall setup is interesting (I find the authors' practical use cases convincing---where one only has access to imperfect data in the first place), and the empirical results are convincing. The theoretical proofs do make strong assumptions (in particular, the fact that the true distribution must be uniquely constrained by its marginal along the measurement). However, in most theoretical analysis of GANs and neural networks in general, I view proofs as a means of gaining intuition rather than being strong guarantees---and to that end, I found the analysis in this paper to be informative.

I would make a  suggestions for possible further experimental analysis: it would be nice to see how robust the approach is to systematic mismatches between the true and modeled measurement functions (for instance, slight differences in the blur kernels, noise variance, etc.). Especially in the kind of settings the paper considers, I imagine it may sometimes also be hard to accurately model the measurement function of a device (or it may be necessary to use a computationally cheaper approximation for training). I think a study of how such mismatches affect the training procedure would be instructive (perhaps more so than some of the quantitative evaluation given that they at best only approximately measure sample quality).","The reviewer provides positive feedback, calling the paper ""well-written and well-motivated"" and the results ""convincing."" They find the theoretical analysis informative, even acknowledging its limitations. While they suggest further experiments, they frame it as potential improvement rather than a flaw.",75,90
"This article aims at understanding the role played by the different words in a sentence, taking into account their order in the sentence. In sentiment analysis for instance, this capacity is critical to model properly negation.
As state-of-the-art approaches rely on LSTM, the authors want to understand which information comes from which gate. After a short remainder regarding LSTM, the authors propose a framework to disambiguate interactions between gates. In order to obtain an analytic formulation of the decomposition, the authors propose to linearize activation functions in the network.
In the experiment section, authors compare themselves to a standard logistic regression (based on a bag of words representation). They also check the unigram sentiment scores (without context).
The main issue consists in modeling the dynamics inside a sentence (when a negation or a 'used to be' reverses the sentiment). The proposed approach works fine on selected samples.


The related work section is entirely focused on deep learning while the experiment section is dedicated to sentiment analysis. This section should be rebalanced. Even if the authors claim that their approach is general, they also show that it fits well the sentiment analysis task in particular.

On top of that, a lot of fine-grained sentiment analysis tools has been developed outside deep-learning: the authors should refer to those works.

Finally, authors should provide some quantitative analysis on sentiment classification: a lot of standard benchmarks are widely use in the literature and we need to see how the proposed method performs with respect to the state-of-the-art.


Given the chosen tasks, this work should be compared to the beermind system:
http://deepx.ucsd.edu/#/home/beermind
and the associated publication
http://arxiv.org/pdf/1511.03683.pdf","The review presents a mixed sentiment. While it acknowledges the relevance of the research question and the proposed framework, it points out significant shortcomings, particularly the lack of comprehensive comparison with existing methods and limited quantitative analysis. The language, however, remains polite and constructive throughout, suggesting areas of improvement rather than outright criticism.",-10,60
"This very well written paper covers the span between W-GAN and VAE. For a reviewer who is not an expert in the domain, it reads very well, and would have been of tutorial quality if space had allowed for more detailed explanations. The appendix are very useful, and tutorial paper material (especially A). 

While I am not sure description would be enough to reproduce and no code is provided, every aspect of the architecture, if not described, if referred as similar to some previous work. There are also some notation shortcuts (not explained) in the proof of theorems that can lead to initial confusion, but they turn out to be non-ambiguous. One that could be improved is P(P_X, P_G) where one loses the fact that the second random variable is Y.


This work contains plenty of novel material, which is clearly compared to previous work:
- The main consequence of the use of Wasserstein distance is the surprisingly simple and useful Theorem 1. I could not verify its novelty, but this seems to be a great contribution.
- Blending GAN and auto-encoders has been tried in the past, but the authors claim better theoretical foundations that lead to solutions that do not rquire min-max
- The use of MMD in the context of GANs has also been tried. The authors claim that their use in the latent space makes it more practival

The experiments are very convincing, both numerically and visually.

Source of confusion: in algorithm 1 and 2, \tilde{z} is ""sampled"" from Q_TH(Z|xi), some one is lead to believe that this is the sampling process as in VAEs, while in reality Q_TH(Z|xi) is deterministic in the experiments.","The review starts with very positive statements about the paper's clarity and contribution ('very well written', 'tutorial quality', 'plenty of novel material', 'great contribution'). It also praises the experiments as 'very convincing'. While it points out areas for improvement, it frames them constructively. The language is formal and focuses on the paper's strengths and weaknesses without resorting to personal criticism.",85,90
"To date the theoretical advantage of deep learning has focused on the concept of ""expressive efficiency"" where one network must grow much larger to replicate functions that another ""more efficient"" network can produce. This has focused so far on depth (i.e. shallow networks have to grow much larger than deeper networks to express the same set of networks)

The authors explore another dimension here, namely that of ""connectivity"". They study dilated convolutional networks and show that intertwining two dilated convolutional networks A and B at various stages (formalized via mixed tensor decompositions) it is more expressively efficient than not intertwining. 

The authors' experiments support their theory showing that their mixed strategy leads to gains over a vanilla dilated convolutional net.

I found the paper very well written despite its level of mathematical depth (the authors provide many helpful pictures) and strongly recommend accepting this paper.

","The review is highly positive. The reviewer explicitly recommends acceptance and praises the paper's clarity, writing, and mathematical depth. They find the paper's exploration of ""connectivity"" in deep learning to be valuable and well-supported by experiments. The language used throughout is professional and courteous.",90,100
"--------------
Summary:
--------------
This paper presents a series of experiments on language emergence through referential games between two agents. They ground these experiments in both fully-specified symbolic worlds and through raw, entangled, visual observations of simple synthetic scenes. They provide rich analysis of the emergent languages the agents produce under different experimental conditions. This analysis (especially on raw pixel images) make up the primary contribution of this work.


--------------
Evaluation:
--------------
Overall I think the paper makes some interesting contributions with respect to the line of recent 'language emergence' papers. The authors provide novel analysis of the learned languages and perceptual system across a number of environmental settings, coming to the (perhaps uncontroversial) finding that varying the environment and restrictions on language result in variations in the learned communication protocols. 

In the context of existing literature, the novelty of this work is somewhat limited -- consisting primarily of the extension of multi-agent reference games to raw-pixel inputs. While this is a non-trivial extension, other works have demonstrated language learning in similar referring-expression contexts (essentially modeling only the listener model [Hermann et.al 2017]). 

I have a number of requests for clarification in the weaknesses section which I think would improve my understanding of this work and result in a stronger submission if included by the authors.  

--------------
Strengths:
--------------
- Clear writing and document structure. 


- Extensive experimental setting tweaks which ablate the information and regularity available to the agents. The discussion of the resulting languages is appropriate and provides some interesting insights.


- A number of novel analyses are presented to evaluate the learned languages and perceptual systems. 


--------------
Weaknesses:
--------------
- How stable are the reported trends / languages across multiple runs within the same experimental setting? The variance of REINFORCE policy gradients (especially without a baseline) plus the general stochasticity of SGD on randomly initialized networks leads me to believe that multiple training runs of these agents might result is significantly different codes / performance. I am interested in hearing the author's experiences in this regard and if multiple runs present similar quantitative and qualitative results. I admit that expecting identical codes is unrealistic, but the form of the codes (i.e. primarily encoding position) might be consistent even if the individual mappings are not).


- I don't recall seeing descriptions of the inference-time procedure used to evaluate training / test accuracy. I will assume argmax decoding for both speaker and listener. Please clarify or let me know if I missed something.


- There is ambiguity in how the ""protocol size"" metric is computed. In Table 1, it is defined as 'the effective number of unique message used'. This comes back to my question about decoding I suppose, but does this count the 'inference-time' messages or those produced during training? 
Furthermore, Table 2 redefines ""protocol size"" as the percentage of novel message. I assume this is an editing error given the values presented and take these columns as counts. It also seems ""protocol size"" is replaced with the term ""lexicon"" from 4.1 onward.

- I'm surprised by how well the agents generalize in the raw pixel data experiments. In fact, it seems that across all games the test accuracy remains very close to the train accuracy. 

Given the dataset is created by taking all combinations of color / shape and then sampling 100 location / floor color variations, it is unlikely that a shape / color combo has not been seen in training. Such that the only novel variations are likely location and floor color. However, taking Game A as an example, the probe classifiers are relatively poor at these attributes -- indicating the speaker's representation is not capturing these attributes well. Then how do the agents effectively differentiate so well between 20 images leveraging primarily color and shape?

I think some additional analysis of this setting might shed some light on this issue. One thought is to compute upper-bounds based on ground truth attributes. Consider a model which knows shape perfectly, but cannot predict other attributes beyond chance. To compute the performance of such a model, you could take the candidate set, remove any instances not matching the ground truth shape, and then pick randomly from the remaining instances. Something similar could be repeated for all attributes independently as well as their combinations -- obviously culminating in 100% accuracy given all 4. It could be that by dataset construction, object location and shape are sufficient to achieve high accuracy because the odds of seeing the same shape at the same location (but different color) is very low. 

Given these are operations on annotations and don't require time-consuming model training, I hope to see this analysis in the rebuttal to put the results into appropriate context.


- What is random chance for the position and floor color probe classifiers? I don't think it is mentioned how many locations / floor colors are used in generation.  


- Relatively minor complaint: Both agents are trained via the REINFORCE policy gradient update rule; however, the listener agent makes a fairly standard classification decision and could be trained with a standard cross-entropy loss. That is to say, the listener policy need not make intermediate discrete policy decisions. This decision to withhold available supervision is not discussed in the paper (as far as I noticed), could the authors speak to this point?



--------------
Curiosities:
--------------
- I got the impression from the results (specifically the lack of discussion about message length) that in these experiments agents always issued full length messages even though they did not need to do so. If true, could the authors give some intuition as to why? If untrue, what sort of distribution of lengths do you observe?

- There is no long term planning involved in this problem, so why use reinforcement learning over some sort of differentiable sampler? With some re-parameterization (i.e. Gumbel-Softmax), this model could be end-to-end differentiable.


--------------
Minor errors:
--------------
[2.2 paragraph 1] LSTM citation should not be in inline form.
[3 paragraph 1] 'Note that these representations do care some' -> carry
[3.3.1 last paragraph] 'still able comprehend' --> to


-------
Edit
-------
Updating rating from 6 to 7.","The review is overall positive. The reviewer finds the paper interesting and well-written, highlighting the extensive experiments and novel analyses as strengths. While acknowledging the work's novelty is somewhat limited and primarily lies in extending existing methods to raw-pixel inputs, the reviewer finds value in the research. The reviewer poses several clarifying questions and suggestions for improvement, indicating a desire to see the work strengthened rather than rejected. The reviewer's tone is constructive and polite, suggesting areas for improvement without resorting to harsh or disrespectful language.",60,80
"I wonder how the ASGD compares to other optimization schemes applicable to DL, like Entropy-SGD, which is yet another algorithm that provably improves over SGD. This question is also valid when it comes to other optimization schemes that are designed for deep learning problems. For instance, Entropy-SGD and Path-SGD should be mentioned and compared with. As a consequence, the literature analysis is insufficient. 

Authors provided necessary clarifications. I am raising my score.




","The review starts with a critical point, questioning the positioning of the research against other optimization schemes. This suggests a somewhat negative sentiment. However, the language remains technical and lacks overtly negative phrasing. The second part of the review, where the reviewer mentions raising their score due to clarifications, indicates a shift towards a more positive view. Overall, the tone is critical but professional, suggesting room for improvement in the paper.",-20,50
"This paper formulates the Jeopardy QA as a query reformulation task that leverages a search engine.  In particular, a user will try a sequence of alternative queries based on the original question in order to find the answer.  The RL formulation essentially tries to mimic this process.  Although this is an interesting formulation, as promoted by some recent work, this paper does not provide compelling reasons why it's a good formulation.  The lack of serious comparisons to baseline methods makes it hard to judge the value of this work.

Detailed comments/questions:
	1. I am actually quite confused on why it's a good RL setting. For a human user, having a series of queries to search for the right answer is a natural process, but it's not natural for a computer program.  For instance, each query can be viewed as different formulation of the same question and can be issued concurrently. Although formulated as an RL problem,  it is not clear to me whether the search result after each episode has been used as the immediate environment feedback. As a result, the dependency between actions seems rather weak.
	2. I also feel that the comparisons to other baselines (not just the variation of the proposed system) are not entirely fair. For instance, the baseline BiDAF model has only one shot, namely using the original question as query.  In this case, AQA should be allowed to use the same budget -- only one query.  Another more realistic baseline is to follow the existing work on query formulation in the IR community.  For example, 20 shorter queries generated by methods like [1] can be used to compare the queries created by AQA.

[1] Kumaran & Carvalho. ""Reducing Long Queries Using Query Quality Predictors"". SIGIR-09
	
Pros:
	1. An interesting RL formulation for query reformulation

Cons:
	1. The use of RL is not properly justified
	2. The empirical result is not convincing that the proposed method is indeed advantageous 

---------------------------------------

After reading the author response and checking the revised paper, I'm both delighted and surprised that the authors improved the submission substantially and presented stronger results.  I believe the updated version has reached the bar and recommend accepting this paper.  ","The reviewer initially expresses confusion and skepticism towards the paper's approach, questioning its validity and fairness of comparisons. This, combined with the use of phrases like ""not compelling reasons,"" ""not entirely fair,"" and ""not convincing,"" suggests a negative sentiment. However, the tone is professional and provides constructive criticism. The final paragraph reveals a significant shift in sentiment after the authors addressed the reviewer's concerns. The reviewer's delight and surprise at the improvements, coupled with the recommendation for acceptance, indicate a now positive sentiment. The language throughout remains polite, offering suggestions and critiques rather than harsh judgments.",-10,70
"The authors clearly describe the problem being addressed in the manuscript and motivate their solution very clearly. The proposed solution seems very intuitive and the empirical evaluations demonstrates its utility. My main concern is the underlying assumption (if I understand correctly) that the adversarial attack technique that the detector has to handle needs to be available at the training time of the detector. Especially since the empirical evaluations are designed in such a way where the training and test data for the detector are perturbed with the same attack technique. However, this does not invalidate the contributions of this manuscript.

Specific comments/questions:
- (Minor) Page 3, Eq 1: I think the expansion dimension cares more about the probability mass in the volume rather than the volume itself even in the Euclidean setting.
- Section 4: The different pieces of the problem (estimation, intuition for adversarial subspaces, efficiency) are very well described.
- Alg 1, L3: Is this where the normal exmaples are converted to adversarial examples using some attack technique? 
- Alg 1, L12: Is LID_norm computed using a leave-one-out estimate? Otherwise, r_1(.) for each point is 0, leading to a somewhat ""under-estimate"" of the true LID of the normal points in the training set. I understand that it is not an issue in the test set.
- Section 4 and Alg 1: S we do not really care about the ""labels/targets"" of the examples. All examples in the dataset are considered ""normal"" to start with. Is this assuming that the ""initial training set"" which is used to obtain the ""pre-trained DNN"" free of adversarial examples?
- Section 5, Experimental Setup: Seems like normal points in the test set would get lesser values if we are not doing the ""leave-one-out"" version of the estimation.
- Section 5: The authors have done a great job at evaluating every aspect of the proposed method.
","The reviewer starts by highlighting the clarity and intuitiveness of the proposed solution, indicating a positive sentiment. While they raise a valid concern about the assumption of attack techniques being available during training, they acknowledge it doesn't invalidate the contributions. The use of phrases like ""very well described"" and ""great job"" further suggests a positive sentiment. The reviewer maintains a neutral and professional tone throughout, focusing on specific aspects of the work without resorting to personal opinions or harsh language.",60,80
